{
    "content": [
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = overall_data[overall_data.intent == 'Suicide']",
                "ASSIGN = overall_data[overall_data.intent == 'Homicide']"
            ],
            "source_orig": [
                "overall_data = pd.read_csv('../input/guns.csv')\n",
                "suicide_data = overall_data[overall_data.intent == 'Suicide']\n",
                "homicide_data = overall_data[overall_data.intent == 'Homicide']"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = [overall_data, suicide_data, homicide_data]",
                "ASSIGN = ['Overall', 'Suicide', 'Homicide']",
                "ASSIGN = ['year', 'month', 'intent', 'sex', 'age', 'race', 'place', 'education']",
                "for i, d in enumerate(ASSIGN):",
                "for col in ASSIGN:",
                "ASSIGN == 'age' or col == 'month':",
                "d[col].value_counts().sort_index().plot(kind = 'line')",
                "ASSIGN == 'year' or col == 'education':",
                "d[col].value_counts().sort_index().plot(kind = 'bar')",
                "else:",
                "d[col].value_counts().plot(kind = 'bar')",
                "plt.title(ASSIGN[i] + ': ' + col)",
                "plt.show()"
            ],
            "source_orig": [
                "data_sources = [overall_data, suicide_data, homicide_data]\n",
                "titles = ['Overall', 'Suicide', 'Homicide']\n",
                "\n",
                "interested_columns = ['year', 'month', 'intent', 'sex', 'age', 'race', 'place', 'education']\n",
                "for i, d in enumerate(data_sources):\n",
                "    for col in interested_columns:\n",
                "        if col == 'age' or col == 'month':\n",
                "            d[col].value_counts().sort_index().plot(kind = 'line')\n",
                "        elif col == 'year' or col == 'education':\n",
                "            d[col].value_counts().sort_index().plot(kind = 'bar')\n",
                "        else:\n",
                "            d[col].value_counts().plot(kind = 'bar')\n",
                "        plt.title(titles[i] + ': ' + col)\n",
                "        plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path')"
            ],
            "source_orig": [
                "menu = pd.read_csv('../input/menu.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "menu.head()"
            ],
            "source_orig": [
                "menu.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "menu.info()"
            ],
            "source_orig": [
                "menu.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "menu.Category.value_counts()"
            ],
            "source_orig": [
                "menu.Category.value_counts()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = ['Calories', 'Total Fat', 'Cholesterol','Sodium', 'Sugars', 'Carbohydrates']",
                "for m in ASSIGN:",
                "ASSIGN = sns.swarmplot(x=\"Category\", y=m, data=menu)",
                "plt.setp(ASSIGN.get_xticklabels(), rotation=45)",
                "plt.title(m)",
                "plt.show()"
            ],
            "source_orig": [
                "measures = ['Calories', 'Total Fat', 'Cholesterol','Sodium', 'Sugars', 'Carbohydrates']\n",
                "\n",
                "for m in measures:   \n",
                "    plot = sns.swarmplot(x=\"Category\", y=m, data=menu)\n",
                "    plt.setp(plot.get_xticklabels(), rotation=45)\n",
                "    plt.title(m)\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('Highest Calorie meal: {} \\n\\",
                "Highest Fat meal: {} \\n\\",
                "Highest Cholesterol meal: {} \\n\\",
                "Highest Sodium meal: {} \\n\\",
                "Highest Sugar meal: {} \\n\\",
                "Highest Carb meal: {}'.format(menu.Item[menu['Calories'].idxmax()],\\",
                "menu.Item[menu['Total Fat'].idxmax()],\\",
                "menu.Item[menu['Cholesterol'].idxmax()],\\",
                "menu.Item[menu['Sodium'].idxmax()],\\",
                "menu.Item[menu['Sugars'].idxmax()],\\",
                "menu.Item[menu['Carbohydrates'].idxmax()]))"
            ],
            "source_orig": [
                "print('Highest Calorie meal: {} \\n\\\n",
                "       Highest Fat meal: {} \\n\\\n",
                "       Highest Cholesterol meal: {} \\n\\\n",
                "       Highest Sodium meal: {} \\n\\\n",
                "       Highest Sugar meal: {} \\n\\\n",
                "       Highest Carb meal: {}'.format(menu.Item[menu['Calories'].idxmax()],\\\n",
                "                                     menu.Item[menu['Total Fat'].idxmax()],\\\n",
                "                                     menu.Item[menu['Cholesterol'].idxmax()],\\\n",
                "                                     menu.Item[menu['Sodium'].idxmax()],\\\n",
                "                                     menu.Item[menu['Sugars'].idxmax()],\\\n",
                "                                     menu.Item[menu['Carbohydrates'].idxmax()]))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "warnings.filterwarnings(\"ignore\")"
            ],
            "source_orig": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import warnings\n",
                "import seaborn as sns\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = pd.read_csv('..path')",
                "print(ASSIGN.head())"
            ],
            "source_orig": [
                "tb = pd.read_csv('../input/tobacco.csv')\n",
                "print(tb.head())"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tb.info()"
            ],
            "source_orig": [
                "tb.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ['Smoke everyday', 'Smoke some days', 'Former smoker', 'Never smoked']",
                "for x in ASSIGN:",
                "ASSIGN = ASSIGN.str.strip('%').astype('float')"
            ],
            "source_orig": [
                "columns = ['Smoke everyday', 'Smoke some days', 'Former smoker', 'Never smoked']\n",
                "\n",
                "for x in columns:\n",
                "    tb[x] = tb[x].str.strip('%').astype('float')\n",
                "    "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tb.head()"
            ],
            "source_orig": [
                "tb.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = tb.groupby(['Year'], as_index = False).mean()",
                "ASSIGN = plt.figure(figsize = (8,6))",
                "ASSIGN = fig.add_subplot(2,2,1)",
                "ASSIGN = fig.add_subplot(2,2,2)",
                "ASSIGN = fig.add_subplot(2,2,3)",
                "ASSIGN = fig.add_subplot(2,2,4)",
                "ASSIGN.head()",
                "ASSIGN = 'Percentage of people'",
                "ASSIGN = 'Year'",
                "ASSIGN.set(title = 'Smoke everyday', ylabel = ASSIGN, xlabel = ASSIGN)",
                "ASSIGN.set(title = 'Smoke some days', ylabel = ASSIGN, xlabel = ASSIGN)",
                "ASSIGN.set(title = 'Former smoker', ylabel = ASSIGN, xlabel = ASSIGN)",
                "ASSIGN.set(title = 'Never smoked', ylabel = ASSIGN, xlabel = ASSIGN)",
                "ASSIGN.scatter(ASSIGN.Year, ASSIGN['Smoke everyday'], )",
                "ASSIGN.scatter(ASSIGN.Year, ASSIGN['Smoke some days'])",
                "ASSIGN.scatter(ASSIGN.Year, ASSIGN['Former smoker'])",
                "ASSIGN.scatter(ASSIGN.Year, ASSIGN['Never smoked'])",
                "ASSIGN.tight_layout()",
                "ASSIGN.autofmt_xdate()",
                "plt.show()"
            ],
            "source_orig": [
                "tb_group = tb.groupby(['Year'], as_index = False).mean()\n",
                "\n",
                "fig = plt.figure(figsize = (8,6))\n",
                "ax1 = fig.add_subplot(2,2,1)\n",
                "ax2 = fig.add_subplot(2,2,2)\n",
                "ax3 = fig.add_subplot(2,2,3)\n",
                "ax4 = fig.add_subplot(2,2,4)\n",
                "\n",
                "tb_group.head()\n",
                "\n",
                "y = 'Percentage of people'\n",
                "x = 'Year'\n",
                "\n",
                "ax1.set(title = 'Smoke everyday', ylabel = y, xlabel = x)\n",
                "ax2.set(title = 'Smoke some days', ylabel = y, xlabel = x)\n",
                "ax3.set(title = 'Former smoker', ylabel = y, xlabel = x)\n",
                "ax4.set(title = 'Never smoked', ylabel = y, xlabel = x)\n",
                "ax1.scatter(tb_group.Year, tb_group['Smoke everyday'], )\n",
                "ax2.scatter(tb_group.Year, tb_group['Smoke some days'])\n",
                "ax3.scatter(tb_group.Year, tb_group['Former smoker'])\n",
                "ax4.scatter(tb_group.Year, tb_group['Never smoked'])\n",
                "\n",
                "fig.tight_layout()\n",
                "fig.autofmt_xdate()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "SETUP",
                "ASSIGN = set(tb.State)",
                "ASSIGN = {}",
                "for state in ASSIGN:",
                "ASSIGN = stats.linregress(tb.Year[tb.State == state], tb['Never smoked'][tb.State == state])",
                "ASSIGN[state] = slope",
                "ASSIGN = pd.DataFrame([slope_dict]).transpose()",
                "ASSIGN.columns = ['slope']",
                "ASSIGN.sort(columns = 'slope', ascending = True, inplace = True)"
            ],
            "source_orig": [
                "from scipy import stats\n",
                "\n",
                "states = set(tb.State)\n",
                "\n",
                "slope_dict = {}\n",
                "\n",
                "for state in states:\n",
                "    slope, intercept, r_value, p_value, std_err = stats.linregress(tb.Year[tb.State == state], tb['Never smoked'][tb.State == state])\n",
                "    slope_dict[state] = slope\n",
                "    \n",
                "slope_df = pd.DataFrame([slope_dict]).transpose()\n",
                "slope_df.columns = ['slope']\n",
                "slope_df.sort(columns = 'slope', ascending = True, inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = {}",
                "for state in states:",
                "ASSIGN = stats.linregress(tb.Year[tb.State == state], tb['Smoke everyday'][tb.State == state])",
                "ASSIGN[state] = slope",
                "ASSIGN = pd.DataFrame([slope_dict1]).transpose()",
                "ASSIGN.columns = ['slope']",
                "ASSIGN.sort(columns = 'slope',ascending = False, inplace = True)"
            ],
            "source_orig": [
                "slope_dict1 = {}\n",
                "\n",
                "for state in states:\n",
                "    slope, intercept, r_value, p_value, std_err = stats.linregress(tb.Year[tb.State == state], tb['Smoke everyday'][tb.State == state])\n",
                "    slope_dict1[state] = slope\n",
                "    \n",
                "slope_df1 = pd.DataFrame([slope_dict1]).transpose()\n",
                "slope_df1.columns = ['slope']\n",
                "slope_df1.sort(columns = 'slope',ascending = False, inplace = True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "slope_df.plot(kind = 'bar', figsize = (10,6), title = 'Never Smoked: % Changes from 1994 to 2010')",
                "slope_df1.plot(kind = 'bar', figsize = (10,6), title = 'Smoke everyday: % Changes from 1994 to 2010')",
                "plt.show()"
            ],
            "source_orig": [
                "slope_df.plot(kind = 'bar', figsize = (10,6), title = 'Never Smoked: % Changes from 1994 to 2010')\n",
                "slope_df1.plot(kind = 'bar', figsize = (10,6), title = 'Smoke everyday: % Changes from 1994 to 2010')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "warnings.filterwarnings(\"ignore\")",
                "ASSIGN = pd.read_csv('..path')",
                "print(ASSIGN.head())"
            ],
            "source_orig": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "import seaborn as sns\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "vg_sales = pd.read_csv('../input/vgsales.csv')\n",
                "print(vg_sales.head())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = [2016, 2017, 2020]",
                "ASSIGN = vg_sales.groupby(['Year']).sum().drop(years)",
                "ASSIGN = vg_sales.groupby(['Year']).mean().drop(years)",
                "ASSIGN = vg_sales.replace(0, np.nan).groupby(['Year']).count().drop(years)"
            ],
            "source_orig": [
                "years = [2016, 2017, 2020]\n",
                "total_sales_group = vg_sales.groupby(['Year']).sum().drop(years)\n",
                "average_sales_group = vg_sales.groupby(['Year']).mean().drop(years)\n",
                "count_sales_group = vg_sales.replace(0, np.nan).groupby(['Year']).count().drop(years)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def lineplot(df, title = 'Sales by Year', ylabel ='Sales' , legendsize = 10, legendloc = 'upper left'):",
                "ASSIGN = df.index.values",
                "ASSIGN = df.NA_Sales",
                "ASSIGN = df.EU_Sales",
                "ASSIGN = df.JP_Sales",
                "ASSIGN = df.Other_Sales",
                "ASSIGN = df.Global_Sales",
                "if df is count_sales_group:",
                "ASSIGN = [na, eu, jp, other]",
                "ASSIGN = ['NA', 'EU', 'JP', 'OTHER']",
                "else:",
                "ASSIGN = [na, eu, jp, other, global_]",
                "ASSIGN = ['NA', 'EU', 'JP', 'OTHER', 'WORLD WIDE']",
                "for i, region in enumerate(ASSIGN):",
                "plt.plot(ASSIGN, region, label = ASSIGN[i])",
                "plt.ylabel(ylabel)",
                "plt.xlabel('Year')",
                "plt.title(title)",
                "plt.legend(loc=legendloc, prop = {'size':legendsize})",
                "plt.show()",
                "plt.clf()",
                "for i, region in enumerate(ASSIGN):",
                "plt.plot(ASSIGN, region, label = ASSIGN[i])",
                "plt.yscale('log')",
                "plt.ylabel(ylabel)",
                "plt.xlabel('Year')",
                "plt.title(title + '(Log)')",
                "plt.legend(loc=legendloc, prop = {'size':legendsize})",
                "plt.show()",
                "plt.clf()"
            ],
            "source_orig": [
                "def lineplot(df, title = 'Sales by Year', ylabel ='Sales' , legendsize = 10, legendloc = 'upper left'):\n",
                "\n",
                "    year = df.index.values\n",
                "    na = df.NA_Sales\n",
                "    eu = df.EU_Sales\n",
                "    jp = df.JP_Sales\n",
                "    other = df.Other_Sales\n",
                "    global_ = df.Global_Sales\n",
                "    \n",
                "    if df is count_sales_group:\n",
                "        region_list = [na, eu, jp, other]\n",
                "        columns = ['NA', 'EU', 'JP', 'OTHER']\n",
                "    else:\n",
                "        region_list = [na, eu, jp, other, global_]\n",
                "        columns = ['NA', 'EU', 'JP', 'OTHER', 'WORLD WIDE']\n",
                "\n",
                "    for i, region in enumerate(region_list):\n",
                "        plt.plot(year, region, label = columns[i])\n",
                "\n",
                "    plt.ylabel(ylabel)\n",
                "    plt.xlabel('Year')\n",
                "    plt.title(title)\n",
                "    plt.legend(loc=legendloc, prop = {'size':legendsize})\n",
                "    plt.show()\n",
                "    plt.clf()\n",
                "\n",
                "    for i, region in enumerate(region_list):\n",
                "        plt.plot(year, region, label = columns[i])\n",
                "\n",
                "    plt.yscale('log')\n",
                "    plt.ylabel(ylabel)\n",
                "    plt.xlabel('Year')\n",
                "    plt.title(title + '(Log)')\n",
                "    plt.legend(loc=legendloc, prop = {'size':legendsize})\n",
                "    plt.show()\n",
                "    plt.clf()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "lineplot(total_sales_group, title = 'Sales by Year', ylabel ='Sales (In Millions)', legendsize = 8)"
            ],
            "source_orig": [
                "lineplot(total_sales_group, title = 'Sales by Year', ylabel ='Sales (In Millions)', legendsize = 8)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = vg_sales[['Name', 'JP_Sales', 'NA_Sales']][(vg_sales.Year>=1992) & (vg_sales.Year<=1996) & (vg_sales.JP_Sales > vg_sales.NA_Sales)].sort(columns = 'JP_Sales', ascending = False)",
                "print(ASSIGN.head(20))"
            ],
            "source_orig": [
                "japan1992_1996 = vg_sales[['Name', 'JP_Sales', 'NA_Sales']][(vg_sales.Year>=1992) & (vg_sales.Year<=1996) & (vg_sales.JP_Sales > vg_sales.NA_Sales)].sort(columns = 'JP_Sales', ascending = False)\n",
                "print(japan1992_1996.head(20))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "lineplot(average_sales_group, title = 'Average Revenue per Game per Year', ylabel ='Sales (In Millions)', legendsize = 8, legendloc = 'upper right')"
            ],
            "source_orig": [
                "lineplot(average_sales_group, title = 'Average Revenue per Game per Year', ylabel ='Sales (In Millions)', legendsize = 8, legendloc = 'upper right')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "lineplot(count_sales_group, title = 'Of All Games Produced Did Every Region Sell Every Game?\\n\\nGames for Sale by Region by Year', ylabel ='Count', legendsize = 8, legendloc = 'upper left')"
            ],
            "source_orig": [
                "lineplot(count_sales_group, title = 'Of All Games Produced Did Every Region Sell Every Game?\\n\\nGames for Sale by Region by Year', ylabel ='Count', legendsize = 8, legendloc = 'upper left')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = vg_sales[['Name', 'Year','Global_Sales']].sort(columns = 'Global_Sales', ascending = False)",
                "print(ASSIGN.head(20))"
            ],
            "source_orig": [
                "Top_games = vg_sales[['Name', 'Year','Global_Sales']].sort(columns = 'Global_Sales', ascending = False)\n",
                "print(Top_games.head(20))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "print(check_output([, ]).decode())"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns \n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path')"
            ],
            "source_orig": [
                "data_mat = pd.read_csv('../input/student-mat.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "data_mat.dtypes"
            ],
            "source_orig": [
                "data_mat.dtypes"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN[['address','studytime','failures','activities','nursery','higher','internet','famrel','freetime','absences','G3','romantic']]"
            ],
            "source_orig": [
                "data_mat = data_mat[['address','studytime','failures','activities','nursery','higher','internet','famrel','freetime','absences','G3','romantic']]"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = ['address','activities','nursery','higher','internet','romantic']",
                "for column in ASSIGN:",
                "print(column,,data_mat[column].unique())"
            ],
            "source_orig": [
                "binary_features = ['address','activities','nursery','higher','internet','romantic']\n",
                "for column in binary_features:\n",
                "    print(column,\"-\",data_mat[column].unique())"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "for column in binary_features:",
                "if (column == 'address'):",
                "data_mat[column] = data_mat[column].apply(lambda x: 1 if (x == 'U') else 0)",
                "else:",
                "data_mat[column] = data_mat[column].apply(lambda x: 1 if (x =='yes') else 0)",
                "print(column,,data_mat[column].unique())"
            ],
            "source_orig": [
                "for column in binary_features:\n",
                "    if (column == 'address'):\n",
                "        data_mat[column] = data_mat[column].apply(lambda x: 1 if (x == 'U') else 0)\n",
                "    else:\n",
                "        data_mat[column] = data_mat[column].apply(lambda x: 1 if (x =='yes') else 0)\n",
                "    print(column,\"-\",data_mat[column].unique())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,15))",
                "sns.heatmap(data_mat.corr(),annot = True,fmt = \".2f\",cbar = True)",
                "plt.xticks(rotation=90)",
                "plt.yticks(rotation = 0)"
            ],
            "source_orig": [
                "plt.figure(figsize=(15,15))\n",
                "sns.heatmap(data_mat.corr(),annot = True,fmt = \".2f\",cbar = True)\n",
                "plt.xticks(rotation=90)\n",
                "plt.yticks(rotation = 0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN[ASSIGN.columns.drop(['address','activities','nursery','freetime'])]"
            ],
            "source_orig": [
                "data_mat = data_mat[data_mat.columns.drop(['address','activities','nursery','freetime'])]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,15))",
                "sns.heatmap(data_mat.corr(),annot = True,fmt = \".2f\",cbar = True)",
                "plt.xticks(rotation=90)",
                "plt.yticks(rotation = 0)"
            ],
            "source_orig": [
                "plt.figure(figsize=(15,15))\n",
                "sns.heatmap(data_mat.corr(),annot = True,fmt = \".2f\",cbar = True)\n",
                "plt.xticks(rotation=90)\n",
                "plt.yticks(rotation = 0)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = pd.get_dummies(data_mat['ASSIGN'], drop_first = True)",
                "ASSIGN = pd.get_dummies(data_mat['ASSIGN'],drop_first = True)",
                "ASSIGN = pd.get_dummies(data_mat['ASSIGN'],drop_first = True)",
                "data_mat.drop(['ASSIGN','ASSIGN','ASSIGN'], axis =1, inplace = True)",
                "ASSIGN = pd.concat([ASSIGN,absences, failures,studytime],axis = 1)",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "absences = pd.get_dummies(data_mat['absences'], drop_first = True)\n",
                "failures = pd.get_dummies(data_mat['failures'],drop_first = True)\n",
                "studytime = pd.get_dummies(data_mat['studytime'],drop_first = True)\n",
                "\n",
                "data_mat.drop(['absences','failures','studytime'], axis =1, inplace = True)\n",
                "data_mat = pd.concat([data_mat,absences, failures,studytime],axis = 1)\n",
                "data_mat.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = data_mat.drop('romantic', axis = 1)",
                "ASSIGN = data_mat['romantic']"
            ],
            "source_orig": [
                "data_matf = data_mat.drop('romantic', axis = 1)\n",
                "data_matl = data_mat['romantic']"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "check_results",
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN=KFold(n_splits=10, shuffle=True, random_state=False)",
                "ASSIGN = DecisionTreeClassifier()",
                "ASSIGN = []",
                "for train_id, test_id in ASSIGN.split(data_matf,data_matl):",
                "X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]",
                "ASSIGN = data_matl.values[train_id], data_matl.values[test_id]",
                "ASSIGN.fit(X_train,y_train)",
                "ASSIGN = dtree.predict(X_test)",
                "ASSIGN = accuracy_score(y_test, predictions)",
                "ASSIGN.append(ASSIGN)",
                "plt.plot(range(10),ASSIGN)",
                "plt.show()",
                "ASSIGN = np.mean(outcomesDt)",
                "print(,ASSIGN)"
            ],
            "source_orig": [
                "from sklearn.model_selection import KFold\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "kf=KFold(n_splits=10, shuffle=True, random_state=False)\n",
                "dtree = DecisionTreeClassifier()\n",
                "\n",
                "outcomesDt = []\n",
                "for train_id, test_id in kf.split(data_matf,data_matl):\n",
                "    X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]\n",
                "    y_train, y_test = data_matl.values[train_id], data_matl.values[test_id]\n",
                "    dtree.fit(X_train,y_train)\n",
                "    predictions = dtree.predict(X_test)\n",
                "    accuracy = accuracy_score(y_test, predictions)\n",
                "    outcomesDt.append(accuracy)\n",
                "plt.plot(range(10),outcomesDt)\n",
                "plt.show()\n",
                "average_error_Dt = np.mean(outcomesDt)\n",
                "print(\"the average error is equal to \",average_error_Dt)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "check_results",
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN=RandomForestClassifier(n_estimators=10)",
                "ASSIGN=[]",
                "for train_id, test_id in kf.split(data_matf,data_matl):",
                "X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]",
                "ASSIGN = data_matl.values[train_id], data_matl.values[test_id]",
                "ASSIGN.fit(X_train,y_train)",
                "ASSIGN = Rf.predict(X_test)",
                "ASSIGN = accuracy_score(y_test, predictions)",
                "ASSIGN.append(ASSIGN)",
                "plt.plot(range(10),ASSIGN)",
                "plt.show()",
                "print(,np.mean(ASSIGN))"
            ],
            "source_orig": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "Rf=RandomForestClassifier(n_estimators=10)\n",
                "outcomesRf=[]\n",
                "for train_id, test_id in kf.split(data_matf,data_matl):\n",
                "    X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]\n",
                "    y_train, y_test = data_matl.values[train_id], data_matl.values[test_id]\n",
                "    Rf.fit(X_train,y_train)\n",
                "    predictions = Rf.predict(X_test)\n",
                "    accuracy = accuracy_score(y_test, predictions)\n",
                "    outcomesRf.append(accuracy)\n",
                "plt.plot(range(10),outcomesRf)\n",
                "plt.show()\n",
                "print(\"the average error is equal to \",np.mean(outcomesRf))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results",
                "visualize_data"
            ],
            "source": [
                "CHECKPOINT",
                "ForestTreesPerformance = []",
                "for n_trees in range(1,11,1):",
                "ASSIGN=RandomForestClassifier(n_estimators=n_trees)",
                "ASSIGN=[]",
                "for train_id, test_id in kf.split(data_matf,data_matl):",
                "X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]",
                "ASSIGN = data_matl.values[train_id], data_matl.values[test_id]",
                "Rf.fit(X_train,y_train)",
                "ASSIGN = Rf.predict(X_test)",
                "ASSIGN = accuracy_score(y_test, predictions)",
                "ASSIGN.append(ASSIGN)",
                "ForestTreesPerformance.append(np.mean(ASSIGN))",
                "plt.plot(range(1,11,1),ForestTreesPerformance)",
                "plt.show()",
                "if (min(ForestTreesPerformance) > average_error_Dt):",
                "print()",
                "else:",
                "print(,np.argmin(ForestTreesPerformance)+1,)"
            ],
            "source_orig": [
                "ForestTreesPerformance = []\n",
                "for n_trees in range(1,11,1):\n",
                "    pRf=RandomForestClassifier(n_estimators=n_trees)\n",
                "    outcomesRfs=[]\n",
                "    for train_id, test_id in kf.split(data_matf,data_matl):\n",
                "        X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]\n",
                "        y_train, y_test = data_matl.values[train_id], data_matl.values[test_id]\n",
                "        Rf.fit(X_train,y_train)\n",
                "        predictions = Rf.predict(X_test)\n",
                "        accuracy = accuracy_score(y_test, predictions)\n",
                "        outcomesRfs.append(accuracy)\n",
                "    ForestTreesPerformance.append(np.mean(outcomesRfs))\n",
                "plt.plot(range(1,11,1),ForestTreesPerformance)\n",
                "plt.show()\n",
                "if (min(ForestTreesPerformance) > average_error_Dt):\n",
                "    print(\"A decision tree works better than a random forest with respect to the probability error\")\n",
                "else:\n",
                "    print(\"A random forest with\",np.argmin(ForestTreesPerformance)+1,\"trees works better than a decision tree with respect to the probability error\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN=np.min(X)",
                "ASSIGN=np.max(X)",
                "ASSIGN=np.min(Y)",
                "ASSIGN=np.max(Y)",
                "print(, ASSIGN, ASSIGN)",
                "print(, ASSIGN, ASSIGN)",
                "plt.xlim(0,ASSIGN+1)",
                "plt.ylim(0,ASSIGN+1)",
                "plt.scatter(X,Y, 10, color = 'blue')",
                "plt.show()"
            ],
            "source_orig": [
                "# Setting up some global values and also plotting the data to get a feel for it.\n",
                "\n",
                "Xmin=np.min(X)\n",
                "Xmax=np.max(X)\n",
                "Ymin=np.min(Y)\n",
                "Ymax=np.max(Y)\n",
                "print(\"X min & max:\", Xmin, Xmax)\n",
                "print(\"Y min & max:\", Ymin, Ymax)\n",
                "\n",
                "\n",
                "# plt.xlim(Xmin-1,Xmax+1)\n",
                "plt.xlim(0,Xmax+1)\n",
                "plt.ylim(0,Ymax+1)\n",
                "\n",
                "plt.scatter(X,Y, 10, color = 'blue')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN=np.empty_like(X, dtype=\"float64\")   # ASSIGN X standardised",
                "ASSIGN=np.empty_like(Y, dtype=\"float64\")   # ASSIGN Y standardised",
                "ASSIGN = (X - Xmin)path(Xmax-Xmin)",
                "ASSIGN = (Y - Ymin)path(Ymax-Ymin)",
                "for i in range(0, X.size):",
                "print(, i, .format(ASSIGN[i]),.format(ASSIGN[i]))",
                "plt.scatter(ASSIGN,ASSIGN, 10, color = 'blue')",
                "plt.show()"
            ],
            "source_orig": [
                "X_s=np.empty_like(X, dtype=\"float64\")     # X_s  X standardised\n",
                "Y_s=np.empty_like(Y, dtype=\"float64\")     # Y_s  Y standardised\n",
                "\n",
                "X_s = (X - Xmin)/(Xmax-Xmin)\n",
                "Y_s = (Y - Ymin)/(Ymax-Ymin)\n",
                "\n",
                "\n",
                "for i in range(0, X.size):\n",
                "    print(\"i= \", i, \"\\t\\tx {:6.2f}\".format(X_s[i]),\"\\ty {:6.2f}\".format(Y_s[i]))\n",
                "\n",
                "\n",
                "\n",
                "plt.scatter(X_s,Y_s, 10, color = 'blue')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def CalculateNewY(X_orig, slope, intercept):",
                "ASSIGN = np.empty_like(X_orig, dtype=\"float64\")",
                "ASSIGN = X_orig*slope+intercept",
                "return Y_calc"
            ],
            "source_orig": [
                "def CalculateNewY(X_orig, slope, intercept):\n",
                "    \n",
                "    Y_calc = np.empty_like(X_orig, dtype=\"float64\")\n",
                "\n",
                "    Y_calc = X_orig*slope+intercept\n",
                "        \n",
                "    return Y_calc"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def CalculateSSE(original_Y, predicted_Y):",
                "ASSIGN=0.0",
                "for i in range(0, original_Y.size):",
                "ASSIGN += (original_Y[i]-predicted_Y[i])**2",
                "ASSIGN = theSSEpath",
                "return theSSE"
            ],
            "source_orig": [
                "def CalculateSSE(original_Y, predicted_Y):\n",
                "    theSSE=0.0\n",
                "    \n",
                "    for i in range(0, original_Y.size):\n",
                "        theSSE += (original_Y[i]-predicted_Y[i])**2\n",
                "        \n",
                "    theSSE = theSSE/2\n",
                "    \n",
                "    return theSSE"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "CHECKPOINT",
                "def DrawLineFromFormula(slope, intercept, color):",
                "plt.xlim(-0.05, 1.05)",
                "plt.ylim(-0.05, 1.05)",
                "ASSIGN = np.arange(-100, 100, 0.1)",
                "plt.plot(ASSIGN, slope*ASSIGN+intercept, color)",
                "return"
            ],
            "source_orig": [
                "# Helper function\n",
                "\n",
                "def DrawLineFromFormula(slope, intercept, color):\n",
                "    plt.xlim(-0.05, 1.05)\n",
                "    plt.ylim(-0.05, 1.05)\n",
                "    x = np.arange(-100, 100, 0.1)\n",
                "    plt.plot(x, slope*x+intercept, color)\n",
                "    return"
            ]
        },
        {
            "tags": [
                "check_results",
                "visualize_data"
            ],
            "source": [
                "CHECKPOINT",
                "print(m, c)",
                "print(, Ymin)",
                "print(, Ymax - Ymin)",
                "ASSIGN = (c * (Ymax - Ymin)) + Ymin",
                "print(.format(m),.format(ASSIGN))",
                "ASSIGN = np.arange(0, Xmax+1, 0.1, dtype=\"float64\")",
                "ASSIGN = np.empty_like(x, dtype=\"float64\")",
                "ASSIGN = m*x + c_final",
                "ASSIGN = np.arange(Xmin-1, Xmax+1, 0.1)",
                "plt.plot(ASSIGN, m*ASSIGN+ASSIGN, 'g--')",
                "plt.scatter(X,Y, 10, color = 'blue')",
                "plt.show()"
            ],
            "source_orig": [
                "print(m, c)\n",
                "print(\"Ymin:\", Ymin)\n",
                "print(\"Y Range:\", Ymax - Ymin)\n",
                "\n",
                "c_final = (c * (Ymax - Ymin)) + Ymin\n",
                "\n",
                "print(\"m {:6.4f}\".format(m),\"\\t final c {:6.4f}\".format(c_final))\n",
                "\n",
                "# plt.xlim(-1,Xmax+1)\n",
                "# plt.ylim(2,Ymax+1)\n",
                "\n",
                "x = np.arange(0, Xmax+1, 0.1, dtype=\"float64\")\n",
                "y = np.empty_like(x, dtype=\"float64\") \n",
                "y = m*x + c_final\n",
                "\n",
                "# plt.plot(x, y, 'black')\n",
                "# Plot the original points and the final line.\n",
                "\n",
                "points = np.arange(Xmin-1, Xmax+1, 0.1)\n",
                "plt.plot(points, m*points+c_final, 'g--')\n",
                "\n",
                "\n",
                "plt.scatter(X,Y, 10, color = 'blue')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import csv\n",
                "import os"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.chdir('path')",
                "os.listdir()"
            ],
            "source_orig": [
                "os.chdir('/kaggle/input/novel-corona-virus-2019-dataset/')\n",
                "os.listdir()"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = pd.read_csv(\"covid_19_data.csv\")",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "covid_data = pd.read_csv(\"covid_19_data.csv\")\n",
                "covid_data.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = covid_data['Provincepath'].unique()",
                "ASSIGN = covid_data['Countrypath'].unique()"
            ],
            "source_orig": [
                "states = covid_data['Province/State'].unique()\n",
                "countries = covid_data['Country/Region'].unique()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "covid_data['Countrypath'].value_counts()"
            ],
            "source_orig": [
                "covid_data['Country/Region'].value_counts()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "covid_data['Last Update'].unique()"
            ],
            "source_orig": [
                "# to get the last updated date of the dataset\n",
                "\n",
                "covid_data['Last Update'].unique()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = {}",
                "ASSIGN = {}",
                "ASSIGN = {}",
                "for country in countries:",
                "ASSIGN = covid_data[covid_data['Countrypath'] == country]",
                "ASSIGN = country_data['ObservationDate'].max()",
                "ASSIGN = country_data[country_data['ObservationDate'] == max_date]",
                "ASSIGN = sub['Confirmed'].sum()",
                "ASSIGN = sub['Deaths'].sum()",
                "ASSIGN = sub['Recovered'].sum()",
                "ASSIGN[country] = ASSIGN",
                "ASSIGN[country] = ASSIGN",
                "ASSIGN[country] = ASSIGN"
            ],
            "source_orig": [
                "confirm_dict = {}\n",
                "deaths_dict = {}\n",
                "recover_dict = {}\n",
                "for country in countries:\n",
                "    country_data = covid_data[covid_data['Country/Region'] == country]\n",
                "    #cummulative, so we can simply take the latest date for final result\n",
                "    max_date = country_data['ObservationDate'].max()\n",
                "    sub = country_data[country_data['ObservationDate'] == max_date]\n",
                "    confirm = sub['Confirmed'].sum()\n",
                "    death = sub['Deaths'].sum()\n",
                "    recover = sub['Recovered'].sum()\n",
                "    \n",
                "    confirm_dict[country] = confirm\n",
                "    deaths_dict[country] = death\n",
                "    recover_dict[country] = recover\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = sorted(confirm_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)",
                "ASSIGN = sorted(deaths_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)",
                "ASSIGN = sorted(recover_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)"
            ],
            "source_orig": [
                "confirm_dict_sorted = sorted(confirm_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\n",
                "deaths_dict_sorted = sorted(deaths_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\n",
                "recover_dict_sorted = sorted(recover_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = confirm_dict_sorted[:10]",
                "ASSIGN = deaths_dict_sorted[:10]",
                "ASSIGN = recover_dict_sorted[:10]",
                "ASSIGN = dict(ASSIGN)",
                "ASSIGN = dict(ASSIGN)",
                "ASSIGN = dict(ASSIGN)"
            ],
            "source_orig": [
                "top10_confirm = confirm_dict_sorted[:10]\n",
                "top10_deaths = deaths_dict_sorted[:10]\n",
                "top10_recover = recover_dict_sorted[:10]\n",
                "top10_confirm = dict(top10_confirm)\n",
                "top10_deaths = dict(top10_deaths)\n",
                "top10_recover = dict(top10_recover)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize = (7,6))",
                "ASSIGN = plt.bar(top10_confirm.keys(), top10_confirm.values())",
                "plt.xlabel('Country')",
                "plt.ylabel('Count')",
                "plt.title('Highest Confirmed Cases in 10 countries')",
                "plt.xticks(list(top10_confirm.keys()), rotation = 90)",
                "for bar in ASSIGN:",
                "ASSIGN = bar.get_height()",
                "plt.text(bar.get_x(), ASSIGN + .005, ASSIGN, rotation = 5)",
                "plt.show()"
            ],
            "source_orig": [
                "plt.figure(figsize = (7,6))\n",
                "bars = plt.bar(top10_confirm.keys(), top10_confirm.values())\n",
                "plt.xlabel('Country')\n",
                "plt.ylabel('Count')\n",
                "plt.title('Highest Confirmed Cases in 10 countries')\n",
                "plt.xticks(list(top10_confirm.keys()), rotation = 90)\n",
                "for bar in bars:\n",
                "    yval = bar.get_height()\n",
                "    plt.text(bar.get_x(), yval + .005, yval, rotation = 5)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize = (7,6))",
                "ASSIGN = plt.bar(top10_deaths.keys(), top10_deaths.values())",
                "plt.xlabel('Country')",
                "plt.ylabel('Count')",
                "plt.title('Highest Death Cases in 10 countries')",
                "plt.xticks(list(top10_deaths.keys()), rotation = 90)",
                "for bar in ASSIGN:",
                "ASSIGN = bar.get_height()",
                "plt.text(bar.get_x(), ASSIGN + .005, ASSIGN, rotation = 5)",
                "plt.show()"
            ],
            "source_orig": [
                "plt.figure(figsize = (7,6))\n",
                "bars = plt.bar(top10_deaths.keys(), top10_deaths.values())\n",
                "plt.xlabel('Country')\n",
                "plt.ylabel('Count')\n",
                "plt.title('Highest Death Cases in 10 countries')\n",
                "plt.xticks(list(top10_deaths.keys()), rotation = 90)\n",
                "for bar in bars:\n",
                "    yval = bar.get_height()\n",
                "    plt.text(bar.get_x(), yval + .005, yval, rotation = 5)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize = (7,6))",
                "ASSIGN = plt.bar(top10_recover.keys(), top10_recover.values())",
                "plt.xlabel('Country')",
                "plt.ylabel('Count')",
                "plt.title('Highest Recovered Cases in 10 countries')",
                "plt.xticks(list(top10_recover.keys()), rotation = 90)",
                "for bar in ASSIGN:",
                "ASSIGN = bar.get_height()",
                "plt.text(bar.get_x(), ASSIGN + .005, ASSIGN, rotation = 5)",
                "plt.show()"
            ],
            "source_orig": [
                "plt.figure(figsize = (7,6))\n",
                "bars = plt.bar(top10_recover.keys(), top10_recover.values())\n",
                "plt.xlabel('Country')\n",
                "plt.ylabel('Count')\n",
                "plt.title('Highest Recovered Cases in 10 countries')\n",
                "plt.xticks(list(top10_recover.keys()), rotation = 90)\n",
                "for bar in bars:\n",
                "    yval = bar.get_height()\n",
                "    plt.text(bar.get_x(), yval + .005, yval, rotation = 5)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = covid_data[covid_data['Countrypath'] == 'Mainland China']",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for dat in ASSIGN['ObservationDate'].unique():",
                "ASSIGN = china_data[china_data['ObservationDate'] == dat]",
                "ASSIGN = sub['Confirmed'].sum()",
                "ASSIGN = sub['Deaths'].sum()",
                "ASSIGN = sub['Recovered'].sum()",
                "ASSIGN.append(dat)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = pd.Series(ASSIGN)",
                "ASSIGN =pd.Series(ASSIGN)",
                "ASSIGN = pd.Series(ASSIGN)",
                "ASSIGN = pd.Series(ASSIGN)",
                "ASSIGN = [date.min(), date[len(date)path], date.max()]",
                "plt.figure(figsize=(8,8))",
                "plt.plot(ASSIGN, ASSIGN, color = 'yellow')",
                "plt.plot(ASSIGN, ASSIGN, color = 'red')",
                "plt.plot(ASSIGN, ASSIGN, color = 'green')",
                "plt.xticks(ASSIGN, ASSIGN)",
                "plt.xlabel('Date')",
                "plt.ylabel('Cummulative Count cases')",
                "plt.title('Trend Curve of Confirmed Cases in China')",
                "plt.legend(['Confirmed', 'Death', 'Recovered'])",
                "plt.show()"
            ],
            "source_orig": [
                "china_data = covid_data[covid_data['Country/Region'] == 'Mainland China']\n",
                "date = []\n",
                "c = []\n",
                "d = []\n",
                "r = []\n",
                "for dat in china_data['ObservationDate'].unique():\n",
                "    sub = china_data[china_data['ObservationDate'] == dat]\n",
                "    confirm = sub['Confirmed'].sum()\n",
                "    death = sub['Deaths'].sum()\n",
                "    recover = sub['Recovered'].sum()\n",
                "    date.append(dat)\n",
                "    c.append(confirm)\n",
                "    d.append(death)\n",
                "    r.append(recover)\n",
                "    \n",
                "date = pd.Series(date)\n",
                "c  =pd.Series(c)\n",
                "d = pd.Series(d)\n",
                "r = pd.Series(r)\n",
                "\n",
                "t = [date.min(), date[len(date)//2], date.max()]\n",
                "plt.figure(figsize=(8,8))\n",
                "plt.plot(date, c, color = 'yellow')\n",
                "plt.plot(date, d, color = 'red')\n",
                "plt.plot(date, r, color = 'green')\n",
                "plt.xticks(t, t)\n",
                "plt.xlabel('Date')\n",
                "plt.ylabel('Cummulative Count cases')\n",
                "plt.title('Trend Curve of Confirmed Cases in China')\n",
                "plt.legend(['Confirmed', 'Death', 'Recovered'])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = covid_data[covid_data['Countrypath'] == 'Italy']",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for dat in ASSIGN['ObservationDate'].unique():",
                "ASSIGN = italy_data[italy_data['ObservationDate'] == dat]",
                "ASSIGN = sub['Confirmed'].sum()",
                "ASSIGN = sub['Deaths'].sum()",
                "ASSIGN = sub['Recovered'].sum()",
                "ASSIGN.append(dat)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = pd.Series(ASSIGN)",
                "ASSIGN =pd.Series(ASSIGN)",
                "ASSIGN = pd.Series(ASSIGN)",
                "ASSIGN = pd.Series(ASSIGN)",
                "ASSIGN = [date.min(), date[len(date)path], date.max()]",
                "plt.figure(figsize=(8,8))",
                "plt.plot(ASSIGN, ASSIGN, color = 'yellow')",
                "plt.plot(ASSIGN, ASSIGN, color = 'red')",
                "plt.plot(ASSIGN, ASSIGN, color = 'green')",
                "plt.xticks(ASSIGN, ASSIGN)",
                "plt.xlabel('Date')",
                "plt.ylabel('Cummulative Count cases')",
                "plt.title('Trend Curve of Confirmed Cases in Italy')",
                "plt.legend(['Confirmed', 'Death', 'Recovered'])",
                "plt.show()"
            ],
            "source_orig": [
                "italy_data = covid_data[covid_data['Country/Region'] == 'Italy']\n",
                "date = []\n",
                "c = []\n",
                "d = []\n",
                "r = []\n",
                "for dat in italy_data['ObservationDate'].unique():\n",
                "    sub = italy_data[italy_data['ObservationDate'] == dat]\n",
                "    confirm = sub['Confirmed'].sum()\n",
                "    death = sub['Deaths'].sum()\n",
                "    recover = sub['Recovered'].sum()\n",
                "    date.append(dat)\n",
                "    c.append(confirm)\n",
                "    d.append(death)\n",
                "    r.append(recover)\n",
                "\n",
                "date = pd.Series(date)\n",
                "c  =pd.Series(c)\n",
                "d = pd.Series(d)\n",
                "r = pd.Series(r)\n",
                "\n",
                "t = [date.min(), date[len(date)//2], date.max()]\n",
                "plt.figure(figsize=(8,8))\n",
                "plt.plot(date, c, color = 'yellow')\n",
                "plt.plot(date, d, color = 'red')\n",
                "plt.plot(date, r, color = 'green')\n",
                "plt.xticks(t, t)\n",
                "plt.xlabel('Date')\n",
                "plt.ylabel('Cummulative Count cases')\n",
                "plt.title('Trend Curve of Confirmed Cases in Italy')\n",
                "plt.legend(['Confirmed', 'Death', 'Recovered'])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = covid_data[covid_data['Countrypath'] == 'US']",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for dat in ASSIGN['ObservationDate'].unique():",
                "ASSIGN = us_data[us_data['ObservationDate'] == dat]",
                "ASSIGN = sub['Confirmed'].sum()",
                "ASSIGN = sub['Deaths'].sum()",
                "ASSIGN = sub['Recovered'].sum()",
                "ASSIGN.append(dat)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = pd.Series(ASSIGN)",
                "ASSIGN =pd.Series(ASSIGN)",
                "ASSIGN = pd.Series(ASSIGN)",
                "ASSIGN = pd.Series(ASSIGN)",
                "ASSIGN = [date.min(), date[len(date)path], date.max()]",
                "plt.figure(figsize=(8,8))",
                "plt.plot(ASSIGN, ASSIGN, color = 'yellow')",
                "plt.plot(ASSIGN, ASSIGN, color = 'red')",
                "plt.plot(ASSIGN, ASSIGN, color = 'green')",
                "plt.xticks(ASSIGN, ASSIGN)",
                "plt.xlabel('Date')",
                "plt.ylabel('Cummulative Count cases')",
                "plt.title('Trend Curve of Confirmed Cases in US')",
                "plt.legend(['Confirmed', 'Death', 'Recovered'])",
                "plt.show()"
            ],
            "source_orig": [
                "us_data = covid_data[covid_data['Country/Region'] == 'US']\n",
                "date = []\n",
                "c = []\n",
                "d = []\n",
                "r = []\n",
                "for dat in us_data['ObservationDate'].unique():\n",
                "    sub = us_data[us_data['ObservationDate'] == dat]\n",
                "    confirm = sub['Confirmed'].sum()\n",
                "    death = sub['Deaths'].sum()\n",
                "    recover = sub['Recovered'].sum()\n",
                "    date.append(dat)\n",
                "    c.append(confirm)\n",
                "    d.append(death)\n",
                "    r.append(recover)\n",
                "\n",
                "date = pd.Series(date)\n",
                "c  =pd.Series(c)\n",
                "d = pd.Series(d)\n",
                "r = pd.Series(r)\n",
                "\n",
                "t = [date.min(), date[len(date)//2], date.max()]\n",
                "plt.figure(figsize=(8,8))\n",
                "plt.plot(date, c, color = 'yellow')\n",
                "plt.plot(date, d, color = 'red')\n",
                "plt.plot(date, r, color = 'green')\n",
                "plt.xticks(t, t)\n",
                "plt.xlabel('Date')\n",
                "plt.ylabel('Cummulative Count cases')\n",
                "plt.title('Trend Curve of Confirmed Cases in US')\n",
                "plt.legend(['Confirmed', 'Death', 'Recovered'])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = covid_data[covid_data['Countrypath'] == 'Germany']",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for dat in ASSIGN['ObservationDate'].unique():",
                "ASSIGN = germany_data[germany_data['ObservationDate'] == dat]",
                "ASSIGN = sub['Confirmed'].sum()",
                "ASSIGN = sub['Deaths'].sum()",
                "ASSIGN = sub['Recovered'].sum()",
                "ASSIGN.append(dat)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = pd.Series(ASSIGN)",
                "ASSIGN =pd.Series(ASSIGN)",
                "ASSIGN = pd.Series(ASSIGN)",
                "ASSIGN = pd.Series(ASSIGN)",
                "ASSIGN = [date.min(), date[len(date)path], date.max()]",
                "plt.figure(figsize=(8,8))",
                "plt.plot(ASSIGN, ASSIGN, color = 'yellow')",
                "plt.plot(ASSIGN, ASSIGN, color = 'red')",
                "plt.plot(ASSIGN, ASSIGN, color = 'green')",
                "plt.xticks(ASSIGN, ASSIGN)",
                "plt.xlabel('Date')",
                "plt.ylabel('Cummulative Count cases')",
                "plt.title('Trend Curve of Confirmed Cases in Germany')",
                "plt.legend(['Confirmed', 'Death', 'Recovered'])",
                "plt.show()"
            ],
            "source_orig": [
                "germany_data = covid_data[covid_data['Country/Region'] == 'Germany']\n",
                "date = []\n",
                "c = []\n",
                "d = []\n",
                "r = []\n",
                "for dat in germany_data['ObservationDate'].unique():\n",
                "    sub = germany_data[germany_data['ObservationDate'] == dat]\n",
                "    confirm = sub['Confirmed'].sum()\n",
                "    death = sub['Deaths'].sum()\n",
                "    recover = sub['Recovered'].sum()\n",
                "    date.append(dat)\n",
                "    c.append(confirm)\n",
                "    d.append(death)\n",
                "    r.append(recover)\n",
                "\n",
                "date = pd.Series(date)\n",
                "c  =pd.Series(c)\n",
                "d = pd.Series(d)\n",
                "r = pd.Series(r)\n",
                "\n",
                "t = [date.min(), date[len(date)//2], date.max()]\n",
                "plt.figure(figsize=(8,8))\n",
                "plt.plot(date, c, color = 'yellow')\n",
                "plt.plot(date, d, color = 'red')\n",
                "plt.plot(date, r, color = 'green')\n",
                "plt.xticks(t, t)\n",
                "plt.xlabel('Date')\n",
                "plt.ylabel('Cummulative Count cases')\n",
                "plt.title('Trend Curve of Confirmed Cases in Germany')\n",
                "plt.legend(['Confirmed', 'Death', 'Recovered'])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = sum(list(confirm_dict.values()))",
                "ASSIGN = sum(list(deaths_dict.values()))",
                "ASSIGN = sum(list(recover_dict.values()))",
                "ASSIGN = total_confirmed -(total_deaths+total_recovered)",
                "print(, ASSIGN)"
            ],
            "source_orig": [
                "total_confirmed = sum(list(confirm_dict.values()))\n",
                "total_deaths = sum(list(deaths_dict.values()))\n",
                "total_recovered = sum(list(recover_dict.values()))\n",
                "\n",
                "total_still_affected = total_confirmed -(total_deaths+total_recovered)\n",
                "print(\"World Population affectedas of 22nd March 2020: \", total_confirmed)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = ['Affected and Uncured', 'Deaths', 'Recovered']",
                "ASSIGN = [total_still_affected, total_deaths, total_recovered]",
                "ASSIGN = ['Yellow', 'Red', 'Green']",
                "ASSIGN = (0, 0.2, 0)",
                "ASSIGN = ['Count']",
                "ASSIGN = ['Affected and Uncured', 'Deaths', 'Recovered']",
                "ASSIGN = [[total_still_affected],[total_deaths], [total_recovered]]",
                "ASSIGN = plt.subplots(1,2, figsize = (9,9))",
                "axs[0].axis('tight')",
                "axs[0].axis('off')",
                "ASSIGN = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')",
                "ASSIGN.set_fontsize(14)",
                "ASSIGN.scale(1.5, 1.5)",
                "axs[1].pie(ASSIGN, labels = ASSIGN, ASSIGN = ASSIGN, colors=ASSIGN, shadow=True, autopct='%1.1f%%')",
                "plt.title('Distribution at world level')",
                "plt.show()"
            ],
            "source_orig": [
                "groups = ['Affected and Uncured', 'Deaths', 'Recovered']\n",
                "sizes = [total_still_affected, total_deaths, total_recovered]\n",
                "colours = ['Yellow', 'Red', 'Green']\n",
                "explode = (0, 0.2, 0)\n",
                "col_labels = ['Count']\n",
                "row_labels = ['Affected and Uncured', 'Deaths', 'Recovered']\n",
                "table_values = [[total_still_affected],[total_deaths], [total_recovered]]\n",
                "\n",
                "\n",
                "fig, axs = plt.subplots(1,2, figsize = (9,9))\n",
                "axs[0].axis('tight')\n",
                "axs[0].axis('off')\n",
                "the_table = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')\n",
                "the_table.set_fontsize(14)\n",
                "the_table.scale(1.5, 1.5)\n",
                "axs[1].pie(sizes, labels = groups, explode = explode, colors=colours, shadow=True, autopct='%1.1f%%')\n",
                "plt.title('Distribution at world level')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = confirm_dict['Mainland China']",
                "ASSIGN = confirm_dict['Italy']",
                "ASSIGN = 0",
                "for key in confirm_dict:",
                "if key != 'Mainland China' and key != 'Italy':",
                "ASSIGN+=confirm_dict[key]",
                "ASSIGN = ['China', 'Italy', 'Others']",
                "ASSIGN = [china_c_number, italy_c_number, others_c]",
                "ASSIGN = ['Red', 'Green', 'Grey']",
                "ASSIGN = (0.1, 0, 0)",
                "ASSIGN = ['Count']",
                "ASSIGN = ['China', 'Italy', 'Others']",
                "ASSIGN = [[china_c_number], [italy_c_number], [others_c]]",
                "ASSIGN = plt.subplots(1,2, figsize = (8,8))",
                "axs[0].axis('tight')",
                "axs[0].axis('off')",
                "ASSIGN = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')",
                "ASSIGN.set_fontsize(14)",
                "ASSIGN.scale(1.5, 1.5)",
                "axs[1].pie(ASSIGN, labels = ASSIGN, ASSIGN = ASSIGN, colors=ASSIGN, shadow=True, autopct='%1.1f%%')",
                "plt.title('Global Proportions of 2 severely striken countries')",
                "plt.show()"
            ],
            "source_orig": [
                "china_c_number = confirm_dict['Mainland China']\n",
                "italy_c_number = confirm_dict['Italy']\n",
                "others_c = 0\n",
                "for key in confirm_dict:\n",
                "    if key != 'Mainland China' and key != 'Italy':\n",
                "        others_c+=confirm_dict[key]\n",
                "        \n",
                "groups = ['China', 'Italy', 'Others']\n",
                "sizes = [china_c_number, italy_c_number, others_c]\n",
                "colours = ['Red', 'Green', 'Grey']\n",
                "explode = (0.1, 0, 0)\n",
                "col_labels = ['Count']\n",
                "row_labels = ['China', 'Italy', 'Others']\n",
                "table_values = [[china_c_number], [italy_c_number], [others_c]]\n",
                "\n",
                "\n",
                "fig, axs = plt.subplots(1,2, figsize = (8,8))\n",
                "axs[0].axis('tight')\n",
                "axs[0].axis('off')\n",
                "the_table = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')\n",
                "the_table.set_fontsize(14)\n",
                "the_table.scale(1.5, 1.5)\n",
                "axs[1].pie(sizes, labels = groups, explode = explode, colors=colours, shadow=True, autopct='%1.1f%%')\n",
                "plt.title('Global Proportions of 2 severely striken countries')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = recover_dict['Mainland China']",
                "ASSIGN = recover_dict['Italy']",
                "ASSIGN = 0",
                "for key in recover_dict:",
                "if key != 'Mainland China' and key != 'Italy':",
                "ASSIGN+=recover_dict[key]",
                "ASSIGN = ['China', 'Italy', 'Others']",
                "ASSIGN = [china_r_number, italy_r_number, others_r]",
                "ASSIGN = ['Red', 'Green', 'Grey']",
                "ASSIGN = (0.1, 0, 0)",
                "ASSIGN = ['Count']",
                "ASSIGN = ['China', 'Italy', 'Others']",
                "ASSIGN = [[china_r_number], [italy_r_number], [others_r]]",
                "ASSIGN = plt.subplots(1,2, figsize = (8,8))",
                "axs[0].axis('tight')",
                "axs[0].axis('off')",
                "ASSIGN = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')",
                "ASSIGN.set_fontsize(14)",
                "ASSIGN.scale(1.5, 1.5)",
                "axs[1].pie(ASSIGN, labels = ASSIGN, ASSIGN = ASSIGN, colors=ASSIGN, shadow=True, autopct='%1.1f%%')",
                "plt.title('Global Proportions of 2 severely striken countries')",
                "plt.show()"
            ],
            "source_orig": [
                "china_r_number = recover_dict['Mainland China']\n",
                "italy_r_number = recover_dict['Italy']\n",
                "others_r = 0\n",
                "for key in recover_dict:\n",
                "    if key != 'Mainland China' and key != 'Italy':\n",
                "        others_r+=recover_dict[key]\n",
                "        \n",
                "groups = ['China', 'Italy', 'Others']\n",
                "sizes = [china_r_number, italy_r_number, others_r]\n",
                "colours = ['Red', 'Green', 'Grey']\n",
                "explode = (0.1, 0, 0)\n",
                "col_labels = ['Count']\n",
                "row_labels = ['China', 'Italy', 'Others']\n",
                "table_values = [[china_r_number], [italy_r_number], [others_r]]\n",
                "\n",
                "\n",
                "fig, axs = plt.subplots(1,2, figsize = (8,8))\n",
                "axs[0].axis('tight')\n",
                "axs[0].axis('off')\n",
                "the_table = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')\n",
                "the_table.set_fontsize(14)\n",
                "the_table.scale(1.5, 1.5)\n",
                "axs[1].pie(sizes, labels = groups, explode = explode, colors=colours, shadow=True, autopct='%1.1f%%')\n",
                "plt.title('Global Proportions of 2 severely striken countries')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = (deaths_dict['Italy']path)*100",
                "print('Death Percentage in Italy: ', ASSIGN)",
                "ASSIGN = (deaths_dict['Mainland China']path)*100",
                "print('Death Percentage in China: ', ASSIGN)",
                "print(total_deaths)"
            ],
            "source_orig": [
                "italian_death_perc = (deaths_dict['Italy']/total_deaths)*100\n",
                "print('Death Percentage in Italy: ', italian_death_perc)\n",
                "\n",
                "china_death_perc = (deaths_dict['Mainland China']/total_deaths)*100\n",
                "print('Death Percentage in China: ', china_death_perc)\n",
                "\n",
                "print(total_deaths)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = deaths_dict['Mainland China']",
                "ASSIGN = deaths_dict['Italy']",
                "ASSIGN = 0",
                "for key in deaths_dict:",
                "if key != 'Mainland China' and key != 'Italy':",
                "ASSIGN+=deaths_dict[key]",
                "ASSIGN = ['China', 'Italy', 'Others']",
                "ASSIGN = [china_d_number, italy_d_number, others_d]",
                "ASSIGN = ['Red', 'Green', 'Grey']",
                "ASSIGN = (0.1, 0, 0)",
                "ASSIGN = ['Count']",
                "ASSIGN = ['China', 'Italy', 'Others']",
                "ASSIGN = [[china_d_number], [italy_d_number], [others_d]]",
                "ASSIGN = plt.subplots(1,2, figsize = (8,8))",
                "axs[0].axis('tight')",
                "axs[0].axis('off')",
                "ASSIGN = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')",
                "ASSIGN.set_fontsize(14)",
                "ASSIGN.scale(1.5, 1.5)",
                "axs[1].pie(ASSIGN, labels = ASSIGN, ASSIGN = ASSIGN, colors=ASSIGN, shadow=True, autopct='%1.1f%%')",
                "plt.title('Global Proportions of 2 severely striken countries')",
                "plt.show()"
            ],
            "source_orig": [
                "china_d_number = deaths_dict['Mainland China']\n",
                "italy_d_number = deaths_dict['Italy']\n",
                "others_d = 0\n",
                "for key in deaths_dict:\n",
                "    if key != 'Mainland China' and key != 'Italy':\n",
                "        others_d+=deaths_dict[key]\n",
                "        \n",
                "groups = ['China', 'Italy', 'Others']\n",
                "sizes = [china_d_number, italy_d_number, others_d]\n",
                "colours = ['Red', 'Green', 'Grey']\n",
                "explode = (0.1, 0, 0)\n",
                "col_labels = ['Count']\n",
                "row_labels = ['China', 'Italy', 'Others']\n",
                "table_values = [[china_d_number], [italy_d_number], [others_d]]\n",
                "\n",
                "\n",
                "fig, axs = plt.subplots(1,2, figsize = (8,8))\n",
                "axs[0].axis('tight')\n",
                "axs[0].axis('off')\n",
                "the_table = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')\n",
                "the_table.set_fontsize(14)\n",
                "the_table.scale(1.5, 1.5)\n",
                "axs[1].pie(sizes, labels = groups, explode = explode, colors=colours, shadow=True, autopct='%1.1f%%')\n",
                "plt.title('Global Proportions of 2 severely striken countries')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "SETUP",
                "SETUP",
                "\"\"\"",
                "Created on Sat Sep  2 20:13:30 2017",
                "\"\"\"",
                "def hurst(data):",
                "ASSIGN = list()",
                "ASSIGN = len(data)",
                "ASSIGN = [2,4,8,16,32,64]",
                "ASSIGN = Npath(ranges)",
                "for i in range(len(ASSIGN)):",
                "for r in range(ASSIGN[i]):",
                "ASSIGN = data[int(L[i]*r):int(L[i]*(r+1))]",
                "ASSIGN = np.mean(Range)",
                "ASSIGN = np.subtract(Range,meanvalue)",
                "ASSIGN = np.sqrt((sum(Deviation*Deviation))path(L[i]-1))",
                "ASSIGN = ASSIGN.cumsum()",
                "ASSIGN = max(Deviation)",
                "ASSIGN = min(Deviation)",
                "ASSIGN.append((ASSIGN-ASSIGN)path)",
                "ARS.append(np.mean(ASSIGN))",
                "ASSIGN = np.log(ARS)",
                "ASSIGN = np.polyfit(GAP,a,1)[0]*2",
                "return(ASSIGN)",
                "def rolling(close_data,ASSIGN):",
                "ASSIGN = close_data.rolling(window=N).apply(hurst)",
                "return(ASSIGN)"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "# -*- coding: utf-8 -*-\n",
                "\"\"\"\n",
                "Created on Sat Sep  2 20:13:30 2017\n",
                "\n",
                "\"\"\"\n",
                "import numpy as np\n",
                "#hurst\n",
                "def hurst(data):\n",
                "    RS = list()\n",
                "    ARS = []\n",
                "    N = len(data)\n",
                "    ranges = [2,4,8,16,32,64]\n",
                "    L = N/np.array(ranges)\n",
                "    for i in range(len(ranges)):\n",
                "        for r in range(ranges[i]):\n",
                "            Range = data[int(L[i]*r):int(L[i]*(r+1))]\n",
                "            meanvalue = np.mean(Range)\n",
                "            Deviation = np.subtract(Range,meanvalue)\n",
                "            sigma = np.sqrt((sum(Deviation*Deviation))/(L[i]-1))\n",
                "            Deviation = Deviation.cumsum()\n",
                "            maxi = max(Deviation)\n",
                "            mini = min(Deviation)\n",
                "        RS.append((maxi-mini)/sigma)\n",
                "        ARS.append(np.mean(RS))\n",
                "    GAP = np.log(L)\n",
                "    a = np.log(ARS)\n",
                "    hurst_exponent = np.polyfit(GAP,a,1)[0]*2\n",
                "    return(hurst_exponent)\n",
                "#,N,\n",
                "def rolling(close_data,N):\n",
                "    hurst_value = close_data.rolling(window=N).apply(hurst)\n",
                "    return(hurst_value)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "#Import packages\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path', encoding='latin-1')",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "#Load dataset\n",
                "data = pd.read_csv('../input/facebook-ads-2/Facebook_Ads_2.csv', encoding='latin-1')\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(, len(data))",
                "print(, len(data[data['Clicked']==1]))",
                "print(, len(data[data['Clicked']==0]))"
            ],
            "source_orig": [
                "print(\"The total number of users in this dataset is: \", len(data))\n",
                "print(\"The number of users who clicked through the ads is: \", len(data[data['Clicked']==1]))\n",
                "print(\"The number of users who did not click through the ads is: \", len(data[data['Clicked']==0]))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data['Country'].value_counts()"
            ],
            "source_orig": [
                "#Count by country\n",
                "data['Country'].value_counts()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=[12,5])",
                "ASSIGN = plt.subplots(2)",
                "sns.distplot(data['Time Spent on Site'], ax = ax[0])",
                "sns.boxplot(x='Clicked', y='Time Spent on Site', data = data, ax = ax[1])",
                "plt.show()"
            ],
            "source_orig": [
                "#Distribution of time spent\n",
                "plt.figure(figsize=[12,5])\n",
                "fig, ax = plt.subplots(2)\n",
                "sns.distplot(data['Time Spent on Site'], ax = ax[0])\n",
                "sns.boxplot(x='Clicked', y='Time Spent on Site', data = data, ax = ax[1])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize = [12,5])",
                "sns.distplot(data[data['Clicked']==0]['Time Spent on Site'], label = 'Clicked==0')",
                "sns.distplot(data[data['Clicked']==1]['Time Spent on Site'], label = 'Clicked==1')",
                "plt.legend()",
                "plt.show()"
            ],
            "source_orig": [
                "plt.figure(figsize = [12,5])\n",
                "sns.distplot(data[data['Clicked']==0]['Time Spent on Site'], label = 'Clicked==0')\n",
                "sns.distplot(data[data['Clicked']==1]['Time Spent on Site'], label = 'Clicked==1')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize = (12,5))",
                "ASSIGN = plt.subplots(1,2)",
                "sns.distplot(data['Salary'], ax = ax[0])",
                "sns.boxplot(data = data, x = 'Clicked', y = 'Salary', ax = ax[1])",
                "plt.show()"
            ],
            "source_orig": [
                "#Distribution of salary\n",
                "plt.figure(figsize = (12,5))\n",
                "fig, ax = plt.subplots(1,2)\n",
                "sns.distplot(data['Salary'], ax = ax[0])\n",
                "sns.boxplot(data = data, x = 'Clicked', y = 'Salary', ax = ax[1])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize = [12,5])",
                "sns.distplot(data[data['Clicked']==0]['Salary'], label = 'Clicked==0')",
                "sns.distplot(data[data['Clicked']==1]['Salary'], label = 'Clicked==1')",
                "plt.legend()",
                "plt.show()"
            ],
            "source_orig": [
                "plt.figure(figsize = [12,5])\n",
                "sns.distplot(data[data['Clicked']==0]['Salary'], label = 'Clicked==0')\n",
                "sns.distplot(data[data['Clicked']==1]['Salary'], label = 'Clicked==1')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data.drop(['Names','emails','Country'], axis = 1, inplace = True)",
                "data.head()"
            ],
            "source_orig": [
                "#Drop name, email, country\n",
                "data.drop(['Names','emails','Country'], axis = 1, inplace = True)\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.heatmap(data.isnull(), yticklabels = False, cmap = 'Blues', cbar = False)"
            ],
            "source_orig": [
                "#Check on missing data\n",
                "sns.heatmap(data.isnull(), yticklabels = False, cmap = 'Blues', cbar = False)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = data.drop(['Clicked'], axis = 1).values",
                "ASSIGN = data['Clicked'].values"
            ],
            "source_orig": [
                "#Split data into dependent and independent variables\n",
                "X = data.drop(['Clicked'], axis = 1).values\n",
                "y = data['Clicked'].values"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "SETUP",
                "ASSIGN = StandardScaler()",
                "ASSIGN = sc.fit_transform(ASSIGN)"
            ],
            "source_orig": [
                "#Scale X\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "sc = StandardScaler()\n",
                "X = sc.fit_transform(X)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "SETUP",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
            ],
            "source_orig": [
                "#Split data into training and testing sets\n",
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "SETUP",
                "ASSIGN = LogisticRegression(random_state = 0)",
                "ASSIGN.fit(X_train, y_train)"
            ],
            "source_orig": [
                "#Import and fit logistic regression mode\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "LR = LogisticRegression(random_state = 0)\n",
                "LR.fit(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = LR.predict(X_test)"
            ],
            "source_orig": [
                "y_pred = LR.predict(X_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = confusion_matrix(y_test, y_pred)",
                "sns.heatmap(ASSIGN, annot = True, fmt = 'd')",
                "print(classification_report(y_test, y_pred))"
            ],
            "source_orig": [
                "#Import and run confusion matrix & classification report\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import classification_report\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "sns.heatmap(cm, annot = True, fmt = 'd')\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "#Import packages\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "data = pd.read_csv('../input/bank-customer-retirement/Bank_Customer_retirement.csv')\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.tail()"
            ],
            "source_orig": [
                "data.tail()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.info()"
            ],
            "source_orig": [
                "data.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(, len(data))",
                "print(, len(data[data['Retire']==1]))",
                "print(, len(data[data['Retire']==0]))"
            ],
            "source_orig": [
                "#Retirement\n",
                "print(\"The total number of customers in this dataset is: \", len(data))\n",
                "print(\"Out of those customers, the number of retired customers is: \", len(data[data['Retire']==1]))\n",
                "print(\"Out of those customers, the number of not retired customers is: \", len(data[data['Retire']==0]))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.distplot(data[data['Retire']==1]['Age'], label = 'Retire == 1')",
                "sns.distplot(data[data['Retire']==0]['Age'], label = 'Retire == 0')",
                "plt.legend()",
                "plt.show()"
            ],
            "source_orig": [
                "#Distribution of age\n",
                "sns.distplot(data[data['Retire']==1]['Age'], label = 'Retire == 1')\n",
                "sns.distplot(data[data['Retire']==0]['Age'], label = 'Retire == 0')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.distplot(data[data['Retire']==1]['401K Savings'], label = 'Retire == 1')",
                "sns.distplot(data[data['Retire']==0]['401K Savings'], label = 'Retire == 0')",
                "plt.legend()",
                "plt.show()"
            ],
            "source_orig": [
                "#Distribution of 401k savings\n",
                "sns.distplot(data[data['Retire']==1]['401K Savings'], label = 'Retire == 1')\n",
                "sns.distplot(data[data['Retire']==0]['401K Savings'], label = 'Retire == 0')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.scatterplot(data = data, x = '401K Savings', y = 'Age', hue = 'Retire')"
            ],
            "source_orig": [
                "#Scatterplot\n",
                "sns.scatterplot(data = data, x = '401K Savings', y = 'Age', hue = 'Retire')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = ASSIGN.drop(['Customer ID'], axis = 1)",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "#Drop customer ID\n",
                "data = data.drop(['Customer ID'], axis = 1)\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = data.drop(['Retire'], axis = 1)",
                "ASSIGN = data['Retire']"
            ],
            "source_orig": [
                "#Split into X and y\n",
                "X = data.drop(['Retire'], axis = 1)\n",
                "y = data['Retire']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = X.min()",
                "ASSIGN = (X - X_min).max()",
                "ASSIGN = (X - X_min)path"
            ],
            "source_orig": [
                "#Normalize X\n",
                "X_min = X.min()\n",
                "X_range = (X - X_min).max()\n",
                "X_scaled = (X - X_min)/X_range"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.scatterplot(data = X, x = 'Age', y = '401K Savings')"
            ],
            "source_orig": [
                "#Inspect normalization\n",
                "sns.scatterplot(data = X, x = 'Age', y = '401K Savings')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.scatterplot(data = X_scaled, x = 'Age', y = '401K Savings')"
            ],
            "source_orig": [
                "sns.scatterplot(data = X_scaled, x = 'Age', y = '401K Savings')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "SETUP",
                "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.25, random_state = 42)"
            ],
            "source_orig": [
                "#Train test split\n",
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.25, random_state = 42)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "SETUP",
                "ASSIGN = SVC()",
                "ASSIGN.fit(X_train, y_train)"
            ],
            "source_orig": [
                "#Fit model\n",
                "from sklearn.svm import SVC\n",
                "svc_model = SVC()\n",
                "svc_model.fit(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = svc_model.predict(X_test)",
                "print(classification_report(y_test, ASSIGN))"
            ],
            "source_orig": [
                "#Evaluate model\n",
                "y_pred = svc_model.predict(X_test)\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "sns.heatmap(confusion_matrix(y_test, y_pred), annot = True, fmt = 'd')",
                "plt.xlabel('Actual Value')",
                "plt.ylabel('Predicted Value')"
            ],
            "source_orig": [
                "sns.heatmap(confusion_matrix(y_test, y_pred), annot = True, fmt = 'd')\n",
                "plt.xlabel('Actual Value')\n",
                "plt.ylabel('Predicted Value')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from sklearn.model_selection import GridSearchCV"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "gs.fit(X_train, y_train)"
            ],
            "source_orig": [
                "#fit gs\n",
                "gs.fit(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "gs.best_params_"
            ],
            "source_orig": [
                "gs.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "gs_1.fit(X_train, y_train)",
                "gs_1.best_params_"
            ],
            "source_orig": [
                "gs_1.fit(X_train, y_train)\n",
                "gs_1.best_params_"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "ASSIGN= gs_1.predict(X_test)"
            ],
            "source_orig": [
                "#predict based on gs\n",
                "y_pred_gs1= gs_1.predict(X_test)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "print(classification_report(y_test, y_pred_gs1))"
            ],
            "source_orig": [
                "#evaluate gs results\n",
                "print(classification_report(y_test, y_pred_gs1))"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "sns.heatmap(confusion_matrix(y_test, y_pred_gs1), annot = True)",
                "plt.xlabel('Actual Value')",
                "plt.ylabel('Predicted Value')"
            ],
            "source_orig": [
                "sns.heatmap(confusion_matrix(y_test, y_pred_gs1), annot = True)\n",
                "plt.xlabel('Actual Value')\n",
                "plt.ylabel('Predicted Value')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "print(os.listdir())"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "import os\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "print(check_output([, ]).decode())                #list the input folder files"
            ],
            "source_orig": [
                "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
                "from keras.models import Sequential                                   # model\n",
                "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D   # layers\n",
                "from keras.optimizers import RMSprop                                  # optimizer\n",
                "from keras.preprocessing.image import ImageDataGenerator              \n",
                "from keras.callbacks import ReduceLROnPlateau                         # callback function\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))                #list the input folder files"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN= pd.read_csv(\"..path\")",
                "print('ASSIGN datasets shape: ',ASSIGN.shape)",
                "print('ASSIGN datasets shape: ',ASSIGN.shape)",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "# create the training & test sets, skipping the header row with [1:]\n",
                "train = pd.read_csv(\"../input/train.csv\")\n",
                "test= pd.read_csv(\"../input/test.csv\")\n",
                "print('train datasets shape: ',train.shape)\n",
                "print('test datasets shape: ',test.shape)\n",
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test.head()"
            ],
            "source_orig": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = train[\"label\"]",
                "ASSIGN = train.drop(labels = [\"label\"], axis =1)",
                "del train",
                "print('x train datasets shape: ',ASSIGN.shape)",
                "print('y label datasets shape: ',ASSIGN.shape)"
            ],
            "source_orig": [
                "y_label = train[\"label\"]\n",
                "x_train = train.drop(labels = [\"label\"], axis =1)\n",
                "del train\n",
                "print('x train datasets shape: ',x_train.shape)\n",
                "print('y label datasets shape: ',y_label.shape)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = x_trainpath",
                "ASSIGN = to_categorical(ASSIGN, 10)",
                "print('x train datasets shape: ',ASSIGN.shape)",
                "print('y label datasets shape: ',ASSIGN.shape)",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "x_train = x_train/255.0\n",
                "y_label = to_categorical(y_label, 10)\n",
                "print('x train datasets shape: ',x_train.shape)\n",
                "print('y label datasets shape: ',y_label.shape)\n",
                "x_train.head()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = ASSIGN.values.reshape(-1, 28, 28, 1)",
                "print('x train datasets shape: ',ASSIGN.shape)"
            ],
            "source_orig": [
                "x_train = x_train.values.reshape(-1, 28, 28, 1)\n",
                "print('x train datasets shape: ',x_train.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = [10, 100,1000]",
                "ASSIGN = 0",
                "for i in ASSIGN:",
                "ASSIGN+=1",
                "plt.subplot(310+(ASSIGN))",
                "plt.imshow(x_train[i][:,:,0])",
                "plt.title(\"this is {}\".format(y_label[i]))"
            ],
            "source_orig": [
                "n = [10, 100,1000]\n",
                "j = 0\n",
                "for i in n:\n",
                "    j+=1\n",
                "    plt.subplot(310+(j))\n",
                "    plt.imshow(x_train[i][:,:,0])\n",
                "    plt.title(\"this is {}\".format(y_label[i]))"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same',",
                "ASSIGN ='relu', input_shape = (28,28,1)))",
                "ASSIGN.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same',",
                "ASSIGN ='relu'))",
                "ASSIGN.add(MaxPool2D(pool_size=(2,2)))",
                "ASSIGN.add(Dropout(0.25))",
                "print(, ASSIGN.output_shape)",
                "ASSIGN.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same',",
                "ASSIGN ='relu'))",
                "ASSIGN.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same',",
                "ASSIGN ='relu'))",
                "ASSIGN.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))",
                "ASSIGN.add(Dropout(0.25))",
                "print(, ASSIGN.output_shape)",
                "ASSIGN.add(Flatten())",
                "print(, ASSIGN.output_shape)",
                "ASSIGN.add(Dense(64, ASSIGN = \"relu\"))",
                "ASSIGN.add(Dropout(0.5))",
                "ASSIGN.add(Dense(10, ASSIGN = \"softmax\"))",
                "ASSIGN.compile(",
                "ASSIGN='adam',",
                "ASSIGN='categorical_crossentropy',",
                "ASSIGN=['accuracy']",
                ")",
                "ASSIGN = ReduceLROnPlateau(",
                "ASSIGN='val_acc',",
                "ASSIGN=3,",
                "ASSIGN=1,",
                "ASSIGN=0.5,",
                "ASSIGN=0.00001",
                ")"
            ],
            "source_orig": [
                "model = Sequential()\n",
                "\n",
                "model.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n",
                "                 activation ='relu', input_shape = (28,28,1)))\n",
                "model.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n",
                "                 activation ='relu'))\n",
                "model.add(MaxPool2D(pool_size=(2,2)))\n",
                "model.add(Dropout(0.25))\n",
                "\n",
                "print(\"model output shape: \", model.output_shape)\n",
                "\n",
                "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
                "                 activation ='relu'))\n",
                "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
                "                 activation ='relu'))\n",
                "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
                "model.add(Dropout(0.25))\n",
                "\n",
                "print(\"model output shape: \", model.output_shape)\n",
                "\n",
                "model.add(Flatten())\n",
                "\n",
                "print(\"model output shape: \", model.output_shape)\n",
                "model.add(Dense(64, activation = \"relu\"))\n",
                "model.add(Dropout(0.5))\n",
                "model.add(Dense(10, activation = \"softmax\"))\n",
                "\n",
                "model.compile(\n",
                "    optimizer='adam',\n",
                "    loss='categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "lr_reduction = ReduceLROnPlateau(\n",
                "    monitor='val_acc',\n",
                "    patience=3,\n",
                "    verbose=1,\n",
                "    factor=0.5,\n",
                "    min_lr=0.00001\n",
                ")"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = model.fit(",
                "x_train,",
                "y_label,",
                "ASSIGN=1000,",
                "ASSIGN=20,",
                "ASSIGN=0.2,",
                "ASSIGN=2,",
                "ASSIGN=[lr_reduction]",
                ")"
            ],
            "source_orig": [
                "history = model.fit(\n",
                "    x_train,\n",
                "    y_label,\n",
                "    batch_size=1000,\n",
                "    epochs=20,\n",
                "    validation_split=0.2,\n",
                "    verbose=2,\n",
                "    callbacks=[lr_reduction]\n",
                ")"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = history.history",
                "ASSIGN = history_dict['loss']",
                "ASSIGN = history_dict['val_loss']",
                "ASSIGN = history_dict['acc']",
                "ASSIGN = history_dict['val_acc']",
                "ASSIGN = range(1,len(loss_values)+1)",
                "plt.clf()",
                "plt.subplot(311)",
                "plt.plot(ASSIGN, ASSIGN,'bo-')",
                "plt.plot(ASSIGN, ASSIGN,'rs-')",
                "plt.xlabel('Iterations')",
                "plt.ylabel('Loss & Accuracy ')",
                "plt.title(\"For Train Data\")",
                "plt.subplot(313)",
                "plt.plot(ASSIGN, ASSIGN,'bo-')",
                "plt.plot(ASSIGN, ASSIGN,'rs-')",
                "plt.xlabel('Iterations')",
                "plt.ylabel('Loss & Accuracy')",
                "plt.title(\"For validation Data\")",
                "plt.show()"
            ],
            "source_orig": [
                "history_dict = history.history\n",
                "loss_values = history_dict['loss']\n",
                "val_loss_values = history_dict['val_loss']\n",
                "acc_values = history_dict['acc']\n",
                "val_acc_values = history_dict['val_acc']\n",
                "epochs = range(1,len(loss_values)+1)\n",
                "plt.clf()\n",
                "plt.subplot(311)\n",
                "plt.plot(epochs, loss_values,'bo-')\n",
                "plt.plot(epochs, acc_values,'rs-')\n",
                "plt.xlabel('Iterations')\n",
                "plt.ylabel('Loss & Accuracy ')\n",
                "plt.title(\"For Train Data\")\n",
                "\n",
                "plt.subplot(313)\n",
                "plt.plot(epochs, val_loss_values,'bo-')\n",
                "plt.plot(epochs, val_acc_values,'rs-')\n",
                "plt.xlabel('Iterations')\n",
                "plt.ylabel('Loss & Accuracy')\n",
                "plt.title(\"For validation Data\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "from glob import glob as gb"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN=os.listdir(\"..path\")",
                "dirs"
            ],
            "source_orig": [
                "#list all the directories\n",
                "dirs=os.listdir(\"../input/zomato_data/\")\n",
                "dirs"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(dirs)"
            ],
            "source_orig": [
                "len(dirs)"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "ASSIGN=[]",
                "for dir1 in dirs:",
                "ASSIGN=os.listdir(r\"..path\"+dir1)",
                "for file in ASSIGN:",
                "ASSIGN=pd.read_csv(\"..path\"+dir1+\"path\"+file,quotechar='\"',delimiter=\"|\")",
                "ASSIGN.append(ASSIGN.values)"
            ],
            "source_orig": [
                "#storing all the files from every directory\n",
                "li=[]\n",
                "for dir1 in dirs:\n",
                "    files=os.listdir(r\"../input/zomato_data/\"+dir1)\n",
                "    #reading each file from list of files from previous step and creating pandas data fame    \n",
                "    for file in files:\n",
                "        \n",
                "        df_file=pd.read_csv(\"../input/zomato_data/\"+dir1+\"/\"+file,quotechar='\"',delimiter=\"|\")\n",
                "#appending the dataframe into a list\n",
                "        li.append(df_file.values)\n",
                "    \n",
                "    "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(li)"
            ],
            "source_orig": [
                "len(li)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=np.vstack(li)"
            ],
            "source_orig": [
                "#numpys vstack method to append all the datafames to stack the sequence of input vertically to make a single array\n",
                "df_np=np.vstack(li)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df_np.shape"
            ],
            "source_orig": [
                "#no of rows is represents the total no restaurants ,now of coloumns(12) is columns for the dataframe\n",
                "df_np.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=pd.DataFrame(df_np)"
            ],
            "source_orig": [
                "#creating final dataframe from the numpy array\n",
                "df_final=pd.DataFrame(df_np)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=pd.DataFrame(ASSIGN.values, columns =[\"NAME\",\"PRICE\",\"CUSINE_CATEGORY\",\"CITY\",\"REGION\",\"URL\",\"PAGE NO\",\"CUSINE TYPE\",\"TIMING\",\"RATING_TYPE\",\"RATING\",\"VOTES\"])"
            ],
            "source_orig": [
                "#adding the header columns\n",
                "df_final=pd.DataFrame(df_final.values, columns =[\"NAME\",\"PRICE\",\"CUSINE_CATEGORY\",\"CITY\",\"REGION\",\"URL\",\"PAGE NO\",\"CUSINE TYPE\",\"TIMING\",\"RATING_TYPE\",\"RATING\",\"VOTES\"])\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df_final"
            ],
            "source_orig": [
                "#displaying the dataframe\n",
                "df_final"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_final.drop(columns=[\"PAGE NO\"],axis=1,inplace=True)"
            ],
            "source_orig": [
                "#header column \"PAGE NO\" is not required ,i used it while scraping the data from zomato to do some sort of validation,lets remove the column\n",
                "df_final.drop(columns=[\"PAGE NO\"],axis=1,inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df_final"
            ],
            "source_orig": [
                "#display the dataframe again\n",
                "df_final"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_final[\"CITY\"].unique()"
            ],
            "source_orig": [
                "#lets count how many unique cities are there \n",
                "\n",
                "df_final[\"CITY\"].unique()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(df_final[\"CITY\"].unique())"
            ],
            "source_orig": [
                "len(df_final[\"CITY\"].unique())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=df_final[\"CITY\"].value_counts().sort_values(ascending=True)"
            ],
            "source_orig": [
                "#lets check city wise restaurant counts and save it in ascending order\n",
                "\n",
                "city_vs_count=df_final[\"CITY\"].value_counts().sort_values(ascending=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "city_vs_count"
            ],
            "source_orig": [
                "city_vs_count"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "ASSIGN=max(city_vs_count)"
            ],
            "source_orig": [
                "#lets check max count\n",
                "count_max=max(city_vs_count)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "for x,y in city_vs_count.items():",
                "if(y==count_max):",
                "print(x)"
            ],
            "source_orig": [
                "#lets find for city count is max\n",
                "\n",
                "for x,y in city_vs_count.items():\n",
                "    if(y==count_max):\n",
                "        print(x)\n",
                "    "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN=min(city_vs_count)",
                "for x,y in city_vs_count.items():",
                "if(y==ASSIGN):",
                "print(x)"
            ],
            "source_orig": [
                "#lets find for city count is min\n",
                "\n",
                "min_count=min(city_vs_count)\n",
                "\n",
                "for x,y in city_vs_count.items():\n",
                "    if(y==min_count):\n",
                "        print(x)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN=plt.figure(figsize=(20,40))",
                "city_vs_count.plot(kind=\"barh\",fontsize=30)",
                "plt.grid(b=True, which='both', color='Black',linestyle='-')",
                "plt.ylabel(\"city names\",fontsize=50,color=\"red\",fontweight='bold')",
                "plt.title(\"CITY VS RESTAURANT COUNT GRAPH\",fontsize=50,color=\"BLUE\",fontweight='bold')"
            ],
            "source_orig": [
                "#lets plot citywise restaurant count in barh form\n",
                "\n",
                "fig=plt.figure(figsize=(20,40))\n",
                "city_vs_count.plot(kind=\"barh\",fontsize=30)\n",
                "plt.grid(b=True, which='both', color='Black',linestyle='-')\n",
                "plt.ylabel(\"city names\",fontsize=50,color=\"red\",fontweight='bold')\n",
                "plt.title(\"CITY VS RESTAURANT COUNT GRAPH\",fontsize=50,color=\"BLUE\",fontweight='bold')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN=plt.figure(figsize=(20,40))",
                "city_vs_count.plot(kind=\"barh\",fontsize=30)",
                "plt.grid(b=True, which='both', color='Black',linestyle='-')",
                "plt.ylabel(\"city names\",fontsize=50,color=\"red\",fontweight='bold')",
                "plt.title(\"CITY VS RESTAURANT COUNT GRAPH\",fontsize=50,color=\"BLUE\",fontweight='bold')",
                "for v in range(len(city_vs_count)):",
                "plt.text(v+city_vs_count[v],v,city_vs_count[v],fontsize=20,color=\"BLUE\",fontweight='bold')"
            ],
            "source_orig": [
                "#lets plot citywise restaurant count in barh form,and each bar should display the count of the corresponding restuants for that city\n",
                "\n",
                "fig=plt.figure(figsize=(20,40))\n",
                "city_vs_count.plot(kind=\"barh\",fontsize=30)\n",
                "plt.grid(b=True, which='both', color='Black',linestyle='-')\n",
                "plt.ylabel(\"city names\",fontsize=50,color=\"red\",fontweight='bold')\n",
                "plt.title(\"CITY VS RESTAURANT COUNT GRAPH\",fontsize=50,color=\"BLUE\",fontweight='bold')\n",
                "for v in range(len(city_vs_count)):\n",
                "    #plt.text(x axis location ,y axis location ,text value ,other parameters......)\n",
                "    plt.text(v+city_vs_count[v],v,city_vs_count[v],fontsize=20,color=\"BLUE\",fontweight='bold')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "from glob import glob as gb"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN=os.listdir(\"..path\")",
                "dirs"
            ],
            "source_orig": [
                "#list all the directories\n",
                "dirs=os.listdir(\"../input/zomato_data/\")\n",
                "dirs"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(dirs)"
            ],
            "source_orig": [
                "len(dirs)"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "ASSIGN=[]",
                "for dir1 in dirs:",
                "ASSIGN=os.listdir(r\"..path\"+dir1)",
                "for file in ASSIGN:",
                "ASSIGN=pd.read_csv(\"..path\"+dir1+\"path\"+file,quotechar='\"',delimiter=\"|\")",
                "ASSIGN.append(ASSIGN.values)"
            ],
            "source_orig": [
                "#storing all the files from every directory\n",
                "li=[]\n",
                "for dir1 in dirs:\n",
                "    files=os.listdir(r\"../input/zomato_data/\"+dir1)\n",
                "    #reading each file from list of files from previous step and creating pandas data fame    \n",
                "    for file in files:\n",
                "        \n",
                "        df_file=pd.read_csv(\"../input/zomato_data/\"+dir1+\"/\"+file,quotechar='\"',delimiter=\"|\")\n",
                "#appending the dataframe into a list\n",
                "        li.append(df_file.values)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(li)"
            ],
            "source_orig": [
                "len(li)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=np.vstack(li)"
            ],
            "source_orig": [
                "#numpys vstack method to append all the datafames to stack the sequence of input vertically to make a single array\n",
                "df_np=np.vstack(li)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df_np.shape"
            ],
            "source_orig": [
                "#no of rows is represents the total no restaurants ,now of coloumns(12) is columns for the dataframe\n",
                "df_np.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=pd.DataFrame(df_np)"
            ],
            "source_orig": [
                "#creating final dataframe from the numpy array\n",
                "df_final=pd.DataFrame(df_np)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=pd.DataFrame(ASSIGN.values, columns =[\"NAME\",\"PRICE\",\"CUSINE_CATEGORY\",\"CITY\",\"REGION\",\"URL\",\"PAGE NO\",\"CUSINE TYPE\",\"TIMING\",\"RATING_TYPE\",\"RATING\",\"VOTES\"])"
            ],
            "source_orig": [
                "#adding the header columns\n",
                "df_final=pd.DataFrame(df_final.values, columns =[\"NAME\",\"PRICE\",\"CUSINE_CATEGORY\",\"CITY\",\"REGION\",\"URL\",\"PAGE NO\",\"CUSINE TYPE\",\"TIMING\",\"RATING_TYPE\",\"RATING\",\"VOTES\"])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df_final"
            ],
            "source_orig": [
                "#displaying the dataframe\n",
                "df_final"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_final.drop(columns=[\"PAGE NO\"],axis=1,inplace=True)"
            ],
            "source_orig": [
                "#header column \"PAGE NO\" is not required ,i used it while scraping the data from zomato to do some sort of validation,lets remove the column\n",
                "df_final.drop(columns=[\"PAGE NO\"],axis=1,inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df_final"
            ],
            "source_orig": [
                "#display the dataframe again\n",
                "df_final"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_final[\"CITY\"].unique()"
            ],
            "source_orig": [
                "#lets count how many unique cities are there \n",
                "\n",
                "df_final[\"CITY\"].unique()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data"
            ],
            "source": [
                "SETUP",
                "ASSIGN=df_final[\"CITY\"].unique()",
                "ASSIGN=[]",
                "ASSIGN ='https:path'",
                "for i in range(len(ASSIGN)):",
                "ASSIGN = {'address': city_name[i], 'key': 'AIzaSyD-kYTK-8FQGueJqA2028t2YHbUX96V0vk'}",
                "ASSIGN = requests.get(geo_s, params=param)",
                "ASSIGN=ASSIGN.text",
                "ASSIGN=json.loads(response)",
                "ASSIGN=data[\"results\"][0][\"geometry\"][\"location\"][\"ASSIGN\"]",
                "ASSIGN=data[\"results\"][0][\"geometry\"][\"location\"][\"ASSIGN\"]",
                "ASSIGN=pd.DataFrame([[city_name[i],lat,lng]])",
                "ASSIGN.append(ASSIGN.values)"
            ],
            "source_orig": [
                "# import json and requests library to use googl apis to get the longitude ant latituide values\n",
                "import requests\n",
                "import json\n",
                "\n",
                "#creating a separate array with all city names as elements of array\n",
                "city_name=df_final[\"CITY\"].unique()\n",
                "li1=[]\n",
                "\n",
                "#googlemap api calling url \n",
                "geo_s ='https://maps.googleapis.com/maps/api/geocode/json'\n",
                "#iterating through a for loop for each city names \n",
                "for i in range(len(city_name)):\n",
                "\n",
                "#i have used my own google map api, please use ypur own api     \n",
                " param = {'address': city_name[i], 'key': 'AIzaSyD-kYTK-8FQGueJqA2028t2YHbUX96V0vk'}\n",
                " \n",
                " response = requests.get(geo_s, params=param)\n",
                " \n",
                " response=response.text\n",
                "\n",
                " data=json.loads(response)\n",
                "\n",
                "#setting up the variable with corresponding city longitude and latitude\n",
                " lat=data[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]\n",
                " lng=data[\"results\"][0][\"geometry\"][\"location\"][\"lng\"]\n",
                "\n",
                "#creating a new data frame with city , latitude and longitude as columns\n",
                " df2=pd.DataFrame([[city_name[i],lat,lng]])\n",
                " li1.append(df2.values)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=np.vstack(li1)"
            ],
            "source_orig": [
                "#numpys vstack method to append all the datafames to stack the sequence of input vertically to make a single array\n",
                "df_np=np.vstack(li1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=pd.DataFrame(df_np,columns=[\"CITY\",\"lat\",\"lng\"])"
            ],
            "source_orig": [
                "#creating a second dataframe with city name, latitude and longitude\n",
                "df_sec=pd.DataFrame(df_np,columns=[\"CITY\",\"lat\",\"lng\"])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df_sec"
            ],
            "source_orig": [
                "#display the second dataframe contents\n",
                "df_sec"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=df_final.merge(df_sec,on=\"CITY\",how=\"left\")"
            ],
            "source_orig": [
                "#merge this data frame to the existing df_final data frame using merge and join features from pandas,and creating a new data frame\n",
                "df_final2=df_final.merge(df_sec,on=\"CITY\",how=\"left\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df_final2"
            ],
            "source_orig": [
                "#display the contents , it will have longitude and latitude now\n",
                "df_final2"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=df_final[\"CITY\"].value_counts().sort_values(ascending=True)"
            ],
            "source_orig": [
                "#creating pandas series to hold the citynames and corresponding count of restuarnats in ascending order\n",
                "li2=df_final[\"CITY\"].value_counts().sort_values(ascending=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "li2"
            ],
            "source_orig": [
                "li2"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "dc"
            ],
            "source_orig": [
                "#displaying the dictionary\n",
                "dc"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=pd.DataFrame.from_dict(dc,orient=\"index\",columns=[\"CITY\",\"COUNT\"])"
            ],
            "source_orig": [
                "#creating another data frame from the above dictionary\n",
                "df_map=pd.DataFrame.from_dict(dc,orient=\"index\",columns=[\"CITY\",\"COUNT\"])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df_map"
            ],
            "source_orig": [
                "#displaying the data frame\n",
                "df_map"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=df_map.merge(df_sec,on=\"CITY\",how=\"left\")"
            ],
            "source_orig": [
                "#merging this data frame with df_sec data frame(which we created using city names,longitude and latitude)\n",
                "df_map_final=df_map.merge(df_sec,on=\"CITY\",how=\"left\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df_map_final"
            ],
            "source_orig": [
                "#displaying the new data frame this frame will be used for map ploting\n",
                "df_map_final"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "#importing the libraries for map ploting\n",
                "from matplotlib import cm\n",
                "from matplotlib.dates import date2num\n",
                "from mpl_toolkits.basemap import Basemap\n",
                "\n",
                "# for date and time processing\n",
                "import datetime"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=df_map_final.tail(20)"
            ],
            "source_orig": [
                "#lets take one data frame for top 20 cities with most retaurants counts \n",
                "df_plot_top=df_map_final.tail(20)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df_plot_top"
            ],
            "source_orig": [
                "#displaying the data frame\n",
                "df_plot_top"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(50,60))",
                "ASSIGN=Basemap(width=120000,height=900000,projection=\"lcc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)",
                "ASSIGN.drawcountries()",
                "ASSIGN.drawmapboundary(color='",
                "ASSIGN.drawcoastlines()",
                "ASSIGN=np.array(df_plot_top[\"lng\"])",
                "ASSIGN=np.array(df_plot_top[\"ASSIGN\"])",
                "ASSIGN=np.array(df_plot_top[\"COUNT\"])",
                "ASSIGN=np.array(df_plot_top[\"CITY\"])",
                "ASSIGN=map(lg,lat)",
                "ASSIGN=df_plot_top[\"COUNT\"].apply(lambda x: int(x)path)",
                "plt.scatter(ASSIGN,s=ASSIGN,marker=\"o\",c='BLUE')",
                "plt.title(\"TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ],
            "source_orig": [
                "#lets plot this inside the map corresponding to the cities exact co-ordinates which we received from google api \n",
                "#plt.subplots(figsize=(20,50))\n",
                "plt.figure(figsize=(50,60))\n",
                "map=Basemap(width=120000,height=900000,projection=\"lcc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n",
                "map.drawcountries()\n",
                "map.drawmapboundary(color='#f2f2f2')\n",
                "\n",
                "map.drawcoastlines()\n",
                "\n",
                "\n",
                "\n",
                "lg=np.array(df_plot_top[\"lng\"])\n",
                "lat=np.array(df_plot_top[\"lat\"])\n",
                "pt=np.array(df_plot_top[\"COUNT\"])\n",
                "city_name=np.array(df_plot_top[\"CITY\"])\n",
                "\n",
                "x,y=map(lg,lat)\n",
                "\n",
                "#using lambda function to create different sizes of marker as per thecount \n",
                "\n",
                "p_s=df_plot_top[\"COUNT\"].apply(lambda x: int(x)/2)\n",
                "\n",
                "#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is always blue.\n",
                "plt.scatter(x,y,s=p_s,marker=\"o\",c='BLUE')\n",
                "plt.title(\"TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(50,60))",
                "ASSIGN=Basemap(width=120000,height=900000,projection=\"lcc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)",
                "ASSIGN.drawcountries()",
                "ASSIGN.drawmapboundary(color='",
                "ASSIGN.drawcoastlines()",
                "ASSIGN=np.array(df_plot_top[\"lng\"])",
                "ASSIGN=np.array(df_plot_top[\"ASSIGN\"])",
                "ASSIGN=np.array(df_plot_top[\"COUNT\"])",
                "ASSIGN=np.array(df_plot_top[\"CITY\"])",
                "ASSIGN=map(lg,lat)",
                "ASSIGN=df_plot_top[\"COUNT\"].apply(lambda x: int(x)path)",
                "plt.scatter(ASSIGN,s=ASSIGN,marker=\"o\",c=ASSIGN)",
                "plt.title(\"TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ],
            "source_orig": [
                "#lets plot this inside the map corresponding to the cities exact co-ordinates which we received from google api ,here marker color will be different as per marker size\n",
                "#plt.subplots(figsize=(20,50))\n",
                "plt.figure(figsize=(50,60))\n",
                "map=Basemap(width=120000,height=900000,projection=\"lcc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n",
                "map.drawcountries()\n",
                "map.drawmapboundary(color='#f2f2f2')\n",
                "\n",
                "map.drawcoastlines()\n",
                "\n",
                "\n",
                "\n",
                "lg=np.array(df_plot_top[\"lng\"])\n",
                "lat=np.array(df_plot_top[\"lat\"])\n",
                "pt=np.array(df_plot_top[\"COUNT\"])\n",
                "city_name=np.array(df_plot_top[\"CITY\"])\n",
                "\n",
                "x,y=map(lg,lat)\n",
                "\n",
                "#using lambda function to create different sizes of marker as per thecount \n",
                "\n",
                "p_s=df_plot_top[\"COUNT\"].apply(lambda x: int(x)/2)\n",
                "\n",
                "#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is different.\n",
                "plt.scatter(x,y,s=p_s,marker=\"o\",c=p_s)\n",
                "plt.title(\"TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(50,60))",
                "ASSIGN=Basemap(width=120000,height=900000,projection=\"lcc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)",
                "ASSIGN.drawcountries()",
                "ASSIGN.drawmapboundary(color='",
                "ASSIGN.drawcoastlines()",
                "ASSIGN=np.array(df_plot_top[\"lng\"])",
                "ASSIGN=np.array(df_plot_top[\"ASSIGN\"])",
                "ASSIGN=np.array(df_plot_top[\"COUNT\"])",
                "ASSIGN=np.array(df_plot_top[\"CITY\"])",
                "ASSIGN=map(lg,lat)",
                "ASSIGN=df_plot_top[\"COUNT\"].apply(lambda x: int(x)path)",
                "plt.scatter(ASSIGN,s=ASSIGN,marker=\"o\",c=ASSIGN)",
                "for a,b ,c,d in zip(ASSIGN,ASSIGN,ASSIGN):",
                "plt.text(a,b,c,fontsize=30,color=\"r\")",
                "plt.title(\"TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ],
            "source_orig": [
                "#lets plot with the city names inside the map corresponding to the cities exact co-ordinates which we received from google api ,here marker color will be different as per marker size\n",
                "#plt.subplots(figsize=(20,50))\n",
                "plt.figure(figsize=(50,60))\n",
                "map=Basemap(width=120000,height=900000,projection=\"lcc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n",
                "map.drawcountries()\n",
                "map.drawmapboundary(color='#f2f2f2')\n",
                "\n",
                "map.drawcoastlines()\n",
                "\n",
                "\n",
                "\n",
                "lg=np.array(df_plot_top[\"lng\"])\n",
                "lat=np.array(df_plot_top[\"lat\"])\n",
                "pt=np.array(df_plot_top[\"COUNT\"])\n",
                "city_name=np.array(df_plot_top[\"CITY\"])\n",
                "\n",
                "x,y=map(lg,lat)\n",
                "\n",
                "#using lambda function to create different sizes of marker as per thecount \n",
                "\n",
                "p_s=df_plot_top[\"COUNT\"].apply(lambda x: int(x)/2)\n",
                "\n",
                "#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is different.\n",
                "plt.scatter(x,y,s=p_s,marker=\"o\",c=p_s)\n",
                "\n",
                "for a,b ,c,d in zip(x,y,city_name,pt):\n",
                "    #plt.text takes x position , y position ,text ,font size and color as arguments\n",
                "    plt.text(a,b,c,fontsize=30,color=\"r\")\n",
                "   \n",
                "    \n",
                "    \n",
                "plt.title(\"TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(50,60))",
                "ASSIGN=Basemap(width=120000,height=900000,projection=\"lcc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)",
                "ASSIGN.drawcountries()",
                "ASSIGN.drawmapboundary(color='",
                "ASSIGN.drawcoastlines()",
                "ASSIGN=np.array(df_plot_top[\"lng\"])",
                "ASSIGN=np.array(df_plot_top[\"ASSIGN\"])",
                "ASSIGN=np.array(df_plot_top[\"COUNT\"])",
                "ASSIGN=np.array(df_plot_top[\"CITY\"])",
                "ASSIGN=map(lg,lat)",
                "ASSIGN=df_plot_top[\"COUNT\"].apply(lambda x: int(x)path)",
                "plt.scatter(ASSIGN,s=ASSIGN,marker=\"o\",c=ASSIGN)",
                "for a,b ,c,d in zip(ASSIGN,ASSIGN,ASSIGN):",
                "plt.text(a,b,c,fontsize=30,color=\"r\")",
                "plt.text(a+60000,b+30000,d,fontsize=30)",
                "plt.title(\"TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ],
            "source_orig": [
                "#lets plot with the city names and restaurants count inside the map corresponding to the cities exact co-ordinates which we received from google api ,here marker color will be different as per marker size\n",
                "#plt.subplots(figsize=(20,50))\n",
                "plt.figure(figsize=(50,60))\n",
                "map=Basemap(width=120000,height=900000,projection=\"lcc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n",
                "map.drawcountries()\n",
                "map.drawmapboundary(color='#f2f2f2')\n",
                "\n",
                "map.drawcoastlines()\n",
                "\n",
                "\n",
                "\n",
                "lg=np.array(df_plot_top[\"lng\"])\n",
                "lat=np.array(df_plot_top[\"lat\"])\n",
                "pt=np.array(df_plot_top[\"COUNT\"])\n",
                "city_name=np.array(df_plot_top[\"CITY\"])\n",
                "\n",
                "x,y=map(lg,lat)\n",
                "\n",
                "#using lambda function to create different sizes of marker as per thecount \n",
                "\n",
                "p_s=df_plot_top[\"COUNT\"].apply(lambda x: int(x)/2)\n",
                "\n",
                "#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is different.\n",
                "plt.scatter(x,y,s=p_s,marker=\"o\",c=p_s)\n",
                "\n",
                "for a,b ,c,d in zip(x,y,city_name,pt):\n",
                "    #plt.text takes x position , y position ,text(city name) ,font size and color as arguments\n",
                "    plt.text(a,b,c,fontsize=30,color=\"r\")\n",
                "    #plt.text takes x position , y position ,text(restaurant counts) ,font size and color as arguments, like above . but only i have changed the x and y position to make it more clean and easier to read\n",
                "    plt.text(a+60000,b+30000,d,fontsize=30)\n",
                "   \n",
                "    \n",
                "    \n",
                "plt.title(\"TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=df_map_final.head(15)"
            ],
            "source_orig": [
                "#lets take one data frame for bottom 15 cities with minimum retaurants counts \n",
                "df_plot_bottom=df_map_final.head(15)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df_plot_bottom"
            ],
            "source_orig": [
                "#displaying the data frame\n",
                "df_plot_bottom"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(50,50))",
                "ASSIGN=Basemap(width=120000,height=900000,projection=\"tmerc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=20,lon_0=88)",
                "ASSIGN.drawcountries()",
                "ASSIGN.drawmapboundary(color='",
                "ASSIGN.drawcoastlines()",
                "ASSIGN=np.array(df_plot_bottom[\"lng\"])",
                "ASSIGN=np.array(df_plot_bottom[\"ASSIGN\"])",
                "ASSIGN=np.array(df_plot_bottom[\"COUNT\"])",
                "ASSIGN=np.array(df_plot_bottom[\"CITY\"])",
                "ASSIGN=map(lg,lat)",
                "ASSIGN=df_plot_bottom[\"COUNT\"].apply(lambda x: int(x)*50)",
                "plt.scatter(ASSIGN,s=ASSIGN,marker=\"o\",c=ASSIGN)",
                "for a,b ,c,d in zip(ASSIGN,ASSIGN,ASSIGN):",
                "plt.text(a-3000,b,c,fontsize=30,color=\"r\")",
                "plt.text(a+60000,b+30000,d,fontsize=30)",
                "plt.title(\"BOTTOM 15 INDIAN CITIES MINIMUM RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ],
            "source_orig": [
                "#lets plot with the city names and restaurants count inside the map corresponding to the cities exact co-ordinates which we received from google api ,here marker color will be different as per marker size\n",
                "#plt.subplots(figsize=(20,50))\n",
                "plt.figure(figsize=(50,50))\n",
                "map=Basemap(width=120000,height=900000,projection=\"tmerc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=20,lon_0=88)\n",
                "map.drawcountries()\n",
                "map.drawmapboundary(color='#f2f2f2')\n",
                "\n",
                "map.drawcoastlines()\n",
                "\n",
                "\n",
                "\n",
                "lg=np.array(df_plot_bottom[\"lng\"])\n",
                "lat=np.array(df_plot_bottom[\"lat\"])\n",
                "pt=np.array(df_plot_bottom[\"COUNT\"])\n",
                "city_name=np.array(df_plot_bottom[\"CITY\"])\n",
                "\n",
                "x,y=map(lg,lat)\n",
                "\n",
                "#using lambda function to create different sizes of marker as per thecount \n",
                "\n",
                "p_s=df_plot_bottom[\"COUNT\"].apply(lambda x: int(x)*50)\n",
                "\n",
                "#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is different.\n",
                "plt.scatter(x,y,s=p_s,marker=\"o\",c=p_s)\n",
                "\n",
                "for a,b ,c,d in zip(x,y,city_name,pt):\n",
                "    #plt.text takes x position , y position ,text(city name) ,font size and color as arguments\n",
                "    plt.text(a-3000,b,c,fontsize=30,color=\"r\")\n",
                "    #plt.text takes x position , y position ,text(restaurant counts) ,font size and color as arguments, like above . but only i have changed the x and y position to make it more clean and easier to read\n",
                "    plt.text(a+60000,b+30000,d,fontsize=30)\n",
                "   \n",
                "    \n",
                "    \n",
                "plt.title(\"BOTTOM 15 INDIAN CITIES MINIMUM RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from fbprophet import Prophet"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path', sep = ',')"
            ],
            "source_orig": [
                "df2 = pd.read_csv('../input/covid19-global-forecasting-week-1/train.csv', sep = ',')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df2.tail()"
            ],
            "source_orig": [
                "## I will use train set only\n",
                "df2.tail()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = df2.groupby('Date').sum()['ConfirmedCases'].reset_index()",
                "ASSIGN = df2.groupby('Date').sum()['Fatalities'].reset_index()"
            ],
            "source_orig": [
                "confirmed = df2.groupby('Date').sum()['ConfirmedCases'].reset_index()\n",
                "death = df2.groupby('Date').sum()['Fatalities'].reset_index()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "confirmed.columns = ['ds','y']",
                "ASSIGN = pd.to_datetime(ASSIGN)"
            ],
            "source_orig": [
                "confirmed.columns = ['ds','y']\n",
                "confirmed['ds'] = pd.to_datetime(confirmed['ds'])"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = Prophet(interval_width=0.97)",
                "ASSIGN.fit(confirmed)",
                "ASSIGN = m.make_future_dataframe(periods=29)",
                "ASSIGN = future.copy()",
                "ASSIGN.tail()"
            ],
            "source_orig": [
                "m = Prophet(interval_width=0.97)\n",
                "m.fit(confirmed)\n",
                "future = m.make_future_dataframe(periods=29)\n",
                "future_confirmed = future.copy() \n",
                "future.tail()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = m.predict(future)",
                "ASSIGN[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
            ],
            "source_orig": [
                "forecast = m.predict(future)\n",
                "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = m.plot_components(forecast)"
            ],
            "source_orig": [
                "forecast_components = m.plot_components(forecast)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=pd.DataFrame(forecast)"
            ],
            "source_orig": [
                "forecast1=pd.DataFrame(forecast)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN= forecast1[['ds', 'yhat']]",
                "ASSIGN.columns = [['ForecastId', 'ConfirmedCases']]"
            ],
            "source_orig": [
                "## These are the predictions for Confirmed Covid-19 cases until 2020 April 22\n",
                "forecastC= forecast1[['ds', 'yhat']]\n",
                "forecastC.columns = [['ForecastId', 'ConfirmedCases']]\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "death.columns = ['ds','y']",
                "ASSIGN = pd.to_datetime(ASSIGN)"
            ],
            "source_orig": [
                "death.columns = ['ds','y']\n",
                "death['ds'] = pd.to_datetime(death['ds'])"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = Prophet(interval_width=0.97)",
                "ASSIGN.fit(death)",
                "ASSIGN = m.make_future_dataframe(periods=29)",
                "ASSIGN = future.copy()",
                "ASSIGN.tail()"
            ],
            "source_orig": [
                "m = Prophet(interval_width=0.97)\n",
                "m.fit(death)\n",
                "future = m.make_future_dataframe(periods=29)\n",
                "future_deaths = future.copy() \n",
                "future.tail()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = m.predict(future)",
                "ASSIGN[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
            ],
            "source_orig": [
                "forecastD = m.predict(future)\n",
                "forecastD[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame(forecastD)"
            ],
            "source_orig": [
                "death_predict = pd.DataFrame(forecastD)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN= death_predict[['ds', 'yhat']]",
                "ASSIGN.columns = [['ForecastId', 'Fatalities']]"
            ],
            "source_orig": [
                "## These are the predictions for Deaths by Covid-19 cases until 2020 April 24\n",
                "\n",
                "forecastDeath= death_predict[['ds', 'yhat']]\n",
                "forecastDeath.columns = [['ForecastId', 'Fatalities']]\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = m.plot(forecastD)"
            ],
            "source_orig": [
                "death_forecast = m.plot(forecastD)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.merge(forecastC, forecastDeath, how='inner')"
            ],
            "source_orig": [
                "submission = pd.merge(forecastC, forecastDeath, how='inner')"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submission.to_csv('submission.csv', index=False)"
            ],
            "source_orig": [
                "submission.to_csv('submission.csv', index=False)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import pandas as pd\n",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv('path', sep=',')"
            ],
            "source_orig": [
                "tscovid = pd.read_csv('/kaggle/input/novel-corona-virus-2019-dataset/time_series_covid_19_deaths.csv', sep=',')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = tscovid.iloc[0:405, 4:54]"
            ],
            "source_orig": [
                "artime = tscovid.iloc[0:405, 4:54]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.date_range('2020path', periods=50, freq='D')"
            ],
            "source_orig": [
                "days = pd.date_range('2020/1/22', periods=50, freq='D')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.T"
            ],
            "source_orig": [
                "artime = artime.T"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "artime.head()"
            ],
            "source_orig": [
                "artime.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame(ASSIGN)"
            ],
            "source_orig": [
                "artime = pd.DataFrame(artime)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = tscovid.iloc[0:405, 4:54]"
            ],
            "source_orig": [
                "df = tscovid.iloc[0:405, 4:54]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = df.sum(axis=0)"
            ],
            "source_orig": [
                "total = df.sum(axis=0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "total"
            ],
            "source_orig": [
                "total"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = total"
            ],
            "source_orig": [
                "artime['Total'] = total"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = days"
            ],
            "source_orig": [
                "artime['Day'] = days"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "artime.set_index('Day', inplace=True)"
            ],
            "source_orig": [
                "artime.set_index('Day', inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "artime['Total']"
            ],
            "source_orig": [
                "artime['Total']"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from statsmodels.tsa.ar_model import AR,ARResults"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = artime.iloc[:42]",
                "ASSIGN = artime.iloc[42:]"
            ],
            "source_orig": [
                "train = artime.iloc[:42]\n",
                "test = artime.iloc[42:]"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "warnings.filterwarnings(\"ignore\")"
            ],
            "source_orig": [
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = AR(train['Total'])",
                "ASSIGN = model.fit(maxlag=2,method='cmle')",
                "print(f'Lag: {ASSIGN.k_ar}')",
                "print(f'Coefficients:\\n{ASSIGN.params}')"
            ],
            "source_orig": [
                "model = AR(train['Total'])\n",
                "AR1fit = model.fit(maxlag=2,method='cmle')\n",
                "print(f'Lag: {AR1fit.k_ar}')\n",
                "print(f'Coefficients:\\n{AR1fit.params}')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "process_data"
            ],
            "source": [
                "ASSIGN=len(train)",
                "ASSIGN=len(train)+len(test)-1",
                "ASSIGN = AR1fit.predict(start=start, end=end).rename('AR(1) Predictions')"
            ],
            "source_orig": [
                "start=len(train)\n",
                "end=len(train)+len(test)-1\n",
                "predictions1 = AR1fit.predict(start=start, end=end).rename('AR(1) Predictions')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "for i in range(len(predictions1)):",
                "print(f)"
            ],
            "source_orig": [
                "for i in range(len(predictions1)):\n",
                "    print(f\"predicted={predictions1[i]}, expected={test['Total'][i]}\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "test['Total'].plot(legend=True)",
                "predictions1.plot(legend=True,figsize=(12,6));"
            ],
            "source_orig": [
                "test['Total'].plot(legend=True)\n",
                "predictions1.plot(legend=True,figsize=(12,6));"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model.fit(maxlag=2,method='cmle')",
                "print(f'Lag: {ASSIGN.k_ar}')",
                "print(f'Coefficients:\\n{ASSIGN.params}')"
            ],
            "source_orig": [
                "AR2fit = model.fit(maxlag=2,method='cmle')\n",
                "print(f'Lag: {AR2fit.k_ar}')\n",
                "print(f'Coefficients:\\n{AR2fit.params}')"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN=len(train)",
                "ASSIGN=len(train)+len(test)-1",
                "ASSIGN = AR2fit.predict(start=start, end=end).rename('AR(2) Predictions')"
            ],
            "source_orig": [
                "start=len(train)\n",
                "end=len(train)+len(test)-1\n",
                "predictions2 = AR2fit.predict(start=start, end=end).rename('AR(2) Predictions')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "test['Total'].plot(legend=True)",
                "predictions1.plot(legend=True)",
                "predictions2.plot(legend=True,figsize=(12,6));"
            ],
            "source_orig": [
                "test['Total'].plot(legend=True)\n",
                "predictions1.plot(legend=True)\n",
                "predictions2.plot(legend=True,figsize=(12,6));"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model.fit(maxlag=2,method='cmle')",
                "print(f'Lag: {ASSIGN.k_ar}')",
                "print(f'Coefficients:\\n{ASSIGN.params}')"
            ],
            "source_orig": [
                "ARfit = model.fit(maxlag=2,method='cmle')\n",
                "print(f'Lag: {ARfit.k_ar}')\n",
                "print(f'Coefficients:\\n{ARfit.params}')"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = len(train)",
                "ASSIGN = len(train)+len(test)-1",
                "ASSIGN = f'AR(12) Predictions'",
                "ASSIGN = ARfit.predict(start=start,end=end).rename(rename)"
            ],
            "source_orig": [
                "start = len(train)\n",
                "end = len(train)+len(test)-1\n",
                "rename = f'AR(12) Predictions'\n",
                "\n",
                "predictions11 = ARfit.predict(start=start,end=end).rename(rename)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "test['Total'].plot(legend=True)",
                "predictions1.plot(legend=True)",
                "predictions2.plot(legend=True)",
                "predictions11.plot(legend=True,figsize=(12,6));"
            ],
            "source_orig": [
                "test['Total'].plot(legend=True)\n",
                "predictions1.plot(legend=True)\n",
                "predictions2.plot(legend=True)\n",
                "predictions11.plot(legend=True,figsize=(12,6));"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = ['AR(1)','AR(2)','AR(11)']",
                "ASSIGN = [predictions1, predictions2, predictions11]",
                "for i in range(3):",
                "ASSIGN = mean_squared_error(test['Total'], preds[i])",
                "print(f'{ASSIGN[i]} Error: {ASSIGN:11.10}')"
            ],
            "source_orig": [
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "labels = ['AR(1)','AR(2)','AR(11)']\n",
                "preds = [predictions1, predictions2, predictions11]  # these are variables, not strings!\n",
                "\n",
                "for i in range(3):\n",
                "    error = mean_squared_error(test['Total'], preds[i])\n",
                "    print(f'{labels[i]} Error: {error:11.10}')"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = AR(artime['Total'])",
                "ASSIGN = model.fit(maxlag=8)",
                "ASSIGN = ARfit.predict(start=len(artime), end=len(artime)+20).rename('Forecast')",
                "artime['Total'].plot(legend=True)",
                "ASSIGN.plot(legend=True, grid=True, figsize=(12,6));"
            ],
            "source_orig": [
                "model = AR(artime['Total'])\n",
                "\n",
                "# Next, fit the model\n",
                "ARfit = model.fit(maxlag=8)\n",
                "\n",
                "# Make predictions\n",
                "fcast = ARfit.predict(start=len(artime), end=len(artime)+20).rename('Forecast')\n",
                "\n",
                "# Plot the results\n",
                "artime['Total'].plot(legend=True)\n",
                "fcast.plot(legend=True, grid=True, figsize=(12,6));"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print('Expectative for coronavirus deaths till April 01 is', fcast)"
            ],
            "source_orig": [
                "print('Expectative for coronavirus deaths till April 01 is', fcast)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "import itertools\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_excel('path', index_col=0)"
            ],
            "source_orig": [
                "df = pd.read_excel('/kaggle/input/covid19/dataset.xlsx', index_col=0)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df.reset_index(drop=False, inplace=True)"
            ],
            "source_orig": [
                "df.reset_index(drop=False, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.tail()"
            ],
            "source_orig": [
                "df.tail()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.isnull().sum()"
            ],
            "source_orig": [
                "df.isnull().sum()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(14,9))",
                "sns.heatmap(df.isnull(), cbar=False, yticklabels=False)"
            ],
            "source_orig": [
                "# o nmero de clulas com valores NaN  muito grande. Exposio grfica para melhor visualizao\n",
                "plt.figure(figsize=(14,9))\n",
                "sns.heatmap(df.isnull(), cbar=False, yticklabels=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.notnull().sum()"
            ],
            "source_orig": [
                "df.notnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=df.columns"
            ],
            "source_orig": [
                "cols=df.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = df[cols[0:20]]"
            ],
            "source_orig": [
                "relevant = df[cols[0:20]]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN"
            ],
            "source_orig": [
                "relevant['Neutrophils'] = df['Neutrophils']"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "relevant.info()"
            ],
            "source_orig": [
                "relevant.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = relevant.columns"
            ],
            "source_orig": [
                "col = relevant.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "relevant.apply(falta_numero, axis=0)"
            ],
            "source_orig": [
                "relevant.apply(falta_numero, axis=0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "relevant.head()"
            ],
            "source_orig": [
                "relevant.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "relevant[col[6:21]] = relevant[col[6:21]].apply(normaliza)"
            ],
            "source_orig": [
                "relevant[col[6:21]] = relevant[col[6:21]].apply(normaliza)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "relevant[col[6:21]] = relevant[col[6:21]].fillna(9)"
            ],
            "source_orig": [
                "relevant[col[6:21]] = relevant[col[6:21]].fillna(9)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(14,9))",
                "sns.heatmap(relevant.isnull(), cbar=False, yticklabels=False)"
            ],
            "source_orig": [
                "plt.figure(figsize=(14,9))\n",
                "sns.heatmap(relevant.isnull(), cbar=False, yticklabels=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "relevant.notnull().sum()"
            ],
            "source_orig": [
                "## Eliminamos todos os valores nulos ou negativos\n",
                "relevant.notnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = relevant"
            ],
            "source_orig": [
                "covid = relevant"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "covid.reset_index(drop=True, inplace=True)"
            ],
            "source_orig": [
                "covid.reset_index(drop=True, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "covid.tail()"
            ],
            "source_orig": [
                "covid.tail()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.drop(['Patient ID'], axis=1, inplace=True)",
                "ASSIGN = relevant"
            ],
            "source_orig": [
                "covid = covid.drop(['Patient ID'], axis=1, inplace=True)\n",
                "covid = relevant"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.drop(['Patient addmited to regular ward (1=yes, 0=no)'], axis=1, inplace=True)",
                "ASSIGN = relevant"
            ],
            "source_orig": [
                "covid = covid.drop(['Patient addmited to regular ward (1=yes, 0=no)'], axis=1, inplace=True)\n",
                "covid = relevant"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.drop(['Patient addmited to semi-intensive unit (1=yes, 0=no)'], axis=1, inplace=True)",
                "ASSIGN = relevant"
            ],
            "source_orig": [
                "covid = covid.drop(['Patient addmited to semi-intensive unit (1=yes, 0=no)'], axis=1, inplace=True)\n",
                "covid = relevant"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.drop(['Patient addmited to intensive care unit (1=yes, 0=no)'], axis=1, inplace=True)",
                "ASSIGN = relevant"
            ],
            "source_orig": [
                "covid = covid.drop(['Patient addmited to intensive care unit (1=yes, 0=no)'], axis=1, inplace=True)\n",
                "covid = relevant"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "covid.head()"
            ],
            "source_orig": [
                "covid.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "covid['SARS-Cov-2 exam result'].value_counts()"
            ],
            "source_orig": [
                "# Verificando os valores da coluna target\n",
                "covid['SARS-Cov-2 exam result'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "covid['SARS-Cov-2 exam result'] = pd.get_dummies(covid['SARS-Cov-2 exam result'])"
            ],
            "source_orig": [
                "# convertendo celulas categricas em numericas\n",
                "covid['SARS-Cov-2 exam result'] = pd.get_dummies(covid['SARS-Cov-2 exam result'])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "covid['SARS-Cov-2 exam result'].value_counts()"
            ],
            "source_orig": [
                "covid['SARS-Cov-2 exam result'].value_counts()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "covid.tail()"
            ],
            "source_orig": [
                "# verificao final\n",
                "covid.tail()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(covid.drop('SARS-Cov-2 exam result',axis=1), covid['SARS-Cov-2 exam result'], test_size=0.25)"
            ],
            "source_orig": [
                "X_train, X_test, y_train, y_test = train_test_split(covid.drop('SARS-Cov-2 exam result',axis=1), covid['SARS-Cov-2 exam result'], test_size=0.25)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = LogisticRegression()",
                "ASSIGN.fit(X_train,y_train)"
            ],
            "source_orig": [
                "logmodel = LogisticRegression()\n",
                "logmodel.fit(X_train,y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = logmodel.predict(X_test)"
            ],
            "source_orig": [
                "predictions = logmodel.predict(X_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from sklearn.metrics import confusion_matrix"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN=confusion_matrix(y_test,predictions)",
                "print(ASSIGN)"
            ],
            "source_orig": [
                "reg_cm=confusion_matrix(y_test,predictions)\n",
                "print(reg_cm)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def matriz_deconfusao(cm, target_names,",
                "ASSIGN='Matriz de Confuso',",
                "ASSIGN=None,",
                "ASSIGN=True):",
                "ASSIGN = np.trace(cm) path(np.sum(cm))",
                "ASSIGN = 1 - accuracy",
                "if ASSIGN is None:",
                "ASSIGN = plt.get_cmap('Blues')",
                "plt.figure(figsize=(8, 6))",
                "plt.imshow(cm, interpolation='nearest', ASSIGN=ASSIGN)",
                "plt.ASSIGN(ASSIGN)",
                "plt.colorbar()",
                "if target_names is not None:",
                "ASSIGN = np.arange(len(target_names))",
                "plt.xticks(ASSIGN, target_names, rotation=45)",
                "plt.yticks(ASSIGN, target_names)",
                "if ASSIGN:",
                "ASSIGN = ASSIGN.astype('float') path(axis=1)[:, np.newaxis]",
                "ASSIGN = cm.max() path() path",
                "for i, j in itertools.product(range(ASSIGN.shape[0]), range(ASSIGN.shape[1])):",
                "if ASSIGN:",
                "plt.text(j, i, \"{:0.4f}\".format(ASSIGN[i, j]),",
                "ASSIGN=\"center\",",
                "ASSIGN=\"white\" if cm[i, j] > thresh else \"black\")",
                "else:",
                "plt.text(j, i, \"{:,}\".format(ASSIGN[i, j]),",
                "ASSIGN=\"center\",",
                "ASSIGN=\"white\" if cm[i, j] > thresh else \"black\")",
                "plt.tight_layout()",
                "plt.ylabel('True label')",
                "plt.xlabel('Predicted label\\naccuracy={:0.4f}; ASSIGN={:0.4f}'.format(ASSIGN, ASSIGN))",
                "plt.show()"
            ],
            "source_orig": [
                "def matriz_deconfusao(cm, target_names,\n",
                "                          title='Matriz de Confuso',\n",
                "                          cmap=None,\n",
                "                          normalize=True):\n",
                "    \n",
                "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
                "    misclass = 1 - accuracy\n",
                "\n",
                "    if cmap is None:\n",
                "        cmap = plt.get_cmap('Blues')\n",
                "\n",
                "    plt.figure(figsize=(8, 6))\n",
                "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
                "    plt.title(title)\n",
                "    plt.colorbar()\n",
                "\n",
                "    if target_names is not None:\n",
                "        tick_marks = np.arange(len(target_names))\n",
                "        plt.xticks(tick_marks, target_names, rotation=45)\n",
                "        plt.yticks(tick_marks, target_names)\n",
                "\n",
                "    if normalize:\n",
                "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "\n",
                "\n",
                "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
                "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
                "        if normalize:\n",
                "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
                "                     horizontalalignment=\"center\",\n",
                "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
                "        else:\n",
                "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
                "                     horizontalalignment=\"center\",\n",
                "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
                "\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.ylabel('True label')\n",
                "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "matriz_deconfusao(cm = reg_cm, normalize  = False,",
                "ASSIGN = ['Positivo - COVID-19', 'No Infectado'],",
                "ASSIGN    = \"Matriz de Confuso\")"
            ],
            "source_orig": [
                "matriz_deconfusao(cm = reg_cm, normalize  = False,\n",
                "                      target_names = ['Positivo - COVID-19', 'No Infectado'],\n",
                "                      title        = \"Matriz de Confuso\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "#generalization and overfitting\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline  "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def make_poly(X, deg):",
                "ASSIGN = len(X)",
                "ASSIGN = [np.ones(n)]",
                "for d in range(deg):",
                "ASSIGN.append(X**(d+1))",
                "return np.vstack(data).T",
                "def fit(X, Y):",
                "return np.linalg.solve(X.T.dot(X), X.T.dot(Y))",
                "def fit_and_display(X, Y, sample, deg):",
                "ASSIGN = len(X)",
                "ASSIGN = np.random.choice(N, sample)",
                "ASSIGN = X[train_idx]",
                "ASSIGN = Y[train_idx]",
                "plt.scatter(ASSIGN, ASSIGN)",
                "plt.show()",
                "ASSIGN = make_poly(Xtrain, deg)",
                "ASSIGN = fit(Xtrain_poly, Ytrain)",
                "ASSIGN = make_poly(X, deg)",
                "ASSIGN = X_poly.dot(w)",
                "plt.plot(X, Y)",
                "plt.plot(X, ASSIGN)",
                "plt.scatter(ASSIGN, ASSIGN)",
                "plt.title(\"deg = %d\" %deg)",
                "plt.show()"
            ],
            "source_orig": [
                "def make_poly(X, deg):\n",
                "    n = len(X)\n",
                "    data = [np.ones(n)]\n",
                "    for d in range(deg):\n",
                "        data.append(X**(d+1))\n",
                "    return np.vstack(data).T\n",
                "\n",
                "def fit(X, Y):\n",
                "    return np.linalg.solve(X.T.dot(X), X.T.dot(Y))\n",
                "\n",
                "def fit_and_display(X, Y, sample, deg):\n",
                "    N = len(X)\n",
                "    train_idx = np.random.choice(N, sample)\n",
                "    Xtrain = X[train_idx]\n",
                "    Ytrain = Y[train_idx]\n",
                "    \n",
                "    plt.scatter(Xtrain, Ytrain)\n",
                "    plt.show()\n",
                "    \n",
                "    #fit poly\n",
                "    Xtrain_poly = make_poly(Xtrain, deg)\n",
                "    w = fit(Xtrain_poly, Ytrain)\n",
                "    \n",
                "    # display the polynomial\n",
                "    X_poly = make_poly(X, deg)\n",
                "    Y_hat = X_poly.dot(w)\n",
                "    plt.plot(X, Y)\n",
                "    plt.plot(X, Y_hat)\n",
                "    plt.scatter(Xtrain, Ytrain)\n",
                "    plt.title(\"deg = %d\" %deg)\n",
                "    plt.show()\n",
                "    \n",
                "#for deg in range(5,10):\n",
                " #   fit_and_display(X, Y, 10, deg)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def get_mse(Y, Yhat):",
                "ASSIGN = Y - Yhat",
                "return ASSIGN.dot(ASSIGN) path(ASSIGN)",
                "def plot_train_vs_test_curves(X, Y, sample=20, max_deg=20):",
                "ASSIGN = len(X)",
                "ASSIGN = np.random.choice(N, sample)",
                "ASSIGN = X[train_idx]",
                "ASSIGN = Y[train_idx]",
                "ASSIGN = [idx for idx in range(N) if idx not in train_idx]",
                "ASSIGN = X[test_idx]",
                "ASSIGN = Y[test_idx]",
                "ASSIGN = []",
                "ASSIGN = []",
                "for deg in range(max_deg+1):",
                "ASSIGN = make_poly(Xtrain, deg)",
                "ASSIGN = fit(Xtrain_poly, Ytrain)",
                "ASSIGN = Xtrain_poly.dot(w)",
                "ASSIGN = get_mse(Ytrain, Yhat_train)",
                "ASSIGN = make_poly(Xtest, deg)",
                "ASSIGN = Xtest_poly.dot(w)",
                "ASSIGN = get_mse(Ytest, Yhat_test)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "plt.plot(ASSIGN, label=\"train mse\")",
                "plt.plot(ASSIGN, label=\"test mse\")",
                "plt.legend()",
                "plt.show()",
                "plt.plot(ASSIGN, label=\"train mse\")",
                "plt.legend()",
                "plt.show()"
            ],
            "source_orig": [
                "def get_mse(Y, Yhat):\n",
                "    d = Y - Yhat\n",
                "    return d.dot(d) / len(d)\n",
                "\n",
                "def plot_train_vs_test_curves(X, Y, sample=20, max_deg=20):\n",
                "    N = len(X)\n",
                "    train_idx = np.random.choice(N, sample)\n",
                "    Xtrain = X[train_idx]\n",
                "    Ytrain = Y[train_idx]\n",
                "    \n",
                "    test_idx = [idx for idx in range(N) if idx not in train_idx]\n",
                "    Xtest = X[test_idx]\n",
                "    Ytest = Y[test_idx]\n",
                "    \n",
                "    mse_trains = []\n",
                "    mse_tests = []\n",
                "    for deg in range(max_deg+1):\n",
                "        Xtrain_poly = make_poly(Xtrain, deg)\n",
                "        w = fit(Xtrain_poly, Ytrain)\n",
                "        Yhat_train = Xtrain_poly.dot(w)\n",
                "        mse_train = get_mse(Ytrain, Yhat_train)\n",
                "\n",
                "        Xtest_poly = make_poly(Xtest, deg)\n",
                "        Yhat_test = Xtest_poly.dot(w)\n",
                "        mse_test = get_mse(Ytest, Yhat_test)\n",
                "\n",
                "        mse_trains.append(mse_train)\n",
                "        mse_tests.append(mse_test)\n",
                "\n",
                "    plt.plot(mse_trains, label=\"train mse\")\n",
                "    plt.plot(mse_tests, label=\"test mse\")\n",
                "    plt.legend()\n",
                "    plt.show()\n",
                "\n",
                "    plt.plot(mse_trains, label=\"train mse\")\n",
                "    plt.legend()\n",
                "    plt.show()\n",
                "        "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN == \"__main__\":",
                "ASSIGN = 100",
                "ASSIGN = np.linspace(0, 6*np.pi, N)",
                "ASSIGN = np.sin(X)",
                "plt.plot(ASSIGN, ASSIGN)",
                "plt.show()",
                "for deg in (5, 6, 7, 8, 9):",
                "fit_and_display(ASSIGN, ASSIGN, 10, deg)",
                "plot_train_vs_test_curves(ASSIGN, ASSIGN)"
            ],
            "source_orig": [
                "if __name__ == \"__main__\":\n",
                "    # make up some data and plot it\n",
                "    N = 100\n",
                "    X = np.linspace(0, 6*np.pi, N)\n",
                "    Y = np.sin(X)\n",
                "\n",
                "    plt.plot(X, Y)\n",
                "    plt.show()\n",
                "\n",
                "    for deg in (5, 6, 7, 8, 9):\n",
                "        fit_and_display(X, Y, 10, deg)\n",
                "    plot_train_vs_test_curves(X, Y)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(os.listdir('..path'))"
            ],
            "source_orig": [
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "source_orig": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "CHECKPOINT",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "source_orig": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "source_orig": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = 1000",
                "ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead)",
                "ASSIGN.dataframeName = 'credits.csv'",
                "nRow, nCol = ASSIGN.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "source_orig": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# credits.csv has 45476 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df1 = pd.read_csv('../input/credits.csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'credits.csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df1.head(5)"
            ],
            "source_orig": [
                "df1.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ],
            "source_orig": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = 1000",
                "ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead)",
                "ASSIGN.dataframeName = 'keywords.csv'",
                "nRow, nCol = ASSIGN.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "source_orig": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# keywords.csv has 46419 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df2 = pd.read_csv('../input/keywords.csv', delimiter=',', nrows = nRowsRead)\n",
                "df2.dataframeName = 'keywords.csv'\n",
                "nRow, nCol = df2.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df2.head(5)"
            ],
            "source_orig": [
                "df2.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotPerColumnDistribution(df2, 10, 5)"
            ],
            "source_orig": [
                "plotPerColumnDistribution(df2, 10, 5)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = 1000",
                "ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead)",
                "ASSIGN.dataframeName = 'links_small.csv'",
                "nRow, nCol = ASSIGN.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "source_orig": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# links_small.csv has 9125 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df3 = pd.read_csv('../input/links_small.csv', delimiter=',', nrows = nRowsRead)\n",
                "df3.dataframeName = 'links_small.csv'\n",
                "nRow, nCol = df3.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df3.head(5)"
            ],
            "source_orig": [
                "df3.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotPerColumnDistribution(df3, 10, 5)"
            ],
            "source_orig": [
                "plotPerColumnDistribution(df3, 10, 5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotCorrelationMatrix(df3, 8)"
            ],
            "source_orig": [
                "plotCorrelationMatrix(df3, 8)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotScatterMatrix(df3, 9, 10)"
            ],
            "source_orig": [
                "plotScatterMatrix(df3, 9, 10)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "CHECKPOINT",
                "SETUP",
                "1"
            ],
            "source_orig": [
                "1\n",
                "import pandas as pd"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "ASSIGN=pd.read_csv('..path')",
                "ASSIGN=data[data.isnull().any(axis=1)].head()",
                "ASSIGN.iloc[0:3]"
            ],
            "source_orig": [
                "data=pd.read_csv('../input/flight-route-database/routes.csv')\n",
                "nan=data[data.isnull().any(axis=1)].head()\n",
                "nan.iloc[0:3]"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN=data.fillna(0)",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "data2=data.fillna(0)\n",
                "data2.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "CHECKPOINT",
                "SETUP",
                "2"
            ],
            "source_orig": [
                "2\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN=['CSK','KKR','DC','MI']",
                "ASSIGN=[149,218,188,143]",
                "plt.bar(ASSIGN,ASSIGN,color=['gold','purple','gold','gold'])",
                "plt.title('IPL TEAM SCORE GRAPH')",
                "plt.xlabel('TEAMS')",
                "plt.ylabel('SCORE')"
            ],
            "source_orig": [
                "team=['CSK','KKR','DC','MI']\n",
                "score=[149,218,188,143]\n",
                "plt.bar(team,score,color=['gold','purple','gold','gold'])\n",
                "plt.title('IPL TEAM SCORE GRAPH')\n",
                "plt.xlabel('TEAMS')\n",
                "plt.ylabel('SCORE')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "CHECKPOINT",
                "SETUP",
                "3"
            ],
            "source_orig": [
                "3\n",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN=np.array([5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100])",
                "ASSIGN=np.array([2,5,6,10,80])",
                "ASSIGN=[m for m, val in enumerate(a1) if val in set(a2)]",
                "ASSIGN=np.delete(a1,temp)",
                "print(,ASSIGN)",
                "print(,ASSIGN)",
                "print(,ASSIGN)",
                "print(,ASSIGN)"
            ],
            "source_orig": [
                "a1=np.array([5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100])\n",
                "a2=np.array([2,5,6,10,80])\n",
                "temp=[m for m, val in enumerate(a1) if val in set(a2)]\n",
                "new_arr=np.delete(a1,temp)\n",
                "print(\"ARRAY 1:\",a1)\n",
                "print(\"ARRAY 2:\",a2)\n",
                "print(\"NEW ARRAY:\",new_arr)\n",
                "print(\"ARRAY 2:\",a2)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "CHECKPOINT",
                "SETUP",
                "4"
            ],
            "source_orig": [
                "4\n",
                "import pandas as pd\n",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = train.drop(\"Survived\",axis=1)",
                "ASSIGN = train[\"Survived\"]",
                "X_train, X_test, y_train, y_test = train_test_split(ASSIGN, ASSIGN, test_size = 0.3, random_state = 101)",
                "ASSIGN = LogisticRegression()",
                "ASSIGN.fit(X_train,y_train)",
                "ASSIGN = logmodel.predict(X_test)",
                "print(,f1_score(y_test, ASSIGN))",
                "print()",
                "confusion_matrix(y_test, ASSIGN)"
            ],
            "source_orig": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.model_selection import cross_val_predict\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import f1_score\n",
                "\n",
                "train = pd.read_csv(\"../input/titanic/train_data.csv\")\n",
                "\n",
                "\n",
                "X = train.drop(\"Survived\",axis=1)\n",
                "y = train[\"Survived\"]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
                "\n",
                "logmodel = LogisticRegression()\n",
                "logmodel.fit(X_train,y_train)\n",
                "\n",
                "predictions = logmodel.predict(X_test)\n",
                "\n",
                "print(\"F1 Score:\",f1_score(y_test, predictions))\n",
                " \n",
                "print(\"\\nConfusion Matrix(below):\\n\")\n",
                "confusion_matrix(y_test, predictions)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN=['O+','A+','B+','AB+','O-','A-','B-','AB-']",
                "ASSIGN=[12,11,2,15,22,14,34,21]",
                "plt.bar(ASSIGN,ASSIGN,color=['red','red','red','red','black','red','red','red'])",
                "plt.title('BLOOD GROUP')",
                "plt.xlabel('X-axis')",
                "plt.ylabel(\"Y-axis\")"
            ],
            "source_orig": [
                "bg=['O+','A+','B+','AB+','O-','A-','B-','AB-']\n",
                "no=[12,11,2,15,22,14,34,21]\n",
                "plt.bar(bg,no,color=['red','red','red','red','black','red','red','red'])\n",
                "plt.title('BLOOD GROUP')\n",
                "plt.xlabel('X-axis')\n",
                "plt.ylabel(\"Y-axis\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.pie(mrks,labels=slc,explode=(0,0,0,0,0,0.1))"
            ],
            "source_orig": [
                "plt.pie(mrks,labels=slc,explode=(0,0,0,0,0,0.1))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import numpy as np\n",
                "import random as rn"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ht"
            ],
            "source_orig": [
                "ht"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ht[10]=172",
                "ht[11]=172",
                "ht[22]=2",
                "ht[49]=2"
            ],
            "source_orig": [
                "ht[10]=172\n",
                "ht[11]=172\n",
                "ht[22]=2\n",
                "ht[49]=2"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.boxplot(ht)",
                "plt.title('Height')",
                "plt.xlabel('X-axis')",
                "plt.ylabel('Y-axis')",
                "plt.show()"
            ],
            "source_orig": [
                "plt.boxplot(ht)\n",
                "plt.title('Height')\n",
                "plt.xlabel('X-axis')\n",
                "plt.ylabel('Y-axis')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "SETUP",
                "ASSIGN = load_breast_cancer()",
                "ASSIGN.keys()"
            ],
            "source_orig": [
                "from sklearn.datasets import load_breast_cancer\n",
                "cancer_data = load_breast_cancer()\n",
                "cancer_data.keys()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame(cancer_data['data'],columns=cancer_data['feature_names'])",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "CC = pd.DataFrame(cancer_data['data'],columns=cancer_data['feature_names'])\n",
                "CC.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "SETUP",
                "ASSIGN = StandardScaler()",
                "ASSIGN.fit(CC)"
            ],
            "source_orig": [
                "from sklearn.preprocessing import StandardScaler\n",
                "scaler = StandardScaler()\n",
                "scaler.fit(CC)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = scaler.transform(CC)"
            ],
            "source_orig": [
                "scaled_data = scaler.transform(CC)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "SETUP",
                "ASSIGN = PCA(n_components=2)",
                "ASSIGN.fit(scaled_data)"
            ],
            "source_orig": [
                "from sklearn.decomposition import PCA\n",
                "pca = PCA(n_components=2)\n",
                "pca.fit(scaled_data)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pca.transform(scaled_data)"
            ],
            "source_orig": [
                "x_pca = pca.transform(scaled_data)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "scaled_data.shape"
            ],
            "source_orig": [
                "scaled_data.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "x_pca.shape"
            ],
            "source_orig": [
                "x_pca.shape"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(8,6))",
                "plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'])",
                "plt.xlabel('First principal component')",
                "plt.ylabel('Second Principal Component')"
            ],
            "source_orig": [
                "plt.figure(figsize=(8,6))\n",
                "plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'])\n",
                "plt.xlabel('First principal component')\n",
                "plt.ylabel('Second Principal Component')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import pandas as pd\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN=pd.read_csv('..path')"
            ],
            "source_orig": [
                "data=pd.read_csv('../input/iris/Iris.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "data.shape"
            ],
            "source_orig": [
                "data.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "data.dtypes"
            ],
            "source_orig": [
                "data.dtypes"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data.iloc[:3]"
            ],
            "source_orig": [
                "data.iloc[:3]"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = int(input(\"Enter the size of the matrix here:-\"))",
                "ASSIGN = np.eye(sz)",
                "print(, ASSIGN)"
            ],
            "source_orig": [
                "import numpy as np\n",
                "from scipy import sparse\n",
                "sz = int(input(\"Enter the size of the matrix here:-\"))\n",
                "arr = np.eye(sz)\n",
                "print(\"NumPy array:\", arr)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.count()"
            ],
            "source_orig": [
                "data.count()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.isnull().sum()"
            ],
            "source_orig": [
                "data.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.keys()"
            ],
            "source_orig": [
                "data.keys()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "data.shape"
            ],
            "source_orig": [
                "data.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "data.columns"
            ],
            "source_orig": [
                "data.columns"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.describe()"
            ],
            "source_orig": [
                "data.describe()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import pandas as pd"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN=pd.read_csv('..path')"
            ],
            "source_orig": [
                "data=pd.read_csv('../input/iris/Iris.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "data.columns"
            ],
            "source_orig": [
                "data.columns"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.iloc[0:10]"
            ],
            "source_orig": [
                "data.iloc[0:10]#selecting 6 cloumns and 10 rows"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import numpy as np\n",
                "import random as rn\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.boxplot(ht)",
                "plt.title('Height')",
                "plt.xlabel('X-axis')",
                "plt.ylabel('Y-axis')",
                "plt.show()"
            ],
            "source_orig": [
                "plt.boxplot(ht)\n",
                "plt.title('Height')\n",
                "plt.xlabel('X-axis')\n",
                "plt.ylabel('Y-axis')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.loc[data.SepalWidthCm.isnull()]"
            ],
            "source_orig": [
                "data.loc[data.SepalWidthCm.isnull()]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "data"
            ],
            "source_orig": [
                "data#total no. of observations"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import pandas as pd"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=pd.DataFrame({'Students':['A','B','C','D'],'Subjects':['DSD','AIpath','COA','ALGORITHMS'],'Marks':[50,90,80,80]})"
            ],
            "source_orig": [
                "df=pd.DataFrame({'Students':['A','B','C','D'],'Subjects':['DSD','AI/ML','COA','ALGORITHMS'],'Marks':[50,90,80,80]})"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df"
            ],
            "source_orig": [
                "df"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN=pd.read_csv('..path')"
            ],
            "source_orig": [
                "data=pd.read_csv('../input/iris/Iris.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.iloc[:3]"
            ],
            "source_orig": [
                "data.iloc[:3]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.loc[:3,'SepalLengthCm']"
            ],
            "source_orig": [
                "data.loc[:3,'SepalLengthCm']"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN=pd.read_csv('..path')"
            ],
            "source_orig": [
                "data2=pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data2.head()"
            ],
            "source_orig": [
                "data2.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data2.loc[data2.serum_creatinine.isnull()]"
            ],
            "source_orig": [
                "data2.loc[data2.serum_creatinine.isnull()]"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "print(os.listdir('..path'))",
                "py.init_notebook_mode(connected=False)",
                "sns.set(rc={'figure.figsize':(20,10)})"
            ],
            "source_orig": [
                "# Other Imports\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import os\n",
                "import pandas as pd\n",
                "import plotly.graph_objs as go\n",
                "import plotly.offline as py\n",
                "import seaborn as sns\n",
                "\n",
                "# Pytorch Imports\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "import torch.utils.data as data\n",
                "import torchvision\n",
                "from torchvision import transforms\n",
                "import torchvision.models as models\n",
                "\n",
                "print(os.listdir('../input'))\n",
                "%matplotlib inline\n",
                "py.init_notebook_mode(connected=False)\n",
                "sns.set(rc={'figure.figsize':(20,10)})\n",
                "\n",
                "%env JOBLIB_TEMP_FOLDER=/tmp"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "ASSIGN = torch.ASSIGN(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ],
            "source_orig": [
                "CAT_TO_NAME_PATH = '../input/hackathon-blossom-flower-classification/cat_to_name.json'\n",
                "TRAIN_DATA_PATH = \"../input/hackathon-blossom-flower-classification/flower_data/flower_data/train\"\n",
                "VAL_DATA_PATH = \"../input/hackathon-blossom-flower-classification/flower_data/flower_data/valid\"\n",
                "TEST_DATA_PATH = '../input/hackathon-blossom-flower-classification/test set/'\n",
                "CHECKPOINT_PATH = '../input/model-checkpoints/'\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "def get_cat_to_name_data(file_path):",
                "\"\"\" Imports the cat_to_name.json file and returns a Pandas DataFrame \"\"\"",
                "with open(file_path, 'r') as f:",
                "ASSIGN = json.load(f)",
                "return cat_to_names"
            ],
            "source_orig": [
                "def get_cat_to_name_data(file_path):\n",
                "    \"\"\" Imports the cat_to_name.json file and returns a Pandas DataFrame \"\"\"\n",
                "    with open(file_path, 'r') as f:\n",
                "        cat_to_names = json.load(f)\n",
                "    return cat_to_names"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = get_cat_to_name_data(CAT_TO_NAME_PATH)"
            ],
            "source_orig": [
                "cat_to_names = get_cat_to_name_data(CAT_TO_NAME_PATH)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "cat_to_names"
            ],
            "source_orig": [
                "cat_to_names"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "for i in cat_to_names:",
                "ASSIGN == '11':",
                "print(cat_to_names['11'])"
            ],
            "source_orig": [
                "for i in cat_to_names:\n",
                "    if i == '11':\n",
                "        print(cat_to_names['11'])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def get_data_loaders(train_data_path, val_data_path):",
                "ASSIGN = transforms.Compose([",
                "transforms.Resize(256),",
                "transforms.CenterCrop(256),",
                "transforms.ToTensor(),",
                "transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))",
                "])",
                "ASSIGN = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)",
                "ASSIGN = data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4)",
                "ASSIGN = torchvision.datasets.ImageFolder(root=val_data_path, transform=transform)",
                "ASSIGN = data.DataLoader(val_data, batch_size=32, shuffle=True, num_workers=4)",
                "ASSIGN = train_data.classes",
                "ASSIGN = val_data.classes",
                "return train_loader, val_loader, train_class_names, val_class_names"
            ],
            "source_orig": [
                "def get_data_loaders(train_data_path, val_data_path):\n",
                "    transform = transforms.Compose([\n",
                "        transforms.Resize(256),\n",
                "        transforms.CenterCrop(256),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
                "        ])\n",
                "\n",
                "    train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
                "    train_loader = data.DataLoader(train_data, batch_size=32, shuffle=True,  num_workers=4)\n",
                "    val_data = torchvision.datasets.ImageFolder(root=val_data_path, transform=transform)\n",
                "    val_loader  = data.DataLoader(val_data, batch_size=32, shuffle=True, num_workers=4) \n",
                "    \n",
                "    train_class_names = train_data.classes\n",
                "    val_class_names = val_data.classes\n",
                "    \n",
                "    return train_loader, val_loader, train_class_names, val_class_names"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = get_data_loaders(TRAIN_DATA_PATH, VAL_DATA_PATH)"
            ],
            "source_orig": [
                "train_loader, val_loader, train_class_names, val_class_names = get_data_loaders(TRAIN_DATA_PATH, VAL_DATA_PATH)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def create_model():",
                "ASSIGN = models.densenet161(pretrained=True)",
                "for param in ASSIGN.parameters():",
                "param.requires_grad = False",
                "ASSIGN = model.classifier.in_features",
                "ASSIGN.classifier = nn.Sequential(nn.Linear(ASSIGN, 2048),",
                "nn.ReLU(),",
                "nn.Linear(2048, 512),",
                "nn.ReLU(),",
                "nn.Linear(512, 102),",
                "nn.LogSoftmax(dim=1))",
                "ASSIGN.to(device)",
                "return model"
            ],
            "source_orig": [
                "def create_model():\n",
                "    model = models.densenet161(pretrained=True)\n",
                "\n",
                "    for param in model.parameters():\n",
                "        param.requires_grad = False\n",
                "\n",
                "    num_filters = model.classifier.in_features\n",
                "    model.classifier = nn.Sequential(nn.Linear(num_filters, 2048),\n",
                "                               nn.ReLU(),\n",
                "                               nn.Linear(2048, 512),\n",
                "                               nn.ReLU(),\n",
                "                               nn.Linear(512, 102),\n",
                "                               nn.LogSoftmax(dim=1))\n",
                "\n",
                "    # Move model to the device specified above\n",
                "    model.to(device)\n",
                "    return model"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = create_model()"
            ],
            "source_orig": [
                "model = create_model()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = nn.NLLLoss()",
                "ASSIGN = optim.Adam(model.parameters())"
            ],
            "source_orig": [
                "criterion = nn.NLLLoss()\n",
                "# Set the optimizer function using torch.optim as optim library\n",
                "optimizer = optim.Adam(model.parameters())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "def train(epochs):",
                "ASSIGN = []",
                "ASSIGN = []",
                "for epoch in range(epochs):",
                "ASSIGN = 0",
                "ASSIGN = 0",
                "ASSIGN = 0",
                "model.train()",
                "ASSIGN = 0",
                "for inputs, labels in train_loader:",
                "ASSIGN = inputs.to(device), labels.to(device)",
                "optimizer.zero_grad()",
                "ASSIGN = model.forward(inputs)",
                "ASSIGN = criterion(output, labels)",
                "ASSIGN.backward()",
                "optimizer.step()",
                "ASSIGN += ASSIGN.item()",
                "ASSIGN += 1",
                "model.eval()",
                "ASSIGN = 0",
                "with torch.no_grad():",
                "for ASSIGN in val_loader:",
                "ASSIGN = inputs.to(device), labels.to(device)",
                "ASSIGN = model.forward(inputs)",
                "ASSIGN = criterion(output, labels)",
                "ASSIGN += ASSIGN.item()",
                "ASSIGN = torch.exp(ASSIGN)",
                "ASSIGN = output.topk(1, dim=1)",
                "ASSIGN = top_class == labels.view(*top_class.shape)",
                "ASSIGN += torch.mean(ASSIGN.type(torch.FloatTensor)).item()",
                "ASSIGN += 1",
                "ASSIGN = train_losspath(train_loader)",
                "ASSIGN = val_losspath(val_loader)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "print('Accuracy: ', accuracypath(val_loader))",
                "print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, ASSIGN, ASSIGN))",
                "return train_losses, valid_losses"
            ],
            "source_orig": [
                "def train(epochs):\n",
                "    train_losses = []\n",
                "    valid_losses = []\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        train_loss = 0\n",
                "        val_loss = 0\n",
                "        accuracy = 0\n",
                "\n",
                "        # Training the model\n",
                "        model.train()\n",
                "        counter = 0\n",
                "        for inputs, labels in train_loader:\n",
                "            # Move to device\n",
                "            inputs, labels = inputs.to(device), labels.to(device)\n",
                "            # Clear optimizers\n",
                "            optimizer.zero_grad()\n",
                "            # Forward pass\n",
                "            output = model.forward(inputs)\n",
                "            # Loss\n",
                "            loss = criterion(output, labels)\n",
                "            # Calculate gradients (backpropogation)\n",
                "            loss.backward()\n",
                "            # Adjust parameters based on gradients\n",
                "            optimizer.step()\n",
                "            # Add the loss to the training set's rnning loss\n",
                "            train_loss += loss.item()\n",
                "\n",
                "            # Print the progress of our training\n",
                "            counter += 1\n",
                "            #print(counter, \"/\", len(train_loader))\n",
                "\n",
                "            # Evaluating the model\n",
                "        model.eval()\n",
                "        counter = 0\n",
                "        # Tell torch not to calculate gradients\n",
                "        with torch.no_grad():\n",
                "            for inputs, labels in val_loader:\n",
                "                # Move to device\n",
                "                inputs, labels = inputs.to(device), labels.to(device)\n",
                "                # Forward pass\n",
                "                output = model.forward(inputs)\n",
                "                # Calculate Loss\n",
                "                valloss = criterion(output, labels)\n",
                "                # Add loss to the validation set's running loss\n",
                "                val_loss += valloss.item()\n",
                "\n",
                "                # Since our model outputs a LogSoftmax, find the real \n",
                "                # percentages by reversing the log function\n",
                "                output = torch.exp(output)\n",
                "                # Get the top class of the output\n",
                "                top_p, top_class = output.topk(1, dim=1)\n",
                "                # See how many of the classes were correct?\n",
                "                equals = top_class == labels.view(*top_class.shape)\n",
                "                # Calculate the mean (get the accuracy for this batch)\n",
                "                # and add it to the running accuracy for this epoch\n",
                "                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
                "\n",
                "                # Print the progress of our evaluation\n",
                "                counter += 1\n",
                "                #print(counter, \"/\", len(val_loader))\n",
                "\n",
                "        # Get the average loss for the entire epoch\n",
                "        train_loss = train_loss/len(train_loader)\n",
                "        valid_loss = val_loss/len(val_loader)\n",
                "\n",
                "        train_losses.append(train_loss)\n",
                "        valid_losses.append(valid_loss)\n",
                "        # Print out the information\n",
                "        print('Accuracy: ', accuracy/len(val_loader))\n",
                "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
                "\n",
                "    return train_losses, valid_losses\n"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "def save_checkpoint():",
                "ASSIGN = {'model': model,",
                "'state_dict': model.state_dict(),",
                "'optimizer' : optimizer.state_dict()}",
                "torch.save(ASSIGN, 'checkpoint1.pth')"
            ],
            "source_orig": [
                "def save_checkpoint():\n",
                "    checkpoint = {'model': model,\n",
                "                  'state_dict': model.state_dict(),\n",
                "                  'optimizer' : optimizer.state_dict()}\n",
                "\n",
                "    torch.save(checkpoint, 'checkpoint1.pth')"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "def predict(test_loader):",
                "model.eval()",
                "ASSIGN = []",
                "with torch.no_grad():",
                "for images, _ in test_loader:",
                "ASSIGN = ASSIGN.to(device)",
                "ASSIGN = model(images)",
                "ASSIGN = torch.exp(output)",
                "ASSIGN = ps.topk(1, dim=1)",
                "ASSIGN += [int(i) for i in list(top_class.data.cpu().numpy())]",
                "return predictions"
            ],
            "source_orig": [
                "def predict(test_loader):\n",
                "    model.eval()\n",
                "    \n",
                "    predictions = []\n",
                "    with torch.no_grad():\n",
                "        for images, _ in test_loader:\n",
                "            images = images.to(device)\n",
                "            output = model(images)\n",
                "            ps = torch.exp(output)\n",
                "            top_p, top_class = ps.topk(1, dim=1)\n",
                "            predictions += [int(i) for i in list(top_class.data.cpu().numpy())]\n",
                "        \n",
                "    return predictions"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = predict(test_loader)"
            ],
            "source_orig": [
                "predictions = predict(test_loader)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "predictions"
            ],
            "source_orig": [
                "predictions"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = create_pred_dataframe(image_names, predictions)"
            ],
            "source_orig": [
                "pred_df_with_species_name, pred_df_with_cat_number = create_pred_dataframe(image_names, predictions)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.sort_values(by=['image-names'])",
                "ASSIGN = ASSIGN.sort_values(by=['image-names'])"
            ],
            "source_orig": [
                "pred_df_with_species_name = pred_df_with_species_name.sort_values(by=['image-names'])\n",
                "pred_df_with_cat_number = pred_df_with_cat_number.sort_values(by=['image-names'])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "with pd.option_context('display.max_rows', None, 'display.max_columns', None):",
                "print(pred_df_with_cat_number)"
            ],
            "source_orig": [
                "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
                "    print(pred_df_with_cat_number)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "with pd.option_context('display.max_rows', None, 'display.max_columns', None):",
                "print(pred_df_with_species_name)"
            ],
            "source_orig": [
                "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
                "    print(pred_df_with_species_name)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "pred_df_with_species_name.to_csv('my_predictions.csv', sep='\\t', encoding='utf-8')"
            ],
            "source_orig": [
                "pred_df_with_species_name.to_csv('my_predictions.csv', sep='\\t', encoding='utf-8')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "plt.rcParams['figure.figsize']=(20,10)",
                "print(os.listdir())",
                "py.init_notebook_mode(connected=False)"
            ],
            "source_orig": [
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.image as mpimg\n",
                "%matplotlib inline\n",
                "import numpy as np\n",
                "import os\n",
                "import pandas as pd\n",
                "import plotly.offline as py\n",
                "import plotly.graph_objs as go\n",
                "import seaborn as sns\n",
                "\n",
                "plt.rcParams['figure.figsize']=(20,10)\n",
                "print(os.listdir(\"../input\"))\n",
                "py.init_notebook_mode(connected=False)\n",
                "\n",
                "%env JOBLIB_TEMP_FOLDER=/tmp"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path')"
            ],
            "source_orig": [
                "df = pd.read_csv('../input/labels.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ],
            "source_orig": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df['breed'].describe()"
            ],
            "source_orig": [
                "df['breed'].describe()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame({'breed': df['breed'].value_counts().index, 'instances': df['breed'].value_counts().values})",
                "ASSIGN = ASSIGN.sort_values(by=['breed'])",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "temp = pd.DataFrame({'breed': df['breed'].value_counts().index, 'instances': df['breed'].value_counts().values})\n",
                "temp = temp.sort_values(by=['breed'])\n",
                "temp.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = go.Bar(x=temp['breed'], y=temp['instances'])",
                "ASSIGN = [trace]",
                "ASSIGN = go.Layout(",
                "ASSIGN='Breed Counts',",
                "ASSIGN=False,",
                "ASSIGN=5000,",
                "ASSIGN=500,",
                "ASSIGN=dict(",
                "ASSIGN=100,",
                "ASSIGN=100,",
                "ASSIGN=100,",
                "ASSIGN=100",
                ")",
                ")",
                "ASSIGN = go.Figure(data=data, layout=layout)",
                "py.iplot(ASSIGN)"
            ],
            "source_orig": [
                "trace = go.Bar(x=temp['breed'], y=temp['instances'])\n",
                "data = [trace]\n",
                "layout = go.Layout(\n",
                "        title='Breed Counts',\n",
                "        autosize=False,\n",
                "        width=5000,\n",
                "        height=500,\n",
                "        margin=dict(\n",
                "            l=100,\n",
                "            r=100,\n",
                "            b=100,\n",
                "            t=100\n",
                "        )\n",
                "    )\n",
                "fig = go.Figure(data=data, layout=layout)\n",
                "py.iplot(fig)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.get_dummies(ASSIGN).values.tolist()"
            ],
            "source_orig": [
                "df['breed'] = pd.get_dummies(df['breed']).values.tolist()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ],
            "source_orig": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from torchvision import transforms\n",
                "from torchvision.datasets import ImageFolder\n",
                "from torch.utils.data import DataLoader\n",
                "from torch.utils.data.dataset import Dataset\n",
                "from PIL import Image\n",
                "\n",
                "import torchvision.models as models\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df['image_path'] = '..path' + df['id'].astype(str) + '.jpg'"
            ],
            "source_orig": [
                "df['image_path'] = '../input/train/' + df['id'].astype(str) + '.jpg'"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ],
            "source_orig": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = torch.tensor(df['breed'].tolist())"
            ],
            "source_orig": [
                "labels = torch.tensor(df['breed'].tolist())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = CustomDataset(df['image_path'], labels, train=True)",
                "ASSIGN = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)"
            ],
            "source_orig": [
                "train_dataset = CustomDataset(df['image_path'], labels, train=True)\n",
                "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = models.densenet161(pretrained=True)",
                "for param in ASSIGN.parameters():",
                "param.requires_grad = False"
            ],
            "source_orig": [
                "model = models.densenet161(pretrained=True)\n",
                "# Turn off training for their parameters\n",
                "for param in model.parameters():\n",
                "    param.requires_grad = False"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = nn.Sequential(nn.Linear(255, 1024),",
                "nn.ReLU(),",
                "nn.Linear(1024, 512),",
                "nn.ReLU(),",
                "nn.Linear(512, 120),",
                "nn.LogSoftmax(dim=1))",
                "model.ASSIGN = ASSIGN"
            ],
            "source_orig": [
                "classifier = nn.Sequential(nn.Linear(255, 1024),\n",
                "                           nn.ReLU(),\n",
                "                           nn.Linear(1024, 512),\n",
                "                           nn.ReLU(),\n",
                "                           nn.Linear(512, 120),\n",
                "                           nn.LogSoftmax(dim=1))\n",
                "# Replace default classifier with new classifier\n",
                "model.classifier = classifier"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "ASSIGN = torch.ASSIGN(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
                "model.to(ASSIGN)"
            ],
            "source_orig": [
                "import torch\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "# Move model to the device specified above\n",
                "model.to(device)\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = nn.NLLLoss()",
                "ASSIGN = optim.Adam(model.classifier.parameters())"
            ],
            "source_orig": [
                "criterion = nn.NLLLoss()\n",
                "# Set the optimizer function using torch.optim as optim library\n",
                "optimizer = optim.Adam(model.classifier.parameters())"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import matplotlib.pyplot as plt\n",
                "import re\n",
                "import os\n",
                "%matplotlib inline\n",
                "\n",
                "\n",
                "import argparse\n",
                "import pickle\n",
                "\n",
                "import numpy as np; np.seterr(invalid='ignore')\n",
                "import pandas as pd"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(os.listdir('..path'))"
            ],
            "source_orig": [
                "print(os.listdir('../input/'))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path').fillna(0)",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "train = pd.read_csv('../input/web-traffic-time-series-forecasting/train_1.csv').fillna(0)\n",
                "train.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "def get_language(page):",
                "ASSIGN = re.search('[a-z][a-z].wikipedia.org', page)",
                "if ASSIGN:",
                "return ASSIGN[0][0:2]",
                "return 'na'",
                "ASSIGN = train.Page.map(get_language)",
                "print(Counter(train.lang))"
            ],
            "source_orig": [
                "def get_language(page):\n",
                "    res = re.search('[a-z][a-z].wikipedia.org', page)\n",
                "    if res:\n",
                "        return res[0][0:2]\n",
                "    return 'na'\n",
                "\n",
                "train['lang'] = train.Page.map(get_language)\n",
                "\n",
                "from collections import Counter\n",
                "\n",
                "print(Counter(train.lang))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = {}",
                "ASSIGN['en'] = train[train.lang=='en'].iloc[:,0:-1]",
                "ASSIGN['ja'] = train[train.lang=='ja'].iloc[:,0:-1]",
                "ASSIGN['de'] = train[train.lang=='de'].iloc[:,0:-1]",
                "ASSIGN['na'] = train[train.lang=='na'].iloc[:,0:-1]",
                "ASSIGN['fr'] = train[train.lang=='fr'].iloc[:,0:-1]",
                "ASSIGN['zh'] = train[train.lang=='zh'].iloc[:,0:-1]",
                "ASSIGN['ru'] = train[train.lang=='ru'].iloc[:,0:-1]",
                "ASSIGN['es'] = train[train.lang=='es'].iloc[:,0:-1]",
                "ASSIGN = {}",
                "for key in ASSIGN:",
                "ASSIGN = ASSIGN.iloc[:,1:].sum(axis=0) ASSIGN.ASSIGN"
            ],
            "source_orig": [
                "lang_sets = {}\n",
                "lang_sets['en'] = train[train.lang=='en'].iloc[:,0:-1]\n",
                "lang_sets['ja'] = train[train.lang=='ja'].iloc[:,0:-1]\n",
                "lang_sets['de'] = train[train.lang=='de'].iloc[:,0:-1]\n",
                "lang_sets['na'] = train[train.lang=='na'].iloc[:,0:-1]\n",
                "lang_sets['fr'] = train[train.lang=='fr'].iloc[:,0:-1]\n",
                "lang_sets['zh'] = train[train.lang=='zh'].iloc[:,0:-1]\n",
                "lang_sets['ru'] = train[train.lang=='ru'].iloc[:,0:-1]\n",
                "lang_sets['es'] = train[train.lang=='es'].iloc[:,0:-1]\n",
                "\n",
                "sums = {}\n",
                "for key in lang_sets:\n",
                "    sums[key] = lang_sets[key].iloc[:,1:].sum(axis=0) / lang_sets[key].shape[0]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = [r for r in range(sums['en'].shape[0])]",
                "ASSIGN = plt.figure(1, figsize=[10, 10])",
                "plt.ylabel('View Per Page')",
                "plt.xlabel('Day')",
                "plt.title('Pages in Different Languages')",
                "ASSIGN={'en':'English','ja':'Japanese','de':'German',",
                "'na':'Media','fr':'French','zh':'Chinese',",
                "'ru':'Russian','es':'Spanish'",
                "}",
                "for key in sums:",
                "plt.plot(ASSIGN,sums[key],label = ASSIGN[key] )",
                "plt.legend()",
                "plt.show()"
            ],
            "source_orig": [
                "days = [r for r in range(sums['en'].shape[0])]\n",
                "\n",
                "fig = plt.figure(1, figsize=[10, 10])\n",
                "plt.ylabel('View Per Page')\n",
                "plt.xlabel('Day')\n",
                "plt.title('Pages in Different Languages')\n",
                "labels={'en':'English','ja':'Japanese','de':'German',\n",
                "        'na':'Media','fr':'French','zh':'Chinese',\n",
                "        'ru':'Russian','es':'Spanish'\n",
                "       }\n",
                "\n",
                "for key in sums:\n",
                "    plt.plot(days,sums[key],label = labels[key] )\n",
                "    \n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print()",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN.iloc[:, 1:] = ASSIGN.iloc[:, 1:].fillna(method='ffill', axis=1).fillna(",
                "ASSIGN='bfill', axis=1)",
                "ASSIGN = pd.date_range(args.forecast_start, args.forecast_end)",
                "for datetime in ASSIGN:",
                "ASSIGN[datetime.date().isoformat()] = 0",
                "print()",
                "ASSIGN = pd.melt(full[list(",
                "ASSIGN.columns[args.offset+1:args.offset+args.val_len+1])+['Page']],",
                "ASSIGN='Page', var_name='Date', value_name=\"Visits\")",
                "ASSIGN = ASSIGN.astype('datetime64[ns]')",
                "ASSIGN = ASSIGN.dt.dayofweek >= 5",
                "print()",
                "ASSIGN = full.iloc[:, :args.offset+1]",
                "print()",
                "for i in args.windows:",
                "print(i, end=' ')",
                "ASSIGN = 'MW'+str(i)",
                "ASSIGN = pd.melt(train[list(train.columns[-i:])+['Page']],",
                "ASSIGN='Page', var_name='Date', value_name=val)",
                "ASSIGN = ASSIGN.astype('datetime64[ns]')",
                "ASSIGN= ASSIGN.dt.dayofweek >= 5",
                "ASSIGN = tmp.groupby(['Page', 'Weekend']).median().reset_index()",
                "ASSIGN = ASSIGN.merge(tmp1, how='left')",
                "print()",
                "print()",
                "ASSIGN = test[[\"MW7\", \"MW7\", \"MW14\", \"MW21\", \"MW35\", \"MW56\", \"MW91\",",
                "\"MW147\", \"MW238\", \"MW385\", \"MW623\"]].median(axis=1)"
            ],
            "source_orig": [
                "print(\"Getting data...\")\n",
                "full = pd.read_csv('../input/web-traffic-time-series-forecasting/train_2.csv')\n",
                "full.iloc[:, 1:] = full.iloc[:, 1:].fillna(method='ffill', axis=1).fillna(\n",
                "        method='bfill', axis=1)\n",
                "datetime_list = pd.date_range(args.forecast_start, args.forecast_end)\n",
                "for datetime in datetime_list:\n",
                "    full[datetime.date().isoformat()] = 0\n",
                "\n",
                "print(\"Constructing test set...\")\n",
                "test = pd.melt(full[list(\n",
                "    full.columns[args.offset+1:args.offset+args.val_len+1])+['Page']],\n",
                "    id_vars='Page', var_name='Date', value_name=\"Visits\")\n",
                "test['Date'] = test['Date'].astype('datetime64[ns]')\n",
                "test['Weekend'] = test['Date'].dt.dayofweek >= 5\n",
                "\n",
                "print(\"Constructing train set...\")\n",
                "train = full.iloc[:, :args.offset+1]\n",
                "\n",
                "print(\"Getting medians...\")\n",
                "for i in args.windows:\n",
                "    print(i, end=' ')\n",
                "    val = 'MW'+str(i)\n",
                "    tmp = pd.melt(train[list(train.columns[-i:])+['Page']],\n",
                "                  id_vars='Page', var_name='Date', value_name=val)\n",
                "    tmp['Date'] = tmp['Date'].astype('datetime64[ns]')\n",
                "    tmp['Weekend']= tmp['Date'].dt.dayofweek >= 5           \n",
                "    tmp1 = tmp.groupby(['Page', 'Weekend']).median().reset_index()\n",
                "    test = test.merge(tmp1, how='left')\n",
                "print(\"\\n\")\n",
                "\n",
                "print(\"Getting median of medians...\")\n",
                "test['Predict'] = test[[\"MW7\", \"MW7\", \"MW14\", \"MW21\", \"MW35\", \"MW56\", \"MW91\",\n",
                "    \"MW147\", \"MW238\", \"MW385\", \"MW623\"]].median(axis=1)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(os.listdir('..path'))"
            ],
            "source_orig": [
                "print(os.listdir('../working'))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import os\n",
                "import argparse\n",
                "import pickle\n",
                "import time\n",
                "\n",
                "import numpy as np; np.seterr(invalid='ignore')\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.autograd import Variable\n",
                "import torch.optim as optim\n",
                "from torch.optim.lr_scheduler import MultiStepLR\n",
                "from torch.utils.data import TensorDataset, DataLoader"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "class DenseLSTMForecast(nn.Module):",
                "def __init__(self, hidden_size):",
                "super(DenseLSTMForecast, self).__init__()",
                "self.lstm1 = nn.LSTMCell(1, hidden_size)",
                "self.lstm2 = nn.LSTMCell(hidden_size+1, hidden_size)",
                "self.lstm3 = nn.LSTMCell(2*hidden_size+1, hidden_size)",
                "self.linear = nn.Linear(3*hidden_size+1, 1)",
                "self.hidden_size = hidden_size",
                "def forward(self, x, future=0):",
                "ASSIGN = []",
                "ASSIGN = torch.cuda if args.cuda else torch",
                "ASSIGN = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())",
                "ASSIGN = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())",
                "ASSIGN = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())",
                "ASSIGN = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())",
                "ASSIGN = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())",
                "ASSIGN = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())",
                "for x_t in x.chunk(x.size(1), dim=1):",
                "ASSIGN = ASSIGN.squeeze(dim=1)",
                "ASSIGN, ASSIGN = self.lstm1(ASSIGN, (ASSIGN, ASSIGN))",
                "ASSIGN = torch.cat([x_t, h1_t], dim=1)",
                "ASSIGN, ASSIGN = self.lstm2(ASSIGN, (ASSIGN, ASSIGN))",
                "ASSIGN = torch.cat([x_t, h1_t, h2_t], dim=1)",
                "ASSIGN, ASSIGN = self.lstm3(ASSIGN, (ASSIGN, ASSIGN))",
                "ASSIGN = torch.cat([x_t, h1_t, h2_t, h3_t], dim=1)",
                "ASSIGN = self.linear(h3d_t)",
                "ASSIGN.append(ASSIGN)",
                "for i in range(future):",
                "ASSIGN, ASSIGN = self.lstm1(ASSIGN, (ASSIGN, ASSIGN))",
                "ASSIGN = torch.cat([o_t, h1_t], dim=1)",
                "ASSIGN, ASSIGN = self.lstm2(ASSIGN, (ASSIGN, ASSIGN))",
                "ASSIGN = torch.cat([o_t, h1_t, h2_t], dim=1)",
                "ASSIGN, ASSIGN = self.lstm3(ASSIGN, (ASSIGN, ASSIGN))",
                "ASSIGN = torch.cat([o_t, h1_t, h2_t, h3_t], dim=1)",
                "ASSIGN = self.linear(h3d_t)",
                "ASSIGN.append(ASSIGN)",
                "return torch.stack(ASSIGN, dim=1)"
            ],
            "source_orig": [
                "class DenseLSTMForecast(nn.Module):\n",
                "    def __init__(self, hidden_size):\n",
                "        super(DenseLSTMForecast, self).__init__()\n",
                "        self.lstm1 = nn.LSTMCell(1, hidden_size)\n",
                "        self.lstm2 = nn.LSTMCell(hidden_size+1, hidden_size)\n",
                "        self.lstm3 = nn.LSTMCell(2*hidden_size+1, hidden_size)\n",
                "        self.linear = nn.Linear(3*hidden_size+1, 1)\n",
                "        self.hidden_size = hidden_size\n",
                "\n",
                "    def forward(self, x, future=0):\n",
                "        o = []\n",
                "        tt = torch.cuda if args.cuda else torch\n",
                "        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
                "        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
                "        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
                "        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
                "        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
                "        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
                "        \n",
                "        for x_t in x.chunk(x.size(1), dim=1):\n",
                "            x_t = x_t.squeeze(dim=1)\n",
                "            h1_t, c1_t = self.lstm1(x_t, (h1_t, c1_t))\n",
                "            h1d_t = torch.cat([x_t, h1_t], dim=1)\n",
                "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
                "            h2d_t = torch.cat([x_t, h1_t, h2_t], dim=1)\n",
                "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
                "            h3d_t = torch.cat([x_t, h1_t, h2_t, h3_t], dim=1)\n",
                "            o_t = self.linear(h3d_t)\n",
                "            o.append(o_t)\n",
                "\n",
                "            \n",
                "        for i in range(future):\n",
                "            h1_t, c1_t = self.lstm1(o_t, (h1_t, c1_t))\n",
                "            h1d_t = torch.cat([o_t, h1_t], dim=1)\n",
                "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
                "            h2d_t = torch.cat([o_t, h1_t, h2_t], dim=1)\n",
                "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
                "            h3d_t = torch.cat([o_t, h1_t, h2_t, h3_t], dim=1)\n",
                "            o_t = self.linear(h3d_t)\n",
                "            o.append(o_t)\n",
                "\n",
                "        return torch.stack(o, dim=1)"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data",
                "transfer_results"
            ],
            "source": [
                "def get_data():",
                "ASSIGN = os.path.join(args.intermediate_path, 'raw_data.pkl')",
                "ASSIGN = os.path.join(args.intermediate_path,",
                "'scaled_data.pkl')",
                "ASSIGN = os.path.join(args.intermediate_path, 'scaler.pkl')",
                "if not args.read_from_file:",
                "ASSIGN = pd.read_csv(os.path.join(args.data_path, args.train_file),",
                "ASSIGN='Page')",
                "ASSIGN = data_df.values.copy()",
                "ASSIGN = ASSIGN.fillna(method='ffill', axis=1).fillna(",
                "ASSIGN='bfill', axis=1)",
                "ASSIGN = np.nan_to_num(data_df.values.astype('float32'))",
                "ASSIGN = np.log1p(ASSIGN)",
                "ASSIGN = StandardScaler()",
                "ASSIGN.fit(np.swapaxes(ASSIGN, 0, 1))",
                "ASSIGN = scaler.transform(np.swapaxes(data, 0, 1))",
                "ASSIGN = np.swapaxes(ASSIGN, 0, 1)",
                "with open(ASSIGN, 'wb') as f:",
                "pickle.dump(ASSIGN, f)",
                "with open(ASSIGN, 'wb') as f:",
                "pickle.dump(ASSIGN, f)",
                "with open(ASSIGN, 'wb') as f:",
                "pickle.dump(ASSIGN, f)",
                "else:",
                "with open(ASSIGN, 'rb') as f:",
                "ASSIGN = pickle.load(f)",
                "with open(ASSIGN, 'rb') as f:",
                "ASSIGN = pickle.load(f)",
                "with open(ASSIGN, 'rb') as f:",
                "ASSIGN = pickle.load(f)",
                "return raw_data, scaled_data, scaler"
            ],
            "source_orig": [
                "def get_data():\n",
                "    raw_data_file = os.path.join(args.intermediate_path, 'raw_data.pkl')\n",
                "    scaled_data_file = os.path.join(args.intermediate_path,\n",
                "                                    'scaled_data.pkl')\n",
                "    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n",
                "    \n",
                "    if not args.read_from_file:\n",
                "        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
                "                              index_col='Page')\n",
                "        raw_data = data_df.values.copy()\n",
                "        data_df = data_df.fillna(method='ffill', axis=1).fillna(\n",
                "            method='bfill', axis=1)\n",
                "        data = np.nan_to_num(data_df.values.astype('float32'))\n",
                "        data = np.log1p(data)\n",
                "        scaler = StandardScaler()\n",
                "        scaler.fit(np.swapaxes(data, 0, 1))\n",
                "        scaled_data = scaler.transform(np.swapaxes(data, 0, 1))\n",
                "        scaled_data = np.swapaxes(scaled_data, 0, 1)\n",
                "        \n",
                "        with open(raw_data_file, 'wb') as f:\n",
                "            pickle.dump(raw_data, f)\n",
                "        with open(scaled_data_file, 'wb') as f:\n",
                "            pickle.dump(scaled_data, f)\n",
                "        with open(scaler_file, 'wb') as f:\n",
                "            pickle.dump(scaler, f)\n",
                "    else:\n",
                "        with open(raw_data_file, 'rb') as f:\n",
                "            raw_data = pickle.load(f)\n",
                "        with open(scaled_data_file, 'rb') as f:\n",
                "            scaled_data = pickle.load(f)\n",
                "        with open(scaler_file, 'rb') as f:\n",
                "            scaler = pickle.load(f)\n",
                "    return raw_data, scaled_data, scaler"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "def save_model(model, epoch, loss):",
                "ASSIGN = os.path.join(args.intermediate_path,",
                "\"model_{}_epoch{}_loss{:.4f}.pth\"",
                ".format(args.seed, epoch, loss))",
                "torch.save(model.state_dict(), os.path.join(ASSIGN))"
            ],
            "source_orig": [
                "def save_model(model, epoch, loss):\n",
                "    model_file = os.path.join(args.intermediate_path,\n",
                "                              \"model_{}_epoch{}_loss{:.4f}.pth\"\n",
                "                              .format(args.seed, epoch, loss))\n",
                "    torch.save(model.state_dict(), os.path.join(model_file))\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = optim.SGD(model.parameters(), lr=0.001)",
                "ASSIGN = MultiStepLR(optimizer, milestones=[2, 4])"
            ],
            "source_orig": [
                "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
                "scheduler = MultiStepLR(optimizer, milestones=[2, 4])"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path', index_col='Page')",
                "ASSIGN = full.iloc[:, -args.test_len:].values"
            ],
            "source_orig": [
                "full = pd.read_csv('../input/web-traffic-time-series-forecasting/train_2.csv', index_col='Page')\n",
                "y_true = full.iloc[:, -args.test_len:].values"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame()",
                "ASSIGN = full.index",
                "ASSIGN = pd.date_range(args.forecast_start, args.forecast_end)",
                "for datetime in ASSIGN:",
                "ASSIGN[datetime.date().isoformat()] = 0",
                "ASSIGN.iloc[:, 1:] = np.around(prediction[:, 2:])"
            ],
            "source_orig": [
                "test = pd.DataFrame()\n",
                "test[\"Page\"] = full.index\n",
                "datetime_list = pd.date_range(args.forecast_start, args.forecast_end)\n",
                "for datetime in datetime_list:\n",
                "    test[datetime.date().isoformat()] = 0\n",
                "test.iloc[:, 1:] = np.around(prediction[:, 2:])"
            ]
        },
        {
            "tags": [
                "process_data",
                "transfer_results",
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.melt(ASSIGN, id_vars='Page', var_name='Date', value_name=\"Visits\")",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN['Date'] = ASSIGN['Page'].apply(lambda a: a[-10:])",
                "ASSIGN['Page'] = ASSIGN['Page'].apply(lambda a: a[:-11])",
                "ASSIGN = ASSIGN.merge(test, how=\"left\")",
                "ASSIGN[['Id', 'Visits']].to_csv(",
                "'..path{}path'.format(args.seed), index=False)"
            ],
            "source_orig": [
                "test = pd.melt(test, id_vars='Page', var_name='Date', value_name=\"Visits\")\n",
                "\n",
                "key_df = pd.read_csv('../input/web-traffic-time-series-forecasting/key_2.csv')\n",
                "key_df['Date'] = key_df['Page'].apply(lambda a: a[-10:])\n",
                "key_df['Page'] = key_df['Page'].apply(lambda a: a[:-11])\n",
                "key_df = key_df.merge(test, how=\"left\")\n",
                "\n",
                "key_df[['Id', 'Visits']].to_csv(\n",
                "    '../working/{}/submission.csv'.format(args.seed), index=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "key_df.head()"
            ],
            "source_orig": [
                "key_df.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "plt.rcParams['figure.figsize']=(20,10)",
                "py.init_notebook_mode(connected=False)",
                "print(os.listdir('..path'))"
            ],
            "source_orig": [
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import plotly.offline as py\n",
                "import plotly.graph_objs as go\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.metrics import roc_auc_score\n",
                " \n",
                "import cv2\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader, sampler, TensorDataset, random_split\n",
                "from torch.optim import lr_scheduler\n",
                "from torchvision import datasets, transforms, models\n",
                "from torch import utils\n",
                "\n",
                "import  time, copy, glob, torchvision, torch, os, json, re\n",
                "from collections import Counter, OrderedDict\n",
                "\n",
                "from PIL import Image\n",
                "from sklearn.metrics import classification_report\n",
                "\n",
                "\n",
                "plt.rcParams['figure.figsize']=(20,10)\n",
                "py.init_notebook_mode(connected=False)\n",
                "%matplotlib inline\n",
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "ASSIGN = torch.ASSIGN(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
            ],
            "source_orig": [
                "ROOT_DIR = '../input/virtual-hack/car_data/car_data'\n",
                "TRAIN_DIR = '../input/virtual-hack/car_data/car_data/train'\n",
                "TEST_DIR = '../input/virtual-hack/car_data/car_data/test'\n",
                "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
                "NUM_CLASSES = 196"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = transforms.Compose([",
                "transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),",
                "transforms.RandomRotation(degrees=15),",
                "transforms.RandomHorizontalFlip(),",
                "transforms.CenterCrop(size=224),",
                "transforms.ToTensor(),",
                "transforms.Normalize([0.485, 0.456, 0.406],",
                "[0.229, 0.224, 0.225])])",
                "ASSIGN = transforms.Compose([transforms.CenterCrop(size=224),",
                "transforms.ToTensor(),",
                "transforms.Normalize([0.485, 0.456, 0.406],",
                "[0.229, 0.224, 0.225])])",
                "ASSIGN = datasets.ImageFolder(TRAIN_DIR, train_transforms)",
                "ASSIGN = datasets.ImageFolder(TEST_DIR, valid_transforms)"
            ],
            "source_orig": [
                "train_transforms = transforms.Compose([\n",
                "    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
                "    transforms.RandomRotation(degrees=15),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.CenterCrop(size=224),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406],\n",
                "                         [0.229, 0.224, 0.225])])\n",
                "\n",
                "valid_transforms = transforms.Compose([transforms.CenterCrop(size=224),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406],\n",
                "                         [0.229, 0.224, 0.225])])\n",
                "\n",
                "dataset = datasets.ImageFolder(TRAIN_DIR, train_transforms)\n",
                "test_dataset = datasets.ImageFolder(TEST_DIR, valid_transforms)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = len(dataset)",
                "ASSIGN = 0.2",
                "ASSIGN = random_split(dataset, [size - int(size * val_split), int(size * val_split)])",
                "ASSIGN = DataLoader(trainset, batch_size=64, shuffle=True)",
                "ASSIGN = DataLoader(valset, batch_size=64, shuffle=True)",
                "ASSIGN = DataLoader(test_dataset, batch_size=64, shuffle=False)"
            ],
            "source_orig": [
                "size = len(dataset)\n",
                "val_split = 0.2\n",
                "trainset, valset = random_split(dataset, [size - int(size * val_split), int(size * val_split)])\n",
                "\n",
                "train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
                "val_loader = DataLoader(valset, batch_size=64, shuffle=True)\n",
                "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "def imshow(img):",
                "ASSIGN = transforms.Compose([",
                "transforms.Normalize(mean = [ 0., 0., 0. ],",
                "ASSIGN = [ 1path, 1path, 1path]),",
                "transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],",
                "ASSIGN = [ 1., 1., 1. ]),",
                "])",
                "ASSIGN = inverse_transform(img)",
                "ASSIGN = img.numpy()",
                "plt.imshow(np.transpose(ASSIGN, (1, 2, 0)))",
                "plt.show()",
                "def visualize():",
                "ASSIGN = iter(train_loader)",
                "ASSIGN = dataiter.next()",
                "ASSIGN = dataset.ASSIGN",
                "imshow(torchvision.utils.make_grid(images[:4]))",
                "for j in range(4):",
                "print(.format(labels[j].item(),",
                "ASSIGN[labels[j]]))",
                "visualize()"
            ],
            "source_orig": [
                "def imshow(img):\n",
                "    inverse_transform = transforms.Compose([\n",
                "      transforms.Normalize(mean = [ 0., 0., 0. ],\n",
                "                           std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
                "      transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
                "                           std = [ 1., 1., 1. ]),\n",
                "      ])\n",
                "\n",
                "    unnormalized_img = inverse_transform(img)\n",
                "    npimg = img.numpy()\n",
                "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
                "    plt.show()\n",
                "  \n",
                "\n",
                "def visualize(): \n",
                "    dataiter = iter(train_loader)\n",
                "    images, labels = dataiter.next()\n",
                "\n",
                "    classes = dataset.classes\n",
                "    imshow(torchvision.utils.make_grid(images[:4]))\n",
                "    for j in range(4):\n",
                "        print(\"label: {},  name: {}\".format(labels[j].item(),\n",
                "                                        classes[labels[j]]))\n",
                "visualize()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def create_model():",
                "ASSIGN = models.densenet121(pretrained=True)",
                "for param in ASSIGN.parameters():",
                "param.requires_grad = False",
                "ASSIGN = model.classifier.in_features",
                "ASSIGN.classifier = nn.Sequential(",
                "nn.Linear(ASSIGN, NUM_CLASSES),",
                "nn.LogSoftmax(dim=1))",
                "ASSIGN.to(device)",
                "return model"
            ],
            "source_orig": [
                "def create_model():\n",
                "    model = models.densenet121(pretrained=True)\n",
                "\n",
                "    for param in model.parameters():\n",
                "        param.requires_grad = False\n",
                "\n",
                "    n_inputs = model.classifier.in_features\n",
                "    model.classifier = nn.Sequential(\n",
                "                    nn.Linear(n_inputs, NUM_CLASSES),\n",
                "                    nn.LogSoftmax(dim=1))\n",
                "    # Move model to the device specified above\n",
                "\n",
                "    model.to(device)\n",
                "    return model"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = create_model()"
            ],
            "source_orig": [
                "model = create_model()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN=torch.FloatTensor(create_class_weights()).to(device)",
                "ASSIGN = nn.NLLLoss()",
                "ASSIGN = optim.Adam(model.parameters(), lr=0.001)",
                "ASSIGN = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
            ],
            "source_orig": [
                "weight=torch.FloatTensor(create_class_weights()).to(device)\n",
                "criterion = nn.NLLLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = {'train': len(trainset), 'valid': len(valset), 'test': len(test_dataset)}"
            ],
            "source_orig": [
                "dataset_sizes = {'train': len(trainset), 'valid': len(valset), 'test': len(test_dataset)}"
            ]
        },
        {
            "tags": [
                "train_model",
                "ingest_data",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "def train_model(dataloaders, model, criterion, optimizer, scheduler, num_epochs=5):",
                "ASSIGN = time.time()",
                "ASSIGN = copy.deepcopy(model.state_dict())",
                "ASSIGN = 0.0",
                "ASSIGN = {'train': [], 'valid':[]}",
                "ASSIGN = {'train': [], 'valid': []}",
                "for epoch in range(num_epochs):",
                "print('Epoch {}path{}'.format(epoch + 1, num_epochs))",
                "print('-' * 10)",
                "for phase in ['train', 'valid']:",
                "ASSIGN == 'train':",
                "scheduler.step()",
                "model.train()",
                "else:",
                "model.eval()",
                "ASSIGN = 0.0",
                "ASSIGN = 0",
                "for inputs, labels in dataloaders[phase]:",
                "ASSIGN = ASSIGN.to(device)",
                "ASSIGN = ASSIGN.to(device)",
                "optimizer.zero_grad()",
                "with torch.set_grad_enabled(phase == 'train'):",
                "ASSIGN = model(inputs)",
                "ASSIGN = torch.max(outputs, 1)",
                "ASSIGN = criterion(outputs, labels)",
                "ASSIGN == 'train':",
                "ASSIGN.backward()",
                "optimizer.step()",
                "ASSIGN += ASSIGN.item() * ASSIGN.size(0)",
                "ASSIGN += torch.sum(preds == ASSIGN.data)",
                "ASSIGN = running_loss path[phase]",
                "ASSIGN = running_corrects.double() path[phase]",
                "ASSIGN[phase].append(ASSIGN)",
                "ASSIGN[phase].append(ASSIGN)",
                "print('{} Loss: {:.4f} Acc: {:.4f}'.format(",
                "phase, ASSIGN, ASSIGN))",
                "ASSIGN == 'valid' and epoch_acc > best_acc:",
                "ASSIGN = epoch_acc",
                "ASSIGN = copy.deepcopy(model.state_dict())",
                "print()",
                "ASSIGN = time.time() - since",
                "print('Training complete in {:.0f}m {:.0f}s'.format(",
                "ASSIGN path, ASSIGN % 60))",
                "print('Best val Acc: {:4f}'.format(ASSIGN))",
                "model.load_state_dict(ASSIGN)",
                "return model"
            ],
            "source_orig": [
                "def train_model(dataloaders, model, criterion, optimizer, scheduler, num_epochs=5):\n",
                "    since = time.time()\n",
                "\n",
                "    best_model_wts = copy.deepcopy(model.state_dict())\n",
                "    best_acc = 0.0\n",
                "    losses = {'train': [], 'valid':[]}\n",
                "    acc = {'train': [], 'valid': []}\n",
                "  \n",
                "    for epoch in range(num_epochs):\n",
                "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
                "        print('-' * 10)\n",
                "\n",
                "    # Each epoch has a training and validation phase\n",
                "        for phase in ['train', 'valid']:\n",
                "            if phase == 'train':\n",
                "                scheduler.step()\n",
                "                model.train()  # Set model to training mode\n",
                "            else:\n",
                "                model.eval()   # Set model to evaluate mode\n",
                "\n",
                "            running_loss = 0.0\n",
                "            running_corrects = 0\n",
                "\n",
                "\n",
                "            for inputs, labels in dataloaders[phase]:\n",
                "                inputs = inputs.to(device)\n",
                "                labels = labels.to(device)\n",
                "\n",
                "                # zero the parameter gradients\n",
                "                optimizer.zero_grad()\n",
                "\n",
                "            # forward\n",
                "            # track history if only in train\n",
                "                with torch.set_grad_enabled(phase == 'train'):\n",
                "                    outputs = model(inputs)\n",
                "                    _, preds = torch.max(outputs, 1)\n",
                "                    loss = criterion(outputs, labels)\n",
                "\n",
                "                    # backward + optimize only if in training phase\n",
                "                    if phase == 'train':\n",
                "                        loss.backward()\n",
                "                        optimizer.step()\n",
                "\n",
                "                    running_loss += loss.item() * inputs.size(0)\n",
                "                    running_corrects += torch.sum(preds == labels.data)\n",
                "\n",
                "            epoch_loss = running_loss / dataset_sizes[phase]\n",
                "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
                "            losses[phase].append(epoch_loss)\n",
                "            acc[phase].append(epoch_acc)\n",
                "\n",
                "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
                "                      phase, epoch_loss, epoch_acc))\n",
                "\n",
                "            # deep copy the model\n",
                "            if phase == 'valid' and epoch_acc > best_acc:\n",
                "                best_acc = epoch_acc\n",
                "                best_model_wts = copy.deepcopy(model.state_dict())\n",
                "\n",
                "        print()\n",
                "\n",
                "    time_elapsed = time.time() - since\n",
                "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
                "      time_elapsed // 60, time_elapsed % 60))\n",
                "    print('Best val Acc: {:4f}'.format(best_acc))\n",
                "\n",
                "    # load best model weights\n",
                "    model.load_state_dict(best_model_wts)\n",
                "    return model"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "ingest_data"
            ],
            "source": [
                "def save(model, path):",
                "torch.save(model, path)",
                "def load(path):",
                "return torch.load(path)"
            ],
            "source_orig": [
                "def save(model, path):\n",
                "    torch.save(model, path)\n",
                "\n",
                "def load(path):\n",
                "    return torch.load(path)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = torch.zeros(NUM_CLASSES, NUM_CLASSES)",
                "with torch.no_grad():",
                "ASSIGN = 0",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for i, (inputs, labels) in enumerate(test_loader):",
                "ASSIGN = ASSIGN.to(device)",
                "ASSIGN = ASSIGN.to(device)",
                "ASSIGN = model(inputs)",
                "ASSIGN = torch.max(outputs, 1)",
                "ASSIGN = torch.sum(preds == labels.data)",
                "ASSIGN += ASSIGN",
                "ASSIGN.append(preds)",
                "ASSIGN.append(ASSIGN)",
                "for t, p in zip(ASSIGN.view(-1), preds.view(-1)):",
                "ASSIGN[t.long(), p.long()] += 1",
                "for dir in os.listdir(TEST_DIR):",
                "for file in os.listdir(os.path.join(TEST_DIR, dir)):",
                "ASSIGN = os.path.splitext(file)[0]",
                "ASSIGN.append(ASSIGN)",
                "print('Accuracy =====>>', ASSIGN.item()path(test_dataset))"
            ],
            "source_orig": [
                "confusion_matrix = torch.zeros(NUM_CLASSES, NUM_CLASSES)\n",
                "\n",
                "with torch.no_grad():\n",
                "    accuracy = 0\n",
                "    pred_labels = []\n",
                "    pred_img_ids = []\n",
                "    true_labels = []\n",
                "    for i, (inputs, labels) in enumerate(test_loader):\n",
                "        inputs = inputs.to(device)\n",
                "        labels = labels.to(device)\n",
                "        outputs = model(inputs)\n",
                "        _, preds = torch.max(outputs, 1)\n",
                "        running_acc = torch.sum(preds == labels.data)\n",
                "        accuracy += running_acc\n",
                "        pred_labels.append(preds)\n",
                "        true_labels.append(labels)\n",
                "        \n",
                "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
                "            confusion_matrix[t.long(), p.long()] += 1\n",
                "        \n",
                "    for dir in os.listdir(TEST_DIR):\n",
                "        for file in os.listdir(os.path.join(TEST_DIR, dir)):\n",
                "            img_id = os.path.splitext(file)[0]\n",
                "            pred_img_ids.append(img_id)\n",
                "            \n",
                "    print('Accuracy =====>>', accuracy.item()/len(test_dataset))"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "sns.set(font_scale=1.4)",
                "sns.heatmap(confusion_matrix[:10, :10], annot=True,annot_kws={\"size\": 16})# font size"
            ],
            "source_orig": [
                "sns.set(font_scale=1.4)#for label size\n",
                "sns.heatmap(confusion_matrix[:10, :10], annot=True,annot_kws={\"size\": 16})# font size"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = []",
                "for l in pred_labels:",
                "for l1 in l:",
                "ASSIGN.append(l1.item())",
                "ASSIGN = []",
                "for l in true_labels:",
                "for l1 in l:",
                "ASSIGN.append(l1.item())",
                "def multiclass_roc_auc_score(truth, pred, average=\"macro\"):",
                "ASSIGN = LabelBinarizer()",
                "ASSIGN.fit(truth)",
                "ASSIGN = lb.transform(ASSIGN)",
                "ASSIGN = lb.transform(ASSIGN)",
                "return roc_auc_score(ASSIGN, ASSIGN, average=average)",
                "print(, multiclass_roc_auc_score(ASSIGN, ASSIGN))"
            ],
            "source_orig": [
                "pred_labels_expanded = []\n",
                "for l in pred_labels:\n",
                "    for l1 in l:\n",
                "        pred_labels_expanded.append(l1.item())\n",
                "\n",
                "true_labels_expanded = []\n",
                "for l in true_labels:\n",
                "    for l1 in l:\n",
                "        true_labels_expanded.append(l1.item())\n",
                "\n",
                "from sklearn.metrics import roc_auc_score\n",
                "from sklearn.preprocessing import LabelBinarizer\n",
                "\n",
                "def multiclass_roc_auc_score(truth, pred, average=\"macro\"):\n",
                "\n",
                "    lb = LabelBinarizer()\n",
                "    lb.fit(truth)\n",
                "\n",
                "    truth = lb.transform(truth)\n",
                "    pred = lb.transform(pred)\n",
                "\n",
                "    return roc_auc_score(truth, pred, average=average)\n",
                "\n",
                "print(\"ROC AUC SCORE: =========>\", multiclass_roc_auc_score(true_labels_expanded, pred_labels_expanded))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "SETUP",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "import pandas as pd\n",
                "sub_sample = pd.read_csv('../input/virtual-hack/sampleSubmission.csv')\n",
                "sub_sample.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = {}",
                "for id, label in zip(pred_img_ids, pred_labels_expanded):",
                "ASSIGN[id] = label"
            ],
            "source_orig": [
                "id_label_dict = {}\n",
                "for id, label in zip(pred_img_ids, pred_labels_expanded):\n",
                "    id_label_dict[id] = label"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame()"
            ],
            "source_orig": [
                "my_submission = pd.DataFrame()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "my_submission['Id'] = od.keys()",
                "my_submission['Predicted'] = od.values()"
            ],
            "source_orig": [
                "my_submission['Id'] = od.keys()\n",
                "my_submission['Predicted'] = od.values()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "my_submission.head()"
            ],
            "source_orig": [
                "my_submission.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "my_submission.to_csv('my_submission.csv', index=False)"
            ],
            "source_orig": [
                "my_submission.to_csv('my_submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "print(os.listdir('..path'))"
            ],
            "source_orig": [
                "# General Imports\n",
                "import copy\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Pytorch Imports\n",
                "import torch\n",
                "from torch import nn\n",
                "import torch.nn.functional as F\n",
                "from torch.autograd import Variable\n",
                "import torchvision.transforms as transforms\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "\n",
                "print(os.listdir('../input/'))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "trainDF = pd.read_csv('../input/train.csv')\n",
                "trainDF.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = train_test_split(trainDF['review'], trainDF['sentiment'], test_size=0.1, shuffle=True)"
            ],
            "source_orig": [
                "train_x, val_x, train_y, val_y = train_test_split(trainDF['review'], trainDF['sentiment'], test_size=0.1, shuffle=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=25000)",
                "ASSIGN.fit(trainDF['review'])",
                "ASSIGN = tfidf_vect.transform(train_x)",
                "ASSIGN = tfidf_vect.transform(val_x)"
            ],
            "source_orig": [
                "# word level tf-idf\n",
                "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=25000)\n",
                "tfidf_vect.fit(trainDF['review'])\n",
                "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
                "xval_tfidf = tfidf_vect.transform(val_x)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.toarray()",
                "ASSIGN = ASSIGN.toarray()",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)"
            ],
            "source_orig": [
                "# Converting the Sparse matrix into a numpy array\n",
                "xtrain_tfidf = xtrain_tfidf.toarray()\n",
                "xval_tfidf = xval_tfidf.toarray()\n",
                "# Converting pandas Series into numpy array\n",
                "train_y = np.array(train_y)\n",
                "val_y = np.array(val_y)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = nn.Sequential(nn.Linear(input_dim, 2048),",
                "nn.Dropout(0.5),",
                "nn.ReLU(),",
                "nn.Linear(2048, 256),",
                "nn.Dropout(0.5),",
                "nn.ReLU(),",
                "nn.Linear(256, output_dim),",
                "nn.Sigmoid()).double()"
            ],
            "source_orig": [
                "model = nn.Sequential(nn.Linear(input_dim, 2048),\n",
                "                      nn.Dropout(0.5),\n",
                "                      nn.ReLU(),\n",
                "                      nn.Linear(2048, 256),\n",
                "                      nn.Dropout(0.5),\n",
                "                      nn.ReLU(),\n",
                "                      nn.Linear(256, output_dim),\n",
                "                      nn.Sigmoid()).double()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = torch.nn.BCEWithLogitsLoss()",
                "ASSIGN = torch.optim.Adam(model.parameters(), lr=lr_rate)"
            ],
            "source_orig": [
                "# Criterion and Optimizer\n",
                "criterion = torch.nn.BCEWithLogitsLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model",
                "ingest_data"
            ],
            "source": [
                "CHECKPOINT",
                "def train_model(model):",
                "ASSIGN = ASSIGN.cuda()",
                "ASSIGN = 0.0",
                "ASSIGN = copy.deepcopy(model.state_dict())",
                "for epoch in range(int(epochs)):",
                "ASSIGN = 0",
                "ASSIGN = 0",
                "ASSIGN = 0",
                "ASSIGN.train()",
                "for inputs, labels in train_dataloader:",
                "ASSIGN = inputs.cuda(), labels.cuda()",
                "optimizer.zero_grad()",
                "ASSIGN = model(inputs)",
                "ASSIGN = criterion(outputs.squeeze(), labels)",
                "ASSIGN.backward()",
                "optimizer.step()",
                "ASSIGN += ASSIGN.item()",
                "else:",
                "ASSIGN.eval()",
                "ASSIGN = 0",
                "for ASSIGN in val_dataloader:",
                "ASSIGN = inputs.cuda(), labels.cuda()",
                "ASSIGN = model(inputs)",
                "ASSIGN = torch.round(outputs.squeeze())",
                "ASSIGN = criterion(predictions, labels)",
                "ASSIGN += ASSIGN.item()",
                "ASSIGN = (predictions == labels.data)",
                "ASSIGN += torch.sum(ASSIGN.data).item()",
                "ASSIGN = num_correct path(val_dataset)",
                "if ASSIGN > ASSIGN:",
                "ASSIGN = val_acc",
                "ASSIGN = copy.deepcopy(model.state_dict())",
                "print('---------Epoch {} -----------'.format(epoch))",
                "print('Train Loss: {:.6f} Val Loss: {:.6f} Val Accuracy: {:.6f}'.format(",
                "train_losspath(train_dataset), val_losspath(val_dataset), ASSIGN))",
                "ASSIGN.load_state_dict(ASSIGN)",
                "return model"
            ],
            "source_orig": [
                "def train_model(model):\n",
                "    model = model.cuda()\n",
                "    best_acc = 0.0\n",
                "    best_model_wts = copy.deepcopy(model.state_dict())\n",
                "    \n",
                "    for epoch in range(int(epochs)):\n",
                "        train_loss = 0\n",
                "        val_loss = 0\n",
                "        val_acc = 0\n",
                "        model.train()\n",
                "        for inputs, labels in train_dataloader:\n",
                "            inputs, labels = inputs.cuda(), labels.cuda()\n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(inputs)\n",
                "            loss = criterion(outputs.squeeze(), labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            train_loss += loss.item()\n",
                "        else:\n",
                "            model.eval()\n",
                "            num_correct = 0\n",
                "            for inputs, labels in val_dataloader:\n",
                "                inputs, labels = inputs.cuda(), labels.cuda()\n",
                "                outputs = model(inputs)\n",
                "                predictions = torch.round(outputs.squeeze())\n",
                "                loss = criterion(predictions, labels)\n",
                "                \n",
                "                val_loss += loss.item()\n",
                "                equals = (predictions == labels.data)\n",
                "    \n",
                "                num_correct += torch.sum(equals.data).item()\n",
                "            \n",
                "            val_acc = num_correct / len(val_dataset)\n",
                "            if val_acc > best_acc:\n",
                "                best_acc = val_acc\n",
                "                best_model_wts = copy.deepcopy(model.state_dict())\n",
                "        print('---------Epoch {} -----------'.format(epoch))\n",
                "        print('Train Loss: {:.6f} Val Loss: {:.6f} Val Accuracy: {:.6f}'.format(\n",
                "                 train_loss/len(train_dataset), val_loss/len(val_dataset), val_acc))\n",
                "        \n",
                "    model.load_state_dict(best_model_wts)\n",
                "    return model"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = tfidf_vect.transform(test_df['review'])",
                "ASSIGN = ASSIGN.toarray()"
            ],
            "source_orig": [
                "test_df = pd.read_csv('../input/test.csv')\n",
                "xtest_tfidf =  tfidf_vect.transform(test_df['review'])\n",
                "xtest_tfidf = xtest_tfidf.toarray()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = np.zeros(xtest_tfidf.shape[0])"
            ],
            "source_orig": [
                "test_y = np.zeros(xtest_tfidf.shape[0])"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "def predict(model, test_dataloader):",
                "model.eval()",
                "ASSIGN = []",
                "for inputs, _ in test_dataloader:",
                "ASSIGN = ASSIGN.cuda()",
                "ASSIGN = model(inputs)",
                "ASSIGN = torch.round(output)",
                "ASSIGN.extend([p.item() for p in ASSIGN])",
                "return predictions",
                "ASSIGN = predict(model, test_dataloader)"
            ],
            "source_orig": [
                "def predict(model, test_dataloader):\n",
                "    model.eval()\n",
                "    predictions = []\n",
                "    for inputs, _ in test_dataloader:\n",
                "        inputs = inputs.cuda()\n",
                "        output = model(inputs)\n",
                "        preds = torch.round(output)\n",
                "        predictions.extend([p.item() for p in preds])\n",
                "    return predictions\n",
                "\n",
                "predictions = predict(model, test_dataloader)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame()",
                "ASSIGN['Id'] = test_df['Id']",
                "ASSIGN['sentiment'] = [int(p) for p in predictions]"
            ],
            "source_orig": [
                "sub_df = pd.DataFrame()\n",
                "sub_df['Id'] = test_df['Id']\n",
                "sub_df['sentiment'] = [int(p) for p in predictions]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "sub_df.head()"
            ],
            "source_orig": [
                "sub_df.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "sub_df.to_csv('my_submission.csv', index=False)"
            ],
            "source_orig": [
                "sub_df.to_csv('my_submission.csv', index=False)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "print(os.listdir('..path'))"
            ],
            "source_orig": [
                "import torch\n",
                "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
                "from keras.preprocessing.sequence import pad_sequences\n",
                "from sklearn.model_selection import train_test_split\n",
                "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
                "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
                "from tqdm import tqdm, trange\n",
                "import pandas as pd\n",
                "import io\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "import torch\n",
                "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
                "import os\n",
                "print(os.listdir('../input/'))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "ASSIGN = torch.ASSIGN(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
                "ASSIGN = torch.cuda.device_count()",
                "torch.cuda.get_device_name(0)"
            ],
            "source_orig": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "n_gpu = torch.cuda.device_count()\n",
                "torch.cuda.get_device_name(0)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')"
            ],
            "source_orig": [
                "train_df = pd.read_csv('../input/nlp-hack/train.csv')\n",
                "test_df = pd.read_csv('../input/nlp-hack/test.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
            ],
            "source_orig": [
                "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "for param in model.bert.parameters():",
                "param.requires_grad = False",
                "for name, param in model.named_parameters():",
                "if param.requires_grad:",
                "print(name)"
            ],
            "source_orig": [
                "for param in model.bert.parameters():\n",
                "  param.requires_grad = False\n",
                "\n",
                "for name, param in model.named_parameters():                \n",
                "    if param.requires_grad:\n",
                "        print(name)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = test_df.text.values",
                "ASSIGN = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in ASSIGN]",
                "ASSIGN = np.random.rand(len(sentences))"
            ],
            "source_orig": [
                "sentences = test_df.text.values\n",
                "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
                "labels = np.random.rand(len(sentences))"
            ]
        },
        {
            "tags": [
                "process_data",
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "ASSIGN = [tokenizer.tokenize(sent) for sent in sentences]",
                "ASSIGN = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],",
                "ASSIGN=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")",
                "ASSIGN = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]",
                "ASSIGN = pad_sequences(ASSIGN, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")",
                "ASSIGN = []",
                "for seq in ASSIGN:",
                "ASSIGN = [float(i>0) for i in seq]",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = torch.tensor(input_ids)",
                "ASSIGN = torch.tensor(attention_masks)",
                "ASSIGN = torch.tensor(labels)",
                "ASSIGN = 32",
                "ASSIGN = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)",
                "ASSIGN = SequentialSampler(prediction_data)",
                "ASSIGN = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
            ],
            "source_orig": [
                "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
                "MAX_LEN = 128\n",
                "# Pad our input tokens\n",
                "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
                "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
                "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
                "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
                "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
                "# Create attention masks\n",
                "attention_masks = []\n",
                "\n",
                "# Create a mask of 1s for each token followed by 0s for padding\n",
                "for seq in input_ids:\n",
                "  seq_mask = [float(i>0) for i in seq]\n",
                "  attention_masks.append(seq_mask) \n",
                "\n",
                "prediction_inputs = torch.tensor(input_ids)\n",
                "prediction_masks = torch.tensor(attention_masks)\n",
                "prediction_labels = torch.tensor(labels)\n",
                "  \n",
                "batch_size = 32  \n",
                "\n",
                "\n",
                "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
                "prediction_sampler = SequentialSampler(prediction_data)\n",
                "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "process_data"
            ],
            "source": [
                "model.eval()",
                "ASSIGN = [], []",
                "for batch in prediction_dataloader:",
                "ASSIGN = tuple(t.to(device) for t in ASSIGN)",
                "ASSIGN = batch",
                "with torch.no_grad():",
                "ASSIGN = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)",
                "ASSIGN = ASSIGN.detach().cpu().numpy()",
                "ASSIGN = b_labels.to('cpu').numpy()",
                "predictions.append(ASSIGN)",
                "true_labels.append(ASSIGN)"
            ],
            "source_orig": [
                "model.eval()\n",
                "\n",
                "# Tracking variables \n",
                "predictions , true_labels = [], []\n",
                "\n",
                "# Predict \n",
                "for batch in prediction_dataloader:\n",
                "  # Add batch to GPU\n",
                "  batch = tuple(t.to(device) for t in batch)\n",
                "  # Unpack the inputs from our dataloader\n",
                "  b_input_ids, b_input_mask, b_labels = batch\n",
                "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
                "  with torch.no_grad():\n",
                "    # Forward pass, calculate logit predictions\n",
                "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
                "\n",
                "  # Move logits and labels to CPU\n",
                "  logits = logits.detach().cpu().numpy()\n",
                "  label_ids = b_labels.to('cpu').numpy()\n",
                "  \n",
                "  # Store predictions and true labels\n",
                "  predictions.append(logits)\n",
                "  true_labels.append(label_ids)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame()",
                "ASSIGN['Id'] = test_df['Id']"
            ],
            "source_orig": [
                "my_submission = pd.DataFrame()\n",
                "my_submission['Id'] = test_df['Id']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "my_submission['target'] = final_preds"
            ],
            "source_orig": [
                "my_submission['target'] = final_preds"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "my_submission['target'] = my_submission['target'].map({0:0, 1:4})"
            ],
            "source_orig": [
                "my_submission['target'] = my_submission['target'].map({0:0, 1:4})"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "my_submission.head()"
            ],
            "source_orig": [
                "my_submission.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "my_submission.to_csv('my_sub.csv', index=False)"
            ],
            "source_orig": [
                "my_submission.to_csv('my_sub.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "warnings.filterwarnings('ignore')",
                "sns.set()",
                "print(os.listdir('..path'))"
            ],
            "source_orig": [
                "from sklearn import model_selection, preprocessing, linear_model, metrics\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
                "from sklearn import decomposition, ensemble\n",
                "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Matplotlib forms basis for visualization in Python\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.ticker as ticker\n",
                "\n",
                "# We will use the Seaborn library\n",
                "import seaborn as sns\n",
                "sns.set()\n",
                "\n",
                "import os, string\n",
                "# Graphics in SVG format are more sharp and legible\n",
                "%config InlineBackend.figure_format = 'svg'\n",
                "print(os.listdir('../input/'))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "df = pd.read_csv('../input/train.csv')\n",
                "df.head()"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv('..path')"
            ],
            "source_orig": [
                "test_df = pd.read_csv('../input/test.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df['char_count'] = df['review'].apply(len)",
                "df['word_count'] = df['review'].apply(lambda x: len(x.split()))",
                "df['word_density'] = df['char_count'] path(df['word_count']+1)",
                "df['punctuation_count'] = df['review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))"
            ],
            "source_orig": [
                "df['char_count'] = df['review'].apply(len)\n",
                "df['word_count'] = df['review'].apply(lambda x: len(x.split()))\n",
                "df['word_density'] = df['char_count'] / (df['word_count']+1)\n",
                "df['punctuation_count'] = df['review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ],
            "source_orig": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = ['char_count', 'word_count', 'word_density', 'punctuation_count']",
                "ASSIGN = plt.subplots(nrows=2, ncols=2, figsize=(10, 6), sharey=True)",
                "for i, feature in enumerate(ASSIGN):",
                "df.hist(column=feature, ax=axes.flatten()[i])"
            ],
            "source_orig": [
                "#df.hist(column=['char_count', 'word_count'])\n",
                "features = ['char_count', 'word_count', 'word_density', 'punctuation_count']\n",
                "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 6), sharey=True)\n",
                "\n",
                "for i, feature in enumerate(features):\n",
                "    df.hist(column=feature, ax=axes.flatten()[i])\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df[features].describe()"
            ],
            "source_orig": [
                "df[features].describe()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = plt.subplots()",
                "sns.boxplot(df['word_count'], order=range(0,max(df['word_count'])), ax=ax)",
                "ax.xaxis.set_major_locator(ticker.MultipleLocator(200))",
                "ax.xaxis.set_major_formatter(ticker.ScalarFormatter())",
                "fig.set_size_inches(8, 4)",
                "plt.show()"
            ],
            "source_orig": [
                "fig, ax = plt.subplots()\n",
                "sns.boxplot(df['word_count'], order=range(0,max(df['word_count'])), ax=ax)\n",
                "\n",
                "ax.xaxis.set_major_locator(ticker.MultipleLocator(200))\n",
                "ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
                "fig.set_size_inches(8, 4)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "SETUP",
                "X_train, X_val, y_train, y_val = train_test_split(df['review'], df['sentiment'], test_size=0.3,",
                "ASSIGN=17)"
            ],
            "source_orig": [
                "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "X_train, X_val, y_train, y_val = train_test_split(df['review'], df['sentiment'], test_size=0.3,\n",
                "random_state=17)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)",
                "ASSIGN.fit(df['review'])",
                "ASSIGN = tfidf_vect.transform(X_train)",
                "ASSIGN = tfidf_vect.transform(X_val)",
                "ASSIGN = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)",
                "ASSIGN.fit(df['review'])",
                "ASSIGN = tfidf_vect_ngram.transform(X_train)",
                "ASSIGN = tfidf_vect_ngram.transform(X_val)",
                "ASSIGN = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)",
                "ASSIGN.fit(df['review'])",
                "ASSIGN = tfidf_vect_ngram_chars.transform(X_train)",
                "ASSIGN = tfidf_vect_ngram_chars.transform(X_val)",
                "ASSIGN = tfidf_vect.transform(test_df['review'])",
                "ASSIGN = tfidf_vect_ngram.transform(test_df['review'])",
                "ASSIGN = tfidf_vect_ngram_chars.transform(test_df['review'])"
            ],
            "source_orig": [
                "# Converting X_train and X_val to tfidf vectors (since out models can't take text data is input)\n",
                "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
                "tfidf_vect.fit(df['review'])\n",
                "xtrain_tfidf =  tfidf_vect.transform(X_train)\n",
                "xvalid_tfidf =  tfidf_vect.transform(X_val)\n",
                "\n",
                "# ngram level tf-idf \n",
                "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
                "tfidf_vect_ngram.fit(df['review'])\n",
                "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
                "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_val)\n",
                "\n",
                "# characters level tf-idf\n",
                "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
                "tfidf_vect_ngram_chars.fit(df['review'])\n",
                "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
                "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_val) \n",
                "\n",
                "# Also creating for the X_test which is essentially test_df['review'] column\n",
                "xtest_tfidf =  tfidf_vect.transform(test_df['review'])\n",
                "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_df['review'])\n",
                "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_df['review']) "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')",
                "ASSIGN.fit(df['review'])",
                "ASSIGN = count_vect.transform(X_train)",
                "ASSIGN = count_vect.transform(X_val)",
                "ASSIGN = count_vect.transform(test_df['review'])"
            ],
            "source_orig": [
                "# create a count vectorizer object \n",
                "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
                "count_vect.fit(df['review'])\n",
                "\n",
                "# transform the training and validation data using count vectorizer object\n",
                "xtrain_count =  count_vect.transform(X_train)\n",
                "xvalid_count =  count_vect.transform(X_val)\n",
                "xtest_count = count_vect.transform(test_df['review'])"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = linear_model.LogisticRegression()",
                "ASSIGN.fit(xtrain_count, y_train)",
                "ASSIGN=model1.score(xvalid_count, y_val)",
                "print('Accuracy Count LR:', ASSIGN)",
                "ASSIGN=model1.predict(xtest_count)",
                "ASSIGN = linear_model.LogisticRegression()",
                "ASSIGN.fit(xtrain_tfidf, y_train)",
                "ASSIGN=model2.score(xvalid_tfidf, y_val)",
                "print('Accuracy TFIDF LR:', ASSIGN)",
                "ASSIGN=model2.predict(xtest_tfidf)",
                "ASSIGN = linear_model.LogisticRegression()",
                "ASSIGN.fit(xtrain_tfidf_ngram, y_train)",
                "ASSIGN = model3.score(xvalid_tfidf_ngram, y_val)",
                "print('Accuracy TFIDF NGRAM LR:', ASSIGN)",
                "ASSIGN = model3.predict(xtest_tfidf_ngram)"
            ],
            "source_orig": [
                "model1 = linear_model.LogisticRegression()\n",
                "model1.fit(xtrain_count, y_train)\n",
                "accuracy=model1.score(xvalid_count, y_val)\n",
                "print('Accuracy Count LR:', accuracy)\n",
                "test_pred1=model1.predict(xtest_count)\n",
                "\n",
                "model2 = linear_model.LogisticRegression()\n",
                "model2.fit(xtrain_tfidf, y_train)\n",
                "accuracy=model2.score(xvalid_tfidf, y_val)\n",
                "print('Accuracy TFIDF LR:', accuracy)\n",
                "test_pred2=model2.predict(xtest_tfidf)\n",
                "\n",
                "model3 = linear_model.LogisticRegression()\n",
                "model3.fit(xtrain_tfidf_ngram, y_train)\n",
                "accuracy = model3.score(xvalid_tfidf_ngram, y_val)\n",
                "print('Accuracy TFIDF NGRAM LR:', accuracy)\n",
                "test_pred3 = model3.predict(xtest_tfidf_ngram)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame()",
                "ASSIGN['Id'] = test_df['Id']",
                "ASSIGN['sentiment'] = [int(i) for i in final_pred]"
            ],
            "source_orig": [
                "sub_df = pd.DataFrame()\n",
                "sub_df['Id'] = test_df['Id']\n",
                "sub_df['sentiment'] = [int(i) for i in final_pred]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "sub_df.head()"
            ],
            "source_orig": [
                "sub_df.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "sub_df.to_csv('my_submission.csv', index=False)"
            ],
            "source_orig": [
                "sub_df.to_csv('my_submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import os\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import keras\n",
                "import cv2\n",
                "from keras.datasets import fashion_mnist#download mnist data and split into train and test sets\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.utils import shuffle\n",
                "import matplotlib.pyplot as plt\n",
                "from keras.utils import to_categorical\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout#create model\n",
                "from keras.optimizers import SGD\n",
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "\n",
                "from keras.preprocessing.image import load_img\n",
                "from keras.preprocessing.image import img_to_array\n",
                "from keras.applications.vgg16 import VGG16\n",
                "\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "CAT_TRAIN_PATH=\"/kaggle/input/cat-and-dog/training_set/training_set/cats/\"\n",
                "DOG_TRAIN_PATH=\"/kaggle/input/cat-and-dog/training_set/training_set/dogs/\"\n",
                "CAT_TEST_PATH=\"/kaggle/input/cat-and-dog/test_set/test_set/cats/\"\n",
                "DOG_TEST_PATH=\"/kaggle/input/cat-and-dog/test_set/test_set/dogs/\""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def preprocess_img(img):",
                "ASSIGN=(100,100)",
                "ASSIGN = cv2.resize(img, dim, interpolation=cv2.INTER_LINEAR)",
                "return res"
            ],
            "source_orig": [
                "def preprocess_img(img):\n",
                "    dim=(100,100)\n",
                "    res = cv2.resize(img, dim, interpolation=cv2.INTER_LINEAR)\n",
                "    return res\n",
                "    "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(np.array(lst).shape)",
                "print(np.array(y).shape)"
            ],
            "source_orig": [
                "\n",
                "print(np.array(lst).shape)\n",
                "print(np.array(y).shape)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data",
                "transfer_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN=np.array(lst)",
                "ASSIGN=np.array(y)",
                "np.savez(\"CAT_DOG_X_train\",ASSIGN)",
                "np.savez(\"CAT_DOG_Y_train\",ASSIGN)",
                "print(ASSIGN.shape)",
                "print(ASSIGN.shape)"
            ],
            "source_orig": [
                "X_train=np.array(lst)\n",
                "Y_train=np.array(y)\n",
                "np.savez(\"CAT_DOG_X_train\",X_train)\n",
                "np.savez(\"CAT_DOG_Y_train\",Y_train)\n",
                "print(X_train.shape)\n",
                "print(Y_train.shape)\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results",
                "transfer_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN=np.array(lst)",
                "ASSIGN=np.array(y)",
                "print(ASSIGN.shape)",
                "print(ASSIGN.shape)",
                "np.save(\"CAT_DOG_X_test\",ASSIGN)",
                "np.save(\"CAT_DOG_Y_test\",ASSIGN)"
            ],
            "source_orig": [
                "X_test=np.array(lst)\n",
                "Y_test=np.array(y)\n",
                "print(X_test.shape)\n",
                "print(Y_test.shape)\n",
                "np.save(\"CAT_DOG_X_test\",X_test)\n",
                "np.save(\"CAT_DOG_Y_test\",Y_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "X_TRAIN_FILE=\"path\"",
                "X_TEST_FILE=\"path\"",
                "Y_TRAIN_FILE=\"path\"",
                "Y_TEST_FILE=\"path\""
            ],
            "source_orig": [
                "X_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_train.npz\"\n",
                "X_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_test.npy\"\n",
                "Y_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_train.npz\"\n",
                "Y_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_test.npy\""
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(X_train.shape)",
                "print(Y_train.shape)",
                "print(X_test.shape)",
                "print(Y_test.shape)"
            ],
            "source_orig": [
                "print(X_train.shape)\n",
                "print(Y_train.shape)\n",
                "print(X_test.shape)\n",
                "print(Y_test.shape)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print()",
                "print(+str(sum(Y_train==1))++str(sum(Y_train==0))+)",
                "print(+str(sum(Y_test==1))++str(sum(Y_test==0))+)"
            ],
            "source_orig": [
                "print(\"Distribution of cats and dogs in the different sets\")\n",
                "print(\"TRAIN  :  \"+str(sum(Y_train==1))+\" cats vs \"+str(sum(Y_train==0))+\" dogs\")\n",
                "print(\"TEST  :  \"+str(sum(Y_test==1))+\" cats vs \"+str(sum(Y_test==0))+\" dogs\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)"
            ],
            "source_orig": [
                "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(X_train.shape)",
                "print(Y_train.shape)",
                "print(X_val.shape)",
                "print(Y_val.shape)",
                "print(X_test.shape)",
                "print(Y_test.shape)"
            ],
            "source_orig": [
                "print(X_train.shape)\n",
                "print(Y_train.shape)\n",
                "print(X_val.shape)\n",
                "print(Y_val.shape)\n",
                "print(X_test.shape)\n",
                "print(Y_test.shape)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print()",
                "print(+str(sum(Y_train==1))++str(sum(Y_train==0))+)",
                "print(+str(sum(Y_val==1))++str(sum(Y_val==0))+)",
                "print(+str(sum(Y_test==1))++str(sum(Y_test==0))+)"
            ],
            "source_orig": [
                "print(\"Distribution of cats and dogs in the different sets\")\n",
                "print(\"TRAIN  :  \"+str(sum(Y_train==1))+\" cats vs \"+str(sum(Y_train==0))+\" dogs\")\n",
                "print(\"VAL  :  \"+str(sum(Y_val==1))+\" cats vs \"+str(sum(Y_val==0))+\" dogs\")\n",
                "print(\"TEST  :  \"+str(sum(Y_test==1))+\" cats vs \"+str(sum(Y_test==0))+\" dogs\")"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print()",
                "for i in range(0,5):",
                "plt.imshow(X_train[i])",
                "print(Y_train[i])",
                "plt.show()"
            ],
            "source_orig": [
                "\n",
                "print(\"Images 1 to 5 :\")\n",
                "for i in range(0,5):\n",
                "    plt.imshow(X_train[i])\n",
                "    print(Y_train[i])\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test):",
                "ASSIGN=normalize_X(ASSIGN)",
                "ASSIGN=normalize_X(ASSIGN)",
                "ASSIGN=normalize_X(ASSIGN)",
                "ASSIGN = shuffle(ASSIGN, random_state=42)",
                "ASSIGN = shuffle(ASSIGN, random_state=42)",
                "ASSIGN = shuffle(ASSIGN, random_state=42)",
                "return X_train,X_val,X_test,y_train,y_val,y_test"
            ],
            "source_orig": [
                "def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test):\n",
                "    X_train=normalize_X(X_train)\n",
                "    X_val=normalize_X(X_val)\n",
                "    X_test=normalize_X(X_test)\n",
                "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
                "    X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
                "    X_test, y_test = shuffle(X_test, y_test, random_state=42)\n",
                "    return X_train,X_val,X_test,y_train,y_val,y_test"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)"
            ],
            "source_orig": [
                "X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "def input_and_run(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10):",
                "ASSIGN = keras.optimizers.Adam(learning_rate=alpha)",
                "ASSIGN=SGD(lr=alpha, momentum=0.9)",
                "model.compile(loss='binary_crossentropy', optimizer=ASSIGN,metrics=['accuracy'])",
                "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=num_epochs)",
                "ASSIGN = model.evaluate(X_train,y_train)",
                "print(+str(ASSIGN[1]*100))",
                "ASSIGN = model.evaluate(X_val,y_val)",
                "print(+str(ASSIGN[1]*100))",
                "ASSIGN = model.evaluate(X_test,y_test)",
                "print(+str(ASSIGN[1]*100))"
            ],
            "source_orig": [
                "\n",
                "\n",
                "def input_and_run(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10):\n",
                "    \n",
                "    #compile model using accuracy to measure model performance\n",
                "    opt = keras.optimizers.Adam(learning_rate=alpha)\n",
                "    opt2=SGD(lr=alpha, momentum=0.9)\n",
                "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
                "    \n",
                "    #train the model\n",
                "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=num_epochs)\n",
                "    \n",
                "    #Getting results\n",
                "    result = model.evaluate(X_train,y_train)\n",
                "    #print(result)\n",
                "    print(\"Training accuracy = \"+str(result[1]*100))\n",
                "    result = model.evaluate(X_val,y_val)\n",
                "    #print(result)\n",
                "    print(\"Validation accuracy = \"+str(result[1]*100))\n",
                "    result = model.evaluate(X_test,y_test)\n",
                "    #print(result)\n",
                "    print(\"Test accuracy = \"+str(result[1]*100))\n",
                "\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)))",
                "ASSIGN.add(MaxPooling2D((2, 2)))",
                "ASSIGN.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))",
                "ASSIGN.add(MaxPooling2D((2, 2)))",
                "ASSIGN.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))",
                "ASSIGN.add(MaxPooling2D((2, 2)))",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))",
                "ASSIGN.add(Dense(1, activation='sigmoid'))",
                "print(ASSIGN.summary())",
                "input_and_run(ASSIGN,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.0001,num_epochs=20)"
            ],
            "source_orig": [
                "##BUILDING THE MODEL 1\n",
                "\n",
                "model1 = Sequential()#add model layers\n",
                "\n",
                "model1.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)))\n",
                "model1.add(MaxPooling2D((2, 2)))\n",
                "model1.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model1.add(MaxPooling2D((2, 2)))\n",
                "model1.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model1.add(MaxPooling2D((2, 2)))\n",
                "model1.add(Flatten())\n",
                "model1.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
                "model1.add(Dense(1, activation='sigmoid'))\n",
                "\n",
                "print(model1.summary())\n",
                "input_and_run(model1,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.0001,num_epochs=20)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)))",
                "ASSIGN.add(MaxPooling2D((2, 2)))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))",
                "ASSIGN.add(MaxPooling2D((2, 2)))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))",
                "ASSIGN.add(MaxPooling2D((2, 2)))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))",
                "ASSIGN.add(MaxPooling2D((2, 2)))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))",
                "ASSIGN.add(Dropout(0.5))",
                "ASSIGN.add(Dense(1, activation='sigmoid'))",
                "print(ASSIGN.summary())",
                "input_and_run(ASSIGN,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.002,num_epochs=200)"
            ],
            "source_orig": [
                "##BUILDING THE MODEL 1\n",
                "\n",
                "model2 = Sequential()#add model layers\n",
                "\n",
                "model2.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)))\n",
                "model2.add(MaxPooling2D((2, 2)))\n",
                "model2.add(Dropout(0.2))\n",
                "model2.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model2.add(MaxPooling2D((2, 2)))\n",
                "model2.add(Dropout(0.2))\n",
                "model2.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model2.add(MaxPooling2D((2, 2)))\n",
                "model2.add(Dropout(0.2))\n",
                "model2.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model2.add(MaxPooling2D((2, 2)))\n",
                "model2.add(Dropout(0.2))\n",
                "model2.add(Flatten())\n",
                "model2.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
                "model2.add(Dropout(0.5))\n",
                "model2.add(Dense(1, activation='sigmoid'))\n",
                "\n",
                "print(model2.summary())\n",
                "input_and_run(model2,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.002,num_epochs=200)"
            ]
        },
        {
            "tags": [
                "check_results",
                "transfer_results"
            ],
            "source": [
                "CHECKPOINT",
                "model2.save(\"model2.h5\")",
                "print()"
            ],
            "source_orig": [
                "model2.save(\"model2.h5\")\n",
                "print(\"Saved model to disk\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "WEIGHTS_FILE=\"/kaggle/input/cat-dog-numpy/model2.h5\""
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN=load_model(WEIGHTS_FILE)",
                "print()"
            ],
            "source_orig": [
                "from keras.models import load_model\n",
                "# load model\n",
                "loaded_model=load_model(WEIGHTS_FILE)\n",
                "print(\"Loaded model from disk\")"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = keras.optimizers.Adam(learning_rate=0.002)",
                "loaded_model.compile(loss='binary_crossentropy', optimizer=ASSIGN,metrics=['accuracy'])",
                "ASSIGN = loaded_model.evaluate(X_train,Y_train)",
                "print(+str(ASSIGN[1]*100))",
                "ASSIGN = loaded_model.evaluate(X_val,Y_val)",
                "print(+str(ASSIGN[1]*100))",
                "ASSIGN = loaded_model.evaluate(X_test,Y_test)",
                "print(+str(ASSIGN[1]*100))"
            ],
            "source_orig": [
                "#Getting results\n",
                "opt = keras.optimizers.Adam(learning_rate=0.002)\n",
                "loaded_model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
                "result = loaded_model.evaluate(X_train,Y_train)\n",
                "#print(result)\n",
                "print(\"Training accuracy = \"+str(result[1]*100))\n",
                "result = loaded_model.evaluate(X_val,Y_val)\n",
                "#print(result)\n",
                "print(\"Validation accuracy = \"+str(result[1]*100))\n",
                "result = loaded_model.evaluate(X_test,Y_test)\n",
                "#print(result)\n",
                "print(\"Test accuracy = \"+str(result[1]*100))\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "X_TRAIN_FILE=\"path\"",
                "X_TEST_FILE=\"path\"",
                "Y_TRAIN_FILE=\"path\"",
                "Y_TEST_FILE=\"path\""
            ],
            "source_orig": [
                "X_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_train.npz\"\n",
                "X_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_test.npy\"\n",
                "Y_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_train.npz\"\n",
                "Y_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_test.npy\""
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "ASSIGN=np.load(X_TRAIN_FILE)",
                "ASSIGN=a.f.arr_0",
                "ASSIGN=np.load(Y_TRAIN_FILE)",
                "ASSIGN=a.f.arr_0",
                "ASSIGN=np.load(X_TEST_FILE)",
                "ASSIGN=a",
                "ASSIGN=np.load(Y_TEST_FILE)",
                "ASSIGN=a",
                "ASSIGN, X_val, ASSIGN, Y_val = train_test_split(ASSIGN, ASSIGN, test_size=0.25, random_state=42)"
            ],
            "source_orig": [
                "a=np.load(X_TRAIN_FILE)\n",
                "X_train=a.f.arr_0\n",
                "a=np.load(Y_TRAIN_FILE)\n",
                "Y_train=a.f.arr_0\n",
                "a=np.load(X_TEST_FILE)\n",
                "X_test=a\n",
                "a=np.load(Y_TEST_FILE)\n",
                "Y_test=a\n",
                "\n",
                "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(X_train.shape)",
                "print(Y_train.shape)",
                "print(X_val.shape)",
                "print(Y_val.shape)",
                "print(X_test.shape)",
                "print(Y_test.shape)"
            ],
            "source_orig": [
                "print(X_train.shape)\n",
                "print(Y_train.shape)\n",
                "print(X_val.shape)\n",
                "print(Y_val.shape)\n",
                "print(X_test.shape)\n",
                "print(Y_test.shape)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test):",
                "ASSIGN=normalize_X(ASSIGN)",
                "ASSIGN=normalize_X(ASSIGN)",
                "ASSIGN=normalize_X(ASSIGN)",
                "ASSIGN = shuffle(ASSIGN, random_state=42)",
                "ASSIGN = shuffle(ASSIGN, random_state=42)",
                "ASSIGN = shuffle(ASSIGN, random_state=42)",
                "return X_train,X_val,X_test,y_train,y_val,y_test"
            ],
            "source_orig": [
                "def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test):\n",
                "    X_train=normalize_X(X_train)\n",
                "    X_val=normalize_X(X_val)\n",
                "    X_test=normalize_X(X_test)\n",
                "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
                "    X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
                "    X_test, y_test = shuffle(X_test, y_test, random_state=42)\n",
                "    return X_train,X_val,X_test,y_train,y_val,y_test"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)"
            ],
            "source_orig": [
                "X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "def input_and_run2(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10):",
                "ASSIGN = ImageDataGenerator(width_shift_range=0.2,height_shift_range=0.2,horizontal_flip=True)",
                "ASSIGN.fit(X_train)",
                "ASSIGN = keras.optimizers.Adam(learning_rate=alpha)",
                "ASSIGN=SGD(lr=alpha, momentum=0.9)",
                "model.compile(loss='binary_crossentropy', optimizer=ASSIGN,metrics=['accuracy'])",
                "model.fit(ASSIGN.flow(X_train, y_train),validation_data=(X_val, y_val), epochs=num_epochs)",
                "ASSIGN = model.evaluate(X_train,y_train)",
                "print(+str(ASSIGN[1]*100))",
                "ASSIGN = model.evaluate(X_val,y_val)",
                "print(+str(ASSIGN[1]*100))",
                "ASSIGN = model.evaluate(X_test,y_test)",
                "print(+str(ASSIGN[1]*100))"
            ],
            "source_orig": [
                "\n",
                "\n",
                "def input_and_run2(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10):\n",
                "    \n",
                "    datagen = ImageDataGenerator(width_shift_range=0.2,height_shift_range=0.2,horizontal_flip=True)\n",
                "    datagen.fit(X_train)\n",
                "    \n",
                "    #compile model using accuracy to measure model performance\n",
                "    opt = keras.optimizers.Adam(learning_rate=alpha)\n",
                "    opt2=SGD(lr=alpha, momentum=0.9)\n",
                "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
                "    \n",
                "    #train the model\n",
                "    model.fit(datagen.flow(X_train, y_train),validation_data=(X_val, y_val), epochs=num_epochs)\n",
                "    \n",
                "    #Getting results\n",
                "    result = model.evaluate(X_train,y_train)\n",
                "    #print(result)\n",
                "    print(\"Training accuracy = \"+str(result[1]*100))\n",
                "    result = model.evaluate(X_val,y_val)\n",
                "    #print(result)\n",
                "    print(\"Validation accuracy = \"+str(result[1]*100))\n",
                "    result = model.evaluate(X_test,y_test)\n",
                "    #print(result)\n",
                "    print(\"Test accuracy = \"+str(result[1]*100))\n",
                "\n",
                "\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)))",
                "ASSIGN.add(MaxPooling2D((2, 2)))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))",
                "ASSIGN.add(MaxPooling2D((2, 2)))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))",
                "ASSIGN.add(MaxPooling2D((2, 2)))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))",
                "ASSIGN.add(MaxPooling2D((2, 2)))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))",
                "ASSIGN.add(Dropout(0.5))",
                "ASSIGN.add(Dense(1, activation='sigmoid'))",
                "print(ASSIGN.summary())",
                "input_and_run2(ASSIGN,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.001,num_epochs=200)"
            ],
            "source_orig": [
                "##BUILDING THE MODEL 1\n",
                "\n",
                "model4 = Sequential()#add model layers\n",
                "\n",
                "model4.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)))\n",
                "model4.add(MaxPooling2D((2, 2)))\n",
                "model4.add(Dropout(0.2))\n",
                "model4.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model4.add(MaxPooling2D((2, 2)))\n",
                "model4.add(Dropout(0.2))\n",
                "model4.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model4.add(MaxPooling2D((2, 2)))\n",
                "model4.add(Dropout(0.2))\n",
                "model4.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model4.add(MaxPooling2D((2, 2)))\n",
                "model4.add(Dropout(0.2))\n",
                "model4.add(Flatten())\n",
                "model4.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
                "model4.add(Dropout(0.5))\n",
                "model4.add(Dense(1, activation='sigmoid'))\n",
                "\n",
                "print(model4.summary())\n",
                "input_and_run2(model4,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.001,num_epochs=200)"
            ]
        },
        {
            "tags": [
                "check_results",
                "transfer_results"
            ],
            "source": [
                "CHECKPOINT",
                "model4.save(\"model4.h5\")",
                "print()"
            ],
            "source_orig": [
                "model4.save(\"model4.h5\")\n",
                "print(\"Saved model to disk\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "X_TRAIN_FILE=\"path\"",
                "X_TEST_FILE=\"path\"",
                "Y_TRAIN_FILE=\"path\"",
                "Y_TEST_FILE=\"path\""
            ],
            "source_orig": [
                "X_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_train.npz\"\n",
                "X_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_test.npy\"\n",
                "Y_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_train.npz\"\n",
                "Y_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_test.npy\""
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "ASSIGN=np.load(X_TRAIN_FILE)",
                "ASSIGN=a.f.arr_0",
                "ASSIGN=np.load(Y_TRAIN_FILE)",
                "ASSIGN=a.f.arr_0",
                "ASSIGN=np.load(X_TEST_FILE)",
                "ASSIGN=a",
                "ASSIGN=np.load(Y_TEST_FILE)",
                "ASSIGN=a",
                "ASSIGN, X_val, ASSIGN, Y_val = train_test_split(ASSIGN, ASSIGN, test_size=0.25, random_state=42)"
            ],
            "source_orig": [
                "a=np.load(X_TRAIN_FILE)\n",
                "X_train=a.f.arr_0\n",
                "a=np.load(Y_TRAIN_FILE)\n",
                "Y_train=a.f.arr_0\n",
                "a=np.load(X_TEST_FILE)\n",
                "X_test=a\n",
                "a=np.load(Y_TEST_FILE)\n",
                "Y_test=a\n",
                "\n",
                "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(X_train.shape)",
                "print(Y_train.shape)",
                "print(X_val.shape)",
                "print(Y_val.shape)",
                "print(X_test.shape)",
                "print(Y_test.shape)"
            ],
            "source_orig": [
                "print(X_train.shape)\n",
                "print(Y_train.shape)\n",
                "print(X_val.shape)\n",
                "print(Y_val.shape)\n",
                "print(X_test.shape)\n",
                "print(Y_test.shape)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test):",
                "ASSIGN=normalize_X(ASSIGN)",
                "ASSIGN=normalize_X(ASSIGN)",
                "ASSIGN=normalize_X(ASSIGN)",
                "ASSIGN = shuffle(ASSIGN, random_state=42)",
                "ASSIGN = shuffle(ASSIGN, random_state=42)",
                "ASSIGN = shuffle(ASSIGN, random_state=42)",
                "return X_train,X_val,X_test,y_train,y_val,y_test"
            ],
            "source_orig": [
                "def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test):\n",
                "    X_train=normalize_X(X_train)\n",
                "    X_val=normalize_X(X_val)\n",
                "    X_test=normalize_X(X_test)\n",
                "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
                "    X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
                "    X_test, y_test = shuffle(X_test, y_test, random_state=42)\n",
                "    return X_train,X_val,X_test,y_train,y_val,y_test"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)"
            ],
            "source_orig": [
                "X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "def input_and_run3(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10):",
                "ASSIGN = keras.optimizers.Adam(learning_rate=alpha)",
                "ASSIGN=SGD(lr=alpha, momentum=0.9)",
                "model.compile(loss='binary_crossentropy', optimizer=ASSIGN,metrics=['accuracy'])",
                "model.fit(X_train,y_train,validation_data=(X_val, y_val), epochs=num_epochs)",
                "ASSIGN = model.evaluate(X_train,y_train)",
                "print(+str(ASSIGN[1]*100))",
                "ASSIGN = model.evaluate(X_val,y_val)",
                "print(+str(ASSIGN[1]*100))",
                "ASSIGN = model.evaluate(X_test,y_test)",
                "print(+str(ASSIGN[1]*100))"
            ],
            "source_orig": [
                "\n",
                "\n",
                "def input_and_run3(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10):\n",
                "    \n",
                "    #datagen = ImageDataGenerator(width_shift_range=0.2,height_shift_range=0.2,horizontal_flip=True)\n",
                "    #datagen.fit(X_train)\n",
                "    \n",
                "    #compile model using accuracy to measure model performance\n",
                "    opt = keras.optimizers.Adam(learning_rate=alpha)\n",
                "    opt2=SGD(lr=alpha, momentum=0.9)\n",
                "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
                "    \n",
                "    #train the model\n",
                "    model.fit(X_train,y_train,validation_data=(X_val, y_val), epochs=num_epochs)\n",
                "    #model.fit(datagen.flow(X_train, y_train),validation_data=(X_val, y_val), epochs=num_epochs)\n",
                "    \n",
                "    #Getting results\n",
                "    result = model.evaluate(X_train,y_train)\n",
                "    #print(result)\n",
                "    print(\"Training accuracy = \"+str(result[1]*100))\n",
                "    result = model.evaluate(X_val,y_val)\n",
                "    #print(result)\n",
                "    print(\"Validation accuracy = \"+str(result[1]*100))\n",
                "    result = model.evaluate(X_test,y_test)\n",
                "    #print(result)\n",
                "    print(\"Test accuracy = \"+str(result[1]*100))\n",
                "\n",
                "\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = Sequential()",
                "ASSIGN.add(VGG16(include_top=False, input_shape=(100, 100, 3)))",
                "for layer in ASSIGN.layers:",
                "layer.trainable = False",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))",
                "ASSIGN.add(Dropout(0.5))",
                "ASSIGN.add(Dense(1, activation='sigmoid'))",
                "print(ASSIGN.summary())",
                "input_and_run3(ASSIGN,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.001,num_epochs=200)"
            ],
            "source_orig": [
                "model5 = Sequential()\n",
                "model5.add(VGG16(include_top=False, input_shape=(100, 100, 3)))\n",
                "# mark loaded layers as not trainable\n",
                "for layer in model5.layers:\n",
                "    layer.trainable = False\n",
                "# add new classifier layers\n",
                "#flat1 = Flatten()(model.layers[-1].output)\n",
                "#class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
                "#output = Dense(1, activation='sigmoid')(class1)\n",
                "\n",
                "model5.add(Flatten())\n",
                "model5.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
                "model5.add(Dropout(0.5))\n",
                "model5.add(Dense(1, activation='sigmoid'))\n",
                "print(model5.summary())\n",
                "input_and_run3(model5,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.001,num_epochs=200)\n",
                "\n",
                "\n",
                "# define new model\n",
                "#model = Model(inputs=model.inputs, outputs=output)\n",
                "# compile model\n",
                "#opt = SGD(lr=0.001, momentum=0.9)\n",
                "#model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
                "#return model"
            ]
        },
        {
            "tags": [
                "check_results",
                "transfer_results"
            ],
            "source": [
                "CHECKPOINT",
                "model5.save(\"model5.h5\")",
                "print()"
            ],
            "source_orig": [
                "model5.save(\"model5.h5\")\n",
                "print(\"Saved model to disk\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "X_TRAIN_FILE=\"path\"",
                "X_TEST_FILE=\"path\"",
                "Y_TRAIN_FILE=\"path\"",
                "Y_TEST_FILE=\"path\""
            ],
            "source_orig": [
                "X_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_train.npz\"\n",
                "X_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_test.npy\"\n",
                "Y_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_train.npz\"\n",
                "Y_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_test.npy\""
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "ASSIGN=np.load(X_TRAIN_FILE)",
                "ASSIGN=a.f.arr_0",
                "ASSIGN=np.load(Y_TRAIN_FILE)",
                "ASSIGN=a.f.arr_0",
                "ASSIGN=np.load(X_TEST_FILE)",
                "ASSIGN=a",
                "ASSIGN=np.load(Y_TEST_FILE)",
                "ASSIGN=a",
                "ASSIGN, X_val, ASSIGN, Y_val = train_test_split(ASSIGN, ASSIGN, test_size=0.25, random_state=42)"
            ],
            "source_orig": [
                "a=np.load(X_TRAIN_FILE)\n",
                "X_train=a.f.arr_0\n",
                "a=np.load(Y_TRAIN_FILE)\n",
                "Y_train=a.f.arr_0\n",
                "a=np.load(X_TEST_FILE)\n",
                "X_test=a\n",
                "a=np.load(Y_TEST_FILE)\n",
                "Y_test=a\n",
                "\n",
                "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(X_train.shape)",
                "print(Y_train.shape)",
                "print(X_val.shape)",
                "print(Y_val.shape)",
                "print(X_test.shape)",
                "print(Y_test.shape)"
            ],
            "source_orig": [
                "print(X_train.shape)\n",
                "print(Y_train.shape)\n",
                "print(X_val.shape)\n",
                "print(Y_val.shape)\n",
                "print(X_test.shape)\n",
                "print(Y_test.shape)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)"
            ],
            "source_orig": [
                "X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)"
            ]
        },
        {
            "tags": [
                "check_results",
                "transfer_results"
            ],
            "source": [
                "CHECKPOINT",
                "model6.save(\"model6.h5\")",
                "print()"
            ],
            "source_orig": [
                "model6.save(\"model6.h5\")\n",
                "print(\"Saved model to disk\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "from keras.datasets import mnist#download mnist data and split into train and test sets\n",
                "from sklearn.model_selection import train_test_split\n",
                "import matplotlib.pyplot as plt\n",
                "from keras.utils import to_categorical\n",
                "\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "(X_train, y_train), (X_test, y_test) = mnist.load_data()",
                "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=10000, random_state=42)"
            ],
            "source_orig": [
                "\n",
                "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=10000, random_state=42)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(X_train.shape)",
                "print(y_train.shape)",
                "print(X_val.shape)",
                "print(y_val.shape)",
                "print(X_test.shape)",
                "print(y_test.shape)"
            ],
            "source_orig": [
                "###Seeing dimensions of the different sets\n",
                "print(X_train.shape)\n",
                "print(y_train.shape)\n",
                "print(X_val.shape)\n",
                "print(y_val.shape)\n",
                "print(X_test.shape)\n",
                "print(y_test.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN=5",
                "ASSIGN=X_train[image_no]",
                "plt.imshow(ASSIGN)"
            ],
            "source_orig": [
                "##Seeing example of random images\n",
                "image_no=5\n",
                "img=X_train[image_no]\n",
                "plt.imshow(img)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "SETUP",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))",
                "ASSIGN.add(Conv2D(32, kernel_size=3, activation=\"relu\"))",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(10, activation='softmax'))"
            ],
            "source_orig": [
                "##BUILDING THE MODEL\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Dense, Conv2D, Flatten#create model\n",
                "\n",
                "model = Sequential()#add model layers\n",
                "\n",
                "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
                "model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(10, activation='softmax'))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(model.summary())"
            ],
            "source_orig": [
                "print(model.summary())"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
            ],
            "source_orig": [
                "#compile model using accuracy to measure model performance\n",
                "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=reshape_X(normalize_X(ASSIGN))",
                "ASSIGN=reshape_X(normalize_X(ASSIGN))",
                "ASSIGN=reshape_X(normalize_X(ASSIGN))",
                "ASSIGN=to_categorical(ASSIGN)",
                "ASSIGN=to_categorical(ASSIGN)",
                "ASSIGN=to_categorical(ASSIGN)"
            ],
            "source_orig": [
                "#Pre processing the data\n",
                "X_train=reshape_X(normalize_X(X_train))\n",
                "X_val=reshape_X(normalize_X(X_val))\n",
                "X_test=reshape_X(normalize_X(X_test))\n",
                "y_train=to_categorical(y_train)\n",
                "y_val=to_categorical(y_val)\n",
                "y_test=to_categorical(y_test)\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)"
            ],
            "source_orig": [
                "#train the model\n",
                "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model.evaluate(X_train,y_train)",
                "print(+str(ASSIGN[1]*100))",
                "ASSIGN = model.evaluate(X_val,y_val)",
                "print(+str(ASSIGN[1]*100))"
            ],
            "source_orig": [
                "result = model.evaluate(X_train,y_train)\n",
                "#print(result)\n",
                "print(\"Training accuracy = \"+str(result[1]*100))\n",
                "result = model.evaluate(X_val,y_val)\n",
                "#print(result)\n",
                "print(\"Validation accuracy = \"+str(result[1]*100))\n",
                "#result = model.evaluate(X_test,y_test)\n",
                "#print(result)\n",
                "#print(\"Test accuracy = \"+str(result[1]*100))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "warnings.filterwarnings('ignore')",
                "print(os.listdir())"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "from datetime import datetime as dt\n",
                "\n",
                "# For Visualisation\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "%matplotlib inline\n",
                "\n",
                "# To Scale our data\n",
                "from sklearn.preprocessing import scale\n",
                "\n",
                "# Supress Warnings\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from sklearn.datasets import fetch_mldata\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn import metrics\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "import os\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = pd.read_csv(\"..path\", sep = ',',encoding = \"ISO-8859-1\", header= 0)",
                "ASSIGN = pd.read_csv(\"..path\", sep = ',',encoding = \"ISO-8859-1\", header= 0)",
                "print(.format(ASSIGN.shape))",
                "print(.format(ASSIGN.shape))"
            ],
            "source_orig": [
                "mnist = pd.read_csv(\"../input/train.csv\",  sep = ',',encoding = \"ISO-8859-1\", header= 0)\n",
                "holdout = pd.read_csv(\"../input/test.csv\",  sep = ',',encoding = \"ISO-8859-1\", header= 0)\n",
                "\n",
                "print(\"Dimensions of train: {}\".format(mnist.shape))\n",
                "print(\"Dimensions of test: {}\".format(holdout.shape))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "mnist.head()"
            ],
            "source_orig": [
                "mnist.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = pd.DataFrame(mnist.isnull().sum().sort_values(ascending=False), columns=['Total'])",
                "ASSIGN = pd.DataFrame(round(100*(mnist.isnull().sum()path[0]),2).sort_values(ascending=False)\\",
                ",columns=['Percentage'])",
                "pd.concat([ASSIGN, ASSIGN], axis = 1).head()"
            ],
            "source_orig": [
                "# Checking for total count and percentage of null values in all columns of the dataframe.\n",
                "\n",
                "total = pd.DataFrame(mnist.isnull().sum().sort_values(ascending=False), columns=['Total'])\n",
                "percentage = pd.DataFrame(round(100*(mnist.isnull().sum()/mnist.shape[0]),2).sort_values(ascending=False)\\\n",
                "                          ,columns=['Percentage'])\n",
                "pd.concat([total, percentage], axis = 1).head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = mnist.drop(['label'], axis=1)",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "# Putting feature variable to X\n",
                "X = mnist.drop(['label'], axis=1)\n",
                "\n",
                "X.head()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = mnist['label']",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "# Putting response variable to y\n",
                "y = mnist['label']\n",
                "\n",
                "y.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from sklearn.preprocessing import StandardScaler"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = StandardScaler()",
                "ASSIGN = scaler.fit_transform(ASSIGN)",
                "pd.DataFrame(ASSIGN).head()"
            ],
            "source_orig": [
                "scaler = StandardScaler()\n",
                "\n",
                "X = scaler.fit_transform(X)\n",
                "\n",
                "pd.DataFrame(X).head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)"
            ],
            "source_orig": [
                "# Splitting the data into train and test\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pd.DataFrame(X_test).head()"
            ],
            "source_orig": [
                "pd.DataFrame(X_test).head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "SETUP",
                "plt.figure(figsize=(20,4))",
                "for index, (image, label) in enumerate(zip(X_train[0:5], y_train[0:5])):",
                "plt.subplot(1, 5, index + 1)",
                "plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)",
                "plt.title('Training: %i\\n' % label, fontsize = 20)"
            ],
            "source_orig": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "plt.figure(figsize=(20,4))\n",
                "for index, (image, label) in enumerate(zip(X_train[0:5], y_train[0:5])):\n",
                " plt.subplot(1, 5, index + 1)\n",
                " plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
                " plt.title('Training: %i\\n' % label, fontsize = 20)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "X_train.shape"
            ],
            "source_orig": [
                "X_train.shape\n",
                "# We have 30 variables after creating our dummy variables for our categories"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "SETUP",
                "ASSIGN = PCA(svd_solver='randomized', random_state=42)"
            ],
            "source_orig": [
                "#Improting the PCA module\n",
                "from sklearn.decomposition import PCA\n",
                "pca = PCA(svd_solver='randomized', random_state=42)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "pca.fit(X_train)"
            ],
            "source_orig": [
                "#Doing the PCA on the train data\n",
                "pca.fit(X_train)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pca.n_components_"
            ],
            "source_orig": [
                "pca.n_components_"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pca.components_"
            ],
            "source_orig": [
                "pca.components_"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = list(pd.DataFrame(X_train).columns)",
                "ASSIGN = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "colnames = list(pd.DataFrame(X_train).columns)\n",
                "pcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\n",
                "pcs_df.head()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "ASSIGN = plt.figure(figsize = (10,10))",
                "plt.scatter(pcs_df.PC1, pcs_df.PC2)",
                "plt.xlabel('Principal Component 1')",
                "plt.ylabel('Principal Component 2')",
                "for i, txt in enumerate(pcs_df.Feature):",
                "plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))",
                "plt.tight_layout()",
                "plt.show()"
            ],
            "source_orig": [
                "%matplotlib inline\n",
                "fig = plt.figure(figsize = (10,10))\n",
                "plt.scatter(pcs_df.PC1, pcs_df.PC2)\n",
                "plt.xlabel('Principal Component 1')\n",
                "plt.ylabel('Principal Component 2')\n",
                "for i, txt in enumerate(pcs_df.Feature):\n",
                "    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pd.options.display.float_format = '{:.2f}'.format",
                "pca.explained_variance_ratio_"
            ],
            "source_orig": [
                "pd.options.display.float_format = '{:.2f}'.format\n",
                "pca.explained_variance_ratio_"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "ASSIGN = plt.figure(figsize = (12,8))",
                "plt.plot(np.cumsum(pca.explained_variance_ratio_))",
                "plt.xlabel('number of components')",
                "plt.ylabel('cumulative explained variance')",
                "plt.show()"
            ],
            "source_orig": [
                "#Making the screeplot - plotting the cumulative variance against the number of components\n",
                "%matplotlib inline\n",
                "fig = plt.figure(figsize = (12,8))\n",
                "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
                "plt.xlabel('number of components')\n",
                "plt.ylabel('cumulative explained variance')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pca.transform(X_train)",
                "ASSIGN = pca.transform(X_test)"
            ],
            "source_orig": [
                "X_train_pca = pca.transform(X_train)\n",
                "X_test_pca = pca.transform(X_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from sklearn.linear_model import LogisticRegression"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = LogisticRegression(solver = 'lbfgs')"
            ],
            "source_orig": [
                "logisticRegr = LogisticRegression(solver = 'lbfgs')"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "logisticRegr.fit(X_train_pca, y_train)"
            ],
            "source_orig": [
                "logisticRegr.fit(X_train_pca, y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "logisticRegr.predict(X_train_pca[0:10])"
            ],
            "source_orig": [
                "logisticRegr.predict(X_train_pca[0:10])"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "logisticRegr.predict(X_train_pca)"
            ],
            "source_orig": [
                "logisticRegr.predict(X_train_pca)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = logisticRegr.ASSIGN(X_train_pca, y_train)",
                "print(ASSIGN)"
            ],
            "source_orig": [
                "score = logisticRegr.score(X_train_pca, y_train)\n",
                "print(score)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "logisticRegr.predict(X_test_pca)"
            ],
            "source_orig": [
                "logisticRegr.predict(X_test_pca)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = logisticRegr.ASSIGN(X_test_pca, y_test)",
                "print(ASSIGN)"
            ],
            "source_orig": [
                "score = logisticRegr.score(X_test_pca, y_test)\n",
                "print(score)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "X_train.shape"
            ],
            "source_orig": [
                "X_train.shape\n",
                "# We have 30 variables after creating our dummy variables for our categories"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "pca_last.fit(X_train)"
            ],
            "source_orig": [
                "#Doing the PCA on the train data\n",
                "pca_last.fit(X_train)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pca_last.n_components_"
            ],
            "source_orig": [
                "pca_last.n_components_"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pca_last.components_"
            ],
            "source_orig": [
                "pca_last.components_"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = list(pd.DataFrame(X_train).columns)",
                "ASSIGN = pd.DataFrame({'PC1':pca_last.components_[0],'PC2':pca_last.components_[1], 'Feature':colnames})",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "colnames = list(pd.DataFrame(X_train).columns)\n",
                "pcs_df = pd.DataFrame({'PC1':pca_last.components_[0],'PC2':pca_last.components_[1], 'Feature':colnames})\n",
                "pcs_df.head()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "ASSIGN = plt.figure(figsize = (10,10))",
                "plt.scatter(pcs_df.PC1, pcs_df.PC2)",
                "plt.xlabel('Principal Component 1')",
                "plt.ylabel('Principal Component 2')",
                "for i, txt in enumerate(pcs_df.Feature):",
                "plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))",
                "plt.tight_layout()",
                "plt.show()"
            ],
            "source_orig": [
                "%matplotlib inline\n",
                "fig = plt.figure(figsize = (10,10))\n",
                "plt.scatter(pcs_df.PC1, pcs_df.PC2)\n",
                "plt.xlabel('Principal Component 1')\n",
                "plt.ylabel('Principal Component 2')\n",
                "for i, txt in enumerate(pcs_df.Feature):\n",
                "    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pca_last.transform(X_train)",
                "ASSIGN = pca_last.transform(X_test)"
            ],
            "source_orig": [
                "X_train_pca = pca_last.transform(X_train)\n",
                "X_test_pca = pca_last.transform(X_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from sklearn.linear_model import LogisticRegression"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = LogisticRegression(solver = 'lbfgs')"
            ],
            "source_orig": [
                "logisticRegr = LogisticRegression(solver = 'lbfgs')"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = logisticRegr.fit(X_train_pca, y_train)",
                "model_pca"
            ],
            "source_orig": [
                "model_pca = logisticRegr.fit(X_train_pca, y_train)\n",
                "model_pca"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "logisticRegr.predict(X_train_pca[0:10])"
            ],
            "source_orig": [
                "logisticRegr.predict(X_train_pca[0:10])"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = logisticRegr.predict(X_train_pca)",
                "predictions"
            ],
            "source_orig": [
                "predictions = logisticRegr.predict(X_train_pca)\n",
                "predictions"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = logisticRegr.ASSIGN(X_train_pca, y_train)",
                "print(ASSIGN)"
            ],
            "source_orig": [
                "score = logisticRegr.score(X_train_pca, y_train)\n",
                "print(score)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "ASSIGN = plt.figure(figsize = (8,8))",
                "plt.scatter(X_train_pca[:,0], X_train_pca[:,1], c = y_train)",
                "plt.xlabel('Principal Component 1')",
                "plt.ylabel('Principal Component 2')",
                "plt.tight_layout()",
                "plt.show()"
            ],
            "source_orig": [
                "%matplotlib inline\n",
                "fig = plt.figure(figsize = (8,8))\n",
                "plt.scatter(X_train_pca[:,0], X_train_pca[:,1], c = y_train)\n",
                "plt.xlabel('Principal Component 1')\n",
                "plt.ylabel('Principal Component 2')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "SETUP",
                "ASSIGN = plt.figure(figsize=(8,8))",
                "ASSIGN = Axes3D(fig)",
                "ASSIGN = plt.axes(projection='3d')",
                "ASSIGN.scatter(X_train_pca[:,2], X_train_pca[:,0], X_train_pca[:,1],zdir='z', s=20, marker = 'o', c=y_train)",
                "ASSIGN.set_xlabel('Principal Component 1')",
                "ASSIGN.set_ylabel('Principal Component 2')",
                "ASSIGN.set_zlabel('Principal Component 3')",
                "plt.tight_layout()",
                "plt.show()"
            ],
            "source_orig": [
                "%matplotlib notebook\n",
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "fig = plt.figure(figsize=(8,8))\n",
                "ax = Axes3D(fig)\n",
                "ax = plt.axes(projection='3d')\n",
                "ax.scatter(X_train_pca[:,2], X_train_pca[:,0], X_train_pca[:,1],zdir='z', s=20, marker = 'o', c=y_train)\n",
                "ax.set_xlabel('Principal Component 1')\n",
                "ax.set_ylabel('Principal Component 2')\n",
                "ax.set_zlabel('Principal Component 3')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "check_results"
            ],
            "source": [
                "SETUP",
                "ASSIGN = sm.GLM(y_train,(sm.add_constant(X_train_pca)), family = sm.families.Binomial())",
                "ASSIGN.fit().summary()"
            ],
            "source_orig": [
                "import statsmodels.api as sm\n",
                "# Logistic regression model\n",
                "logpca = sm.GLM(y_train,(sm.add_constant(X_train_pca)), family = sm.families.Binomial())\n",
                "logpca.fit().summary()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn import metrics"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "SETUP",
                "ASSIGN = plot_confusion_matrix(conf_mat=cm)",
                "plt.show()"
            ],
            "source_orig": [
                "import matplotlib.pyplot as plt\n",
                "from mlxtend.plotting import plot_confusion_matrix\n",
                "\n",
                "fig, ax = plot_confusion_matrix(conf_mat=cm)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(metrics.accuracy_score(y_train, predictions))"
            ],
            "source_orig": [
                "print(metrics.accuracy_score(y_train, predictions))"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = np.diag(cm) path(cm, axis = 1)",
                "recall"
            ],
            "source_orig": [
                "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
                "recall"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from sklearn.metrics import precision_score, recall_score"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = np.diag(cm) path(cm, axis = 0)",
                "precision"
            ],
            "source_orig": [
                "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
                "precision"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "precision_score(y_train, predictions,average='macro')"
            ],
            "source_orig": [
                "precision_score(y_train, predictions,average='macro')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = score(y_train, predictions)",
                "print('precision: {}'.format(precision))",
                "print('recall: {}'.format(recall))",
                "print('fscore: {}'.format(fscore))",
                "print('support: {}'.format(support))"
            ],
            "source_orig": [
                "from sklearn.metrics import precision_recall_fscore_support as score\n",
                "\n",
                "precision, recall, fscore, support = score(y_train, predictions)\n",
                "\n",
                "print('precision: {}'.format(precision))\n",
                "print('recall: {}'.format(recall))\n",
                "print('fscore: {}'.format(fscore))\n",
                "print('support: {}'.format(support))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "print(classification_report(y_train, predictions))"
            ],
            "source_orig": [
                "from sklearn.metrics import classification_report\n",
                "print(classification_report(y_train, predictions))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = logisticRegr.predict(X_test_pca)",
                "predict_test"
            ],
            "source_orig": [
                "predict_test = logisticRegr.predict(X_test_pca)\n",
                "predict_test"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = logisticRegr.ASSIGN(X_test_pca, y_test)",
                "print(ASSIGN)"
            ],
            "source_orig": [
                "score = logisticRegr.score(X_test_pca, y_test)\n",
                "print(score)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(metrics.accuracy_score(y_test, predict_test))"
            ],
            "source_orig": [
                "print(metrics.accuracy_score(y_test, predict_test))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "print(classification_report(y_test, predict_test))"
            ],
            "source_orig": [
                "from sklearn.metrics import classification_report\n",
                "print(classification_report(y_test, predict_test))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "holdout.head()"
            ],
            "source_orig": [
                "holdout.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "holdout.shape"
            ],
            "source_orig": [
                "holdout.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame(holdout.isnull().sum().sort_values(ascending=False), columns=['Total'])",
                "ASSIGN = pd.DataFrame(round(100*(holdout.isnull().sum()path[0]),2).sort_values(ascending=False)\\",
                ",columns=['Percentage'])",
                "pd.concat([ASSIGN, ASSIGN], axis = 1).head()"
            ],
            "source_orig": [
                "# Checking for total count and percentage of null values in all columns of the dataframe.\n",
                "\n",
                "total = pd.DataFrame(holdout.isnull().sum().sort_values(ascending=False), columns=['Total'])\n",
                "percentage = pd.DataFrame(round(100*(holdout.isnull().sum()/holdout.shape[0]),2).sort_values(ascending=False)\\\n",
                "                          ,columns=['Percentage'])\n",
                "pd.concat([total, percentage], axis = 1).head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from sklearn.preprocessing import StandardScaler"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = scaler.transform(ASSIGN)",
                "pd.DataFrame(ASSIGN).head()"
            ],
            "source_orig": [
                "holdout = scaler.transform(holdout)\n",
                "\n",
                "pd.DataFrame(holdout).head()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = pca_last.transform(holdout)",
                "pd.DataFrame(ASSIGN).head()"
            ],
            "source_orig": [
                "holdout_pca = pca_last.transform(holdout)\n",
                "pd.DataFrame(holdout_pca).head()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = logisticRegr.predict(holdout_pca)",
                "predict_holdout"
            ],
            "source_orig": [
                "predict_holdout = logisticRegr.predict(holdout_pca)\n",
                "predict_holdout"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submission.to_csv(\"submission.csv\",index=False)"
            ],
            "source_orig": [
                "submission.to_csv(\"submission.csv\",index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "SETUP",
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN = pd.read_csv('path')"
            ],
            "source_orig": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "\n",
                "train = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
                "test = pd.read_csv('/kaggle/input/titanic/test.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "test"
            ],
            "source_orig": [
                "test"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.describe()"
            ],
            "source_orig": [
                "train.describe()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "train.dtypes"
            ],
            "source_orig": [
                "train.dtypes"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "SETUP",
                "sns.heatmap(train.isnull(), cbar=False)"
            ],
            "source_orig": [
                "import seaborn as sns\n",
                "\n",
                "sns.heatmap(train.isnull(), cbar=False)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.fillna(ASSIGN.median())",
                "ASSIGN = ASSIGN.fillna(ASSIGN.median())",
                "sns.heatmap(test.isnull(), cbar=False)"
            ],
            "source_orig": [
                "test['Age'] = test['Age'].fillna(test['Age'].median())\n",
                "test['Fare'] = test['Fare'].fillna(test['Fare'].median())\n",
                "\n",
                "sns.heatmap(test.isnull(), cbar=False)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.drop('Name',axis = 1)",
                "ASSIGN = ASSIGN.drop('Sex',axis = 1)",
                "ASSIGN = ASSIGN.drop('Ticket',axis = 1)",
                "ASSIGN = ASSIGN.drop('Cabin',axis = 1)",
                "ASSIGN = ASSIGN.drop('Name',axis = 1)",
                "ASSIGN = ASSIGN.drop('Sex',axis = 1)",
                "ASSIGN = ASSIGN.drop('Ticket',axis = 1)",
                "ASSIGN = ASSIGN.drop('Cabin',axis = 1)"
            ],
            "source_orig": [
                "train = train.drop('Name',axis = 1)\n",
                "train = train.drop('Sex',axis = 1)\n",
                "train = train.drop('Ticket',axis = 1)\n",
                "train = train.drop('Cabin',axis = 1)\n",
                "\n",
                "\n",
                "test = test.drop('Name',axis = 1)\n",
                "test = test.drop('Sex',axis = 1)\n",
                "test = test.drop('Ticket',axis = 1)\n",
                "test = test.drop('Cabin',axis = 1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.heatmap(train.isnull(), cbar=False)"
            ],
            "source_orig": [
                "sns.heatmap(train.isnull(), cbar=False)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.heatmap(test.isnull(), cbar=False)"
            ],
            "source_orig": [
                "sns.heatmap(test.isnull(), cbar=False)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for i in [train, test]:",
                "ASSIGN = ASSIGN.fillna('S')"
            ],
            "source_orig": [
                "for i in [train, test]:\n",
                "    i['Embarked'] = i['Embarked'].fillna('S')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "SETUP",
                "ASSIGN = OneHotEncoder(handle_unknown='ignore')",
                "ASSIGN = pd.DataFrame(enc.fit_transform(train[['Embarked']]).toarray())",
                "ASSIGN = ASSIGN.join(enc_df)",
                "ASSIGN = pd.DataFrame(enc.fit_transform(test[['Embarked']]).toarray())",
                "ASSIGN = ASSIGN.join(enc_df)",
                "ASSIGN = ASSIGN.drop('Embarked',axis = 1)",
                "ASSIGN = ASSIGN.drop('Embarked',axis = 1)"
            ],
            "source_orig": [
                "from sklearn.preprocessing import OneHotEncoder\n",
                "\n",
                "enc = OneHotEncoder(handle_unknown='ignore')\n",
                "\n",
                "\n",
                "enc_df = pd.DataFrame(enc.fit_transform(train[['Embarked']]).toarray())\n",
                "train = train.join(enc_df)\n",
                "\n",
                "enc_df = pd.DataFrame(enc.fit_transform(test[['Embarked']]).toarray())\n",
                "test = test.join(enc_df)\n",
                "\n",
                "\n",
                "train = train.drop('Embarked',axis = 1)\n",
                "test = test.drop('Embarked',axis = 1)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.describe()"
            ],
            "source_orig": [
                "train.describe()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "train.dtypes"
            ],
            "source_orig": [
                "train.dtypes"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = np.array(train.iloc[:,train.columns != 'Survived'])",
                "ASSIGN = np.array(train.Survived).reshape(-1,1)"
            ],
            "source_orig": [
                "X = np.array(train.iloc[:,train.columns != 'Survived'])\n",
                "y = np.array(train.Survived).reshape(-1,1)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "SETUP",
                "ASSIGN = RandomForestClassifier().fit(X, y.reshape(-1))",
                "ASSIGN = rfc.predict(X)"
            ],
            "source_orig": [
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.metrics import accuracy_score, classification_report, plot_confusion_matrix\n",
                "\n",
                "\n",
                "rfc = RandomForestClassifier().fit(X, y.reshape(-1))\n",
                "y_prima = rfc.predict(X)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print('Cross_val Score RandomForestClassifier = ', cross_val_score(rfc, X, y.reshape(-1), cv=5).mean())"
            ],
            "source_orig": [
                "print('Cross_val Score RandomForestClassifier = ', cross_val_score(rfc, X, y.reshape(-1), cv=5).mean())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "SETUP",
                "ASSIGN = plot_confusion_matrix(rfc, X, y,",
                "ASSIGN=class_names,",
                "ASSIGN=plt.cm.Blues,",
                "ASSIGN=None)",
                "plt.show()"
            ],
            "source_orig": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "disp = plot_confusion_matrix(rfc, X, y,\n",
                "                         display_labels=class_names,\n",
                "                         cmap=plt.cm.Blues,\n",
                "                         normalize=None)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "test"
            ],
            "source_orig": [
                "test"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = rfc.predict(test.iloc[:,test.columns != 'Survived'])"
            ],
            "source_orig": [
                "\n",
                "predictions = rfc.predict(test.iloc[:,test.columns != 'Survived'])\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(test)"
            ],
            "source_orig": [
                "print(test)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "transfer_results",
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = pd.DataFrame(ASSIGN, columns=['Survived'])",
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN = pd.concat((test.iloc[:, 0], ASSIGN), axis = 1)",
                "ASSIGN.to_csv('submission1.csv', sep=\",\", index = False)",
                "print('end')"
            ],
            "source_orig": [
                "predictions = pd.DataFrame(predictions, columns=['Survived'])\n",
                "test = pd.read_csv('/kaggle/input/titanic/test.csv')\n",
                "predictions = pd.concat((test.iloc[:, 0], predictions), axis = 1)\n",
                "predictions.to_csv('submission1.csv', sep=\",\", index = False)\n",
                "\n",
                "print('end')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "sns.set_style('whitegrid')",
                "print(check_output([, ]).decode())",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = data[(data.Sex == 'Female')]",
                "ASSIGN = women['Category'].value_counts()",
                "ASSIGN = by_categorypath",
                "ASSIGN = data['Category'].unique()",
                "plt.text(0.1, 1, 'Women Laureates Category',",
                "ASSIGN='center',",
                "ASSIGN='center',",
                "ASSIGN=12, color='k')",
                "plt.pie(ASSIGN.values, labels = ASSIGN)",
                "plt.axis('equal')",
                "plt.show()",
                "plt.clf()"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "sns.set_style('whitegrid')\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "data = pd.read_csv('../input/archive.csv')\n",
                "women = data[(data.Sex == 'Female')]\n",
                "by_category = women['Category'].value_counts()\n",
                "percent = by_category/50\n",
                "categories = data['Category'].unique() \n",
                "\n",
                "#title\n",
                "plt.text(0.1, 1, 'Women Laureates Category',\n",
                "        horizontalalignment='center',\n",
                "        verticalalignment='center',\n",
                "        fontsize=12, color='k')\n",
                "\n",
                "# Plot\n",
                "plt.pie(percent.values, labels = categories)\n",
                "plt.axis('equal')\n",
                "plt.show()\n",
                "plt.clf()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = data['Birth Country'].unique()",
                "ASSIGN = {}",
                "for category in categories:",
                "for country in ASSIGN:",
                "ASSIGN = data.Category[(data['Category'] == category) & (data['Birth Country'] == country)].count()",
                "ASSIGN.setdefault(category, []).append(ASSIGN)",
                "for country in ASSIGN:",
                "ASSIGN.setdefault('country', []).append(country)",
                "ASSIGN = pd.DataFrame(dict)",
                "for lab, row in ASSIGN.iterrows() :",
                "ASSIGN.loc[lab, 'total'] = row['Economics'] + row['Chemistry'] + row['Literature'] + row['Medicine'] + row['Peace'] + row['Physics']",
                "ASSIGN = ASSIGN.sort_values('total')"
            ],
            "source_orig": [
                "countries = data['Birth Country'].unique()\n",
                "\n",
                "dict = {}\n",
                "\n",
                "for category in categories:\n",
                "    for country in countries:\n",
                "        x = data.Category[(data['Category'] == category) & (data['Birth Country'] == country)].count()\n",
                "        #print(category, country, x)\n",
                "        dict.setdefault(category, []).append(x)\n",
                "        \n",
                "for country in countries:\n",
                "    dict.setdefault('country', []).append(country)\n",
                "\n",
                "df = pd.DataFrame(dict)\n",
                "\n",
                "for lab, row in df.iterrows() :\n",
                "    df.loc[lab, 'total'] = row['Economics'] + row['Chemistry'] + row['Literature'] + row['Medicine'] + row['Peace'] + row['Physics']\n",
                "   \n",
                "df = df.sort_values('total')\n",
                "        "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = len(countries)",
                "print(ASSIGN)",
                "ASSIGN = range(N)",
                "ASSIGN = 0.8",
                "plt.figure(figsize=(100,150))",
                "ASSIGN = plt.barh(ind, df['Chemistry'], width, color = '",
                "ASSIGN = plt.barh(ind, df['Literature'], width, df['Chemistry'], color = '",
                "ASSIGN = plt.barh(ind, df['Medicine'], width, df['Chemistry'] + df['Literature'], color = '",
                "ASSIGN = plt.barh(ind, df['Peace'], width, df['Chemistry'] + df['Literature'] + df['Medicine'], color = '",
                "ASSIGN = plt.barh(ind, df['Physics'], width, df['Chemistry'] + df['Literature'] + df['Medicine'] + df['Peace'], color = '",
                "ASSIGN = plt.barh(ind, df['Economics'], width, df['Chemistry'] + df['Literature'] + df['Medicine'] + df['Peace'] + df['Physics'], color = '",
                "plt.xticks(np.arange(0, 280, 1))",
                "plt.yticks(ASSIGN,df['country'], fontsize=56)",
                "plt.show()",
                "plt.clf()"
            ],
            "source_orig": [
                "N = len(countries)\n",
                "print(N)\n",
                "ind = range(N)  \n",
                "width = 0.8\n",
                "plt.figure(figsize=(100,150))\n",
                "\n",
                "p1 = plt.barh(ind, df['Chemistry'], width, color = '#137e6d')\n",
                "p2 = plt.barh(ind, df['Literature'], width, df['Chemistry'],  color = '#95d0fc')\n",
                "p3 = plt.barh(ind, df['Medicine'], width, df['Chemistry'] + df['Literature'], color = '#03719c')\n",
                "p4 = plt.barh(ind, df['Peace'], width, df['Chemistry'] + df['Literature'] + df['Medicine'], color = '#6a79f7')\n",
                "p5 = plt.barh(ind, df['Physics'], width, df['Chemistry'] + df['Literature'] + df['Medicine'] + df['Peace'], color = '#137e6d')\n",
                "p6 = plt.barh(ind, df['Economics'], width, df['Chemistry'] + df['Literature'] + df['Medicine'] + df['Peace'] + df['Physics'], color = '#95d0fc')\n",
                "\n",
                "plt.xticks(np.arange(0, 280, 1))\n",
                "plt.yticks(ind,df['country'],  fontsize=56)\n",
                "\n",
                "plt.show()\n",
                "plt.clf()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN = pd.read_csv('path')"
            ],
            "source_orig": [
                "trainset = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
                "testset = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "def column_nan(missing):",
                "ASSIGN = []",
                "for i in missing:",
                "if i > 0:",
                "ASSIGN.append(i)",
                "print(len(ASSIGN))",
                "ASSIGN = trainset.isnull().sum().sort_values(ascending = False)",
                "ASSIGN = testset.isnull().sum().sort_values(ascending = False)"
            ],
            "source_orig": [
                "#build function to count caolumn that has missing value\n",
                "def column_nan(missing):\n",
                "    s = []\n",
                "    for i in missing:\n",
                "        if i > 0:\n",
                "            s.append(i)\n",
                "    print(len(s))\n",
                "train = trainset.isnull().sum().sort_values(ascending = False)\n",
                "test = testset.isnull().sum().sort_values(ascending = False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print()",
                "column_nan(train)",
                "print()",
                "column_nan(test)"
            ],
            "source_orig": [
                "print(\"Sum of column has Nan in trainset:\")\n",
                "column_nan(train)\n",
                "print(\"Sum of column has Nan in testset:\")\n",
                "column_nan(test)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "trainset.isnull().sum().sort_values(ascending = False).head(20)path(trainset)"
            ],
            "source_orig": [
                "#summing missing value trainset\n",
                "trainset.isnull().sum().sort_values(ascending = False).head(20)/len(trainset)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "testset.isnull().sum().sort_values(ascending = False).head(33)path(trainset)"
            ],
            "source_orig": [
                "#summing percentage missing value\n",
                "testset.isnull().sum().sort_values(ascending = False).head(33)/len(trainset)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = trainset.drop(['PoolQC','MiscFeature','Alley','Fence'], axis= 1)",
                "ASSIGN = testset.drop(['PoolQC','MiscFeature','Alley','Fence'], axis= 1)"
            ],
            "source_orig": [
                "#clean columns 'PoolQC','MiscFeature','Alley','Fence' because columns has missing values more than 80%\n",
                "cleaning_train = trainset.drop(['PoolQC','MiscFeature','Alley','Fence'], axis= 1)\n",
                "cleaning_test = testset.drop(['PoolQC','MiscFeature','Alley','Fence'], axis= 1)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = cleaning_train.isnull().sum(axis=0).reset_index().sort_values(0,ascending=False).head(15)",
                "ASSIGN.columns = ['variable','missing']",
                "ASSIGN = s['variable'].tolist()",
                "ASSIGN = cleaning_train[col_miss_train]",
                "ASSIGN.describe()"
            ],
            "source_orig": [
                "#cleaning trainig_set\n",
                "s = cleaning_train.isnull().sum(axis=0).reset_index().sort_values(0,ascending=False).head(15)\n",
                "s.columns = ['variable','missing']\n",
                "col_miss_train = s['variable'].tolist()\n",
                "miss = cleaning_train[col_miss_train]\n",
                "miss.describe()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = plt.subplots(1,3, figsize = (20,6))",
                "sns.distplot(miss['LotFrontage'], color = 'b',ax = axes[0])",
                "sns.distplot(miss['GarageYrBlt'], color = 'r', ax = axes[1])",
                "sns.distplot(miss['MasVnrArea'], color = 'y',ax = axes[2])"
            ],
            "source_orig": [
                "# see distribution\n",
                "fig,axes = plt.subplots(1,3, figsize = (20,6))\n",
                "sns.distplot(miss['LotFrontage'], color = 'b',ax = axes[0])\n",
                "sns.distplot(miss['GarageYrBlt'], color = 'r', ax = axes[1])\n",
                "sns.distplot(miss['MasVnrArea'], color = 'y',ax = axes[2])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']] = cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']].fillna(cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']].median())"
            ],
            "source_orig": [
                "# clean with median on numerical\n",
                "cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']] = cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']].fillna(cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']].median())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = cleaning_train.isnull().sum().sort_values(ascending = False).head(12).index.values.tolist()",
                "cleaning_train[ASSIGN] = cleaning_train[ASSIGN].fillna(cleaning_train[ASSIGN].mode().iloc[0])"
            ],
            "source_orig": [
                "# clean with mode on categorical\n",
                "list_miss = cleaning_train.isnull().sum().sort_values(ascending = False).head(12).index.values.tolist()\n",
                "cleaning_train[list_miss] = cleaning_train[list_miss].fillna(cleaning_train[list_miss].mode().iloc[0])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = cleaning_test.isnull().sum().sort_values(ascending = False).head(29).index.values.tolist()",
                "ASSIGN = cleaning_test[list_miss_test].describe().columns.values.tolist()",
                "cleaning_test[ASSIGN] = cleaning_test[ASSIGN].fillna(cleaning_test[ASSIGN].median())"
            ],
            "source_orig": [
                "# Cleaning testset in numerical\n",
                "list_miss_test = cleaning_test.isnull().sum().sort_values(ascending = False).head(29).index.values.tolist()\n",
                "list_numeric = cleaning_test[list_miss_test].describe().columns.values.tolist()\n",
                "# handling with median\n",
                "cleaning_test[list_numeric] = cleaning_test[list_numeric].fillna(cleaning_test[list_numeric].median())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = cleaning_test.isnull().sum().sort_values(ascending = False).head(18).index.values.tolist()",
                "cleaning_test[ASSIGN] = cleaning_test[ASSIGN].fillna(cleaning_test[ASSIGN].mode().iloc[0])"
            ],
            "source_orig": [
                "# cleaning testset in categorical\n",
                "lst_categ = cleaning_test.isnull().sum().sort_values(ascending = False).head(18).index.values.tolist()\n",
                "cleaning_test[lst_categ] = cleaning_test[lst_categ].fillna(cleaning_test[lst_categ].mode().iloc[0])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cleaning_train.isnull().sum().sort_values()"
            ],
            "source_orig": [
                "cleaning_train.isnull().sum().sort_values()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cleaning_test.isnull().sum().sort_values()"
            ],
            "source_orig": [
                "cleaning_test.isnull().sum().sort_values()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import pandas as pd\n",
                "import os\n",
                "from shutil import copyfile\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.image import imread\n",
                "import time\n",
                "\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense,Dropout,Activation\n",
                "\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import StratifiedShuffleSplit"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "CHECKPOINT",
                "def plot_img(img):",
                "plt.imshow(img, cmap='gray')",
                "plt.axis(\"off\")",
                "plt.show()",
                "def plot_img_by_id(id, species = ''):",
                "ASSIGN = '.path' + str(id) + '.jpg'",
                "ASSIGN = imread(src)",
                "plt.imshow(ASSIGN, cmap='gray')",
                "plt.suptitle('Predicted species: ' + species)",
                "plt.axis(\"off\")",
                "plt.show()",
                "def plot_img_by_species(species):",
                "ASSIGN = '.path' + str(species) + 'path'",
                "plt.figure(figsize=(28,28))",
                "ASSIGN = len(os.listdir(ldir)), 1",
                "ASSIGN = 1",
                "print(species)",
                "for d in os.listdir(ASSIGN):",
                "ASSIGN = ldir + d",
                "ASSIGN = imread(src)",
                "plt.subplot(y, x, ASSIGN)",
                "plt.imshow(ASSIGN, cmap='gray')",
                "plt.axis(\"off\")",
                "ASSIGN += 1",
                "plt.show()"
            ],
            "source_orig": [
                "def plot_img(img):\n",
                "    plt.imshow(img, cmap='gray')\n",
                "    plt.axis(\"off\")\n",
                "    plt.show()\n",
                "    \n",
                "def plot_img_by_id(id, species = ''):\n",
                "    src = './LeafClassification/' + str(id) + '.jpg'\n",
                "    img = imread(src)\n",
                "    plt.imshow(img, cmap='gray')\n",
                "    plt.suptitle('Predicted species: ' + species)\n",
                "    plt.axis(\"off\")\n",
                "    plt.show()\n",
                "    \n",
                "def plot_img_by_species(species):\n",
                "    ldir = './training_data/' + str(species) + '/'\n",
                "    plt.figure(figsize=(28,28))\n",
                "    #plt.suptitle('Predicted species: ' + species)\n",
                "    x, y = len(os.listdir(ldir)), 1\n",
                "     \n",
                "    i = 1\n",
                "    print(species)\n",
                "    for d in os.listdir(ldir):\n",
                "        src = ldir + d\n",
                "        img = imread(src)\n",
                "        \n",
                "        plt.subplot(y, x, i)\n",
                "        plt.imshow(img, cmap='gray')\n",
                "        plt.axis(\"off\")\n",
                "        i += 1\n",
                "            \n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "def reload_data():",
                "ASSIGN = pd.read_csv(\"train.csv\")",
                "ASSIGN = pd.read_csv(\"test.csv\")",
                "ASSIGN = [train_data, test_data]",
                "ASSIGN = pd.concat(ASSIGN, axis=0, sort=False)",
                "return train_data, test_data, df",
                "def load_pred():",
                "ASSIGN = pd.read_csv(\"predictions.csv\", index_col='id')",
                "return pred",
                "ASSIGN = reload_data()",
                "ASSIGN = reload_data()"
            ],
            "source_orig": [
                "# method to reload data\n",
                "def reload_data():\n",
                "    # Load test & train datasets\n",
                "    train_data = pd.read_csv(\"train.csv\")\n",
                "    test_data = pd.read_csv(\"test.csv\")\n",
                "    df = [train_data, test_data]\n",
                "    df = pd.concat(df, axis=0, sort=False)\n",
                "    \n",
                "    return train_data, test_data, df\n",
                "\n",
                "# load predictions\n",
                "def load_pred():\n",
                "    # id should be index\n",
                "    pred = pd.read_csv(\"predictions.csv\", index_col='id')\n",
                "    return pred\n",
                "\n",
                "train_data, test_data, df = reload_data()\n",
                "# for backup reason\n",
                "train_data_copy, test_data_copy, df = reload_data()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print(train_data.shape)",
                "train_data.describe()",
                "train_data.head()",
                "print(df.shape)",
                "df.describe()",
                "df.head()",
                "print(test_data.shape)",
                "test_data.describe()",
                "test_data.head()"
            ],
            "source_orig": [
                "print(train_data.shape)\n",
                "train_data.describe()\n",
                "train_data.head()\n",
                "\n",
                "print(df.shape)\n",
                "df.describe()\n",
                "df.head()\n",
                "\n",
                "print(test_data.shape)\n",
                "test_data.describe()\n",
                "test_data.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = train_data.pop('id')",
                "ASSIGN = test_data.pop('id')",
                "ASSIGN = train_data.pop('species')",
                "ASSIGN = LabelEncoder().fit(ASSIGN).transform(ASSIGN)",
                "ASSIGN = to_categorical(ASSIGN)",
                "ASSIGN = StandardScaler().fit(train_data).transform(train_data)",
                "ASSIGN = StandardScaler().fit(test_data).transform(test_data)"
            ],
            "source_orig": [
                "# remove column 'id' from train data und save in variable train_id\n",
                "train_id = train_data.pop('id')\n",
                "test_id = test_data.pop('id')\n",
                "\n",
                "# remove column 'species' from train data und save in variable train_y, then transform into categorical\n",
                "train_y = train_data.pop('species')\n",
                "train_y = LabelEncoder().fit(train_y).transform(train_y)\n",
                "train_y = to_categorical(train_y)\n",
                "\n",
                "#scale training data\n",
                "train_x = StandardScaler().fit(train_data).transform(train_data)\n",
                "test_x = StandardScaler().fit(test_data).transform(test_data)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = StratifiedShuffleSplit(n_splits=10, test_size=0.2,random_state=12345)",
                "ASSIGN = next(iter(sss.split(train_x, train_y)))",
                "ASSIGN = train_x[train_index], train_x[val_index]",
                "ASSIGN = train_y[train_index], train_y[val_index]",
                "print(,x_train.shape)",
                "print(,x_val.shape)"
            ],
            "source_orig": [
                "## retain class balances\n",
                "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2,random_state=12345)\n",
                "train_index, val_index = next(iter(sss.split(train_x, train_y)))\n",
                "x_train, x_val = train_x[train_index], train_x[val_index]\n",
                "y_train, y_val = train_y[train_index], train_y[val_index]\n",
                "print(\"x_train dim: \",x_train.shape)\n",
                "print(\"x_val dim:   \",x_val.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "ASSIGN = train_x.shape[1]",
                "ASSIGN = 128"
            ],
            "source_orig": [
                "input_dim = train_x.shape[1]\n",
                "EPOCHS = 100\n",
                "batch_size = 128"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = Sequential()",
                "ASSIGN.add(Dense(1024,input_dim=input_dim))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Activation('sigmoid'))",
                "ASSIGN.add(Dense(512))",
                "ASSIGN.add(Dropout(0.3))",
                "ASSIGN.add(Activation('sigmoid'))",
                "ASSIGN.add(Dense(99))",
                "ASSIGN.add(Activation('softmax'))"
            ],
            "source_orig": [
                "model = Sequential()\n",
                "model.add(Dense(1024,input_dim=input_dim))\n",
                "model.add(Dropout(0.2))\n",
                "model.add(Activation('sigmoid'))\n",
                "model.add(Dense(512))\n",
                "model.add(Dropout(0.3))\n",
                "model.add(Activation('sigmoid'))\n",
                "model.add(Dense(99))\n",
                "model.add(Activation('softmax'))"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])"
            ],
            "source_orig": [
                "# compile model\n",
                "model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = time.time()",
                "ASSIGN = model.fit(train_x,train_y,validation_data=(x_val, y_val),batch_size=batch_size,epoch=EPOCHS,verbose=0)",
                "ASSIGN = time.time()",
                "print(round((ASSIGN-ASSIGN),2), )"
            ],
            "source_orig": [
                "# fit model\n",
                "start = time.time()\n",
                "history = model.fit(train_x,train_y,validation_data=(x_val, y_val),batch_size=batch_size,epoch=EPOCHS,verbose=0)\n",
                "end = time.time()\n",
                "print(round((end-start),2), \"seconds\")"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "plt.plot(history.history['acc'])",
                "plt.plot(history.history['val_acc'])",
                "plt.ylabel('accuracy')",
                "plt.xlabel('epoch')",
                "plt.legend(['train', 'valid'], loc='lower right')",
                "plt.show()",
                "print('-'*50)",
                "print('Training accuracy: ' + str(max(history.history['acc'])))",
                "print('Validation accuracy: ' + str(max(history.history['val_acc'])))",
                "print('-'*50)",
                "plt.plot(history.history['loss'])",
                "plt.plot(history.history['val_loss'])",
                "plt.ylabel('loss')",
                "plt.xlabel('epoch')",
                "plt.legend(['train', 'valid'], loc='upper right')",
                "plt.show()",
                "print('-'*50)",
                "print('Training loss: ' + str(min(history.history['loss'])))",
                "print('Validation loss: ' + str(min(history.history['val_loss'])))",
                "print('-'*50)"
            ],
            "source_orig": [
                "plt.plot(history.history['acc'])\n",
                "plt.plot(history.history['val_acc'])\n",
                "#plt.title('model accuracy')\n",
                "plt.ylabel('accuracy')\n",
                "plt.xlabel('epoch')\n",
                "plt.legend(['train', 'valid'], loc='lower right')\n",
                "plt.show()\n",
                "\n",
                "print('-'*50)\n",
                "print('Training accuracy: ' + str(max(history.history['acc'])))\n",
                "print('Validation accuracy: ' + str(max(history.history['val_acc'])))\n",
                "print('-'*50)\n",
                "\n",
                "plt.plot(history.history['loss'])\n",
                "plt.plot(history.history['val_loss'])\n",
                "#plt.title('model loss')\n",
                "plt.ylabel('loss')\n",
                "plt.xlabel('epoch')\n",
                "plt.legend(['train', 'valid'], loc='upper right')\n",
                "plt.show()\n",
                "\n",
                "print('-'*50)\n",
                "print('Training loss: ' + str(min(history.history['loss'])))\n",
                "print('Validation loss: ' + str(min(history.history['val_loss'])))\n",
                "print('-'*50)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = model.predict_proba(test_x)"
            ],
            "source_orig": [
                "predict_y = model.predict_proba(test_x)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = train_data_copy.ASSIGN.unique()",
                "ASSIGN = pd.DataFrame(predict_y,index=test_id,columns=sorted(species))",
                "ASSIGN['predicted ASSIGN'] = ASSIGN.idxmax(axis=1)"
            ],
            "source_orig": [
                "species = train_data_copy.species.unique()\n",
                "predict_out = pd.DataFrame(predict_y,index=test_id,columns=sorted(species))\n",
                "predict_out['predicted species'] = predict_out.idxmax(axis=1)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "predict_out.head()"
            ],
            "source_orig": [
                "predict_out.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "model.save('.path')"
            ],
            "source_orig": [
                "#model.save_weights('./models/leaf_classification_weights_best.h5')\n",
                "model.save('./models/leaf_classification_model_best.h5')"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "ASSIGN = open('predictions_neuralnetwork_1.csv','w')",
                "ASSIGN.write(predict_out.to_csv())"
            ],
            "source_orig": [
                "# write file to csv\n",
                "fp = open('predictions_neuralnetwork_1.csv','w')\n",
                "fp.write(predict_out.to_csv())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "SETUP",
                "ASSIGN = os.listdir(INPUT_FOLDER)",
                "ASSIGN.sort()"
            ],
            "source_orig": [
                "%matplotlib inline\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import dicom\n",
                "import os\n",
                "import scipy.ndimage\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from skimage import measure, morphology\n",
                "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
                "\n",
                "# Some constants \n",
                "INPUT_FOLDER = '../input/sample_images/'\n",
                "patients = os.listdir(INPUT_FOLDER)\n",
                "patients.sort()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "ASSIGN = load_scan(INPUT_FOLDER + patients[0])",
                "ASSIGN = get_pixels_hu(first_patient)",
                "plt.hist(ASSIGN.flatten(), bins=180, color='c')",
                "plt.xlabel(\"Hounsfield Units (HU)\")",
                "plt.ylabel(\"Frequency\")",
                "plt.show()",
                "plt.imshow(ASSIGN[80], cmap=plt.cm.gray)",
                "plt.show()"
            ],
            "source_orig": [
                "first_patient = load_scan(INPUT_FOLDER + patients[0])\n",
                "first_patient_pixels = get_pixels_hu(first_patient)\n",
                "plt.hist(first_patient_pixels.flatten(), bins=180, color='c')\n",
                "plt.xlabel(\"Hounsfield Units (HU)\")\n",
                "plt.ylabel(\"Frequency\")\n",
                "plt.show()\n",
                "\n",
                "# Show some slice in the middle\n",
                "plt.imshow(first_patient_pixels[80], cmap=plt.cm.gray)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = resample(first_patient_pixels, first_patient, [1,1,1])",
                "print(, first_patient_pixels.shape)",
                "print(, pix_resampled.shape)"
            ],
            "source_orig": [
                "pix_resampled, spacing = resample(first_patient_pixels, first_patient, [1,1,1])\n",
                "print(\"Shape before resampling\\t\", first_patient_pixels.shape)\n",
                "print(\"Shape after resampling\\t\", pix_resampled.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def plot_3d(image, threshold=-300):",
                "ASSIGN = image.transpose(2,1,0)",
                "ASSIGN = measure.marching_cubes(p, threshold)",
                "ASSIGN = plt.figure(figsize=(10, 10))",
                "ASSIGN = fig.add_subplot(111, projection='3d')",
                "ASSIGN = Poly3DCollection(verts[faces], alpha=0.1)",
                "ASSIGN = [0.5, 0.5, 1]",
                "ASSIGN.set_facecolor(ASSIGN)",
                "ASSIGN.add_collection3d(ASSIGN)",
                "ASSIGN.set_xlim(0, ASSIGN.shape[0])",
                "ASSIGN.set_ylim(0, ASSIGN.shape[1])",
                "ASSIGN.set_zlim(0, ASSIGN.shape[2])",
                "plt.show()"
            ],
            "source_orig": [
                "def plot_3d(image, threshold=-300):\n",
                "    \n",
                "    # Position the scan upright, \n",
                "    # so the head of the patient would be at the top facing the camera\n",
                "    p = image.transpose(2,1,0)\n",
                "    \n",
                "    verts, faces = measure.marching_cubes(p, threshold)\n",
                "\n",
                "    fig = plt.figure(figsize=(10, 10))\n",
                "    ax = fig.add_subplot(111, projection='3d')\n",
                "\n",
                "    # Fancy indexing: `verts[faces]` to generate a collection of triangles\n",
                "    mesh = Poly3DCollection(verts[faces], alpha=0.1)\n",
                "    face_color = [0.5, 0.5, 1]\n",
                "    mesh.set_facecolor(face_color)\n",
                "    ax.add_collection3d(mesh)\n",
                "\n",
                "    ax.set_xlim(0, p.shape[0])\n",
                "    ax.set_ylim(0, p.shape[1])\n",
                "    ax.set_zlim(0, p.shape[2])\n",
                "\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = segment_lung_mask(pix_resampled, False)",
                "ASSIGN = segment_lung_mask(pix_resampled, True)"
            ],
            "source_orig": [
                "segmented_lungs = segment_lung_mask(pix_resampled, False)\n",
                "segmented_lungs_fill = segment_lung_mask(pix_resampled, True)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "MIN_BOUND = -1000.0\n",
                "MAX_BOUND = 400.0\n",
                "    \n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "PIXEL_MEAN = 0.25\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = '..path'",
                "ASSIGN = pd.read_csv(iowa_file_path)",
                "ASSIGN = home_data.SalePrice",
                "ASSIGN = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']",
                "ASSIGN = home_data[features]",
                "train_X, val_X, train_y, val_y = train_test_split(ASSIGN, ASSIGN, random_state=1)",
                "ASSIGN = DecisionTreeRegressor(random_state=1)",
                "ASSIGN.fit(train_X, train_y)",
                "ASSIGN = iowa_model.predict(val_X)",
                "ASSIGN = mean_absolute_error(val_predictions, val_y)",
                "print(.format(ASSIGN))",
                "ASSIGN = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)",
                "ASSIGN.fit(train_X, train_y)",
                "ASSIGN = iowa_model.predict(val_X)",
                "ASSIGN = mean_absolute_error(val_predictions, val_y)",
                "print(.format(ASSIGN))",
                "ASSIGN = RandomForestRegressor(random_state=1)",
                "ASSIGN.fit(train_X, train_y)",
                "ASSIGN = rf_model.predict(val_X)",
                "ASSIGN = mean_absolute_error(rf_val_predictions, val_y)",
                "print(.format(ASSIGN))"
            ],
            "source_orig": [
                "# Code you have previously used to load data\n",
                "import pandas as pd\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from learntools.core import *\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "\n",
                "# Path of the file to read. We changed the directory structure to simplify submitting to a competition\n",
                "iowa_file_path = '../input/train.csv'\n",
                "\n",
                "home_data = pd.read_csv(iowa_file_path)\n",
                "# Create target object and call it y\n",
                "y = home_data.SalePrice\n",
                "# Create X\n",
                "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
                "X = home_data[features]\n",
                "\n",
                "# Split into validation and training data\n",
                "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
                "\n",
                "# Specify Model\n",
                "iowa_model = DecisionTreeRegressor(random_state=1)\n",
                "# Fit Model\n",
                "iowa_model.fit(train_X, train_y)\n",
                "\n",
                "# Make validation predictions and calculate mean absolute error\n",
                "val_predictions = iowa_model.predict(val_X)\n",
                "val_mae = mean_absolute_error(val_predictions, val_y)\n",
                "print(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n",
                "\n",
                "# Using best value for max_leaf_nodes\n",
                "iowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\n",
                "iowa_model.fit(train_X, train_y)\n",
                "val_predictions = iowa_model.predict(val_X)\n",
                "val_mae = mean_absolute_error(val_predictions, val_y)\n",
                "print(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))\n",
                "\n",
                "# Define the model. Set random_state to 1\n",
                "rf_model = RandomForestRegressor(random_state=1)\n",
                "rf_model.fit(train_X, train_y)\n",
                "rf_val_predictions = rf_model.predict(val_X)\n",
                "rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\n",
                "\n",
                "print(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = RandomForestRegressor(random_state = 1)",
                "ASSIGN.fit(train_X, train_y)"
            ],
            "source_orig": [
                "# To improve accuracy, create a new Random Forest model which you will train on all training data\n",
                "rf_model_on_full_data = RandomForestRegressor(random_state = 1)\n",
                "\n",
                "# fit rf_model_on_full_data on all data from the training data\n",
                "rf_model_on_full_data.fit(train_X, train_y)\n"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results",
                "ingest_data"
            ],
            "source": [
                "ASSIGN = '..path'",
                "ASSIGN = pd.read_csv(test_data_path)",
                "ASSIGN = test_data[features]",
                "ASSIGN = np.nan_to_num(ASSIGN)",
                "ASSIGN = rf_model_on_full_data.predict(test_X)",
                "ASSIGN = pd.DataFrame({'Id': test_data.Id,",
                "'SalePrice': ASSIGN})",
                "ASSIGN.to_csv('submission.csv', index=False)"
            ],
            "source_orig": [
                "# path to file you will use for predictions\n",
                "test_data_path = '../input/test.csv'\n",
                "\n",
                "# read test data file using pandas\n",
                "test_data = pd.read_csv(test_data_path)\n",
                "\n",
                "# create test_X which comes from test_data but includes only the columns you used for prediction.\n",
                "# The list of columns is stored in a variable called features\n",
                "test_X = test_data[features]\n",
                "test_X = np.nan_to_num(test_X)\n",
                "\n",
                "# make predictions which we will submit. \n",
                "test_preds = rf_model_on_full_data.predict(test_X)\n",
                "\n",
                "# The lines below shows how to save predictions in format used for competition scoring\n",
                "# Just uncomment them.\n",
                "\n",
                "output = pd.DataFrame({'Id': test_data.Id,\n",
                "                       'SalePrice': test_preds})\n",
                "output.to_csv('submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "sys.path.append(os.path.abspath('.path'))"
            ],
            "source_orig": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import sys\n",
                "import os\n",
                "\n",
                "sys.path.append(os.path.abspath('./ivis-explain'))\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix, average_precision_score, roc_auc_score, classification_report, roc_curve, precision_recall_curve\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "from ivis import Ivis\n",
                "from ivis_explanations import LinearExplainer\n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN = data['Class']",
                "ASSIGN = data.drop(['Class','Time'], axis=1)"
            ],
            "source_orig": [
                "data = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\n",
                "Y = data['Class']\n",
                "X = data.drop(['Class','Time'], axis=1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, stratify=Y, test_size=0.8, random_state=1234)"
            ],
            "source_orig": [
                "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, stratify=Y, test_size=0.8, random_state=1234)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = StandardScaler().fit(train_X[['Amount']])",
                "train_X.loc[:, ['Amount']] = ASSIGN.transform(train_X[['Amount']])",
                "test_X.loc[:, ['Amount']] = ASSIGN.transform(test_X[['Amount']])",
                "ASSIGN = MinMaxScaler().fit(train_X)",
                "ASSIGN = minmax_scaler.transform(ASSIGN)",
                "ASSIGN = minmax_scaler.transform(ASSIGN)"
            ],
            "source_orig": [
                "# standard_scaler = StandardScaler().fit(train_X[['Time', 'Amount']])\n",
                "# train_X.loc[:, ['Time', 'Amount']] = standard_scaler.transform(train_X[['Time', 'Amount']])\n",
                "# test_X.loc[:, ['Time', 'Amount']] = standard_scaler.transform(test_X[['Time', 'Amount']])\n",
                "\n",
                "standard_scaler = StandardScaler().fit(train_X[['Amount']])\n",
                "train_X.loc[:, ['Amount']] = standard_scaler.transform(train_X[['Amount']])\n",
                "test_X.loc[:, ['Amount']] = standard_scaler.transform(test_X[['Amount']])\n",
                "minmax_scaler = MinMaxScaler().fit(train_X)\n",
                "train_X = minmax_scaler.transform(train_X)\n",
                "test_X = minmax_scaler.transform(test_X)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = Ivis(embedding_dims=2, model='maaten',",
                "ASSIGN=15, n_epochs_without_progress=5,",
                "ASSIGN=0.95,",
                "ASSIGN=0)",
                "ASSIGN.fit(train_X, train_Y.values)"
            ],
            "source_orig": [
                "ivis = Ivis(embedding_dims=2, model='maaten',\n",
                "            k=15, n_epochs_without_progress=5,\n",
                "            supervision_weight=0.95,\n",
                "            verbose=0)\n",
                "ivis.fit(train_X, train_Y.values)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "ivis.save_model('ivis-supervised-fraud', overwrite=True)"
            ],
            "source_orig": [
                "ivis.save_model('ivis-supervised-fraud', overwrite=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ivis.transform(train_X)",
                "ASSIGN = ivis.transform(test_X)"
            ],
            "source_orig": [
                "train_embeddings = ivis.transform(train_X)\n",
                "test_embeddings = ivis.transform(test_X)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = plt.subplots(1, 2, figsize=(17, 7), dpi=200)",
                "ax[0].scatter(x=train_embeddings[:, 0], y=train_embeddings[:, 1], c=train_Y, s=3, cmap='RdYlBu_r')",
                "ax[0].set_xlabel('ivis 1')",
                "ax[0].set_ylabel('ivis 2')",
                "ax[0].set_title('Training Set')",
                "ax[1].scatter(x=test_embeddings[:, 0], y=test_embeddings[:, 1], c=test_Y, s=3, cmap='RdYlBu_r')",
                "ax[1].set_xlabel('ivis 1')",
                "ax[1].set_ylabel('ivis 2')",
                "ax[1].set_title('Testing Set')"
            ],
            "source_orig": [
                "fig, ax = plt.subplots(1, 2, figsize=(17, 7), dpi=200)\n",
                "ax[0].scatter(x=train_embeddings[:, 0], y=train_embeddings[:, 1], c=train_Y, s=3, cmap='RdYlBu_r')\n",
                "ax[0].set_xlabel('ivis 1')\n",
                "ax[0].set_ylabel('ivis 2')\n",
                "ax[0].set_title('Training Set')\n",
                "\n",
                "ax[1].scatter(x=test_embeddings[:, 0], y=test_embeddings[:, 1], c=test_Y, s=3, cmap='RdYlBu_r')\n",
                "ax[1].set_xlabel('ivis 1')\n",
                "ax[1].set_ylabel('ivis 2')\n",
                "ax[1].set_title('Testing Set')"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = LogisticRegression(solver=\"lbfgs\").fit(train_embeddings, train_Y)"
            ],
            "source_orig": [
                "clf = LogisticRegression(solver=\"lbfgs\").fit(train_embeddings, train_Y)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = clf.predict(test_embeddings)",
                "ASSIGN = clf.predict_proba(test_embeddings)"
            ],
            "source_orig": [
                "labels = clf.predict(test_embeddings)\n",
                "proba = clf.predict_proba(test_embeddings)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "print(classification_report(test_Y, labels))",
                "print('Confusion Matrix')",
                "print(confusion_matrix(test_Y, labels))",
                "print('Average Precision: '+str(average_precision_score(test_Y, proba[:, 1])))",
                "print('ROC AUC: '+str(roc_auc_score(test_Y, labels)))"
            ],
            "source_orig": [
                "print(classification_report(test_Y, labels))\n",
                "\n",
                "print('Confusion Matrix')\n",
                "print(confusion_matrix(test_Y, labels))\n",
                "print('Average Precision: '+str(average_precision_score(test_Y, proba[:, 1])))\n",
                "print('ROC AUC: '+str(roc_auc_score(test_Y, labels)))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = proba[:, 1]",
                "ASSIGN = roc_curve(test_Y, pos_probs)",
                "plt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')",
                "plt.plot(fpr, tpr, marker='.', label='Logistic')",
                "plt.xlabel('False Positive Rate')",
                "plt.ylabel('True Positive Rate')",
                "plt.legend()",
                "plt.show()"
            ],
            "source_orig": [
                "# retrieve just the probabilities for the positive class\n",
                "pos_probs = proba[:, 1]\n",
                "\n",
                "# calculate roc curve for model\n",
                "fpr, tpr, thresholds = roc_curve(test_Y, pos_probs)\n",
                "\n",
                "# plot no skill roc curve\n",
                "plt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
                "# plot model roc curve\n",
                "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
                "# axis labels\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "# show the legend\n",
                "plt.legend()\n",
                "# show the plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = len(Y[Y==1]) path(Y)",
                "plt.plot([0, 1], [ASSIGN, ASSIGN], linestyle='--', label='No Skill')",
                "ASSIGN = precision_recall_curve(test_Y, pos_probs)",
                "plt.plot(recall, precision, marker='.', label='Logistic')",
                "plt.xlabel('Recall')",
                "plt.ylabel('Precision')",
                "plt.legend()",
                "plt.show()"
            ],
            "source_orig": [
                "# calculate the no skill line as the proportion of the positive class\n",
                "no_skill = len(Y[Y==1]) / len(Y)\n",
                "# plot the no skill precision-recall curve\n",
                "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
                "# calculate model precision-recall curve\n",
                "precision, recall, _ = precision_recall_curve(test_Y, pos_probs)\n",
                "# plot the model precision-recall curve\n",
                "plt.plot(recall, precision, marker='.', label='Logistic')\n",
                "# axis labels\n",
                "plt.xlabel('Recall')\n",
                "plt.ylabel('Precision')\n",
                "# show the legend\n",
                "plt.legend()\n",
                "# show the plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.hist(pos_probs, bins=100)",
                "plt.show()"
            ],
            "source_orig": [
                "# create a histogram of the predicted probabilities\n",
                "plt.hist(pos_probs, bins=100)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "warnings.filterwarnings('ignore')"
            ],
            "source_orig": [
                "import os\n",
                "\n",
                "\n",
                "import copy\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import cv2\n",
                "import keras\n",
                "from keras import backend as K\n",
                "from keras.models import Model, Sequential\n",
                "from keras.layers import Dense, Dropout, BatchNormalization, Flatten, Input\n",
                "from keras.layers import Conv2D, Activation, GlobalAveragePooling2D\n",
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "from keras.preprocessing.image import load_img, img_to_array\n",
                "from keras.applications.resnet50 import preprocess_input, ResNet50\n",
                "import matplotlib\n",
                "import matplotlib.pylab as plt\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import shap\n",
                "from sklearn.utils import shuffle\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "Images, Classes = get_images()",
                "Images.shape, Classes.shape"
            ],
            "source_orig": [
                "## get images / labels\n",
                "\n",
                "Images, Classes = get_images()\n",
                "\n",
                "Images.shape, Classes.shape"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = Images.shape[0]",
                "for target_cls in [0, 1, 2]:",
                "ASSIGN = np.where(Classes == target_cls)[0]",
                "ASSIGN = indices.shape[0]",
                "ASSIGN = class_to_label[target_cls]",
                "print(ASSIGN, ASSIGN, n_target_clspath)",
                "ASSIGN = 10",
                "ASSIGN = plt.subplots(ncols=n_cols, figsize=(25, 3))",
                "for i in range(ASSIGN):",
                "axs[i].imshow(np.uint8(Images[ASSIGN[i]]))",
                "axs[i].axis('off')",
                "axs[i].set_title(ASSIGN)",
                "plt.show()"
            ],
            "source_orig": [
                "## visualize some images / labels\n",
                "\n",
                "n_total_images = Images.shape[0]\n",
                "\n",
                "for target_cls in [0, 1, 2]:\n",
                "    \n",
                "    indices = np.where(Classes == target_cls)[0] # get target class indices on Images / Classes\n",
                "    n_target_cls = indices.shape[0]\n",
                "    label = class_to_label[target_cls]\n",
                "    print(label, n_target_cls, n_target_cls/n_total_images)\n",
                "\n",
                "    n_cols = 10 # # of sample plot\n",
                "    fig, axs = plt.subplots(ncols=n_cols, figsize=(25, 3))\n",
                "\n",
                "    for i in range(n_cols):\n",
                "\n",
                "        axs[i].imshow(np.uint8(Images[indices[i]]))\n",
                "        axs[i].axis('off')\n",
                "        axs[i].set_title(label)\n",
                "\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = train_test_split(list(range(Images.shape[0])), train_size=0.8, test_size=0.2, shuffle=False)",
                "ASSIGN = Images[indices_train]",
                "ASSIGN = Classes[indices_train]",
                "ASSIGN = Images[indices_test]",
                "ASSIGN = Classes[indices_test]",
                "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
            ],
            "source_orig": [
                "## split train / test\n",
                "\n",
                "indices_train, indices_test = train_test_split(list(range(Images.shape[0])), train_size=0.8, test_size=0.2, shuffle=False)\n",
                "\n",
                "x_train = Images[indices_train]\n",
                "y_train = Classes[indices_train]\n",
                "x_test = Images[indices_test]\n",
                "y_test = Classes[indices_test]\n",
                "\n",
                "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = keras.utils.to_categorical(ASSIGN, n_classes)",
                "ASSIGN = keras.utils.to_categorical(ASSIGN, n_classes)",
                "y_train.shape, y_test.shape"
            ],
            "source_orig": [
                "## to one-hot\n",
                "\n",
                "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
                "y_test = keras.utils.to_categorical(y_test, n_classes)\n",
                "\n",
                "y_train.shape, y_test.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "SETUP",
                "def build_model():",
                "\"\"\"build model function\"\"\"",
                "ASSIGN = Input(shape=(W, H, 3))",
                "ASSIGN = MobileNetV2(",
                "ASSIGN=False,",
                "ASSIGN='imagenet',",
                "ASSIGN=ASSIGN,",
                ")",
                "ASSIGN = Sequential()",
                "ASSIGN.add(GlobalAveragePooling2D())",
                "ASSIGN.add(Dense(n_classes, activation='softmax'))",
                "ASSIGN = Model(input=densenet_121.input, output=top_model(densenet_121.output))",
                "for layer in ASSIGN.layers[:-11]:",
                "layer.trainable = False or isinstance(layer, BatchNormalization)",
                "ASSIGN.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])",
                "return model"
            ],
            "source_orig": [
                "import keras\n",
                "import tensorflow as tf\n",
                "from keras.applications.mobilenet_v2 import MobileNetV2\n",
                "\n",
                "def build_model():\n",
                "    \"\"\"build model function\"\"\"\n",
                "    \n",
                "    # Resnet\n",
                "    input_tensor = Input(shape=(W, H, 3)) # To change input shape\n",
                "    densenet_121 = MobileNetV2(\n",
                "        include_top=False,                # To change output shape\n",
                "        weights='imagenet',               # Use pre-trained model\n",
                "        input_tensor=input_tensor,        # Change input shape for this task\n",
                "    )\n",
                "    \n",
                "    # fc layer\n",
                "    top_model = Sequential()\n",
                "    top_model.add(GlobalAveragePooling2D())               # Add GAP for cam\n",
                "    top_model.add(Dense(n_classes, activation='softmax')) # Change output shape for this task\n",
                "    \n",
                "    # model\n",
                "    model = Model(input=densenet_121.input, output=top_model(densenet_121.output))\n",
                "    \n",
                "    # frozen weights\n",
                "    for layer in model.layers[:-11]:\n",
                "        layer.trainable = False or isinstance(layer, BatchNormalization) # If Batch Normalization layer, it should be trainable\n",
                "        \n",
                "    # compile\n",
                "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
                "    \n",
                "    return model"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = build_model()"
            ],
            "source_orig": [
                "model = build_model()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "model.summary()"
            ],
            "source_orig": [
                "model.summary()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = 32",
                "ASSIGN = model.fit_generator(",
                "datagen_train.flow(x_train, y_train, ASSIGN=32),",
                "ASSIGN= 50,",
                "ASSIGN=datagen_test.flow(x_test, y_test, batch_size=32),",
                "ASSIGN = 1,",
                "ASSIGN= int(len(x_train)path),",
                "ASSIGN= int(len(x_test)path)",
                ")"
            ],
            "source_orig": [
                "## finetuning\n",
                "\n",
                "batch_size = 32\n",
                "\n",
                "history = model.fit_generator(\n",
                "    datagen_train.flow(x_train, y_train, batch_size=32),\n",
                "    epochs= 50,\n",
                "    validation_data=datagen_test.flow(x_test, y_test, batch_size=32),\n",
                "    verbose = 1,\n",
                "    #callbacks=callbacks,\n",
                "    steps_per_epoch=  int(len(x_train)//batch_size),\n",
                "    validation_steps= int(len(x_test)// batch_size)\n",
                ")"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = preprocess_input(copy.deepcopy(x_test))",
                "ASSIGN = model.predict(x)",
                "ASSIGN = np.argmax(ASSIGN, axis=1)",
                "ASSIGN = np.argmax(y_test, axis=1)",
                "ASSIGN = confusion_matrix(y_trues, y_preds)",
                "ASSIGN = plt.subplots(figsize=(7, 6))",
                "sns.heatmap(ASSIGN, annot=True, fmt='d', cmap='Blues', cbar_kws={'shrink': .3}, linewidths=.1, ax=ax)",
                "ax.set(",
                "ASSIGN=list(label_to_class.keys()),",
                "ASSIGN=list(label_to_class.keys()),",
                "ASSIGN='confusion matrix',",
                "ASSIGN='True label',",
                "ASSIGN='Predicted label'",
                ")",
                "ASSIGN = dict(rotation=45, ha='center', rotation_mode='anchor')",
                "plt.setp(ax.get_yticklabels(), **ASSIGN)",
                "plt.setp(ax.get_xticklabels(), **ASSIGN)",
                "plt.show()"
            ],
            "source_orig": [
                "## plot confusion matrix\n",
                "\n",
                "x = preprocess_input(copy.deepcopy(x_test))\n",
                "y_preds = model.predict(x)\n",
                "y_preds = np.argmax(y_preds, axis=1)\n",
                "y_trues = np.argmax(y_test, axis=1)\n",
                "cm = confusion_matrix(y_trues, y_preds)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(7, 6))\n",
                "\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'shrink': .3}, linewidths=.1, ax=ax)\n",
                "\n",
                "ax.set(\n",
                "    xticklabels=list(label_to_class.keys()),\n",
                "    yticklabels=list(label_to_class.keys()),\n",
                "    title='confusion matrix',\n",
                "    ylabel='True label',\n",
                "    xlabel='Predicted label'\n",
                ")\n",
                "params = dict(rotation=45, ha='center', rotation_mode='anchor')\n",
                "plt.setp(ax.get_yticklabels(), **params)\n",
                "plt.setp(ax.get_xticklabels(), **params)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def superimpose(img, cam):",
                "\"\"\"superimpose original image and cam heatmap\"\"\"",
                "ASSIGN = cv2.resize(cam, (img.shape[1], img.shape[0]))",
                "ASSIGN = np.uint8(255 * ASSIGN)",
                "ASSIGN = cv2.applyColorMap(ASSIGN, cv2.COLORMAP_JET)",
                "ASSIGN = heatmap * .45 + img * 1.2",
                "ASSIGN = np.minimum(ASSIGN, 255.0).astype(np.uint8)",
                "ASSIGN = cv2.cvtColor(ASSIGN, cv2.COLOR_BGR2RGB)",
                "return img, heatmap, superimposed_img"
            ],
            "source_orig": [
                "def superimpose(img, cam):\n",
                "    \"\"\"superimpose original image and cam heatmap\"\"\"\n",
                "    \n",
                "    heatmap = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
                "    heatmap = np.uint8(255 * heatmap)\n",
                "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
                "\n",
                "    superimposed_img = heatmap * .45 + img * 1.2\n",
                "    superimposed_img = np.minimum(superimposed_img, 255.0).astype(np.uint8)  # scale 0 to 255  \n",
                "    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n",
                "    \n",
                "    return img, heatmap, superimposed_img"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def _plot(model, cam_func, img, cls_true):",
                "\"\"\"plot original image, heatmap from cam and superimpose image\"\"\"",
                "ASSIGN = np.expand_dims(img, axis=0)",
                "ASSIGN = preprocess_input(copy.deepcopy(ASSIGN))",
                "ASSIGN = np.uint8(ASSIGN)",
                "ASSIGN = cam_func(model=model, x=x, layer_name=model.layers[-2].name)",
                "ASSIGN = superimpose(img, cam)",
                "ASSIGN = plt.subplots(ncols=2, figsize=(8, 6))",
                "axs[0].imshow(ASSIGN)",
                "axs[0].axis('off')",
                "axs[1].imshow(superimposed_img)",
                "axs[1].axis('off')",
                "plt.suptitle('True label: ' + class_to_label[cls_true] + ' path: ' + class_to_label[cls_pred])",
                "plt.tight_layout()",
                "plt.show()"
            ],
            "source_orig": [
                "def _plot(model, cam_func, img, cls_true):\n",
                "    \"\"\"plot original image, heatmap from cam and superimpose image\"\"\"\n",
                "    \n",
                "    # for cam\n",
                "    x = np.expand_dims(img, axis=0)\n",
                "    x = preprocess_input(copy.deepcopy(x))\n",
                "\n",
                "    # for superimpose\n",
                "    img = np.uint8(img)\n",
                "\n",
                "    # cam / superimpose\n",
                "    cls_pred, cam = cam_func(model=model, x=x, layer_name=model.layers[-2].name)\n",
                "    img, heatmap, superimposed_img = superimpose(img, cam)\n",
                "\n",
                "    fig, axs = plt.subplots(ncols=2, figsize=(8, 6))\n",
                "\n",
                "    axs[0].imshow(img)\n",
                "    #axs[0].set_title('True label: ' + class_to_label[cls_true] + ' / Predicted label : ' + class_to_label[cls_pred])\n",
                "    axs[0].axis('off')\n",
                "\n",
                "    #axs[1].imshow(heatmap)\n",
                "    #axs[1].set_title('heatmap')\n",
                "    #axs[1].axis('off')\n",
                "\n",
                "    axs[1].imshow(superimposed_img)\n",
                "    #axs[1].set_title(class_to_label[cls_true])\n",
                "    axs[1].axis('off')\n",
                "\n",
                "    plt.suptitle('True label: ' + class_to_label[cls_true] + ' / Predicted label : ' + class_to_label[cls_pred])\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    #fig.savefig(\"colon_aca_prewitt.jpeg\",bbox_inches='tight', pad_inches=0)\n",
                "    \n",
                "    "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "for i in range(200):",
                "_plot(model=model, cam_func=grad_cam, img=Images[i], cls_true=Classes[i])"
            ],
            "source_orig": [
                "for i in range(200):\n",
                "\n",
                "    _plot(model=model, cam_func=grad_cam, img=Images[i], cls_true=Classes[i])\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "sns.set()"
            ],
            "source_orig": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "sns.set()\n",
                "from PIL import Image\n",
                "from glob import glob\n",
                "from skimage.io import imread\n",
                "from os import listdir\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "import time\n",
                "import cv2\n",
                "import copy\n",
                "from random import shuffle\n",
                "from tqdm import tqdm_notebook as tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from keras.utils.np_utils import to_categorical\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import classification_report\n",
                "from sklearn.metrics import plot_roc_curve\n",
                "from sklearn.metrics import precision_recall_fscore_support\n",
                "from imblearn.metrics import sensitivity_specificity_support\n",
                "from imgaug import augmenters as iaa\n",
                "import imgaug as ia\n",
                "\n",
                "\n",
                "# import numpy as np\n",
                "# import matplotlib.pyplot as plt\n",
                "from itertools import cycle\n",
                "\n",
                "# from sklearn import svm, datasets\n",
                "from sklearn.metrics import roc_curve, auc\n",
                "# from sklearn.model_selection import train_test_split\n",
                "# from sklearn.preprocessing import label_binarize\n",
                "# from sklearn.multiclass import OneVsRestClassifier\n",
                "from scipy import interp\n",
                "from sklearn.metrics import roc_auc_score\n",
                "\n",
                "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
                "from keras.utils.vis_utils import plot_model\n",
                "from keras.optimizers import SGD,Adam\n",
                "import numpy as np\n",
                "from keras.applications.vgg16 import VGG16\n",
                "from keras.layers import Dense, GlobalAveragePooling2D, Dropout,concatenate\n",
                "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, BatchNormalization\n",
                "from keras.layers import Input\n",
                "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard,CSVLogger\n",
                "# import tools\n",
                "import gc\n",
                "from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix\n",
                "from keras.models import Model\n",
                "import keras\n",
                "# import channel_attention"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = os.listdir(\"..path\")",
                "print(ASSIGN)"
            ],
            "source_orig": [
                "folder = os.listdir(\"../input/lung-colon-sobel/trainable_sobel\")\n",
                "print(folder)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = \"..path\"",
                "ASSIGN = 0",
                "ASSIGN =[]",
                "for n in range(len(folder)):",
                "ASSIGN = os.path.join(base_path, folder[n])",
                "print(ASSIGN)",
                "ASSIGN = os.listdir(image_path)",
                "print(len(ASSIGN))",
                "ASSIGN.append(len(ASSIGN))",
                "ASSIGN += len(ASSIGN)",
                "print(.format(ASSIGN))",
                "print(ASSIGN)"
            ],
            "source_orig": [
                "base_path = \"../input/lung-colon-sobel/trainable_sobel\"\n",
                "total_images = 0\n",
                "image_class =[]\n",
                "for n in range(len(folder)):\n",
                "  image_path = os.path.join(base_path, folder[n]) \n",
                "  print(image_path)\n",
                "  # class_path = patient_path + \"/\" + str(c) + \"/\"\n",
                "  subfiles = os.listdir(image_path)\n",
                "  print(len(subfiles))\n",
                "  image_class.append(len(subfiles))\n",
                "  total_images += len(subfiles)\n",
                "print(\"The number of total images are:{}\".format(total_images))  \n",
                "print(image_class)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame(index=np.arange(0, total_images), columns=[\"path\", \"target\"])",
                "ASSIGN = 0",
                "for n in range(len(folder)):",
                "ASSIGN = folder[n]",
                "ASSIGN = os.path.join(base_path,class_id)",
                "ASSIGN = os.listdir(final_path)",
                "for m in range(len(ASSIGN)):",
                "ASSIGN = subfiles[m]",
                "ASSIGN.iloc[ASSIGN][\"path\"] = os.path.join(ASSIGN,ASSIGN)",
                "ASSIGN.iloc[ASSIGN][\"target\"] = ASSIGN",
                "ASSIGN += 1",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "data = pd.DataFrame(index=np.arange(0, total_images), columns=[\"path\", \"target\"])\n",
                "\n",
                "k = 0\n",
                "for n in range(len(folder)):\n",
                "    class_id = folder[n]\n",
                "    final_path = os.path.join(base_path,class_id) \n",
                "    subfiles = os.listdir(final_path)\n",
                "    for m in range(len(subfiles)):\n",
                "      image_path = subfiles[m]\n",
                "      data.iloc[k][\"path\"] = os.path.join(final_path,image_path)\n",
                "      data.iloc[k][\"target\"] = class_id\n",
                "      k += 1  \n",
                "\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data['target'].unique()"
            ],
            "source_orig": [
                "data['target'].unique()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = LabelEncoder()",
                "data['target_label'] = ASSIGN.fit_transform(data['target'])",
                "ASSIGN = ASSIGN.sample(frac=1).reset_index(drop=True)",
                "data"
            ],
            "source_orig": [
                "# creating instance of labelencoder\n",
                "labelencoder = LabelEncoder()\n",
                "# Assigning numerical values and storing in another column\n",
                "data['target_label'] = labelencoder.fit_transform(data['target'])\n",
                "data = data.sample(frac=1).reset_index(drop=True)\n",
                "data"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = plt.subplots(1,2,figsize=(20,5))",
                "sns.distplot(data.groupby(\"target_label\").size(), ax=ax[0], color=\"Orange\", kde=False)",
                "ax[0].set_xlabel(\"Number of images\")",
                "ax[0].set_ylabel(\"Frequency\");",
                "sns.countplot(data.target, palette=\"Set2\", ax=ax[1]);",
                "ax[1].set_xlabel(\"Names of Class\")",
                "ax[1].set_title(\"Data Distribution\");"
            ],
            "source_orig": [
                "# cancer_perc = data.groupby(\"patient_id\").target.value_counts()/ data.groupby(\"patient_id\").target.size()\n",
                "# cancer_perc = cancer_perc.unstack()\n",
                "\n",
                "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
                "sns.distplot(data.groupby(\"target_label\").size(), ax=ax[0], color=\"Orange\", kde=False)\n",
                "ax[0].set_xlabel(\"Number of images\")\n",
                "ax[0].set_ylabel(\"Frequency\");\n",
                "\n",
                "sns.countplot(data.target, palette=\"Set2\", ax=ax[1]);\n",
                "ax[1].set_xlabel(\"Names of Class\")\n",
                "ax[1].set_title(\"Data Distribution\");"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data.path",
                "ASSIGN = data.target_label",
                "X_train, X_test_sub ,y_train,y_test_sub= train_test_split(ASSIGN,ASSIGN, test_size=0.3, random_state=0,shuffle = True)",
                "print(X_train.shape)",
                "print(X_test_sub.shape)"
            ],
            "source_orig": [
                "X = data.path\n",
                "y = data.target_label\n",
                "X_train, X_test_sub ,y_train,y_test_sub= train_test_split(X,y, test_size=0.3, random_state=0,shuffle = True)\n",
                "print(X_train.shape)\n",
                "print(X_test_sub.shape)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "X_test,X_valid,y_test,y_valid = train_test_split(X_test_sub, y_test_sub, test_size=0.5, random_state=0 , shuffle =False)",
                "print(X_test.shape)",
                "print(X_valid.shape)"
            ],
            "source_orig": [
                "X_test,X_valid,y_test,y_valid = train_test_split(X_test_sub, y_test_sub, test_size=0.5, random_state=0 , shuffle =False)\n",
                "print(X_test.shape)\n",
                "print(X_valid.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = plt.subplots(1,3,figsize=(20,5))",
                "sns.countplot(y_train, ax=ax[0], palette=\"Reds\")",
                "ax[0].set_title(\"Train data\")",
                "sns.countplot(y_valid, ax=ax[1], palette=\"Blues\")",
                "ax[1].set_title(\"Dev data\")",
                "sns.countplot(y_test, ax=ax[2], palette=\"Greens\");",
                "ax[2].set_title(\"Test data\");"
            ],
            "source_orig": [
                "fig, ax = plt.subplots(1,3,figsize=(20,5))\n",
                "sns.countplot(y_train, ax=ax[0], palette=\"Reds\")\n",
                "ax[0].set_title(\"Train data\")\n",
                "sns.countplot(y_valid, ax=ax[1], palette=\"Blues\")\n",
                "ax[1].set_title(\"Dev data\")\n",
                "sns.countplot(y_test, ax=ax[2], palette=\"Greens\");\n",
                "ax[2].set_title(\"Test data\");"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "data.path.values"
            ],
            "source_orig": [
                "data.path.values"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = {k:v for k,v in zip(data.path.values,data.target_label.values)}"
            ],
            "source_orig": [
                "target_label_map = {k:v for k,v in zip(data.path.values,data.target_label.values)}"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "def squeeze_excitation_layer(x, out_dim, ratio = 4, concate = True):",
                "'''",
                "SE module performs inter-channel weighting.",
                "'''",
                "ASSIGN = GlobalAveragePooling2D()(x)",
                "ASSIGN = Dense(units=out_dim path)(squeeze)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Dense(units=out_dim)(ASSIGN)",
                "ASSIGN = Activation('sigmoid')(ASSIGN)",
                "print(ASSIGN.shape)",
                "ASSIGN = Reshape((1, 1, out_dim))(ASSIGN)",
                "ASSIGN = multiply([x, excitation])",
                "if concate:",
                "ASSIGN = concatenate([ASSIGN, x],axis=3)",
                "return scale"
            ],
            "source_orig": [
                "from keras.layers import Activation, Reshape, Lambda, dot, add\n",
                "from keras.layers import Conv1D, Conv2D, Conv3D\n",
                "from keras.layers import MaxPool1D,GlobalAveragePooling2D,Dense,multiply,Activation,concatenate\n",
                "from keras import backend as K\n",
                "\n",
                "\n",
                "def squeeze_excitation_layer(x, out_dim, ratio = 4, concate = True):\n",
                "    '''\n",
                "    SE module performs inter-channel weighting.\n",
                "    '''\n",
                "    squeeze = GlobalAveragePooling2D()(x)\n",
                "\n",
                "    excitation = Dense(units=out_dim //ratio)(squeeze)\n",
                "    excitation = Activation('relu')(excitation)\n",
                "    excitation = Dense(units=out_dim)(excitation)\n",
                "    excitation = Activation('sigmoid')(excitation)\n",
                "    print(excitation.shape)\n",
                "    excitation = Reshape((1, 1, out_dim))(excitation)\n",
                "\n",
                "    scale = multiply([x, excitation])\n",
                "\n",
                "    if concate:\n",
                "        scale = concatenate([scale, x],axis=3)\n",
                "    return scale"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0009)",
                "ASSIGN = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)",
                "ASSIGN = Input(shape=(224,224, 3))",
                "ASSIGN = ResNet50(input_tensor= input_tensor, weights='imagenet', include_top=False)",
                "ASSIGN = base_model.output",
                "print(ASSIGN.shape)",
                "ASSIGN = squeeze_excitation_layer(base_output, 2048, ratio=4, concate=False)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = concatenate([base_output, ASSIGN], axis=3)",
                "ASSIGN = GlobalAveragePooling2D()(x)",
                "ASSIGN = Flatten()(ASSIGN)",
                "ASSIGN = concatenate([gap,ASSIGN])",
                "ASSIGN = Dense(512, activation='relu')(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Dense(512, activation='relu')(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Dense(5, activation='softmax')(x)",
                "ASSIGN = Model(inputs=input_tensor, outputs=predict)",
                "for layer in (ASSIGN.layers):",
                "layer.trainable = False",
                "ASSIGN.compile(optimizer=ASSIGN,",
                "ASSIGN='categorical_crossentropy',",
                "ASSIGN=[keras.ASSIGN.categorical_accuracy])"
            ],
            "source_orig": [
                "\n",
                "\n",
                "adam = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0009)\n",
                "sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
                "\n",
                "input_tensor = Input(shape=(224,224, 3))\n",
                "#backbone\n",
                "base_model = ResNet50(input_tensor= input_tensor, weights='imagenet', include_top=False)\n",
                "base_output = base_model.output\n",
                "print(base_output.shape)\n",
                "# channel-attention\n",
                "x = squeeze_excitation_layer(base_output, 2048, ratio=4, concate=False)\n",
                "x = BatchNormalization()(x)\n",
                "\n",
                "# #concat\n",
                "x = concatenate([base_output, x], axis=3)\n",
                "# spp\n",
                "\n",
                "gap = GlobalAveragePooling2D()(x)\n",
                "x = Flatten()(x)\n",
                "x = concatenate([gap,x])\n",
                "x = Dense(512, activation='relu')(x)\n",
                "x = BatchNormalization()(x)\n",
                "x = Dense(512, activation='relu')(x)\n",
                "x = BatchNormalization()(x)\n",
                "predict = Dense(5, activation='softmax')(x)\n",
                "model = Model(inputs=input_tensor, outputs=predict)\n",
                "\n",
                "for layer in (base_model.layers):\n",
                "    layer.trainable = False\n",
                "\n",
                "model.compile(optimizer=adam,\n",
                "                      loss='categorical_crossentropy',\n",
                "                      metrics=[keras.metrics.categorical_accuracy])    \n",
                "\n",
                "# for l in model.layers:\n",
                "#   print(l.name)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "model.summary()"
            ],
            "source_orig": [
                "\n",
                "model.summary()\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN=32",
                "ASSIGN = model.fit_generator(",
                "data_gen(X_train, target_label_map, ASSIGN, augment=True),",
                "ASSIGN=data_gen(X_valid, target_label_map, batch_size),",
                "ASSIGN=50,",
                "ASSIGN = 1,",
                "ASSIGN= int(len(X_train)path),",
                "ASSIGN= int(len(X_valid)path)",
                ")"
            ],
            "source_orig": [
                "batch_size=32\n",
                "history = model.fit_generator(\n",
                "    data_gen(X_train, target_label_map, batch_size, augment=True),\n",
                "    validation_data=data_gen(X_valid, target_label_map, batch_size),\n",
                "    epochs=50, \n",
                "    verbose = 1,\n",
                "    #callbacks=callbacks,\n",
                "    steps_per_epoch=  int(len(X_train)//batch_size),\n",
                "    validation_steps= int(len(X_valid)// batch_size)\n",
                ")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "ASSIGN = '.path'"
            ],
            "source_orig": [
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential, save_model, load_model\n",
                "\n",
                "filepath = './'"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "tf.keras.models.save_model(",
                "model,",
                "filepath,",
                "ASSIGN=True,",
                "ASSIGN=True,",
                "ASSIGN=None,",
                "ASSIGN=None,",
                "ASSIGN=None",
                ")"
            ],
            "source_orig": [
                "tf.keras.models.save_model(\n",
                "    model,\n",
                "    filepath,\n",
                "    overwrite=True,\n",
                "    include_optimizer=True,\n",
                "    save_format=None,\n",
                "    signatures=None,\n",
                "    options=None\n",
                ")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "sns.set()"
            ],
            "source_orig": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "sns.set()\n",
                "from PIL import Image\n",
                "from glob import glob\n",
                "from skimage.io import imread\n",
                "from os import listdir\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "import time\n",
                "import cv2\n",
                "import copy\n",
                "from random import shuffle\n",
                "from tqdm import tqdm_notebook as tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from keras.utils.np_utils import to_categorical\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import classification_report\n",
                "from sklearn.metrics import plot_roc_curve\n",
                "from sklearn.metrics import precision_recall_fscore_support\n",
                "from imblearn.metrics import sensitivity_specificity_support\n",
                "from imgaug import augmenters as iaa\n",
                "import imgaug as ia\n",
                "import tensorflow as tf\n",
                "\n",
                "# import numpy as np\n",
                "# import matplotlib.pyplot as plt\n",
                "from itertools import cycle\n",
                "\n",
                "# from sklearn import svm, datasets\n",
                "from sklearn.metrics import roc_curve, auc\n",
                "# from sklearn.model_selection import train_test_split\n",
                "# from sklearn.preprocessing import label_binarize\n",
                "# from sklearn.multiclass import OneVsRestClassifier\n",
                "from scipy import interp\n",
                "from sklearn.metrics import roc_auc_score\n",
                "\n",
                "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
                "from keras.utils.vis_utils import plot_model\n",
                "from keras.optimizers import SGD,Adam\n",
                "import numpy as np\n",
                "\n",
                "# from keras.applications.vgg16 import VGG16\n",
                "from keras.layers import Dense, GlobalAveragePooling2D, Dropout,concatenate\n",
                "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, BatchNormalization\n",
                "from keras.layers import Input\n",
                "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard,CSVLogger\n",
                "# import tools\n",
                "import gc\n",
                "from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix\n",
                "from keras.models import Model\n",
                "import keras\n",
                "# import channel_attention"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = os.listdir(\"..path\")",
                "print(ASSIGN)"
            ],
            "source_orig": [
                "folder = os.listdir(\"../input/lung-colon-normal/trainable_normal\")\n",
                "print(folder)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = \"..path\"",
                "ASSIGN = 0",
                "ASSIGN =[]",
                "for n in range(len(folder)):",
                "ASSIGN = os.path.join(base_path, folder[n])",
                "print(ASSIGN)",
                "ASSIGN = os.listdir(image_path)",
                "print(len(ASSIGN))",
                "ASSIGN.append(len(ASSIGN))",
                "ASSIGN += len(ASSIGN)",
                "print(.format(ASSIGN))",
                "print(ASSIGN)"
            ],
            "source_orig": [
                "base_path = \"../input/lung-colon-normal/trainable_normal\"\n",
                "total_images = 0\n",
                "image_class =[]\n",
                "for n in range(len(folder)):\n",
                "  image_path = os.path.join(base_path, folder[n]) \n",
                "  print(image_path)\n",
                "  # class_path = patient_path + \"/\" + str(c) + \"/\"\n",
                "  subfiles = os.listdir(image_path)\n",
                "  print(len(subfiles))\n",
                "  image_class.append(len(subfiles))\n",
                "  total_images += len(subfiles)\n",
                "print(\"The number of total images are:{}\".format(total_images))  \n",
                "print(image_class)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "ASSIGN = pd.DataFrame(index=np.arange(0, total_images), columns=[\"path\", \"target\"])",
                "ASSIGN = 0",
                "for n in range(len(folder)):",
                "ASSIGN = folder[n]",
                "ASSIGN = os.path.join(base_path,class_id)",
                "ASSIGN = os.listdir(final_path)",
                "for m in range(len(ASSIGN)):",
                "ASSIGN = subfiles[m]",
                "ASSIGN.iloc[ASSIGN][\"path\"] = os.path.join(ASSIGN,ASSIGN)",
                "ASSIGN.iloc[ASSIGN][\"target\"] = ASSIGN",
                "ASSIGN += 1",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "data = pd.DataFrame(index=np.arange(0, total_images), columns=[\"path\", \"target\"])\n",
                "\n",
                "k = 0\n",
                "for n in range(len(folder)):\n",
                "    class_id = folder[n]\n",
                "    final_path = os.path.join(base_path,class_id) \n",
                "    subfiles = os.listdir(final_path)\n",
                "    for m in range(len(subfiles)):\n",
                "      image_path = subfiles[m]\n",
                "      data.iloc[k][\"path\"] = os.path.join(final_path,image_path)\n",
                "      data.iloc[k][\"target\"] = class_id\n",
                "      k += 1  \n",
                "\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data['target'].unique()"
            ],
            "source_orig": [
                "data['target'].unique()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = LabelEncoder()",
                "data['target_label'] = ASSIGN.fit_transform(data['target'])",
                "ASSIGN = ASSIGN.sample(frac=1).reset_index(drop=True)",
                "data"
            ],
            "source_orig": [
                "# creating instance of labelencoder\n",
                "labelencoder = LabelEncoder()\n",
                "# Assigning numerical values and storing in another column\n",
                "data['target_label'] = labelencoder.fit_transform(data['target'])\n",
                "data = data.sample(frac=1).reset_index(drop=True)\n",
                "data"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.iloc[1000,:]"
            ],
            "source_orig": [
                "data.iloc[1000,:]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.groupby(\"target_label\").size()"
            ],
            "source_orig": [
                "data.groupby(\"target_label\").size()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = plt.subplots(1,2,figsize=(20,5))",
                "sns.distplot(data.groupby(\"target_label\").size(), ax=ax[0], color=\"Orange\", kde=False)",
                "ax[0].set_xlabel(\"Number of images\")",
                "ax[0].set_ylabel(\"Frequency\");",
                "sns.countplot(data.target, palette=\"Set2\", ax=ax[1]);",
                "ax[1].set_xlabel(\"Names of Class\")",
                "ax[1].set_title(\"Data Distribution\");"
            ],
            "source_orig": [
                "# cancer_perc = data.groupby(\"patient_id\").target.value_counts()/ data.groupby(\"patient_id\").target.size()\n",
                "# cancer_perc = cancer_perc.unstack()\n",
                "\n",
                "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
                "sns.distplot(data.groupby(\"target_label\").size(), ax=ax[0], color=\"Orange\", kde=False)\n",
                "ax[0].set_xlabel(\"Number of images\")\n",
                "ax[0].set_ylabel(\"Frequency\");\n",
                "\n",
                "sns.countplot(data.target, palette=\"Set2\", ax=ax[1]);\n",
                "ax[1].set_xlabel(\"Names of Class\")\n",
                "ax[1].set_title(\"Data Distribution\");"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.info()"
            ],
            "source_orig": [
                "data.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.describe()"
            ],
            "source_orig": [
                "data.describe()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "data.target_label"
            ],
            "source_orig": [
                "data.target_label"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data.path",
                "ASSIGN = data.target_label",
                "X_train, X_test_sub ,y_train,y_test_sub= train_test_split(ASSIGN,ASSIGN, test_size=0.3, random_state=0,shuffle = True)",
                "print(X_train.shape)",
                "print(X_test_sub.shape)"
            ],
            "source_orig": [
                "X = data.path\n",
                "y = data.target_label\n",
                "X_train, X_test_sub ,y_train,y_test_sub= train_test_split(X,y, test_size=0.3, random_state=0,shuffle = True)\n",
                "print(X_train.shape)\n",
                "print(X_test_sub.shape)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "X_test,X_valid,y_test,y_valid = train_test_split(X_test_sub, y_test_sub, test_size=0.5, random_state=0 , shuffle =False)",
                "print(X_test.shape)",
                "print(X_valid.shape)"
            ],
            "source_orig": [
                "X_test,X_valid,y_test,y_valid = train_test_split(X_test_sub, y_test_sub, test_size=0.5, random_state=0 , shuffle =False)\n",
                "print(X_test.shape)\n",
                "print(X_valid.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = plt.subplots(1,3,figsize=(20,5))",
                "sns.countplot(y_train, ax=ax[0], palette=\"Reds\")",
                "ax[0].set_title(\"Train data\")",
                "sns.countplot(y_valid, ax=ax[1], palette=\"Blues\")",
                "ax[1].set_title(\"Dev data\")",
                "sns.countplot(y_test, ax=ax[2], palette=\"Greens\");",
                "ax[2].set_title(\"Test data\");"
            ],
            "source_orig": [
                "fig, ax = plt.subplots(1,3,figsize=(20,5))\n",
                "sns.countplot(y_train, ax=ax[0], palette=\"Reds\")\n",
                "ax[0].set_title(\"Train data\")\n",
                "sns.countplot(y_valid, ax=ax[1], palette=\"Blues\")\n",
                "ax[1].set_title(\"Dev data\")\n",
                "sns.countplot(y_test, ax=ax[2], palette=\"Greens\");\n",
                "ax[2].set_title(\"Test data\");"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "data.path.values"
            ],
            "source_orig": [
                "data.path.values"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = {k:v for k,v in zip(data.path.values,data.target_label.values)}"
            ],
            "source_orig": [
                "target_label_map = {k:v for k,v in zip(data.path.values,data.target_label.values)}"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0009)",
                "ASSIGN = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)",
                "ASSIGN = Input(shape=(224,224, 3))",
                "ASSIGN = tf.keras.applications.ResNet50(input_tensor= input_tensor, weights='imagenet', include_top=False)",
                "ASSIGN = base_model.output",
                "print(ASSIGN.shape)",
                "ASSIGN = GlobalAveragePooling2D()(base_output)",
                "ASSIGN = Flatten()(base_output)",
                "ASSIGN = concatenate([gap,ASSIGN])",
                "ASSIGN = Dense(512, activation='relu')(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Dense(512, activation='relu')(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Dense(5, activation='softmax')(x)",
                "ASSIGN = Model(inputs=input_tensor, outputs=predict)",
                "for layer in (ASSIGN.layers):",
                "layer.trainable = False"
            ],
            "source_orig": [
                "adam = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0009)\n",
                "sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
                "\n",
                "input_tensor = Input(shape=(224,224, 3))\n",
                "#backbone\n",
                "\n",
                "base_model = tf.keras.applications.ResNet50(input_tensor= input_tensor, weights='imagenet', include_top=False)\n",
                "base_output = base_model.output\n",
                "print(base_output.shape)\n",
                "# channel-attention\n",
                "# x = squeeze_excitation_layer(base_output, 2048, ratio=4, concate=False)\n",
                "# x = BatchNormalization()(x)\n",
                "\n",
                "# #concat\n",
                "# x = concatenate([base_output, x], axis=3)\n",
                "# spp\n",
                "\n",
                "gap = GlobalAveragePooling2D()(base_output)\n",
                "x = Flatten()(base_output)\n",
                "x = concatenate([gap,x])\n",
                "x = Dense(512, activation='relu')(x)\n",
                "x = BatchNormalization()(x)\n",
                "x = Dense(512, activation='relu')(x)\n",
                "x = BatchNormalization()(x)\n",
                "predict = Dense(5, activation='softmax')(x)\n",
                "model = Model(inputs=input_tensor, outputs=predict)\n",
                "\n",
                "for layer in (base_model.layers):\n",
                "    layer.trainable = False\n",
                "\n",
                "# for l in model.layers:\n",
                "#   print(l.name)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "model.compile(optimizer=adam,",
                "ASSIGN='categorical_crossentropy',",
                "ASSIGN=[keras.ASSIGN.categorical_accuracy])",
                "model.summary()"
            ],
            "source_orig": [
                "model.compile(optimizer=adam,\n",
                "              \n",
                "                  loss='categorical_crossentropy',\n",
                "                  metrics=[keras.metrics.categorical_accuracy])\n",
                "model.summary()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN=32",
                "ASSIGN = model.fit_generator(",
                "data_gen(X_train, target_label_map, ASSIGN, augment=True),",
                "ASSIGN=data_gen(X_valid, target_label_map, batch_size),",
                "ASSIGN=50,",
                "ASSIGN = 1,",
                "ASSIGN= int(len(X_train)path),",
                "ASSIGN= int(len(X_valid)path)",
                ")"
            ],
            "source_orig": [
                "batch_size=32\n",
                "history = model.fit_generator(\n",
                "    data_gen(X_train, target_label_map, batch_size, augment=True),\n",
                "    validation_data=data_gen(X_valid, target_label_map, batch_size),\n",
                "    epochs=50, \n",
                "    verbose = 1,\n",
                "    #callbacks=callbacks,\n",
                "    steps_per_epoch=  int(len(X_train)//batch_size),\n",
                "    validation_steps= int(len(X_valid)// batch_size)\n",
                ")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "ASSIGN = '.path'"
            ],
            "source_orig": [
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential, save_model, load_model\n",
                "\n",
                "filepath = './'"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "tf.keras.models.save_model(",
                "model,",
                "filepath,",
                "ASSIGN=True,",
                "ASSIGN=True,",
                "ASSIGN=None,",
                "ASSIGN=None,",
                "ASSIGN=None",
                ")"
            ],
            "source_orig": [
                "tf.keras.models.save_model(\n",
                "    model,\n",
                "    filepath,\n",
                "    overwrite=True,\n",
                "    include_optimizer=True,\n",
                "    save_format=None,\n",
                "    signatures=None,\n",
                "    options=None\n",
                ")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "py.init_notebook_mode(connected=True)",
                "print(os.listdir())"
            ],
            "source_orig": [
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "from scipy import stats\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve\n",
                "from sklearn.metrics import  recall_score, classification_report\n",
                "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
                "from statsmodels.tools.tools import add_constant\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn import metrics\n",
                "\n",
                "# Plotting the graphs\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "import plotly.graph_objs as go\n",
                "import plotly.offline as py\n",
                "py.init_notebook_mode(connected=True)\n",
                "\n",
                "import os\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = '..path'",
                "ASSIGN = pd.read_csv(lowafilepath)"
            ],
            "source_orig": [
                "#Importing data\n",
                "lowafilepath = '../input/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
                "data = pd.read_csv(lowafilepath)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.head()"
            ],
            "source_orig": [
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "print (\"Rows     : \" ,data.shape[0])",
                "print (\"Columns  : \" ,data.shape[1])",
                "print (\"\\nFeatures : \\n\" ,data.columns.tolist())",
                "print(,data.info())"
            ],
            "source_orig": [
                "print (\"Rows     : \" ,data.shape[0])\n",
                "print (\"Columns  : \" ,data.shape[1])\n",
                "print (\"\\nFeatures : \\n\" ,data.columns.tolist())\n",
                "print(\"\\nData Information : \\n\",data.info())"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = ASSIGN[pd.notnull(ASSIGN['TotalCharges'])]",
                "print(,ASSIGN['TotalCharges'].isna().sum())",
                "ASSIGN = pd.to_numeric(ASSIGN,errors='coerce')"
            ],
            "source_orig": [
                "# Removing the missing values\n",
                "data = data[pd.notnull(data['TotalCharges'])]\n",
                "print(\"Number of null values in total charges:\",data['TotalCharges'].isna().sum())\n",
                "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'],errors='coerce')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.pairplot(data,vars = ['tenure','MonthlyCharges','TotalCharges'], hue=\"Churn\")"
            ],
            "source_orig": [
                "sns.pairplot(data,vars = ['tenure','MonthlyCharges','TotalCharges'], hue=\"Churn\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = data.nunique()[data.nunique()<5].keys().tolist()",
                "ASSIGN = all_cat_var[:-1]"
            ],
            "source_orig": [
                "# Getting the categorical variables \n",
                "all_cat_var = data.nunique()[data.nunique()<5].keys().tolist()\n",
                "\n",
                "# Getting the categorical variables without churn\n",
                "cat_var = all_cat_var[:-1]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def pie_plot(Column):",
                "ASSIGN = pd.crosstab(data[Column],data['Churn'])",
                "ASSIGN = go.Pie(labels = ct1.index,",
                "ASSIGN = ct1.iloc[:,0],",
                "ASSIGN=0.3,",
                "ASSIGN=dict(x=[0,.45]))",
                "ASSIGN = go.Pie(labels = ct1.index,",
                "ASSIGN = ct1.iloc[:,1],",
                "ASSIGN=dict(x=[.55,1]),",
                "ASSIGN=0.3)",
                "ASSIGN = go.Layout(dict(title = Column + \" distribution in customer attrition \",",
                "ASSIGN = \"rgb(243,243,243)\",",
                "ASSIGN = \"rgb(243,243,243)\",",
                "ASSIGN = [dict(text = \"churn customers\",",
                "ASSIGN = dict(size = 13),",
                "ASSIGN = False,",
                "ASSIGN = .15, y = 1),",
                "dict(text = \"Non churn customers\",",
                "ASSIGN = dict(size = 13),",
                "ASSIGN = False,",
                "ASSIGN = .88,y = 1)",
                "]",
                ")",
                ")",
                "ASSIGN = go.Figure(data=[trace1,trace2],layout=layout)",
                "py.iplot(ASSIGN)"
            ],
            "source_orig": [
                "def pie_plot(Column):    \n",
                "    ct1 = pd.crosstab(data[Column],data['Churn'])\n",
                "    trace1 = go.Pie(labels = ct1.index,\n",
                "                    values = ct1.iloc[:,0],\n",
                "                    hole=0.3,\n",
                "                    domain=dict(x=[0,.45]))\n",
                "    trace2 = go.Pie(labels = ct1.index,\n",
                "                    values = ct1.iloc[:,1],\n",
                "                    domain=dict(x=[.55,1]),\n",
                "                    hole=0.3)\n",
                "\n",
                "    layout = go.Layout(dict(title = Column + \" distribution in customer attrition \",\n",
                "                                plot_bgcolor  = \"rgb(243,243,243)\",\n",
                "                                paper_bgcolor = \"rgb(243,243,243)\",\n",
                "                                annotations = [dict(text = \"churn customers\",\n",
                "                                                    font = dict(size = 13),\n",
                "                                                    showarrow = False,\n",
                "                                                    x = .15, y = 1),\n",
                "                                               dict(text = \"Non churn customers\",\n",
                "                                                    font = dict(size = 13),\n",
                "                                                    showarrow = False,\n",
                "                                                    x = .88,y = 1)\n",
                "\n",
                "                                              ]\n",
                "                               )\n",
                "                          )\n",
                "\n",
                "    fig = go.Figure(data=[trace1,trace2],layout=layout)\n",
                "    py.iplot(fig)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "del data['customerID']"
            ],
            "source_orig": [
                "# Removing the customer id\n",
                "del data['customerID'] #customerID is a uninque id so it dosn't give any information"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = add_constant(data_new)",
                "pd.Series([variance_inflation_factor(ASSIGN.values, i)",
                "for i in range(ASSIGN.shape[1])], index=ASSIGN.columns)"
            ],
            "source_orig": [
                "X = add_constant(data_new)\n",
                "pd.Series([variance_inflation_factor(X.values, i) \n",
                "           for i in range(X.shape[1])], index=X.columns)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = (data_new.loc[:, data_new.columns != 'Churn'])",
                "ASSIGN = (data_new.loc[:, data_new.columns == 'Churn'])",
                "print('Shape of ASSIGN: {}'.format(ASSIGN.shape))",
                "print('Shape of ASSIGN: {}'.format(ASSIGN.shape))",
                "X_train, X_test, y_train, y_test = train_test_split(ASSIGN, ASSIGN, test_size=0.2, random_state=0)",
                "ASSIGN = X_train.ASSIGN"
            ],
            "source_orig": [
                "# Splitiing to x and y\n",
                "\n",
                "X = (data_new.loc[:, data_new.columns != 'Churn'])\n",
                "y = (data_new.loc[:, data_new.columns == 'Churn'])\n",
                "print('Shape of X: {}'.format(X.shape))\n",
                "print('Shape of y: {}'.format(y.shape))\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
                "columns = X_train.columns"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = LogisticRegression()",
                "ASSIGN.fit(X_train, y_train)",
                "ASSIGN = logreg.predict(X_test)",
                "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(ASSIGN.score(X_test, y_test)))",
                "print(confusion_matrix(y_test, ASSIGN))",
                "print(classification_report(y_test, ASSIGN))"
            ],
            "source_orig": [
                "# Logistic regression Model\n",
                "\n",
                "logreg = LogisticRegression()\n",
                "logreg.fit(X_train, y_train)\n",
                "\n",
                "#  Model metrics\n",
                "y_pred = logreg.predict(X_test)\n",
                "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
                "print(confusion_matrix(y_test, y_pred))\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train.drop(['gender','PhoneService','TotalCharges','tenure'], axis=1, inplace=True)",
                "X_test.drop(['gender','PhoneService','TotalCharges','tenure'], axis=1, inplace=True)"
            ],
            "source_orig": [
                "# Removing unimportant features ['gender','PhoneService','TotalCharges','tenure']\n",
                "X_train.drop(['gender','PhoneService','TotalCharges','tenure'], axis=1, inplace=True)\n",
                "X_test.drop(['gender','PhoneService','TotalCharges','tenure'], axis=1, inplace=True)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = LogisticRegression()",
                "ASSIGN.fit(X_train, y_train)",
                "ASSIGN = logreg_new.predict(X_test)",
                "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(ASSIGN.score(X_test, y_test)))",
                "print(confusion_matrix(y_test, ASSIGN))",
                "print(classification_report(y_test, ASSIGN))"
            ],
            "source_orig": [
                "# New model\n",
                "logreg_new = LogisticRegression()\n",
                "logreg_new.fit(X_train, y_train)\n",
                "\n",
                "#  Model metrics\n",
                "y_pred = logreg_new.predict(X_test)\n",
                "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg_new.score(X_test, y_test)))\n",
                "print(confusion_matrix(y_test, y_pred))\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100,max_depth = 3)",
                "ASSIGN = ASSIGN.fit(X_train,y_train)",
                "ASSIGN = clf.predict(X_test)",
                "print(,metrics.accuracy_score(y_test, ASSIGN))",
                "print(, classification_report(y_test, ASSIGN))"
            ],
            "source_orig": [
                "# Create Decision Tree classifer object\n",
                "clf = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100,max_depth = 3)\n",
                "\n",
                "# Train Decision Tree Classifer\n",
                "clf = clf.fit(X_train,y_train)\n",
                "\n",
                "#Predict the response for test dataset\n",
                "y_pred = clf.predict(X_test)\n",
                "\n",
                "# Model Accuracy, how often is the classifier correct?\n",
                "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
                "print(\"Report : \",  classification_report(y_test, y_pred)) "
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
                "#MAYUKH GHOSH 18BCE0417"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import sklearn as sklearn\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import sys\n",
                "import re"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv(\"path\")"
            ],
            "source_orig": [
                "data = pd.read_csv(\"/kaggle/input/wildlife-strikes/database.csv\")"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data[\"Species Name\"]",
                "ASSIGN=species.value_counts()",
                "print(ASSIGN)"
            ],
            "source_orig": [
                "species = data[\"Species Name\"]\n",
                "species_count=species.value_counts()\n",
                "print(species_count)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN=ASSIGN[ASSIGN>4000]",
                "print(ASSIGN)"
            ],
            "source_orig": [
                "species_count=species_count[species_count>4000]\n",
                "print(species_count)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = [\"UNKNOWN MEDIUM BIRD\",\"UNKNOWN SMALL BIRD\",\"MOURNING DOVE\", \"GULL\",\"UNKNOWN BIRD\",\"KILLDEER\", \"AMERICAN KESTREL\",\"BARN SWALLOW\"]",
                "ASSIGN = species[species.isin(ASSIGN)]",
                "print(ASSIGN.value_counts())"
            ],
            "source_orig": [
                "top_species = [\"UNKNOWN MEDIUM BIRD\",\"UNKNOWN SMALL BIRD\",\"MOURNING DOVE\", \"GULL\",\"UNKNOWN BIRD\",\"KILLDEER\", \"AMERICAN KESTREL\",\"BARN SWALLOW\"]\n",
                "top_species = species[species.isin(top_species)]\n",
                "print(top_species.value_counts())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(top_species)",
                "plt.title(\"Top Species That Impact with Aircraft\")",
                "plt.xticks(rotation='vertical')"
            ],
            "source_orig": [
                "sns.countplot(top_species)\n",
                "plt.title(\"Top Species That Impact with Aircraft\")\n",
                "plt.xticks(rotation='vertical')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = [\"MOURNING DOVE\", \"GULL\",\"KILLDEER\", \"AMERICAN KESTREL\",\"BARN SWALLOW\"]",
                "ASSIGN = species[species.isin(ASSIGN)]",
                "print(ASSIGN.value_counts())"
            ],
            "source_orig": [
                "top_known_species = [\"MOURNING DOVE\", \"GULL\",\"KILLDEER\", \"AMERICAN KESTREL\",\"BARN SWALLOW\"]\n",
                "top_known_species = species[species.isin(top_known_species)]\n",
                "print(top_known_species.value_counts())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(top_known_species)",
                "plt.title(\"Top Known Species That Impact with Aircraft\")",
                "plt.xticks(rotation='vertical')"
            ],
            "source_orig": [
                "sns.countplot(top_known_species)\n",
                "plt.title(\"Top Known Species That Impact with Aircraft\")\n",
                "plt.xticks(rotation='vertical')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "damage_x"
            ],
            "source_orig": [
                "damage_x"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.bar(damage_x,damage_y)",
                "plt.title(\"Parts Damaged in the Aircraft\")",
                "plt.xticks(rotation='vertical')"
            ],
            "source_orig": [
                "plt.bar(damage_x,damage_y)\n",
                "plt.title(\"Parts Damaged in the Aircraft\")\n",
                "plt.xticks(rotation='vertical')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.bar(strike_x,strike_y,color='orange')",
                "plt.title(\"Parts Striked in the Aircraft\")",
                "plt.xticks(rotation='vertical')"
            ],
            "source_orig": [
                "plt.bar(strike_x,strike_y,color='orange')\n",
                "plt.title(\"Parts Striked in the Aircraft\")\n",
                "plt.xticks(rotation='vertical')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.bar(parts,damage_per_strike,color='red')",
                "plt.title(\"Parts Damage per strike in the Aircraft\")",
                "plt.xticks(rotation='vertical')"
            ],
            "source_orig": [
                "plt.bar(parts,damage_per_strike,color='red')\n",
                "plt.title(\"Parts Damage per strike in the Aircraft\")\n",
                "plt.xticks(rotation='vertical')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "if \"models\" in pathlib.Path.cwd().parts:",
                "while \"models\" in pathlib.Path.cwd().parts:",
                "os.chdir('..')",
                "elif not pathlib.Path('models').exists():"
            ],
            "source_orig": [
                "import os\n",
                "import pathlib\n",
                "\n",
                "# Clone the tensorflow models repository if it doesn't already exist\n",
                "if \"models\" in pathlib.Path.cwd().parts:\n",
                "    while \"models\" in pathlib.Path.cwd().parts:\n",
                "        os.chdir('..')\n",
                "elif not pathlib.Path('models').exists():\n",
                "    !git clone --depth 1 https://github.com/tensorflow/models"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import matplotlib\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import os\n",
                "import random\n",
                "import io\n",
                "import imageio\n",
                "import glob\n",
                "import scipy.misc\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "from six import BytesIO\n",
                "from PIL import Image, ImageDraw, ImageFont\n",
                "from IPython.display import display, Javascript\n",
                "from IPython.display import Image as IPyImage\n",
                "\n",
                "import tensorflow as tf\n",
                "\n",
                "from object_detection.utils import label_map_util\n",
                "from object_detection.utils import config_util\n",
                "from object_detection.utils import visualization_utils as viz_utils\n",
                "# from object_detection.utils import colab_utils\n",
                "# this was for the annotator can be done away with in this kaggle env\n",
                "from object_detection.builders import model_builder\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv('path')"
            ],
            "source_orig": [
                "train_df = pd.read_csv('/kaggle/input/global-wheat-detection/train.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_df['bboxs'] = train_df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=','))"
            ],
            "source_orig": [
                "train_df['bboxs'] = train_df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=','))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_df.head()"
            ],
            "source_orig": [
                "train_df.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_df.width.nunique(),train_df.height.nunique()"
            ],
            "source_orig": [
                "train_df.width.nunique(),train_df.height.nunique()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_df['bbox_adj'] = train_df['bboxs'].map(xyxy_to_yxyx)"
            ],
            "source_orig": [
                "train_df['bbox_adj'] = train_df['bboxs'].map(xyxy_to_yxyx)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame(train_df['image_id'].value_counts())"
            ],
            "source_orig": [
                "val_counts_image_id = pd.DataFrame(train_df['image_id'].value_counts())"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "val_counts_image_id.head()"
            ],
            "source_orig": [
                "val_counts_image_id.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "val_counts_image_id['bbox_count'] = val_counts_image_id['image_id']"
            ],
            "source_orig": [
                "val_counts_image_id['bbox_count'] = val_counts_image_id['image_id'] "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "val_counts_image_id['image_id']  = val_counts_image_id.index"
            ],
            "source_orig": [
                "val_counts_image_id['image_id']  = val_counts_image_id.index"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "val_counts_image_id.head()"
            ],
            "source_orig": [
                "val_counts_image_id.head() # "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = val_counts_image_id['bbox_count']==1",
                "ASSIGN = val_counts_image_id['bbox_count']==2",
                "ASSIGN = val_counts_image_id['bbox_count']==3"
            ],
            "source_orig": [
                "cond_1 = val_counts_image_id['bbox_count']==1\n",
                "cond_2 = val_counts_image_id['bbox_count']==2\n",
                "cond_3 = val_counts_image_id['bbox_count']==3"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "val_counts_image_id['image_id'][cond_1 | cond_2]"
            ],
            "source_orig": [
                "val_counts_image_id['image_id'][cond_1 | cond_2]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = val_counts_image_id['image_id'][cond_1 | cond_2].tolist()"
            ],
            "source_orig": [
                "image_names_list = val_counts_image_id['image_id'][cond_1 | cond_2].tolist()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "image_names_list"
            ],
            "source_orig": [
                "image_names_list # only images with one or 2  or 3 wheat heads higher numbers dont converge well... for now..."
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "train_df.shape"
            ],
            "source_orig": [
                "train_df.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "tf.keras.backend.clear_session()",
                "print('Building model and restoring weights for fine-tuning...', flush=True)",
                "ASSIGN = 1",
                "ASSIGN = 'path'",
                "ASSIGN = 'path'"
            ],
            "source_orig": [
                "tf.keras.backend.clear_session()\n",
                "\n",
                "print('Building model and restoring weights for fine-tuning...', flush=True)\n",
                "num_classes = 1\n",
                "pipeline_config = '/kaggle/working/models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
                "checkpoint_path = '/kaggle/working/models/research/object_detection/test_data/checkpoint/ckpt-0'\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "tf.keras.backend.set_learning_phase(True)",
                "ASSIGN = 4",
                "ASSIGN = 0.001",
                "ASSIGN = 200",
                "ASSIGN = tf.keras.optimizers.Adam(learning_rate=learning_rate, decay=learning_rate path)",
                "ASSIGN = get_model_train_step_function(",
                "detection_model, ASSIGN, to_fine_tune)",
                "print('Start fine-tuning!', flush=True)",
                "for idx in range(ASSIGN):",
                "ASSIGN = list(range(len(train_images_np)))",
                "random.shuffle(ASSIGN)",
                "ASSIGN = all_keys[:batch_size]",
                "ASSIGN = [gt_box_tensors[key] for key in example_keys]",
                "ASSIGN = [gt_classes_one_hot_tensors[key] for key in example_keys]",
                "ASSIGN = [train_image_tensors[key] for key in example_keys]",
                "ASSIGN = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)",
                "if idx % 10 == 0:",
                "print('batch ' + str(idx) + ' of ' + str(ASSIGN)",
                "+ ', loss=' + str(ASSIGN.numpy()), flush=True)",
                "print('Done fine-tuning!')"
            ],
            "source_orig": [
                "%%time\n",
                "tf.keras.backend.set_learning_phase(True)\n",
                "\n",
                "# These parameters can be tuned; since our training set has 5 images\n",
                "# it doesn't make sense to have a much larger batch size, though we could\n",
                "# fit more examples in memory if we wanted to.\n",
                "batch_size = 4\n",
                "learning_rate = 0.001\n",
                "num_batches = 200\n",
                "\n",
                "# optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
                "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,  decay=learning_rate / num_batches)\n",
                "train_step_fn = get_model_train_step_function(\n",
                "    detection_model, optimizer, to_fine_tune)\n",
                "\n",
                "\n",
                "print('Start fine-tuning!', flush=True)\n",
                "for idx in range(num_batches):\n",
                "  # Grab keys for a random subset of examples\n",
                "  all_keys = list(range(len(train_images_np)))\n",
                "  random.shuffle(all_keys)\n",
                "  example_keys = all_keys[:batch_size]\n",
                "\n",
                "  # Note that we do not do data augmentation in this demo.  If you want a\n",
                "  # a fun exercise, we recommend experimenting with random horizontal flipping\n",
                "  # and random cropping :)\n",
                "  gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
                "  gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
                "  image_tensors = [train_image_tensors[key] for key in example_keys]\n",
                "\n",
                "  # Training step (forward pass + backwards pass)\n",
                "  total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
                "\n",
                "  if idx % 10 == 0:\n",
                "    print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
                "    + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
                "\n",
                "print('Done fine-tuning!')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(test_image_dir)"
            ],
            "source_orig": [
                "os.listdir(test_image_dir)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "Image(filename = 'inf_348a992bb.jpg', width = 512, height = 512)"
            ],
            "source_orig": [
                "#Import library\n",
                "from IPython.display import Image# Load image from local storage\n",
                "Image(filename = 'inf_348a992bb.jpg', width = 512, height = 512)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "# source https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/dicom.ipynb#scrollTo=WodUv8O1VKmr"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "list(os.listdir(\"..path\"))"
            ],
            "source_orig": [
                "list(os.listdir(\"../input/rsna-str-pulmonary-embolism-detection\"))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "ASSIGN = pd.read_csv(f\"{DATA_PATH}path\")",
                "ASSIGN = pd.read_csv(f\"{DATA_PATH}path\")",
                "ASSIGN = pd.read_csv(f\"{DATA_PATH}path\")"
            ],
            "source_orig": [
                "%%time\n",
                "DATA_PATH = \"../input/rsna-str-pulmonary-embolism-detection\"\n",
                "\n",
                "train = pd.read_csv(f\"{DATA_PATH}/train.csv\")\n",
                "test = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
                "sample_submission = pd.read_csv(f\"{DATA_PATH}/sample_submission.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head(10)"
            ],
            "source_orig": [
                "train.head(10)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.info()"
            ],
            "source_orig": [
                "train.info()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import tensorflow as tf\n",
                "import tensorflow_io as tfio\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "DATA_PATH = Path(DATA_PATH)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "type(skipped), type(skipped.numpy()), skipped.numpy()"
            ],
            "source_orig": [
                "type(skipped), type(skipped.numpy()), skipped.numpy() "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "image.numpy().shape, lossy_image.numpy().shape"
            ],
            "source_orig": [
                "image.numpy().shape, lossy_image.numpy().shape"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = plt.subplots(1,3, figsize=(10,10))",
                "axes[0].imshow(np.squeeze(image.numpy()), cmap='gray')",
                "axes[0].set_title('image')",
                "axes[1].imshow(np.squeeze(lossy_image.numpy()), cmap='gray')",
                "axes[1].set_title('lossy image');",
                "axes[2].imshow(np.squeeze(lossy_image.numpy() - image.numpy()), cmap='gray')",
                "axes[2].set_title('diff bpath');"
            ],
            "source_orig": [
                "fig, axes = plt.subplots(1,3, figsize=(10,10))\n",
                "\n",
                "axes[0].imshow(np.squeeze(image.numpy()), cmap='gray')\n",
                "axes[0].set_title('image')\n",
                "axes[1].imshow(np.squeeze(lossy_image.numpy()), cmap='gray')\n",
                "axes[1].set_title('lossy image');\n",
                "axes[2].imshow(np.squeeze(lossy_image.numpy() - image.numpy()), cmap='gray')\n",
                "axes[2].set_title('diff b/w images');"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = plt.subplots(3,1, figsize=(20,20))",
                "axes[0].imshow(np.squeeze(image.numpy()), cmap='gray')",
                "axes[0].set_title('image')",
                "axes[1].imshow(np.squeeze(lossy_image.numpy()), cmap='gray')",
                "axes[1].set_title('lossy image');"
            ],
            "source_orig": [
                "fig, axes = plt.subplots(3,1, figsize=(20,20))\n",
                "\n",
                "axes[0].imshow(np.squeeze(image.numpy()), cmap='gray')\n",
                "axes[0].set_title('image')\n",
                "axes[1].imshow(np.squeeze(lossy_image.numpy()), cmap='gray')\n",
                "axes[1].set_title('lossy image');"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "np.sum(image.numpy() - lossy_image.numpy())"
            ],
            "source_orig": [
                "np.sum(image.numpy() - lossy_image.numpy())"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "#     for filename in filenames:\n",
                "#         print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torchvision import datasets, transforms\n",
                "from torch.utils.data import Dataset\n",
                "import torchaudio\n",
                "import pandas as pd\n",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = torch.ASSIGN(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
                "print(ASSIGN)"
            ],
            "source_orig": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(device)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = pd.read_csv('path')",
                "print(ASSIGN.iloc[0, :])"
            ],
            "source_orig": [
                "csvData = pd.read_csv('/kaggle/input/urbansound8k/UrbanSound8K.csv')\n",
                "print(csvData.iloc[0, :])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "csvData.head()"
            ],
            "source_orig": [
                "csvData.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "SETUP",
                "ipd.Audio('path')"
            ],
            "source_orig": [
                "import IPython.display as ipd\n",
                "ipd.Audio('/kaggle/input/urbansound8k/fold1/108041-9-0-5.wav')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "sound, sound [0] , sound [1]"
            ],
            "source_orig": [
                " sound, sound [0] , sound [1] # an array and freq"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = torch.mean(sound[0], dim=0).unsqueeze(1)",
                "soundData, soundData.shape"
            ],
            "source_orig": [
                "soundData = torch.mean(sound[0], dim=0).unsqueeze(1)\n",
                "# was as below\n",
                "# soundData = torch.mean(sound[0], dim=0).unsqueeze(0)# add a dim at idx 0 <-unsqueeze? \n",
                "# dim 0 is where the second channel comes in\n",
                "soundData, soundData.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "soundData.numel()"
            ],
            "source_orig": [
                "soundData.numel()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "tempData.shape"
            ],
            "source_orig": [
                "tempData.shape"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = tempData",
                "ASSIGN = torch.zeros([32000, 1])",
                "ASSIGN[:32000] = ASSIGN[::5]",
                "soundFormatted.shape"
            ],
            "source_orig": [
                "soundData = tempData\n",
                "soundFormatted = torch.zeros([32000, 1])\n",
                "soundFormatted[:32000] = soundData[::5] #take every fifth sample of soundData\n",
                "soundFormatted.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.permute(1, 0)"
            ],
            "source_orig": [
                "soundFormatted = soundFormatted.permute(1, 0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "soundFormatted.shape"
            ],
            "source_orig": [
                "soundFormatted.shape"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "class Net(nn.Module):",
                "def __init__(self):",
                "super(Net, self).__init__()",
                "self.conv1 = nn.Conv1d(1, 128, 80, 4)",
                "self.bn1 = nn.BatchNorm1d(128)",
                "self.pool1 = nn.MaxPool1d(4)",
                "self.conv2 = nn.Conv1d(128, 128, 3)",
                "self.bn2 = nn.BatchNorm1d(128)",
                "self.pool2 = nn.MaxPool1d(4)",
                "self.conv3 = nn.Conv1d(128, 256, 3)",
                "self.bn3 = nn.BatchNorm1d(256)",
                "self.pool3 = nn.MaxPool1d(4)",
                "self.conv4 = nn.Conv1d(256, 512, 3)",
                "self.bn4 = nn.BatchNorm1d(512)",
                "self.pool4 = nn.MaxPool1d(4)",
                "self.avgPool = nn.AvgPool1d(30)",
                "self.fc1 = nn.Linear(512, 10)",
                "def forward(self, x):",
                "ASSIGN = self.conv1(ASSIGN)",
                "ASSIGN = F.relu(self.bn1(ASSIGN))",
                "ASSIGN = self.pool1(ASSIGN)",
                "ASSIGN = self.conv2(ASSIGN)",
                "ASSIGN = F.relu(self.bn2(ASSIGN))",
                "ASSIGN = self.pool2(ASSIGN)",
                "ASSIGN = self.conv3(ASSIGN)",
                "ASSIGN = F.relu(self.bn3(ASSIGN))",
                "ASSIGN = self.pool3(ASSIGN)",
                "ASSIGN = self.conv4(ASSIGN)",
                "ASSIGN = F.relu(self.bn4(ASSIGN))",
                "ASSIGN = self.pool4(ASSIGN)",
                "ASSIGN = self.avgPool(ASSIGN)",
                "ASSIGN = ASSIGN.permute(0, 2, 1)",
                "ASSIGN = self.fc1(ASSIGN)",
                "return F.log_softmax(ASSIGN, dim = 2)",
                "ASSIGN = Net()",
                "ASSIGN.to(device)",
                "print(ASSIGN)"
            ],
            "source_orig": [
                "class Net(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(Net, self).__init__()\n",
                "        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n",
                "        self.bn1 = nn.BatchNorm1d(128)\n",
                "        self.pool1 = nn.MaxPool1d(4)\n",
                "        self.conv2 = nn.Conv1d(128, 128, 3)\n",
                "        self.bn2 = nn.BatchNorm1d(128)\n",
                "        self.pool2 = nn.MaxPool1d(4)\n",
                "        self.conv3 = nn.Conv1d(128, 256, 3)\n",
                "        self.bn3 = nn.BatchNorm1d(256)\n",
                "        self.pool3 = nn.MaxPool1d(4)\n",
                "        self.conv4 = nn.Conv1d(256, 512, 3)\n",
                "        self.bn4 = nn.BatchNorm1d(512)\n",
                "        self.pool4 = nn.MaxPool1d(4)\n",
                "        self.avgPool = nn.AvgPool1d(30) #input should be 512x30 so this outputs a 512x1\n",
                "        self.fc1 = nn.Linear(512, 10)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.conv1(x)\n",
                "        x = F.relu(self.bn1(x))\n",
                "        x = self.pool1(x)\n",
                "        x = self.conv2(x)\n",
                "        x = F.relu(self.bn2(x))\n",
                "        x = self.pool2(x)\n",
                "        x = self.conv3(x)\n",
                "        x = F.relu(self.bn3(x))\n",
                "        x = self.pool3(x)\n",
                "        x = self.conv4(x)\n",
                "        x = F.relu(self.bn4(x))\n",
                "        x = self.pool4(x)\n",
                "        x = self.avgPool(x)\n",
                "        x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n",
                "        x = self.fc1(x)\n",
                "        return F.log_softmax(x, dim = 2)\n",
                "\n",
                "model = Net()\n",
                "model.to(device)\n",
                "print(model)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)",
                "ASSIGN = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
            ],
            "source_orig": [
                "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
                "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "def train(model, epoch):",
                "model.train()",
                "for batch_idx, (data, target) in enumerate(train_loader):",
                "optimizer.zero_grad()",
                "ASSIGN = ASSIGN.to(device)",
                "ASSIGN = ASSIGN.to(device)",
                "ASSIGN = ASSIGN.requires_grad_()",
                "ASSIGN = model(data)",
                "ASSIGN = ASSIGN.permute(1, 0, 2)",
                "ASSIGN = F.nll_loss(output[0], target)",
                "ASSIGN.backward()",
                "optimizer.step()",
                "if batch_idx % log_interval == 0:",
                "print('Train Epoch: {} [{}path{} ({:.0f}%)]\\tLoss: {:.6f}'.format(",
                "epoch, batch_idx * len(ASSIGN), len(train_loader.dataset),",
                "100. * batch_idx path(train_loader), ASSIGN))"
            ],
            "source_orig": [
                "def train(model, epoch):\n",
                "    model.train()\n",
                "    for batch_idx, (data, target) in enumerate(train_loader):\n",
                "        optimizer.zero_grad()\n",
                "        data = data.to(device)\n",
                "        target = target.to(device)\n",
                "        data = data.requires_grad_() #set requires_grad to True for training\n",
                "        output = model(data)\n",
                "        output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x10 \n",
                "        loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex10 input\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        if batch_idx % log_interval == 0: #print training stats\n",
                "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
                "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
                "                100. * batch_idx / len(train_loader), loss))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "def test(model, epoch):",
                "model.eval()",
                "ASSIGN = 0",
                "for data, target in test_loader:",
                "ASSIGN = ASSIGN.to(device)",
                "ASSIGN = ASSIGN.to(device)",
                "ASSIGN = model(data)",
                "ASSIGN = ASSIGN.permute(1, 0, 2)",
                "ASSIGN = output.max(2)[1]",
                "ASSIGN += ASSIGN.eq(ASSIGN).cpu().sum().item()",
                "print('\\nTest set: Accuracy: {}path{} ({:.0f}%)\\n'.format(",
                "ASSIGN, len(test_loader.dataset),",
                "100. * ASSIGN path(test_loader.dataset)))"
            ],
            "source_orig": [
                "def test(model, epoch):\n",
                "    model.eval()\n",
                "    correct = 0\n",
                "    for data, target in test_loader:\n",
                "        data = data.to(device)\n",
                "        target = target.to(device)\n",
                "        output = model(data)\n",
                "        output = output.permute(1, 0, 2)\n",
                "        pred = output.max(2)[1] # get the index of the max log-probability\n",
                "        correct += pred.eq(target).cpu().sum().item()\n",
                "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
                "        correct, len(test_loader.dataset),\n",
                "        100. * correct / len(test_loader.dataset)))"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = 20",
                "for epoch in range(1, 2):",
                "ASSIGN == 31:",
                "print()",
                "scheduler.step()",
                "train(model, epoch)",
                "test(model, epoch)"
            ],
            "source_orig": [
                "log_interval = 20\n",
                "for epoch in range(1, 2): # use 41\n",
                "    if epoch == 31:\n",
                "        print(\"First round of training complete. Setting learn rate to 0.001.\")\n",
                "    scheduler.step()\n",
                "    train(model, epoch)\n",
                "    test(model, epoch)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import os\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN.sample(n=5)"
            ],
            "source_orig": [
                "df_train = pd.read_csv(\"../input/train_users_2.csv\")\n",
                "df_train.sample(n=5) "
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN.sample(n=5)"
            ],
            "source_orig": [
                "df_test = pd.read_csv(\"../input/test_users.csv\")\n",
                "df_test.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = pd.concat((df_train, df_test), axis=0, ignore_index=True)",
                "ASSIGN.head(n=5)"
            ],
            "source_orig": [
                "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
                "df_all.head(n=5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_all.drop('date_first_booking', axis=1, inplace=True)"
            ],
            "source_orig": [
                "df_all.drop('date_first_booking', axis=1, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_all.sample(n=5)"
            ],
            "source_orig": [
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_all['date_account_created'] = pd.to_datetime(df_all['date_account_created'],format='%Y-%m-%d')"
            ],
            "source_orig": [
                "df_all['date_account_created'] = pd.to_datetime(df_all['date_account_created'],format='%Y-%m-%d')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_all.sample(n=5)"
            ],
            "source_orig": [
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "df_all['timestamp_first_active']=pd.to_datetime(df_all['timestamp_first_active'], format='%Y%m%d%H%M%S')",
                "df_all.sample(n=5)"
            ],
            "source_orig": [
                "df_all['timestamp_first_active']=pd.to_datetime(df_all['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def remove_age_outliers(x, min_value=15, max_value=90):",
                "if np.logical_or(x<=min_value, x>=max_value):",
                "return np.nan",
                "else:",
                "return x"
            ],
            "source_orig": [
                "def remove_age_outliers(x, min_value=15, max_value=90):\n",
                "    if np.logical_or(x<=min_value, x>=max_value):\n",
                "        return np.nan\n",
                "    else:\n",
                "        return x"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_all['age'] = df_all['age'].apply(lambda x: remove_age_outliers(x) if(not np.isnan(x)) else x)"
            ],
            "source_orig": [
                "df_all['age'] = df_all['age'].apply(lambda x: remove_age_outliers(x) if(not np.isnan(x)) else x)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_all['age'].fillna(-1, inplace=True)"
            ],
            "source_orig": [
                "df_all['age'].fillna(-1, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_all.sample(n=5)"
            ],
            "source_orig": [
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "df_all.age = df_all.age.astype(int)",
                "df_all.sample(n=5)"
            ],
            "source_orig": [
                "df_all.age = df_all.age.astype(int)\n",
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "def check_NaN_Values_in_df(df):",
                "for col in df:",
                "ASSIGN = df[col].isnull().sum()",
                "if ASSIGN != 0:",
                "print (col + \" => \" + str(ASSIGN) + \" NaN Values\")"
            ],
            "source_orig": [
                "def check_NaN_Values_in_df(df):\n",
                "    for col in df:\n",
                "        nan_count = df[col].isnull().sum()\n",
                "        \n",
                "        if nan_count != 0:\n",
                "            print (col + \" => \" + str(nan_count) + \" NaN Values\")\n",
                "        "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "check_NaN_Values_in_df(df_all)"
            ],
            "source_orig": [
                "check_NaN_Values_in_df(df_all)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_all['first_affiliate_tracked'].fillna(-1, inplace=True)"
            ],
            "source_orig": [
                "df_all['first_affiliate_tracked'].fillna(-1, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_all.sample(n=5)"
            ],
            "source_orig": [
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_all.drop('timestamp_first_active', axis=1, inplace=True)",
                "df_all.drop('language', axis=1, inplace=True)"
            ],
            "source_orig": [
                "df_all.drop('timestamp_first_active', axis=1, inplace=True)\n",
                "df_all.drop('language', axis=1, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_all.sample(n=5)"
            ],
            "source_orig": [
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN[ASSIGN['date_account_created'] > '2013-02-01']"
            ],
            "source_orig": [
                "df_all = df_all[df_all['date_account_created'] > '2013-02-01']"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_all.sample(n=5)"
            ],
            "source_orig": [
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "if not os.path.exists(\"output\"):",
                "os.makedirs(\"output\")",
                "df_all.to_csv(\"outputpath\", sep=',', index=False)"
            ],
            "source_orig": [
                "if not os.path.exists(\"output\"):\n",
                "    os.makedirs(\"output\")\n",
                "    \n",
                "df_all.to_csv(\"output/cleaned.csv\", sep=',', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP",
                "warnings.filterwarnings(\"ignore\")"
            ],
            "source_orig": [
                "#For data processing\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "#For visualizations\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "#For ignoring warnings\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "source_orig": [
                "df1 = pd.read_csv(\"../input/titanic/train.csv\")\n",
                "tf1 = pd.read_csv(\"../input/titanic/test.csv\")\n",
                "result = pd.read_csv(\"../input/titanic/gender_submission.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df1.head()"
            ],
            "source_orig": [
                "df1.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tf1.head()"
            ],
            "source_orig": [
                "tf1.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "ASSIGN = df1.copy()",
                "df1.describe()"
            ],
            "source_orig": [
                "df = df1.copy()\n",
                "df1.describe()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "ASSIGN = tf1.copy()",
                "tf1.describe()"
            ],
            "source_orig": [
                "tf = tf1.copy()\n",
                "tf1.describe()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df.shape"
            ],
            "source_orig": [
                "df.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "tf.shape"
            ],
            "source_orig": [
                "tf.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df.dtypes"
            ],
            "source_orig": [
                "df.dtypes"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "tf.dtypes"
            ],
            "source_orig": [
                "tf.dtypes"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.isnull().sum()"
            ],
            "source_orig": [
                "df.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tf.isnull().sum()"
            ],
            "source_orig": [
                "tf.isnull().sum()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.set(rc={'figure.figsize':(15,5)})",
                "sns.heatmap(df.isnull(),yticklabels=False)"
            ],
            "source_orig": [
                "sns.set(rc={'figure.figsize':(15,5)})\n",
                "sns.heatmap(df.isnull(),yticklabels=False)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.heatmap(tf.isnull(),yticklabels=False)"
            ],
            "source_orig": [
                "sns.heatmap(tf.isnull(),yticklabels=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df['Survived'].value_counts()"
            ],
            "source_orig": [
                "df['Survived'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.concat([df,tf],axis = 0)",
                "ASSIGN.drop(['Survived'],axis = 1,inplace = True)"
            ],
            "source_orig": [
                "final = pd.concat([df,tf],axis = 0)\n",
                "final.drop(['Survived'],axis = 1,inplace = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.head()"
            ],
            "source_orig": [
                "final.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "final.shape"
            ],
            "source_orig": [
                "final.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final['Age'].isnull().sum()"
            ],
            "source_orig": [
                "final['Age'].isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final['Age'].fillna(final['Age'].median(),inplace = True)"
            ],
            "source_orig": [
                "final['Age'].fillna(final['Age'].median(),inplace = True)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final['Fare'].isnull().sum()"
            ],
            "source_orig": [
                "final['Fare'].isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.fillna(ASSIGN.median())"
            ],
            "source_orig": [
                "final[\"Fare\"] = final[\"Fare\"].fillna(final[\"Fare\"].median())\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "final['Fare'].dtypes"
            ],
            "source_orig": [
                "final['Fare'].dtypes"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.map(lambda n: np.log(n) if n > 0 else 0)"
            ],
            "source_orig": [
                "final[\"Fare\"] = final[\"Fare\"].map(lambda n: np.log(n) if n > 0 else 0)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = final['Name'].str.split('.', n=1, expand = True)",
                "ASSIGN = ASSIGN",
                "ASSIGN = ASSIGN",
                "ASSIGN = final['First'].str.split(',', n=1, expand = True)",
                "final['Last Name'] = ASSIGN[0]",
                "ASSIGN = new1[1]",
                "ASSIGN = final['Title'].str.split('', n=1, expand = True)"
            ],
            "source_orig": [
                "new = final['Name'].str.split('.', n=1, expand = True)\n",
                "final['First'] = new[0]\n",
                "final['Last'] = new[1]\n",
                "new1 = final['First'].str.split(',', n=1, expand = True)\n",
                "final['Last Name'] = new1[0]\n",
                "final['Title'] = new1[1]\n",
                "new2 = final['Title'].str.split('', n=1, expand = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final['Title'].value_counts()"
            ],
            "source_orig": [
                "final['Title'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final.drop(['First','Last','Name','Last Name'],axis = 1,inplace = True)"
            ],
            "source_orig": [
                "final.drop(['First','Last','Name','Last Name'],axis = 1,inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final.replace(to_replace = [ ' Don', ' Rev', ' Dr', ' Mme',",
                "' Major', ' Sir', ' Col', ' Capt',' Jonkheer'], value = ' Honorary(M)', inplace = True)",
                "final.replace(to_replace = [ ' Ms', ' Lady', ' Mlle',' the Countess', ' Dona'], value = ' Honorary(F)', inplace = True)"
            ],
            "source_orig": [
                "final.replace(to_replace = [ ' Don', ' Rev', ' Dr', ' Mme',\n",
                "        ' Major', ' Sir', ' Col', ' Capt',' Jonkheer'], value = ' Honorary(M)', inplace = True)\n",
                "final.replace(to_replace = [ ' Ms', ' Lady', ' Mlle',' the Countess', ' Dona'], value = ' Honorary(F)', inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = final.copy()",
                "ASSIGN = ASSIGN[:891]",
                "ASSIGN = pd.concat([ASSIGN,df1['Survived']],axis = 1)",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "df3 = final.copy()\n",
                "df3 =  df3[:891]\n",
                "df3 = pd.concat([df3,df1['Survived']],axis = 1)\n",
                "df3.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final['Title'].value_counts()"
            ],
            "source_orig": [
                "final['Title'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.get_dummies(ASSIGN, columns = [\"Title\"])"
            ],
            "source_orig": [
                "final = pd.get_dummies(final, columns = [\"Title\"])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.head()"
            ],
            "source_orig": [
                "final.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "ASSIGN = ASSIGN + ASSIGN + 1"
            ],
            "source_orig": [
                "final[\"Family\"] = final[\"SibSp\"] + final[\"Parch\"] + 1"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.map(lambda s: 1 if s == 1 else 0)",
                "ASSIGN = ASSIGN.map(lambda s: 1 if s == 2 else 0)",
                "ASSIGN = ASSIGN.map(lambda s: 1 if 3 <= s <= 4 else 0)",
                "ASSIGN = ASSIGN.map(lambda s: 1 if s >= 5 else 0)"
            ],
            "source_orig": [
                "final['Single'] = final['Family'].map(lambda s: 1 if s == 1 else 0)\n",
                "final['SmallF'] = final['Family'].map(lambda s: 1 if  s == 2  else 0)\n",
                "final['MedF'] = final['Family'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
                "final['LargeF'] = final['Family'].map(lambda s: 1 if s >= 5 else 0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final['Embarked'].fillna(\"S\",inplace = True)"
            ],
            "source_orig": [
                "final['Embarked'].fillna(\"S\",inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.get_dummies(ASSIGN, columns = [\"Embarked\"], prefix=\"Embarked_from_\")"
            ],
            "source_orig": [
                "final = pd.get_dummies(final, columns = [\"Embarked\"], prefix=\"Embarked_from_\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.Cabin.isnull().sum()"
            ],
            "source_orig": [
                "final.Cabin.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.Cabin.value_counts()"
            ],
            "source_orig": [
                "final.Cabin.value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final['Cabin_final'] = df['Cabin'].str[0]"
            ],
            "source_orig": [
                "final['Cabin_final'] = df['Cabin'].str[0]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final['Cabin_final'].fillna('Unknown',inplace = True)"
            ],
            "source_orig": [
                "final['Cabin_final'].fillna('Unknown',inplace = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final['Cabin_final'].value_counts()"
            ],
            "source_orig": [
                "final['Cabin_final'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final.drop(['Cabin'],axis = 1,inplace = True)"
            ],
            "source_orig": [
                "final.drop(['Cabin'],axis = 1,inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.get_dummies(ASSIGN, columns = [\"Cabin_final\"],prefix=\"Cabin_\")"
            ],
            "source_orig": [
                "final = pd.get_dummies(final, columns = [\"Cabin_final\"],prefix=\"Cabin_\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.head()"
            ],
            "source_orig": [
                "final.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.Ticket.unique()"
            ],
            "source_orig": [
                "final.Ticket.unique()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.Ticket.value_counts()"
            ],
            "source_orig": [
                "final.Ticket.value_counts()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.astype(str)",
                "final['Ticket_length'] = final.Ticket.apply(len)",
                "final['Ticket_length'].astype(int)",
                "final['Ticket_length'].unique()"
            ],
            "source_orig": [
                "final['Ticket'] = final['Ticket'].astype(str)\n",
                "final['Ticket_length'] = final.Ticket.apply(len)\n",
                "final['Ticket_length'].astype(int)\n",
                "final['Ticket_length'].unique()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final['Ticket_length'] = np.where(((final.Ticket_length == 3) | (final.Ticket_length == 4) | (final.Ticket_length == 5)),4,final.Ticket_length)",
                "final['Ticket_length'] = np.where(((final.Ticket_length == 6)),5,final.Ticket_length)",
                "final['Ticket_length'] = np.where(((final.Ticket_length == 7) | (final.Ticket_length == 8) | (final.Ticket_length == 9) | (final.Ticket_length == 10) | (final.Ticket_length == 13)",
                "| (final.Ticket_length == 17)| (final.Ticket_length == 16)| (final.Ticket_length == 13)| (final.Ticket_length == 12) | (final.Ticket_length == 15)",
                "| (final.Ticket_length == 11)| (final.Ticket_length == 18)),12,final.Ticket_length)"
            ],
            "source_orig": [
                "final['Ticket_length'] = np.where(((final.Ticket_length == 3) | (final.Ticket_length == 4) | (final.Ticket_length == 5)),4,final.Ticket_length)\n",
                "\n",
                "final['Ticket_length'] = np.where(((final.Ticket_length == 6)),5,final.Ticket_length)\n",
                "\n",
                "final['Ticket_length'] = np.where(((final.Ticket_length == 7) | (final.Ticket_length == 8) | (final.Ticket_length == 9) | (final.Ticket_length == 10) | (final.Ticket_length == 13)\n",
                "                                 | (final.Ticket_length == 17)| (final.Ticket_length == 16)| (final.Ticket_length == 13)| (final.Ticket_length == 12) | (final.Ticket_length == 15)\n",
                "                                 | (final.Ticket_length == 11)| (final.Ticket_length == 18)),12,final.Ticket_length)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final['Ticket_length'].value_counts()"
            ],
            "source_orig": [
                "final['Ticket_length'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final['Ticket_length'] = final['Ticket_length'].astype(str)",
                "final['Ticket_length'] = np.where(((final.Ticket_length == '4')),'Below 6',final.Ticket_length)",
                "final['Ticket_length'] = np.where(((final.Ticket_length == '5')),'At 6',final.Ticket_length)",
                "final['Ticket_length'] = np.where(((final.Ticket_length == '12')),'Above 6',final.Ticket_length)"
            ],
            "source_orig": [
                "final['Ticket_length'] = final['Ticket_length'].astype(str)\n",
                "\n",
                "final['Ticket_length'] = np.where(((final.Ticket_length == '4')),'Below 6',final.Ticket_length)\n",
                "final['Ticket_length'] = np.where(((final.Ticket_length == '5')),'At 6',final.Ticket_length)\n",
                "final['Ticket_length'] = np.where(((final.Ticket_length == '12')),'Above 6',final.Ticket_length)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.get_dummies(final.Ticket_length, prefix = 'Ticket Length')",
                "ASSIGN = pd.concat([ASSIGN , conversion], axis = 1)",
                "ASSIGN.drop(['Ticket','Ticket_length'],axis = 1, inplace = True)"
            ],
            "source_orig": [
                "conversion = pd.get_dummies(final.Ticket_length, prefix = 'Ticket Length')\n",
                "final = pd.concat([final , conversion], axis = 1)\n",
                " \n",
                "final.drop(['Ticket','Ticket_length'],axis = 1, inplace = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.head()"
            ],
            "source_orig": [
                "final.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.get_dummies(ASSIGN, columns = [\"Sex\"],prefix=\"Gender_\")"
            ],
            "source_orig": [
                "final = pd.get_dummies(final, columns = [\"Sex\"],prefix=\"Gender_\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final.head()",
                "final.drop(['PassengerId'],axis = 1,inplace = True)",
                "final.drop(['SibSp','Parch','Family'],axis = 1,inplace = True)"
            ],
            "source_orig": [
                "final.head()\n",
                "final.drop(['PassengerId'],axis = 1,inplace = True)\n",
                "final.drop(['SibSp','Parch','Family'],axis = 1,inplace = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "final.dtypes"
            ],
            "source_orig": [
                "final.dtypes"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.isnull().sum()"
            ],
            "source_orig": [
                "final.isnull().sum()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(x = 'Survived', data = df1)"
            ],
            "source_orig": [
                "sns.countplot(x = 'Survived', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(x = 'Pclass', data = df1)"
            ],
            "source_orig": [
                "sns.countplot(x = 'Pclass', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(x = 'Title', data = df3)"
            ],
            "source_orig": [
                "sns.countplot(x = 'Title', data = df3)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(x = 'Sex', data = df1)"
            ],
            "source_orig": [
                "sns.countplot(x = 'Sex', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.set(rc={'figure.figsize':(40,5)})",
                "sns.countplot(x = 'Age', data = df1)"
            ],
            "source_orig": [
                "sns.set(rc={'figure.figsize':(40,5)})\n",
                "sns.countplot(x = 'Age', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = df1['Age']",
                "sns.distplot(ASSIGN, hist=True, rug=True)"
            ],
            "source_orig": [
                "x = df1['Age']\n",
                "sns.distplot(x, hist=True, rug=True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = df1['Fare']",
                "sns.distplot(ASSIGN, hist=True, rug=True)"
            ],
            "source_orig": [
                "x = df1['Fare']\n",
                "sns.distplot(x, hist=True, rug=True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = final['Fare']",
                "sns.distplot(ASSIGN, hist=True, rug=True)"
            ],
            "source_orig": [
                "x = final['Fare']\n",
                "sns.distplot(x, hist=True, rug=True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.set(rc={'figure.figsize':(15,5)})",
                "sns.countplot(x = 'SibSp', data = df1)"
            ],
            "source_orig": [
                "sns.set(rc={'figure.figsize':(15,5)})\n",
                "sns.countplot(x = 'SibSp', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(x = 'Parch', data = df1)"
            ],
            "source_orig": [
                "sns.countplot(x = 'Parch', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(x = 'Embarked', data = df1)"
            ],
            "source_orig": [
                "sns.countplot(x = 'Embarked', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x ='Survived', y ='Age', data = df1)"
            ],
            "source_orig": [
                "sns.catplot(x ='Survived', y ='Age', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x ='Survived', y ='SibSp', data = df1)"
            ],
            "source_orig": [
                "sns.catplot(x ='Survived', y ='SibSp', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x ='Survived', y ='Parch', data = df1)"
            ],
            "source_orig": [
                "sns.catplot(x ='Survived', y ='Parch', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x = 'Sex',y='Survived',hue = 'Pclass', kind = 'bar', data = df1, col = 'Pclass', color = 'purple')"
            ],
            "source_orig": [
                "sns.catplot(x = 'Sex',y='Survived',hue = 'Pclass', kind = 'bar', data = df1, col = 'Pclass', color = 'purple')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x = 'Title',y='Survived',hue = 'Sex', kind = 'bar', data = df3, col = 'Sex', palette = 'GnBu_d',aspect =2)"
            ],
            "source_orig": [
                "sns.catplot(x = 'Title',y='Survived',hue = 'Sex', kind = 'bar', data = df3, col = 'Sex', palette = 'GnBu_d',aspect =2)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x = 'SibSp',y='Survived',hue = 'Pclass',kind = 'violin', data = df3, palette = 'BuGn_r', col = 'Pclass')"
            ],
            "source_orig": [
                "sns.catplot(x = 'SibSp',y='Survived',hue = 'Pclass',kind = 'violin', data = df3, palette = 'BuGn_r', col = 'Pclass')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x = 'Parch',y='Survived',hue = 'Pclass',kind = 'violin', data = df3, palette = 'cubehelix', col = 'Pclass')"
            ],
            "source_orig": [
                "sns.catplot(x = 'Parch',y='Survived',hue = 'Pclass',kind = 'violin', data = df3, palette = 'cubehelix', col = 'Pclass')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x = 'Embarked',y='Survived',kind = 'point', data = df3, hue = 'Pclass', col = 'Pclass')"
            ],
            "source_orig": [
                "sns.catplot(x = 'Embarked',y='Survived',kind = 'point', data = df3, hue = 'Pclass', col = 'Pclass')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.jointplot(x=df1['Age'], y=df1['SibSp'], kind = 'kde')"
            ],
            "source_orig": [
                "sns.jointplot(x=df1['Age'], y=df1['SibSp'], kind = 'kde')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = final.copy()",
                "ASSIGN = pd.concat([df['Survived'],result['Survived']],axis = 0)",
                "ASSIGN = pd.concat([ASSIGN,sur],axis = 1)"
            ],
            "source_orig": [
                "correlation = final.copy()\n",
                "sur = pd.concat([df['Survived'],result['Survived']],axis = 0)\n",
                "correlation = pd.concat([correlation,sur],axis = 1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(30,30))",
                "sns.heatmap(correlation.corr(), annot=True, linewidth=0.5, cmap='coolwarm')"
            ],
            "source_orig": [
                "plt.figure(figsize=(30,30))\n",
                "sns.heatmap(correlation.corr(), annot=True, linewidth=0.5, cmap='coolwarm')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "#The models trained\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import LinearSVC\n",
                "from sklearn import svm\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from xgboost import XGBClassifier\n",
                "from sklearn.naive_bayes import BernoulliNB\n",
                "\n",
                "#For Scaling and Hyperparameter Tuning\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.model_selection import RandomizedSearchCV\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.metrics import accuracy_score,confusion_matrix\n",
                "from sklearn import metrics\n",
                "\n",
                "#Voting Classifier\n",
                "from sklearn.ensemble import VotingClassifier "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = final[:891]",
                "ASSIGN = MinMaxScaler()",
                "ASSIGN = feature_scaler.fit_transform(ASSIGN)"
            ],
            "source_orig": [
                "x_train = final[:891]\n",
                "feature_scaler = MinMaxScaler()\n",
                "x_train = feature_scaler.fit_transform(x_train)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = final[891:]",
                "ASSIGN = MinMaxScaler()",
                "ASSIGN = feature_scaler.fit_transform(ASSIGN)"
            ],
            "source_orig": [
                "y_train = final[891:]\n",
                "feature_scaler = MinMaxScaler()\n",
                "y_train = feature_scaler.fit_transform(y_train)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = df1['Survived']"
            ],
            "source_orig": [
                "x_test = df1['Survived']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = result['Survived']"
            ],
            "source_orig": [
                "y_test = result['Survived']"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results",
                "process_data"
            ],
            "source": [
                "LR.fit(x_train,x_test)",
                "ASSIGN = LR.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('LogisticRegression(No HT).csv',index = False)"
            ],
            "source_orig": [
                "LR.fit(x_train,x_test)\n",
                "model1pred = LR.predict(y_train)\n",
                "submission1 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission1['PassengerId'] = result['PassengerId']\n",
                "submission1['Survived'] = model1pred\n",
                "submission1.to_csv('LogisticRegression(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "LR.score(y_train,y_test)"
            ],
            "source_orig": [
                "LR.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = cross_val_score(SVC,x_train,x_test,ASSIGN=10)",
                "ASSIGN = cv.mean()",
                "accuracy.append(ASSIGN)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "SVC = LinearSVC()\n",
                "#estimator.append(('LSVC',LinearSVC()))\n",
                "cv = cross_val_score(SVC,x_train,x_test,cv=10)\n",
                "accuracy2 = cv.mean()\n",
                "accuracy.append(accuracy2)\n",
                "print(cv)\n",
                "print(cv.mean())\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "SVC.fit(x_train,x_test)",
                "SVC.score(y_train,y_test)"
            ],
            "source_orig": [
                "SVC.fit(x_train,x_test)\n",
                "SVC.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "SVC.fit(x_train,x_test)",
                "ASSIGN = SVC.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('LinearSVC(No HT).csv',index = False)"
            ],
            "source_orig": [
                "SVC.fit(x_train,x_test)\n",
                "model2pred = SVC.predict(y_train)\n",
                "submission2 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission2['PassengerId'] = result['PassengerId']\n",
                "submission2['Survived'] = model2pred\n",
                "submission2.to_csv('LinearSVC(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = svm.SVC(kernel = 'ASSIGN', gamma = 'scale')",
                "ASSIGN = cross_val_score(poly,x_train,x_test,ASSIGN=10)",
                "ASSIGN = cv.mean()",
                "accuracy.append(ASSIGN)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "poly = svm.SVC(kernel = 'poly', gamma = 'scale')\n",
                "#estimator.append(('PSVC',svm.SVC(kernel = 'poly', gamma = 'scale')))\n",
                "cv = cross_val_score(poly,x_train,x_test,cv=10)\n",
                "accuracy3 = cv.mean()\n",
                "accuracy.append(accuracy3)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "poly.fit(x_train,x_test)",
                "poly.score(y_train,y_test)"
            ],
            "source_orig": [
                "poly.fit(x_train,x_test)\n",
                "poly.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = poly.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('PolynomialSVC(No HT).csv',index = False)"
            ],
            "source_orig": [
                "model3pred = poly.predict(y_train)\n",
                "submission3 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission3['PassengerId'] = result['PassengerId']\n",
                "submission3['Survived'] = model3pred\n",
                "submission3.to_csv('PolynomialSVC(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = DecisionTreeClassifier(random_state = 5)",
                "estimator.append(('ASSIGN',DecisionTreeClassifier(random_state = 5)))",
                "ASSIGN = cross_val_score(DT,x_train,x_test,ASSIGN=10)",
                "ASSIGN = cv.mean()",
                "accuracy.append(ASSIGN)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "DT = DecisionTreeClassifier(random_state = 5)\n",
                "estimator.append(('DT',DecisionTreeClassifier(random_state = 5)))\n",
                "cv = cross_val_score(DT,x_train,x_test,cv=10)\n",
                "accuracy4 = cv.mean()\n",
                "accuracy.append(accuracy4)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "DT.fit(x_train,x_test)",
                "DT.score(y_train,y_test)"
            ],
            "source_orig": [
                "DT.fit(x_train,x_test)\n",
                "DT.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = DT.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('Decision Tree(No HT).csv',index = False)"
            ],
            "source_orig": [
                "model4pred = DT.predict(y_train)\n",
                "submission4 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission4['PassengerId'] = result['PassengerId']\n",
                "submission4['Survived'] = model4pred\n",
                "submission4.to_csv('Decision Tree(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "estimator.append(('GNB',GaussianNB()))",
                "ASSIGN = cross_val_score(GNB,x_train,x_test,ASSIGN=10)",
                "ASSIGN = cv.mean()",
                "accuracy.append(ASSIGN)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "GNB = GaussianNB()\n",
                "estimator.append(('GNB',GaussianNB()))\n",
                "cv = cross_val_score(GNB,x_train,x_test,cv=10)\n",
                "accuracy5 = cv.mean()\n",
                "accuracy.append(accuracy5)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "GNB.fit(x_train,x_test)",
                "GNB.score(y_train,y_test)"
            ],
            "source_orig": [
                "GNB.fit(x_train,x_test)\n",
                "GNB.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = GNB.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('Gaussian NB(No HT).csv',index = False)"
            ],
            "source_orig": [
                "model5pred = GNB.predict(y_train)\n",
                "submission5 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission5['PassengerId'] = result['PassengerId']\n",
                "submission5['Survived'] = model5pred\n",
                "submission5.to_csv('Gaussian NB(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "estimator.append(('MNB',MultinomialNB()))",
                "ASSIGN = cross_val_score(MNB,x_train,x_test,ASSIGN=10)",
                "ASSIGN = cv.mean()",
                "accuracy.append(ASSIGN)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "MNB = MultinomialNB()\n",
                "estimator.append(('MNB',MultinomialNB()))\n",
                "cv = cross_val_score(MNB,x_train,x_test,cv=10)\n",
                "accuracy6 = cv.mean()\n",
                "accuracy.append(accuracy6)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "MNB.fit(x_train,x_test)",
                "MNB.score(y_train,y_test)"
            ],
            "source_orig": [
                "MNB.fit(x_train,x_test)\n",
                "MNB.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "MNB.fit(x_train,x_test)",
                "ASSIGN = MNB.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('MultinomialNB(No HT).csv',index = False)"
            ],
            "source_orig": [
                "MNB.fit(x_train,x_test)\n",
                "model6pred = MNB.predict(y_train)\n",
                "submission6 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission6['PassengerId'] = result['PassengerId']\n",
                "submission6['Survived'] = model6pred\n",
                "submission6.to_csv('MultinomialNB(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = RandomForestClassifier(random_state = 5)",
                "estimator.append(('ASSIGN',RandomForestClassifier(random_state = 5)))",
                "ASSIGN = cross_val_score(RF,x_train,x_test,ASSIGN=10)",
                "ASSIGN = cv.mean()",
                "accuracy.append(ASSIGN)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "RF = RandomForestClassifier(random_state = 5)\n",
                "estimator.append(('RF',RandomForestClassifier(random_state = 5)))\n",
                "cv = cross_val_score(RF,x_train,x_test,cv=10)\n",
                "accuracy7 = cv.mean()\n",
                "accuracy.append(accuracy7)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "RF.fit(x_train,x_test)",
                "RF.score(y_train,y_test)"
            ],
            "source_orig": [
                "RF.fit(x_train,x_test)\n",
                "RF.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "RF.fit(x_train,x_test)",
                "ASSIGN = RF.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('RandomForest(No HT).csv',index = False)"
            ],
            "source_orig": [
                "RF.fit(x_train,x_test)\n",
                "model7pred = RF.predict(y_train)\n",
                "submission7 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission7['PassengerId'] = result['PassengerId']\n",
                "submission7['Survived'] = model7pred\n",
                "submission7.to_csv('RandomForest(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "estimator.append(('GBC',GradientBoostingClassifier(random_state = 5)))",
                "ASSIGN = cross_val_score(GBC,x_train,x_test,ASSIGN=10)",
                "ASSIGN = cv.mean()",
                "accuracy.append(ASSIGN)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "GBC = GradientBoostingClassifier(random_state = 5)\n",
                "estimator.append(('GBC',GradientBoostingClassifier(random_state = 5)))\n",
                "cv = cross_val_score(GBC,x_train,x_test,cv=10)\n",
                "accuracy8 = cv.mean()\n",
                "accuracy.append(accuracy8)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "GBC.fit(x_train,x_test)",
                "GBC.score(y_train,y_test)"
            ],
            "source_orig": [
                "GBC.fit(x_train,x_test)\n",
                "GBC.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "GBC.fit(x_train,x_test)",
                "ASSIGN = GBC.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('GradientBoosting(No HT).csv',index = False)"
            ],
            "source_orig": [
                "GBC.fit(x_train,x_test)\n",
                "model8pred = GBC.predict(y_train)\n",
                "submission8 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission8['PassengerId'] = result['PassengerId']\n",
                "submission8['Survived'] = model8pred\n",
                "submission8.to_csv('GradientBoosting(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "estimator.append(('XGB', XGBClassifier(random_state = 5)))",
                "ASSIGN = cross_val_score(XGB,x_train,x_test,ASSIGN=10)",
                "ASSIGN = cv.mean()",
                "accuracy.append(ASSIGN)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "XGB = XGBClassifier(random_state = 5)\n",
                "estimator.append(('XGB', XGBClassifier(random_state = 5)))\n",
                "cv = cross_val_score(XGB,x_train,x_test,cv=10)\n",
                "accuracy9 = cv.mean()\n",
                "accuracy.append(accuracy9)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "XGB.fit(x_train,x_test)",
                "XGB.score(y_train,y_test)"
            ],
            "source_orig": [
                "XGB.fit(x_train,x_test)\n",
                "XGB.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "XGB.fit(x_train,x_test)",
                "ASSIGN = XGB.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('XGBoosting(No HT).csv',index = False)"
            ],
            "source_orig": [
                "XGB.fit(x_train,x_test)\n",
                "model9pred = XGB.predict(y_train)\n",
                "submission9 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission9['PassengerId'] = result['PassengerId']\n",
                "submission9['Survived'] = model9pred\n",
                "submission9.to_csv('XGBoosting(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data",
                "train_model"
            ],
            "source": [
                "ASSIGN = range(1,20)",
                "ASSIGN = {}",
                "ASSIGN = []",
                "for k in ASSIGN:",
                "ASSIGN = KNeighborsClassifier(n_neighbors = k)",
                "ASSIGN.fit(x_train,x_test)",
                "ASSIGN = knn.predict(y_train)",
                "ASSIGN = metrics.accuracy_score(ASSIGN,y_pred)",
                "ASSIGN.append(metrics.accuracy_score(result['Survived'],ASSIGN))",
                "plt.plot(ASSIGN,ASSIGN)",
                "plt.xlabel(\"Value of K\")",
                "plt.ylabel(\"Accuracy\")"
            ],
            "source_orig": [
                "Krange = range(1,20)\n",
                "scores = {}\n",
                "scores_list = []\n",
                "for k in Krange:\n",
                "    knn = KNeighborsClassifier(n_neighbors = k)\n",
                "    knn.fit(x_train,x_test)\n",
                "    y_pred = knn.predict(y_train)\n",
                "    scores[k] = metrics.accuracy_score(result['Survived'],y_pred)\n",
                "    scores_list.append(metrics.accuracy_score(result['Survived'],y_pred))\n",
                "    \n",
                "plt.plot(Krange,scores_list)\n",
                "plt.xlabel(\"Value of K\")\n",
                "plt.ylabel(\"Accuracy\")"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "estimator.append(('KNN',KNeighborsClassifier(n_neighbors = 11)))",
                "ASSIGN = cross_val_score(KNN,x_train,x_test,ASSIGN=10)",
                "ASSIGN = cv.mean()",
                "accuracy.append(ASSIGN)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "KNN = KNeighborsClassifier(n_neighbors = 11)\n",
                "estimator.append(('KNN',KNeighborsClassifier(n_neighbors = 11)))\n",
                "cv = cross_val_score(KNN,x_train,x_test,cv=10)\n",
                "accuracy10 = cv.mean()\n",
                "accuracy.append(accuracy10)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "KNN.fit(x_train,x_test)",
                "KNN.score(y_train,y_test)"
            ],
            "source_orig": [
                "KNN.fit(x_train,x_test)\n",
                "KNN.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "KNN.fit(x_train,x_test)",
                "ASSIGN = KNN.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('KNN(No HT).csv',index = False)"
            ],
            "source_orig": [
                "KNN.fit(x_train,x_test)\n",
                "model10pred = KNN.predict(y_train)\n",
                "submission10 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission10['PassengerId'] = result['PassengerId']\n",
                "submission10['Survived'] = model10pred\n",
                "submission10.to_csv('KNN(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = output1.groupby(['Models'])['Accuracy'].mean().reset_index().sort_values(by='Accuracy',ascending=False)",
                "ASSIGN.head(10).style.background_gradient(cmap='Reds')"
            ],
            "source_orig": [
                "o = output1.groupby(['Models'])['Accuracy'].mean().reset_index().sort_values(by='Accuracy',ascending=False)\n",
                "o.head(10).style.background_gradient(cmap='Reds')\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = VotingClassifier(estimators = estimator, voting ='soft')",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN = vot_soft.predict(y_train)",
                "ASSIGN.score(y_train,y_test)"
            ],
            "source_orig": [
                "vot_soft = VotingClassifier(estimators = estimator, voting ='soft') \n",
                "vot_soft.fit(x_train, x_test) \n",
                "y_pred = vot_soft.predict(y_train)\n",
                "vot_soft.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = vot_soft.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('SoftVoting(NO HT).csv',index = False)"
            ],
            "source_orig": [
                "modelpred1 = vot_soft.predict(y_train)\n",
                "sub1 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "sub1['PassengerId'] = result['PassengerId']\n",
                "sub1['Survived'] = modelpred1\n",
                "sub1.to_csv('SoftVoting(NO HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = VotingClassifier(estimators = estimator, voting ='hard')",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN = vot_hard.predict(y_train)",
                "ASSIGN.score(y_train,y_test)"
            ],
            "source_orig": [
                "vot_hard = VotingClassifier(estimators = estimator, voting ='hard') \n",
                "vot_hard.fit(x_train, x_test) \n",
                "y_pred = vot_hard.predict(y_train)\n",
                "vot_hard.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = vot_hard.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('HardVoting(NO HT).csv',index = False)"
            ],
            "source_orig": [
                "modelpred2 = vot_hard.predict(y_train)\n",
                "sub2 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "sub2['PassengerId'] = result['PassengerId']\n",
                "sub2['Survived'] = modelpred2\n",
                "sub2.to_csv('HardVoting(NO HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "\"\"\"",
                "ASSIGN = [0.01,0.1, 1, 10,50, 100]",
                "ASSIGN = ['l2']",
                "ASSIGN = ['newton-cg','lbfgs','liblinear']",
                "ASSIGN = ['dict','balanced','None']",
                "ASSIGN = [900,1000,1100,1200]",
                "ASSIGN = LogisticRegression()",
                "ASSIGN = {'C': [0.01,0.1, 1, 10,50, 100],'penalty' : ['l2'],'solver' : ['newton-cg','lbfgs','liblinear'],'class_weight':['dict','balanced','None'],'max_iter':[900,1000,1100,1200]}",
                "ASSIGN = GridSearchCV(Log, parameters, scoring='accuracy',cv =10)",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN.best_params_",
                "\"\"\""
            ],
            "source_orig": [
                "\"\"\"\n",
                "C = [0.01,0.1, 1, 10,50, 100]\n",
                "penalty = ['l2']\n",
                "solver = ['newton-cg','lbfgs','liblinear']\n",
                "class_weight = ['dict','balanced','None']\n",
                "max_iter = [900,1000,1100,1200]\n",
                "\n",
                "Log = LogisticRegression()\n",
                "\n",
                "parameters = {'C': [0.01,0.1, 1, 10,50, 100],'penalty' : ['l2'],'solver' : ['newton-cg','lbfgs','liblinear'],'class_weight':['dict','balanced','None'],'max_iter':[900,1000,1100,1200]}\n",
                "\n",
                "log_regressor = GridSearchCV(Log, parameters, scoring='accuracy',cv =10)\n",
                "log_regressor.fit(x_train, x_test)\n",
                "log_regressor.best_params_\n",
                "\"\"\""
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = LogisticRegression(C = 100,penalty = 'l2', solver = 'newton-cg',class_weight = 'dict', max_iter = 900)",
                "Estimator.append(('ASSIGN',LogisticRegression(C = 1,penalty = 'l2', solver = 'newton-cg',class_weight = 'dict', max_iter = 900)))",
                "ASSIGN = cross_val_score(lr,x_train,x_test,ASSIGN=10)",
                "Accuracy1 = ASSIGN.mean()",
                "Accuracy.append(Accuracy1)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "lr = LogisticRegression(C = 100,penalty = 'l2', solver = 'newton-cg',class_weight = 'dict', max_iter = 900)\n",
                "Estimator.append(('lr',LogisticRegression(C = 1,penalty = 'l2', solver = 'newton-cg',class_weight = 'dict', max_iter = 900)))\n",
                "cv = cross_val_score(lr,x_train,x_test,cv=10)\n",
                "Accuracy1 = cv.mean()\n",
                "Accuracy.append(Accuracy1)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "lr.fit(x_train,x_test)",
                "lr.score(y_train,y_test)"
            ],
            "source_orig": [
                "lr.fit(x_train,x_test)\n",
                "lr.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = lr.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('LogisticRegression(HT).csv',index = False)"
            ],
            "source_orig": [
                "model11pred = lr.predict(y_train)\n",
                "submission11 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission11['PassengerId'] = result['PassengerId']\n",
                "submission11['Survived'] = model11pred\n",
                "submission11.to_csv('LogisticRegression(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "SETUP",
                "'''",
                "ASSIGN = ['l1','l2']",
                "ASSIGN = ['hinge','squared_hinge']",
                "ASSIGN = ['dict','balanced','None']",
                "ASSIGN = [.1,1,10,50,100,150]",
                "ASSIGN = {'penalty':['l1','l2'],'loss':['hinge','squared_hinge'],'class_weight':['dict','balanced','None'] ,'C': [.1,1,10,50,100,150]}",
                "ASSIGN = GridSearchCV(SVM, parameters, scoring='accuracy' ,cv =10)",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN.best_params_",
                "'''"
            ],
            "source_orig": [
                "'''\n",
                "penalty = ['l1','l2']\n",
                "loss = ['hinge','squared_hinge']\n",
                "class_weight = ['dict','balanced','None']\n",
                "C = [.1,1,10,50,100,150]\n",
                "\n",
                "SVM = LinearSVC()\n",
                "\n",
                "parameters = {'penalty':['l1','l2'],'loss':['hinge','squared_hinge'],'class_weight':['dict','balanced','None'] ,'C': [.1,1,10,50,100,150]}\n",
                "\n",
                "SVM_classifier = GridSearchCV(SVM, parameters, scoring='accuracy' ,cv =10)\n",
                "SVM_classifier.fit(x_train, x_test)\n",
                "SVM_classifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = LinearSVC(C = 0.1,penalty = 'l2', loss = 'hinge',class_weight = 'balanced')",
                "ASSIGN = cross_val_score(svc,x_train,x_test,ASSIGN=10)",
                "Accuracy2 = ASSIGN.mean()",
                "Accuracy.append(Accuracy2)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "svc = LinearSVC(C = 0.1,penalty = 'l2', loss = 'hinge',class_weight = 'balanced')\n",
                "cv = cross_val_score(svc,x_train,x_test,cv=10)\n",
                "Accuracy2 = cv.mean()\n",
                "Accuracy.append(Accuracy2)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "svc.fit(x_train,x_test)",
                "svc.score(y_train,y_test)"
            ],
            "source_orig": [
                "svc.fit(x_train,x_test)\n",
                "svc.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = svc.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('SVCLinear(HT).csv',index = False)"
            ],
            "source_orig": [
                "model12pred = svc.predict(y_train)\n",
                "submission12 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission12['PassengerId'] = result['PassengerId']\n",
                "submission12['Survived'] = model12pred\n",
                "submission12.to_csv('SVCLinear(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "'''",
                "ASSIGN = ['poly']",
                "ASSIGN = [1,2,3]",
                "ASSIGN = ['balanced','dict']",
                "ASSIGN = [.1,1,10,]",
                "ASSIGN = ['scale','auto']",
                "ASSIGN = svm.SVC()",
                "ASSIGN = {'kernel':['poly'],'class_weight':['balanced','dict'] ,'C': [.1,1,10],'degree':[1,2,3],'gamma':['scale','auto']}",
                "ASSIGN = GridSearchCV(s, parameters, scoring='accuracy' ,cv =10)",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN.best_params_",
                "'''"
            ],
            "source_orig": [
                "'''\n",
                "kernel = ['poly']\n",
                "degree = [1,2,3]\n",
                "class_weight = ['balanced','dict']\n",
                "C = [.1,1,10,]\n",
                "gamma = ['scale','auto']\n",
                "\n",
                "s = svm.SVC()\n",
                "\n",
                "parameters = {'kernel':['poly'],'class_weight':['balanced','dict'] ,'C': [.1,1,10],'degree':[1,2,3],'gamma':['scale','auto']}\n",
                "\n",
                "svcc = GridSearchCV(s, parameters, scoring='accuracy' ,cv =10)\n",
                "svcc.fit(x_train, x_test)\n",
                "svcc.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = svm.SVC(C = 1,degree = 2, kernel = 'poly',class_weight = 'balanced',gamma = 'scale')",
                "ASSIGN = cross_val_score(svc,x_train,x_test,ASSIGN=10)",
                "Accuracy3 = ASSIGN.mean()",
                "Accuracy.append(Accuracy3)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "SVM_all = svm.SVC(C = 1,degree = 2, kernel = 'poly',class_weight = 'balanced',gamma = 'scale')\n",
                "cv = cross_val_score(svc,x_train,x_test,cv=10)\n",
                "Accuracy3 = cv.mean()\n",
                "Accuracy.append(Accuracy3)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "SVM_all.fit(x_train,x_test)",
                "SVM_all.score(y_train,y_test)"
            ],
            "source_orig": [
                "SVM_all.fit(x_train,x_test)\n",
                "SVM_all.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = SVM_all.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('PolynomialSVM(HT).csv',index = False)"
            ],
            "source_orig": [
                "model13pred = SVM_all.predict(y_train)\n",
                "submission13 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission13['PassengerId'] = result['PassengerId']\n",
                "submission13['Survived'] = model13pred\n",
                "submission13.to_csv('PolynomialSVM(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "'''",
                "ASSIGN = ['gini','entropy']",
                "ASSIGN = ['best','random']",
                "ASSIGN = [5,10,15,20,25]",
                "ASSIGN = [2,3,4,5]",
                "ASSIGN = ['dict','balanced','None']",
                "ASSIGN = [5,6]",
                "ASSIGN = DecisionTreeClassifier()",
                "ASSIGN = {'criterion': ['gini','entropy'],'splitter': ['best','random'], 'max_depth':[5,10,15,20,25],'min_samples_split':[2,3,4,5],'class_weight':['dict','balanced','None'],'random_state':[5,6]}",
                "ASSIGN = GridSearchCV(Tree, parameters, scoring='accuracy' ,cv = 10)",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN.best_params_",
                "'''"
            ],
            "source_orig": [
                "'''\n",
                "criterion = ['gini','entropy']\n",
                "splitter = ['best','random']\n",
                "max_depth = [5,10,15,20,25]\n",
                "min_samples_split = [2,3,4,5]\n",
                "class_weight = ['dict','balanced','None']\n",
                "random_state = [5,6]\n",
                "\n",
                "\n",
                "Tree = DecisionTreeClassifier()\n",
                "\n",
                "parameters = {'criterion': ['gini','entropy'],'splitter': ['best','random'], 'max_depth':[5,10,15,20,25],'min_samples_split':[2,3,4,5],'class_weight':['dict','balanced','None'],'random_state':[5,6]}\n",
                "\n",
                "tree_classifier = GridSearchCV(Tree, parameters, scoring='accuracy' ,cv = 10)\n",
                "tree_classifier.fit(x_train, x_test)\n",
                "tree_classifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = DecisionTreeClassifier(class_weight = 'balanced',criterion = 'entropy',max_depth = 5,min_samples_split = 2,splitter = 'best',random_state = 6)",
                "Estimator.append(('ASSIGN',DecisionTreeClassifier(class_weight = 'balanced',criterion = 'entropy',max_depth = 5,min_samples_split = 2,splitter = 'best',random_state = 6)))",
                "ASSIGN = cross_val_score(dt,x_train,x_test,ASSIGN=10)",
                "Accuracy4 = ASSIGN.mean()",
                "Accuracy.append(Accuracy4)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "dt = DecisionTreeClassifier(class_weight = 'balanced',criterion = 'entropy',max_depth = 5,min_samples_split = 2,splitter = 'best',random_state = 6)\n",
                "Estimator.append(('dt',DecisionTreeClassifier(class_weight = 'balanced',criterion = 'entropy',max_depth = 5,min_samples_split = 2,splitter = 'best',random_state = 6)))\n",
                "cv = cross_val_score(dt,x_train,x_test,cv=10)\n",
                "Accuracy4 = cv.mean()\n",
                "Accuracy.append(Accuracy4)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "dt.fit(x_train,x_test)",
                "dt.score(y_train,y_test)"
            ],
            "source_orig": [
                "dt.fit(x_train,x_test)\n",
                "dt.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = SVM_all.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('DecisionTrees(HT).csv',index = False)"
            ],
            "source_orig": [
                "model14pred = SVM_all.predict(y_train)\n",
                "submission14 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission14['PassengerId'] = result['PassengerId']\n",
                "submission14['Survived'] = model14pred\n",
                "submission14.to_csv('DecisionTrees(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "'''",
                "ASSIGN = [0.01,0.1, 1, 10, 100]",
                "ASSIGN = [True,False]",
                "ASSIGN = MultinomialNB()",
                "ASSIGN = {'alpha': [0.01,0.1, 1, 10, 100],'fit_prior' : [True,False]}",
                "ASSIGN = GridSearchCV(mnb, parameters, scoring='accuracy',cv =10)",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN.best_params_",
                "'''"
            ],
            "source_orig": [
                "'''\n",
                "alpha = [0.01,0.1, 1, 10, 100]\n",
                "fit_prior = [True,False]\n",
                "\n",
                "mnb = MultinomialNB()\n",
                "\n",
                "parameters = {'alpha': [0.01,0.1, 1, 10, 100],'fit_prior' : [True,False]}\n",
                "\n",
                "mn = GridSearchCV(mnb, parameters, scoring='accuracy',cv =10)\n",
                "mn.fit(x_train, x_test)\n",
                "mn.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = MultinomialNB(alpha = 1,fit_prior = True)",
                "Estimator.append(('ASSIGN',MultinomialNB(alpha = 1,fit_prior = True)))",
                "ASSIGN = cross_val_score(mnb,x_train,x_test,ASSIGN=10)",
                "Accuracy5 = ASSIGN.mean()",
                "Accuracy.append(Accuracy5)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "mnb = MultinomialNB(alpha = 1,fit_prior = True)\n",
                "Estimator.append(('mnb',MultinomialNB(alpha = 1,fit_prior = True)))\n",
                "cv = cross_val_score(mnb,x_train,x_test,cv=10)\n",
                "Accuracy5 = cv.mean()\n",
                "Accuracy.append(Accuracy5)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "mnb.fit(x_train,x_test)",
                "mnb.score(y_train,y_test)"
            ],
            "source_orig": [
                "mnb.fit(x_train,x_test)\n",
                "mnb.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = mnb.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('MultinomialNB(HT).csv',index = False)"
            ],
            "source_orig": [
                "model15pred = mnb.predict(y_train)\n",
                "submission15 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission15['PassengerId'] = result['PassengerId']\n",
                "submission15['Survived'] = model15pred\n",
                "submission15.to_csv('MultinomialNB(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "'''",
                "ASSIGN = [250,500,750,1000]",
                "ASSIGN = ['gini','entropy']",
                "ASSIGN = [5,10,15,20,25]",
                "ASSIGN = [2,3,4,5]",
                "ASSIGN = [True,False]",
                "ASSIGN = [True,False]",
                "ASSIGN = ['balanced','balanced_subsample','dict']",
                "ASSIGN = ['auto','sqrt','log2']",
                "ASSIGN = RandomForestClassifier()",
                "ASSIGN = {'n_estimators': [250,500,750,1000],'criterion': ['gini','entropy'],'max_depth':[5,10,15,20,25],'min_samples_split':[2,3,4,5],'bootstrap':[True,False]",
                ",'ASSIGN':[True,False],'ASSIGN':['balanced','balanced_subsample','dict'],'ASSIGN':['auto','sqrt','log2']}",
                "ASSIGN = RandomizedSearchCV(RF, parameters, scoring='accuracy' ,cv =50)",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN.best_params_",
                "'''"
            ],
            "source_orig": [
                "'''\n",
                "n_estimators = [250,500,750,1000]\n",
                "criterion = ['gini','entropy']\n",
                "max_depth = [5,10,15,20,25]\n",
                "min_samples_split = [2,3,4,5]\n",
                "bootstrap = [True,False]\n",
                "oob_score = [True,False]\n",
                "class_weight = ['balanced','balanced_subsample','dict']\n",
                "max_features = ['auto','sqrt','log2']\n",
                "\n",
                "RF = RandomForestClassifier()\n",
                "\n",
                "parameters = {'n_estimators': [250,500,750,1000],'criterion': ['gini','entropy'],'max_depth':[5,10,15,20,25],'min_samples_split':[2,3,4,5],'bootstrap':[True,False]\n",
                "              ,'oob_score':[True,False],'class_weight':['balanced','balanced_subsample','dict'],'max_features':['auto','sqrt','log2']}\n",
                "\n",
                "RFClassifier = RandomizedSearchCV(RF, parameters, scoring='accuracy' ,cv =50)\n",
                "RFClassifier.fit(x_train, x_test)\n",
                "RFClassifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "\"\"\"",
                "ASSIGN = [650,700,750,800,850]",
                "ASSIGN = ['gini']",
                "ASSIGN = [4,5]",
                "ASSIGN = [5,6]",
                "ASSIGN = [False,True]",
                "ASSIGN = [False,True]",
                "ASSIGN = ['balanced_subsample']",
                "ASSIGN = ['log2']",
                "ASSIGN = RandomForestClassifier()",
                "ASSIGN = {'n_estimators': [650,700,750,800,850],'criterion': ['gini'],'max_depth':[5,6],'min_samples_split':[4,5],'bootstrap':[False,True]",
                ",'ASSIGN':[False,True],'ASSIGN':['balanced_subsample'],'ASSIGN':['log2']}",
                "ASSIGN = GridSearchCV(rF, parameters, scoring='accuracy' ,cv =5)",
                "ASSIGN.fit(x_train,x_test)",
                "ASSIGN.best_params_",
                "\"\"\""
            ],
            "source_orig": [
                "\"\"\"\n",
                "n_estimators = [650,700,750,800,850]\n",
                "criterion = ['gini']\n",
                "max_depth = [4,5]\n",
                "min_samples_split = [5,6]\n",
                "bootstrap = [False,True]\n",
                "oob_score = [False,True]\n",
                "class_weight = ['balanced_subsample']\n",
                "max_features = ['log2']\n",
                "\n",
                "rF = RandomForestClassifier()\n",
                "\n",
                "parameters = {'n_estimators': [650,700,750,800,850],'criterion': ['gini'],'max_depth':[5,6],'min_samples_split':[4,5],'bootstrap':[False,True]\n",
                "              ,'oob_score':[False,True],'class_weight':['balanced_subsample'],'max_features':['log2']}\n",
                "\n",
                "RClassifier = GridSearchCV(rF, parameters, scoring='accuracy' ,cv =5)\n",
                "RClassifier.fit(x_train,x_test)\n",
                "RClassifier.best_params_\n",
                "\"\"\""
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = RandomForestClassifier(oob_score = True,n_estimators =650 ,min_samples_split = 4,max_features = 'log2',max_depth =6,criterion = 'gini',class_weight = 'balanced_subsample',bootstrap = True)",
                "Estimator.append(('ASSIGN',RandomForestClassifier(oob_score = True,n_estimators =650 ,min_samples_split = 4,max_features = 'log2',max_depth =6,criterion = 'gini',class_weight = 'balanced_subsample',bootstrap = True)))",
                "ASSIGN = cross_val_score(rf,x_train,x_test,ASSIGN=10)",
                "Accuracy6 = ASSIGN.mean()",
                "Accuracy.append(Accuracy6)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "rf = RandomForestClassifier(oob_score = True,n_estimators =650 ,min_samples_split = 4,max_features = 'log2',max_depth =6,criterion = 'gini',class_weight = 'balanced_subsample',bootstrap = True)\n",
                "Estimator.append(('rf',RandomForestClassifier(oob_score = True,n_estimators =650 ,min_samples_split = 4,max_features = 'log2',max_depth =6,criterion = 'gini',class_weight = 'balanced_subsample',bootstrap = True)))\n",
                "cv = cross_val_score(rf,x_train,x_test,cv=10)\n",
                "Accuracy6 = cv.mean()\n",
                "Accuracy.append(Accuracy6)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "rf.fit(x_train,x_test)",
                "rf.score(y_train,y_test)"
            ],
            "source_orig": [
                "rf.fit(x_train,x_test)\n",
                "rf.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "ASSIGN = rf.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('RandomForest(HT).csv',index = False)"
            ],
            "source_orig": [
                "model16pred = rf.predict(y_train)\n",
                "submission16 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission16['PassengerId'] = result['PassengerId']\n",
                "submission16['Survived'] = model16pred\n",
                "submission16.to_csv('RandomForest(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "'''",
                "ASSIGN = [250,500,750,1000]",
                "ASSIGN = [.01,.1,1,5]",
                "ASSIGN = [.01,.1,1,5]",
                "ASSIGN = [2,3,4,5]",
                "ASSIGN = [5,10,15,20,25]",
                "ASSIGN = ['deviance','exponential']",
                "ASSIGN = ['auto','sqrt','log2']",
                "ASSIGN = GradientBoostingClassifier()",
                "ASSIGN = {'n_estimators': [250,500,750,1000],'loss': ['deviance','exponential'],'max_features':['auto','sqrt','log2'],'learning_rate':[.01,.1,1,5],'subsample':[.01,.1,1,5],",
                "'ASSIGN':[2,3,4,5],'ASSIGN':[5,10,15,20,25]}",
                "ASSIGN = RandomizedSearchCV(GB, parameters, scoring='accuracy' ,cv =50)",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN.best_params_",
                "'''"
            ],
            "source_orig": [
                "'''\n",
                "n_estimators = [250,500,750,1000]\n",
                "learning_rate = [.01,.1,1,5]\n",
                "subsample = [.01,.1,1,5]\n",
                "min_samples_split = [2,3,4,5]\n",
                "max_depth = [5,10,15,20,25]\n",
                "loss = ['deviance','exponential']\n",
                "max_features = ['auto','sqrt','log2']\n",
                "\n",
                "GB = GradientBoostingClassifier()\n",
                "\n",
                "parameters = {'n_estimators': [250,500,750,1000],'loss': ['deviance','exponential'],'max_features':['auto','sqrt','log2'],'learning_rate':[.01,.1,1,5],'subsample':[.01,.1,1,5],\n",
                "             'min_samples_split':[2,3,4,5],'max_depth':[5,10,15,20,25]}\n",
                "\n",
                "GBClassifier = RandomizedSearchCV(GB, parameters, scoring='accuracy' ,cv =50)\n",
                "GBClassifier.fit(x_train, x_test)\n",
                "GBClassifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "'''",
                "ASSIGN = [150,200,250,300,350]",
                "ASSIGN = [.01,.1]",
                "ASSIGN = [.05,.1]",
                "ASSIGN = [3,4,5]",
                "ASSIGN = [9,10,11]",
                "ASSIGN = ['exponential']",
                "ASSIGN = ['auto']",
                "ASSIGN = GradientBoostingClassifier()",
                "ASSIGN = {'n_estimators': [150,200,250,300,350],'loss': ['exponential'],'max_features':['auto'],'learning_rate':[.01,.1],'subsample':[.05,.1],",
                "'ASSIGN':[3,4,5],'ASSIGN':[9,10,11]}",
                "ASSIGN = GridSearchCV(GB, parameters, scoring='accuracy' ,cv =5)",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN.best_params_",
                "'''"
            ],
            "source_orig": [
                "'''\n",
                "n_estimators = [150,200,250,300,350]\n",
                "learning_rate = [.01,.1]\n",
                "subsample = [.05,.1]\n",
                "min_samples_split = [3,4,5]\n",
                "max_depth = [9,10,11]\n",
                "loss = ['exponential']\n",
                "max_features = ['auto']\n",
                "\n",
                "GB = GradientBoostingClassifier()\n",
                "\n",
                "parameters = {'n_estimators': [150,200,250,300,350],'loss': ['exponential'],'max_features':['auto'],'learning_rate':[.01,.1],'subsample':[.05,.1],\n",
                "             'min_samples_split':[3,4,5],'max_depth':[9,10,11]}\n",
                "\n",
                "GBClassifier = GridSearchCV(GB, parameters, scoring='accuracy' ,cv =5)\n",
                "GBClassifier.fit(x_train, x_test)\n",
                "GBClassifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = GradientBoostingClassifier(loss = 'exponential',n_estimators =200 ,min_samples_split = 4,max_features = 'auto',max_depth =9,learning_rate = .01,subsample = .1)",
                "Estimator.append(('ASSIGN',GradientBoostingClassifier(loss = 'exponential',n_estimators =200 ,min_samples_split = 4,max_features = 'auto',max_depth =9,learning_rate = .01,subsample = .1)))",
                "ASSIGN = cross_val_score(gbc,x_train,x_test,ASSIGN=10)",
                "Accuracy7 = ASSIGN.mean()",
                "Accuracy.append(Accuracy7)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "gbc = GradientBoostingClassifier(loss = 'exponential',n_estimators =200 ,min_samples_split = 4,max_features = 'auto',max_depth =9,learning_rate = .01,subsample = .1)\n",
                "Estimator.append(('gbc',GradientBoostingClassifier(loss = 'exponential',n_estimators =200 ,min_samples_split = 4,max_features = 'auto',max_depth =9,learning_rate = .01,subsample = .1)))\n",
                "cv = cross_val_score(gbc,x_train,x_test,cv=10)\n",
                "Accuracy7 = cv.mean()\n",
                "Accuracy.append(Accuracy7)\n",
                "print(cv)\n",
                "\n",
                "\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "gbc.fit(x_train,x_test)",
                "gbc.score(y_train,y_test)"
            ],
            "source_orig": [
                "gbc.fit(x_train,x_test)\n",
                "gbc.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = gbc.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('GradientBoosting(HT).csv',index = False)"
            ],
            "source_orig": [
                "model17pred = gbc.predict(y_train)\n",
                "submission17 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission17['PassengerId'] = result['PassengerId']\n",
                "submission17['Survived'] = model17pred\n",
                "submission17.to_csv('GradientBoosting(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model"
            ],
            "source": [
                "'''",
                "ASSIGN = [1,5,10]",
                "ASSIGN = [.5,1,1.5,2,2.5]",
                "ASSIGN = [.6,.8,1]",
                "ASSIGN = [.6,.8,1]",
                "ASSIGN = [.01,.05,.1,.5,.2]",
                "ASSIGN = [3,4,5,6,7,8,9,10]",
                "ASSIGN = XGBClassifier()",
                "ASSIGN = {'min_child_weight': [1,5,10],'gamma': [.5,1,1.5,2,2.5],'subsample':[.6,.8,1],'colsample_bytree':[.6,.8,1],'subsample':[.6,.8,1],",
                "'ASSIGN':[.01,.05,.1,.5,.2],'ASSIGN':[3,4,5,6,7,8,9,10]}",
                "ASSIGN = RandomizedSearchCV(XB, parameters, scoring='accuracy' ,cv =50)",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN.best_params_",
                "'''"
            ],
            "source_orig": [
                "'''\n",
                "min_child_weight = [1,5,10]\n",
                "gamma = [.5,1,1.5,2,2.5]\n",
                "subsample = [.6,.8,1]\n",
                "colsample_bytree = [.6,.8,1]\n",
                "eta = [.01,.05,.1,.5,.2]\n",
                "max_depth = [3,4,5,6,7,8,9,10]\n",
                "\n",
                "XB = XGBClassifier()\n",
                "\n",
                "parameters = {'min_child_weight': [1,5,10],'gamma': [.5,1,1.5,2,2.5],'subsample':[.6,.8,1],'colsample_bytree':[.6,.8,1],'subsample':[.6,.8,1],\n",
                "             'eta':[.01,.05,.1,.5,.2],'max_depth':[3,4,5,6,7,8,9,10]}\n",
                "\n",
                "XBClassifier = RandomizedSearchCV(XB, parameters, scoring='accuracy' ,cv =50)\n",
                "XBClassifier.fit(x_train, x_test)\n",
                "XBClassifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "'''",
                "ASSIGN = [4,5,6]",
                "ASSIGN = [1,1.5,2.0,2.5,3]",
                "ASSIGN = [.6,.8,1,1.2]",
                "ASSIGN = [.6,.8,1,1.2]",
                "ASSIGN = [.5,.01]",
                "ASSIGN = [5,6,7,8]",
                "ASSIGN = XGBClassifier()",
                "ASSIGN = {'min_child_weight': [4,5,6],'gamma': [1,1.5,2.0,2.5,3],'subsample':[.6,.8,1,1.2],'colsample_bytree':[.6,.8,1,1.2],",
                "'ASSIGN':[.5,.01],'ASSIGN':[5,6,7,8]}",
                "ASSIGN = GridSearchCV(XB, parameters, scoring='accuracy' ,cv =5)",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN.best_params_",
                "'''"
            ],
            "source_orig": [
                "'''\n",
                "min_child_weight = [4,5,6]\n",
                "gamma = [1,1.5,2.0,2.5,3]\n",
                "subsample = [.6,.8,1,1.2]\n",
                "colsample_bytree = [.6,.8,1,1.2]\n",
                "eta = [.5,.01]\n",
                "\n",
                "max_depth = [5,6,7,8]\n",
                "\n",
                "XB = XGBClassifier()\n",
                "\n",
                "parameters = {'min_child_weight': [4,5,6],'gamma': [1,1.5,2.0,2.5,3],'subsample':[.6,.8,1,1.2],'colsample_bytree':[.6,.8,1,1.2],\n",
                "             'eta':[.5,.01],'max_depth':[5,6,7,8]}\n",
                "\n",
                "XBClassifier = GridSearchCV(XB, parameters, scoring='accuracy' ,cv =5)\n",
                "XBClassifier.fit(x_train, x_test)\n",
                "XBClassifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = XGBClassifier(colsample_bytree = .6,eta = 0.5,gamma = 1,max_depth = 5,min_child_weight = 6,subsample = 1)",
                "Estimator.append(('ASSIGN',XGBClassifier(colsample_bytree = .6,eta = 0.5,gamma = 1,max_depth = 5,min_child_weight = 6,subsample = 1)))",
                "ASSIGN = cross_val_score(xgb,x_train,x_test,ASSIGN=10)",
                "Accuracy8 = ASSIGN.mean()",
                "Accuracy.append(Accuracy8)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "xgb = XGBClassifier(colsample_bytree = .6,eta = 0.5,gamma = 1,max_depth = 5,min_child_weight = 6,subsample = 1)\n",
                "Estimator.append(('xgb',XGBClassifier(colsample_bytree = .6,eta = 0.5,gamma = 1,max_depth = 5,min_child_weight = 6,subsample = 1)))\n",
                "cv = cross_val_score(xgb,x_train,x_test,cv=10)\n",
                "Accuracy8 = cv.mean()\n",
                "Accuracy.append(Accuracy8)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "xgb.fit(x_train,x_test)",
                "gbc.score(y_train,y_test)"
            ],
            "source_orig": [
                "xgb.fit(x_train,x_test)\n",
                "gbc.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = xgb.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('XGBoosting(HT).csv',index = False)"
            ],
            "source_orig": [
                "model18pred = xgb.predict(y_train)\n",
                "submission18 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission18['PassengerId'] = result['PassengerId']\n",
                "submission18['Survived'] = model18pred\n",
                "submission18.to_csv('XGBoosting(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = final[:891]",
                "ASSIGN = StandardScaler()",
                "ASSIGN = feature_scaler.fit_transform(ASSIGN)",
                "ASSIGN = final[891:]",
                "ASSIGN = StandardScaler()",
                "ASSIGN = feature_scaler.fit_transform(ASSIGN)"
            ],
            "source_orig": [
                "x_train1 = final[:891]\n",
                "feature_scaler = StandardScaler()\n",
                "x_train1 = feature_scaler.fit_transform(x_train1)\n",
                "y_train1 = final[891:]\n",
                "feature_scaler = StandardScaler()\n",
                "y_train1 = feature_scaler.fit_transform(y_train1)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data",
                "train_model"
            ],
            "source": [
                "Krange1 = range(1,20)",
                "ASSIGN = {}",
                "ASSIGN = []",
                "for k in Krange1:",
                "ASSIGN = KNeighborsClassifier(n_neighbors = k)",
                "ASSIGN.fit(x_train1,x_test)",
                "ASSIGN = knn.predict(y_train1)",
                "ASSIGN[k] = metrics.accuracy_score(result['Survived'],ASSIGN)",
                "ASSIGN.append(metrics.accuracy_score(result['Survived'],ASSIGN))",
                "plt.plot(Krange,scores_list)",
                "plt.xlabel(\"Value of K\")",
                "plt.ylabel(\"Accuracy\")"
            ],
            "source_orig": [
                "Krange1 = range(1,20)\n",
                "scores1 = {}\n",
                "scores_list1 = []\n",
                "for k in Krange1:\n",
                "    knn = KNeighborsClassifier(n_neighbors = k)\n",
                "    knn.fit(x_train1,x_test)\n",
                "    y_pred = knn.predict(y_train1)\n",
                "    scores1[k] = metrics.accuracy_score(result['Survived'],y_pred)\n",
                "    scores_list1.append(metrics.accuracy_score(result['Survived'],y_pred))\n",
                "    \n",
                "plt.plot(Krange,scores_list)\n",
                "plt.xlabel(\"Value of K\")\n",
                "plt.ylabel(\"Accuracy\")"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = KNeighborsClassifier(n_neighbors = 11)",
                "Estimator.append(('ASSIGN',KNeighborsClassifier(n_neighbors = 13)))",
                "ASSIGN = cross_val_score(knn,x_train1,x_test,ASSIGN=10)",
                "Accuracy9 = ASSIGN.mean()",
                "Accuracy.append(Accuracy9)",
                "print(ASSIGN)",
                "print(ASSIGN.mean())"
            ],
            "source_orig": [
                "knn = KNeighborsClassifier(n_neighbors = 11)\n",
                "Estimator.append(('knn',KNeighborsClassifier(n_neighbors = 13)))\n",
                "cv = cross_val_score(knn,x_train1,x_test,cv=10)\n",
                "Accuracy9 = cv.mean()\n",
                "Accuracy.append(Accuracy9)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "knn.fit(x_train1,x_test)",
                "knn.score(y_train1,y_test)"
            ],
            "source_orig": [
                "knn.fit(x_train1,x_test)\n",
                "knn.score(y_train1,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN = knn.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('KNN(StdScaler).csv',index = False)"
            ],
            "source_orig": [
                "model19pred = knn.predict(y_train)\n",
                "submission19 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission19['PassengerId'] = result['PassengerId']\n",
                "submission19['Survived'] = model19pred\n",
                "submission19.to_csv('KNN(StdScaler).csv',index = False)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = output2.groupby(['Models after Hyperparameter Tuning'])['Accuracy after HT'].mean().reset_index().sort_values(by='Accuracy after HT',ascending=False)",
                "ASSIGN.head(10).style.background_gradient(cmap='Reds')"
            ],
            "source_orig": [
                "r = output2.groupby(['Models after Hyperparameter Tuning'])['Accuracy after HT'].mean().reset_index().sort_values(by='Accuracy after HT',ascending=False)\n",
                "r.head(10).style.background_gradient(cmap='Reds')\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "ASSIGN = VotingClassifier(estimators = Estimator, voting ='soft')",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN = vot_soft1.predict(y_train)",
                "ASSIGN.score(y_train,y_test)",
                "ASSIGN = vot_soft1.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('SoftVoting(HT).csv',index = False)"
            ],
            "source_orig": [
                "vot_soft1 = VotingClassifier(estimators = Estimator, voting ='soft') \n",
                "vot_soft1.fit(x_train, x_test) \n",
                "y_pred = vot_soft1.predict(y_train)\n",
                "vot_soft1.score(y_train,y_test)\n",
                "\n",
                "modelpred3 = vot_soft1.predict(y_train)\n",
                "sub3 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "sub3['PassengerId'] = result['PassengerId']\n",
                "sub3['Survived'] = modelpred3\n",
                "sub3.to_csv('SoftVoting(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "vot_soft1.fit(x_train, x_test)",
                "vot_soft1.score(y_train,y_test)"
            ],
            "source_orig": [
                "vot_soft1.fit(x_train, x_test) \n",
                "vot_soft1.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "ASSIGN = VotingClassifier(estimators = Estimator, voting ='hard')",
                "ASSIGN.fit(x_train, x_test)",
                "ASSIGN = vot_hard1.predict(y_train)",
                "ASSIGN.score(y_train,y_test)",
                "ASSIGN = vot_hard1.predict(y_train)",
                "ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived'])",
                "ASSIGN['PassengerId'] = result['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN",
                "ASSIGN.to_csv('HardVoting(HT).csv',index = False)"
            ],
            "source_orig": [
                "vot_hard1 = VotingClassifier(estimators = Estimator, voting ='hard') \n",
                "vot_hard1.fit(x_train, x_test) \n",
                "y_pred = vot_hard1.predict(y_train)\n",
                "vot_hard1.score(y_train,y_test)\n",
                "\n",
                "modelpred4 = vot_hard1.predict(y_train)\n",
                "sub4 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "sub4['PassengerId'] = result['PassengerId']\n",
                "sub4['Survived'] = modelpred4\n",
                "sub4.to_csv('HardVoting(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "vot_hard1.fit(x_train, x_test)",
                "vot_hard1.score(y_train,y_test)"
            ],
            "source_orig": [
                "vot_hard1.fit(x_train, x_test) \n",
                "vot_hard1.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = pd.concat([output1,output2],axis = 1)",
                "ASSIGN.sort_values(by=['Accuracy after HT'], inplace=True, ascending=False)",
                "ASSIGN.head(10)"
            ],
            "source_orig": [
                "output = pd.concat([output1,output2],axis = 1)\n",
                "output.sort_values(by=['Accuracy after HT'], inplace=True, ascending=False)\n",
                "output.head(10)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "source_orig": [
                "cbd = pd.read_csv(\"../input/crimeanalysis/crime_by_district.csv\")\n",
                "cbdr = pd.read_csv(\"../input/crimeanalysis/crime_by_district_rt.csv\")\n",
                "cbs = pd.read_csv(\"../input/crimeanalysis/crime_by_state.csv\")\n",
                "cbsr = pd.read_csv(\"../input/crimeanalysis/crime_by_state_rt.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cbd.head()"
            ],
            "source_orig": [
                "cbd.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cbdr.head()"
            ],
            "source_orig": [
                "cbdr.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "cbd.shape"
            ],
            "source_orig": [
                "cbd.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "cbdr.shape"
            ],
            "source_orig": [
                "cbdr.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.concat([cbd,cbdr]).drop_duplicates(keep = False)"
            ],
            "source_orig": [
                "total1 = pd.concat([cbd,cbdr]).drop_duplicates(keep = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "total1['Total Atrocities'] = total1['Murder'] +total1['Assault on women']+total1['Kidnapping and Abduction']+total1['Dacoity']+total1['Robbery']+total1['Arson']+total1['Hurt']+total1['Prevention of atrocities (POA) Act']+total1['Protection of Civil Rights (PCR) Act']+total1['Other Crimes Against SCs']",
                "total1.head()"
            ],
            "source_orig": [
                "total1['Total Atrocities'] = total1['Murder'] +total1['Assault on women']+total1['Kidnapping and Abduction']+total1['Dacoity']+total1['Robbery']+total1['Arson']+total1['Hurt']+total1['Prevention of atrocities (POA) Act']+total1['Protection of Civil Rights (PCR) Act']+total1['Other Crimes Against SCs']\n",
                "total1.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "cbs.shape"
            ],
            "source_orig": [
                "cbs.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "cbsr.shape"
            ],
            "source_orig": [
                "cbsr.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.concat([cbs,cbsr]).drop_duplicates(keep = False)"
            ],
            "source_orig": [
                "total = pd.concat([cbs,cbsr]).drop_duplicates(keep = False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "total.shape"
            ],
            "source_orig": [
                "total.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "total.head(20)"
            ],
            "source_orig": [
                "total.head(20)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "total.tail(20)"
            ],
            "source_orig": [
                "total.tail(20)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "total.drop(total[total['STATEpath'] == 'TOTAL (UTs)'].index , inplace = True)",
                "total.drop(total[total['STATEpath'] == 'TOTAL (STATES)'].index , inplace = True)"
            ],
            "source_orig": [
                "total.drop(total[total['STATE/UT'] == 'TOTAL (UTs)'].index , inplace = True) \n",
                "total.drop(total[total['STATE/UT'] == 'TOTAL (STATES)'].index , inplace = True) "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cbdr.isnull().sum()"
            ],
            "source_orig": [
                "cbdr.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cbsr.isnull().sum()"
            ],
            "source_orig": [
                "cbsr.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "total.isnull().sum()"
            ],
            "source_orig": [
                "total.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cbdr['Total Atrocities'] = cbdr['Murder'] +cbdr['Assault on women']+cbdr['Kidnapping and Abduction']+cbdr['Dacoity']+cbdr['Robbery']+cbdr['Arson']+cbdr['Hurt']+cbdr['Prevention of atrocities (POA) Act']+cbdr['Protection of Civil Rights (PCR) Act']+cbdr['Other Crimes Against SCs']"
            ],
            "source_orig": [
                "cbdr['Total Atrocities'] = cbdr['Murder'] +cbdr['Assault on women']+cbdr['Kidnapping and Abduction']+cbdr['Dacoity']+cbdr['Robbery']+cbdr['Arson']+cbdr['Hurt']+cbdr['Prevention of atrocities (POA) Act']+cbdr['Protection of Civil Rights (PCR) Act']+cbdr['Other Crimes Against SCs']\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "source": [
                "ASSIGN= cbdr.groupby(['STATEpath','DISTRICT','Year'])['Murder'].sum().reset_index().sort_values(by='Murder',ascending=False)",
                "ASSIGN.head(10).style.background_gradient(cmap='Reds')"
            ],
            "source_orig": [
                "s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Murder'].sum().reset_index().sort_values(by='Murder',ascending=False)\n",
                "s.head(10).style.background_gradient(cmap='Reds')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN= cbdr.groupby(['STATEpath','DISTRICT','Year'])['Assault on women'].sum().reset_index().sort_values(by='Assault on women',ascending=False)"
            ],
            "source_orig": [
                "s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Assault on women'].sum().reset_index().sort_values(by='Assault on women',ascending=False)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN= cbdr.groupby(['STATEpath','DISTRICT','Year'])['Robbery'].sum().reset_index().sort_values(by='Robbery',ascending=False)",
                "ASSIGN.head(10).style.background_gradient(cmap='Oranges')"
            ],
            "source_orig": [
                "s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Robbery'].sum().reset_index().sort_values(by='Robbery',ascending=False)\n",
                "s.head(10).style.background_gradient(cmap='Oranges')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN= cbdr.groupby(['STATEpath','DISTRICT','Year'])['Arson'].sum().reset_index().sort_values(by='Arson',ascending=False)",
                "ASSIGN.head(10).style.background_gradient(cmap='RdPu')"
            ],
            "source_orig": [
                "s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Arson'].sum().reset_index().sort_values(by='Arson',ascending=False)\n",
                "s.head(10).style.background_gradient(cmap='RdPu')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN= cbdr.groupby(['STATEpath','DISTRICT','Year'])['Hurt'].sum().reset_index().sort_values(by='Hurt',ascending=False)",
                "ASSIGN.head(10).style.background_gradient(cmap='Greys')"
            ],
            "source_orig": [
                "s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Hurt'].sum().reset_index().sort_values(by='Hurt',ascending=False)\n",
                "s.head(10).style.background_gradient(cmap='Greys')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Murder', data=cbdr,height = 5, aspect = 4)"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Murder', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Assault on women', data=cbdr,height = 5, aspect = 4)"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Assault on women', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Kidnapping and Abduction', data=cbdr,height = 5, aspect = 4)"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Kidnapping and Abduction', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Dacoity', data=cbdr,height = 5, aspect = 4)"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Dacoity', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Robbery', data=cbdr,height = 5, aspect = 4)"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Robbery', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Arson', data=cbdr,height = 5, aspect = 4)"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Arson', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Hurt', data=cbdr,height = 5, aspect = 4)"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Hurt', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Prevention of atrocities (POA) Act', data=cbdr,height = 5, aspect = 4)"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Prevention of atrocities (POA) Act', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Protection of Civil Rights (PCR) Act', data=cbdr,height = 5, aspect = 4)"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Protection of Civil Rights (PCR) Act', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Other Crimes Against SCs', data=cbdr,height = 5, aspect = 4)"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Other Crimes Against SCs', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Total Atrocities', data=cbdr,height = 5, aspect = 4)"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Total Atrocities', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "cbsr['Total Atrocities'] = cbsr['Murder'] +cbsr['Assault on women']+cbsr['Kidnapping and Abduction']+cbsr['Dacoity']+cbsr['Robbery']+cbsr['Arson']+cbsr['Hurt']+cbsr['Prevention of atrocities (POA) Act']+cbsr['Protection of Civil Rights (PCR) Act']+cbsr['Other Crimes Against SCs']",
                "cbsr.head()"
            ],
            "source_orig": [
                "cbsr['Total Atrocities'] = cbsr['Murder'] +cbsr['Assault on women']+cbsr['Kidnapping and Abduction']+cbsr['Dacoity']+cbsr['Robbery']+cbsr['Arson']+cbsr['Hurt']+cbsr['Prevention of atrocities (POA) Act']+cbsr['Protection of Civil Rights (PCR) Act']+cbsr['Other Crimes Against SCs']\n",
                "cbsr.head()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.relplot(x ='Total Atrocities', y ='Year', col = 'STATEpath', data = cbsr, height=3 ,col_wrap = 9)"
            ],
            "source_orig": [
                "sns.relplot(x ='Total Atrocities', y ='Year', col = 'STATE/UT', data = cbsr, height=3 ,col_wrap = 9)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.axes_style('white')",
                "sns.jointplot(x=x, y=y, kind = 'hex', color = 'green')"
            ],
            "source_orig": [
                "sns.axes_style('white')\n",
                "sns.jointplot(x=x, y=y, kind = 'hex', color = 'green')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = plt.subplots(figsize=(6,6))",
                "ASSIGN = sns.cubehelix_palette(as_cmap = True, dark=0,light = 1,reverse=True)",
                "sns.kdeplot(x,y,ASSIGN=ASSIGN, n_levels = 60, shade= True)"
            ],
            "source_orig": [
                "f, ax = plt.subplots(figsize=(6,6))\n",
                "cmap = sns.cubehelix_palette(as_cmap = True, dark=0,light = 1,reverse=True)\n",
                "sns.kdeplot(x,y,cmap=cmap, n_levels = 60, shade= True)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "total['Total Atrocities'] = total['Murder'] +total['Assault on women']+total['Kidnapping and Abduction']+total['Dacoity']+total['Robbery']+total['Arson']+total['Hurt']+total['Prevention of atrocities (POA) Act']+total['Protection of Civil Rights (PCR) Act']+total['Other Crimes Against SCs']",
                "total.head(15)"
            ],
            "source_orig": [
                "total['Total Atrocities'] = total['Murder'] +total['Assault on women']+total['Kidnapping and Abduction']+total['Dacoity']+total['Robbery']+total['Arson']+total['Hurt']+total['Prevention of atrocities (POA) Act']+total['Protection of Civil Rights (PCR) Act']+total['Other Crimes Against SCs']\n",
                "total.head(15)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN= total.groupby(['Year'])['Murder'].sum().reset_index().sort_values(by='Murder',ascending=False)",
                "ASSIGN.head(15).style.background_gradient(cmap='Reds')"
            ],
            "source_orig": [
                "s= total.groupby(['Year'])['Murder'].sum().reset_index().sort_values(by='Murder',ascending=False)\n",
                "s.head(15).style.background_gradient(cmap='Reds')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Murder', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Murder', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Assault on women', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Assault on women', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Kidnapping and Abduction', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Kidnapping and Abduction', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Dacoity', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Dacoity', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Robbery', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Robbery', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Arson', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Arson', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Hurt', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Hurt', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Prevention of atrocities (POA) Act', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Prevention of atrocities (POA) Act', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Protection of Civil Rights (PCR) Act', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Protection of Civil Rights (PCR) Act', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Other Crimes Against SCs', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Other Crimes Against SCs', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Total Atrocities', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y='Total Atrocities', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "source_orig": [
                "data = pd.read_csv(\"../input/vehicle-dataset-from-cardekho/car data.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.head()"
            ],
            "source_orig": [
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.isnull().sum()"
            ],
            "source_orig": [
                "data.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "data.describe"
            ],
            "source_orig": [
                "data.describe"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.relplot( y = 'Selling_Price', x = 'Kms_Driven',  data = data)"
            ],
            "source_orig": [
                "sns.relplot( y = 'Selling_Price', x = 'Kms_Driven',  data = data)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Owner', y ='Selling_Price', kind = 'violin',data= data)"
            ],
            "source_orig": [
                "sns.catplot(x='Owner', y ='Selling_Price', kind = 'violin',data= data)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Transmission', y ='Selling_Price', kind = 'swarm',data= data)"
            ],
            "source_orig": [
                "sns.catplot(x='Transmission', y ='Selling_Price', kind = 'swarm',data= data)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Seller_Type', y ='Selling_Price', kind = 'swarm',data= data,hue = 'Fuel_Type')"
            ],
            "source_orig": [
                "sns.catplot(x='Seller_Type', y ='Selling_Price', kind = 'swarm',data= data,hue = 'Fuel_Type')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y ='Selling_Price', kind = 'swarm',data= data)"
            ],
            "source_orig": [
                "sns.catplot(x='Year', y ='Selling_Price', kind = 'swarm',data= data)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = data.corr()"
            ],
            "source_orig": [
                "correlation = data.corr()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.subplots(figsize=(10,15))",
                "sns.heatmap(correlation, annot = True)"
            ],
            "source_orig": [
                "plt.subplots(figsize=(10,15))\n",
                "sns.heatmap(correlation, annot = True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.jointplot(x = 'Present_Price', y ='Selling_Price', data=data, color = 'Green')"
            ],
            "source_orig": [
                "sns.jointplot(x = 'Present_Price', y ='Selling_Price', data=data, color = 'Green')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.pairplot(data = data)"
            ],
            "source_orig": [
                "sns.pairplot(data = data)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.get_dummies(data.Fuel_Type)",
                "ASSIGN = pd.get_dummies(data.Seller_Type)",
                "ASSIGN = pd.get_dummies(data.Transmission)"
            ],
            "source_orig": [
                "dummy1 = pd.get_dummies(data.Fuel_Type)\n",
                "dummy2 = pd.get_dummies(data.Seller_Type)\n",
                "dummy3 = pd.get_dummies(data.Transmission)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.concat([data,dummy1,dummy2,dummy3], axis = 'columns')"
            ],
            "source_orig": [
                "merge = pd.concat([data,dummy1,dummy2,dummy3], axis = 'columns')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = merge.drop(['Car_Name','Fuel_Type','Seller_Type','Transmission','CNG','Individual','Automatic','Owner','Kms_Driven'], axis = 'columns')"
            ],
            "source_orig": [
                "final = merge.drop(['Car_Name','Fuel_Type','Seller_Type','Transmission','CNG','Individual','Automatic','Owner','Kms_Driven'], axis = 'columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "final"
            ],
            "source_orig": [
                "final\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = final.drop(['Selling_Price'],axis = 'columns')"
            ],
            "source_orig": [
                "X = final.drop(['Selling_Price'],axis = 'columns')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = final['Selling_Price']"
            ],
            "source_orig": [
                "y = final['Selling_Price']"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "SETUP",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,random_state = 20)"
            ],
            "source_orig": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,random_state = 20)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "SETUP",
                "ASSIGN = LinearRegression()"
            ],
            "source_orig": [
                "from sklearn.linear_model import LinearRegression\n",
                "model = LinearRegression()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model.fit(X_train,y_train)"
            ],
            "source_orig": [
                "model.fit(X_train,y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "model.score(X_test,y_test)"
            ],
            "source_orig": [
                "model.score(X_test,y_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "source_orig": [
                "df = pd.read_csv(\"../input/graduate-admissions/Admission_Predict_Ver1.1.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "df.shape"
            ],
            "source_orig": [
                "df.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.isnull().sum()"
            ],
            "source_orig": [
                "df.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ],
            "source_orig": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df.drop(['Serial No.'], axis = 1, inplace = True)"
            ],
            "source_orig": [
                "df.drop(['Serial No.'], axis = 1, inplace = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ],
            "source_orig": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(25,10))",
                "sns.heatmap(df.corr(), annot=True, linewidth=0.5, cmap='coolwarm')"
            ],
            "source_orig": [
                "plt.figure(figsize=(25,10))\n",
                "sns.heatmap(df.corr(), annot=True, linewidth=0.5, cmap='coolwarm')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.pairplot(df)"
            ],
            "source_orig": [
                "sns.pairplot(df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = df['Chance of Admit ']",
                "sns.distplot(ASSIGN , kde= True,rug = False, bins = 30)"
            ],
            "source_orig": [
                "x = df['Chance of Admit ']\n",
                "sns.distplot(x , kde= True,rug = False, bins = 30)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = df['GRE Score']",
                "sns.distplot(ASSIGN , kde= True,rug = False, bins = 30)"
            ],
            "source_orig": [
                "x = df['GRE Score']\n",
                "sns.distplot(x , kde= True,rug = False, bins = 30)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = df['TOEFL Score']",
                "sns.distplot(ASSIGN , kde= True,rug = False, bins = 30)"
            ],
            "source_orig": [
                "x = df['TOEFL Score']\n",
                "sns.distplot(x , kde= True,rug = False, bins = 30)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = df['CGPA']",
                "sns.distplot(ASSIGN , kde= True,rug = False, bins = 30)"
            ],
            "source_orig": [
                "x = df['CGPA']\n",
                "sns.distplot(x , kde= True,rug = False, bins = 30)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.lineplot(x = 'GRE Score', y = 'CGPA', data = df)"
            ],
            "source_orig": [
                "sns.lineplot(x = 'GRE Score', y = 'CGPA', data = df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.lineplot(x = 'TOEFL Score', y = 'CGPA', data = df)"
            ],
            "source_orig": [
                "sns.lineplot(x = 'TOEFL Score', y = 'CGPA', data = df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.jointplot(x = 'GRE Score', y = 'CGPA', data=df)"
            ],
            "source_orig": [
                "sns.jointplot(x = 'GRE Score', y = 'CGPA', data=df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.jointplot(x = 'TOEFL Score', y = 'CGPA', data=df)"
            ],
            "source_orig": [
                "sns.jointplot(x = 'TOEFL Score', y = 'CGPA', data=df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.jointplot(x = 'TOEFL Score', y = 'University Rating', data=df)"
            ],
            "source_orig": [
                "sns.jointplot(x = 'TOEFL Score', y = 'University Rating', data=df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.jointplot(x = 'GRE Score', y = 'University Rating', data=df)"
            ],
            "source_orig": [
                "sns.jointplot(x = 'GRE Score', y = 'University Rating', data=df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.relplot(x ='SOP', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')"
            ],
            "source_orig": [
                "sns.relplot(x ='SOP', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.relplot(x ='LOR ', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')"
            ],
            "source_orig": [
                "sns.relplot(x ='LOR ', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.relplot(x ='Research', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')"
            ],
            "source_orig": [
                "sns.relplot(x ='Research', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn import metrics\n",
                "from sklearn import linear_model\n",
                "from sklearn.linear_model import Lasso\n",
                "from sklearn.linear_model import ElasticNet\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.svm import SVR\n",
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "\n",
                "from sklearn_pandas import DataFrameMapper\n",
                "from numpy import asarray\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.preprocessing import StandardScaler"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = df.drop(['Chance of Admit '], axis = 1)",
                "X"
            ],
            "source_orig": [
                "X = df.drop(['Chance of Admit '], axis = 1)\n",
                "X"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = df['Chance of Admit ']",
                "y"
            ],
            "source_orig": [
                "y = df['Chance of Admit ']\n",
                "y"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,shuffle = False)"
            ],
            "source_orig": [
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,shuffle = False)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = LinearRegression()",
                "ASSIGN.fit(X_train, y_train)",
                "ASSIGN = model1.score(X_test,y_test)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model1 = LinearRegression()\n",
                "model1.fit(X_train, y_train)\n",
                "\n",
                "accuracy1 = model1.score(X_test,y_test)\n",
                "print(accuracy1*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model1.predict(X_test)",
                "ASSIGN = mean_squared_error(y_test, y_pred1, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred1 = model1.predict(X_test)\n",
                "\n",
                "val = mean_squared_error(y_test, y_pred1, squared=False)\n",
                "val1 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = DecisionTreeRegressor()",
                "ASSIGN.fit(X_train, y_train)",
                "ASSIGN = model2.score(X_test,y_test)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model2 = DecisionTreeRegressor()\n",
                "model2.fit(X_train, y_train)\n",
                "\n",
                "accuracy2 = model2.score(X_test,y_test)\n",
                "print(accuracy2*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model2.predict(X_test)",
                "ASSIGN = mean_squared_error(y_test, y_pred2, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred2 = model2.predict(X_test)\n",
                "\n",
                "val = mean_squared_error(y_test, y_pred2, squared=False)\n",
                "val2 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "SETUP",
                "ASSIGN = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]",
                "ASSIGN = RandomForestRegressor()",
                "ASSIGN = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}",
                "RFR.fit(X_train, y_train)",
                "RFR.best_params_"
            ],
            "source_orig": [
                "n_estimators = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]\n",
                "\n",
                "RF = RandomForestRegressor()\n",
                "\n",
                "parameters = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}\n",
                "\n",
                "RFR = GridSearchCV(RF, parameters,scoring='neg_mean_squared_error', cv=5)\n",
                "\n",
                "RFR.fit(X_train, y_train)\n",
                "\n",
                "RFR.best_params_\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = RandomForestRegressor(n_estimators = 190)",
                "ASSIGN.fit(X_train, y_train)",
                "ASSIGN = model3.score(X_test,y_test)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model3 = RandomForestRegressor(n_estimators = 190)\n",
                "model3.fit(X_train, y_train)\n",
                "\n",
                "accuracy3 = model3.score(X_test,y_test)\n",
                "print(accuracy3*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model3.predict(X_test)",
                "ASSIGN = mean_squared_error(y_test, y_pred3, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred3 = model3.predict(X_test)\n",
                "\n",
                "val = mean_squared_error(y_test, y_pred3, squared=False)\n",
                "val3 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred3)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = Lasso()",
                "ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}",
                "ASSIGN = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv = 100)",
                "ASSIGN.fit(X_train, y_train)",
                "ASSIGN.best_params_"
            ],
            "source_orig": [
                "lasso = Lasso()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "lasso_regressor.fit(X_train, y_train)\n",
                "\n",
                "lasso_regressor.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = linear_model.Lasso(alpha=.001)",
                "ASSIGN.fit(X_train,y_train)",
                "ASSIGN = model4.score(X_test,y_test)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model4 = linear_model.Lasso(alpha=.001)\n",
                "model4.fit(X_train,y_train)\n",
                "\n",
                "accuracy4 = model4.score(X_test,y_test)\n",
                "print(accuracy4*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model4.predict(X_test)",
                "ASSIGN= mean_squared_error(y_test, y_pred4, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred4 = model4.predict(X_test)\n",
                "\n",
                "val= mean_squared_error(y_test, y_pred4, squared=False)\n",
                "val4 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred4)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]",
                "ASSIGN = Ridge()",
                "ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}",
                "ASSIGN = GridSearchCV(ridge, parameters,scoring='neg_mean_squared_error', cv=100)",
                "ASSIGN.fit(X_train, y_train)",
                "ASSIGN.best_params_"
            ],
            "source_orig": [
                "alpha = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
                "\n",
                "ridge = Ridge()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "ridge_regressor = GridSearchCV(ridge, parameters,scoring='neg_mean_squared_error', cv=100)\n",
                "\n",
                "ridge_regressor.fit(X_train, y_train)\n",
                "ridge_regressor.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = linear_model.Ridge(alpha=1)",
                "ASSIGN.fit(X_train,y_train)",
                "ASSIGN = model5.score(X_test,y_test)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model5 = linear_model.Ridge(alpha=1)\n",
                "model5.fit(X_train,y_train)\n",
                "\n",
                "accuracy5 = model5.score(X_test,y_test)\n",
                "print(accuracy5*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model5.predict(X_test)",
                "ASSIGN = mean_squared_error(y_test, y_pred5, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred5 = model5.predict(X_test)\n",
                "\n",
                "val = mean_squared_error(y_test, y_pred5, squared=False)\n",
                "val5 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred5)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = ElasticNet()",
                "ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}",
                "ASSIGN = GridSearchCV(Elasticnet, parameters, scoring='neg_mean_squared_error', cv = 100)",
                "ASSIGN.fit(X_train, y_train)",
                "ASSIGN.best_params_"
            ],
            "source_orig": [
                "Elasticnet = ElasticNet()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "en_regressor = GridSearchCV(Elasticnet, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "en_regressor.fit(X_train, y_train)\n",
                "en_regressor.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = linear_model.ElasticNet(alpha=0.001)",
                "ASSIGN.fit(X_train,y_train)",
                "ASSIGN = model6.score(X_test,y_test)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model6 = linear_model.ElasticNet(alpha=0.001)\n",
                "model6.fit(X_train,y_train)\n",
                "\n",
                "accuracy6 = model6.score(X_test,y_test)\n",
                "print(accuracy6*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model6.predict(X_test)",
                "ASSIGN = mean_squared_error(y_test, y_pred6, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred6 = model6.predict(X_test)\n",
                "\n",
                "val = mean_squared_error(y_test, y_pred6, squared=False)\n",
                "val6 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred6)))\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data.drop(['Chance of Admit '], axis = 1)",
                "x"
            ],
            "source_orig": [
                "x = data.drop(['Chance of Admit '], axis = 1)\n",
                "x"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data['Chance of Admit ']",
                "Y"
            ],
            "source_orig": [
                "Y = data['Chance of Admit ']\n",
                "Y"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x_train, x_test, Y_train, Y_test = train_test_split(x, Y, test_size = 0.20,shuffle = False)"
            ],
            "source_orig": [
                "x_train, x_test, Y_train, Y_test = train_test_split(x, Y, test_size = 0.20,shuffle = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = LinearRegression()",
                "ASSIGN.fit(x_train, Y_train)",
                "ASSIGN = model7.score(x_test,Y_test)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model7 = LinearRegression()\n",
                "model7.fit(x_train, Y_train)\n",
                "\n",
                "accuracy7 = model7.score(x_test,Y_test)\n",
                "print(accuracy7*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model7.predict(x_test)",
                "ASSIGN = mean_squared_error(Y_test, y_pred7, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred7 = model7.predict(x_test)\n",
                "\n",
                "val = mean_squared_error(Y_test, y_pred7, squared=False)\n",
                "val7 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred7)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = DecisionTreeRegressor()",
                "ASSIGN.fit(x_train, Y_train)",
                "ASSIGN = model8.score(x_test,Y_test)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model8 = DecisionTreeRegressor()\n",
                "model8.fit(x_train, Y_train)\n",
                "\n",
                "accuracy8 = model8.score(x_test,Y_test)\n",
                "print(accuracy8*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model8.predict(x_test)",
                "ASSIGN = mean_squared_error(Y_test, y_pred8, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred8 = model8.predict(x_test)\n",
                "\n",
                "val = mean_squared_error(Y_test, y_pred8, squared=False)\n",
                "val8 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred8)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]",
                "ASSIGN = RandomForestRegressor()",
                "ASSIGN = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}",
                "ASSIGN = GridSearchCV(rf, parameters,scoring='neg_mean_squared_error', cv=10)",
                "ASSIGN.fit(x_train, Y_train)",
                "ASSIGN.best_params_"
            ],
            "source_orig": [
                "n_estimators = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]\n",
                "\n",
                "rf = RandomForestRegressor()\n",
                "\n",
                "parameters = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}\n",
                "\n",
                "rfr = GridSearchCV(rf, parameters,scoring='neg_mean_squared_error', cv=10)\n",
                "\n",
                "rfr.fit(x_train, Y_train)\n",
                "\n",
                "rfr.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = RandomForestRegressor(n_estimators = 220)",
                "ASSIGN.fit(x_train, Y_train)",
                "ASSIGN = model9.score(x_test,Y_test)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model9 = RandomForestRegressor(n_estimators = 220)\n",
                "model9.fit(x_train, Y_train)\n",
                "\n",
                "accuracy9 = model9.score(x_test,Y_test)\n",
                "print(accuracy9*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model9.predict(x_test)",
                "ASSIGN = mean_squared_error(Y_test, y_pred9, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred9 = model9.predict(x_test)\n",
                "\n",
                "val = mean_squared_error(Y_test, y_pred9, squared=False)\n",
                "val9 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred9)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = Lasso()",
                "ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}",
                "ASSIGN = GridSearchCV(L, parameters, scoring='neg_mean_squared_error', cv = 100)",
                "ASSIGN.fit(x_train, Y_train)",
                "ASSIGN.best_params_"
            ],
            "source_orig": [
                "L = Lasso()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "LR = GridSearchCV(L, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "LR.fit(x_train, Y_train)\n",
                "LR.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = linear_model.Lasso(alpha=.01)",
                "ASSIGN.fit(x_train,Y_train)",
                "ASSIGN = model10.score(x_test,Y_test)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model10 = linear_model.Lasso(alpha=.01)\n",
                "model10.fit(x_train,Y_train)\n",
                "\n",
                "accuracy10 = model10.score(x_test,Y_test)\n",
                "print(accuracy10*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model10.predict(x_test)",
                "ASSIGN = mean_squared_error(Y_test, y_pred10, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred10 = model10.predict(x_test)\n",
                "\n",
                "val = mean_squared_error(Y_test, y_pred10, squared=False)\n",
                "val10 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred10)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "SETUP",
                "ASSIGN = ElasticNet()",
                "ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}",
                "ENR.fit(x_train, Y_train)",
                "ENR.best_params_"
            ],
            "source_orig": [
                "EN = ElasticNet()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "ENR = GridSearchCV(Elasticnet, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "ENR.fit(x_train, Y_train)\n",
                "ENR.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = linear_model.Lasso(alpha=.01)",
                "ASSIGN.fit(x_train,Y_train)",
                "ASSIGN = model11.score(x_test,Y_test)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model11 = linear_model.Lasso(alpha=.01)\n",
                "model11.fit(x_train,Y_train)\n",
                "\n",
                "accuracy11 = model11.score(x_test,Y_test)\n",
                "print(accuracy11*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model11.predict(x_test)",
                "ASSIGN = mean_squared_error(Y_test, y_pred11, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred11 = model11.predict(x_test)\n",
                "\n",
                "val = mean_squared_error(Y_test, y_pred11, squared=False)\n",
                "val11 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred11)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "SETUP",
                "ASSIGN = {'C':[.0001 ,.001 ,0.1, 1, 10, 100, 1000],",
                "'epsilon':[0.001, 0.01, 0.1, 0.5, 1, 2, 4]",
                "}",
                "ENR.fit(x_train, Y_train)",
                "ENR.best_params_"
            ],
            "source_orig": [
                "SVR = SVR()\n",
                "\n",
                "parameters = {'C':[.0001 ,.001 ,0.1, 1, 10, 100, 1000],\n",
                "              'epsilon':[0.001, 0.01, 0.1, 0.5, 1, 2, 4]\n",
                "             }\n",
                "\n",
                "ENR = GridSearchCV(SVR, parameters, scoring='neg_mean_squared_error', cv = 10)\n",
                "\n",
                "ENR.fit(x_train, Y_train)\n",
                "ENR.best_params_"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = SVR(C=1, epsilon=0.1)",
                "ASSIGN.fit(x_train,Y_train)",
                "ASSIGN = ASSIGN.score(x_test,Y_test)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "from sklearn.svm import SVR\n",
                "model12 = SVR(C=1, epsilon=0.1)\n",
                "model12.fit(x_train,Y_train)\n",
                "\n",
                "model12 = model12.score(x_test,Y_test)\n",
                "print(model12*100,'%')"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = Ridge()",
                "ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}",
                "ASSIGN = GridSearchCV(ASSIGN, parameters, scoring='neg_mean_squared_error', cv = 100)",
                "ASSIGN.fit(x_train, Y_train)",
                "ASSIGN.best_params_"
            ],
            "source_orig": [
                "R = Ridge()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "R = GridSearchCV(R, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "R.fit(x_train, Y_train)\n",
                "R.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = linear_model.Ridge(alpha=10)",
                "ASSIGN.fit(x_train,Y_train)",
                "ASSIGN = model13.score(x_test,Y_test)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model13 = linear_model.Ridge(alpha=10)\n",
                "model13.fit(x_train,Y_train)\n",
                "\n",
                "accuracy13 = model13.score(x_test,Y_test)\n",
                "print(accuracy13*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model13.predict(x_test)",
                "ASSIGN = mean_squared_error(Y_test, y_pred13, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred13 = model13.predict(x_test)\n",
                "\n",
                "val = mean_squared_error(Y_test, y_pred13, squared=False)\n",
                "val13 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred13)))\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "SETUP",
                "ASSIGN = MinMaxScaler()",
                "ASSIGN = trans.fit_transform(df)",
                "ASSIGN = DataFrame(dat)",
                "df.head()"
            ],
            "source_orig": [
                "from pandas import DataFrame\n",
                "trans = MinMaxScaler()\n",
                "dat = trans.fit_transform(df)\n",
                "dataset = DataFrame(dat)\n",
                "\n",
                "df.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "dataset.columns = ['GRE Score','TOEFL Score','University Rating','SOP','LOR','CGPA','Research','Chance of Admit']"
            ],
            "source_orig": [
                "dataset.columns = ['GRE Score','TOEFL Score','University Rating','SOP','LOR','CGPA','Research','Chance of Admit']\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = dataset.drop(['Chance of Admit'], axis = 1)",
                "ex"
            ],
            "source_orig": [
                "ex = dataset.drop(['Chance of Admit'], axis = 1)\n",
                "ex"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = dataset['Chance of Admit']",
                "ey"
            ],
            "source_orig": [
                "ey = dataset['Chance of Admit']\n",
                "ey"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x_t, x_es, Y_t, Y_es = train_test_split(ex, ey, test_size = 0.20,shuffle = False)"
            ],
            "source_orig": [
                "x_t, x_es, Y_t, Y_es = train_test_split(ex, ey, test_size = 0.20,shuffle = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = LinearRegression()",
                "ASSIGN.fit(x_t, Y_t)",
                "ASSIGN = model14.score(x_es,Y_es)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model14 = LinearRegression()\n",
                "model14.fit(x_t, Y_t)\n",
                "\n",
                "accuracy14 = model14.score(x_es,Y_es)\n",
                "print(accuracy14*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model14.predict(x_es)",
                "ASSIGN = mean_squared_error(Y_es, y_pred14, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred14 = model14.predict(x_es)\n",
                "\n",
                "val = mean_squared_error(Y_es, y_pred14, squared=False)\n",
                "val14 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred14)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = Lasso()",
                "ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}",
                "ASSIGN = GridSearchCV(l, parameters, scoring='neg_mean_squared_error', cv = 100)",
                "ASSIGN.fit(x_t, Y_t)",
                "ASSIGN.best_params_"
            ],
            "source_orig": [
                "l = Lasso()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "lr = GridSearchCV(l, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "lr.fit(x_t, Y_t)\n",
                "lr.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = linear_model.Lasso(alpha=.001)",
                "ASSIGN.fit(x_t,Y_t)",
                "ASSIGN = model15.score(x_es,Y_es)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model15 = linear_model.Lasso(alpha=.001)\n",
                "model15.fit(x_t,Y_t)\n",
                "\n",
                "accuracy15 = model15.score(x_es,Y_es)\n",
                "print(accuracy15*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model15.predict(x_es)",
                "ASSIGN = mean_squared_error(Y_es, y_pred15, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred15 = model15.predict(x_es)\n",
                "\n",
                "val = mean_squared_error(Y_es, y_pred15, squared=False)\n",
                "val15 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred15)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = Ridge()",
                "ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}",
                "ASSIGN = GridSearchCV(ASSIGN, parameters, scoring='neg_mean_squared_error', cv = 100)",
                "ASSIGN.fit(x_t, Y_t)",
                "ASSIGN.best_params_"
            ],
            "source_orig": [
                "r = Ridge()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "r = GridSearchCV(r, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "r.fit(x_t, Y_t)\n",
                "r.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = linear_model.Ridge(alpha=0.01)",
                "ASSIGN.fit(x_t,Y_t)",
                "ASSIGN = model16.score(x_es,Y_es)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model16 = linear_model.Ridge(alpha=0.01)\n",
                "model16.fit(x_t,Y_t)\n",
                "\n",
                "accuracy16 = model16.score(x_es,Y_es)\n",
                "print(accuracy16*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model16.predict(x_es)",
                "ASSIGN = mean_squared_error(Y_es, y_pred16, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred16 = model16.predict(x_es)\n",
                "\n",
                "val = mean_squared_error(Y_es, y_pred16, squared=False)\n",
                "val16 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred16)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = ElasticNet()",
                "ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}",
                "ASSIGN = GridSearchCV(en, parameters, scoring='neg_mean_squared_error', cv = 100)",
                "ASSIGN.fit(x_t, Y_t)",
                "ASSIGN.best_params_"
            ],
            "source_orig": [
                "en = ElasticNet()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "enr = GridSearchCV(en, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "enr.fit(x_t, Y_t)\n",
                "enr.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = linear_model.Lasso(alpha=.001)",
                "ASSIGN.fit(x_t,Y_t)",
                "ASSIGN = model17.score(x_es,Y_es)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model17 = linear_model.Lasso(alpha=.001)\n",
                "model17.fit(x_t,Y_t)\n",
                "\n",
                "accuracy17 = model17.score(x_es,Y_es)\n",
                "print(accuracy17*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model17.predict(x_es)",
                "ASSIGN = mean_squared_error(Y_es, y_pred17, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred17 = model17.predict(x_es)\n",
                "\n",
                "val = mean_squared_error(Y_es, y_pred17, squared=False)\n",
                "val17 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred17)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = DecisionTreeRegressor()",
                "ASSIGN.fit(x_t, Y_t)",
                "ASSIGN = model18.score(x_es,Y_es)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model18 = DecisionTreeRegressor()\n",
                "model18.fit(x_t, Y_t)\n",
                "\n",
                "accuracy18 = model18.score(x_es,Y_es)\n",
                "print(accuracy18*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model18.predict(x_es)",
                "ASSIGN = mean_squared_error(Y_es, y_pred8, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred18 = model18.predict(x_es)\n",
                "\n",
                "val = mean_squared_error(Y_es, y_pred8, squared=False)\n",
                "val18 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred18)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]",
                "ASSIGN = RandomForestRegressor()",
                "ASSIGN = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}",
                "ASSIGN = GridSearchCV(Rf, parameters,scoring='neg_mean_squared_error', cv=10)",
                "ASSIGN.fit(x_t, Y_t)",
                "ASSIGN.best_params_"
            ],
            "source_orig": [
                "n_estimators = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]\n",
                "\n",
                "Rf = RandomForestRegressor()\n",
                "\n",
                "parameters = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}\n",
                "\n",
                "Rfr = GridSearchCV(Rf, parameters,scoring='neg_mean_squared_error', cv=10)\n",
                "\n",
                "Rfr.fit(x_t, Y_t)\n",
                "\n",
                "Rfr.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = RandomForestRegressor(n_estimators = 100)",
                "ASSIGN.fit(x_t, Y_t)",
                "ASSIGN = model19.score(x_es,Y_es)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "model19 = RandomForestRegressor(n_estimators = 100)\n",
                "model19.fit(x_t, Y_t)\n",
                "\n",
                "accuracy19 = model19.score(x_es,Y_es)\n",
                "print(accuracy19*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = model19.predict(x_es)",
                "ASSIGN = mean_squared_error(Y_es, y_pred19, squared=False)",
                "ASSIGN = str(round(val, 4))",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, ASSIGN)))"
            ],
            "source_orig": [
                "y_pred19 = model19.predict(x_es)\n",
                "\n",
                "val = mean_squared_error(Y_es, y_pred19, squared=False)\n",
                "val19 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred19)))\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "train_model"
            ],
            "source": [
                "SETUP",
                "ASSIGN = SVR()",
                "ASSIGN = {'C':[.0001 ,.001 ,0.1, 1, 10, 100, 1000],",
                "'epsilon':[0.001, 0.01, 0.1, 0.5, 1, 2, 4]",
                "}",
                "ASSIGN = GridSearchCV(Svr, parameters, scoring='neg_mean_squared_error', cv = 10)",
                "ASSIGN.fit(x_t, Y_t)",
                "ASSIGN.best_params_"
            ],
            "source_orig": [
                "from sklearn.svm import SVR\n",
                "\n",
                "Svr = SVR()\n",
                "\n",
                "parameters = {'C':[.0001 ,.001 ,0.1, 1, 10, 100, 1000],\n",
                "              'epsilon':[0.001, 0.01, 0.1, 0.5, 1, 2, 4]\n",
                "             }\n",
                "\n",
                "Enr = GridSearchCV(Svr, parameters, scoring='neg_mean_squared_error', cv = 10)\n",
                "\n",
                "Enr.fit(x_t, Y_t)\n",
                "Enr.best_params_"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = SVR(C=1, epsilon=0.1)",
                "ASSIGN.fit(x_t,Y_t)",
                "ASSIGN = ASSIGN.score(x_es,Y_es)",
                "print(ASSIGN*100,'%')"
            ],
            "source_orig": [
                "from sklearn.svm import SVR\n",
                "model20 = SVR(C=1, epsilon=0.1)\n",
                "model20.fit(x_t,Y_t)\n",
                "\n",
                "model20 = model20.score(x_es,Y_es)\n",
                "print(model20*100,'%')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = [Half1RMSE,Half2RMSE,Half3RMSE]",
                "FullRMSE = pd.concat(ASSIGN, axis = 1)"
            ],
            "source_orig": [
                "frames = [Half1RMSE,Half2RMSE,Half3RMSE] \n",
                "FullRMSE = pd.concat(frames, axis = 1)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "FullRMSE"
            ],
            "source_orig": [
                "FullRMSE"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import seaborn as sns\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "ASSIGN=pd.read_csv('path')",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "train_data=pd.read_csv('/kaggle/input/ghouls-goblins-and-ghosts-boo/train.csv.zip')\n",
                "train_data.head()"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN=pd.read_csv('path')"
            ],
            "source_orig": [
                "test_data=pd.read_csv('/kaggle/input/ghouls-goblins-and-ghosts-boo/test.csv.zip')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "train_data.shape"
            ],
            "source_orig": [
                "train_data.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_data.info()"
            ],
            "source_orig": [
                "train_data.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ['bone_length','rotting_flesh','hair_length','has_soul']",
                "ASSIGN = ['color','type']"
            ],
            "source_orig": [
                "numerical = ['bone_length','rotting_flesh','hair_length','has_soul']\n",
                "categorical = ['color','type']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = train_data.ASSIGN()"
            ],
            "source_orig": [
                "corr = train_data.corr()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))"
            ],
            "source_orig": [
                "#heatmap gives a visual representation of correlation between different attributes of the data\n",
                "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.pairplot(train_data,hue='type')"
            ],
            "source_orig": [
                "sns.pairplot(train_data,hue='type')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "train_data[numerical].hist(bins=15, figsize=(15, 6), layout=(2, 4),rwidth=0.9,grid=False,color='purple');"
            ],
            "source_orig": [
                "#Check if numerical attributes have normal distributions\n",
                "train_data[numerical].hist(bins=15, figsize=(15, 6), layout=(2, 4),rwidth=0.9,grid=False,color='purple');"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.set()",
                "sns.countplot(train_data['color'])"
            ],
            "source_orig": [
                "#Visualize categorical variables\n",
                "sns.set()\n",
                "sns.countplot(train_data['color'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.set()",
                "sns.countplot(train_data['type'])"
            ],
            "source_orig": [
                "sns.set()\n",
                "sns.countplot(train_data['type'])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "test_data.shape"
            ],
            "source_orig": [
                "test_data.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test_data.head()"
            ],
            "source_orig": [
                "test_data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test_data.info()"
            ],
            "source_orig": [
                "test_data.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_data['color'].unique()"
            ],
            "source_orig": [
                "train_data['color'].unique()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test_data['color'].unique()"
            ],
            "source_orig": [
                "#Check if train and test data have the same categories\n",
                "test_data['color'].unique()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN=pd.concat([ASSIGN,pd.get_dummies(ASSIGN['color'])],axis=1)",
                "ASSIGN.drop('color',axis=1,inplace=True)",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "#one-hot-encoding categorrical attribute:color\n",
                "train_data=pd.concat([train_data,pd.get_dummies(train_data['color'])],axis=1)\n",
                "train_data.drop('color',axis=1,inplace=True)\n",
                "train_data.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN=pd.concat([ASSIGN,pd.get_dummies(ASSIGN['color'])],axis=1)",
                "ASSIGN.drop('color',axis=1,inplace=True)",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "test_data=pd.concat([test_data,pd.get_dummies(test_data['color'])],axis=1)\n",
                "test_data.drop('color',axis=1,inplace=True)\n",
                "test_data.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X=train_data.drop(['id','type'],axis=1)",
                "ASSIGN=pd.get_dummies(train_data['type'])"
            ],
            "source_orig": [
                "X=train_data.drop(['id','type'],axis=1)\n",
                "y=pd.get_dummies(train_data['type'])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)",
                "print(X_train.shape,y_train.shape)",
                "print(X_test.shape,y_test.shape)"
            ],
            "source_orig": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
                "print(X_train.shape,y_train.shape)\n",
                "print(X_test.shape,y_test.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from keras.layers import Dense\n",
                "from keras.models import Sequential"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ASSIGN = Sequential()",
                "ASSIGN.add(Dense(100,input_shape=(X.shape[1],)))",
                "ASSIGN.add(Dense(100,activation='relu'))",
                "ASSIGN.add(Dense(100,activation='relu'))",
                "ASSIGN.add(Dense(3,activation='softmax'))",
                "ASSIGN.summary()"
            ],
            "source_orig": [
                "model = Sequential()\n",
                "model.add(Dense(100,input_shape=(X.shape[1],)))\n",
                "model.add(Dense(100,activation='relu'))\n",
                "model.add(Dense(100,activation='relu'))\n",
                "model.add(Dense(3,activation='softmax'))\n",
                "model.summary()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model.compile(optimizer='adam',",
                "ASSIGN='categorical_crossentropy',",
                "ASSIGN=['accuracy'])"
            ],
            "source_orig": [
                "model.compile(optimizer='adam',\n",
                "              loss='categorical_crossentropy',\n",
                "              metrics=['accuracy'])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN=model.fit(x=X_train,y=y_train,batch_size=10,epochs=10,verbose=2,validation_data=(X_test,y_test))"
            ],
            "source_orig": [
                "train=model.fit(x=X_train,y=y_train,batch_size=10,epochs=10,verbose=2,validation_data=(X_test,y_test))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(5,5))",
                "plt.plot(train.history['accuracy'],'r',label='Training accuracy')",
                "plt.plot(train.history['val_accuracy'],'b',label='Validation accuracy')",
                "plt.legend()"
            ],
            "source_orig": [
                "plt.figure(figsize=(5,5))\n",
                "plt.plot(train.history['accuracy'],'r',label='Training accuracy')\n",
                "plt.plot(train.history['val_accuracy'],'b',label='Validation accuracy')\n",
                "plt.legend()"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "source": [
                "ASSIGN=model.predict(test_data.drop('id',axis=1))"
            ],
            "source_orig": [
                "pred=model.predict(test_data.drop('id',axis=1))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=[np.argmax(i) for i in pred]"
            ],
            "source_orig": [
                "pred_final=[np.argmax(i) for i in pred]"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame({'id':test_data['id'], 'type':pred_final})",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "submission = pd.DataFrame({'id':test_data['id'], 'type':pred_final})\n",
                "submission.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submission.to_csv('..path', index=False)"
            ],
            "source_orig": [
                "submission.to_csv('../working/submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "SETUP",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "source_orig": [
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn import preprocessing\n",
                "sample_submission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n",
                "test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train[:5]"
            ],
            "source_orig": [
                "train[:5]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.describe()"
            ],
            "source_orig": [
                "train.describe()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = train['Id']",
                "ASSIGN = test['Id']",
                "train.drop(\"Id\", axis = 1, inplace = True)",
                "test.drop(\"Id\", axis = 1, inplace = True)"
            ],
            "source_orig": [
                "#Save the 'Id' column\n",
                "train_ID = train['Id']\n",
                "test_ID = test['Id']\n",
                "\n",
                "#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
                "train.drop(\"Id\", axis = 1, inplace = True)\n",
                "test.drop(\"Id\", axis = 1, inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN[ASSIGN.GrLivArea < 4500]",
                "ASSIGN.reset_index(drop=True, inplace=True)",
                "ASSIGN = [30, 88, 462, 631, 1322]",
                "ASSIGN = ASSIGN.drop(ASSIGN.index[outliers])"
            ],
            "source_orig": [
                "train = train[train.GrLivArea < 4500]\n",
                "train.reset_index(drop=True, inplace=True)\n",
                "outliers = [30, 88, 462, 631, 1322]\n",
                "train = train.drop(train.index[outliers])"
            ]
        },
        {
            "tags": [
                "check_results",
                "visualize_data"
            ],
            "source": [
                "CHECKPOINT",
                "sns.distplot(train['SalePrice'])",
                "print('Skewness: %f' % train['SalePrice'].skew())",
                "print('Kurtosis: %f' % train['SalePrice'].kurt())"
            ],
            "source_orig": [
                "sns.distplot(train['SalePrice'])\n",
                "print('Skewness: %f' % train['SalePrice'].skew())\n",
                "print('Kurtosis: %f' % train['SalePrice'].kurt())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "visualize_data"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = plt.figure(figsize=(15,5))",
                "plt.subplot(1,2,1)",
                "sns.distplot(train[\"SalePrice\"], fit=norm)",
                "ASSIGN = norm.fit(train['SalePrice'])",
                "plt.legend(['Norm dist. mu={:.2f}, sigma={:.2f}'.format(mu,sigma)])",
                "plt.subplot(1,2,2)",
                "stats.probplot(train['SalePrice'],plot=plt)",
                "plt.title('Before transfomation')",
                "print('mu = {:.2f},\\nsigma = {:.2f}'.format(mu,sigma))",
                "train.SalePrice = np.log1p(train.SalePrice)",
                "ASSIGN = train.SalePrice.values",
                "ASSIGN = train.SalePrice",
                "ASSIGN = plt.figure(figsize=(15,5))",
                "plt.subplot(1,2,1)",
                "sns.distplot(train.SalePrice, fit=norm)",
                "ASSIGN = norm.fit(train.SalePrice)",
                "plt.legend(['Norm distribution.(mu={:.2f}, sigma={:.2f})'.format(ASSIGN)])",
                "plt.subplot(1,2,2)",
                "plt.ylabel('Frequency')",
                "stats.probplot(train.SalePrice, plot=plt)",
                "plt.title('After transformation')",
                "print('\\n mu={:.2f}, sigma={:.2f}'.format(mu,sigma))"
            ],
            "source_orig": [
                "#the qq-plot before log-transfomation\n",
                "from scipy import stats\n",
                "from scipy.stats import norm\n",
                "fig = plt.figure(figsize=(15,5)) #\n",
                "plt.subplot(1,2,1) # \n",
                "sns.distplot(train[\"SalePrice\"], fit=norm)\n",
                "mu, sigma = norm.fit(train['SalePrice'])\n",
                "#plt.legend(['Norm dist. mu = %f, sigma = %f' %(mu,sigma)], loc='upper right')\n",
                "plt.legend(['Norm dist. mu={:.2f}, sigma={:.2f}'.format(mu,sigma)])\n",
                "plt.subplot(1,2,2)\n",
                "stats.probplot(train['SalePrice'],plot=plt)\n",
                "plt.title('Before transfomation')\n",
                "print('mu = {:.2f},\\nsigma = {:.2f}'.format(mu,sigma))\n",
                "\n",
                "#Do the transformation\n",
                "train.SalePrice = np.log1p(train.SalePrice)\n",
                "y_train = train.SalePrice.values\n",
                "y_train_orig = train.SalePrice\n",
                "#the reason why we do this is because the models like linear regression and SVM need the data to be norm distribution.\n",
                "\n",
                "#after the transformation\n",
                "fig = plt.figure(figsize=(15,5))\n",
                "plt.subplot(1,2,1)\n",
                "sns.distplot(train.SalePrice, fit=norm)\n",
                "mu, sigma = norm.fit(train.SalePrice)\n",
                "plt.legend(['Norm distribution.(mu={:.2f}, sigma={:.2f})'.format(mu, sigma)])\n",
                "plt.subplot(1,2,2)\n",
                "plt.ylabel('Frequency')\n",
                "stats.probplot(train.SalePrice, plot=plt)\n",
                "plt.title('After transformation')\n",
                "print('\\n mu={:.2f}, sigma={:.2f}'.format(mu,sigma))"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "CHECKPOINT",
                "ASSIGN = train.drop('SalePrice',axis=1)",
                "ASSIGN = pd.concat((train_X, test)).reset_index(drop=True)",
                "print(ASSIGN.shape)",
                "data_features.columns"
            ],
            "source_orig": [
                "train_X = train.drop('SalePrice',axis=1)\n",
                "data_features = pd.concat((train_X, test)).reset_index(drop=True)\n",
                "print(data_features.shape)\n",
                "data_features.columns\n",
                "#We concatenate the train set and the test set since we need to handle the data both on the train set and the test set."
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features[['MSSubClass', 'MSZoning']]"
            ],
            "source_orig": [
                "data_features[['MSSubClass', 'MSZoning']]"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data_features.isnull().sum().sort_values(ascending=False)",
                "ASSIGN = ASSIGN[ASSIGN>0]",
                "data_features_na"
            ],
            "source_orig": [
                "data_features_na = data_features.isnull().sum().sort_values(ascending=False)\n",
                "data_features_na = data_features_na[data_features_na>0]\n",
                "data_features_na"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = (data_features.isnull().sum()path().count()).sort_values(ascending=False)",
                "ASSIGN = ASSIGN[ASSIGN>0]",
                "pd.concat([data_features_na, ASSIGN],axis=1,keys=['total', 'ASSIGN'])"
            ],
            "source_orig": [
                "percent = (data_features.isnull().sum()/data_features.isnull().count()).sort_values(ascending=False)\n",
                "percent = percent[percent>0]\n",
                "pd.concat([data_features_na, percent],axis=1,keys=['total', 'percent'])"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data_features.isnull().sum().sort_values(ascending = False)",
                "ASSIGN = ASSIGN[ASSIGN > 0]",
                "missing_data"
            ],
            "source_orig": [
                "#Now the check the missing values\n",
                "missing_data = data_features.isnull().sum().sort_values(ascending = False)\n",
                "missing_data = missing_data[missing_data > 0]\n",
                "missing_data"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = [\"MSSubClass\",'MoSold','YrSold']",
                "for var in ASSIGN:",
                "data_features[var] = data_features[var].apply(str)"
            ],
            "source_orig": [
                "#Some number features stand for categories.\n",
                "str_var = [\"MSSubClass\",'MoSold','YrSold']\n",
                "for var in str_var:\n",
                "    data_features[var] = data_features[var].apply(str)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = preprocessing.LabelEncoder()",
                "ASSIGN = ['YearBuilt','YearRemodAdd','GarageYrBlt']",
                "for var in ASSIGN:",
                "data_features[var] = ASSIGN.fit_transform(data_features[var])",
                "data_features[ASSIGN]"
            ],
            "source_orig": [
                "#For variables about years,we use labelencoder method to transform. \n",
                "#You can also use one-hot encode but I don't like the dimensional disaster...\n",
                "le = preprocessing.LabelEncoder()\n",
                "#ohe = preprocessing.OneHotEncoder()\n",
                "Year_var = ['YearBuilt','YearRemodAdd','GarageYrBlt']\n",
                "for var in Year_var:\n",
                "    data_features[var] = le.fit_transform(data_features[var])\n",
                "data_features[Year_var]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_features['MasVnrScore'] = data_features['MasVnrType'] * data_features['MasVnrArea']",
                "data_features['BsmtScore2'] = data_features['BsmtQual'] * data_features['BsmtCond'] * data_features['BsmtExposure']",
                "data_features['TotalSF'] = data_features['TotalBsmtSF'] + data_features['1stFlrSF'] + data_features['2ndFlrSF']",
                "data_features[\"AllSF\"] = data_features[\"GrLivArea\"] + data_features[\"TotalBsmtSF\"]",
                "data_features['TotalBath'] = data_features['FullBath'] + 0.5*data_features['HalfBath'] + data_features['BsmtFullBath'] + 0.5*data_features['BsmtHalfBath']",
                "data_features['Total_porch_sf'] = data_features['OpenPorchSF'] + data_features['3SsnPorch'] + data_features['EnclosedPorch'] + data_features['ScreenPorch'] + data_features['WoodDeckSF']"
            ],
            "source_orig": [
                "data_features['MasVnrScore'] = data_features['MasVnrType'] * data_features['MasVnrArea']\n",
                "data_features['BsmtScore2'] = data_features['BsmtQual'] * data_features['BsmtCond'] * data_features['BsmtExposure']\n",
                "data_features['TotalSF'] = data_features['TotalBsmtSF'] + data_features['1stFlrSF'] + data_features['2ndFlrSF']\n",
                "data_features[\"AllSF\"] = data_features[\"GrLivArea\"] + data_features[\"TotalBsmtSF\"]\n",
                "data_features['TotalBath'] = data_features['FullBath'] + 0.5*data_features['HalfBath'] + data_features['BsmtFullBath'] + 0.5*data_features['BsmtHalfBath']\n",
                "data_features['Total_porch_sf'] = data_features['OpenPorchSF'] + data_features['3SsnPorch'] + data_features['EnclosedPorch'] + data_features['ScreenPorch'] + data_features['WoodDeckSF']"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data_features[:len(y_train)]",
                "ASSIGN.loc[:,'SalePrice'] = y_train",
                "ASSIGN = train1.corr()['SalePrice'].sort_values(ascending = False)",
                "ASSIGN = corr1[:15]",
                "corr15"
            ],
            "source_orig": [
                "train1 = data_features[:len(y_train)]\n",
                "train1.loc[:,'SalePrice'] = y_train\n",
                "corr1 = train1.corr()['SalePrice'].sort_values(ascending = False)\n",
                "corr15 = corr1[:15]\n",
                "corr15"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = corr15.drop(['SalePrice','AllSF','1stFlrSF','GarageFinish','GarageArea'])",
                "ASSIGN = corr10.index.tolist()",
                "for col in ASSIGN:",
                "data_features[col + '-2'] = data_features[col] **2",
                "data_features[col + '-3'] = data_features[col] **3",
                "data_features[col + '-sqrt'] = np.sqrt(data_features[col])"
            ],
            "source_orig": [
                "#Polynomial on the top 10 features.For the features have connection themself, only use one to do polynomail.\n",
                "corr10 = corr15.drop(['SalePrice','AllSF','1stFlrSF','GarageFinish','GarageArea'])\n",
                "corr10_var = corr10.index.tolist()\n",
                "for col in corr10_var:\n",
                "    data_features[col + '-2'] = data_features[col] **2\n",
                "    data_features[col + '-3'] = data_features[col] **3\n",
                "    data_features[col + '-sqrt'] = np.sqrt(data_features[col])\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data_features.select_dtypes(include = ['object']).columns",
                "ASSIGN = data_features.select_dtypes(exclude = ['object']).columns",
                "print(ASSIGN)",
                "print('Categorial features :' + str(len(ASSIGN)) + '\\n')",
                "print(ASSIGN)",
                "print('Numerical features :' + str(len(ASSIGN)))"
            ],
            "source_orig": [
                "cat_features = data_features.select_dtypes(include = ['object']).columns\n",
                "num_features = data_features.select_dtypes(exclude = ['object']).columns\n",
                "print(cat_features)\n",
                "print('Categorial features :' + str(len(cat_features)) + '\\n')\n",
                "\n",
                "print(num_features)\n",
                "print('Numerical features :' + str(len(num_features)))"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = data_features[num_features]",
                "ASSIGN = data_features[cat_features]",
                "ASSIGN.head()",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "data_num = data_features[num_features]\n",
                "data_cat = data_features[cat_features]\n",
                "data_num.head()\n",
                "data_cat.head()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data_num.skew().sort_values(ascending = False)",
                "skew_features"
            ],
            "source_orig": [
                "skew_features = data_num.skew().sort_values(ascending = False)\n",
                "skew_features"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from scipy.special import boxcox1p\n",
                "from scipy.stats import boxcox_normmax\n",
                "from sklearn.base import BaseEstimator, TransformerMixin"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "CHECKPOINT",
                "ASSIGN = ASSIGN[abs(ASSIGN) > 0.5]",
                "print('The mean skewness of the variables is{}'.format(np.mean(data_num.skew())))",
                "print('There are {} features have to boxcox1p transform'.format(len(ASSIGN)))",
                "skew_features"
            ],
            "source_orig": [
                "skew_features = skew_features[abs(skew_features) > 0.5]\n",
                "print('The mean skewness of the variables is{}'.format(np.mean(data_num.skew())))\n",
                "print('There are {} features have to boxcox1p transform'.format(len(skew_features)))\n",
                "skew_features"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(data_features)"
            ],
            "source_orig": [
                "len(data_features)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = pd.get_dummies(data_features)",
                "ASSIGN = final_features.iloc[:len(y_train),:]",
                "ASSIGN = final_features.iloc[len(y_train):,:]",
                "print('The shape of train set is{},y set is{},and the shape of test set is{}'.format(ASSIGN.shape,y_train.shape,ASSIGN.shape))"
            ],
            "source_orig": [
                "final_features = pd.get_dummies(data_features)\n",
                "X_train = final_features.iloc[:len(y_train),:]\n",
                "X_test = final_features.iloc[len(y_train):,:]\n",
                "print('The shape of train set is{},y set is{},and the shape of test set is{}'.format(X_train.shape,y_train.shape,X_test.shape))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "type(X)"
            ],
            "source_orig": [
                "type(X)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X.isnull().sum().sort_values(ascending = False)"
            ],
            "source_orig": [
                "X.isnull().sum().sort_values(ascending = False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from datetime import datetime\n",
                "from sklearn.preprocessing import RobustScaler\n",
                "from sklearn.model_selection import KFold, cross_val_score\n",
                "from sklearn.metrics import mean_squared_error , make_scorer\n",
                "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
                "from sklearn.pipeline import make_pipeline\n",
                "from sklearn.linear_model import LinearRegression\n",
                "\n",
                "from sklearn.ensemble import GradientBoostingRegressor\n",
                "from sklearn.svm import SVR\n",
                "from mlxtend.regressor import StackingCVRegressor\n",
                "from sklearn.linear_model import LinearRegression\n",
                "\n",
                "from xgboost import XGBRegressor\n",
                "from lightgbm import LGBMRegressor"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN = {'XGBoost':XGBRegressor(n_estimators=1000,",
                "ASSIGN=6,",
                "ASSIGN ='reg:squarederror')}"
            ],
            "source_orig": [
                "models = {'XGBoost':XGBRegressor(n_estimators=1000,\n",
                "                                max_depth=6,\n",
                "                                objective ='reg:squarederror')}"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "models.keys()"
            ],
            "source_orig": [
                "models.keys()"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "for name in models.keys():",
                "ASSIGN = models[name]",
                "ASSIGN.fit(X_train, y_train)",
                "ASSIGN = cv_rmse(model,X_train, y_train)",
                "print(ASSIGN.mean)"
            ],
            "source_orig": [
                "for name in models.keys():\n",
                "    model = models[name]\n",
                "    model.fit(X_train, y_train)\n",
                "    scores = cv_rmse(model,X_train, y_train)\n",
                "    print(scores.mean)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "scores.mean()"
            ],
            "source_orig": [
                "scores.mean()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "sample_submission.to_csv(\"submission.csv\", index=False)"
            ],
            "source_orig": [
                "sample_submission.to_csv(\"submission.csv\", index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "source_orig": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "SETUP",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "source_orig": [
                "import pandas as pd\n",
                "sample_submission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n",
                "test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train[:10]"
            ],
            "source_orig": [
                "train[:10]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = train['Id']",
                "ASSIGN = test['Id']",
                "train.drop('Id',inplace = True, axis = 1)",
                "test.drop('Id',inplace = True, axis = 1)"
            ],
            "source_orig": [
                "train_id = train['Id']\n",
                "test_id = test['Id']\n",
                "train.drop('Id',inplace = True, axis = 1)\n",
                "test.drop('Id',inplace = True, axis = 1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN[ASSIGN.GrLivArea < 4500]",
                "ASSIGN.reset_index(drop = True, inplace = True)",
                "ASSIGN = [30, 88, 462, 631, 1322]",
                "ASSIGN.drop(ASSIGN.index[ASSIGN], inplace = True)"
            ],
            "source_orig": [
                "train = train[train.GrLivArea < 4500]\n",
                "train.reset_index(drop = True, inplace = True)\n",
                "outliars = [30, 88, 462, 631, 1322]\n",
                "train.drop(train.index[outliars], inplace = True)\n",
                "#If you want to know why remove these values u can read kernels about EDA"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train.drop('SalePrice', inplace=True, axis=1)",
                "ASSIGN = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],test.loc[:,'MSSubClass':'SaleCondition'])).reset_index(drop=True)"
            ],
            "source_orig": [
                "#we don't need SalePrice so drop it\n",
                "train.drop('SalePrice', inplace=True, axis=1)\n",
                "data_features = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],test.loc[:,'MSSubClass':'SaleCondition'])).reset_index(drop=True)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data_features.isnull().sum().sort_values(ascending=False)",
                "ASSIGN = ASSIGN[ASSIGN>0]",
                "ASSIGN = totalpath(data_features)",
                "percent",
                "ASSIGN = pd.concat((total,percent),axis=1,keys=['total','percent'])",
                "missing_data"
            ],
            "source_orig": [
                "#first get the total missing values\n",
                "total = data_features.isnull().sum().sort_values(ascending=False)\n",
                "total = total[total>0]\n",
                "percent = total/len(data_features)\n",
                "percent\n",
                "missing_data = pd.concat((total,percent),axis=1,keys=['total','percent'])\n",
                "missing_data"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features.loc[(data_features['PoolQC'].notnull()),['PoolArea','PoolQC']]"
            ],
            "source_orig": [
                "data_features.loc[(data_features['PoolQC'].notnull()),['PoolArea','PoolQC']]"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = data_features.loc[data_features['MiscFeature'].notnull(),'MiscFeature']",
                "data_features.loc[data_features['MiscFeature'].notnull(),'MiscFeature'].value_counts()"
            ],
            "source_orig": [
                "#The code below just for a simple look\n",
                "a = data_features.loc[data_features['MiscFeature'].notnull(),'MiscFeature']\n",
                "data_features.loc[data_features['MiscFeature'].notnull(),'MiscFeature'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = data_features.loc[data_features['Alley'].notnull(),'Alley']",
                "data_features.loc[data_features['Alley'].notnull(),'Alley'].value_counts()"
            ],
            "source_orig": [
                "a = data_features.loc[data_features['Alley'].notnull(),'Alley']\n",
                "data_features.loc[data_features['Alley'].notnull(),'Alley'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = data_features.loc[data_features['Fence'].notnull(),'Fence']",
                "data_features.loc[data_features['Fence'].notnull(),'Fence'].value_counts()"
            ],
            "source_orig": [
                "a = data_features.loc[data_features['Fence'].notnull(),'Fence']\n",
                "data_features.loc[data_features['Fence'].notnull(),'Fence'].value_counts()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['FireplaceQu'].groupby([data_features['Fireplaces'],data_features['FireplaceQu']]).count()"
            ],
            "source_orig": [
                "data_features['FireplaceQu'].groupby([data_features['Fireplaces'],data_features['FireplaceQu']]).count()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features.groupby('Neighborhood')['LotFrontage'].mean()"
            ],
            "source_orig": [
                "data_features.groupby('Neighborhood')['LotFrontage'].mean()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(16,9))",
                "plt.plot(data_features['LotArea'], data_features['LotFrontage'])"
            ],
            "source_orig": [
                "plt.figure(figsize=(16,9))\n",
                "plt.plot(data_features['LotArea'], data_features['LotFrontage'])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['GarageCond'],data_features['GarageQual'])).count()"
            ],
            "source_orig": [
                "data_features['MSSubClass'].groupby((data_features['GarageCond'],data_features['GarageQual'])).count()\n",
                "#These two variables have positive correlation.So we can use the mode to fill the missing in GarageCond and GarageQual. "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageQual'].notnull()), ['GarageQual']]"
            ],
            "source_orig": [
                "data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageQual'].notnull()), ['GarageQual']]\n",
                "#it seems like they have the same missing values.\n",
                "#the same for the two below\n",
                "#so fill them all with NONE"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageYrBlt'].notnull()), ['GarageYrBlt']]"
            ],
            "source_orig": [
                "data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageYrBlt'].notnull()), ['GarageYrBlt']]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageFinish'].notnull()), ['GarageFinish']]"
            ],
            "source_orig": [
                "data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageFinish'].notnull()), ['GarageFinish']]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_features.loc[(data_features['GarageYrBlt'].isnull() & data_features['GarageType'].notnull())",
                ",['GarageYrBlt','GarageType','GarageCond','GarageFinish','GarageQual']]",
                "ASSIGN = ['GarageYrBlt','GarageType','GarageCond','GarageFinish','GarageQual']",
                "ASSIGN = (data_features['GarageYrBlt'].isnull() & data_features['GarageType'].notnull())",
                "for col in ASSIGN:",
                "data_features.loc[ASSIGN,col] = data_features[(data_features['GarageType'] == 'Detchd')][col].mode()[0]"
            ],
            "source_orig": [
                "data_features.loc[(data_features['GarageYrBlt'].isnull() & data_features['GarageType'].notnull())\n",
                "                  ,['GarageYrBlt','GarageType','GarageCond','GarageFinish','GarageQual']]\n",
                "#so we use the value of GarageType to fill the other four variables\n",
                "garage_var = ['GarageYrBlt','GarageType','GarageCond','GarageFinish','GarageQual']\n",
                "condition1 = (data_features['GarageYrBlt'].isnull() & data_features['GarageType'].notnull())\n",
                "for col in garage_var:\n",
                "    data_features.loc[condition1,col] = data_features[(data_features['GarageType'] == 'Detchd')][col].mode()[0]\n",
                "#Note that we still have 156 missing values to fill for all 5 variables"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "missing_data[11:]"
            ],
            "source_orig": [
                "#handle them in the same way as garage\n",
                "missing_data[11:]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features[(data_features['BsmtFinType1'].isnull() & data_features['BsmtCond'].notnull())]"
            ],
            "source_orig": [
                "data_features[(data_features['BsmtFinType1'].isnull() & data_features['BsmtCond'].notnull())]\n",
                "#We detect that all bsmt variables have 79 common missing values. Obviously these data mean there are no basements in these houses.\n",
                "#So we use the NONE to fill them like the garage variables."
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtExposure'].isnull())][",
                "['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]"
            ],
            "source_orig": [
                "data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtExposure'].isnull())][\n",
                "    ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['BsmtQual'],data_features['BsmtExposure'])).count()"
            ],
            "source_orig": [
                "data_features['MSSubClass'].groupby((data_features['BsmtQual'],data_features['BsmtExposure'])).count()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['BsmtCond'],data_features['BsmtExposure'])).count()"
            ],
            "source_orig": [
                "data_features['MSSubClass'].groupby((data_features['BsmtCond'],data_features['BsmtExposure'])).count()\n",
                "#When Con is TA and Qual is Gd we shuold choose No to fill the missing value in 'BsmtExposure'"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = (data_features['BsmtFinType1'].notnull() & data_features['BsmtExposure'].isnull())",
                "data_features.loc[ASSIGN,'BsmtExposure'] = 'No'"
            ],
            "source_orig": [
                "condition2 = (data_features['BsmtFinType1'].notnull() & data_features['BsmtExposure'].isnull())\n",
                "data_features.loc[condition2,'BsmtExposure'] = 'No'"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtCond'].isnull())][",
                "['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]"
            ],
            "source_orig": [
                "data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtCond'].isnull())][\n",
                "    ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['BsmtExposure'],data_features['BsmtCond'])).count()"
            ],
            "source_orig": [
                "data_features['MSSubClass'].groupby((data_features['BsmtExposure'],data_features['BsmtCond'])).count()\n",
                "#The proportion of TA is larger than other values so use TA to fill 'BsmtCond'"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = (data_features['BsmtFinType1'].notnull() & data_features['BsmtCond'].isnull())",
                "data_features.loc[ASSIGN,'BsmtCond'] = 'TA'"
            ],
            "source_orig": [
                "condition3 = (data_features['BsmtFinType1'].notnull() & data_features['BsmtCond'].isnull())\n",
                "data_features.loc[condition3,'BsmtCond'] = 'TA'"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtQual'].isnull())][",
                "['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]"
            ],
            "source_orig": [
                "data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtQual'].isnull())][\n",
                "    ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['BsmtExposure'],data_features['BsmtQual'])).count()"
            ],
            "source_orig": [
                "data_features['MSSubClass'].groupby((data_features['BsmtExposure'],data_features['BsmtQual'])).count()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['BsmtCond'],data_features['BsmtQual'])).count()"
            ],
            "source_orig": [
                "data_features['MSSubClass'].groupby((data_features['BsmtCond'],data_features['BsmtQual'])).count()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = (data_features['BsmtFinType2'].isnull() & data_features['BsmtFinType1'].notnull())",
                "data_features[ASSIGN][['BsmtFinType1','BsmtFinType2']]"
            ],
            "source_orig": [
                "#The last one \n",
                "condition5 = (data_features['BsmtFinType2'].isnull() & data_features['BsmtFinType1'].notnull())\n",
                "data_features[condition5][['BsmtFinType1','BsmtFinType2']]\n",
                "#From the data description "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['BsmtFinType1'],data_features['BsmtFinType2'])).count()",
                "data_features.loc[condition5, 'BsmtFinType2'] = 'Unf'"
            ],
            "source_orig": [
                "data_features['MSSubClass'].groupby((data_features['BsmtFinType1'],data_features['BsmtFinType2'])).count()\n",
                "#I guess even if Type1 is good , the Type 2 is more likely to be Unf. So fill 'BsmtFinType2' by Unf\n",
                "data_features.loc[condition5, 'BsmtFinType2'] = 'Unf'"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']",
                "ASSIGN = ['GarageType','GarageCond','GarageFinish','GarageQual']",
                "ASSIGN = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu']",
                "ASSIGN = data_features.isnull().sum().sort_values(ascending = False)",
                "ASSIGN = ASSIGN[ASSIGN > 0]",
                "missing_data"
            ],
            "source_orig": [
                "bsmt_var = ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']\n",
                "garage_var = ['GarageType','GarageCond','GarageFinish','GarageQual']\n",
                "NONE_var = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu']\n",
                "missing_data = data_features.isnull().sum().sort_values(ascending = False)\n",
                "missing_data = missing_data[missing_data > 0]\n",
                "missing_data"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for col in bsmt_var, garage_var,NONE_var:",
                "data_features[col] = data_features[col].fillna('NONE')",
                "data_features['GarageYrBlt'] = data_features['GarageYrBlt'].fillna(0)"
            ],
            "source_orig": [
                "for col in bsmt_var, garage_var,NONE_var:\n",
                "    data_features[col] = data_features[col].fillna('NONE')\n",
                "data_features['GarageYrBlt'] = data_features['GarageYrBlt'].fillna(0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['LotFrontage']"
            ],
            "source_orig": [
                "data_features['LotFrontage']"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data_features.isnull().sum().sort_values(ascending = False)",
                "ASSIGN = ASSIGN[ASSIGN > 0]",
                "missing_data"
            ],
            "source_orig": [
                "missing_data = data_features.isnull().sum().sort_values(ascending = False)\n",
                "missing_data = missing_data[missing_data > 0]\n",
                "missing_data"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ASSIGN = (data_features['MasVnrType'].isnull() & data_features['MasVnrArea'].notnull())",
                "data_features.loc[ASSIGN,['MasVnrType','MasVnrArea']]"
            ],
            "source_orig": [
                "condition6 = (data_features['MasVnrType'].isnull() & data_features['MasVnrArea'].notnull())\n",
                "data_features.loc[condition6,['MasVnrType','MasVnrArea']]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MasVnrArea'].groupby(data_features['MasVnrType']).describe()"
            ],
            "source_orig": [
                "data_features['MasVnrArea'].groupby(data_features['MasVnrType']).describe()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MasVnrArea'].groupby(data_features['MasVnrType']).median()"
            ],
            "source_orig": [
                "data_features['MasVnrArea'].groupby(data_features['MasVnrType']).median()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.boxplot(data_features['MasVnrType'],data_features['MasVnrArea'])"
            ],
            "source_orig": [
                "sns.boxplot(data_features['MasVnrType'],data_features['MasVnrArea'])\n",
                "#Maybe fill it with 'Stone' is more reasonable.\n",
                "#Something strange here. None means no masonry but there still several values. May theuy are outliars that I should remove?"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_features.loc[condition6,'MasVnrType'] = 'Stone'",
                "data_features['MasVnrType'] = data_features['MasVnrType'].fillna('None')",
                "data_features['MasVnrArea'] = data_features['MasVnrArea'].fillna(0)"
            ],
            "source_orig": [
                "data_features.loc[condition6,'MasVnrType'] = 'Stone'\n",
                "data_features['MasVnrType'] = data_features['MasVnrType'].fillna('None')\n",
                "data_features['MasVnrArea'] = data_features['MasVnrArea'].fillna(0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_features['MSZoning'].groupby([data_features['MSSubClass'],data_features['MSZoning']]).count()",
                "data_features['MSZoning'] = data_features['MSZoning'].groupby(data_features['MSSubClass']).transform(lambda x:x.fillna(x.mode()[0]))"
            ],
            "source_orig": [
                "data_features['MSZoning'].groupby([data_features['MSSubClass'],data_features['MSZoning']]).count()\n",
                "data_features['MSZoning'] = data_features['MSZoning'].groupby(data_features['MSSubClass']).transform(lambda x:x.fillna(x.mode()[0]))"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data_features.isnull().sum().sort_values(ascending = False)",
                "ASSIGN = ASSIGN[ASSIGN > 0]",
                "missing_data"
            ],
            "source_orig": [
                "missing_data = data_features.isnull().sum().sort_values(ascending = False)\n",
                "missing_data = missing_data[missing_data > 0]\n",
                "missing_data"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "NA_for_0 = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath',",
                "'GarageArea', 'GarageCars','MasVnrArea']",
                "for col in NA_for_0:",
                "data_features[col] = data_features[col].fillna(0)"
            ],
            "source_orig": [
                "NA_for_0 = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath',\n",
                "            'GarageArea', 'GarageCars','MasVnrArea']\n",
                "for col in NA_for_0:\n",
                "    data_features[col] = data_features[col].fillna(0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ['Exterior1st','Exterior2nd','SaleType','Electrical','KitchenQual']",
                "for col in ASSIGN:",
                "data_features[col].fillna(data_features[col].mode()[0], inplace = True)"
            ],
            "source_orig": [
                "common_for_NA = ['Exterior1st','Exterior2nd','SaleType','Electrical','KitchenQual']\n",
                "for col in common_for_NA:\n",
                "    data_features[col].fillna(data_features[col].mode()[0], inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_features['Functional'] = data_features['Functional'].fillna('Typ')",
                "data_features['Utilities'] = data_features['Utilities'].fillna('None')"
            ],
            "source_orig": [
                "data_features['Functional'] = data_features['Functional'].fillna('Typ')\n",
                "data_features['Utilities'] = data_features['Utilities'].fillna('None')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = data_features.isnull().sum().sort_values(ascending = False)",
                "ASSIGN = ASSIGN[ASSIGN > 0]",
                "missing_data"
            ],
            "source_orig": [
                "missing_data = data_features.isnull().sum().sort_values(ascending = False)\n",
                "missing_data = missing_data[missing_data > 0]\n",
                "missing_data"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features[(data_features['BsmtCond'].isnull() & data_features['BsmtFinType1'].notnull())][['BsmtQual','BsmtFinType1','BsmtFinType2']]"
            ],
            "source_orig": [
                "data_features[(data_features['BsmtCond'].isnull() & data_features['BsmtFinType1'].notnull())][['BsmtQual','BsmtFinType1','BsmtFinType2']]"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ASSIGN = (data_features['BsmtCond'].isnull() & data_features['BsmtExposure'].notnull())",
                "data_features[ASSIGN]"
            ],
            "source_orig": [
                "condition1 = (data_features['BsmtCond'].isnull() & data_features['BsmtExposure'].notnull())\n",
                "data_features[condition1]"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import os\n",
                "import re\n",
                "from glob import glob\n",
                "from tqdm import tqdm\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import ast\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = glob('..path*.csv')",
                "ASSIGN = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']",
                "ASSIGN = []",
                "for f in ASSIGN[0:6]:",
                "ASSIGN = pd.read_csv(f, nrows=10)",
                "ASSIGN = ASSIGN[ASSIGN.recognized==True].head(2)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = pd.DataFrame(np.concatenate(drawlist), columns=cnames)",
                "draw_df"
            ],
            "source_orig": [
                "fnames = glob('../input/train_simplified/*.csv') #<class 'list'>\n",
                "cnames = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n",
                "drawlist = []\n",
                "for f in fnames[0:6]: # num of word : 5\n",
                "    first = pd.read_csv(f, nrows=10) # make sure we get a recognized drawing\n",
                "    first = first[first.recognized==True].head(2) # top head 2 get \n",
                "    drawlist.append(first)\n",
                "draw_df = pd.DataFrame(np.concatenate(drawlist), columns=cnames) # <class 'pandas.core.frame.DataFrame'>\n",
                "draw_df"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "draw_df.drawing.values[0]"
            ],
            "source_orig": [
                "draw_df.drawing.values[0]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = range(0,11,2)",
                "ASSIGN = range(1,12, 2)",
                "ASSIGN = draw_df[draw_df.index.isin(evens)]",
                "ASSIGN = draw_df[draw_df.index.isin(odds)]",
                "ASSIGN = [ast.literal_eval(pts) for pts in df1.drawing.values]",
                "ASSIGN = [ast.literal_eval(pts) for pts in df2.drawing.values]",
                "ASSIGN = df2.word.tolist()",
                "for i, example in enumerate(ASSIGN):",
                "plt.figure(figsize=(6,3))",
                "for x,y in example:",
                "plt.subplot(1,2,1)",
                "plt.plot(x, y, marker='.')",
                "plt.axis('off')",
                "for x,y, in ASSIGN[i]:",
                "plt.subplot(1,2,2)",
                "plt.plot(x, y, marker='.')",
                "plt.axis('off')",
                "ASSIGN = labels[i]",
                "plt.title(ASSIGN, fontsize=10)",
                "plt.show()"
            ],
            "source_orig": [
                "evens = range(0,11,2)\n",
                "odds = range(1,12, 2)\n",
                "# We have drawing images, 2 per label, consecutively\n",
                "df1 = draw_df[draw_df.index.isin(evens)]\n",
                "df2 = draw_df[draw_df.index.isin(odds)]\n",
                "\n",
                "example1s = [ast.literal_eval(pts) for pts in df1.drawing.values]\n",
                "example2s = [ast.literal_eval(pts) for pts in df2.drawing.values]\n",
                "labels = df2.word.tolist()\n",
                "\n",
                "for i, example in enumerate(example1s):\n",
                "    plt.figure(figsize=(6,3))\n",
                "    \n",
                "    for x,y in example:\n",
                "        plt.subplot(1,2,1)\n",
                "        plt.plot(x, y, marker='.')\n",
                "        plt.axis('off')\n",
                "\n",
                "    for x,y, in example2s[i]:\n",
                "        plt.subplot(1,2,2)\n",
                "        plt.plot(x, y, marker='.')\n",
                "        plt.axis('off')\n",
                "        label = labels[i]\n",
                "        plt.title(label, fontsize=10)\n",
                "\n",
                "    plt.show()  "
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "\n",
                "import os\n",
                "from glob import glob\n",
                "import re\n",
                "import ast\n",
                "import numpy as np \n",
                "import pandas as pd\n",
                "from PIL import Image, ImageDraw \n",
                "from tqdm import tqdm\n",
                "from dask import bag\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras.models import Sequential\n",
                "from keras.utils import to_categorical\n",
                "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
                "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "def pyth_test (x1, x2):",
                "print (x1 + x2)",
                "pyth_test(1,2)"
            ],
            "source_orig": [
                "def pyth_test (x1, x2):\n",
                "   \n",
                "    print (x1 + x2)\n",
                "\n",
                "pyth_test(1,2)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import os\n",
                "import re\n",
                "from glob import glob\n",
                "from tqdm import tqdm\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import ast\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "draw_df.drawing.values[0]"
            ],
            "source_orig": [
                "draw_df.drawing.values[0]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ASSIGN = range(0,11,2)",
                "ASSIGN = range(1,12, 2)",
                "ASSIGN = draw_df[draw_df.index.isin(evens)]",
                "ASSIGN = draw_df[draw_df.index.isin(odds)]",
                "ASSIGN = [ast.literal_eval(pts) for pts in df1.drawing.values]",
                "ASSIGN = [ast.literal_eval(pts) for pts in df2.drawing.values]",
                "ASSIGN = df2.word.tolist()",
                "for i, example in enumerate(ASSIGN):",
                "plt.figure(figsize=(6,3))",
                "for x,y in example:",
                "plt.subplot(1,2,1)",
                "plt.plot(x, y, marker='.')",
                "plt.axis('off')",
                "for x,y, in ASSIGN[i]:",
                "plt.subplot(1,2,2)",
                "plt.plot(x, y, marker='.')",
                "plt.axis('off')",
                "ASSIGN = labels[i]",
                "plt.title(ASSIGN, fontsize=10)",
                "plt.show()"
            ],
            "source_orig": [
                "evens = range(0,11,2)\n",
                "odds = range(1,12, 2)\n",
                "# We have drawing images, 2 per label, consecutively\n",
                "df1 = draw_df[draw_df.index.isin(evens)]\n",
                "df2 = draw_df[draw_df.index.isin(odds)]\n",
                "\n",
                "example1s = [ast.literal_eval(pts) for pts in df1.drawing.values]\n",
                "example2s = [ast.literal_eval(pts) for pts in df2.drawing.values]\n",
                "labels = df2.word.tolist()\n",
                "\n",
                "for i, example in enumerate(example1s):\n",
                "    plt.figure(figsize=(6,3))\n",
                "    \n",
                "    for x,y in example:\n",
                "        plt.subplot(1,2,1)\n",
                "        plt.plot(x, y, marker='.')\n",
                "        plt.axis('off')\n",
                "\n",
                "    for x,y, in example2s[i]:\n",
                "        plt.subplot(1,2,2)\n",
                "        plt.plot(x, y, marker='.')\n",
                "        plt.axis('off')\n",
                "        label = labels[i]\n",
                "        plt.title(label, fontsize=10)\n",
                "\n",
                "    plt.show()  "
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "\n",
                "import os\n",
                "from glob import glob\n",
                "import re\n",
                "import ast\n",
                "import numpy as np \n",
                "import pandas as pd\n",
                "from PIL import Image, ImageDraw \n",
                "from tqdm import tqdm\n",
                "from dask import bag\n",
                "import json\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras.models import Sequential\n",
                "from keras.models import Model\n",
                "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
                "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
                "from keras.layers import Input, Conv1D, Dense, Dropout, BatchNormalization, Flatten, MaxPool1D\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN = 0.1",
                "ASSIGN = int(valfrac * train_grand.shape[0])",
                "print(ASSIGN)",
                "np.random.shuffle(train_grand)",
                "y_train, X_train = train_grand[ASSIGN: , 0], train_grand[ASSIGN: , 1:]",
                "y_val, X_val = train_grand[0:ASSIGN, 0], train_grand[0:ASSIGN, 1:]",
                "del train_grand",
                "ASSIGN = keras.utils.to_categorical(ASSIGN, num_classes)",
                "ASSIGN = ASSIGN.reshape(-1, sequence_length,2)",
                "ASSIGN = keras.utils.to_categorical(ASSIGN, num_classes)",
                "ASSIGN = ASSIGN.reshape(-1, sequence_length,2)",
                "print(ASSIGN.shape, \"\\n\",",
                "ASSIGN.shape, \"\\n\",",
                "ASSIGN.shape, \"\\n\",",
                "ASSIGN.shape)"
            ],
            "source_orig": [
                "valfrac = 0.1 \n",
                "cutpt = int(valfrac * train_grand.shape[0])\n",
                "print(cutpt)\n",
                "\n",
                "np.random.shuffle(train_grand)\n",
                "y_train, X_train = train_grand[cutpt: , 0], train_grand[cutpt: , 1:]\n",
                "y_val, X_val = train_grand[0:cutpt, 0], train_grand[0:cutpt, 1:]\n",
                "\n",
                "del train_grand\n",
                "\n",
                "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
                "X_train = X_train.reshape(-1, sequence_length,2)\n",
                "\n",
                "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
                "X_val = X_val.reshape(-1, sequence_length,2)\n",
                "\n",
                "print(y_train.shape, \"\\n\",\n",
                "      X_train.shape, \"\\n\",\n",
                "      y_val.shape, \"\\n\",\n",
                "      X_val.shape)\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def createNetwork(seq_len):",
                "def addConv(network, features, kernel):",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "return Conv1D(features, kernel, padding='same', activation='relu')(ASSIGN)",
                "def addDense(ASSIGN, size):",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Dropout(0.2)(ASSIGN)",
                "return Dense(size, activation='relu')(ASSIGN)",
                "ASSIGN = Input(shape=(seq_len, 2))",
                "ASSIGN = input",
                "for features in [16, 24, 32]:",
                "ASSIGN = addConv(ASSIGN, features, 5)",
                "ASSIGN = MaxPool1D(pool_size=5)(ASSIGN)",
                "for features in [64, 96, 128]:",
                "ASSIGN = addConv(ASSIGN, features, 5)",
                "ASSIGN = MaxPool1D(pool_size=5)(ASSIGN)",
                "for features in [256, 384, 512]:",
                "ASSIGN = addConv(ASSIGN, features, 5)",
                "ASSIGN = Flatten()(ASSIGN)",
                "for size in [128, 128]:",
                "ASSIGN = addDense(ASSIGN, size)",
                "ASSIGN = Dense(len(files), activation='softmax')(network)",
                "ASSIGN = Model(inputs = input, outputs = output)",
                "return model",
                "ASSIGN = createNetwork(sequence_length)"
            ],
            "source_orig": [
                "def createNetwork(seq_len):\n",
                "    \n",
                "    # Function to add a convolution layer with batch normalization\n",
                "    def addConv(network, features, kernel):\n",
                "        network = BatchNormalization()(network)\n",
                "        return Conv1D(features, kernel, padding='same', activation='relu')(network)\n",
                "    \n",
                "    # Function to add a dense layer with batch normalization and dropout\n",
                "    def addDense(network, size):\n",
                "        network = BatchNormalization()(network)\n",
                "        network = Dropout(0.2)(network)\n",
                "        return Dense(size, activation='relu')(network)\n",
                "    \n",
                "    \n",
                "    # Input layer\n",
                "    input = Input(shape=(seq_len, 2))\n",
                "    network = input\n",
                "    \n",
                "    # Add 1D Convolution\n",
                "    for features in [16, 24, 32]:\n",
                "        network = addConv(network, features, 5)\n",
                "    network = MaxPool1D(pool_size=5)(network)\n",
                "    \n",
                "    # Add 1D Convolution\n",
                "    for features in [64, 96, 128]:\n",
                "        network = addConv(network, features, 5)\n",
                "    network = MaxPool1D(pool_size=5)(network)\n",
                "\n",
                "    # Add 1D Convolution\n",
                "    for features in [256, 384, 512]:\n",
                "        network = addConv(network, features, 5)\n",
                "    #network = MaxPool1D(pool_size=5)(network)\n",
                "\n",
                "    # Flatten\n",
                "    network = Flatten()(network)\n",
                "    \n",
                "    # Dense layer for combination\n",
                "    for size in [128, 128]:\n",
                "        network = addDense(network, size)\n",
                "    \n",
                "    # Output layer\n",
                "    output = Dense(len(files), activation='softmax')(network)\n",
                "\n",
                "\n",
                "    # Create and compile model\n",
                "    model = Model(inputs = input, outputs = output)\n",
                "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
                "\n",
                "#     # Display model\n",
                "#     model.summary()\n",
                "    return model\n",
                "\n",
                "model = createNetwork(sequence_length)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "def top_3_accuracy(x,y):",
                "ASSIGN = top_k_categorical_accuracy(x,y, 3)",
                "return t3",
                "ASSIGN = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3,",
                "ASSIGN=1, mode='auto', min_delta=0.005, cooldown=5, min_lr=0)",
                "ASSIGN = EarlyStopping(monitor='val_loss', mode='auto', patience=2,verbose=0)",
                "model.compile(loss='categorical_crossentropy',",
                "ASSIGN='adam',",
                "ASSIGN=['accuracy', top_3_accuracy])",
                "model.summary()",
                "model.fit(x=X_train, y=y_train,",
                "ASSIGN = 1000,",
                "ASSIGN = 25,",
                "ASSIGN = (X_val, y_val),",
                "ASSIGN = 1)"
            ],
            "source_orig": [
                "def top_3_accuracy(x,y): \n",
                "    t3 = top_k_categorical_accuracy(x,y, 3)\n",
                "    return t3\n",
                "\n",
                "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, \n",
                "                                   verbose=1, mode='auto', min_delta=0.005, cooldown=5, min_lr=0)\n",
                "\n",
                "earlystop = EarlyStopping(monitor='val_loss', mode='auto', patience=2,verbose=0) \n",
                "\n",
                "#callbacks = [reduceLROnPlat, earlystop]\n",
                "#callbacks = earlystop\n",
                "\n",
                "model.compile(loss='categorical_crossentropy',\n",
                "              optimizer='adam',\n",
                "              metrics=['accuracy', top_3_accuracy])\n",
                "\n",
                "model.summary()\n",
                "\n",
                "# model.fit(x=X_train, y=y_train,\n",
                "#           batch_size = 1000,\n",
                "#           epochs = 100,\n",
                "#           validation_data = (X_val, y_val),\n",
                "#           callbacks = callbacks,\n",
                "#           verbose = 1)\n",
                "model.fit(x=X_train, y=y_train,\n",
                "          batch_size = 1000,\n",
                "          epochs = 25,\n",
                "          validation_data = (X_val, y_val),\n",
                "          verbose = 1)\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data",
                "process_data"
            ],
            "source": [
                "ASSIGN = []",
                "ASSIGN = pd.read_csv('..path', index_col=['key_id'],",
                "ASSIGN=2048)",
                "for chunk in tqdm(ASSIGN, total=55):",
                "ASSIGN =[]",
                "for values in chunk.drawing.values:",
                "ASSIGN = json.loads(values)",
                "ASSIGN = []",
                "for x_axis, y_axis in ASSIGN:",
                "ASSIGN.extend(list(zip(x_axis, y_axis)))",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.zeros((sequence_length, 2))",
                "if sequence_length>ASSIGN.shape[0]:",
                "ASSIGN[:ASSIGN.shape[0],:] = ASSIGN",
                "else:",
                "ASSIGN = strokes[:sequence_length, :]",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.reshape(ASSIGN, (-1,sequence_length, 2))",
                "ASSIGN = model.predict(X, verbose=0)",
                "ASSIGN = np.argsort(-testpreds)[:, 0:3]",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = np.concatenate(ttvlist)"
            ],
            "source_orig": [
                "#%% get test set\n",
                "ttvlist = []\n",
                "reader = pd.read_csv('../input/test_simplified.csv', index_col=['key_id'],\n",
                "    chunksize=2048)\n",
                "\n",
                "for chunk in tqdm(reader, total=55):\n",
                "    X =[]\n",
                "    for values in chunk.drawing.values:\n",
                "        image = json.loads(values)\n",
                "        strokes = []\n",
                "        for x_axis, y_axis in image:\n",
                "            strokes.extend(list(zip(x_axis, y_axis)))\n",
                "        strokes = np.array(strokes)\n",
                "        pad = np.zeros((sequence_length, 2))\n",
                "        if sequence_length>strokes.shape[0]:\n",
                "            pad[:strokes.shape[0],:] = strokes\n",
                "        else:\n",
                "            pad = strokes[:sequence_length, :]\n",
                "        X.append(pad)\n",
                "        \n",
                "    X = np.array(X)\n",
                "    X = np.reshape(X, (-1,sequence_length, 2))\n",
                "    testpreds = model.predict(X, verbose=0)\n",
                "    ttvs = np.argsort(-testpreds)[:, 0:3]\n",
                "    ttvlist.append(ttvs)\n",
                "#     imagebag = bag.from_sequence(chunk.drawing.values).map(draw_it)\n",
                "#     testarray = np.array(imagebag.compute())\n",
                "\n",
                "#     testarray = np.reshape(testarray, (testarray.shape[0], imheight, imwidth, 1))\n",
                "#     testpreds = model.predict(testarray, verbose=0)\n",
                "#     ttvs = np.argsort(-testpreds)[:, 0:3]  # top 3\n",
                "#     ttvlist.append(ttvs)\n",
                "    \n",
                "ttvarray = np.concatenate(ttvlist)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "check_results",
                "ingest_data",
                "process_data"
            ],
            "source": [
                "ASSIGN = pd.DataFrame({'first': ttvarray[:,0], 'second': ttvarray[:,1], 'third': ttvarray[:,2]})",
                "ASSIGN = ASSIGN.replace(numstonames)",
                "ASSIGN['words'] = ASSIGN['first'] + \" \" + ASSIGN['second'] + \" \" + ASSIGN['third']",
                "ASSIGN = pd.read_csv('..path', index_col=['key_id'])",
                "ASSIGN = preds_df.words.values",
                "ASSIGN.to_csv('subcnn_small.csv')",
                "ASSIGN.head()"
            ],
            "source_orig": [
                "preds_df = pd.DataFrame({'first': ttvarray[:,0], 'second': ttvarray[:,1], 'third': ttvarray[:,2]})\n",
                "preds_df = preds_df.replace(numstonames)\n",
                "preds_df['words'] = preds_df['first'] + \" \" + preds_df['second'] + \" \" + preds_df['third']\n",
                "\n",
                "sub = pd.read_csv('../input/sample_submission.csv', index_col=['key_id'])\n",
                "sub['word'] = preds_df.words.values\n",
                "sub.to_csv('subcnn_small.csv')\n",
                "sub.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sb\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import r2_score"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "ASSIGN=pd.read_csv(\"..path\")"
            ],
            "source_orig": [
                "zomato_orgnl=pd.read_csv(\"../input/zomato-bangalore-restaurants/zomato.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato_orgnl.head()"
            ],
            "source_orig": [
                "zomato_orgnl.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato_orgnl.isnull().sum()"
            ],
            "source_orig": [
                "zomato_orgnl.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato_orgnl.info()"
            ],
            "source_orig": [
                "zomato_orgnl.info()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "ASSIGN=zomato_orgnl.drop(['url','dish_liked','phone'],axis=1)",
                "zomato.columns"
            ],
            "source_orig": [
                "zomato=zomato_orgnl.drop(['url','dish_liked','phone'],axis=1)\n",
                "zomato.columns"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "zomato.rename({'approx_cost(for two people)': 'approx_cost_for_2_people',",
                "'listed_in(type)':'listed_in_type',",
                "'listed_in(city)':'listed_in_city'",
                "}, axis=1, inplace=True)",
                "zomato.columns"
            ],
            "source_orig": [
                "zomato.rename({'approx_cost(for two people)': 'approx_cost_for_2_people',\n",
                "               'listed_in(type)':'listed_in_type',\n",
                "               'listed_in(city)':'listed_in_city'\n",
                "              }, axis=1, inplace=True)\n",
                "zomato.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = lambda x: int(x.replace(',', '')) if type(x) == np.str and x != np.nan else x",
                "zomato.votes = zomato.votes.astype('int')",
                "zomato['approx_cost_for_2_people'] = zomato['approx_cost_for_2_people'].apply(ASSIGN)"
            ],
            "source_orig": [
                "remove_comma = lambda x: int(x.replace(',', '')) if type(x) == np.str and x != np.nan else x \n",
                "zomato.votes = zomato.votes.astype('int')\n",
                "zomato['approx_cost_for_2_people'] = zomato['approx_cost_for_2_people'].apply(remove_comma)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato.info()"
            ],
            "source_orig": [
                "zomato.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato['rate'].unique()"
            ],
            "source_orig": [
                "zomato['rate'].unique()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = ASSIGN.loc[ASSIGN.rate !='NEW']",
                "ASSIGN = ASSIGN.loc[ASSIGN.rate !='-'].reset_index(drop=True)"
            ],
            "source_orig": [
                "zomato = zomato.loc[zomato.rate !='NEW']\n",
                "zomato = zomato.loc[zomato.rate !='-'].reset_index(drop=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = lambda x: x.replace('path', '') if type(x) == np.str else x",
                "zomato.rate = zomato.rate.apply(ASSIGN).str.strip().astype('float')"
            ],
            "source_orig": [
                "remove_slash = lambda x: x.replace('/5', '') if type(x) == np.str else x\n",
                "zomato.rate = zomato.rate.apply(remove_slash).str.strip().astype('float')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato['rate'].head()"
            ],
            "source_orig": [
                "zomato['rate'].head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato.info()"
            ],
            "source_orig": [
                "zomato.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "zomato_en['rate'] = zomato_en['rate'].fillna(zomato_en['rate'].mean())",
                "zomato_en['approx_cost_for_2_people'] = zomato_en['approx_cost_for_2_people'].fillna(zomato_en['approx_cost_for_2_people'].mean())"
            ],
            "source_orig": [
                "zomato_en['rate'] = zomato_en['rate'].fillna(zomato_en['rate'].mean())\n",
                "zomato_en['approx_cost_for_2_people'] = zomato_en['approx_cost_for_2_people'].fillna(zomato_en['approx_cost_for_2_people'].mean())"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato_en.isna().sum()"
            ],
            "source_orig": [
                "zomato_en.isna().sum()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "transfer_results"
            ],
            "source": [
                "plt.figure(figsize=(15,8))",
                "sns.heatmap(corr, annot=True)",
                "plt.savefig(\"image0.png\")"
            ],
            "source_orig": [
                "plt.figure(figsize=(15,8))\n",
                "sns.heatmap(corr, annot=True)\n",
                "plt.savefig(\"image0.png\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "CHECKPOINT",
                "zomato_en.columns"
            ],
            "source_orig": [
                "zomato_en.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN = zomato_en.iloc[:,[2,3,5,6,7,8,9,11]]",
                "ASSIGN = zomato_en['rate']"
            ],
            "source_orig": [
                "x = zomato_en.iloc[:,[2,3,5,6,7,8,9,11]]\n",
                "y = zomato_en['rate']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=train_test_split(x,y,test_size=.1,random_state=353)"
            ],
            "source_orig": [
                "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "x_train.head()"
            ],
            "source_orig": [
                "x_train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "y_train.head()"
            ],
            "source_orig": [
                "y_train.head()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ASSIGN=LinearRegression()",
                "ASSIGN.fit(x_train,y_train)"
            ],
            "source_orig": [
                "reg=LinearRegression()\n",
                "reg.fit(x_train,y_train)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "source": [
                "SETUP",
                "ASSIGN=reg.predict(x_test)",
                "r2_score(y_test,ASSIGN)"
            ],
            "source_orig": [
                "y_pred=reg.predict(x_test)\n",
                "from sklearn.metrics import r2_score\n",
                "r2_score(y_test,y_pred)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "SETUP",
                "'''reg_score=[]",
                "for j in range(1000):",
                "ASSIGN =train_test_split(x,y,random_state=j,test_size=0.1)",
                "ASSIGN=LinearRegression().fit(x_train,y_train)",
                "reg_score.append(ASSIGN.score(x_test,y_test))",
                "K=reg_score.index(np.max(reg_score))"
            ],
            "source_orig": [
                "'''reg_score=[]\n",
                "import numpy as np\n",
                "for j in range(1000):\n",
                "    x_train,x_test,y_train,y_test =train_test_split(x,y,random_state=j,test_size=0.1)\n",
                "    lr=LinearRegression().fit(x_train,y_train)\n",
                "    reg_score.append(lr.score(x_test,y_test))\n",
                "K=reg_score.index(np.max(reg_score))\n",
                "#K=353'''"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from sklearn.tree import DecisionTreeRegressor"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ASSIGN=train_test_split(x,y,test_size=.1,random_state=105)"
            ],
            "source_orig": [
                "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=105)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "DTree.fit(x_train,y_train)",
                "ASSIGN=DTree.predict(x_test)"
            ],
            "source_orig": [
                "DTree.fit(x_train,y_train)\n",
                "y_predict=DTree.predict(x_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "SETUP"
            ],
            "source_orig": [
                "from sklearn.metrics import r2_score"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "r2_score(y_test,y_predict)"
            ],
            "source_orig": [
                "r2_score(y_test,y_predict)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "CHECKPOINT",
                "'''from sklearn.tree import DecisionTreeRegressor",
                "ASSIGN=[]",
                "for j in range(1000):",
                "ASSIGN=train_test_split(x,y,test_size=.1,random_state=j)",
                "ASSIGN=DecisionTreeRegressor().fit(x_train,y_train)",
                "ASSIGN.append(ASSIGN.score(x_test,y_test))",
                "J= ASSIGN.index(np.max(ASSIGN))",
                "J"
            ],
            "source_orig": [
                "'''from sklearn.tree import DecisionTreeRegressor\n",
                "ts_score=[]\n",
                "for j in range(1000):\n",
                "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=j)\n",
                "    dc=DecisionTreeRegressor().fit(x_train,y_train)\n",
                "    ts_score.append(dc.score(x_test,y_test))\n",
                "J= ts_score.index(np.max(ts_score))\n",
                "\n",
                "J\n",
                "#J=105'''"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "SETUP",
                "ASSIGN=RandomForestRegressor(n_estimators=5,random_state=329,min_samples_leaf=.0001)"
            ],
            "source_orig": [
                "from sklearn.ensemble import RandomForestRegressor\n",
                "RForest=RandomForestRegressor(n_estimators=5,random_state=329,min_samples_leaf=.0001)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "RForest.fit(x_train,y_train)",
                "ASSIGN=RForest.predict(x_test)"
            ],
            "source_orig": [
                "RForest.fit(x_train,y_train)\n",
                "y_predict=RForest.predict(x_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "source": [
                "SETUP",
                "r2_score(y_test,y_predict)"
            ],
            "source_orig": [
                "from sklearn.metrics import r2_score\n",
                "r2_score(y_test,y_predict)"
            ]
        }
    ]
}