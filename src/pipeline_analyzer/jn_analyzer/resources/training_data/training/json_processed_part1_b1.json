{
    "content": [
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import os\n",
                "print(os.listdir('../input/sasuke'))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "visualize_data"
            ],
            "source": [
                "img = Image.open('../input/sasuke/.jpg')\n",
                "img = img.resize((224,224))\n",
                "plt.imshow(img)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "check_results"
            ],
            "source": [
                "import numpy as np\n",
                "test_x = np.array(img) / 255.0\n",
                "print(test_x.shape)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "test_x = test_x.reshape(1,224,224,3)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "# Import Model\n",
                "#from tensorflow.keras.applications import VGG16\n",
                "#from tensorflow.keras.applications import ResNet101V2\n",
                "from tensorflow.keras.applications import InceptionV3\n",
                "\n",
                "#from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
                "#from tensorflow.keras.applications.resnet import preprocess_input, decode_predictions\n",
                "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
                "\n",
                "# Load Model\n",
                "#model = VGG16(weights='imagenet')\n",
                "#model = ResNet101V2(weights='imagenet')\n",
                "model = InceptionV3(weights='imagenet')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "# model prediction\n",
                "preds = model.predict(test_x)\n",
                "# decode prediction\n",
                "dec_preds =  decode_predictions(preds, top=3)[0]\n",
                "print('Predicted:', dec_preds)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import os\n",
                "print(os.listdir('../input/naruto'))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "visualize_data",
                "process_data"
            ],
            "source": [
                "img = Image.open('../input/naruto/naruto.jpg')\n",
                "img = img.resize((224,224))\n",
                "plt.imshow(img)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "check_results"
            ],
            "source": [
                "import numpy as np\n",
                "test_x = np.array(img) / 255.0\n",
                "print(test_x.shape)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "test_x = test_x.reshape(1,224,224,3)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "# Import Model\n",
                "#from tensorflow.keras.applications import VGG16\n",
                "#from tensorflow.keras.applications import ResNet101V2\n",
                "from tensorflow.keras.applications import InceptionV3\n",
                "\n",
                "#from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
                "#from tensorflow.keras.applications.resnet import preprocess_input, decode_predictions\n",
                "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
                "\n",
                "# Load Model\n",
                "#model = VGG16(weights='imagenet')\n",
                "#model = ResNet101V2(weights='imagenet')\n",
                "model = InceptionV3(weights='imagenet')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "# model prediction\n",
                "preds = model.predict(test_x)\n",
                "# decode prediction\n",
                "dec_preds =  decode_predictions(preds, top=3)[0]\n",
                "print('Predicted:', dec_preds)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import geopandas as gpd\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import seaborn as sns\n",
                "import folium\n",
                "from folium import plugins"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/new-york-city-airbnb-open-data/')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data=pd.read_csv('../input/new-york-city-airbnb-open-data/AB_NYC_2019.csv')\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data=data.dropna(subset=['name'])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.neighbourhood_group.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(20,6))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "data.neighbourhood_group.value_counts().plot.bar(color=clr,ax=ax[0])\n",
                "ax[0].set_title('The number of rooms in each neighbourhood_group',size=20)\n",
                "ax[0].set_ylabel('rooms',size=18)\n",
                "ax[0].tick_params(axis='x', rotation=360)\n",
                "ax[0].tick_params(labelsize=18)\n",
                "\n",
                "data.groupby(['neighbourhood_group','room_type'])['id'].agg('count').unstack('room_type').plot.bar(ax=ax[1])\n",
                "ax[1].tick_params(axis='x', rotation=360)\n",
                "ax[1].set_title('The number of rooms in each room_type',size=20)\n",
                "ax[1].set_ylabel('rooms',size=18)\n",
                "ax[1].set_xlabel('')\n",
                "ax[1].tick_params(labelsize=18)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.room_type.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(3,1,figsize=(15,36))\n",
                "sns.boxplot(x=\"neighbourhood_group\", y=\"price\", data=data[data.room_type=='Shared room'],ax=ax[0])\n",
                "ax[0].set_title(\"Boxplot of Price for 'Shared room' in each neighbourhood_group\",size=20)\n",
                "\n",
                "sns.boxplot(x=\"neighbourhood_group\", y=\"price\", data=data[data.room_type=='Private room'],ax=ax[1])\n",
                "ax[1].set_title(\"Boxplot of Price for 'Private room' in each neighbourhood_group\",size=20)\n",
                "\n",
                "sns.boxplot(x=\"neighbourhood_group\", y=\"price\", data=data[data.room_type=='Entire home/apt'],ax=ax[2])\n",
                "ax[2].set_title(\"Boxplot of Price for 'Entire home/apt' in each neighbourhood_group\",size=20)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_manha=data[data.neighbourhood_group=='Manhattan']\n",
                "data_manha.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_manha.neighbourhood.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "data_manha.neighbourhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 neighbourhood by the number of rooms\",size=20)\n",
                "ax[0].set_xlabel('rooms',size=18)\n",
                "\n",
                "\n",
                "count=data_manha['neighbourhood'].value_counts()\n",
                "groups=list(data_manha['neighbourhood'].value_counts().index)[:10]\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.15,0.4)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data_manha_65=data_manha[data_manha.price<65]\n",
                "data_manha_65['label']=data_manha_65.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n",
                "data_manha_65.head()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "Long=-73.92\n",
                "Lat=40.86\n",
                "manha_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "manha_rooms_map=plugins.MarkerCluster().add_to(manha_map)\n",
                "for lat,lon,label in zip(data_manha_65.latitude,data_manha_65.longitude,data_manha_65.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(manha_rooms_map)\n",
                "manha_map.add_child(manha_rooms_map)\n",
                "\n",
                "manha_map"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "data_manha_65_80=data_manha.loc[(data_manha['price'] >=65) & (data_manha['price'] <80)]\n",
                "data_manha_65_80['label']=data_manha_65_80.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n",
                "Long=-73.92\n",
                "Lat=40.86\n",
                "manha_65_80_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "manha_65_80_rooms_map=plugins.MarkerCluster().add_to(manha_65_80_map)\n",
                "for lat,lon,label in zip(data_manha_65_80.latitude,data_manha_65_80.longitude,data_manha_65_80.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(manha_65_80_rooms_map)\n",
                "manha_65_80_map.add_child(manha_65_80_rooms_map)\n",
                "\n",
                "manha_65_80_map"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_Brooklyn=data[data.neighbourhood_group=='Brooklyn']\n",
                "data_Brooklyn.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_Brooklyn.neighbourhood.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "data_Brooklyn.neighbourhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 neighbourhood by the number of rooms\",size=20)\n",
                "ax[0].set_xlabel('rooms',size=18)\n",
                "\n",
                "\n",
                "count=data_Brooklyn['neighbourhood'].value_counts()\n",
                "groups=list(data_Brooklyn['neighbourhood'].value_counts().index)[:10]\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.15,0.4)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data_Brooklyn_10_65=data_Brooklyn.loc[(data_Brooklyn['price'] >=10) & (data_Brooklyn['price'] <65)][:2000]\n",
                "data_Brooklyn_10_65['label']=data_Brooklyn_10_65.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n",
                "data_Brooklyn_10_65.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "Long=-73.94\n",
                "Lat=40.72\n",
                "Brooklyn_10_65_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "Brooklyn_10_65_rooms_map=plugins.MarkerCluster().add_to(Brooklyn_10_65_map)\n",
                "for lat,lon,label in zip(data_Brooklyn_10_65.latitude,data_Brooklyn_10_65.longitude,data_Brooklyn_10_65.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(Brooklyn_10_65_rooms_map)\n",
                "Brooklyn_10_65_map.add_child(Brooklyn_10_65_rooms_map)\n",
                "\n",
                "Brooklyn_10_65_map"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_Queens=data[data.neighbourhood_group=='Queens']\n",
                "data_Queens.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_Queens.neighbourhood.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "data_Queens.neighbourhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 neighbourhood by the number of rooms\",size=20)\n",
                "ax[0].set_xlabel('rooms',size=18)\n",
                "\n",
                "\n",
                "count=data_Queens['neighbourhood'].value_counts()\n",
                "groups=list(data_Queens['neighbourhood'].value_counts().index)[:10]\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.15,0.4)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data_Queens_100_1=data_Queens.loc[(data_Queens['price'] <100)][:2000]\n",
                "data_Queens_100_1['label']=data_Queens_100_1.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n",
                "data_Queens_100_1.head()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "Long=-73.80\n",
                "Lat=40.70\n",
                "data_Queens_100_1_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "data_Queens_100_1_rooms_map=plugins.MarkerCluster().add_to(data_Queens_100_1_map)\n",
                "for lat,lon,label in zip(data_Queens_100_1.latitude,data_Queens_100_1.longitude,data_Queens_100_1.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_Queens_100_1_rooms_map)\n",
                "data_Queens_100_1_map.add_child(data_Queens_100_1_rooms_map)\n",
                "\n",
                "data_Queens_100_1_map"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "data_Queens_100_2=data_Queens.loc[(data_Queens['price'] <100)][2000:2800]\n",
                "data_Queens_100_2['label']=data_Queens_100_2.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n",
                "Long=-73.80\n",
                "Lat=40.70\n",
                "data_Queens_100_2_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "data_Queens_100_2_rooms_map=plugins.MarkerCluster().add_to(data_Queens_100_2_map)\n",
                "for lat,lon,label in zip(data_Queens_100_2.latitude,data_Queens_100_2.longitude,data_Queens_100_2.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_Queens_100_2_rooms_map)\n",
                "data_Queens_100_2_map.add_child(data_Queens_100_2_rooms_map)\n",
                "\n",
                "data_Queens_100_2_map"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "data_Queens_100_3=data_Queens.loc[(data_Queens['price'] <100)][2801:-1]\n",
                "data_Queens_100_3['label']=data_Queens_100_3.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n",
                "Long=-73.80\n",
                "Lat=40.70\n",
                "data_Queens_100_3_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "data_Queens_100_3_rooms_map=plugins.MarkerCluster().add_to(data_Queens_100_3_map)\n",
                "for lat,lon,label in zip(data_Queens_100_3.latitude,data_Queens_100_3.longitude,data_Queens_100_3.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_Queens_100_3_rooms_map)\n",
                "data_Queens_100_3_map.add_child(data_Queens_100_3_rooms_map)\n",
                "\n",
                "data_Queens_100_3_map"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(18,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "data.groupby(['host_name'])['number_of_reviews'].agg('sum').sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 host by the number of reviews\",size=20)\n",
                "ax[0].set_xlabel('reviews',size=18)\n",
                "ax[0].set_ylabel('')\n",
                "\n",
                "data.groupby(['host_name'])['price'].agg('mean').sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[1])\n",
                "ax[1].set_title(\"Top 10 host by the average of price for rooms\",size=20)\n",
                "ax[1].set_xlabel('average of price',size=18)\n",
                "ax[1].set_ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,15))\n",
                "corr = data.corr()\n",
                "sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns)\n",
                "plt.title(\"correlation plot\",size=28)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "data_tree=data[['neighbourhood_group','neighbourhood','room_type','minimum_nights','number_of_reviews','price']]\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "labelencoder = LabelEncoder()\n",
                "data_tree['neighbourhood_group_new'] = labelencoder.fit_transform(data_tree['neighbourhood_group'])\n",
                "data_tree['neighbourhood_new'] = labelencoder.fit_transform(data_tree['neighbourhood'])\n",
                "data_tree['room_type_new'] = labelencoder.fit_transform(data_tree['room_type'])\n",
                "data_tree.head()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_tree=data_tree[data_tree.price<=180]\n",
                "data_tree=data_tree[data_tree.price>=90]\n",
                "len(data_tree)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "x_train,x_test,y_train,y_test=train_test_split(data_tree[['neighbourhood_group_new','neighbourhood_new','room_type_new','minimum_nights','number_of_reviews']],data_tree[['price']],test_size=0.1,random_state=0)\n",
                "Reg_tree=DecisionTreeRegressor(criterion='mse',max_depth=3,random_state=0)\n",
                "Reg_tree=Reg_tree.fit(x_train,y_train)\n",
                "y=y_test['price']\n",
                "predict=Reg_tree.predict(x_test)\n",
                "print(\"median absolute deviation (MAD): \",np.mean(abs(np.multiply(np.array(y_test.T-predict),np.array(1/y_test)))))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "transfer_results",
                "ingest_data"
            ],
            "source": [
                "from subprocess import check_call\n",
                "from PIL import Image, ImageDraw, ImageFont\n",
                "from IPython.display import Image as PImage\n",
                "from sklearn.tree import export_graphviz\n",
                "with open(\"tree1.dot\", 'w') as f:\n",
                "     f = export_graphviz(Reg_tree,\n",
                "                              out_file=f,\n",
                "                              max_depth = 3,\n",
                "                              impurity = True,\n",
                "                              feature_names = ['neighbourhood_group_new','neighbourhood_new','room_type_new','minimum_nights','number_of_reviews'],\n",
                "                              rounded = True,\n",
                "                              filled= True )\n",
                "check_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n",
                "img = Image.open(\"tree1.png\")\n",
                "draw = ImageDraw.Draw(img)\n",
                "img.save('sample-out.png')\n",
                "PImage(\"sample-out.png\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os \n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "import folium\n",
                "import datetime\n",
                "import plotly.graph_objs as go\n",
                "import matplotlib.ticker as ticker\n",
                "import matplotlib.animation as animation\n",
                "from IPython.display import HTML"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/novel-corona-virus-2019-dataset')"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data=pd.read_csv('../input/novel-corona-virus-2019-dataset/covid_19_data.csv')\n",
                "data['date']=data.ObservationDate.apply(lambda x:datetime.datetime.strptime(str(x),'%m/%d/%Y').strftime('%Y-%m-%d'))\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data2=pd.read_csv('../input/novel-corona-virus-2019-dataset/COVID19_open_line_list.csv')\n",
                "data2.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "source": [
                "data2=data2.dropna(subset=['wuhan(0)_not_wuhan(1)','latitude'])\n",
                "data2=data2.fillna(value = {'sex' : 'Unknown', 'age' : 'Unknown'})\n",
                "data2=data2.drop(index=data2[data2.sex=='4000'].index)\n",
                "data2=data2.replace(to_replace = 'Female', value ='female')\n",
                "data2=data2.replace(to_replace = 'Male', value ='male')\n",
                "data2['label']=data2.apply(lambda x: ('age:'+str(x['age']),'sex:'+str(x['sex']),'geo_resolution:'+str(x['geo_resolution']),'Confirmed_date:'+str(x['date_confirmation'])),axis=1)\n",
                "plt.figure(figsize=(10,10))\n",
                "sns.barplot(x=data2.isnull().sum().sort_values(ascending=False),y=data2.isnull().sum().sort_values(ascending=False).index)\n",
                "plt.title(\"counts of missing value for COVID19_open_line_list\",size=20)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('The data size is',len(data))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "formated_gdf = data.groupby(['date','Country/Region']).agg('sum')\n",
                "formated_gdf = formated_gdf.reset_index()\n",
                "formated_gdf=formated_gdf.drop(columns='SNo')\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "source": [
                "Confirmed_last=data[data['date']==max(data['date'])].groupby(['Country/Region']).agg('sum').sort_values('Confirmed',ascending=False)[:10]\n",
                "Confirmed_last=Confirmed_last.reset_index()\n",
                "Confirmed_last=Confirmed_last.drop(columns='SNo')\n",
                "Confirmed_last['Recovered rate']=Confirmed_last['Recovered']/Confirmed_last['Confirmed']\n",
                "Confirmed_last.style.background_gradient(cmap='Blues')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = go.Figure(data=[\n",
                "    go.Line(name='Confirmed', x=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['Confirmed']),\n",
                "    go.Line(name='Deaths', x=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['Deaths']),\n",
                "    go.Line(name='Recovered', x=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['Recovered']),\n",
                "])\n",
                "\n",
                "fig.update_layout(\n",
                "    title=\"Number of Confirmed,Recovered,death in china for each day\",\n",
                "    xaxis_title=\"date\",\n",
                "    yaxis_title=\"People\",\n",
                "    font=dict(\n",
                "        family=\"Courier New, monospace\",\n",
                "        size=18,\n",
                "        color=\"#7f7f7f\"\n",
                "    ))\n",
                "fig"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = go.Figure(data=[\n",
                "    go.Line(name='Confirmed', x=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['Confirmed']),\n",
                "    go.Line(name='Deaths', x=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['Deaths']),\n",
                "    go.Line(name='Recovered', x=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['Recovered']),\n",
                "])\n",
                "\n",
                "fig.update_layout(\n",
                "    title=\"Number of Confirmed,Recovered,death in Taiwan for each day\",\n",
                "    xaxis_title=\"date\",\n",
                "    yaxis_title=\"People\",\n",
                "    font=dict(\n",
                "        family=\"Courier New, monospace\",\n",
                "        size=18,\n",
                "        color=\"#7f7f7f\"\n",
                "    ))\n",
                "fig"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "formated_gdf['size'] = formated_gdf['Confirmed'].pow(0.3)\n",
                "\n",
                "fig = px.scatter_geo(formated_gdf, locations=\"Country/Region\", locationmode='country names', \n",
                "                     color=\"Confirmed\", size='size', hover_name=\"Country/Region\", \n",
                "                     range_color= [0, max(formated_gdf['Confirmed'])+2], \n",
                "                     projection=\"natural earth\", animation_frame=\"date\", \n",
                "                     title='Confirmed for each day')\n",
                "fig.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "formated_gdf['size'] = formated_gdf['Deaths'].pow(0.3)\n",
                "\n",
                "fig = px.scatter_geo(formated_gdf, locations=\"Country/Region\", locationmode='country names', \n",
                "                     color=\"Deaths\", size='size', hover_name=\"Country/Region\", \n",
                "                     range_color= [0, max(formated_gdf['Deaths'])+2], \n",
                "                     projection=\"natural earth\", animation_frame=\"date\", \n",
                "                     title='Deaths for each day')\n",
                "fig.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "formated_gdf['size'] = formated_gdf['Recovered'].pow(0.3)\n",
                "\n",
                "fig = px.scatter_geo(formated_gdf, locations=\"Country/Region\", locationmode='country names', \n",
                "                     color=\"Recovered\", size='size', hover_name=\"Country/Region\", \n",
                "                     range_color= [0, max(formated_gdf['Recovered'])+2], \n",
                "                     projection=\"natural earth\", animation_frame=\"date\", \n",
                "                     title='Recovered for each day')\n",
                "fig.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "china=data[data['Country/Region']=='Mainland China']\n",
                "china= china.groupby(['date','Province/State'])['Confirmed', 'Deaths', 'Recovered'].max()\n",
                "china = china.reset_index()\n",
                "china.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "china['Province/State'].unique()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "china_table=china[china['date']==max(china['date'])].sort_values('Confirmed',ascending=False)[:10]\n",
                "china_table=china_table.drop(columns=['date'])\n",
                "china_table['Recovered rate']=china_table['Recovered']/china_table['Confirmed']\n",
                "china_table.style.background_gradient(cmap='Reds')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "colors = dict(zip(\n",
                "    ['Anhui', 'Beijing', 'Chongqing', 'Fujian', 'Gansu', 'Guangdong',\n",
                "       'Guangxi', 'Guizhou', 'Hainan', 'Hebei', 'Heilongjiang', 'Henan',\n",
                "       'Hubei', 'Hunan', 'Inner Mongolia', 'Jiangsu', 'Jiangxi', 'Jilin',\n",
                "       'Liaoning', 'Ningxia', 'Qinghai', 'Shaanxi', 'Shandong',\n",
                "       'Shanghai', 'Shanxi', 'Sichuan', 'Tianjin', 'Tibet', 'Xinjiang',\n",
                "       'Yunnan', 'Zhejiang'],\n",
                "    ['#800000', '#8B0000', '#A52A2A', '#B22222', '#DC143C', '#FF0000', '#FF6347','#FF7F50','#CD5C5C','#F08080',\n",
                "    '#E9967A','#FA8072','#FFA07A','#FF4500','#FF8C00','#FFA500','#FFD700','#B8860B','#DAA520','#EEE8AA',\n",
                "    '#BDB76B','#F0E68C','#808000','#FFFF00','#9ACD32','#556B2F','#6B8E23','#7CFC00','#7FFF00','#ADFF2F',\n",
                "    '#006400']\n",
                "))\n",
                "\n",
                "\n",
                "def race_barchart(date):\n",
                "    dff = china[china['date'].eq(date)].sort_values(by='Confirmed', ascending=True).tail(10)\n",
                "    ax.clear()\n",
                "    ax.barh(dff['Province/State'], dff['Confirmed'], color=[colors[x] for x in dff['Province/State']],height=0.8)\n",
                "    dx = dff['Confirmed'].max() / 200\n",
                "    \n",
                "    for i, (value, name) in enumerate(zip(dff['Confirmed'], dff['Province/State'])):\n",
                "        ax.text(0, i,name+' ',size=16, weight=600, ha='right', va='center') \n",
                "        ax.text(value+dx, i,f'{value:,.0f}',  size=16, ha='left',  va='center') \n",
                "            \n",
                "    ax.text(0.9, 0.2, date, transform=ax.transAxes, color='#777777', size=72, ha='right', weight=1000) \n",
                "    ax.text(0.59, 0.14, 'Total Confirmed:'+str(int(dff['Confirmed'].sum())), transform=ax.transAxes, size=24, color='#000000',ha='left')\n",
                "    ax.tick_params(axis='x', colors='#777777', labelsize=12) \n",
                "    ax.xaxis.set_ticks_position('top') \n",
                "    ax.set_yticks([])\n",
                "    ax.margins(0, 0.01)\n",
                "    ax.grid(which='major', axis='x', linestyle='-') \n",
                "    ax.text(0, 1.15, 'Confirmed for each date in China ',\n",
                "                transform=ax.transAxes, size=24, weight=600, ha='left', va='top') \n",
                "\n",
                "    plt.box(False)\n",
                "    \n",
                "\n",
                "day = list(set(china.date.values))\n",
                "day.sort()\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(16, 9))\n",
                "\n",
                "HTML(animation.FuncAnimation(fig, race_barchart, frames=day).to_jshtml())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = go.Figure(data=[\n",
                "    go.Line(name='Confirmed', x=china[china['Province/State']=='Hubei']['date'], y=china[china['Province/State']=='Hubei']['Confirmed']),\n",
                "    go.Line(name='Deaths', x=china[china['Province/State']=='Hubei']['date'], y=china[china['Province/State']=='Hubei']['Deaths']),\n",
                "    go.Line(name='Recovered', x=china[china['Province/State']=='Hubei']['date'], y=china[china['Province/State']=='Hubei']['Recovered']),\n",
                "])\n",
                "\n",
                "fig.update_layout(\n",
                "    title=\"Number of Confirmed,Recovered,death in Huibel for each day\",\n",
                "    xaxis_title=\"date\",\n",
                "    yaxis_title=\"People\",\n",
                "    font=dict(\n",
                "        family=\"Courier New, monospace\",\n",
                "        size=18,\n",
                "        color=\"#7f7f7f\"\n",
                "    ))\n",
                "fig"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "Italy=data[data['Country/Region']=='Italy']\n",
                "Italy= Italy.groupby(['date'])['Confirmed', 'Deaths', 'Recovered'].max()\n",
                "Italy = Italy.reset_index()\n",
                "Italy.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = go.Figure(data=[\n",
                "    go.Line(name='Confirmed', x=Italy['date'], y=Italy['Confirmed']),\n",
                "    go.Line(name='Deaths', x=Italy['date'], y=Italy['Deaths']),\n",
                "    go.Line(name='Recovered', x=Italy['date'], y=Italy['Recovered']),\n",
                "])\n",
                "\n",
                "fig.update_layout(\n",
                "    title=\"Number of Confirmed,Recovered,death in Italy for each day\",\n",
                "    xaxis_title=\"date\",\n",
                "    yaxis_title=\"People\",\n",
                "    font=dict(\n",
                "        family=\"Courier New, monospace\",\n",
                "        size=18,\n",
                "        color=\"#7f7f7f\"\n",
                "    ))\n",
                "fig"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(20,10))\n",
                "sns.barplot(x=data2.sex.value_counts().sort_values(ascending=False),y=data2.sex.value_counts().sort_values(ascending=False).index,ax=ax[0])\n",
                "ax[0].set_title(\"Number of patient by sex\",size=20)\n",
                "ax[0].set_xlabel('patient',size=18)\n",
                "sns.barplot(x=data2.country.value_counts().sort_values(ascending=False),y=data2.country.value_counts().sort_values(ascending=False).index,ax=ax[1])\n",
                "ax[1].set_title(\"Number of patient by country\",size=20)\n",
                "ax[1].set_xlabel('patient',size=18)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "transfer_results"
            ],
            "source": [
                "incidents=folium.map.FeatureGroup()\n",
                "\n",
                "Lat=data2.latitude.mean()\n",
                "Lon=data2.longitude.mean()\n",
                "from folium import plugins\n",
                "\n",
                "map1=folium.Map([Lat,Lon],zoom_start=3)\n",
                "\n",
                "COVID_map=plugins.MarkerCluster().add_to(map1)\n",
                "for lat,lon,label in zip(data2.latitude,data2.longitude,data2.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(COVID_map)\n",
                "map1.add_child(COVID_map)\n",
                "map1.save(\"COVID\"+\".html\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "HTML('<iframe src=COVID.html width=1000 height=450></iframe>')"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "source": [
                "US=data[data['Country/Region']=='US']\n",
                "US= US.groupby(['date','Province/State'])['Confirmed', 'Deaths', 'Recovered'].max()\n",
                "US =US.reset_index()\n",
                "\n",
                "US_table=US[US['date']==max(US['date'])].sort_values('Confirmed',ascending=False)[:10]\n",
                "US_table=US_table.drop(columns=['date'])\n",
                "US_table['Recovered rate']=US_table['Recovered']/US_table['Confirmed']\n",
                "US_table.style.background_gradient(cmap='Purples')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "import random\n",
                "random.seed( 199 )\n",
                "def randomcolor():\n",
                "    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']\n",
                "    color = \"\"\n",
                "    for i in range(6):\n",
                "        color += colorArr[random.randint(0,14)]\n",
                "    return \"#\"+color\n",
                "color_US=[]\n",
                "for i in range(len(US['Province/State'].unique())):\n",
                "    color_US.append(randomcolor())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "colors_US = dict(zip(\n",
                "    US['Province/State'].unique(),\n",
                "    color_US\n",
                "))\n",
                "\n",
                "\n",
                "def race_barchart_US(date):\n",
                "    dff = US[US['date'].eq(date)].sort_values(by='Confirmed', ascending=True).tail(10)\n",
                "    ax.clear()\n",
                "    ax.barh(dff['Province/State'], dff['Confirmed'], color=[colors_US[x] for x in dff['Province/State']],height=0.8)\n",
                "    dx = dff['Confirmed'].max() / 200\n",
                "    \n",
                "    for i, (value, name) in enumerate(zip(dff['Confirmed'], dff['Province/State'])):\n",
                "        ax.text(0, i,name+' ',size=16, weight=600, ha='right', va='center') \n",
                "        ax.text(value+dx, i,f'{value:,.0f}',  size=16, ha='left',  va='center') \n",
                "            \n",
                "    ax.text(0.9, 0.2, date, transform=ax.transAxes, color='#777777', size=72, ha='right', weight=1000) \n",
                "    ax.text(0.59, 0.14, 'Total Confirmed:'+str(int(dff['Confirmed'].sum())), transform=ax.transAxes, size=24, color='#000000',ha='left')\n",
                "    ax.tick_params(axis='x', colors='#777777', labelsize=12) \n",
                "    ax.xaxis.set_ticks_position('top') \n",
                "    ax.set_yticks([])\n",
                "    ax.margins(0, 0.01)\n",
                "    ax.grid(which='major', axis='x', linestyle='-') \n",
                "    ax.text(0, 1.15, 'Confirmed for each date in US ',\n",
                "                transform=ax.transAxes, size=24, weight=600, ha='left', va='top') \n",
                "\n",
                "    plt.box(False)\n",
                "    \n",
                "\n",
                "day = list(set(US.date.values))\n",
                "day.sort()\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(16, 9))\n",
                "\n",
                "HTML(animation.FuncAnimation(fig, race_barchart_US, frames=day,interval=400).to_jshtml())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = go.Figure(data=[\n",
                "    go.Line(name='Confirmed', x=formated_gdf[formated_gdf['Country/Region']=='US']['date'], y=formated_gdf[formated_gdf['Country/Region']=='US']['Confirmed']),\n",
                "    go.Line(name='Deaths', x=formated_gdf[formated_gdf['Country/Region']=='US']['date'], y=formated_gdf[formated_gdf['Country/Region']=='US']['Deaths']),\n",
                "    go.Line(name='Recovered', x=formated_gdf[formated_gdf['Country/Region']=='US']['date'], y=formated_gdf[formated_gdf['Country/Region']=='US']['Recovered']),\n",
                "])\n",
                "\n",
                "fig.update_layout(\n",
                "    title=\"Number of Confirmed,Recovered,death in US for each day\",\n",
                "    xaxis_title=\"date\",\n",
                "    yaxis_title=\"People\",\n",
                "    font=dict(\n",
                "        family=\"Courier New, monospace\",\n",
                "        size=18,\n",
                "        color=\"#7f7f7f\"\n",
                "    ))\n",
                "fig"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os \n",
                "import seaborn as sns\n",
                "import geopandas as gpd\n",
                "import folium\n",
                "from folium import plugins\n",
                "import datetime\n",
                "import math"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/denver-crime-data')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data=pd.read_csv('../input/denver-crime-data/crime.csv')\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "y=data.isnull().sum().sort_values(ascending=False)[:6].index\n",
                "x=data.isnull().sum().sort_values(ascending=False)[:6]\n",
                "plt.figure(figsize=(8,8))\n",
                "sns.barplot(x,y)\n",
                "plt.title(\"counts of missing value\",size=20)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data=data.dropna(subset=['GEO_LAT','GEO_LON'])\n",
                "data.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data['REPORTED_DATE']=data.REPORTED_DATE.apply(lambda x:datetime.datetime.strptime(x,'%m/%d/%Y %I:%M:%S %p'))\n",
                "data['year']=data.REPORTED_DATE.apply(lambda x:x.strftime('%Y'))\n",
                "data['month']=data.REPORTED_DATE.apply(lambda x:x.strftime('%m'))\n",
                "data['hour']=data.REPORTED_DATE.apply(lambda x:x.strftime('%H'))\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data=data[data.GEO_LAT>39]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "Top10_crime_type=data[data['OFFENSE_CATEGORY_ID'].isin(list(data.OFFENSE_CATEGORY_ID.value_counts()[:10].index[:10]))]\n",
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "y=Top10_crime_type.OFFENSE_CATEGORY_ID.value_counts().index\n",
                "x=Top10_crime_type.OFFENSE_CATEGORY_ID.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"Top 10 crime type by counts\",size=20)\n",
                "ax[0,0].set_xlabel('counts',size=18)\n",
                "ax[0,0].set_ylabel('')\n",
                "\n",
                "\n",
                "Top10_crime_type.groupby(['year','OFFENSE_CATEGORY_ID'])['INCIDENT_ID'].agg('count').unstack('OFFENSE_CATEGORY_ID').plot(ax=ax[0,1])\n",
                "ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "ax[0,1].set_title(\"Top 10 crime type counts by year\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "ax[0,1].set_xlabel('year',size=18)\n",
                "\n",
                "Top10_crime_type.groupby(['month','OFFENSE_CATEGORY_ID'])['INCIDENT_ID'].agg('count').unstack('OFFENSE_CATEGORY_ID').plot(ax=ax[1,0])\n",
                "ax[1,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(-0.25,1))\n",
                "ax[1,0].set_title(\"Top 10 crime type counts by month\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "ax[1,0].set_xlabel('month',size=18)\n",
                "\n",
                "sns.scatterplot(x=\"GEO_LON\", y=\"GEO_LAT\", hue=\"OFFENSE_CATEGORY_ID\",data=Top10_crime_type,ax=ax[1,1])\n",
                "ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "ax[1,1].set_title(\"The distribution of Top 10 crime type\",size=20)\n",
                "ax[1,1].set(xlabel='Longitude', ylabel='LATITUDE')\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_2000=data[:2000]\n",
                "Long=data_2000.GEO_LON.mean()\n",
                "Lat=data_2000.GEO_LAT.mean()\n",
                "data_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "data_crime_map=plugins.MarkerCluster().add_to(data_map)\n",
                "for lat,lon,label in zip(data_2000.GEO_LAT,data_2000.GEO_LON,data_2000.OFFENSE_CATEGORY_ID):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_crime_map)\n",
                "data_map.add_child(data_crime_map)\n",
                "\n",
                "data_map"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "y=data.year.value_counts()\n",
                "x=data.year.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"The number of crimes by year\",size=20)\n",
                "ax[0,0].set_ylabel('counts',size=18)\n",
                "ax[0,0].set_xlabel('')\n",
                "\n",
                "\n",
                "y=data.month.value_counts()\n",
                "x=data.month.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"The number of crimes by month\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "ax[0,1].set_xlabel('')\n",
                "\n",
                "y=data.hour.value_counts()\n",
                "x=data.hour.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"The number of crimes by hour\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "ax[1,0].set_xlabel('')\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "y=data.NEIGHBORHOOD_ID.value_counts()[:10].index\n",
                "x=data.NEIGHBORHOOD_ID.value_counts()[:10]\n",
                "sns.barplot(x=x,y=y,ax=ax[1,1])\n",
                "ax[1,1].set_title(\"The 10 NEIGHBORHOOD_ID by the number of crimes\",size=20)\n",
                "ax[1,1].set_xlabel('counts',size=18)\n",
                "ax[1,1].set_ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "map_all=folium.Map([39.7,-105],zoom_start=12)\n",
                "crime_new=pd.DataFrame({\"Lat\":data['GEO_LAT'],\"Long\":data['GEO_LON']})\n",
                "crime_new=crime_new[:20000]\n",
                "map_all.add_child(plugins.HeatMap(data=crime_new))\n",
                "map_all"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_traf=data[data.OFFENSE_CATEGORY_ID=='traffic-accident']\n",
                "data_traf.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "y=data_traf.year.value_counts()\n",
                "x=data_traf.year.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"The number of traffic-accident by year\",size=20)\n",
                "ax[0,0].set_ylabel('counts',size=18)\n",
                "ax[0,0].set_xlabel('')\n",
                "\n",
                "\n",
                "y=data_traf.month.value_counts()\n",
                "x=data_traf.month.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"The number of traffic-accident by month\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "ax[0,1].set_xlabel('')\n",
                "\n",
                "y=data_traf.hour.value_counts()\n",
                "x=data_traf.hour.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"The number of traffic-accident by hour\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "ax[1,0].set_xlabel('')\n",
                "\n",
                "\n",
                "sns.scatterplot(x=\"GEO_LON\", y=\"GEO_LAT\", hue=\"NEIGHBORHOOD_ID\",data=data_traf,ax=ax[1,1])\n",
                "ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,2.5))\n",
                "ax[1,1].set_title(\"The distribution of traffic-accident\",size=20)\n",
                "ax[1,1].set(xlabel='Longitude', ylabel='LATITUDE')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_IS_TRAFFIC=data[data.IS_TRAFFIC==1]\n",
                "data_IS_TRAFFIC.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_IS_TRAFFIC=data_IS_TRAFFIC[:2000]\n",
                "colors = {'2014' : 'red', '2015' : 'blue','2016' :'green','2017':'brown','2018':'plum','2019':'purple'}\n",
                "Long=data_IS_TRAFFIC.GEO_LON.mean()\n",
                "Lat=data_IS_TRAFFIC.GEO_LAT.mean()\n",
                "data_IS_TRAFFIC_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "for i in range(len(data_IS_TRAFFIC.groupby(['GEO_LAT','GEO_LON'])['INCIDENT_ID'].agg('count').index)):\n",
                "    lat,lon=data_IS_TRAFFIC.groupby(['GEO_LAT','GEO_LON'])['INCIDENT_ID'].agg('count').index[i]\n",
                "    folium.Circle(location=[lat,lon],\n",
                "    popup=data_IS_TRAFFIC.iloc[i]['OFFENSE_TYPE_ID'],\n",
                "    radius=int(data_IS_TRAFFIC.groupby(['GEO_LAT','GEO_LON'])['INCIDENT_ID'].agg('count')[i])*70,\n",
                "    fill=True,\n",
                "    fill_color=colors[data_IS_TRAFFIC['year'].iloc[i]],\n",
                "    fill_opacity=0.7,).add_to(data_IS_TRAFFIC_map)\n",
                "\n",
                "data_IS_TRAFFIC_map"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_IS_TRAFFIC=data[data.IS_TRAFFIC==1]\n",
                "plt.figure(figsize=(20,20))\n",
                "for i in range(6):\n",
                "    traffic=data_IS_TRAFFIC[data_IS_TRAFFIC.year==str(2014+i)]\n",
                "    plt.subplot(3,2,i+1)\n",
                "    plt.scatter('GEO_LON', 'GEO_LAT', data=traffic, c=colors[traffic['year'].iloc[0]])\n",
                "    plt.title(\"The distribution of traffic-accident in \"+str(2014+i),size=20)\n",
                "    plt.xlabel('Longitude')\n",
                "    plt.ylabel('LATITUDE')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_larceny=data[data.OFFENSE_CATEGORY_ID=='larceny']\n",
                "data_larceny.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "y=data_larceny.year.value_counts()\n",
                "x=data_larceny.year.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"The number of larceny by year\",size=20)\n",
                "ax[0,0].set_ylabel('counts',size=18)\n",
                "ax[0,0].set_xlabel('')\n",
                "\n",
                "\n",
                "y=data_larceny.month.value_counts()\n",
                "x=data_larceny.month.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"The number of larceny by month\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "ax[0,1].set_xlabel('')\n",
                "\n",
                "y=data_larceny.hour.value_counts()\n",
                "x=data_larceny.hour.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"The number of larceny by hour\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "ax[1,0].set_xlabel('')\n",
                "\n",
                "\n",
                "sns.scatterplot(x=\"GEO_LON\", y=\"GEO_LAT\", hue=\"NEIGHBORHOOD_ID\",data=data_larceny,ax=ax[1,1])\n",
                "ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,2.5))\n",
                "ax[1,1].set_title(\"The distribution of larency\",size=20)\n",
                "ax[1,1].set(xlabel='Longitude', ylabel='LATITUDE')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def rank_NEIGHBORHOOD(NEIGHBORHOOD_ID):\n",
                "    year=['2014','2015','2016','2017','2018','2019']\n",
                "    B={}\n",
                "    for i in range(len(year)):\n",
                "        A=data[data.year==year[i]]\n",
                "        value=A.groupby(['NEIGHBORHOOD_ID'])['OFFENSE_ID'].agg('count')\n",
                "        rank=A.groupby(['NEIGHBORHOOD_ID'])['OFFENSE_ID'].agg('count').rank(method='min',ascending=False)\n",
                "        new=pd.DataFrame({'rank':rank,'value':value})\n",
                "        B['rank '+year[i]]=str(new[new.index==NEIGHBORHOOD_ID].iloc[0,0])+\"/\"+str(max(rank))\n",
                "        B['value '+year[i]]=str(new[new.index==NEIGHBORHOOD_ID].iloc[0,1])\n",
                "\n",
                "    return B"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def rank_plot(NEIGHBORHOOD_ID):\n",
                "    ID=rank_NEIGHBORHOOD(NEIGHBORHOOD_ID)\n",
                "    y=[]\n",
                "    x=[]\n",
                "    n=[]\n",
                "    for i in range(6):\n",
                "        r1,r2=ID['rank '+str(i+2014)].split('/')\n",
                "        R=float(r1)/float(r2)\n",
                "        R=1-R\n",
                "        y.append(1.5+R*math.sin(0+i*2*math.pi/6))\n",
                "        x.append(1.5+R*math.cos(0+i*2*math.pi/6))\n",
                "        n.append('rank '+str(i+2014)+' '+ID['rank '+str(i+2014)])\n",
                "    \n",
                "    x.append(x[0])\n",
                "    y.append(y[0])\n",
                "    plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "    for i, txt in enumerate(n):\n",
                "        plt.annotate(txt, (x[i], y[i]))\n",
                "        plt.xlim(0.45,2.7)\n",
                "        plt.ylim(0.45,2.7)\n",
                "        plt.fill(x, y,\"plum\")\n",
                "        plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "        plt.title(\"The rank of the number of crime by year in \"+NEIGHBORHOOD_ID,size=18) "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(20,30))\n",
                "plt.subplot(3,2,1)\n",
                "rank_plot('five-points')\n",
                "plt.subplot(3,2,2)\n",
                "rank_plot('stapleton')\n",
                "plt.subplot(3,2,3)\n",
                "rank_plot('cbd')\n",
                "plt.subplot(3,2,4)\n",
                "rank_plot('capitol-hill')\n",
                "plt.subplot(3,2,5)\n",
                "rank_plot('virginia-village')\n",
                "plt.subplot(3,2,6)\n",
                "rank_plot('city-park')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import re #library to clean data\n",
                "import nltk #Natural Language tool kit\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns \n",
                "import os \n",
                "import datetime\n",
                "from nltk.corpus import stopwords #to remove stopword\n",
                "from nltk.stem.porter import PorterStemmer \n",
                "from PIL import Image\n",
                "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/google-play-store-apps')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data=pd.read_csv(\"../input/google-play-store-apps/googleplaystore.csv\")\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.isnull().sum().sort_values(ascending=False)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data_review=pd.read_csv(\"../input/google-play-store-apps/googleplaystore_user_reviews.csv\")\n",
                "data_review.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_review.isnull().sum().sort_values(ascending=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('data size: ',len(data))\n",
                "print('data_review size: ',len(data_review))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data=data.dropna(subset=['Rating','Current Ver','Android Ver','Content Rating'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(25,16))\n",
                "sns.barplot(x=data.Category.value_counts(),y=data.Category.value_counts().index,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"Counts of Category\",size=20)\n",
                "ax[0,0].set_xlabel(\"\")\n",
                "\n",
                "data.Reviews=data.Reviews.astype('int')\n",
                "sns.barplot(x=data.groupby(['Category'])['Reviews'].agg('sum').sort_values(ascending=False),y=data.groupby(['Category'])['Reviews'].agg('sum').sort_values(ascending=False).index,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"Number of reviews by Category\",size=20)\n",
                "ax[0,1].set_ylabel(\"\")\n",
                "\n",
                "data['new_install']=data.Installs.apply(lambda x:x.split('+')[0].strip(',').replace(',',''))\n",
                "data.new_install=data.new_install.astype('int')\n",
                "\n",
                "sns.barplot(x=data.groupby(['Category'])['new_install'].agg('sum').sort_values(ascending=False),y=data.groupby(['Category'])['new_install'].agg('sum').sort_values(ascending=False).index,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"Number of installs by Category\",size=20)\n",
                "ax[1,0].set_ylabel(\"\")\n",
                "ax[1,0].set_xlabel(\"\")\n",
                "\n",
                "sns.boxplot(y=\"Category\",x=\"Rating\",data=data,ax=ax[1,1])\n",
                "ax[1,1].set_ylabel(\"\")\n",
                "ax[1,1].set_title(\"Distribution of rating by Category\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(8,8))\n",
                "corr = data.corr()\n",
                "sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns,annot=True)\n",
                "plt.title(\"correlation plot\",size=28)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(25,16))\n",
                "sns.barplot(x=data.Size.value_counts()[:10],y=data.Size.value_counts()[:10].index,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 Size by counts\",size=20)\n",
                "ax[0].set_xlabel(\"\")\n",
                "\n",
                "sns.boxplot(y=\"Size\",x=\"Rating\",data=data[data.Size.isin(list(data.Size.value_counts()[:10].index))],ax=ax[1])\n",
                "ax[1].set_ylabel(\"\")\n",
                "ax[1].set_title(\"Distribution of rating by Size for Top 10\",size=20)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data['new_Price']=data.Price.apply(lambda x:  x.strip('$') if x!='0' else x.strip(''))\n",
                "data.new_Price=data.new_Price.astype(float)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(25,16))\n",
                "sns.barplot(x=data['Content Rating'].value_counts(),y=data['Content Rating'].value_counts().index,ax=ax[0])\n",
                "ax[0].set_title(\"Counts of Content Rating\",size=20)\n",
                "ax[0].set_xlabel(\"\")\n",
                "\n",
                "sns.barplot(x=data.groupby(['Content Rating'])['new_Price'].agg('sum'),y=data.groupby(['Content Rating'])['new_Price'].agg('sum').index,ax=ax[1])\n",
                "ax[1].set_title(\"Total Price by Content Rating\",size=20)\n",
                "ax[1].set_ylabel(\"\")\n",
                "ax[1].set_xlabel(\"Total Price\")\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_review=data_review.dropna(subset=['Translated_Review'])\n",
                "data_review=data_review.reindex(range(len(data_review)), method='ffill')\n",
                "headline_text_new=[]#Initialize empty array to append clean text\n",
                "for i in range(len(data_review)):\n",
                "    headline=re.sub('[^a-zA-Z]',' ',data_review['Translated_Review'][i]) \n",
                "    headline=headline.lower() #convert to lower case\n",
                "    headline=headline.split() #split to array(default delimiter is \" \")\n",
                "    ps=PorterStemmer() #creating porterStemmer object to take main stem of each word\n",
                "    headline=[ps.stem(word) for word in headline if not word in set(stopwords.words('english'))] #loop for stemming each word  in string array at ith row\n",
                "    headline_text_new.extend(headline)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "wordcloud = WordCloud(background_color=\"black\",max_words=200,max_font_size=40,random_state=10).generate(str(headline_text_new))\n",
                "\n",
                "plt.figure(figsize=(20,15))\n",
                "plt.imshow(wordcloud, interpolation='bilinear')\n",
                "plt.axis(\"off\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import seaborn as sns\n",
                "import math\n",
                "import datetime"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(\"../input/india-trade-data\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data_export=pd.read_csv(\"../input/india-trade-data/2018-2010_export.csv\")\n",
                "data_import=pd.read_csv(\"../input/india-trade-data/2018-2010_import.csv\")\n",
                "data_export.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_export.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_export=data_export.dropna(subset=['value'])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_import.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_import.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_import=data_import.dropna(subset=['value'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(30,30))\n",
                "j=0\n",
                "for i in ['2010','2011','2012','2013','2014','2015','2016','2017','2018']:\n",
                "    j+=1\n",
                "    plt.subplot(3,3,j)\n",
                "    y=data_export[data_export.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10].index\n",
                "    x=data_export[data_export.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10]\n",
                "    sns.barplot(x=x,y=y)\n",
                "    plt.title('Top 10 value of export in '+i,size=24)\n",
                "    plt.xlabel('million US$')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(30,30))\n",
                "j=0\n",
                "for i in ['2010','2011','2012','2013','2014','2015','2016','2017','2018']:\n",
                "    j+=1\n",
                "    plt.subplot(3,3,j)\n",
                "    y=data_import[data_import.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10].index\n",
                "    x=data_import[data_import.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10]\n",
                "    sns.barplot(x=x,y=y)\n",
                "    plt.title('Top 10 value of import in '+i,size=24)\n",
                "    plt.xlabel('million US$')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def rank_export(country):\n",
                "    year=['2010','2011','2012','2013','2014','2015','2016','2017','2018']\n",
                "    B={}\n",
                "    for i in range(len(year)):\n",
                "        A=data_export[data_export.year==int(year[i])]\n",
                "        value=A.groupby(['country'])['value'].agg('sum')\n",
                "        rank=A.groupby(['country'])['value'].agg('sum').rank(method='min',ascending=False)\n",
                "        new=pd.DataFrame({'rank':rank,'value':value})\n",
                "        B['rank '+year[i]]=str(new[new.index==country].iloc[0,0])+\"/\"+str(max(rank))\n",
                "        B['value '+year[i]]=str(new[new.index==country].iloc[0,1])\n",
                "\n",
                "    return B"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def rank_import(country):\n",
                "    year=['2010','2011','2012','2013','2014','2015','2016','2017','2018']\n",
                "    B={}\n",
                "    for i in range(len(year)):\n",
                "        A=data_import[data_import.year==int(year[i])]\n",
                "        value=A.groupby(['country'])['value'].agg('sum')\n",
                "        rank=A.groupby(['country'])['value'].agg('sum').rank(method='min',ascending=False)\n",
                "        new=pd.DataFrame({'rank':rank,'value':value})\n",
                "        B['rank '+year[i]]=str(new[new.index==country].iloc[0,0])+\"/\"+str(max(rank))\n",
                "        B['value '+year[i]]=str(new[new.index==country].iloc[0,1])\n",
                "\n",
                "    return B"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(24,10))\n",
                "plt.subplot(1,2,1)\n",
                "CHINA=rank_export('CHINA P RP')\n",
                "y=[]\n",
                "x=[]\n",
                "n=[]\n",
                "for i in range(9):\n",
                "    r1,r2=CHINA['rank '+str(i+2010)].split('/')\n",
                "    R=float(r1)/float(r2)\n",
                "    R=1-R\n",
                "    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n",
                "    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n",
                "    n.append('rank '+str(i+2010)+' '+CHINA['rank '+str(i+2010)])\n",
                "    \n",
                "x.append(x[0])\n",
                "y.append(y[0])\n",
                "plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "for i, txt in enumerate(n):\n",
                "    plt.annotate(txt, (x[i], y[i]))\n",
                "    plt.fill(x, y,\"coral\")\n",
                "    plt.xlim(0.45,2.7)\n",
                "    plt.ylim(0.45,2.7)\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"CHINA's export rank by year\",size=18)\n",
                "    \n",
                "plt.subplot(1,2,2)\n",
                "CHINA=rank_import('CHINA P RP')\n",
                "y=[]\n",
                "x=[]\n",
                "n=[]\n",
                "for i in range(9):\n",
                "    r1,r2=CHINA['rank '+str(i+2010)].split('/')\n",
                "    R=float(r1)/float(r2)\n",
                "    R=1-R\n",
                "    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n",
                "    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n",
                "    n.append('rank '+str(i+2010)+' '+CHINA['rank '+str(i+2010)])\n",
                "    \n",
                "x.append(x[0])\n",
                "y.append(y[0])\n",
                "plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "for i, txt in enumerate(n):\n",
                "    plt.annotate(txt, (x[i], y[i]))\n",
                "    plt.xlim(0.45,2.7)\n",
                "    plt.ylim(0.45,2.7)\n",
                "    plt.fill(x, y,\"plum\")\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"CHINA's import rank by year\",size=18)   "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(24,10))\n",
                "plt.subplot(1,2,1)\n",
                "USA=rank_export('U S A')\n",
                "y=[]\n",
                "x=[]\n",
                "n=[]\n",
                "for i in range(9):\n",
                "    r1,r2=USA['rank '+str(i+2010)].split('/')\n",
                "    R=float(r1)/float(r2)\n",
                "    R=1-R\n",
                "    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n",
                "    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n",
                "    n.append('rank '+str(i+2010)+' '+USA['rank '+str(i+2010)])\n",
                "    \n",
                "x.append(x[0])\n",
                "y.append(y[0])\n",
                "plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "for i, txt in enumerate(n):\n",
                "    plt.annotate(txt, (x[i], y[i]))\n",
                "    plt.fill(x, y,\"coral\")\n",
                "    plt.xlim(0.45,2.7)\n",
                "    plt.ylim(0.45,2.7)\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"USA's export rank by year\",size=18)\n",
                "    \n",
                "plt.subplot(1,2,2)\n",
                "USA=rank_import('U S A')\n",
                "y=[]\n",
                "x=[]\n",
                "n=[]\n",
                "for i in range(9):\n",
                "    r1,r2=USA['rank '+str(i+2010)].split('/')\n",
                "    R=float(r1)/float(r2)\n",
                "    R=1-R\n",
                "    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n",
                "    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n",
                "    n.append('rank '+str(i+2010)+' '+USA['rank '+str(i+2010)])\n",
                "    \n",
                "x.append(x[0])\n",
                "y.append(y[0])\n",
                "plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "for i, txt in enumerate(n):\n",
                "    plt.annotate(txt, (x[i], y[i]))\n",
                "    plt.xlim(0.45,2.7)\n",
                "    plt.ylim(0.45,2.7)\n",
                "    plt.fill(x, y,\"plum\")\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"USA's import rank by year\",size=18)    \n",
                "    "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(24,10))\n",
                "plt.subplot(1,2,1)\n",
                "ALBANIA=rank_export('ALBANIA')\n",
                "y=[]\n",
                "x=[]\n",
                "n=[]\n",
                "for i in range(9):\n",
                "    r1,r2=ALBANIA['rank '+str(i+2010)].split('/')\n",
                "    R=float(r1)/float(r2)\n",
                "    R=1-R\n",
                "    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n",
                "    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n",
                "    n.append('rank '+str(i+2010)+' '+ALBANIA['rank '+str(i+2010)])\n",
                "    \n",
                "x.append(x[0])\n",
                "y.append(y[0])\n",
                "plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "for i, txt in enumerate(n):\n",
                "    plt.annotate(txt, (x[i], y[i]))\n",
                "    plt.fill(x, y,\"coral\")\n",
                "    plt.xlim(0.45,2.7)\n",
                "    plt.ylim(0.45,2.7)\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"ALBANIA's export rank by year\",size=18)\n",
                "    \n",
                "plt.subplot(1,2,2)\n",
                "ALBANIA=rank_import('ALBANIA')\n",
                "y=[]\n",
                "x=[]\n",
                "n=[]\n",
                "for i in range(9):\n",
                "    r1,r2=ALBANIA['rank '+str(i+2010)].split('/')\n",
                "    R=float(r1)/float(r2)\n",
                "    R=1-R\n",
                "    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n",
                "    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n",
                "    n.append('rank '+str(i+2010)+' '+ALBANIA['rank '+str(i+2010)])\n",
                "    \n",
                "x.append(x[0])\n",
                "y.append(y[0])\n",
                "plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "for i, txt in enumerate(n):\n",
                "    plt.annotate(txt, (x[i], y[i]))\n",
                "    plt.xlim(0.45,2.7)\n",
                "    plt.ylim(0.45,2.7)\n",
                "    plt.fill(x, y,\"plum\")\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"ALBANIA's import rank by year\",size=18)    "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(9,2,figsize=(20,55))\n",
                "for i in range(9):\n",
                "    count=data_export[data_export.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False)\n",
                "    groups=list(data_export[data_export.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False).index[:10])\n",
                "    counts=list(count[:10])\n",
                "    counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "    groups.append('Other')\n",
                "    type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "    clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "    type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[i,0])\n",
                "    ax[i,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "    ax[i,0].set_title(\"Top 10 export of commodity in \"+str(i+2010))\n",
                "    ax[i,0].set_ylabel('')\n",
                "\n",
                "    count=data_import[data_import.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False)\n",
                "    groups=list(data_import[data_import.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False).index[:10])\n",
                "    counts=list(count[:10])\n",
                "    counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "    groups.append('Other')\n",
                "    type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "    clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "    qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[i,1])\n",
                "    ax[i,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "    ax[i,1].set_title(\"Top 10 import of commodity in \"+str(i+2010))\n",
                "    ax[i,1].set_ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(10,2,figsize=(30,55))\n",
                "for i in range(10):\n",
                "    data_export.groupby(['HSCode','year'])['value'].agg(sum).unstack(['HSCode']).iloc[:,i*10:(i+1)*10].plot(ax=ax[i,0])\n",
                "    ax[i,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "    ax[i,0].set_title(\"Export HSCode by year\")\n",
                "    ax[i,0].set_ylabel('values')\n",
                "\n",
                "    data_import.groupby(['HSCode','year'])['value'].agg(sum).unstack(['HSCode']).iloc[:,i*10:(i+1)*10].plot(ax=ax[i,1])\n",
                "    ax[i,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "    ax[i,1].set_title(\"Import HSCode by year\")\n",
                "    ax[i,1].set_ylabel('values')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os \n",
                "import seaborn as sns\n",
                "import geopandas as gpd\n",
                "import folium\n",
                "from folium import plugins\n",
                "import datetime\n",
                "import math"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(\"../input/chicago-food-inspections\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data=pd.read_csv(\"../input/chicago-food-inspections/food-inspections.csv\")\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(data)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(10,10))\n",
                "sns.barplot(x=data.isnull().sum().sort_values(ascending=False),y=data.isnull().sum().sort_values(ascending=False).index)\n",
                "plt.title(\"counts of missing value\",size=20)\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data=data.dropna(subset=['Violations','Facility Type','Latitude','Longitude','AKA Name'])\n",
                "data.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data['year']=data['Inspection Date'].apply(lambda x:x.split('-')[0])\n",
                "data['month']=data['Inspection Date'].apply(lambda x:x.split('-')[1])\n",
                "data['day']=data['Inspection Date'].apply(lambda x:x.split('-')[2].split('T')[0])\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "x=data.year.value_counts().index\n",
                "y=data.year.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"The counts of inspection by year\",size=20)\n",
                "ax[0,0].set_ylabel('counts',size=18)\n",
                "ax[0,0].set_xlabel('')\n",
                "\n",
                "x=data.month.value_counts().index\n",
                "y=data.month.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"The counts of inspection by month\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "ax[0,1].set_xlabel('')\n",
                "\n",
                "x=data.day.value_counts().index\n",
                "y=data.day.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"The counts of inspection by day\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "ax[1,0].set_xlabel('')\n",
                "\n",
                "data.groupby(['year','month'])['Inspection ID'].agg('count').unstack('year').plot(ax=ax[1,1])\n",
                "ax[1,1].set_title(\"The counts of inspection for every month by year\",size=20)\n",
                "ax[1,1].set_ylabel('counts',size=18)\n",
                "ax[1,1].set_xlabel('month')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(15,16))\n",
                "data.Risk.value_counts().plot(kind='bar',color=['red','yellow','green'],ax=ax[0,0])\n",
                "ax[0,0].tick_params(axis='x',labelrotation=360)\n",
                "ax[0,0].set_title(\"The counts of Risk\",size=20)\n",
                "ax[0,0].set_ylabel('counts',size=18)\n",
                "\n",
                "\n",
                "data.groupby(['year','Risk'])['Inspection ID'].agg('count').unstack('Risk').plot(ax=ax[0,1],color=['red','yellow','green'])\n",
                "ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,0.75))\n",
                "ax[0,1].set_title(\"The counts of Risk by year\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "\n",
                "data.groupby(['month','Risk'])['Inspection ID'].agg('count').unstack('Risk').plot(ax=ax[1,0],color=['red','yellow','green'])\n",
                "ax[1,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(-0.25,0.75))\n",
                "ax[1,0].set_title(\"The counts of Risk by month\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "\n",
                "sns.scatterplot(x='Longitude',y='Latitude',hue='Risk' ,data=data, ax=ax[1,1])\n",
                "ax[1,1].set_title(\"The distribution of inspections by risk\",size=20)\n",
                "ax[1,1].set_xlabel('Longitude')\n",
                "ax[1,1].set_ylabel('LATITUDE')\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_risk1=data[data.Risk=='Risk 1 (High)']\n",
                "data_risk1.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "sns.barplot(x=data_risk1['Facility Type'].value_counts()[:10],y=data_risk1['Facility Type'].value_counts()[:10].index,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 Facility Type by the counts of risk 1 \",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=data_risk1.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False)\n",
                "groups=list(data_risk1.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10])\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "ax[1].set_ylabel('')\n",
                "ax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_risk1_2000=data_risk1[:2000]\n",
                "Long=data_risk1_2000.Longitude.mean()\n",
                "Lat=data_risk1_2000.Latitude.mean()\n",
                "risk1_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "risk1_distribution_map=plugins.MarkerCluster().add_to(risk1_map)\n",
                "for lat,lon,label in zip(data_risk1_2000.Latitude,data_risk1_2000.Longitude,data_risk1_2000['AKA Name']):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(risk1_distribution_map)\n",
                "risk1_map.add_child(risk1_distribution_map)\n",
                "\n",
                "risk1_map"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_risk2=data[data.Risk=='Risk 2 (Medium)']\n",
                "data_risk2.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "sns.barplot(x=data_risk2['Facility Type'].value_counts()[:10],y=data_risk2['Facility Type'].value_counts()[:10].index,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 Facility Type by the counts of risk 2 \",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=data_risk2.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False)\n",
                "groups=list(data_risk2.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10])\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "ax[1].set_ylabel('')\n",
                "ax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_risk2_2000=data_risk2[:2000]\n",
                "Long=data_risk2_2000.Longitude.mean()\n",
                "Lat=data_risk2_2000.Latitude.mean()\n",
                "risk2_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "risk2_distribution_map=plugins.MarkerCluster().add_to(risk2_map)\n",
                "for lat,lon,label in zip(data_risk2_2000.Latitude,data_risk2_2000.Longitude,data_risk2_2000['AKA Name']):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(risk2_distribution_map)\n",
                "risk2_map.add_child(risk2_distribution_map)\n",
                "\n",
                "risk2_map"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_risk3=data[data.Risk=='Risk 3 (Low)']\n",
                "data_risk3.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "sns.barplot(x=data_risk3['Facility Type'].value_counts()[:10],y=data_risk3['Facility Type'].value_counts()[:10].index,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 Facility Type by the counts of risk 3 \",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=data_risk3.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False)\n",
                "groups=list(data_risk3.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10])\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "ax[1].set_ylabel('')\n",
                "ax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_risk3_2000=data_risk3[:2000]\n",
                "data_risk3_2000['AKA Name']=data_risk3_2000['AKA Name'].apply(lambda x:x.strip('`').strip())\n",
                "Long=data_risk3_2000.Longitude.mean()\n",
                "Lat=data_risk3_2000.Latitude.mean()\n",
                "risk3_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "risk3_distribution_map=plugins.MarkerCluster().add_to(risk3_map)\n",
                "for lat,lon,label in zip(data_risk3_2000.Latitude,data_risk3_2000.Longitude,data_risk3_2000['AKA Name']):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(risk3_distribution_map)\n",
                "risk3_map.add_child(risk3_distribution_map)\n",
                "\n",
                "risk3_map"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,16))\n",
                "y=data['Facility Type'].value_counts()[:10].index\n",
                "x=data['Facility Type'].value_counts()[:10]\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"Top 10 Facility Type by the counts of inspection \",size=20)\n",
                "ax[0,0].set_xlabel('counts',size=18)\n",
                "ax[0,0].set_ylabel('')\n",
                "\n",
                "sns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=data[data['Facility Type']=='Restaurant'], ax=ax[0,1])\n",
                "ax[0,1].set_title(\"The distribution of inspections for restaurant\",size=20)\n",
                "ax[0,1].set_xlabel('Longitude')\n",
                "ax[0,1].set_ylabel('LATITUDE')\n",
                "\n",
                "sns.scatterplot(x='Longitude',y='Latitude',hue='Risk' ,hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'],data=data[data['Facility Type']=='Grocery Store'], ax=ax[1,0])\n",
                "ax[1,0].set_title(\"The distribution of inspections for Grocery Store\",size=20)\n",
                "ax[1,0].set_xlabel('Longitude')\n",
                "ax[1,0].set_ylabel('LATITUDE')\n",
                "\n",
                "sns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=data[data['Facility Type']=='School'], ax=ax[1,1])\n",
                "ax[1,1].set_title(\"The distribution of inspections for School\",size=20)\n",
                "ax[1,1].set_xlabel('Longitude')\n",
                "ax[1,1].set_ylabel('LATITUDE')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,16))\n",
                "x=data.Results.value_counts().index\n",
                "y=data.Results.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"The counts of Results of inspection \",size=20)\n",
                "ax[0,0].set_ylabel('counts',size=18)\n",
                "ax[0,0].set_xlabel('')\n",
                "\n",
                "data.groupby(['Results','year'])['Inspection ID'].agg('count').unstack('Results').plot(kind='bar',ax=ax[0,1])\n",
                "ax[0,1].tick_params(axis='x',labelrotation=360)\n",
                "ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,0.75))\n",
                "ax[0,1].set_title(\"The counts of results of inspection by year \",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "\n",
                "sns.scatterplot(x='Longitude',y='Latitude',hue='Risk' ,hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'],data=data[data.Results=='Pass'], ax=ax[1,0])\n",
                "ax[1,0].set_title(\"The distribution of result is pass\",size=20)\n",
                "ax[1,0].set_xlabel('Longitude')\n",
                "ax[1,0].set_ylabel('LATITUDE')\n",
                "\n",
                "sns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=data[data.Results=='Fail'], ax=ax[1,1])\n",
                "ax[1,1].set_title(\"The distribution of result is fail\",size=20)\n",
                "ax[1,1].set_xlabel('Longitude')\n",
                "ax[1,1].set_ylabel('LATITUDE')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import re #library to clean data\n",
                "import nltk #Natural Language tool kit\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns \n",
                "import os \n",
                "import datetime\n",
                "from nltk.corpus import stopwords #to remove stopword\n",
                "from nltk.stem.porter import PorterStemmer \n",
                "from PIL import Image\n",
                "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/ireland-historical-news')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "latnigrin=pd.read_csv(\"../input/ireland-historical-news/w3-latnigrin-text.csv\")\n",
                "latnigrin.head()\n",
                "irishtimes=pd.read_csv(\"../input/ireland-historical-news/irishtimes-date-text.csv\")\n",
                "irishtimes.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "irishtimes.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(irishtimes)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "irishtimes['date']=irishtimes.publish_date.apply(lambda x:datetime.datetime.strptime(str(x),'%Y%m%d').strftime('%Y-%m-%d'))\n",
                "irishtimes['year']=irishtimes.date.apply(lambda x:x.split('-')[0])\n",
                "irishtimes['month']=irishtimes.date.apply(lambda x:x.split('-')[1])\n",
                "irishtimes['day']=irishtimes.date.apply(lambda x:x.split('-')[2])\n",
                "irishtimes.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(16,16))\n",
                "Top10_category=irishtimes[irishtimes['headline_category'].isin(list(irishtimes.headline_category.value_counts()[:10].index[:10]))]\n",
                "sns.barplot(y=Top10_category.headline_category.value_counts().index,x=Top10_category.headline_category.value_counts(),ax=ax[0,0])\n",
                "ax[0,0].set_title(\"Top 10 category by counts\",size=20)\n",
                "ax[0,0].set_xlabel('counts',size=18)\n",
                "ax[0,0].set_ylabel('')\n",
                "\n",
                "Top10_category.groupby(['year','headline_category'])['headline_category'].agg('count').unstack('headline_category').plot(ax=ax[0,1])\n",
                "ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "ax[0,1].set_title(\"Top 10 category counts by year\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "ax[0,1].set_xlabel('year',size=18)\n",
                "\n",
                "Top10_category.groupby(['month','headline_category'])['headline_category'].agg('count').unstack('headline_category').plot(ax=ax[1,0])\n",
                "ax[1,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(-0.25,1))\n",
                "ax[1,0].set_title(\"Top 10 category counts by month\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "ax[1,0].set_xlabel('month',size=18)\n",
                "\n",
                "Top10_category.groupby(['day','headline_category'])['headline_category'].agg('count').unstack('headline_category').plot(ax=ax[1,1])\n",
                "ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "ax[1,1].set_title(\"Top 10 category counts by day\",size=20)\n",
                "ax[1,1].set_ylabel('counts',size=18)\n",
                "ax[1,1].set_xlabel('day',size=18)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(25,25))\n",
                "sns.barplot(x=irishtimes.year.value_counts().index,y=irishtimes.year.value_counts(),ax=ax[0,0])\n",
                "ax[0,0].set_title(\"Bar chart for year\",size=30)\n",
                "ax[0,0].set_xlabel('year',size=20)\n",
                "ax[0,0].set_ylabel('counts',size=20)\n",
                "\n",
                "sns.barplot(x=irishtimes.month.value_counts().index,y=irishtimes.month.value_counts(),ax=ax[0,1])\n",
                "ax[0,1].set_title(\"Bar chart for month\",size=30)\n",
                "ax[0,1].set_xlabel('month',size=20)\n",
                "ax[0,1].set_ylabel('counts',size=20)\n",
                "\n",
                "sns.barplot(x=irishtimes.day.value_counts().index,y=irishtimes.day.value_counts(),ax=ax[1,0])\n",
                "ax[1,0].set_title(\"Bar chart for day\",size=30)\n",
                "ax[1,0].set_xlabel('day',size=20)\n",
                "ax[1,0].set_ylabel('counts',size=20)\n",
                "\n",
                "irishtimes.groupby(['date'])['headline_category'].agg('count').plot(ax=ax[1,1])\n",
                "ax[1,1].set_title(\"Number of news for date\",size=30)\n",
                "ax[1,1].set_xlabel('date',size=20)\n",
                "ax[1,1].set_ylabel('counts',size=20)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "irishtimes_headline_text=irishtimes[:100000]\n",
                "headline_text_new=[]#Initialize empty array to append clean text\n",
                "for i in range(len(irishtimes_headline_text)):\n",
                "\theadline=re.sub('[^a-zA-Z]',' ',irishtimes_headline_text['headline_text'][i]) \n",
                "\theadline=headline.lower() #convert to lower case\n",
                "\theadline=headline.split() #split to array(default delimiter is \" \")\n",
                "\tps=PorterStemmer() #creating porterStemmer object to take main stem of each word\n",
                "\theadline=[ps.stem(word) for word in headline if not word in set(stopwords.words('english'))] #loop for stemming each word  in string array at ith row\n",
                "\theadline_text_new.extend(headline)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "\n",
                "wordcloud = WordCloud(background_color=\"black\",max_words=200,max_font_size=40,random_state=10).generate(str(headline_text_new))\n",
                "\n",
                "plt.figure(figsize=(20,15))\n",
                "plt.imshow(wordcloud, interpolation='bilinear')\n",
                "plt.axis(\"off\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def headline_by_year(year):\n",
                "    headline_text_new=[]#Initialize empty array to append clean text\n",
                "    irishtimes_headline_text=irishtimes[irishtimes.year==str(year)]\n",
                "    headline=None\n",
                "    for i in range(len(irishtimes_headline_text)):\n",
                "        headline=re.sub('[^a-zA-Z]',' ',irishtimes_headline_text['headline_text'][irishtimes_headline_text.index[i]]) \n",
                "        headline=headline.lower() #convert to lower case\n",
                "        headline=headline.split() #split to array(default delimiter is \" \")\n",
                "        ps=PorterStemmer() #creating porterStemmer object to take main stem of each word\n",
                "        headline=[ps.stem(word) for word in headline if not word in set(stopwords.words('english'))] #loop for stemming each word  in string array at ith row\n",
                "        headline_text_new.extend(headline)\n",
                "    wordcloud = WordCloud(background_color=\"black\",random_state=40,max_words=200,max_font_size=40).generate(str(headline_text_new))\n",
                "    plt.figure(figsize=(20,15))\n",
                "    plt.imshow(wordcloud, interpolation='bilinear')\n",
                "    plt.title(\"Wordcloud of headline in \"+str(year),size=20)\n",
                "    plt.axis(\"off\")\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "headline_by_year(1996)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "headline_by_year(2000)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "headline_by_year(2005)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "headline_by_year(2010)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "headline_by_year(2015)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "headline_by_year(2018)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os \n",
                "import seaborn as sns\n",
                "import geopandas as gpd\n",
                "import folium\n",
                "from folium import plugins\n",
                "import datetime\n",
                "import math"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(\"../input/ieee-fraud-detection\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train_identity=pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\")\n",
                "train_transaction=pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\")\n",
                "test_transaction=pd.read_csv(\"../input/ieee-fraud-detection/test_transaction.csv\")\n",
                "test_identity=pd.read_csv(\"../input/ieee-fraud-detection/test_identity.csv\")\n",
                "print(\"train_identity_data_size: \",len(train_identity))\n",
                "print(\"train_transaction_data_size: \",len(train_transaction))\n",
                "print(\"test_transaction_data_size: \",len(test_transaction))\n",
                "print(\"test_identity_data_size: \",len(test_identity))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_identity.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(10,10))\n",
                "sns.barplot(x=train_identity.isnull().sum().sort_values(ascending=False),y=train_identity.isnull().sum().sort_values(ascending=False).index)\n",
                "plt.title(\"counts of missing value for train_identity\",size=20)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "train_identity_new=pd.DataFrame(train_identity,columns=['TransactionID','id_01','id_12','id_38','id_37','id_36','id_35','id_15','id_29','id_28','id_11','id_02','DeviceType','id_31','id_17','id_19','id_20'])\n",
                "train_identity_new=train_identity_new.dropna(subset=['id_38','id_37','id_36','id_35','id_15','id_29','id_28','id_11','id_02','DeviceType','id_31','id_17','id_19','id_20'])\n",
                "train_identity_new.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(train_identity_new)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(8,8))\n",
                "corr = train_identity_new.corr()\n",
                "sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns,annot=True)\n",
                "plt.title(\"correlation plot for train_identity_new\",size=28)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "y=train_identity_new.id_01.value_counts().index\n",
                "x=train_identity_new.id_01.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0],orient='h')\n",
                "ax[0,0].set_title(\"Bar chart for id_01\",size=20)\n",
                "ax[0,0].set_xlabel('counts',size=18)\n",
                "ax[0,0].set_ylabel('')\n",
                "\n",
                "y=train_identity_new.id_12.value_counts().index\n",
                "x=train_identity_new.id_12.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"Bar chart for id_12\",size=20)\n",
                "ax[0,1].set_xlabel('counts',size=18)\n",
                "ax[0,1].set_ylabel('')\n",
                "\n",
                "y=train_identity_new.id_38.value_counts().index\n",
                "x=train_identity_new.id_38.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[1,0],order=['T','F'])\n",
                "ax[1,0].set_title(\"Bar chart for id_38\",size=20)\n",
                "ax[1,0].set_xlabel('counts',size=18)\n",
                "ax[1,0].set_ylabel('')\n",
                "\n",
                "y=train_identity_new.id_37.value_counts().index\n",
                "x=train_identity_new.id_37.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[1,1],order=['T','F'])\n",
                "ax[1,1].set_title(\"Bar chart for id_37\",size=20)\n",
                "ax[1,1].set_xlabel('counts',size=18)\n",
                "ax[1,1].set_ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "y=train_identity_new.id_36.value_counts().index\n",
                "x=train_identity_new.id_36.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0],order=['T','F'])\n",
                "ax[0,0].set_title(\"Bar chart for id_36\",size=20)\n",
                "ax[0,0].set_xlabel('counts',size=18)\n",
                "ax[0,0].set_ylabel('')\n",
                "\n",
                "y=train_identity_new.id_35.value_counts().index\n",
                "x=train_identity_new.id_35.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,1],order=['T','F'])\n",
                "ax[0,1].set_title(\"Bar chart for id_35\",size=20)\n",
                "ax[0,1].set_xlabel('counts',size=18)\n",
                "ax[0,1].set_ylabel('')\n",
                "\n",
                "y=train_identity_new.id_15.value_counts().index\n",
                "x=train_identity_new.id_15.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"Bar chart for id_15\",size=20)\n",
                "ax[1,0].set_xlabel('counts',size=18)\n",
                "ax[1,0].set_ylabel('')\n",
                "\n",
                "y=train_identity_new.id_29.value_counts().index\n",
                "x=train_identity_new.id_29.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[1,1])\n",
                "ax[1,1].set_title(\"Bar chart for id_29\",size=20)\n",
                "ax[1,1].set_xlabel('counts',size=18)\n",
                "ax[1,1].set_ylabel('')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "del train_identity\n",
                "del test_identity\n",
                "train_transaction.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(train_transaction)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(18,70))\n",
                "sns.barplot(x=train_transaction.isnull().sum().sort_values(ascending=False),y=train_transaction.isnull().sum().sort_values(ascending=False).index)\n",
                "plt.title(\"counts of missing value for train_transaction\",size=20)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "\n",
                "train_transaction_new=pd.DataFrame(train_transaction,columns=train_transaction.isnull().sum().sort_values()[:250].index)\n",
                "train_transaction_new=train_transaction_new.drop(columns=['TransactionID'])\n",
                "train_transaction_new_label=train_transaction_new.isFraud\n",
                "train_transaction_new=train_transaction_new.drop(columns=['isFraud'])\n",
                "train_transaction_new.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(train_transaction_new)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test_transaction.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(test_transaction)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(18,70))\n",
                "sns.barplot(x=test_transaction.isnull().sum().sort_values(ascending=False),y=test_transaction.isnull().sum().sort_values(ascending=False).index)\n",
                "plt.title(\"counts of missing value for test_transaction\",size=20)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "test_transaction_new=pd.DataFrame(test_transaction,columns=train_transaction.isnull().sum().sort_values()[:250].index)\n",
                "del test_transaction\n",
                "del train_transaction\n",
                "ID=test_transaction_new.TransactionID\n",
                "test_transaction_new=test_transaction_new.drop(columns=['TransactionID','isFraud'])\n",
                "test_transaction_new.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(test_transaction_new)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import LabelEncoder\n",
                "labelencoder = LabelEncoder()\n",
                "for i in list(train_transaction_new.select_dtypes(include=['object']).columns):\n",
                "    test_transaction_new[i] = labelencoder.fit_transform(test_transaction_new[i].astype('str'))\n",
                "    train_transaction_new[i] = labelencoder.fit_transform(train_transaction_new[i].astype('str'))\n",
                "test_transaction_new.ProductCD[:5]\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_transaction_new.ProductCD[:5]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#train_transaction_new=train_transaction_new.fillna(-999)\n",
                "#test_transaction_new=test_transaction_new.fillna(-999)\n",
                "train_transaction_new=train_transaction_new.fillna(train_transaction_new.median())\n",
                "test_transaction_new=test_transaction_new.fillna(train_transaction_new.median())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "from sklearn.linear_model import LogisticRegression  \n",
                "from sklearn.preprocessing import StandardScaler  \n",
                "from sklearn.model_selection import train_test_split\n",
                "X_train,X_test,y_train,y_test=train_test_split(train_transaction_new,train_transaction_new_label,test_size=0.2)\n",
                "del train_transaction_new\n",
                "lr = LogisticRegression(C=0.09,solver='lbfgs')  \n",
                "lr.fit(X_train, y_train)  \n",
                "proba_test = lr.predict_proba(X_test)[:, 1]\n",
                "LR_result=pd.DataFrame({'pred':proba_test,'real':y_test})\n",
                "LR_result['pred_0_1']=LR_result.pred.apply(lambda x:1 if x>=0.5 else 0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('LR_acc: ',sum(LR_result.real==LR_result.pred_0_1)/len(LR_result))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "import lightgbm as lgb  \n",
                "import pickle  \n",
                "from sklearn.metrics import roc_auc_score  \n",
                "lgb_train = lgb.Dataset(X_train, y_train)  \n",
                "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train) \n",
                "params = {  \n",
                "    'boosting_type': 'gbdt',  \n",
                "    'objective': 'binary',  \n",
                "    'metric': {'binary_logloss', 'auc'},  \n",
                "    'num_leaves':240,  \n",
                "    'max_depth': 15,  \n",
                "    'min_data_in_leaf': 100,  \n",
                "    'learning_rate': 0.05,  \n",
                "    'feature_fraction': 0.95,  \n",
                "    'bagging_fraction': 0.95,  \n",
                "    'bagging_freq': 5,  \n",
                "    'lambda_l1': 0,    \n",
                "    'lambda_l2': 0, \n",
                "    'min_gain_to_split': 0.1,  \n",
                "    'verbose': 0,  \n",
                "    'is_unbalance': True  \n",
                "}  "
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "gbm = lgb.train(params,  lgb_train,  \n",
                "                num_boost_round=10000,  \n",
                "                valid_sets=lgb_eval,early_stopping_rounds=500)  "
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "gbm.predict(test_transaction_new[:10], num_iteration=gbm.best_iteration) "
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "LR_TEST=lr.predict_proba(test_transaction_new)[:, 1]\n",
                "LGBM_TEST= gbm.predict(test_transaction_new, num_iteration=gbm.best_iteration) \n",
                "\n",
                "prediction=pd.DataFrame({'TransactionID':ID,'LR_TEST':LR_TEST,'LGBM_TEST':LGBM_TEST})\n",
                "\n",
                "prediction.to_csv('prediction.csv',index=False)\n",
                "\n",
                "submission=pd.DataFrame({'TransactionID':ID,'isFraud':LGBM_TEST})\n",
                "\n",
                "\n",
                "submission.to_csv('submission.csv',index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from keras.utils import np_utils\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Convolution2D,Dense,MaxPool2D,Activation,Dropout,Flatten\n",
                "from keras.layers import GlobalAveragePooling2D\n",
                "from keras.optimizers import Adam\n",
                "from sklearn.model_selection import train_test_split\n",
                "from keras.layers.normalization import BatchNormalization\n",
                "import os \n",
                "import pandas as pd\n",
                "import plotly.graph_objs as go\n",
                "import matplotlib.ticker as ticker\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "import cv2\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/plant-pathology-2020-fgvc7')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results",
                "process_data"
            ],
            "source": [
                "train=pd.read_csv(\"../input/plant-pathology-2020-fgvc7/train.csv\")\n",
                "test=pd.read_csv(\"../input/plant-pathology-2020-fgvc7/test.csv\")\n",
                "train['image_id']=train['image_id']+'.jpg'\n",
                "test['image_id']=test['image_id']+'.jpg'\n",
                "train.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "sns.barplot(y=train.healthy.value_counts(),x=train.healthy.value_counts().index,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"Value count for healthy\",size=20)\n",
                "ax[0,0].set_xlabel('healthy',size=18)\n",
                "ax[0,0].set_ylabel('',size=18)\n",
                "\n",
                "sns.barplot(y=train.multiple_diseases.value_counts(),x=train.multiple_diseases.value_counts().index,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"Value count for multiple_diseases\",size=20)\n",
                "ax[0,1].set_xlabel('multiple_diseases',size=18)\n",
                "ax[0,1].set_ylabel('',size=18)\n",
                "\n",
                "sns.barplot(y=train.rust.value_counts(),x=train.rust.value_counts().index,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"Value count for rust\",size=20)\n",
                "ax[1,0].set_xlabel('rust',size=18)\n",
                "ax[1,0].set_ylabel('',size=18)\n",
                "\n",
                "sns.barplot(y=train.scab.value_counts(),x=train.scab.value_counts().index,ax=ax[1,1])\n",
                "ax[1,1].set_title(\"Value count for scab\",size=20)\n",
                "ax[1,1].set_xlabel('healthy',size=18)\n",
                "ax[1,1].set_ylabel('',size=18)\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "img=[]\n",
                "filename=train.image_id\n",
                "for file in filename:\n",
                "    image=cv2.imread(\"../input/plant-pathology-2020-fgvc7/images/\"+file)\n",
                "    res=cv2.resize(image,(256,256))\n",
                "    img.append(res)\n",
                "img=np.array(img)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(img.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,15))\n",
                "for i in range(9):\n",
                "    plt.subplot(3,3,i+1)\n",
                "    plt.imshow(img[i])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_labels = np.float32(train.loc[:, 'healthy':'scab'].values)\n",
                "\n",
                "train, val = train_test_split(train, test_size = 0.15)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "train_datagen = ImageDataGenerator( horizontal_flip=True,\n",
                "    vertical_flip=True,\n",
                "    rotation_range=10,\n",
                "    width_shift_range=0.1,\n",
                "    height_shift_range=0.1,\n",
                "    zoom_range=.1,\n",
                "    fill_mode='nearest',\n",
                "    shear_range=0.1,\n",
                "    rescale=1/255,\n",
                "    brightness_range=[0.5, 1.5])\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train_generator=train_datagen.flow_from_dataframe(train,directory='/kaggle/input/plant-pathology-2020-fgvc7/images/',\n",
                "                                                      target_size=(384,384),\n",
                "                                                      x_col=\"image_id\",\n",
                "                                                      y_col=['healthy','multiple_diseases','rust','scab'],\n",
                "                                                      class_mode='raw',\n",
                "                                                      shuffle=False,\n",
                "                                                       subset='training',\n",
                "                                                      batch_size=32)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "val_generator=train_datagen.flow_from_dataframe(val,directory='/kaggle/input/plant-pathology-2020-fgvc7/images/',\n",
                "                                                      target_size=(384,384),\n",
                "                                                      x_col=\"image_id\",\n",
                "                                                      y_col=['healthy','multiple_diseases','rust','scab'],\n",
                "                                                      class_mode='raw',\n",
                "                                                      shuffle=False,\n",
                "                                                      batch_size=32,\n",
                "                                                  )"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "test_generator=train_datagen.flow_from_dataframe(test,directory='/kaggle/input/plant-pathology-2020-fgvc7/images/',\n",
                "                                                      target_size=(384,384),\n",
                "                                                      x_col=\"image_id\",\n",
                "                                                      y_col=None,\n",
                "                                                      class_mode=None,\n",
                "                                                      shuffle=False,\n",
                "                                                      batch_size=32)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "check_results"
            ],
            "source": [
                "from keras.applications.resnet50 import ResNet50\n",
                "from keras.models import Model\n",
                "import keras\n",
                "from keras import optimizers\n",
                "model_finetuned = ResNet50(include_top=False, weights='imagenet', input_shape=(384,384,3))\n",
                "x = model_finetuned.output\n",
                "x = GlobalAveragePooling2D()(x)\n",
                "x = Dense(128, activation=\"relu\")(x)\n",
                "x = Dense(64, activation=\"relu\")(x)\n",
                "predictions = Dense(4, activation=\"softmax\")(x)\n",
                "model_finetuned = Model(inputs=model_finetuned.input, outputs=predictions)\n",
                "model_finetuned.compile(optimizer='adam',\n",
                "                  loss = 'categorical_crossentropy',\n",
                "                  metrics=['accuracy'])\n",
                "model_finetuned.summary()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from keras.callbacks import ReduceLROnPlateau"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "history_1 = model_finetuned.fit_generator(train_generator,                                    \n",
                "                                  steps_per_epoch=100, \n",
                "                                  epochs=25,validation_data=val_generator,validation_steps=100\n",
                "                                  ,verbose=1,callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.3,patience=3, min_lr=0.000001)],use_multiprocessing=False,\n",
                "               shuffle=True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = go.Figure(data=[\n",
                "    go.Line(name='train_acc', x=history_1.epoch, y=history_1.history['accuracy']),\n",
                "    go.Line(name='Val_acc', x=history_1.epoch, y=history_1.history['val_accuracy'])])\n",
                "\n",
                "fig.update_layout(\n",
                "    title=\"Accuracy\",\n",
                "    xaxis_title=\"epoch\",\n",
                "    yaxis_title=\"accuracy\",\n",
                "    font=dict(\n",
                "        family=\"Courier New, monospace\",\n",
                "        size=18,\n",
                "        color=\"#7f7f7f\"\n",
                "    ))\n",
                "fig"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "SUB_PATH = \"../input/plant-pathology-2020-fgvc7/sample_submission.csv\"\n",
                "\n",
                "sub = pd.read_csv(SUB_PATH)\n",
                "probs_RESNET = model_finetuned.predict(test_generator, verbose=1)\n",
                "sub.loc[:, 'healthy':] = probs_RESNET\n",
                "sub.to_csv('submission_RESNET.csv', index=False)\n",
                "sub.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data"
            ],
            "source": [
                "import os\n",
                "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
                "import sys\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow as tf\n",
                "import seaborn as sns\n",
                "from PIL import Image\n",
                "from sklearn.model_selection import train_test_split\n",
                "train_dir = '../input/severstal-steel-defect-detection/' \n",
                "train_image_dir = os.path.join(train_dir, 'train_images') \n",
                "train = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
                "train['ClassId_EncodedPixels'] = train.apply(lambda row: (row['ClassId'], row['EncodedPixels']), axis = 1)\n",
                "grouped_EncodedPixels = train.groupby('ImageId')['ClassId_EncodedPixels'].apply(list)\n",
                "img_h=256\n",
                "img_w=256\n",
                "k_size=3\n",
                "batch_size=10\n",
                "epochs=1\n",
                "train=train.dropna(subset=['EncodedPixels'])\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def rle2mask(rle,img):\n",
                "\twidth=img.shape[0]\n",
                "\tlength=img.shape[1]\n",
                "\tmask= np.zeros(width*length).astype(np.uint8)\n",
                "\trle=rle.split()\n",
                "\tstarts = rle[0::2]\n",
                "\tlengths = rle[1::2]\n",
                "\tfor i in range(len(starts)):\n",
                "\t\tmask[int(starts[i]):(int(starts[i])+int(lengths[i]))]=1\n",
                "\treturn np.flipud(np.rot90(mask.reshape(length, width), k=1 ) )\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(len(train.ImageId))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(10,10))\n",
                "sns.barplot(x=train.ClassId.value_counts().index,y=train.ClassId.value_counts())\n",
                "plt.ylabel('')\n",
                "plt.xlabel('ClassId')\n",
                "plt.title(\"Number of images for each class\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "fig=plt.figure(figsize=(20,10))\n",
                "col=2\n",
                "row=5\n",
                "for i in range(1,11):\n",
                "\tfig.add_subplot(row,col,i)\n",
                "\tGraph=train['ImageId'][i]\n",
                "\timg_new=cv2.imread(\"../input/severstal-steel-defect-detection/train_images/\"+Graph)\n",
                "\timg_new= cv2.cvtColor(img_new,cv2.COLOR_BGR2RGB)\n",
                "\tmask = rle2mask(train['EncodedPixels'].iloc[i], img_new)\n",
                "\timg_new[mask==1,0] = 255\n",
                "\tplt.imshow(img_new)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def mask2rle(img):\n",
                "\timg_flt= img.T.flatten()\n",
                "\timg_flt= np.concatenate([[0],img_flt,[0]]) \n",
                "\truns = np.where(img_flt[1:] != img_flt[:-1])[0]  \n",
                "\truns[1::2] -= runs[::2]\n",
                "\treturn ' '.join(str(x) for x in runs)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def ResUNet(img_h,img_w):\n",
                "\tf=[16,32,64,128,256]\n",
                "\tinputs=Input((img_h,img_w,1))\n",
                "\n",
                "\te0=inputs\n",
                "\te1=stem(e0,f[0])\n",
                "\te2=residual_block(e1,f[1],strides=2)\n",
                "\te3=residual_block(e2,f[2],strides=2)\n",
                "\te4=residual_block(e3,f[3],strides=2)\n",
                "\te5=residual_block(e4,f[4],strides=2)\n",
                "\n",
                "\tb0=conv_block(e5,f[4],strides=1)\n",
                "\tb1=conv_block(b0,f[4],strides=1)\n",
                "\n",
                "\tu1=upsample_concat_block(b1,e4)\n",
                "\td1=residual_block(u1,f[4])\n",
                "\n",
                "\tu2=upsample_concat_block(d1,e3)\n",
                "\td2=residual_block(u2,f[3])\n",
                "\n",
                "\tu3=upsample_concat_block(d2,e2)\n",
                "\td3=residual_block(u3,f[2])\n",
                "\n",
                "\tu4=upsample_concat_block(d3,e1)\n",
                "\td4=residual_block(u4,f[1])\n",
                "\n",
                "\toutputs=tf.keras.layers.Conv2D(4,(1,1),padding='same',activation='sigmoid')(d4)\n",
                "\tmodel=tf.keras.models.Model(inputs,outputs)\n",
                "\treturn model\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def dsc(y_true, y_pred):\n",
                "    smooth = 1.\n",
                "    y_true_f = tf.keras.layers.Flatten()(y_true)\n",
                "    y_pred_f = tf.keras.layers.Flatten()(y_pred)\n",
                "    intersection = reduce_sum(y_true_f * y_pred_f)\n",
                "    score = (2. * intersection + smooth) / (reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth)\n",
                "    return score\n",
                "\n",
                "def dice_loss(y_true, y_pred):\n",
                "    loss = 1 - dsc(y_true, y_pred)\n",
                "    return loss\n",
                "\n",
                "def bce_dice_loss(y_true, y_pred):\n",
                "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
                "    return loss\n",
                "def tversky(y_true, y_pred, smooth=1e-6):\n",
                "    y_true_pos = tf.keras.layers.Flatten()(y_true)\n",
                "    y_pred_pos = tf.keras.layers.Flatten()(y_pred)\n",
                "    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)\n",
                "    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n",
                "    false_pos = tf.reduce_sum((1-y_true_pos)*y_pred_pos)\n",
                "    alpha = 0.7\n",
                "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
                "\n",
                "def tversky_loss(y_true, y_pred):\n",
                "    return 1 - tversky(y_true,y_pred)\n",
                "\n",
                "def focal_tversky_loss(y_true,y_pred):\n",
                "    pt_1 = tversky(y_true, y_pred)\n",
                "    gamma = 0.75\n",
                "    return tf.keras.backend.pow((1-pt_1), gamma)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model = ResUNet(img_h=img_h, img_w=img_w)\n",
                "adam = tf.keras.optimizers.Adam(lr = 0.05, epsilon = 0.1)\n",
                "model.compile(optimizer=adam, loss=focal_tversky_loss, metrics=[tversky])"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import seaborn as sns\n",
                "import math\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(\"../input/house-prices-advanced-regression-techniques\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n",
                "test=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "train"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(8,8))\n",
                "train.isnull().sum().sort_values(ascending=False)[:19].sort_values().plot.barh(color='plum')\n",
                "plt.title('counts of missing value in the train data',size=20)\n",
                "plt.xlabel('counts')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,15))\n",
                "corr = train.corr()\n",
                "sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns)\n",
                "plt.title(\"correlation plot\",size=28)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "train.MSSubClass.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 MSSubClass by counts\",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=train.MSSubClass.value_counts()\n",
                "groups=list(train.MSSubClass.value_counts().index)[:10]\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.15,0.8)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,1,figsize=(15,15))\n",
                "sns.boxplot(x=\"MSSubClass\", y=\"SalePrice\", data=train,ax=ax[0])\n",
                "ax[0].set_title(\"Boxplot of Price for MSSubClass\",size=20)\n",
                "\n",
                "train=train[train.SalePrice<=400000]\n",
                "sns.boxplot(x=\"MSSubClass\", y=\"SalePrice\", data=train,ax=ax[1])\n",
                "ax[1].set_title(\"Boxplot of Price for MSSubClass(price<=400000)\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "train.MSZoning.value_counts().sort_values(ascending=False).plot(kind='bar',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"bar chart for MSZoning\",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "ax[0].tick_params(axis='x',rotation=360)\n",
                "\n",
                "sns.boxplot(x=\"MSZoning\", y=\"SalePrice\", data=train,ax=ax[1])\n",
                "ax[1].set_title(\"Boxplot of Price for MSZoning\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "train.Neighborhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 Neighborhood by counts\",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=train.Neighborhood.value_counts()\n",
                "groups=list(train.Neighborhood.value_counts().index)[:10]\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.15,0.8)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(figsize=(25,15))\n",
                "sns.boxplot(x=\"Neighborhood\", y=\"SalePrice\", data=train,ax=ax)\n",
                "ax.set_title(\"Boxplot of Price for Neighborhood\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "train.groupby(['OverallQual'])['Id'].agg('count').plot(kind='bar',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 OverallQual by counts\",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=train.groupby(['OverallQual'])['Id'].agg('count')\n",
                "groups=list(train.groupby(['OverallQual'])['Id'].agg('count').index)\n",
                "counts=list(count)\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=(\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.20,0.8)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(figsize=(25,15))\n",
                "sns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=train,ax=ax)\n",
                "ax.set_title(\"Boxplot of Price for OverallQual\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "train.groupby(['OverallCond'])['Id'].agg('count').plot(kind='bar',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 OverallCond by counts\",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=train.groupby(['OverallCond'])['Id'].agg('count')\n",
                "groups=list(train.groupby(['OverallCond'])['Id'].agg('count').index)\n",
                "counts=list(count)\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=(\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.20,0.8)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(figsize=(25,15))\n",
                "sns.boxplot(x=\"OverallCond\", y=\"SalePrice\", data=train,ax=ax)\n",
                "ax.set_title(\"Boxplot of Price for OverallCond\",size=20)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "data_tree=train[['MSSubClass','MSZoning','Neighborhood','OverallQual','OverallCond','SalePrice']]\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "labelencoder = LabelEncoder()\n",
                "data_tree['MSZoning_new'] = labelencoder.fit_transform(data_tree['MSZoning'])\n",
                "data_tree['Neighborhood_new'] = labelencoder.fit_transform(data_tree['Neighborhood'])\n",
                "data_tree.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "x_train,x_test,y_train,y_test=train_test_split(data_tree[['MSSubClass','MSZoning_new','Neighborhood_new','OverallQual','OverallCond']],data_tree[['SalePrice']],test_size=0.1,random_state=300)\n",
                "tree=DecisionTreeRegressor(criterion='mse',max_depth=4,random_state=0)\n",
                "tree=tree.fit(x_train,y_train)\n",
                "y=y_test['SalePrice']\n",
                "predict=tree.predict(x_test)\n",
                "print(np.mean(abs(np.multiply(np.array(y_test.T-predict),np.array(1/y_test)))))"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_tree_for_test=test[['Id','MSSubClass','MSZoning','Neighborhood','OverallQual','OverallCond']]\n",
                "data_tree_for_test.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_tree_for_test[data_tree_for_test.MSZoning.isnull()==True]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_tree_for_test.MSZoning.value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_tree_for_test.MSZoning[[455,756,790,1444]]='RL'"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import LabelEncoder\n",
                "labelencoder = LabelEncoder()\n",
                "data_tree_for_test['MSZoning_new'] = labelencoder.fit_transform(data_tree_for_test['MSZoning'])\n",
                "data_tree_for_test['Neighborhood_new'] = labelencoder.fit_transform(data_tree_for_test['Neighborhood'])\n",
                "data_tree_for_test.head()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "predict_test=tree.predict(data_tree_for_test[['MSSubClass','MSZoning_new','Neighborhood_new','OverallQual','OverallCond']])\n",
                "submit=pd.DataFrame({'Id':data_tree_for_test.Id,'SalePrice':predict_test})\n",
                "submit.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submit.to_csv('submission.csv',index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import lxml\n",
                "import os\n",
                "import urllib\n",
                "import sys\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import cv2\n",
                "import csv\n",
                "import multiprocessing\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/iwildcam-2019-fgvc6')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train=pd.read_csv(\"../input/iwildcam-2019-fgvc6/train.csv\")\n",
                "test=pd.read_csv(\"../input/iwildcam-2019-fgvc6/test.csv\")\n",
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(os.listdir('../input/iwildcam-2019-fgvc6/train_images'))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(train.id)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "img=[]\n",
                "filename=train.id[:10000]\n",
                "label=train.category_id[:10000]\n",
                "for file in filename:\n",
                "    image=cv2.imread(\"../input/iwildcam-2019-fgvc6/train_images/\"+file+'.jpg')\n",
                "    res=cv2.resize(image,(32,32))\n",
                "    img.append(res)\n",
                "img=np.array(img)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,15))\n",
                "for i in range(9):\n",
                "    plt.subplot(3,3,i+1)\n",
                "    plt.imshow(img[i])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "np.random.seed(921)\n",
                "from keras.utils import np_utils\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Convolution2D,Dense,MaxPool2D,Activation,Dropout,Flatten\n",
                "from keras.optimizers import Adam\n",
                "from sklearn.model_selection import train_test_split\n",
                "from keras.layers.normalization import BatchNormalization\n",
                "X_train,X_test,y_train,y_test=train_test_split(img,label,test_size=0.2)\n",
                "del img\n",
                "y_train=y_train.astype(int)\n",
                "y_test=y_test.astype(int)\n",
                "y_train=np.array(y_train).reshape(-1,1)\n",
                "y_test=np.array(y_test).reshape(-1,1)\n",
                "X_train=X_train.reshape(-1,32,32,3)/255 #Normalize\n",
                "X_test=X_test.reshape(-1,32,32,3)/255\n",
                "y_train=np_utils.to_categorical(y_train,num_classes=max(label)+1)\n",
                "y_test=np_utils.to_categorical(y_test,num_classes=max(label)+1)"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model=Sequential()\n",
                "model.add(Convolution2D(filters=32,kernel_size=(3,3),input_shape=(32,32,3),padding='same'))\n",
                "model.add(BatchNormalization())\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(rate=0.35))\n",
                "\n",
                "model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n",
                "\n",
                "model.add(Convolution2D(filters=64,kernel_size=(3,3),padding='same'))\n",
                "model.add(BatchNormalization())\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(rate=0.45))\n",
                "\n",
                "model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n",
                "\n",
                "model.add(Flatten())\n",
                "\n",
                "model.add(Dense(1024,activation='relu'))\n",
                "model.add(BatchNormalization())\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(rate=0.75))\n",
                "\n",
                "model.add(Dense(max(label)+1,activation='softmax'))\n",
                "\n",
                "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
                "\n",
                "train_history=model.fit(X_train,y_train,validation_split=0.2,epochs=20,batch_size=128,verbose=1)\n",
                "accuracy=model.evaluate(X_test,y_test,verbose=1)\n",
                "print(\"test accuracy:\",accuracy[1])#accuracy for test set\n",
                "\n",
                "\n",
                "\n",
                "def show_train_history(train_history,train,validation):\n",
                "\tplt.plot(train_history.history[train])\n",
                "\tplt.plot(train_history.history[validation])\n",
                "\tplt.title('Train History')\n",
                "\tplt.ylabel('train')\n",
                "\tplt.xlabel('Epoch')\n",
                "\tplt.legend(['train','validation'],loc='upper left')\n",
                "\tplt.show()\n",
                "\n",
                "show_train_history(train_history,'acc','val_acc') #acc:accuracy for training set. val_acc:accuracy for validation."
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "prediction=model.predict_classes(X_test)\n",
                "print(prediction[0:10])"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "img_test=[]\n",
                "filename_test=test.id[:10000]\n",
                "for file in filename_test:\n",
                "    image=cv2.imread(\"../input/iwildcam-2019-fgvc6/test_images/\"+file+'.jpg')\n",
                "    res=cv2.resize(image,(32,32))\n",
                "    img_test.append(res)\n",
                "img_test=np.array(img_test)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "prediction=model.predict_classes(img_test)\n",
                "print(prediction[0:10])"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submit=pd.DataFrame({'Id':filename_test,'Predicted':prediction})\n",
                "submit.to_csv('submission.csv',index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt \n",
                "import seaborn as sns\n",
                "import os\n",
                "import geopandas as gpd\n",
                "import folium\n",
                "from folium import plugins\n",
                "import datetime \n",
                "import re"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(\"../input/autotel-shared-car-locations\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "check_results"
            ],
            "source": [
                "data=pd.read_csv(\"../input/autotel-shared-car-locations/sample_table.csv\")\n",
                "data.head()\n",
                "data=data[data['total_cars']>0]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data['date']=data.timestamp.apply(lambda x:x.split(' ')[0])\n",
                "pd.to_datetime(data['date'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(12,12))\n",
                "y=data.groupby('date')['total_cars'].agg('sum').index\n",
                "x=data.groupby('date')['total_cars'].agg('sum')\n",
                "sns.barplot(x=x,y=y)\n",
                "plt.title(\"total_car in the data by date\",size=24)\n",
                "plt.xlabel('cars')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "index=data.groupby('date')['timestamp'].agg('count').index\n",
                "plt.figure(figsize=(15,100))\n",
                "for i in range(len(index)):\n",
                "    plt.subplot(15,2,i+1)\n",
                "    sns.scatterplot(x='latitude',y='longitude',alpha=0.01,data=data[data.date==index[i]])\n",
                "    plt.title('total_car '+index[i],size=20)\n",
                "    plt.xlim(32,32.17)\n",
                "    plt.ylim(34.74,34.85)\n",
                "    "
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data.carsList.apply(lambda x:x.strip('[]'))\n",
                "data['carsList']=data.carsList.apply(lambda x:x.strip('[]'))\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_37=data[data.carsList.apply(lambda x:re.match('37',x)!=None)]\n",
                "data_37_new=data_37[data.date==data_37.date.unique()[0]].drop_duplicates(subset=['longitude'])\n",
                "for i in range(1,len(data_37.date.unique())):\n",
                "    data_37_concate=data_37[data.date==data_37.date.unique()[i]].drop_duplicates(subset=['longitude'])\n",
                "    data_37_new=pd.concat([data_37_new,data_37_concate],axis=0)\n",
                "\n",
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_37_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "data_37_cars_map=plugins.MarkerCluster().add_to(data_37_map)\n",
                "for lat,lon,label in zip(data_37_new.latitude,data_37_new.longitude,data_37_new.timestamp):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_37_cars_map)\n",
                "data_37_map.add_child(data_37_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_37=data[data.carsList.apply(lambda x:re.match('37',x)!=None)]\n",
                "data_37_new=data_37[data.date==data_37.date.unique()[0]].drop_duplicates(subset=['latitude'])\n",
                "for i in range(1,len(data_37.date.unique())):\n",
                "    data_37_concate=data_37[data.date==data_37.date.unique()[i]].drop_duplicates(subset=['latitude'])\n",
                "    data_37_new=pd.concat([data_37_new,data_37_concate],axis=0)\n",
                "\n",
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_37_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "data_37_cars_map=plugins.MarkerCluster().add_to(data_37_map)\n",
                "for lat,lon,label in zip(data_37_new.latitude,data_37_new.longitude,data_37_new.timestamp):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_37_cars_map)\n",
                "data_37_map.add_child(data_37_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_total_cars_1=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[:2000]\n",
                "data_total_cars_1.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_1_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_1['label']=data_total_cars_1.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_1_cars_map=plugins.MarkerCluster().add_to(data_total_cars_1_map)\n",
                "for lat,lon,label in zip(data_total_cars_1.latitude,data_total_cars_1.longitude,data_total_cars_1.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_1_cars_map)\n",
                "data_total_cars_1_map.add_child(data_total_cars_1_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_total_cars_2=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[2000:4000]\n",
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_2_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_2['label']=data_total_cars_2.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_2_cars_map=plugins.MarkerCluster().add_to(data_total_cars_2_map)\n",
                "for lat,lon,label in zip(data_total_cars_2.latitude,data_total_cars_2.longitude,data_total_cars_2.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_2_cars_map)\n",
                "data_total_cars_2_map.add_child(data_total_cars_2_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_total_cars_3=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[4000:6000]\n",
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_3_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_3['label']=data_total_cars_3.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_3_cars_map=plugins.MarkerCluster().add_to(data_total_cars_3_map)\n",
                "for lat,lon,label in zip(data_total_cars_3.latitude,data_total_cars_3.longitude,data_total_cars_3.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_3_cars_map)\n",
                "data_total_cars_3_map.add_child(data_total_cars_3_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_total_cars_4=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[6000:8000]\n",
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_4_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_4['label']=data_total_cars_4.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_4_cars_map=plugins.MarkerCluster().add_to(data_total_cars_4_map)\n",
                "for lat,lon,label in zip(data_total_cars_4.latitude,data_total_cars_4.longitude,data_total_cars_4.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_4_cars_map)\n",
                "data_total_cars_4_map.add_child(data_total_cars_4_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_total_cars_5=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[8000:-1]\n",
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_5_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_5['label']=data_total_cars_5.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_5_cars_map=plugins.MarkerCluster().add_to(data_total_cars_5_map)\n",
                "for lat,lon,label in zip(data_total_cars_5.latitude,data_total_cars_5.longitude,data_total_cars_5.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_5_cars_map)\n",
                "data_total_cars_5_map.add_child(data_total_cars_5_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_total_cars_6=data[data.total_cars==2].drop_duplicates(subset=['latitude'])\n",
                "data_total_cars_6.head()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_6_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_6['label']=data_total_cars_6.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_6_cars_map=plugins.MarkerCluster().add_to(data_total_cars_6_map)\n",
                "for lat,lon,label in zip(data_total_cars_6.latitude,data_total_cars_6.longitude,data_total_cars_6.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_6_cars_map)\n",
                "data_total_cars_6_map.add_child(data_total_cars_6_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_total_cars_7=data[data.total_cars>=3].drop_duplicates(subset=['latitude'])\n",
                "data_total_cars_7.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_7_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_7['label']=data_total_cars_7.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_7_cars_map=plugins.MarkerCluster().add_to(data_total_cars_7_map)\n",
                "for lat,lon,label in zip(data_total_cars_7.latitude,data_total_cars_7.longitude,data_total_cars_7.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_7_cars_map)\n",
                "data_total_cars_7_map.add_child(data_total_cars_7_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import lxml\n",
                "import os\n",
                "import urllib\n",
                "import sys\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import cv2\n",
                "import csv\n",
                "import multiprocessing\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "##https://bulkresizephotos.com/zh-tw <- This website can change your image to 32*32 pixels\n",
                "new_train=pd.read_csv('../input/landmark-id-from-0-to-499/new_train_id0_499.csv')\n",
                "filename=os.listdir(\"../input/graph-id0-499/landgraphnew_0_499\")\n",
                "filename.sort(key=lambda x:int(x[:-4]))\n",
                "img=[]\n",
                "for file in filename:\n",
                "\timg.append(np.array(Image.open(\"../input/graph-id0-499/landgraphnew_0_499/\"+file)))\n",
                "img=np.array(img)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "new_train.groupby(['landmark_id']).agg('count').sort_values(by='id',ascending=False).style.background_gradient(cmap='Blues')\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "np.random.seed(1337)\n",
                "from keras.utils import np_utils\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Convolution2D,Dense,MaxPool2D,Activation,Dropout,Flatten\n",
                "from keras.optimizers import Adam\n",
                "from sklearn.model_selection import train_test_split\n",
                "from keras.layers.normalization import BatchNormalization\n",
                "X_train,X_test,y_train,y_test=train_test_split(img,new_train['landmark_id'],test_size=0.2)\n",
                "y_train=y_train.astype(int)\n",
                "y_test=y_test.astype(int)\n",
                "y_train=np.array(y_train).reshape(-1,1)\n",
                "y_test=np.array(y_test).reshape(-1,1)\n",
                "X_train=X_train.reshape(-1,32,32,3)/255 #Normalize\n",
                "X_test=X_test.reshape(-1,32,32,3)/255\n",
                "y_train=np_utils.to_categorical(y_train,num_classes=500)\n",
                "y_test=np_utils.to_categorical(y_test,num_classes=500)#landmark_id is from 0 to 499"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model=Sequential()\n",
                "model.add(Convolution2D(filters=32,kernel_size=(3,3),input_shape=(32,32,3),padding='same'))\n",
                "model.add(BatchNormalization())\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(rate=0.35))\n",
                "\n",
                "model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n",
                "\n",
                "model.add(Convolution2D(filters=64,kernel_size=(3,3),padding='same'))\n",
                "model.add(BatchNormalization())\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(rate=0.45))\n",
                "\n",
                "model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n",
                "\n",
                "model.add(Flatten())\n",
                "\n",
                "model.add(Dense(1024,activation='relu'))\n",
                "model.add(BatchNormalization())\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(rate=0.75))\n",
                "\n",
                "model.add(Dense(500,activation='softmax'))\n",
                "\n",
                "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
                "\n",
                "train_history=model.fit(X_train,y_train,validation_split=0.2,epochs=100,batch_size=128,verbose=1)\n",
                "accuracy=model.evaluate(X_test,y_test,verbose=1)\n",
                "print(\"test accuracy:\",accuracy[1])#accuracy for test set\n",
                "\n",
                "\n",
                "\n",
                "def show_train_history(train_history,train,validation):\n",
                "\tplt.plot(train_history.history[train])\n",
                "\tplt.plot(train_history.history[validation])\n",
                "\tplt.title('Train History')\n",
                "\tplt.ylabel('train')\n",
                "\tplt.xlabel('Epoch')\n",
                "\tplt.legend(['train','validation'],loc='upper left')\n",
                "\tplt.show()\n",
                "\n",
                "show_train_history(train_history,'acc','val_acc') #acc:accuracy for training set. val_acc:accuracy for validation."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from matplotlib.pyplot import figure, show\n",
                "from matplotlib.ticker import MaxNLocator\n",
                "import os \n",
                "import time\n",
                "from matplotlib import cm\n",
                "import pylab as pl"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data",
                "process_data"
            ],
            "source": [
                "data=pd.read_csv(\"../input/crimes-in-boston/crime.csv\",encoding=\"gbk\")\n",
                "data=data.loc[(data['Lat']>35)&(data['Long']< -60)] #remove NA from 'Lat' and 'Long'\n",
                "data=data.dropna(subset=[\"STREET\"])\n",
                "columns=['OFFENSE_CODE_GROUP']\n",
                "for j in columns:\n",
                "\tprint(j,data[j].unique())\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(data.isnull().sum())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "count=data['OFFENSE_CODE_GROUP'].value_counts()\n",
                "groups=list(data['OFFENSE_CODE_GROUP'].value_counts().index)[:9]\n",
                "counts=list(count[:9])\n",
                "counts.append(count.agg(sum)-count[:9].agg('sum'))\n",
                "\n",
                "groups.append('other_type')\n",
                "type_dict={\"group\":groups,\n",
                "          \"counts\":counts}\n",
                "type_dict=pd.DataFrame(type_dict)\n",
                "qx = type_dict.plot(kind='pie', figsize=(10,7), y='counts', labels=groups,\n",
                "             autopct='%1.1f%%', pctdistance=0.9, radius=1.2)\n",
                "plt.legend(loc=0, bbox_to_anchor=(0.95,0.6)) \n",
                "\n",
                "plt.title('Top 10 for crime type', weight='bold', size=14,y=1.08)\n",
                "plt.axis('equal')\n",
                "plt.ylabel('')\n",
                "plt.show()\n",
                "plt.clf()\n",
                "plt.close()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(data['OFFENSE_CODE_GROUP'].value_counts())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.bar(index.value_counts().index,index.value_counts())\n",
                "plt.xlabel(\"Crime type\")\n",
                "plt.ylabel(\"Count\")\n",
                "plt.title(\"Counting for Crime type\")\n",
                "plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "Crime_year=pd.Index(data['YEAR'])\n",
                "ax =figure().gca()\n",
                "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
                "ax.bar(Crime_year.value_counts().index,Crime_year.value_counts())\n",
                "plt.xlabel(\"Year\")\n",
                "plt.ylabel(\"Count\")\n",
                "plt.title(\"Counting the number for Crime (Year)\")\n",
                "plt.show(ax)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "\n",
                "data.groupby(['YEAR','MONTH'])['OFFENSE_CODE_GROUP'].agg('count').unstack('YEAR')\n",
                "fig,ax=plt.subplots(figsize=(15,6))\n",
                "data.groupby(['MONTH','YEAR'])['OFFENSE_CODE_GROUP'].agg('count').unstack().plot(ax=ax)\n",
                "plt.title(\"Counting the number for Crime (month)\")\n",
                "plt.grid(True)\n",
                "\n",
                "data.groupby(['YEAR','MONTH'])['OFFENSE_CODE_GROUP'].agg('count').unstack('YEAR')\n",
                "fig,ax=plt.subplots(figsize=(18,12))\n",
                "data.groupby(['MONTH','YEAR'])['OFFENSE_CODE_GROUP'].agg('count').unstack().plot(kind='bar',ax=ax)\n",
                "pl.xticks(rotation=360)\n",
                "plt.title(\"Counting the number for Crime (month)\")\n",
                "plt.grid(True)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "import geopandas as gpd\n",
                "import folium\n",
                "\n",
                "\n",
                "incidents=folium.map.FeatureGroup()\n",
                "\n",
                "#for lat,lon, in zip(data.Lat,data.Long):\n",
                "#\tincidents.add_child(folium.CircleMarker([lat,lon],radius=7,color='yellow',fill=True,fill_color='red',fill_opacity=0.4))\n",
                "\n",
                "Lat=42.3\n",
                "Lon=-71.1\n",
                "#boston_map=folium.Map([Lat,Lon],zoom_start=12)\n",
                "#boston_map.add_child(incidents)\n",
                "#boston_map.save(\"mymap.html\")\n",
                "\n",
                "from folium import plugins\n",
                "\n",
                "data1=data[data['YEAR']==2015][0:2000]\n",
                "filename=\"Crime2015\"\n",
                "boston_map=folium.Map([Lat,Lon],zoom_start=12)\n",
                "incidents2=plugins.MarkerCluster().add_to(boston_map)\n",
                "for lat,lon,label in zip(data1.Lat,data1.Long,data1.OFFENSE_CODE_GROUP):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(incidents2)\n",
                "boston_map.add_child(incidents2)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data1=data[data['YEAR']==2016][0:2000]\n",
                "filename=\"Crime2016\"\n",
                "boston_map=folium.Map([Lat,Lon],zoom_start=12)\n",
                "incidents2=plugins.MarkerCluster().add_to(boston_map)\n",
                "for lat,lon,label in zip(data1.Lat,data1.Long,data1.OFFENSE_CODE_GROUP):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(incidents2)\n",
                "boston_map.add_child(incidents2)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data1=data[data['YEAR']==2017][0:2000]\n",
                "filename=\"Crime2017\"\n",
                "boston_map=folium.Map([Lat,Lon],zoom_start=12)\n",
                "incidents2=plugins.MarkerCluster().add_to(boston_map)\n",
                "for lat,lon,label in zip(data1.Lat,data1.Long,data1.OFFENSE_CODE_GROUP):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(incidents2)\n",
                "boston_map.add_child(incidents2)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data1=data[data['YEAR']==2018][0:2000]\n",
                "filename=\"Crime2018\"\n",
                "boston_map=folium.Map([Lat,Lon],zoom_start=12)\n",
                "incidents2=plugins.MarkerCluster().add_to(boston_map)\n",
                "for lat,lon,label in zip(data1.Lat,data1.Long,data1.OFFENSE_CODE_GROUP):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(incidents2)\n",
                "boston_map.add_child(incidents2)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import seaborn as sns\n",
                "import math\n",
                "import datetime"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(\"../input/pga-tour-20102018-data/\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data_2019=pd.read_csv(\"../input/pga-tour-20102018-data/2019_data.csv\")\n",
                "data_2019.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(data_2019)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_2019.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data_2019['year']=data_2019.Date.apply(lambda x:x.split('-')[0])\n",
                "data_2019['month']=data_2019.Date.apply(lambda x:x.split('-')[1])\n",
                "data_2019['day']=data_2019.Date.apply(lambda x:x.split('-')[2])\n",
                "data_2019.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_2019.Statistic.unique()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "type_stat=pd.DataFrame({'type':data_2019.Statistic.value_counts().sort_values(ascending=False)[:10].index,'value':data_2019.Statistic.value_counts().sort_values(ascending=False)[:10].values})\n",
                "type_stat"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "\n",
                "qx = type_stat.plot(kind='pie', figsize=(10,7), y='value', labels=list(type_stat.type),autopct='%1.1f%%', pctdistance=0.9, radius=1.2)\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.10,1.0)) \n",
                "plt.title('Top 10 Statistic by number', weight='bold', size=14,y=1.08)\n",
                "plt.axis('equal')\n",
                "plt.ylabel('')\n",
                "plt.show()\n",
                "plt.clf()\n",
                "plt.close()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "DD=data_2019[data_2019.Variable=='Driving Distance - (TOTAL DISTANCE)'].iloc[:,[0,4]]\n",
                "DD['Value_new']=DD['Value'].apply(lambda x:''.join(x.split(',')))\n",
                "DD.Value_new=DD.Value_new.astype(int)\n",
                "DD=DD.drop(columns='Value')\n",
                "plt.figure(figsize=(22,15))\n",
                "plt.subplot(2,2,1)\n",
                "DD.groupby('Player Name')['Value_new'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='darkorange')\n",
                "plt.title(\"Top 10 player by Total Driving Distance\",size=20)\n",
                "plt.xlabel('meters')\n",
                "plt.ylabel('')\n",
                "plt.subplot(2,2,2)\n",
                "DD=data_2019[data_2019.Variable=='Driving Distance - (AVG.)'].iloc[:,[0,4]]\n",
                "DD.Value=DD.Value.astype(float)\n",
                "DD.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='navy')\n",
                "plt.title(\"Top 10 player by Average Driving Distance\",size=20)\n",
                "plt.xlabel('meters')\n",
                "plt.ylabel('')\n",
                "plt.subplot(2,2,3)\n",
                "DD=data_2019[data_2019.Variable=='Driving Distance - (ROUNDS)'].iloc[:,[0,4]]\n",
                "DD.Value=DD.Value.astype(float)\n",
                "DD.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='purple')\n",
                "plt.title(\"Top 10 player by Rounds Driving Distance\",size=20)\n",
                "plt.xlabel('meters')\n",
                "plt.ylabel('')\n",
                "plt.subplot(2,2,4)\n",
                "DD=data_2019[data_2019.Variable=='Driving Distance - (TOTAL DRIVES)'].iloc[:,[0,4]]\n",
                "DD.Value=DD.Value.astype(float)\n",
                "DD.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh()\n",
                "plt.title(\"Top 10 player by TOTAL DRIVES Driving Distance\",size=20)\n",
                "plt.xlabel('meters')\n",
                "plt.ylabel('')\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "DD=data_2019[data_2019.Variable=='Driving Distance - (TOTAL DISTANCE)'].iloc[:,[0,4,6]]\n",
                "DD['Value_new']=DD['Value'].apply(lambda x:''.join(x.split(',')))\n",
                "DD.Value_new=DD.Value_new.astype(int)\n",
                "DD=DD.drop(columns='Value')\n",
                "plt.figure(figsize=(22,22))\n",
                "for i in range(len(DD.month.unique())):\n",
                "    DD_month=DD[DD.month==DD.month.unique()[i]]\n",
                "    plt.subplot(4,2,i+1)\n",
                "    DD_month.groupby('Player Name')['Value_new'].max().sort_values(ascending=False)[:10].sort_values().plot(kind='barh')\n",
                "    plt.title(\"Top 10 player by Total Driving Distance in month\"+DD.month.unique()[i],size=20)\n",
                "    plt.xlabel('meters')\n",
                "    plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "TE=data_2019[data_2019.Variable=='Total Eagles - (ROUNDS)'].iloc[:,[0,4]]\n",
                "plt.figure(figsize=(22,15))\n",
                "\n",
                "plt.subplot(1,2,1)\n",
                "TE.Value=TE.Value.astype(int)\n",
                "TE.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='firebrick')\n",
                "plt.title(\"Top 10 player by Total Eagles(Rounds)\",size=20)\n",
                "plt.xlabel('counts')\n",
                "plt.ylabel('')\n",
                "\n",
                "plt.subplot(1,2,2)\n",
                "TE=data_2019[data_2019.Variable=='Total Eagles - (TOTAL)'].iloc[:,[0,4]]\n",
                "TE=TE.dropna(subset=['Value'])\n",
                "TE.Value=TE.Value.astype(int)\n",
                "TE.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='cyan')\n",
                "plt.xticks(np.linspace(0, 20, 5))\n",
                "plt.title(\"Top 10 player by Total Eagles(Total)\",size=20)\n",
                "plt.xlabel('counts')\n",
                "plt.ylabel('')\n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def TOTAvgRank_DD_DAP_TE_TB(player,month):\n",
                "    Ranktype=['Driving Distance - (TOTAL DRIVES)','Driving Accuracy Percentage - (%)','Total Eagles - (TOTAL)','Total Birdies - (TOTAL)']\n",
                "    B={}\n",
                "    for i in range(len(Ranktype)):\n",
                "        A=data_2019[data_2019.Variable==Ranktype[i]]\n",
                "        A=A[A.month==month]\n",
                "        A.dropna(subset=['Value'])\n",
                "        A.Value=A.Value.astype(float)\n",
                "        value=A.groupby(['Player Name'])['Value'].agg('mean')\n",
                "        rank=A.groupby(['Player Name'])['Value'].agg('mean').rank(method='min',ascending=False)\n",
                "        new=pd.DataFrame({'rank':rank,'value':value})\n",
                "        B['type'+str(i)]=str(Ranktype[i])\n",
                "        B['rank'+str(i)]=str(new[new.index==player].iloc[0,0])+\"/\"+str(max(rank))\n",
                "        B['value'+str(i)]=str(new[new.index==player].iloc[0,1])\n",
                "\n",
                "    return B\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(25,35))\n",
                "j=1\n",
                "for k in ['01','02','03','04','05','06','07','08']:\n",
                "    Aaron=TOTAvgRank_DD_DAP_TE_TB('Aaron Baddeley',str(k))\n",
                "    y=[]\n",
                "    x=[]\n",
                "    n=[]\n",
                "    for i in range(4):\n",
                "        r1,r2=Aaron['rank'+str(i)].split('/')\n",
                "        R=float(r1)/float(r2)\n",
                "        R=1-R\n",
                "        y.append(1.5+R*math.sin(math.pi/4+i*math.pi/2))\n",
                "        x.append(1.5+R*math.cos(math.pi/4+i*math.pi/2))\n",
                "        n.append(Aaron['type'+str(i)]+\" rank: \"+Aaron['rank'+str(i)])\n",
                "\n",
                "    x.append(x[0])\n",
                "    y.append(y[0])\n",
                "    plt.subplot(4,2,j)\n",
                "    plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "    for i, txt in enumerate(n):\n",
                "        plt.annotate(txt, (x[i]-0.2, y[i]))\n",
                "    plt.xlim(0.5,2.5)\n",
                "    plt.ylim(0.5,2.5)\n",
                "    plt.fill(x, y,\"coral\")\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"Aaron Baddeley's performance in month \"+str(k),size=18)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(25,35))\n",
                "j=1\n",
                "for k in ['01','02','03','04','05','06','07','08']:\n",
                "    Sungjae=TOTAvgRank_DD_DAP_TE_TB('Sungjae Im',str(k))\n",
                "    y=[]\n",
                "    x=[]\n",
                "    n=[]\n",
                "    for i in range(4):\n",
                "        r1,r2=Sungjae['rank'+str(i)].split('/')\n",
                "        R=float(r1)/float(r2)\n",
                "        R=1-R\n",
                "        y.append(1.5+R*math.sin(math.pi/4+i*math.pi/2))\n",
                "        x.append(1.5+R*math.cos(math.pi/4+i*math.pi/2))\n",
                "        n.append(Sungjae['type'+str(i)]+\" rank: \"+Sungjae['rank'+str(i)])\n",
                "\n",
                "    x.append(x[0])\n",
                "    y.append(y[0])\n",
                "    plt.subplot(4,2,j)\n",
                "    plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "    for i, txt in enumerate(n):\n",
                "        plt.annotate(txt, (x[i]-0.2, y[i]))\n",
                "    plt.xlim(0.5,2.5)\n",
                "    plt.ylim(0.5,2.5)\n",
                "    plt.fill(x, y,\"greenyellow\")\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"Sungjae Im's performance in month \"+str(k),size=18)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_2019[data_2019.Statistic=='Official World Golf Ranking'].Variable.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "OWGR_TP=data_2019[data_2019.Variable=='Official World Golf Ranking - (TOTAL POINTS)']\n",
                "OWGR_TP.Value=OWGR_TP.Value.astype(float)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2,figsize=(18,8))\n",
                "\n",
                "OWGR_TP.groupby('Player Name')['Value'].mean().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color='brown',ax=axes[0])\n",
                "axes[0].set_title('Top 10 player by Average of Total Points',size=20)\n",
                "axes[0].set_xlabel('points')\n",
                "axes[0].set_ylabel('')\n",
                "\n",
                "OWGE_TP10=OWGR_TP[OWGR_TP['Player Name'].isin(list(OWGR_TP.groupby('Player Name')['Value'].mean().sort_values(ascending=False)[:10].index[:10]))]\n",
                "OWGE_TP10.groupby(['Player Name','Date'])['Value'].agg('mean').unstack('Player Name').plot(ax=axes[1])\n",
                "axes[1].set_title('Official World Golf Ranking - (TOTAL POINTS)',size=20)\n",
                "axes[1].set_ylabel('Total points')\n",
                "axes[1].set_xlim([0,30])\n",
                "plt.xticks([0,5,10,15,20,25,29], ['2019-01-27','2019-03-03','2019-04-07','2019-05-12','2019-06-16','2019-07-21','2019-08-18'], rotation=0)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os \n",
                "import seaborn as sns\n",
                "import pylab as pl\n",
                "import geopandas as gpd\n",
                "import folium\n",
                "from folium import plugins\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(os.listdir(\"../input/city-lines/\"))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "cities=pd.read_csv(\"../input/city-lines/cities.csv\")\n",
                "stations=pd.read_csv(\"../input/city-lines/stations.csv\")\n",
                "tracks=pd.read_csv(\"../input/city-lines/tracks.csv\")\n",
                "lines=pd.read_csv(\"../input/city-lines/lines.csv\")\n",
                "track_lines=pd.read_csv(\"../input/city-lines/track_lines.csv\")\n",
                "print(\"cities size:\",len(cities))\n",
                "print(\"stations size:\",len(stations))\n",
                "print(\"tracks size:\",len(tracks))\n",
                "print(\"lines size:\",len(lines))\n",
                "print(\"track_lines size:\",len(track_lines))"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "stations=stations.dropna(subset=['closure','name','opening'])\n",
                "stations=stations[stations.closure>=9999]\n",
                "stations=stations[stations.opening>0]\n",
                "stations=stations[stations.opening<=2030]\n",
                "stations.columns=['id','stations_name','geometry','buildstart','opening','closure','city_id']\n",
                "stations['Long']=stations['geometry'].apply(lambda x: x.split('POINT(')[1].split(' ')[0])\n",
                "stations['Lat']=stations['geometry'].apply(lambda x: x.split('POINT(')[1].split(' ')[1].split(')')[0])\n",
                "id_country=pd.DataFrame({'city_id':cities.id,'country':cities.country,'name':cities.name})\n",
                "\n",
                "stations=pd.merge(stations,id_country)\n",
                "stations.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(18, 6))\n",
                "plt.subplot(1, 2, 1) \n",
                "stations.name.value_counts()[:10].sort_values().plot.barh()\n",
                "plt.title(\"Top 10 city by stations\",size=18)\n",
                "plt.xlabel(\"stations\")\n",
                "plt.subplot(1, 2, 2) \n",
                "stations.country.value_counts()[:10].sort_values().plot.barh()\n",
                "plt.title(\"Top 10 country by stations\",size=18)\n",
                "plt.xlabel(\"stations\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(figsize=(10,6))\n",
                "stations.groupby(['opening','country'])['stations_name'].agg('count').unstack().plot(ax=ax)\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.05,1.1))\n",
                "plt.xlabel('')\n",
                "plt.ylabel('stations')\n",
                "plt.title(\"The number of opening stations by year (all country)\",size=18)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(35,55))\n",
                "for i in range(len(stations.country.unique())):\n",
                "    plt.subplot(9,3,i+1)\n",
                "    stations[stations.country==stations.country.unique()[i]].name.value_counts().sort_values().plot.barh()\n",
                "    plt.xlabel('stations')\n",
                "    plt.title(\"The number of stations in \"+stations.country.unique()[i],size=20)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "map_all=folium.Map()\n",
                "stations_new=pd.DataFrame({\"Lat\":stations['Lat'],\"Long\":stations['Long']})\n",
                "map_all.add_child(plugins.HeatMap(data=stations_new))\n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tokyo_lines=lines[lines.city_id==114]\n",
                "tokyo_track_lines=track_lines[track_lines.city_id==114]\n",
                "tokyo_tracks=tracks[tracks.city_id==114].drop(columns=['buildstart','opening','closure','city_id'])\n",
                "tokyo_tracks.columns=['section_id','geometry','length']\n",
                "tokyo_track_lines=pd.merge(tokyo_track_lines,tokyo_tracks)\n",
                "tokyo_track_lines=tokyo_track_lines.drop(columns=['id','created_at','updated_at','city_id'])\n",
                "tokyo_track_lines.columns=['section_id','id','geometry','length']\n",
                "tokyo_lines=pd.merge(tokyo_track_lines,tokyo_lines)\n",
                "tokyo_stations=stations[stations['city_id']==114]\n",
                "tokyo_stations.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "x=[]\n",
                "y=[]\n",
                "z=[]\n",
                "for i in range(len(tokyo_lines)):\n",
                "    sp=tokyo_lines.iloc[i].geometry.split('(')[1].split(')')[0].split(',')\n",
                "    for j in range(len(sp)):\n",
                "        x.append(sp[j].split(' ')[0])\n",
                "        y.append(sp[j].split(' ')[1])\n",
                "        z.append(tokyo_lines.url_name[i])\n",
                "fix=pd.DataFrame({'x':x,'y':y,'z':z})\n",
                "fix['x']=fix['x'].astype(float)\n",
                "fix['y']=fix['y'].astype(float)\n",
                "plt.figure(figsize=(25, 25))\n",
                "plt.subplot(2, 2, 1) \n",
                "ax=sns.scatterplot(x=\"x\", y=\"y\", hue=\"z\",data=fix)\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.05,0.6))\n",
                "plt.title(\"lines for Tokyo\",size=20)\n",
                "ax.get_legend().remove()\n",
                "ax.set(xlabel='Longitude', ylabel='LATITUDE')\n",
                "plt.subplot(2,2,2)\n",
                "(tokyo_lines.groupby(['url_name'])['length'].sum()/1000).sort_values(ascending= False)[:10].sort_values().plot.barh()\n",
                "plt.ylabel(' ')\n",
                "plt.xlabel('length(km)')\n",
                "plt.title(\"Top 10 track by length\",size=20)\n",
                "plt.subplot(2,2,3)\n",
                "tokyo_stations.groupby(['opening'])['id'].agg('count').plot()\n",
                "plt.xlabel(' ')\n",
                "plt.ylabel('stations')\n",
                "plt.title(\"Number of opening stations by year\",size=20)\n",
                "plt.subplot(2,2,4)\n",
                "tokyo_lines.name.value_counts()[:10].sort_values().plot.barh()\n",
                "plt.xlabel('counts')\n",
                "plt.title(\"Top 10 line by number\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "\n",
                "stations_tokyo_2000=stations[stations['city_id']==114][0:2000]\n",
                "Long=139.75\n",
                "Lat=35.67\n",
                "tokyo_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "tokyo_stations_map=plugins.MarkerCluster().add_to(tokyo_map)\n",
                "for lat,lon,label in zip(stations_tokyo_2000.Lat,stations_tokyo_2000.Long,stations_tokyo_2000.stations_name):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(tokyo_stations_map)\n",
                "tokyo_map.add_child(tokyo_stations_map)\n",
                "\n",
                "tokyo_map\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "osaka_lines=lines[lines.city_id==91]\n",
                "osaka_track_lines=track_lines[track_lines.city_id==91]\n",
                "osaka_tracks=tracks[tracks.city_id==91].drop(columns=['buildstart','opening','closure','city_id'])\n",
                "osaka_tracks.columns=['section_id','geometry','length']\n",
                "osaka_track_lines=pd.merge(osaka_track_lines,osaka_tracks)\n",
                "osaka_track_lines=osaka_track_lines.drop(columns=['id','created_at','updated_at','city_id'])\n",
                "osaka_track_lines.columns=['section_id','id','geometry','length']\n",
                "osaka_lines=pd.merge(osaka_track_lines,osaka_lines)\n",
                "osaka_stations=stations[stations['city_id']==91]\n",
                "osaka_stations.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "x=[]\n",
                "y=[]\n",
                "z=[]\n",
                "for i in range(len(osaka_lines)):\n",
                "    sp=osaka_lines.iloc[i].geometry.split('(')[1].split(')')[0].split(',')\n",
                "    for j in range(len(sp)):\n",
                "        x.append(sp[j].split(' ')[0])\n",
                "        y.append(sp[j].split(' ')[1])\n",
                "        z.append(osaka_lines.url_name[i])\n",
                "fix=pd.DataFrame({'x':x,'y':y,'z':z})\n",
                "fix['x']=fix['x'].astype(float)\n",
                "fix['y']=fix['y'].astype(float)\n",
                "plt.figure(figsize=(27, 27))\n",
                "plt.subplot(2, 2, 1) \n",
                "ax=sns.scatterplot(x=\"x\", y=\"y\", hue=\"z\",data=fix)\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.05,0.6))\n",
                "plt.title(\"lines for osaka\",size=20)\n",
                "ax.get_legend().remove()\n",
                "ax.set(xlabel='Longitude', ylabel='LATITUDE')\n",
                "plt.subplot(2,2,2)\n",
                "(osaka_lines.groupby(['url_name'])['length'].sum()/1000).sort_values(ascending= False)[:10].sort_values().plot.barh()\n",
                "plt.ylabel(' ')\n",
                "plt.xlabel('length(km)')\n",
                "plt.title(\"Top 10 track by length\",size=20)\n",
                "plt.subplot(2,2,3)\n",
                "osaka_stations.groupby(['opening'])['id'].agg('count').plot()\n",
                "plt.xlabel(' ')\n",
                "plt.ylabel('stations')\n",
                "plt.title(\"Number of opening stations by year\",size=20)\n",
                "plt.subplot(2,2,4)\n",
                "osaka_lines.name.value_counts()[:10].sort_values().plot.barh()\n",
                "plt.xlabel('counts')\n",
                "plt.title(\"Top 10 line by number\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "\n",
                "stations_osaka=stations[stations['city_id']==91]\n",
                "Long=135.5\n",
                "Lat=34.53\n",
                "osaka_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "osaka_stations_map=plugins.MarkerCluster().add_to(osaka_map)\n",
                "for lat,lon,label in zip(stations_osaka.Lat,stations_osaka.Long,stations_osaka.stations_name):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(osaka_stations_map)\n",
                "osaka_map.add_child(osaka_stations_map)\n",
                "\n",
                "osaka_map"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "New_York_lines=lines[lines.city_id==206]\n",
                "New_York_track_lines=track_lines[track_lines.city_id==206]\n",
                "New_York_tracks=tracks[tracks.city_id==206].drop(columns=['buildstart','opening','closure','city_id'])\n",
                "New_York_tracks.columns=['section_id','geometry','length']\n",
                "New_York_track_lines=pd.merge(New_York_track_lines,New_York_tracks)\n",
                "New_York_track_lines=New_York_track_lines.drop(columns=['id','created_at','updated_at','city_id'])\n",
                "New_York_track_lines.columns=['section_id','id','geometry','length']\n",
                "New_York_lines=pd.merge(New_York_track_lines,New_York_lines)\n",
                "New_York_stations=stations[stations['city_id']==206]\n",
                "New_York_stations.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "x=[]\n",
                "y=[]\n",
                "z=[]\n",
                "for i in range(len(New_York_lines)):\n",
                "    sp=New_York_lines.iloc[i].geometry.split('(')[1].split(')')[0].split(',')\n",
                "    for j in range(len(sp)):\n",
                "        x.append(sp[j].split(' ')[0])\n",
                "        y.append(sp[j].split(' ')[1])\n",
                "        z.append(New_York_lines.url_name[i])\n",
                "fix=pd.DataFrame({'x':x,'y':y,'z':z})\n",
                "fix['x']=fix['x'].astype(float)\n",
                "fix['y']=fix['y'].astype(float)\n",
                "plt.figure(figsize=(27, 27))\n",
                "plt.subplot(2, 2, 1) \n",
                "ax=sns.scatterplot(x=\"x\", y=\"y\", hue=\"z\",data=fix)\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.05,0.6))\n",
                "plt.title(\"lines for New_York\",size=20)\n",
                "ax.get_legend().remove()\n",
                "ax.set(xlabel='Longitude', ylabel='LATITUDE')\n",
                "plt.subplot(2,2,2)\n",
                "(New_York_lines.groupby(['url_name'])['length'].sum()/1000).sort_values(ascending= False)[:10].sort_values().plot.barh()\n",
                "plt.ylabel(' ')\n",
                "plt.xlabel('length(km)')\n",
                "plt.title(\"Top 10 track by length\",size=20)\n",
                "plt.subplot(2,2,3)\n",
                "New_York_stations.groupby(['opening'])['id'].agg('count').plot()\n",
                "plt.xlabel(' ')\n",
                "plt.ylabel('stations')\n",
                "plt.title(\"Number of opening stations by year\",size=20)\n",
                "plt.subplot(2,2,4)\n",
                "New_York_lines.name.value_counts()[:10].sort_values().plot.barh()\n",
                "plt.xlabel('counts')\n",
                "plt.title(\"Top 10 line by number\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "stations_New_York=stations[stations['city_id']==206]\n",
                "Long=-73.97\n",
                "Lat=40.78\n",
                "New_York_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "New_York_stations_map=plugins.MarkerCluster().add_to(New_York_map)\n",
                "for lat,lon,label in zip(stations_New_York.Lat,stations_New_York.Long,stations_New_York.stations_name):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(New_York_stations_map)\n",
                "New_York_map.add_child(New_York_stations_map)\n",
                "\n",
                "New_York_map"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import tensorflow as tf\n",
                "print(tf.__version__)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# Import Libraries\n",
                "import numpy as np\n",
                "\n",
                "import tensorflow as tf\n",
                "import tensorflow.keras as keras\n",
                "from tensorflow.keras.datasets import mnist\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
                "\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "# Load Dataset\n",
                "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(x_train.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "# Show image of training data\n",
                "plt.figure(figsize = (10, 10)) # set size of figure 10x10\n",
                "rand_indexes = np.random.randint(0, x_train.shape[0], 8) # select 8 digits(0~9) randomly \n",
                "print(rand_indexes)\n",
                "for index,im_index in enumerate(rand_indexes):\n",
                "    plt.subplot(4, 4, index+1)\n",
                "    plt.imshow(x_train[im_index], cmap = 'gray', interpolation = 'none')\n",
                "    plt.title('Class %d' % y_train[im_index])\n",
                "plt.tight_layout()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# data convert to floating point\n",
                "#x_train = x_train.astype('float32') / 255\n",
                "x_train = x_train / 255.0\n",
                "#x_test = x_test.astype('float32') / 255\n",
                "x_test = x_test / 255.0\n",
                "print('x_train shape:', x_train.shape)\n",
                "print(x_train.shape[0], 'train samples')\n",
                "print(x_test.shape[0], 'test samples')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(x_test.shape)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "img_rows, img_cols = 28, 28 # input image dimensions\n",
                "\n",
                "if keras.backend.image_data_format() == 'channels_first':\n",
                "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
                "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
                "    input_shape = (1, img_rows, img_cols)\n",
                "else:\n",
                "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
                "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
                "    input_shape = (img_rows, img_cols, 1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "y_train = keras.utils.to_categorical(y_train)\n",
                "y_test = keras.utils.to_categorical(y_test)\n",
                "num_classes = y_train.shape[1]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(y_train.shape)\n",
                "print(y_test.shape)\n",
                "print(num_classes)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "# Build Model\n",
                "model = Sequential()\n",
                "# 1st Conv layer\n",
                "model.add(Conv2D(32, kernel_size = (3, 3), padding = 'same', activation = 'relu', input_shape = input_shape))\n",
                "model.add(MaxPool2D(pool_size = (2, 2)))\n",
                "model.add(Dropout(0.25))\n",
                "# 2nd Conv layer        \n",
                "model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
                "model.add(MaxPool2D(pool_size = (2, 2)))\n",
                "model.add(Dropout(0.25))\n",
                "# Fully Connected layer        \n",
                "model.add(Flatten())\n",
                "model.add(Dense(128, activation = 'relu'))\n",
                "model.add(Dropout(0.2))\n",
                "model.add(Dense(num_classes, activation = 'softmax'))\n",
                "\n",
                "model.summary()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# Compile Model\n",
                "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# Train Model\n",
                "num_epochs = 30\n",
                "history = model.fit(x_train, y_train, batch_size = 128, epochs = num_epochs, verbose = 1, validation_data=(x_test, y_test));"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "# Save Model\n",
                "model.save('mnist_cnn.h5')"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "# Evaluate Model\n",
                "score = model.evaluate(x_test, y_test, verbose = 0)\n",
                "print('Test loss: ', score[0])\n",
                "print('Test accuracy: ', score[1])"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "# Show Train History\n",
                "keys=history.history.keys()\n",
                "print(keys)\n",
                "\n",
                "def show_train_history(hisData,train,test): \n",
                "    plt.plot(hisData.history[train])\n",
                "    plt.plot(hisData.history[test])\n",
                "    plt.title('Training History')\n",
                "    plt.ylabel(train)\n",
                "    plt.xlabel('Epoch')\n",
                "    plt.legend(['train', 'test'], loc='upper left')\n",
                "    plt.show()\n",
                "\n",
                "show_train_history(history, 'loss', 'val_loss')\n",
                "show_train_history(history, 'accuracy', 'val_accuracy')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "import math\n",
                "import csv\n",
                "import sys\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "df = pd.read_csv(\"../input/raw-benford-numbers-edited/Raw Benford Numbers.csv\", index_col = 'Unnamed: 0')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#an example of some of our data\n",
                "df.tail()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "## Remove all \"nan\" values for blank cells\n",
                "all = []\n",
                "for x in df:\n",
                "    all.append(df[x].values)\n",
                "\n",
                "cData = []\n",
                "for i in range(0,3):\n",
                "    cleanedList = [x for x in all[i] if str(x) != 'nan']\n",
                "    cData.append(cleanedList)\n",
                "finalData = []\n",
                "for z in range(len(cData)):\n",
                "    for y in cData[z]:\n",
                "        finalData.append(y)\n",
                "#finalData is the cleaned data without nan values, we still have to clear the trailing decimal points and zeroes\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#this for and if loop evaluates if the value ends with '.0' and if it does, the last two digits are removed\n",
                "for val in range(len(finalData)):\n",
                "    if str(finalData[val])[-2:] == '.0':\n",
                "        finalData[val] = str(finalData[val])[:-2]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(finalData)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "second_Digit = secondDigit(finalData)\n",
                "third_Digit = thirdDigit(finalData)\n",
                "first_Digit = firstDigit(finalData)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "Benford_percentiles = pd.DataFrame({\n",
                "    'First Digit Expected': [0, .301, .176, .125, .097, .079, .067, .058, .051, .046],\n",
                "    'Second Digit Expected': [.12, .114, .109, .104, .100, .097, .093, .090, .088, .085],\n",
                "    'Third Digit Expected': [.102, .101, .101, .101, .100, .100, .099, .099, .099, .098]\n",
                "                                    })"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "Benford_percentiles"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#This is just a quick script to seperate out the expected first digit from the rest of the Data Frame as these are on a\n",
                "#scale of 1-9 instead of 0-9 like the second and third digits\n",
                "First_digit_benfords = []\n",
                "for x in Benford_percentiles['First Digit Expected']:\n",
                "    if x > 0:\n",
                "        First_digit_benfords.append(x)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "first_digit_percentile = [(x / sum(first_Digit)) for x in first_Digit]\n",
                "fig = plt.figure()\n",
                "ax = fig.add_axes([0,0,1,1])\n",
                "index = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
                "x = np.arange(len(index))\n",
                "plt.xticks(x, index)\n",
                "ax.plot(x, First_digit_benfords, label= 'Actual')\n",
                "ax.plot(x, first_digit_percentile, label= 'Expected')\n",
                "plt.title('First Digit; Expected values are in blue, actual are in orange')\n",
                "plt.show"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "second_Digit_percentile = [(x / sum(second_Digit)) for x in second_Digit]\n",
                "fig = plt.figure()\n",
                "ax = fig.add_axes([0,0,1,1])\n",
                "index = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
                "x = np.arange(len(index))\n",
                "plt.xticks(x, index)\n",
                "ax.plot(x, Benford_percentiles['Second Digit Expected'])\n",
                "ax.plot(x, second_Digit_percentile)\n",
                "plt.title('Second Digit; Expected values are in blue, actual are in orange')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "third_Digit_percentile = [(x / sum(third_Digit)) for x in third_Digit]\n",
                "fig = plt.figure()\n",
                "ax = fig.add_axes([0,0,1,1])\n",
                "index = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
                "x = np.arange(len(index))\n",
                "plt.xticks(x, index)\n",
                "ax.plot(x, Benford_percentiles['Third Digit Expected'])\n",
                "ax.plot(x, third_Digit_percentile)\n",
                "plt.title('Third Digit; Expected values are in blue, actual are in orange')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#reinitialize these variables before I alter them just to keep the code clean\n",
                "first_digit_percentile = [(x / sum(first_Digit)) for x in first_Digit]\n",
                "first_digit_percentile.insert(0, 0)\n",
                "index = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
                "\n",
                "final_series = {'First Digit Test': first_digit_percentile,\n",
                "                'Second Digit Test': second_Digit_percentile,\n",
                "                'Third Digit Test': third_Digit_percentile}\n",
                "digit_df = pd.DataFrame(data=final_series, index=index)\n",
                "final_df = pd.merge(digit_df, Benford_percentiles, on=digit_df.index, how='outer')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final_df"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "final_df.plot(kind='box', rot=-30)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('Skew\\n', final_df.skew(), '\\nKurtosis:\\n', final_df.kurt())"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final_df.std()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def myDataFrame(sample, digit_test, expected):\n",
                "    difference = []\n",
                "    for x in range(len(digit_test)):\n",
                "        difference.append(digit_test[x]-expected[x])\n",
                "    if len(sample) < 10:\n",
                "        sample.insert(0, 0)\n",
                "    output = pd.DataFrame({\n",
                "        'Sample Count': sample, \n",
                "        'Digit Test (%)': digit_test, \n",
                "        'Expected Values (%)': expected, \n",
                "        'Difference (%)': difference\n",
                "                        })\n",
                "    return output"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#This runs the function we created before on each of my different tests\n",
                "first_digit_df = myDataFrame(first_Digit, final_df['First Digit Test'], final_df['First Digit Expected'])\n",
                "second_digit_df = myDataFrame(second_Digit, final_df['Second Digit Test'], final_df['Second Digit Expected'])\n",
                "third_digit_df = myDataFrame(third_Digit, final_df['Third Digit Test'], final_df['Third Digit Expected'])\n",
                "\n",
                "#now I convert the numbers into percentage values to make my data tables more readable\n",
                "for x in [first_digit_df, second_digit_df, third_digit_df]:\n",
                "    for y in ['Digit Test (%)', 'Expected Values (%)', 'Difference (%)']:\n",
                "        x[y] = round(x[y].apply(lambda i: i*100), 2)\n",
                "\n",
                "#Below is an example of the completed data table\n",
                "first_digit_df"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "last_two = last_two_digit_test(finalData)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "index = []\n",
                "for x in range(100):\n",
                "    index.append(x)\n",
                "\n",
                "#Plotting\n",
                "fig = plt.figure()\n",
                "ax = fig.add_axes([0,0,1,1])\n",
                "width = .35\n",
                "x = np.arange(len(index))\n",
                "plt.xticks(x, index)\n",
                "ax.bar(x - width/2, last_two, width= width)\n",
                "plt.title('Last Two Digit Count')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "last_two_df = pd.DataFrame(last_two, index=index)\n",
                "print([ j for (i,j) in zip(last_two, index) if i >= 4 ])"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#We create new variables called 'significant numbers' which we will alter in order to deliver us just the significant digits\n",
                "\n",
                "first_digit_significant_numbers = first_digit_df.copy()\n",
                "second_digit_significant_numbers = second_digit_df.copy()\n",
                "third_digit_significant_numbers = third_digit_df.copy()\n",
                "\n",
                "#This for loop adds the significant digit to a list called 'temp' when the difference of the number is greater than 5%\n",
                "temp = []\n",
                "for x in [first_digit_significant_numbers, second_digit_significant_numbers, third_digit_significant_numbers]:\n",
                "    temp.append(x.index.where(x['Difference (%)'] > 5).dropna())\n",
                "\n",
                "#Now we convert the list into a cleaner version which will be easier to work with and put it in a dictionary\n",
                "significant_numbers = {\n",
                "    'First Digit Significant Values' : temp[0].astype(int).values,\n",
                "    'Second Digit Significant Values' : temp[1].astype(int).values,\n",
                "    'Third Digit Significant Values' : temp[2].astype(int).values\n",
                "}\n",
                "#here is an example of what those numbers are\n",
                "significant_numbers"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#This cell changes all of my integers into strings which are easier to slice in order to locate the sections of the 990 that\n",
                "#might contain fraudulent data\n",
                "finalData = [str(x) for x in finalData]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#these lines below actually are finding the numbers in our entire dataset where the digit at some given position\n",
                "#matches the number and the position that we are looking for\n",
                "first_numbers = [x for x in finalData if x[:1] in significant_numbers['First Digit Significant Values'].astype(str)]\n",
                "second_numbers = [x for x in finalData if x[1:2] in significant_numbers['Second Digit Significant Values'].astype(str)]\n",
                "third_numbers = [x for x in finalData if x[2:3] in significant_numbers['Third Digit Significant Values'].astype(str)]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(first_numbers, second_numbers, third_numbers)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def intersection(lst1, lst2): \n",
                "    lst3 = [value for value in lst1 if value in lst2] \n",
                "    return lst3 "
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "inter_one_two = intersection(first_numbers, second_numbers)\n",
                "inter_one_three = intersection(first_numbers, third_numbers)\n",
                "inter_two_three = intersection(second_numbers, third_numbers)\n",
                "interfinal = intersection(inter_one_two, third_numbers)\n",
                "print(inter_one_two)\n",
                "print(inter_one_three)\n",
                "print(inter_two_three)\n",
                "print(interfinal)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "in_first = set(inter_one_two)\n",
                "in_second = set(inter_two_three)\n",
                "\n",
                "in_second_but_not_in_first = in_second - in_first\n",
                "\n",
                "result = inter_one_two + list(in_second_but_not_in_first)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(result)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#delete all values that arent on our list 'result'\n",
                "all_to_investigate = df[df.isin(result)]\n",
                "\n",
                "#remove every column that is entirely 'NaN' values\n",
                "all_to_investigate = all_to_investigate.dropna(axis='columns', how='all')\n",
                "\n",
                "#remove every row that is entirely 'NaN' values\n",
                "all_to_investigate = all_to_investigate.dropna(axis='index', how='all')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "all_to_investigate"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "to_drop = ['revenue less expenses', 'Total expenses']\n",
                "final_list = all_to_investigate.drop(to_drop)\n",
                "final_list"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final_list"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "transfer_results",
                "visualize_data",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output.\n",
                "\n",
                "\n",
                "train_df = pd.read_csv('../input/train.csv')\n",
                "test_df = pd.read_csv('../input/test.csv')\n",
                "# combine = [train_df, test_df]\n",
                "\n",
                "train_df['color'] = train_df['color'].map({'clear':0, 'white':1, 'green':2, 'blood':3, 'blue':4, 'black':5})\n",
                "import seaborn as sns\n",
                "sns.set()\n",
                "sns.pairplot(train_df, hue=\"type\")\n",
                "\n",
                "#print(combine)\n",
                "#print(test_df.columns.values)\n",
                "# train_df.head()\n",
                "\n",
                "# train_df.info()\n",
                "# print('_'*40)\n",
                "# test_df.info()\n",
                "\n",
                "#What is the distribution of numerical feature values across the samples?\n",
                "# train_df.describe()\n",
                "\n",
                "#What is the distribution of categorical features?\n",
                "# train_df.describe(include=['O'])\n",
                "\n",
                "\n",
                "# submission = pd.DataFrame({\n",
                "#         \"id\": test_df[\"id\"],\n",
                "#         \"type\": \"Ghoul\"\n",
                "#     })\n",
                "print(submission)\n",
                "submission.to_csv('../output/submission.csv', index=False)\n",
                "submission.to_csv('submission.csv', index=False)\n",
                "\n",
                "\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# modules we'll use\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# helpful modules\n",
                "import fuzzywuzzy\n",
                "from fuzzywuzzy import process\n",
                "import chardet\n",
                "\n",
                "# set seed for reproducibility\n",
                "np.random.seed(0)"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data"
            ],
            "source": [
                "# look at the first ten thousand bytes to guess the character encoding\n",
                "with open(\"../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", 'rb') as rawdata:\n",
                "    result = chardet.detect(rawdata.read(100000))\n",
                "\n",
                "# check what the character encoding might be\n",
                "print(result)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "# read in our dat\n",
                "suicide_attacks = pd.read_csv(\"../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", \n",
                "                              encoding='Windows-1252')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# get all the unique values in the 'City' column\n",
                "cities = suicide_attacks['City'].unique()\n",
                "\n",
                "# sort them alphabetically and then take a closer look\n",
                "cities.sort()\n",
                "cities"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# convert to lower case\n",
                "suicide_attacks['City'] = suicide_attacks['City'].str.lower()\n",
                "# remove trailing white spaces\n",
                "suicide_attacks['City'] = suicide_attacks['City'].str.strip()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# Your turn! Take a look at all the unique values in the \"Province\" column. \n",
                "# Then convert the column to lowercase and remove any trailing white spaces\n",
                "\n",
                "# get all the unique values in the 'City' column\n",
                "provinces = suicide_attacks['Province'].unique()\n",
                "\n",
                "# sort them alphabetically and then take a closer look\n",
                "provinces.sort()\n",
                "provinces"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# convert to lower case\n",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.lower()\n",
                "# remove trailing white spaces\n",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.strip()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# get all the unique values in the 'City' column\n",
                "cities = suicide_attacks['City'].unique()\n",
                "\n",
                "# sort them alphabetically and then take a closer look\n",
                "cities.sort()\n",
                "cities"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# get the top 10 closest matches to \"d.i khan\"\n",
                "matches = fuzzywuzzy.process.extract(\"d.i khan\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
                "\n",
                "# take a look at them\n",
                "matches"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# function to replace rows in the provided column of the provided dataframe\n",
                "# that match the provided string above the provided ratio with the provided string\n",
                "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n",
                "    # get a list of unique strings\n",
                "    strings = df[column].unique()\n",
                "    \n",
                "    # get the top 10 closest matches to our input string\n",
                "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
                "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
                "\n",
                "    # only get matches with a ratio > 90\n",
                "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
                "\n",
                "    # get the rows of all the close matches in our dataframe\n",
                "    rows_with_matches = df[column].isin(close_matches)\n",
                "\n",
                "    # replace all rows with close matches with the input matches \n",
                "    df.loc[rows_with_matches, column] = string_to_match\n",
                "    \n",
                "    # let us know the function's done\n",
                "    print(\"All done!\")"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# get all the unique values in the 'City' column\n",
                "cities = suicide_attacks['City'].unique()\n",
                "\n",
                "# sort them alphabetically and then take a closer look\n",
                "cities.sort()\n",
                "cities"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# Your turn! It looks like 'kuram agency' and 'kurram agency' should\n",
                "# be the same city. Correct the dataframe so that they are.\n",
                "\n",
                "\n",
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"kuram agency\")\n",
                "\n",
                "# get all the unique values in the 'City' column\n",
                "cities = suicide_attacks['City'].unique()\n",
                "\n",
                "# sort them alphabetically and then take a closer look\n",
                "cities.sort()\n",
                "cities"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "# modules we'll use\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# for Box-Cox Transformation\n",
                "from scipy import stats\n",
                "\n",
                "# for min_max scaling\n",
                "from mlxtend.preprocessing import minmax_scaling\n",
                "\n",
                "# plotting modules\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# read in all our data\n",
                "kickstarters_2017 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201801.csv\")\n",
                "\n",
                "# set seed for reproducibility\n",
                "np.random.seed(0)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# generate 1000 data points randomly drawn from an exponential distribution\n",
                "original_data = np.random.exponential(size = 1000)\n",
                "\n",
                "# mix-max scale the data between 0 and 1\n",
                "scaled_data = minmax_scaling(original_data, columns = [0])\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(original_data, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(scaled_data, ax=ax[1])\n",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# normalize the exponential data with boxcox\n",
                "normalized_data = stats.boxcox(original_data)\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(original_data, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(normalized_data[0], ax=ax[1])\n",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# select the usd_goal_real column\n",
                "usd_goal = kickstarters_2017.usd_goal_real\n",
                "\n",
                "# scale the goals from 0 to 1\n",
                "scaled_data = minmax_scaling(usd_goal, columns = [0])\n",
                "\n",
                "# plot the original & scaled data together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(scaled_data, ax=ax[1])\n",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Your turn! \n",
                "\n",
                "# We just scaled the \"usd_goal_real\" column. What about the \"goal\" column?\n",
                "\n",
                "# select the usd_goal_real column\n",
                "goal = kickstarters_2017.goal\n",
                "\n",
                "# scale the goals from 0 to 1\n",
                "scaled_data = minmax_scaling(goal, columns = [0])\n",
                "\n",
                "# plot the original & scaled data together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(kickstarters_2017.goal, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(scaled_data, ax=ax[1])\n",
                "ax[1].set_title(\"Scaled data\")\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# get the index of all positive pledges (Box-Cox only takes postive values)\n",
                "index_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0\n",
                "\n",
                "# get only positive pledges (using their indexes)\n",
                "positive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]\n",
                "\n",
                "# normalize the pledges (w/ Box-Cox)\n",
                "normalized_pledges = stats.boxcox(positive_pledges)[0]\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(positive_pledges, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(normalized_pledges, ax=ax[1])\n",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Your turn! \n",
                "# We looked as the usd_pledged_real column. What about the \"pledged\" column? Does it have the same info?\n",
                "\n",
                "# get the index of all positive pledges (Box-Cox only takes postive values)\n",
                "index_of_pledged = kickstarters_2017.pledged > 0\n",
                "\n",
                "# get only positive pledges (using their indexes)\n",
                "positive_pledges = kickstarters_2017.pledged.loc[index_of_pledged]\n",
                "\n",
                "# normalize the pledges (w/ Box-Cox)\n",
                "normalized_pledges = stats.boxcox(positive_pledges)[0]\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(positive_pledges, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(normalized_pledges, ax=ax[1])\n",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# modules we'll use\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# for Box-Cox Transformation\n",
                "from scipy import stats\n",
                "\n",
                "# for min_max scaling\n",
                "from mlxtend.preprocessing import minmax_scaling\n",
                "\n",
                "# plotting modules\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# read in all our data\n",
                "kickstarters_2017 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201801.csv\")\n",
                "\n",
                "# set seed for reproducibility\n",
                "np.random.seed(0)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# generate 1000 data points randomly drawn from an exponential distribution\n",
                "original_data = np.random.exponential(size = 1000)\n",
                "\n",
                "# mix-max scale the data between 0 and 1\n",
                "scaled_data = minmax_scaling(original_data, columns = [0])\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(original_data, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(scaled_data, ax=ax[1])\n",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# normalize the exponential data with boxcox\n",
                "normalized_data = stats.boxcox(original_data)\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(original_data, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(normalized_data[0], ax=ax[1])\n",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# select the usd_goal_real column\n",
                "usd_goal = kickstarters_2017.usd_goal_real\n",
                "\n",
                "# scale the goals from 0 to 1\n",
                "scaled_data = minmax_scaling(usd_goal, columns = [0])\n",
                "\n",
                "# plot the original & scaled data together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(scaled_data, ax=ax[1])\n",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Your turn! \n",
                "\n",
                "# We just scaled the \"usd_goal_real\" column. What about the \"goal\" column?\n",
                "\n",
                "usd_goal = kickstarters_2017.goal\n",
                "\n",
                "# scale the goals from 0 to 1\n",
                "scaled_data = minmax_scaling(usd_goal, columns = [0])\n",
                "\n",
                "# plot the original & scaled data together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(kickstarters_2017.goal, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(scaled_data, ax=ax[1])\n",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# get the index of all positive pledges (Box-Cox only takes postive values)\n",
                "index_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0\n",
                "\n",
                "# get only positive pledges (using their indexes)\n",
                "positive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]\n",
                "\n",
                "# normalize the pledges (w/ Box-Cox)\n",
                "normalized_pledges = stats.boxcox(positive_pledges)[0]\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(positive_pledges, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(normalized_pledges, ax=ax[1])\n",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Your turn! \n",
                "# We looked as the usd_pledged_real column. What about the \"pledged\" column? Does it have the same info?\n",
                "\n",
                "index_of_positive_pledges = kickstarters_2017.pledged > 0\n",
                "\n",
                "# get only positive pledges (using their indexes)\n",
                "positive_pledges = kickstarters_2017.pledged.loc[index_of_positive_pledges]\n",
                "\n",
                "# normalize the pledges (w/ Box-Cox)\n",
                "normalized_pledges = stats.boxcox(positive_pledges)[0]\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(positive_pledges, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(normalized_pledges, ax=ax[1])\n",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# books.csv has 13719 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df1 = pd.read_csv('../input/books.csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'books.csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df1.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotCorrelationMatrix(df1, 8)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotScatterMatrix(df1, 18, 10)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import joblib\n",
                "import lightgbm as lgb\n",
                "import time\n",
                "import pickle\n",
                "import math\n",
                "import string\n",
                "import datetime\n",
                "from sklearn.model_selection import KFold\n",
                "from sklearn.metrics import f1_score, mean_squared_error\n",
                "import glob\n",
                "import json\n",
                "\n",
                "def challenge_data_conversion(challenge_data):\n",
                "    output = []\n",
                "    output.append(challenge_data['id'])\n",
                "    output.append(1 if len(challenge_data['winners']) > 0 else 0)\n",
                "    output.append(len(challenge_data['winners']))\n",
                "    \n",
                "    return output\n",
                "\n",
                "def data_conversion(training_file_path):\n",
                "    data_df = pd.DataFrame(columns=['id', 'hasWinner', 'numOfWinners'])\n",
                "    file_list = []\n",
                "    extensions = [\"json\"]\n",
                "    for extension in extensions:\n",
                "        file_glob = glob.glob(training_file_path+\"/*.\"+extension)\n",
                "        file_list.extend(file_glob)\n",
                "    print(str(len(file_list))+' files')\n",
                "        \n",
                "    for file_path in file_list:\n",
                "        with open(file_path,'r') as f:\n",
                "            data_dict = json.load(f)\n",
                "        for challenge_data in data_dict:\n",
                "            #try:\n",
                "            data_df.loc[len(data_df)] = challenge_data_conversion(challenge_data)\n",
                "            #except:\n",
                "            #    print(challenge_data_conversion(challenge_data))\n",
                "            \n",
                "            \n",
                "    return data_df\n",
                "\n",
                "test_data = data_conversion('../input/challenge-health-notification-test-data/')\n",
                "reg_output = pd.read_csv('../input/challenge-health-notification-reg-output/lightgbm_numOfWinners_prediction.csv')\n",
                "cls_output = pd.read_csv('../input/challenge-health-notification-cls-output/lightgbm_hasWinner_prediction.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('f1 score:')\n",
                "print(f1_score(test_data['hasWinner'].astype(\"int\").values, cls_output['hasWinner'].values))\n",
                "\n",
                "print('')\n",
                "\n",
                "print('mean squared error:')\n",
                "print(mean_squared_error(test_data['numOfWinners'].values, reg_output['numOfWinners'].values))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import cv2\n",
                "import json\n",
                "import os\n",
                "import matplotlib\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.patches import Polygon\n",
                "from matplotlib.collections import PatchCollection\n",
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "import seaborn as sns"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "DATASET_DIR = '/kaggle/input/pku-autonomous-driving/'\n",
                "JSON_DIR = os.path.join(DATASET_DIR, 'car_models_json')\n",
                "NUM_IMG_SAMPLES = 10 # The number of image samples used for visualization"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "df = pd.read_csv(os.path.join(DATASET_DIR, 'train.csv'))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "image_ids = np.array(df['ImageId'])\n",
                "prediction_strings = np.array(df['PredictionString'])\n",
                "prediction_strings = [\n",
                "    np.array(prediction_string.split(' ')).astype(np.float32).reshape(-1, 7) \\\n",
                "    for prediction_string in prediction_strings\n",
                "]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('Image ID:', image_ids[0])\n",
                "print('Annotations:\\n', prediction_strings[0])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "models_map = dict((y, x) for x, y in models.items())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cars = []\n",
                "for prediction_string in prediction_strings:\n",
                "    for car in prediction_string:\n",
                "        cars.append(car)\n",
                "cars = np.array(cars)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "unique, counts = np.unique(cars[..., 0].astype(np.uint8), return_counts=True)\n",
                "all_model_types = zip(unique, counts)\n",
                "\n",
                "for i, model_type in enumerate(all_model_types):\n",
                "    print('{}.\\t Model type: {:<22} | {} cars'.format(i, models_map[model_type[0]], model_type[1]))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def plot_figures(\n",
                "    sizes,\n",
                "    pie_title,\n",
                "    start_angle,\n",
                "    bar_title,\n",
                "    bar_ylabel,\n",
                "    labels,\n",
                "    explode,\n",
                "    colors=None,\n",
                "):\n",
                "    fig, ax = plt.subplots(figsize=(14, 14))\n",
                "\n",
                "    y_pos = np.arange(len(labels))\n",
                "    barlist = ax.bar(y_pos, sizes, align='center')\n",
                "    ax.set_xticks(y_pos, labels)\n",
                "    ax.set_ylabel(bar_ylabel)\n",
                "    ax.set_title(bar_title)\n",
                "    if colors is not None:\n",
                "        for idx, item in enumerate(barlist):\n",
                "            item.set_color(colors[idx])\n",
                "\n",
                "    def autolabel(rects):\n",
                "        \"\"\"\n",
                "        Attach a text label above each bar displaying its height\n",
                "        \"\"\"\n",
                "        for rect in rects:\n",
                "            height = rect.get_height()\n",
                "            ax.text(\n",
                "                rect.get_x() + rect.get_width()/2., height,\n",
                "                '%d' % int(height),\n",
                "                ha='center', va='bottom', fontweight='bold'\n",
                "            )\n",
                "\n",
                "    autolabel(barlist)\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(14, 14))\n",
                "    \n",
                "    pielist = ax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=False, startangle=start_angle, counterclock=False)\n",
                "    ax.axis('equal')\n",
                "    ax.set_title(pie_title)\n",
                "    if colors is not None:\n",
                "        for idx, item in enumerate(pielist[0]):\n",
                "            item.set_color(colors[idx])\n",
                "\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plot_figures(\n",
                "    counts,\n",
                "    pie_title='The percentage of the number of cars of each model type',\n",
                "    start_angle=170,\n",
                "    bar_title='Distribution of cars of each model type',\n",
                "    bar_ylabel='Frequency',\n",
                "    labels=[label for label in unique],\n",
                "    explode=np.zeros(len(unique))\n",
                ")"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "# Get all json files\n",
                "files = [file for file in os.listdir(JSON_DIR) if os.path.isfile(os.path.join(JSON_DIR, file))]\n",
                "\n",
                "# For each json file, plot figure\n",
                "for file in files:\n",
                "    model_path = os.path.join(JSON_DIR, file)\n",
                "    with open(model_path) as src:\n",
                "        data = json.load(src)\n",
                "        car_type = data['car_type']\n",
                "        faces = data['faces']\n",
                "        vertices = np.array(data['vertices'])\n",
                "        triangles = np.array(faces) - 1\n",
                "\n",
                "        fig = plt.figure(figsize=(16, 5))\n",
                "        ax11 = fig.add_subplot(1, 2, 1, projection='3d')\n",
                "        ax11.set_title('Model: {} | Type: {}'.format(file.split('.')[0], car_type))\n",
                "        ax11.set_xlim([-2, 3])\n",
                "        ax11.set_ylim([-3, 2])\n",
                "        ax11.set_zlim([0, 3])\n",
                "        ax11.view_init(30, -50)\n",
                "        ax11.plot_trisurf(vertices[:,0], vertices[:,2], triangles, -vertices[:,1], shade=True, color='lime')\n",
                "        \n",
                "        ax12 = fig.add_subplot(1, 2, 2, projection='3d')\n",
                "        ax12.set_title('Model: {} | Type: {}'.format(file.split('.')[0], car_type))\n",
                "        ax12.set_xlim([-2, 3])\n",
                "        ax12.set_ylim([-3, 2])\n",
                "        ax12.set_zlim([0, 3])\n",
                "        ax12.view_init(30, 40)\n",
                "        ax12.plot_trisurf(vertices[:,0], vertices[:,2], triangles, -vertices[:,1], shade=True, color='lime')"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "def show_samples(samples):\n",
                "    for sample in samples:\n",
                "        fig, ax = plt.subplots(figsize=(18, 16))\n",
                "        \n",
                "        # Get image\n",
                "        img_path = os.path.join(DATASET_DIR, 'train_images', '{}.{}'.format(sample, 'jpg'))\n",
                "        img = cv2.imread(img_path, 1)\n",
                "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "        # Get corresponding mask\n",
                "        mask_path = os.path.join(DATASET_DIR, 'train_masks', '{}.{}'.format(sample, 'jpg'))\n",
                "        mask = cv2.imread(mask_path, 0)\n",
                "\n",
                "        patches = []\n",
                "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
                "        for contour in contours:\n",
                "            poly_patch = Polygon(contour.reshape(-1, 2), closed=True, linewidth=2, edgecolor='r', facecolor='r', fill=True)\n",
                "            patches.append(poly_patch)\n",
                "        p = PatchCollection(patches, match_original=True, cmap=matplotlib.cm.jet, alpha=0.3)\n",
                "\n",
                "        ax.imshow(img/255)\n",
                "        ax.set_title(sample)\n",
                "        ax.add_collection(p)\n",
                "        ax.set_xticklabels([])\n",
                "        ax.set_yticklabels([])\n",
                "        plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# Randomly select samples\n",
                "samples = image_ids[np.random.choice(image_ids.shape[0], NUM_IMG_SAMPLES, replace=False)]\n",
                "\n",
                "# Show images and corresponding masks of too-far-away (not of interest) cars\n",
                "show_samples(samples)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "lens = [len(str2coords(s)) for s in train['PredictionString']]\n",
                "\n",
                "plt.figure(figsize=(15,6))\n",
                "sns.countplot(lens);\n",
                "plt.xlabel('Number of cars in image');"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "points_df = pd.DataFrame()\n",
                "for col in ['x', 'y', 'z', 'yaw', 'pitch', 'roll']:\n",
                "    arr = []\n",
                "    for ps in train['PredictionString']:\n",
                "        coords = str2coords(ps)\n",
                "        arr += [c[col] for c in coords]\n",
                "    points_df[col] = arr"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,6))\n",
                "sns.distplot(points_df['x'], bins=500);\n",
                "plt.xlabel('x')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,6))\n",
                "sns.distplot(points_df['y'], bins=500);\n",
                "plt.xlabel('y')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,6))\n",
                "sns.distplot(points_df['z'], bins=500);\n",
                "plt.xlabel('z')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,6))\n",
                "sns.distplot(points_df['yaw'], bins=500);\n",
                "plt.xlabel('yaw')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,6))\n",
                "sns.distplot(points_df['pitch'], bins=500);\n",
                "plt.xlabel('pitch')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def rotate(x, angle):\n",
                "    x = x + angle\n",
                "    x = x - (x + np.pi) // (2 * np.pi) * 2 * np.pi\n",
                "    return x\n",
                "\n",
                "plt.figure(figsize=(15,6))\n",
                "sns.distplot(points_df['roll'].map(lambda x: rotate(x, np.pi)), bins=500);\n",
                "plt.xlabel('roll rotated by pi')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def get_img_coords(input_item, input_type=str, output_z=False):\n",
                "    '''\n",
                "    Input is a PredictionString (e.g. from train dataframe)\n",
                "    Output is two arrays:\n",
                "        xs: x coordinates in the image (row)\n",
                "        ys: y coordinates in the image (column)\n",
                "    '''\n",
                "    if input_type == str:\n",
                "        coords = str2coords(input_item)\n",
                "    else:\n",
                "        coords = input_item\n",
                "    \n",
                "    xs = [c['x'] for c in coords]\n",
                "    ys = [c['y'] for c in coords]\n",
                "    zs = [c['z'] for c in coords]\n",
                "    P = np.array(list(zip(xs, ys, zs))).T\n",
                "    img_p = np.dot(camera_matrix, P).T\n",
                "    img_p[:, 0] /= img_p[:, 2]\n",
                "    img_p[:, 1] /= img_p[:, 2]\n",
                "    img_xs = img_p[:, 0]\n",
                "    img_ys = img_p[:, 1]\n",
                "    img_zs = img_p[:, 2] # z = Distance from the camera\n",
                "    if output_z:\n",
                "        return img_xs, img_ys, img_zs\n",
                "    return img_xs, img_ys\n",
                "\n",
                "plt.figure(figsize=(14,14))\n",
                "plt.imshow(imread(PATH + 'train_images/' + train['ImageId'][2217] + '.jpg'))\n",
                "plt.scatter(*get_img_coords(train['PredictionString'][2217]), color='red', s=100);"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "xs, ys = [], []\n",
                "\n",
                "for ps in train['PredictionString']:\n",
                "    x, y = get_img_coords(ps)\n",
                "    xs += list(x)\n",
                "    ys += list(y)\n",
                "\n",
                "plt.figure(figsize=(18,18))\n",
                "plt.imshow(imread(PATH + 'train_images/' + train['ImageId'][2217] + '.jpg'), alpha=0.3)\n",
                "plt.scatter(xs, ys, color='red', s=10, alpha=0.2);"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "n_rows = 6\n",
                "\n",
                "for idx in range(n_rows):\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(20,20))\n",
                "    img = imread(PATH + 'train_images/' + train['ImageId'].iloc[idx] + '.jpg')\n",
                "    axes[0].imshow(img)\n",
                "    img_vis = visualize(img, str2coords(train['PredictionString'].iloc[idx]))\n",
                "    axes[1].imshow(img_vis)\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "import random\n",
                "sample_index_list = random.sample(list(range(len(points_df))), 5000)\n",
                "v = np.vstack([points_df['x'][sample_index_list], points_df['y'][sample_index_list], \n",
                "               points_df['z'][sample_index_list], points_df['yaw'][sample_index_list], \n",
                "               points_df['pitch'][sample_index_list], points_df['roll'][sample_index_list]])\n",
                "CM = np.corrcoef(v)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(7, 7))\n",
                "im = ax.imshow(CM)\n",
                "ax.set_xticks(np.arange(6))\n",
                "ax.set_yticks(np.arange(6))\n",
                "ax.set_xticklabels(['x', 'y', 'z', 'yaw', 'pitch', 'roll'])\n",
                "ax.set_yticklabels(['x', 'y', 'z', 'yaw', 'pitch', 'roll'])\n",
                "for i in range(6):\n",
                "    for j in range(6):\n",
                "        text = ax.text(j, i, round(CM[i, j], 2),\n",
                "                       ha=\"center\", va=\"center\", color=\"w\")\n",
                "fig.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "coords = []\n",
                "for sample_index in sample_index_list:\n",
                "    coord = {}\n",
                "    coord['x'] = points_df['x'][sample_index] \n",
                "    coord['y'] = points_df['y'][sample_index] \n",
                "    coord['z'] = points_df['z'][sample_index]\n",
                "    coords.append(coord)\n",
                "img_x_list, img_y_list, img_z_list = get_img_coords(coords, input_type=list, output_z=True)\n",
                "\n",
                "v = np.vstack([points_df['x'][sample_index_list], points_df['y'][sample_index_list], \n",
                "               points_df['z'][sample_index_list], img_x_list, img_y_list, img_z_list])\n",
                "CM = np.corrcoef(v)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(7, 7))\n",
                "im = ax.imshow(CM)\n",
                "ax.set_xticks(np.arange(6))\n",
                "ax.set_yticks(np.arange(6))\n",
                "ax.set_xticklabels(['x', 'y', 'z', 'img_x', 'img_y', 'img_z'])\n",
                "ax.set_yticklabels(['x', 'y', 'z', 'img_x', 'img_y', 'img_z'])\n",
                "for i in range(6):\n",
                "    for j in range(6):\n",
                "        text = ax.text(j, i, round(CM[i, j], 2),\n",
                "                       ha=\"center\", va=\"center\", color=\"w\")\n",
                "fig.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "mask_path = os.path.join(DATASET_DIR, 'train_masks', '{}.{}'.format(image_ids[0], 'jpg'))\n",
                "mask_accru = cv2.imread(mask_path, 0).astype(np.int) / 255\n",
                "for id in image_ids[1:]:\n",
                "    mask_path = os.path.join(DATASET_DIR, 'train_masks', '{}.{}'.format(id, 'jpg'))\n",
                "    try:\n",
                "        mask = cv2.imread(mask_path, 0).astype(np.int) / 255\n",
                "        mask_accru = np.add(mask_accru, mask)\n",
                "    except:\n",
                "        pass\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(18, 16))\n",
                "ax.set_title('mask distribution')\n",
                "im = ax.imshow(mask_accru)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import os\n",
                "from sklearn import preprocessing\n",
                "import lightgbm as lgb\n",
                "from sklearn.model_selection import KFold, StratifiedKFold\n",
                "from sklearn.model_selection import train_test_split\n",
                "from bayes_opt import BayesianOptimization\n",
                "import seaborn as sns\n",
                "from sklearn import metrics\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import mean_squared_error\n",
                "from sklearn.metrics import roc_auc_score\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.metrics import recall_score\n",
                "from sklearn.metrics import f1_score\n",
                "from sklearn.metrics import auc\n",
                "from sklearn.metrics import precision_score\n",
                "from sklearn.metrics import roc_curve\n",
                "from scipy.interpolate import interp1d\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.metrics import confusion_matrix"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train_id=pd.read_csv('../input/train_identity.csv')\n",
                "test_id=pd.read_csv('../input/test_identity.csv')\n",
                "train_id.head()\n",
                "#print(train_id.describe())\n",
                "#print(train_id.isnull().sum())"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train_data=pd.read_csv('../input/train_transaction.csv')\n",
                "test_data=pd.read_csv('../input/test_transaction.csv')\n",
                "#print(train_data.describe())\n",
                "#print(train_data.isnull().sum())"
            ]
        },
        {
            "tags": [
                "process_data",
                "setup_notebook"
            ],
            "source": [
                "import gc\n",
                "train = train_data.merge(train_id, how='left', on='TransactionID')\n",
                "test = test_data.merge(test_id, how='left',on='TransactionID')\n",
                "del train_id; gc.collect()\n",
                "del test_id; gc.collect()\n",
                "del train_data; gc.collect()\n",
                "del test_data; gc.collect()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "index_pro=train['P_emaildomain']=='protonmail.com'\n",
                "index_pro_data=train['P_emaildomain'][index_pro]\n",
                "train['isprotonmail']=index_pro_data\n",
                "del index_pro\n",
                "del index_pro_data\n",
                "index_pro=test['P_emaildomain']=='protonmail.com'\n",
                "index_pro_data=test['P_emaildomain'][index_pro]\n",
                "test['isprotonmail']=index_pro_data\n",
                "del index_pro\n",
                "del index_pro_data\n",
                "\n",
                "train.loc[ (train.isprotonmail.isnull()), 'isprotonmail' ] = 0\n",
                "train['isprotonmail'].describe(include=\"all\")\n",
                "\n",
                "test.loc[ (test.isprotonmail.isnull()), 'isprotonmail' ] = 0\n",
                "test['isprotonmail'].describe(include=\"all\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "\n",
                "'''cols_to_drop=['V108', 'id_22', 'id_26', 'id_25', 'C3', 'V14', 'id_24', 'id_07', 'V121', 'V65', 'V305', 'V311', 'V286', 'V111', 'V300', 'V115', 'V113', 'V301',  'V25', 'V123', 'V118', 'V109', 'id_23', 'V112', 'V114', 'V120', 'V88', 'V107', 'V117', 'V122', 'V116', 'id_08', 'id_27', 'id_21', 'V110', 'V119']\n",
                "\n",
                "train = train.drop(cols_to_drop, axis=1)\n",
                "test = test.drop(cols_to_drop, axis=1)'''"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "training_missing = train.isna().sum(axis=0) / train.shape[0] \n",
                "test_missing = test.isna().sum(axis=0) / test.shape[0] \n",
                "change = (training_missing / test_missing).sort_values(ascending=False)\n",
                "change = change[change<1e6] # remove the divide by zero errors\n",
                "train_more=change[change>4].reset_index()\n",
                "train_more.columns=['train_more_id','rate']\n",
                "test_more=change[change<0.4].reset_index()\n",
                "test_more.columns=['test_more_id','rate']\n",
                "train_more_id=train_more['train_more_id'].values\n",
                "train[train_more_id].head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig, axs = plt.subplots(ncols=2)\n",
                "\n",
                "train_vals = train[\"V80\"].fillna(-999)\n",
                "test_vals = test[test[\"TransactionDT\"]>2.5e7][\"V80\"].fillna(-999) # values following the shift\n",
                "\n",
                "\n",
                "axs[0].hist(train_vals, alpha=0.5, normed=True, bins=25)\n",
                "    \n",
                "axs[1].hist(test_vals, alpha=0.5, normed=True, bins=25)\n",
                "\n",
                "\n",
                "fig.set_size_inches(7,3)\n",
                "plt.tight_layout()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
                "us_emails = ['gmail', 'net', 'edu']\n",
                "for c in ['P_emaildomain', 'R_emaildomain']:\n",
                "    train[c + '_bin'] = train[c].map(emails)\n",
                "    test[c + '_bin'] = test[c].map(emails)\n",
                "    \n",
                "    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n",
                "    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n",
                "    \n",
                "    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
                "    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import Imputer\n",
                "imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
                "test[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']]=imp.fit_transform(test[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']])\n",
                "train[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']]=imp.fit_transform(train[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "isna = train.isna().sum(axis=1)\n",
                "isna_test = test.isna().sum(axis=1)\n",
                "train['isna']=train.isna().sum(axis=1)\n",
                "test['isna']=test.isna().sum(axis=1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "test[test.TransactionDT>2.5e7][train_more_id]=imp.fit_transform(test[test.TransactionDT>2.5e7][train_more_id])\n",
                "train[train_more_id]=imp.fit_transform(train[train_more_id])"
            ]
        },
        {
            "tags": [
                "process_data",
                "setup_notebook"
            ],
            "source": [
                "import datetime\n",
                "START_DATE = '2017-12-01'\n",
                "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
                "train['TransactionDT'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
                "test['TransactionDT'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
                "\n",
                "train['hour'] = train['TransactionDT'].dt.hour\n",
                "test['hour'] = test['TransactionDT'].dt.hour\n",
                "\n",
                "train['month'] = train['TransactionDT'].dt.month\n",
                "test['month'] = test['TransactionDT'].dt.month\n",
                "#print(train['TransactionDT'])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x=test['hour']<7\n",
                "x1=test['hour']>19\n",
                "x.astype(int)\n",
                "x1.astype(int)\n",
                "x2=np.add(x,x1)\n",
                "x2.astype(int)\n",
                "test['night']=x2.astype(int)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x=train['hour']<7\n",
                "x1=train['hour']>19\n",
                "x.astype(int)\n",
                "x1.astype(int)\n",
                "x2=np.add(x,x1)\n",
                "x2.astype(int)\n",
                "train['night']=x2.astype(int)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train=train.drop('TransactionDT',axis=1)\n",
                "test=test.drop('TransactionDT',axis=1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for c1, c2 in train.dtypes.reset_index().values:\n",
                "    if c2=='O':\n",
                "        train[c1] = train[c1].map(lambda x: labels[str(x).lower()])\n",
                "        test[c1] = test[c1].map(lambda x: labels[str(x).lower()])\n",
                "train.fillna(-999, inplace=True)\n",
                "test.fillna(-999, inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train = train.fillna(-999)\n",
                "test = test.fillna(-999)\n",
                "for f in test.columns:\n",
                "    if train[f].dtype=='object' or test[f].dtype=='object': \n",
                "        print(f)\n",
                "        lbl = preprocessing.LabelEncoder()\n",
                "        lbl.fit(list(train[f].values) + list(test[f].values))\n",
                "        train[f] = lbl.transform(list(train[f].values))\n",
                "        test[f] = lbl.transform(list(test[f].values)) "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train['TransactionAmt']=np.log(train['TransactionAmt'])\n",
                "test['TransactionAmt']=np.log(test['TransactionAmt'])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "'''columns_np=train.columns.values\n",
                "columns_list=list(train)\n",
                "train_12=train[train.month==12]\n",
                "train_1=train[train.month==1]\n",
                "train_2=train[train.month==2]\n",
                "train_3=train[train.month==3]\n",
                "train_4=train[train.month==4]\n",
                "train_5=train[train.month==5]\n",
                "train_6=train[train.month==6]'''\n",
                "'''\n",
                "test_7=test[test.month==7]\n",
                "test_8=test[test.month==8]\n",
                "test_9=test[test.month==9]\n",
                "test_10=test[test.month==10]\n",
                "test_11=test[test.month==11]\n",
                "test_12=test[test.month==12]'''"
            ]
        },
        {
            "tags": [
                "process_data",
                "setup_notebook"
            ],
            "source": [
                "'''from imblearn.under_sampling import RandomUnderSampler\n",
                "ran=RandomUnderSampler(return_indices=True) ##intialize to return indices of dropped rows\n",
                "train_12,y_train12,dropped12 = ran.fit_sample(train_12,train_12['isFraud'])\n",
                "train_1,y_train1,dropped1 = ran.fit_sample(train_1,train_1['isFraud'])\n",
                "train_2,y_train2,dropped2 = ran.fit_sample(train_2,train_2['isFraud'])\n",
                "train_3,y_train3,dropped3 = ran.fit_sample(train_3,train_3['isFraud'])\n",
                "train_4,y_train4,dropped4 = ran.fit_sample(train_4,train_4['isFraud'])\n",
                "train_5,y_train5,dropped5 = ran.fit_sample(train_5,train_5['isFraud'])\n",
                "train_6,y_train6,dropped6 = ran.fit_sample(train_6,train_6['isFraud'])\n",
                "del train\n",
                "del y_train12\n",
                "del y_train1\n",
                "del y_train2\n",
                "del y_train3\n",
                "del y_train4\n",
                "del y_train5\n",
                "del y_train6'''"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "'''train_12=pd.DataFrame(np.array(train_12))\n",
                "train_1=pd.DataFrame(np.array(train_1))\n",
                "train_2=pd.DataFrame(np.array(train_2))\n",
                "train_3=pd.DataFrame(np.array(train_3))\n",
                "train_4=pd.DataFrame(np.array(train_4))\n",
                "train_5=pd.DataFrame(np.array(train_5))\n",
                "train_6=pd.DataFrame(np.array(train_6))\n",
                "train=pd.concat([train_12,train_1,train_2,train_3,train_4,train_5,train_6],axis=0)\n",
                "train.columns=columns_list\n",
                "del train_1\n",
                "del train_2\n",
                "del train_12\n",
                "del train_3\n",
                "del train_4\n",
                "del train_5\n",
                "del train_6'''"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "'''train=pd.DataFrame(np.array(train))\n",
                "train.columns=columns_list'''"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "y_train=train['isFraud']\n",
                "train=train.drop('isFraud',axis=1)\n",
                "train=train.drop('month',axis=1)\n",
                "test=test.drop('month',axis=1)\n",
                "X_test = test.copy()\n",
                "del test; gc.collect()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from imblearn.over_sampling import RandomOverSampler\n",
                "ros = RandomOverSampler(random_state=0)\n",
                "train_data, y_train = ros.fit_sample(train, y_train)\n",
                "del train; gc.collect()'''\n",
                "from imblearn.over_sampling import SMOTE\n",
                "smote = SMOTE(frac=0.1, random_state=1)\n",
                "train_data, y_train = smote.fit_sample(train, y_train)'''\n",
                "\n",
                "'''from imblearn.under_sampling import RandomUnderSampler\n",
                "ran=RandomUnderSampler(return_indices=True) ##intialize to return indices of dropped rows\n",
                "train_data,y_train,dropped = ran.fit_sample(train,y_train)\n",
                "del train; gc.collect()'''"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(y_train)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cols_to_drop=['V108', 'id_22', 'id_26', 'id_25', 'C3', 'V14', 'id_24', 'id_07', 'V121', 'V65', 'V305', 'V311', 'V286', 'V111', 'V300', 'V115', 'V113', 'V301',  'V25', 'V123', 'V118', 'V109', 'id_23', 'V112', 'V114', 'V120', 'V88', 'V107', 'V117', 'V122', 'V116', 'id_08', 'id_27', 'id_21', 'V110', 'V119']\n",
                "\n",
                "train = train.drop(cols_to_drop, axis=1)\n",
                "X_test = X_test.drop(cols_to_drop, axis=1)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "process_data"
            ],
            "source": [
                "def fast_auc(y_true, y_prob):\n",
                "    \"\"\"\n",
                "    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n",
                "    \"\"\"\n",
                "    y_true = np.asarray(y_true)\n",
                "    y_true = y_true[np.argsort(y_prob)]\n",
                "    nfalse = 0\n",
                "    auc = 0\n",
                "    n = len(y_true)\n",
                "    for i in range(n):\n",
                "        y_i = y_true[i]\n",
                "        nfalse += (1 - y_i)\n",
                "        auc += y_i * nfalse\n",
                "    auc /= (nfalse * (n - nfalse))\n",
                "    return auc\n",
                "\n",
                "\n",
                "def eval_auc(y_true, y_pred):\n",
                "    \"\"\"\n",
                "    Fast auc eval function for lgb.\n",
                "    \"\"\"\n",
                "    return 'auc', fast_auc(y_true, y_pred), True\n",
                "\n",
                "\n",
                "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
                "    \"\"\"\n",
                "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
                "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
                "    \"\"\"\n",
                "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
                "    return np.log(maes.map(lambda x: max(x, floor))).mean()\n",
                "def train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n",
                "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n",
                "    \"\"\"\n",
                "    A function to train a variety of classification models.\n",
                "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
                "    \n",
                "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
                "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
                "    :params: y - target\n",
                "    :params: folds - folds to split data\n",
                "    :params: model_type - type of model to use\n",
                "    :params: eval_metric - metric to use\n",
                "    :params: columns - columns to use. If None - use all columns\n",
                "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
                "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
                "    \n",
                "    \"\"\"\n",
                "    columns = X.columns if columns is None else columns\n",
                "    n_splits = folds.n_splits if splits is None else n_folds\n",
                "    X_test = X_test[columns]\n",
                "    \n",
                "    # to set up scoring parameters\n",
                "    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n",
                "                        'catboost_metric_name': 'AUC',\n",
                "                        'sklearn_scoring_function': metrics.roc_auc_score},\n",
                "                    }\n",
                "    \n",
                "    result_dict = {}\n",
                "    if averaging == 'usual':\n",
                "        # out-of-fold predictions on train data\n",
                "        oof = np.zeros((len(X), 1))\n",
                "\n",
                "        # averaged predictions on train data\n",
                "        prediction = np.zeros((len(X_test), 1))\n",
                "        \n",
                "    elif averaging == 'rank':\n",
                "        # out-of-fold predictions on train data\n",
                "        oof = np.zeros((len(X), 1))\n",
                "\n",
                "        # averaged predictions on train data\n",
                "        prediction = np.zeros((len(X_test), 1))\n",
                "\n",
                "    \n",
                "    # list of scores on folds\n",
                "    scores = []\n",
                "    feature_importance = pd.DataFrame()\n",
                "    \n",
                "    # split and train on folds\n",
                "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
                "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
                "        if type(X) == np.ndarray:\n",
                "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
                "            y_train, y_valid = y[train_index], y[valid_index]\n",
                "        else:\n",
                "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
                "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
                "            \n",
                "        if model_type == 'lgb':\n",
                "            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n",
                "            model.fit(X_train, y_train, \n",
                "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
                "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
                "            \n",
                "            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n",
                "            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n",
                "            \n",
                "        if model_type == 'xgb':\n",
                "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
                "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
                "\n",
                "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
                "            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n",
                "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
                "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
                "        \n",
                "        if model_type == 'sklearn':\n",
                "            model = model\n",
                "            model.fit(X_train, y_train)\n",
                "            \n",
                "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
                "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
                "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
                "            print('')\n",
                "            \n",
                "            y_pred = model.predict_proba(X_test)\n",
                "        \n",
                "        if model_type == 'cat':\n",
                "            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
                "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
                "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
                "\n",
                "            y_pred_valid = model.predict(X_valid)\n",
                "            y_pred = model.predict(X_test)\n",
                "        \n",
                "        if averaging == 'usual':\n",
                "            \n",
                "            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n",
                "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
                "            \n",
                "            prediction += y_pred.reshape(-1, 1)\n",
                "\n",
                "        elif averaging == 'rank':\n",
                "                                  \n",
                "            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n",
                "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
                "                                  \n",
                "            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n",
                "        \n",
                "        if model_type == 'lgb' and plot_feature_importance:\n",
                "            # feature importance\n",
                "            fold_importance = pd.DataFrame()\n",
                "            fold_importance[\"feature\"] = columns\n",
                "            fold_importance[\"importance\"] = model.feature_importances_\n",
                "            fold_importance[\"fold\"] = fold_n + 1\n",
                "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
                "\n",
                "    prediction /= n_splits\n",
                "    \n",
                "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
                "    \n",
                "    result_dict['oof'] = oof\n",
                "    result_dict['prediction'] = prediction\n",
                "    result_dict['scores'] = scores\n",
                "    \n",
                "    if model_type == 'lgb':\n",
                "        if plot_feature_importance:\n",
                "            feature_importance[\"importance\"] /= n_splits\n",
                "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
                "                by=\"importance\", ascending=False)[:50].index\n",
                "\n",
                "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
                "\n",
                "            plt.figure(figsize=(16, 12));\n",
                "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
                "            plt.title('LGB Features (avg over folds)');\n",
                "            \n",
                "            result_dict['feature_importance'] = feature_importance\n",
                "            result_dict['top_columns'] = cols\n",
                "        \n",
                "    return result_dict"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.model_selection import TimeSeriesSplit\n",
                "n_fold = 7\n",
                "folds = TimeSeriesSplit(n_splits=n_fold)\n",
                "folds = KFold(n_splits=5)"
            ]
        },
        {
            "tags": [
                "train_model",
                "setup_notebook"
            ],
            "source": [
                "import time\n",
                "params = {'num_leaves': 256,\n",
                "          'min_child_samples': 79,\n",
                "          'objective': 'binary',\n",
                "          'max_depth': 13,\n",
                "          'learning_rate': 0.03,\n",
                "          \"boosting_type\": \"gbdt\",\n",
                "          \"subsample_freq\": 3,\n",
                "          \"subsample\": 0.9,\n",
                "          \"bagging_seed\": 11,\n",
                "          \"metric\": 'auc',\n",
                "          \"verbosity\": -1,\n",
                "          'reg_alpha': 0.3,\n",
                "          'reg_lambda': 0.3,\n",
                "          'colsample_bytree': 0.9,\n",
                "          # 'categorical_feature': cat_cols\n",
                "          'tree_method':'gpu_hist'\n",
                "          }\n",
                "result_dict_lgb = train_model_classification(X=train, X_test=X_test, y=y_train, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n",
                "                                             verbose=500, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "transfer_results",
                "process_data"
            ],
            "source": [
                "sample_submission = pd.read_csv('../input/sample_submission.csv')\n",
                "sample_submission['isFraud'] = result_dict_lgb['prediction'] \n",
                "sample_submission.to_csv('submission_IEEE_.csv',index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import datetime\n",
                "import gc\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import lightgbm as lgb\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import mean_squared_error\n",
                "import time\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "np.random.seed(4590)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "df_train = pd.read_csv('../input/elo-merchant-category-recommendation/train.csv')\n",
                "df_test = pd.read_csv('../input/elo-merchant-category-recommendation/test.csv')\n",
                "df_hist_trans = pd.read_csv('../input/elo-merchant-category-recommendation/historical_transactions.csv')\n",
                "df_new_merchant_trans = pd.read_csv('../input/elo-merchant-category-recommendation/new_merchant_transactions.csv')\n",
                "df_merchants = pd.read_csv('../input/elo-merchant-category-recommendation/merchants.csv')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "gdf = df_hist_trans.groupby(\"card_id\")\n",
                "print(type(gdf))\n",
                "gdf = gdf.agg({'merchant_category_id':['min']}).reset_index()\n",
                "print(type(gdf))\n",
                "gdf.columns = [\"card_id\", \"merchant_category_id\"]\n",
                "df_train = pd.merge(df_train, gdf, on=\"card_id\", how=\"left\")\n",
                "df_test = pd.merge(df_test, gdf, on=\"card_id\", how=\"left\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "gdf = df_hist_trans.groupby(\"card_id\")\n",
                "print(type(gdf))\n",
                "gdf = gdf.agg({'merchant_category_id':['max']}).reset_index()\n",
                "print(type(gdf))\n",
                "gdf.columns = [\"card_id\", \"max_merchant_category_id\"]\n",
                "df_train = pd.merge(df_train, gdf, on=\"card_id\", how=\"left\")\n",
                "df_test = pd.merge(df_test, gdf, on=\"card_id\", how=\"left\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for df in [df_hist_trans,df_new_merchant_trans]:\n",
                "    df['category_2'].fillna(1.0,inplace=True)\n",
                "    df['category_3'].fillna('A',inplace=True)\n",
                "    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for df in [df_hist_trans,df_new_merchant_trans]:\n",
                "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
                "    df['year'] = df['purchase_date'].dt.year\n",
                "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
                "    df['month'] = df['purchase_date'].dt.month\n",
                "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
                "    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n",
                "    df['hour'] = df['purchase_date'].dt.hour\n",
                "    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n",
                "    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n",
                "    #https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/73244\n",
                "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
                "    df['month_diff'] += df['month_lag']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "aggs = {}\n",
                "for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
                "    aggs[col] = ['nunique']\n",
                "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
                "aggs['installments'] = ['sum','max','min','mean','var']\n",
                "aggs['purchase_date'] = ['max','min']\n",
                "aggs['month_lag'] = ['max','min','mean','var']\n",
                "aggs['month_diff'] = ['mean']\n",
                "aggs['weekend'] = ['sum', 'mean']\n",
                "aggs['category_1'] = ['sum', 'mean']\n",
                "aggs['card_id'] = ['size']\n",
                "\n",
                "for col in ['category_2','category_3']:\n",
                "    df_new_merchant_trans[col+'_mean'] = df_new_merchant_trans.groupby([col])['purchase_amount'].transform('mean')\n",
                "    aggs[col+'_mean'] = ['mean']\n",
                "    \n",
                "new_columns = get_new_columns('new_hist',aggs)\n",
                "df_hist_trans_group = df_new_merchant_trans.groupby('card_id').agg(aggs)\n",
                "df_hist_trans_group.columns = new_columns\n",
                "df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
                "df_hist_trans_group['new_hist_purchase_date_diff'] = (df_hist_trans_group['new_hist_purchase_date_max'] - df_hist_trans_group['new_hist_purchase_date_min']).dt.days\n",
                "df_hist_trans_group['new_hist_purchase_date_average'] = df_hist_trans_group['new_hist_purchase_date_diff']/df_hist_trans_group['new_hist_card_id_size']\n",
                "df_hist_trans_group['new_hist_purchase_date_uptonow'] = (datetime.datetime.today() - df_hist_trans_group['new_hist_purchase_date_max']).dt.days\n",
                "df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n",
                "df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n",
                "del df_hist_trans_group;gc.collect()\n",
                "del df_new_merchant_trans;gc.collect()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_train.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "aggs = {}\n",
                "for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
                "    aggs[col] = ['nunique']\n",
                "\n",
                "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
                "aggs['installments'] = ['sum','max','min','mean','var']\n",
                "aggs['purchase_date'] = ['max','min']\n",
                "aggs['month_lag'] = ['max','min','mean','var']\n",
                "aggs['month_diff'] = ['mean']\n",
                "aggs['authorized_flag'] = ['sum', 'mean']\n",
                "aggs['weekend'] = ['sum', 'mean']\n",
                "aggs['category_1'] = ['sum', 'mean']\n",
                "aggs['card_id'] = ['size']\n",
                "\n",
                "for col in ['category_2','category_3']:\n",
                "    df_hist_trans[col+'_mean'] = df_hist_trans.groupby([col])['purchase_amount'].transform('mean')\n",
                "    aggs[col+'_mean'] = ['mean']    \n",
                "\n",
                "new_columns = get_new_columns('hist',aggs)\n",
                "df_hist_trans_group = df_hist_trans.groupby('card_id').agg(aggs)\n",
                "df_hist_trans_group.columns = new_columns\n",
                "df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
                "df_hist_trans_group['hist_purchase_date_diff'] = (df_hist_trans_group['hist_purchase_date_max'] - df_hist_trans_group['hist_purchase_date_min']).dt.days\n",
                "df_hist_trans_group['hist_purchase_date_average'] = df_hist_trans_group['hist_purchase_date_diff']/df_hist_trans_group['hist_card_id_size']\n",
                "df_hist_trans_group['hist_purchase_date_uptonow'] = (datetime.datetime.today() - df_hist_trans_group['hist_purchase_date_max']).dt.days\n",
                "df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n",
                "df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n",
                "del df_hist_trans_group;gc.collect()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "del df_hist_trans;gc.collect()\n",
                "#del df_new_merchant_trans;gc.collect()\n",
                "df_train.head(5)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import Imputer\n",
                "imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
                "df_merchants['avg_sales_lag3']=imp.fit(pd.DataFrame(df_merchants['avg_sales_lag3'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['avg_sales_lag3'].values.reshape(-1,1)))\n",
                "df_merchants['avg_sales_lag6']=imp.fit(pd.DataFrame(df_merchants['avg_sales_lag6'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['avg_sales_lag6'].values.reshape(-1,1)))\n",
                "df_merchants['avg_sales_lag12']=imp.fit(pd.DataFrame(df_merchants['avg_sales_lag12'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['avg_sales_lag12'].values.reshape(-1,1)))\n",
                "df_merchants['category_2']=imp.fit(pd.DataFrame(df_merchants['category_2'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['category_2'].values.reshape(-1,1)))\n",
                "df_merchants.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "aggs={}\n",
                "for col in ['avg_sales_lag3','avg_purchases_lag3','active_months_lag3','avg_sales_lag6','avg_purchases_lag6','active_months_lag6','avg_sales_lag12','avg_purchases_lag12','active_months_lag12','numerical_1','numerical_2']:\n",
                "    aggs[col]= ['mean']\n",
                "    \n",
                "new_columns= get_new_columns('merchants',aggs)\n",
                "df_merchants_group = df_merchants.groupby('merchant_category_id').agg(aggs)\n",
                "df_merchants_group.columns = new_columns\n",
                "df_merchants_group.reset_index(drop=False,inplace=True)\n",
                "df_train=df_train.merge(df_merchants_group,on='merchant_category_id',how='left')\n",
                "df_test=df_test.merge(df_merchants_group.reset_index(),on='merchant_category_id',how='left')\n",
                "df_train.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "df_merchants['max_merchant_category_id']=df_merchants['merchant_category_id']\n",
                "#df_test['max_merchant_category_id']=df_test['merchant_category_id']\n",
                "\n",
                "aggs={}\n",
                "for col in ['avg_sales_lag3','avg_purchases_lag3','active_months_lag3','avg_sales_lag6','avg_purchases_lag6','active_months_lag6','avg_sales_lag12','avg_purchases_lag12','active_months_lag12','numerical_1','numerical_2']:\n",
                "    aggs[col]= ['mean']\n",
                "    \n",
                "new_columns= get_new_columns('max_merchants',aggs)\n",
                "df_merchants_group = df_merchants.groupby('max_merchant_category_id').agg(aggs)\n",
                "df_merchants_group.columns = new_columns\n",
                "df_merchants_group.reset_index(drop=False,inplace=True)\n",
                "df_train=df_train.merge(df_merchants_group,on='max_merchant_category_id',how='left')\n",
                "df_test=df_test.merge(df_merchants_group.reset_index(),on='max_merchant_category_id',how='left')\n",
                "df_train.head()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "df_train['outliers'] = 0\n",
                "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
                "df_train['outliers'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for df in [df_train,df_test]:\n",
                "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
                "    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n",
                "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
                "    df['month'] = df['first_active_month'].dt.month\n",
                "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
                "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
                "    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
                "    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n",
                "                     'new_hist_purchase_date_min']:\n",
                "        df[f] = df[f].astype(np.int64) * 1e-9\n",
                "    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n",
                "    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
                "\n",
                "for f in ['feature_1','feature_2','feature_3']:\n",
                "    order_label = df_train.groupby([f])['outliers'].mean()\n",
                "    df_train[f] = df_train[f].map(order_label)\n",
                "    df_test[f] = df_test[f].map(order_label)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n",
                "target = df_train['target']\n",
                "del df_train['target']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "features = [c for c in df_train.columns if c not in ['card_id', 'first_active_month']]\n",
                "categorical_feats = [c for c in features if 'feature_' in c]"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "param = {'num_leaves': 31,\n",
                "         'min_data_in_leaf': 30, \n",
                "         'objective':'binary',\n",
                "         'max_depth': 6,\n",
                "         'learning_rate': 0.01,\n",
                "         \"boosting\": \"rf\",\n",
                "         \"feature_fraction\": 0.9,\n",
                "         \"bagging_freq\": 1,\n",
                "         \"bagging_fraction\": 0.9 ,\n",
                "         \"bagging_seed\": 11,\n",
                "         \"metric\": 'binary_logloss',\n",
                "         \"lambda_l1\": 0.1,\n",
                "         \"verbosity\": -1,\n",
                "         \"random_state\": 2333}"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "from sklearn.model_selection import StratifiedKFold, KFold\n",
                "from sklearn.metrics import mean_squared_error\n",
                "from sklearn.metrics import log_loss\n",
                "%%time\n",
                "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
                "oof = np.zeros(len(df_train))\n",
                "predictions = np.zeros(len(df_test))\n",
                "feature_importance_df = pd.DataFrame()\n",
                "\n",
                "start = time.time()\n",
                "\n",
                "\n",
                "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, target.values)):\n",
                "    print(\"fold n{}\".format(fold_))\n",
                "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
                "    val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
                "\n",
                "    num_round = 10000\n",
                "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
                "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
                "    \n",
                "    fold_importance_df = pd.DataFrame()\n",
                "    fold_importance_df[\"feature\"] = features\n",
                "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
                "    fold_importance_df[\"fold\"] = fold_ + 1\n",
                "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
                "    \n",
                "    predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
                "\n",
                "print(\"CV score: {:<8.5f}\".format(log_loss(target, oof)))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "outlier_id = pd.DataFrame(df_outlier_prob.sort_values(by='target',ascending = False).head(25000)['card_id'])"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "check_results"
            ],
            "source": [
                "best_submission = pd.read_csv('../input/finaldata/submission_ashish.csv')\n",
                "most_likely_liers = best_submission.merge(outlier_id,how='right')\n",
                "most_likely_liers.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "%%time\n",
                "for card_id in most_likely_liers['card_id']:\n",
                "    model_without_outliers.loc[model_without_outliers['card_id']==card_id,'target']\\\n",
                "    = most_likely_liers.loc[most_likely_liers['card_id']==card_id,'target'].values"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "model_without_outliers.to_csv(\"combining_submission.csv\", index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import gc\n",
                "import os\n",
                "import pickle\n",
                "import random\n",
                "import re\n",
                "import sklearn.metrics\n",
                "import sklearn.model_selection\n",
                "import time\n",
                "import warnings\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from collections import OrderedDict\n",
                "from tqdm import tqdm, tqdm_notebook\n",
                "import functools\n",
                "import multiprocessing\n",
                "import sklearn.preprocessing\n",
                "import unicodedata\n",
                "import copy\n",
                "import time\n",
                "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.utils.data"
            ]
        },
        {
            "tags": [
                "check_results",
                "setup_notebook"
            ],
            "source": [
                "KAGGLE_RUN = (not os.path.exists('/opt/conda/home/.history'))\n",
                "if KAGGLE_RUN: print('Kaggle run')\n",
                "\n",
                "pd.options.display.float_format = '{:.6f}'.format"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "def tf_seed_everything(seed):\n",
                "    import tensorflow as tf\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    tf.set_random_seed(seed)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def f1_curve(target, preds, t_min=0.01, t_max=0.99, steps=99):\n",
                "    curve = {}\n",
                "    for t in np.linspace(t_min, t_max, steps):\n",
                "        with warnings.catch_warnings():\n",
                "            warnings.simplefilter(\"ignore\")\n",
                "            curve[t] = sklearn.metrics.f1_score(target, preds >= t)\n",
                "    return pd.Series(curve).sort_index()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "def torch_seed(seed):\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "def torch_gc():\n",
                "    import gc\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def keras_rnn_init(m):\n",
                "    for name, param in m.named_parameters():\n",
                "        if 'weight_ih' in name: torch.nn.init.xavier_uniform_(param)\n",
                "        if 'weight_hh' in name: torch.nn.init.orthogonal_(param)\n",
                "        if 'bias_' in name: torch.nn.init.constant_(param, 0)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "%%time\n",
                "torch_seed(42)\n",
                "\n",
                "qd = QuoraData()\n",
                "qd.convert_start(['glove', 'wiki'])\n",
                "\n",
                "prep = QuoraPreprocessor()\n",
                "input_df = qd.read_input()\n",
                "input_df['question_text'] = prep.transform(input_df.question_text)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "%%time\n",
                "qfe = QuoraFeatureExtractor(num_words=95000, max_len=70)\n",
                "input_X = qfe.fit_transform(input_df.question_text, fit_mask=input_df.target.notnull().values)\n",
                "print({ k: v.shape for (k, v) in input_X.items() })"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "%%time\n",
                "fold = Fold(input_df, input_X, seed=1005); torch_seed(fold.seed)\n",
                "learn1 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "%%time\n",
                "fold = Fold(input_df, input_X, seed=555); torch_seed(fold.seed)\n",
                "learn2 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "%%time\n",
                "fold = Fold(input_df, input_X, seed=1111); torch_seed(fold.seed)\n",
                "learn3 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "%%time\n",
                "fold = Fold(input_df, input_X, seed=1111); torch_seed(fold.seed)\n",
                "learn3 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "%%time\n",
                "fold = Fold(input_df, input_X, seed=3333); torch_seed(fold.seed)\n",
                "learn4 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "transfer_results"
            ],
            "source": [
                "%%time\n",
                "test_idx = fold.test_idx\n",
                "hold_idx = fold.holdout_idx\n",
                "hold_y = input_df.loc[fold.holdout_idx, 'target']\n",
                "ensemble_hold = []\n",
                "ensemble_test = []\n",
                "for li, learn in enumerate([learn1, learn2, learn3, learn4]):\n",
                "    for ep in [5]:\n",
                "        if ep >= len(learn.epoch_params): continue\n",
                "        learn.restore_epoch(ep)\n",
                "        ensemble_hold.append(learn.predict(idx=hold_idx))\n",
                "        f1 = f1_curve(hold_y, ensemble_hold[-1]).max()\n",
                "        print('learn%d ep%d %.6f' % (li+1, ep, f1))\n",
                "        ensemble_test.append(learn.predict(idx=test_idx))\n",
                "\n",
                "ensemble_test_s = pd.Series(np.mean(ensemble_test, axis=0), index=fold.data.loc[test_idx, 'qid'])\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "score = f1_curve(hold_y, np.mean(ensemble_hold, axis=0))\n",
                "thresh = score.idxmax()\n",
                "print('F1score=',score.max())\n",
                "\n",
                "\n",
                "\n",
                "submission_df = pd.read_csv('../input/sample_submission.csv')\n",
                "submission_df['prediction'] = (ensemble_test_s.loc[submission_df.qid] >= thresh).astype(int).values\n",
                "submission_df.to_csv('submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "from tqdm import tqdm"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "# Read dataset, divide it into train and test set\n",
                "dataset = pd.read_csv(\"/kaggle/input/iris/Iris.csv\")\n",
                "dataset.loc[dataset[\"Species\"] == \"Iris-setosa\",\"Species\"] = 1\n",
                "dataset.loc[dataset[\"Species\"] == \"Iris-versicolor\",\"Species\"] = 2\n",
                "dataset.loc[dataset[\"Species\"] == \"Iris-virginica\",\"Species\"] = 3\n",
                "dataset = dataset.to_numpy()\n",
                "np.random.shuffle(dataset)\n",
                "\n",
                "dataset = np.asarray(dataset,dtype = np.float64)\n",
                "len_dataset = dataset.shape[0]\n",
                "\n",
                "train = dataset[:int(0.75*len_dataset),:]\n",
                "X = train[:,1:-1]\n",
                "Y = train[:,-1]\n",
                "Y = pd.get_dummies(Y)\n",
                "Y = Y.to_numpy()\n",
                "op_neurons=len(Y[0])\n",
                "N,p = X.shape\n",
                "\n",
                "test = dataset[int(0.75*len_dataset):,:]\n",
                "Xt = test[:,1:-1]\n",
                "Yt = test[:,-1]\n",
                "Yt = pd.get_dummies(Yt)\n",
                "Yt = Yt.to_numpy()\n",
                "Nt,pt = Xt.shape"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "# dictionary\n",
                "# Implementation of Network using Gradient Descent\n",
                "epochs = 10000\n",
                "alpha = 0.1\n",
                "costs = []\n",
                "for num in tqdm(range(epochs)):\n",
                "    #Forward Propogation\n",
                "    a1 = np.dot(parameters[\"W1\"],X.T) + parameters[\"b1\"]\n",
                "    h1 = sigmoid(a1)\n",
                "    a2 = np.dot(parameters[\"W2\"],h1) + parameters[\"b2\"]\n",
                "    h2 = sigmoid(a2)\n",
                "    a3 = np.dot(parameters[\"W3\"],h2) + parameters[\"b3\"]\n",
                "    h3 = softmax(a3)\n",
                "    # Gradients for Backpropogation\n",
                "    \n",
                "    dL_da3 = -( Y.T - h3)\n",
                "    dL_dW3 = (1/N)*np.dot(dL_da3 , h2.T)\n",
                "    dL_db3 = (1/N)*(np.sum(dL_da3,axis=1,keepdims = True))\n",
                "    \n",
                "    dL_dh2 = np.dot(parameters[\"W3\"].T , dL_da3)\n",
                "    dL_da2 = np.multiply(dL_dh2,sigmoid_derivative(a2))\n",
                "    dL_dW2 = (1/N)*np.dot(dL_da2 , h1.T)\n",
                "    dL_db2 = (1/N)*(np.sum(dL_da2,axis=1,keepdims = True))\n",
                "    \n",
                "    dL_dh1 = np.dot(parameters[\"W2\"].T , dL_da2)\n",
                "    dL_da1 = np.multiply(dL_dh1,sigmoid_derivative(a1))\n",
                "    dL_dW1 = (1/N)*np.dot(dL_da1 , X)\n",
                "    dL_db1 = (1/N)*(np.sum(dL_da1,axis = 1,keepdims = True))\n",
                "    \n",
                "    # GD Updates\n",
                "    parameters[\"W3\"] = parameters[\"W3\"] - (alpha)*dL_dW3\n",
                "    parameters[\"b3\"] = parameters[\"b3\"] - (alpha)*dL_db3\n",
                "    parameters[\"W2\"] = parameters[\"W2\"] - (alpha)*dL_dW2\n",
                "    parameters[\"b2\"] = parameters[\"b2\"] - (alpha)*dL_db2\n",
                "    parameters[\"W1\"] = parameters[\"W1\"] - (alpha)*dL_dW1\n",
                "    parameters[\"b1\"] = parameters[\"b1\"] - (alpha)*dL_db1\n",
                "    costs.append(compute_cost(h3.T,Y))\n",
                "plt.plot(costs)\n",
                "print(\"Training Cost\",costs[-1])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "# Libraries\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "\n",
                "# set seed for reproducibility\n",
                "np.random.seed(0)\n",
                "\n",
                "#Data\n",
                "tumor_data = pd.read_csv('../input/data.csv')\n",
                "tumor_data.sample(5)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(\"The dataset has %d rows and %d columns\" % (tumor_data.shape[0], tumor_data.shape[1]))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tumor_data.describe(include='all')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tumor_data.dtypes"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tumor_data['Unnamed: 32'].sample(8)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "missing_values = tumor_data['Unnamed: 32'].isnull().sum()\n",
                "number_of_rows = tumor_data['Unnamed: 32'].shape[0]\n",
                "if missing_values == number_of_rows:\n",
                "    print('The whole \\'Unnamed: 32\\' column has empty values.')\n",
                "else:\n",
                "    print('There are non-empty values in the \\'Unnamed: 32\\' column.')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tumor_data.drop(['Unnamed: 32'], axis= 1, inplace = True)\n",
                "tumor_data.columns"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tumor_data.isna().sum()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tumor_data['target'] = tumor_data['diagnosis'].replace({'B': 1, 'M': 0})\n",
                "# Let's show if the convertion was rightly done\n",
                "tumor_data[['id', 'diagnosis', 'target']].sample(5)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tumor_data.drop(['diagnosis'], axis = 1, inplace = True)\n",
                "tumor_data.columns"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tumor_data.drop(['id'], axis = 1, inplace=True)\n",
                "tumor_data.columns"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.pairplot(\n",
                "    tumor_data,\n",
                "    vars = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean'],\n",
                "    hue = 'target'\n",
                ")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# First, we make sure that the graphic is crearly visible\n",
                "plt.figure(figsize = (30, 20))\n",
                "# And now, draw the heatmap\n",
                "sns.heatmap(tumor_data.corr(), cmap = \"RdBu_r\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Independent variables\n",
                "X = tumor_data.drop(['target'], axis = 1)\n",
                "# Dependent variable\n",
                "Y = tumor_data['target']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "from sklearn.svm import SVC\n",
                "svc_model = SVC()\n",
                "svc_model.fit(X_train, Y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "Y_predicted = svc_model.predict(X_test)\n",
                "Y_predicted"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "cm = confusion_matrix(Y_test, Y_predicted)\n",
                "sns.heatmap(cm, annot = True, cmap=\"Blues\")"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "print(classification_report(Y_test, Y_predicted))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tumor_data.describe()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "min_train = X_train.min()\n",
                "range_train = (X_train - min_train).max()\n",
                "X_train_scaled = (X_train - min_train)/range_train\n",
                "X_train_scaled.describe()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = plt.figure(figsize = (20, 5))\n",
                "ax1 = fig.add_subplot(121)\n",
                "ax2 = fig.add_subplot(122)\n",
                "ax1.set_title('Values without normalization')\n",
                "ax2.set_title('Values with normalization')\n",
                "sns.scatterplot(x = X_train['texture_mean'], y = X_train['area_mean'], hue = Y_train, ax = ax1)\n",
                "sns.scatterplot(x = X_train_scaled['texture_mean'], y = X_train_scaled['area_mean'], hue = Y_train, ax = ax2)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "# Train the model again\n",
                "svc_model.fit(X_train_scaled, Y_train)\n",
                "# Create scaled test data\n",
                "X_test_scaled = (X_test - X_test.min())/(X_test - X_test.min()).max()\n",
                "# Calculate new predictions\n",
                "Y_predicted = svc_model.predict(X_test_scaled)\n",
                "# Draw confusion matrix\n",
                "cm = confusion_matrix(Y_test, Y_predicted)\n",
                "sns.heatmap(cm, annot = True, cmap='Blues')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "print(classification_report(Y_test, Y_predicted))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "check_results"
            ],
            "source": [
                "# We can automate the refinement of C and gamma using the GridSearchCV library\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "C_values =  [0.1, 1, 10, 100, 1000]\n",
                "gamma_values = [1, 0.1, 0.01, 0.001]\n",
                "grid = GridSearchCV(SVC(), {'C': C_values, 'gamma': gamma_values, 'kernel': ['rbf']}, refit = True, verbose = 4)\n",
                "# Find best pair of C and gamma values\n",
                "grid.fit(X_train_scaled, Y_train)\n",
                "grid.best_params_"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data",
                "check_results"
            ],
            "source": [
                "# We can use the optimized grid object directly to get predictions\n",
                "grid_predicted = grid.predict(X_test_scaled)\n",
                "\n",
                "cm = confusion_matrix(Y_test, grid_predicted)\n",
                "sns.heatmap(cm, annot = True, cmap = 'Blues')\n",
                "print(classification_report(Y_test, grid_predicted))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#1 Write a python program using Scikit_learn to print the keys, number of rows-columns , feature names and the description of the iris data.\n",
                "import pandas as pd\n",
                "iris_data = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(\"\\nKeys of Iris dataset:\")\n",
                "print(iris_data.keys())\n",
                "print(\"\\nNumber of rows and columns of Iris dataset:\")\n",
                "print(iris_data.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#2 Write a python program to get the number of observations,missing values and nan values.\n",
                "import pandas as pd\n",
                "iris = pd.read_csv(\"../input/daily-sun-spot-data-1818-to-2019/sunspot_data.csv\")\n",
                "print(iris.info())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "check_results"
            ],
            "source": [
                "#3 Write a python program to create a 2-D array with ones on the diagonal and zeroes elsewhere.\n",
                "import numpy as np\n",
                "from scipy import sparse\n",
                "eye = np.eye(6)\n",
                "print(\"NumPy array:\\n\", eye)\n",
                "sparse_matrix = sparse.csr_matrix(eye)\n",
                "print(\"\\nSciPy sparse CSR matrix:\\n\", sparse_matrix)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#4 Write a python program to load the iris data framefrom a given csv file into a dataframe and print the shape of the data, type of the data and first 3 rows.\n",
                "import pandas as pd\n",
                "data = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(\"Shape of the data:\")\n",
                "print(data.shape)\n",
                "print(\"\\nData Type:\")\n",
                "print(type(data))\n",
                "print(\"\\nFirst 3 rows:\")\n",
                "print(data.head(3))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "#Q.1 Write a Pandas program to get the powers of an array values element-wise.\n",
                "import pandas as pd\n",
                "df = pd.DataFrame({\"a\":[2,3,5,7,11,13,17],\"b\":[19,23,29,31,37,41,42],\"c\":[21,26,34,38,48,54,59]});\n",
                "print(df)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#Q.3 Write a panda sprogram to get the first three rows of a given DataFrame.\n",
                "import pandas as pd\n",
                "df = pd.read_csv(\"../input/prediction-of-asteroid-diameter/Asteroid.csv\");\n",
                "row = df[0:3];\n",
                "print(row)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#Q.4 Write a pandas program to select the specified columns and rows from a given DataFrame.\n",
                "import pandas as pd\n",
                "df = pd.read_csv(\"../input/prediction-of-asteroid-diameter/Asteroid.csv\");\n",
                "print(df.iloc[1:10,3:5])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#Q.5 write a pandas program to select the rows where the score is mising.\n",
                "import pandas as pd\n",
                "df = pd.read_csv(\"../input/prediction-of-asteroid-diameter/Asteroid.csv\");\n",
                "print(df[df[:].isnull()])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "import pandas as pd\n",
                "data = pd.read_csv(\"../input/titanic/train_and_test2.csv\")\n",
                "data\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "# 1.Create 1D,2D and boolean array using numpy.\n",
                "#1D array\n",
                "import numpy as np\n",
                "a = np.array([1,1,2,3,5,8,13,21,34,55,79])\n",
                "print(a)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "#2D array\n",
                "import numpy as np\n",
                "b = [2,3,5,7],[73,37,19,17],[101,111,153,59]\n",
                "c = np.array(b)\n",
                "print(c)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "#boolean array\n",
                "import numpy as np\n",
                "d = np.array([1,1,.5,0,0.66], dtype=bool)\n",
                "print(d)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "# 2. Extract the odd numbers from a 2D array using numpy package.\n",
                "import numpy as np\n",
                "b = np.array([[2,3,5], [1,6,11]], np.int32)\n",
                "c = (b[b%2!=0])\n",
                "print(c)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# 4. How to get the common items between two python numpy arrays?\n",
                "import numpy as np\n",
                "a1 = np.array([2,3,5,7,11])\n",
                "print(a1)\n",
                "a2 = [1,2,3,4,5,6,7,8,9]\n",
                "print(a2)\n",
                "print(\"common values between two arrays are\")\n",
                "print(np.intersect1d(a1,a2))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tupple1 = (1,1,2,3,5,8,21)\n",
                "print(tupple1)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "from sklearn.utils import shuffle\n",
                "import torch\n",
                "from torch import nn\n",
                "from torch.utils.data import Dataset\n",
                "import torch.nn.functional as F\n",
                "from torch.autograd import Variable\n",
                "from sklearn.metrics import mean_squared_error\n",
                "from datetime import datetime\n",
                "from datetime import timedelta\n",
                "from sklearn import preprocessing\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train = pd.read_csv('/kaggle/input/covid19-with-population/update_train_processed.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# The character ' will make later query function report an error,so it's replaced by a space\n",
                "train.Country.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "train.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "\n",
                "# There are few infinite values in the weather data,it will cause the training loss become NAN.Since the amount of np.inf is very few,it's simply replace by 0.\n",
                "train.replace(np.inf,0,inplace=True)\n",
                "\n",
                "# Transform percentage data to float\n",
                "def get_percent(x):\n",
                "    x = str(x)\n",
                "    x = x.strip('%')\n",
                "    x = int(x)/100\n",
                "    return x\n",
                "\n",
                "train.UrbanPopRate = train.UrbanPopRate.apply(lambda x:get_percent(x))\n",
                "\n",
                "# Transform date type\n",
                "def get_dt(x):\n",
                "    return datetime.strptime(x,'%Y-%m-%d')\n",
                "\n",
                "train.Date = train.Date.apply(lambda x:get_dt(x))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for index,row in train.iterrows():\n",
                "    if train.iloc[index].Province == train.iloc[index - 1].Province and train.iloc[index].ConfirmedCases < train.iloc[index-1].ConfirmedCases:\n",
                "        train.iloc[index,4] = train.iloc[index-1,4]\n",
                "    if train.iloc[index].Province == train.iloc[index - 1].Province and train.iloc[index].Fatalities < train.iloc[index-1].Fatalities:\n",
                "        train.iloc[index,5] = train.iloc[index-1,5]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_exam = train[['Country','Province','Date','ConfirmedCases','Fatalities']]\n",
                "diff_df = pd.DataFrame(columns = ['Country','Province','Date','ConfirmedCases','Fatalities'])\n",
                "for country in train_exam.Country.unique():\n",
                "    for province in train_exam[train_exam.Country == country].Province.unique():\n",
                "        province_df = train_exam.query(f\"Country == '{country}' and Province == '{province}'\")\n",
                "        conf = province_df.ConfirmedCases\n",
                "        fata = province_df.Fatalities\n",
                "        diff_conf = conf.diff()\n",
                "        diff_fata = fata.diff()\n",
                "        province_df.ConfirmedCases = diff_conf\n",
                "        province_df.Fatalities = diff_fata\n",
                "        diff_df = pd.concat([diff_df,province_df],0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(sum(diff_df.ConfirmedCases < 0),sum(diff_df.Fatalities<0))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "pd.set_option('mode.chained_assignment', None)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "scale_train = pd.DataFrame(columns = ['Id_x', 'Province', 'Country', 'Date', 'ConfirmedCases', 'Fatalities',\n",
                "       'Days_After_1stJan', 'Dayofweek', 'Month', 'Day', 'Population',\n",
                "       'Density', 'Land_Area', 'Migrants', 'MedAge', 'UrbanPopRate', 'Id_y',\n",
                "       'Lat', 'Long', 'temp', 'min', 'max', 'stp', 'slp', 'dewp', 'rh', 'ah',\n",
                "       'wdsp', 'prcp', 'fog', 'API_beds'])\n",
                "for country in train.Country.unique():\n",
                "    for province in train.query(f\"Country=='{country}'\").Province.unique():\n",
                "        province_df = train.query(f\"Country=='{country}' and Province=='{province}'\")\n",
                "        province_confirm = province_df.ConfirmedCases\n",
                "        province_fatalities = province_df.Fatalities\n",
                "        province_confirm = np.array(province_confirm).reshape(-1,1)\n",
                "        province_fatalities = np.array(province_confirm).reshape(-1,1)\n",
                "        scaler1= preprocessing.MinMaxScaler()\n",
                "        scaled_confirm = scaler1.fit_transform(province_confirm)\n",
                "        scaler2 = preprocessing.MinMaxScaler()\n",
                "        scaled_fata = scaler2.fit_transform(province_fatalities)\n",
                "        province_df['ConfirmedCases'] = scaled_confirm\n",
                "        province_df['Fatalities'] = scaled_fata\n",
                "        scale_train = pd.concat((scale_train,province_df),axis = 0,sort=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "trend_df = pd.DataFrame(columns={\"infection_trend\",\"fatality_trend\",\"quarantine_trend\",\"school_trend\",\"total_population\",\"expected_cases\",\"expected_fatalities\"})\n",
                "\n",
                "train_df = scale_train\n",
                "days_in_sequence = 14\n",
                "\n",
                "trend_list = []\n",
                "\n",
                "with tqdm(total=len(list(train_df.Country.unique()))) as pbar:\n",
                "    for country in train_df.Country.unique():\n",
                "        for province in train_df.query(f\"Country=='{country}'\").Province.unique():\n",
                "            province_df = train_df.query(f\"Country=='{country}' and Province=='{province}'\")\n",
                "            \n",
                "\n",
                "            for i in range(0,len(province_df)):\n",
                "                if i+days_in_sequence<=len(province_df):\n",
                "                    #prepare all the trend inputs\n",
                "                    infection_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].ConfirmedCases.values]\n",
                "                    fatality_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Fatalities.values]\n",
                "\n",
                "                    #preparing all the stable inputs\n",
                "                    days_after_1stJan = float(province_df.iloc[i].Days_After_1stJan)\n",
                "                    dayofweek = float(province_df.iloc[i].Dayofweek)\n",
                "                    month = float(province_df.iloc[i].Month)\n",
                "                    day= float(province_df.iloc[i].Day)\n",
                "                    population = float(province_df.iloc[i].Population)\n",
                "                    density = float(province_df.iloc[i].Density)\n",
                "                    land_area = float(province_df.iloc[i].Land_Area)\n",
                "                    migrants = float(province_df.iloc[i].Migrants)\n",
                "                    medage = float(province_df.iloc[i].MedAge)\n",
                "                    urbanpoprate = float(province_df.iloc[i].UrbanPopRate)\n",
                "                    beds = float(province_df.iloc[i].API_beds)\n",
                "\n",
                "                    #True cases in i+days_in_sequence-1 day\n",
                "                    expected_cases = float(province_df.iloc[i+days_in_sequence-1].ConfirmedCases)\n",
                "                    expected_fatalities = float(province_df.iloc[i+days_in_sequence-1].Fatalities)\n",
                "\n",
                "                    trend_list.append({\"infection_trend\":infection_trend,\n",
                "                                     \"fatality_trend\":fatality_trend,\n",
                "                                     \"stable_inputs\":[population,density,land_area,migrants,medage,urbanpoprate,beds],\n",
                "                                     \"expected_cases\":expected_cases,\n",
                "                                     \"expected_fatalities\":expected_fatalities})\n",
                "        pbar.update(1)\n",
                "trend_df = pd.DataFrame(trend_list)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "trend_df[\"temporal_inputs\"] = [np.asarray([trends[\"infection_trend\"],trends[\"fatality_trend\"]]) for idx,trends in trend_df.iterrows()]\n",
                "trend_df = shuffle(trend_df)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "trend_df.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Only keeping 25 sequences where the number of cases stays at 0, as there were way too many of these samples in our dataset.\n",
                "i=0\n",
                "temp_df = pd.DataFrame()\n",
                "for idx,row in trend_df.iterrows():\n",
                "    if sum(row.infection_trend)>0:\n",
                "        temp_df = temp_df.append(row)\n",
                "    else:\n",
                "        if i<25:\n",
                "            temp_df = temp_df.append(row)\n",
                "            i+=1\n",
                "trend_df = temp_df"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "sequence_length = 13\n",
                "training_percentage = 0.9\n",
                "# The purpose of '-2'and'+2' is to make the number of samples in the training test set divisible by batchsize\n",
                "training_item_count = int(len(trend_df)*training_percentage)\n",
                "validation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\n",
                "training_df = trend_df[:training_item_count-2]\n",
                "validation_df = trend_df[training_item_count+2:]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"temporal_inputs\"].values]),(training_item_count-2,2,sequence_length)),(0,2,1) )).astype(np.float32)\n",
                "X_stable_train = np.asarray([np.asarray(x) for x in training_df[\"stable_inputs\"]]).astype(np.float32)\n",
                "Y_cases_train = np.asarray([np.asarray(x) for x in training_df[\"expected_cases\"]]).astype(np.float32)\n",
                "Y_fatalities_train = np.asarray([np.asarray(x) for x in training_df[\"expected_fatalities\"]]).astype(np.float32)\n",
                "\n",
                "X_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in validation_df[\"temporal_inputs\"]]),(validation_item_count-2,2,sequence_length)),(0,2,1)) ).astype(np.float32)\n",
                "X_stable_test = np.asarray([np.asarray(x) for x in validation_df[\"stable_inputs\"]]).astype(np.float32)\n",
                "Y_cases_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_cases\"]]).astype(np.float32)\n",
                "Y_fatalities_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_fatalities\"]]).astype(np.float32)\n",
                "\n",
                "# Transform to tensor type\n",
                "X_temporal_train = torch.from_numpy(X_temporal_train)\n",
                "X_stable_train = torch.from_numpy(X_stable_train)\n",
                "Y_cases_train = torch.from_numpy(Y_cases_train)\n",
                "Y_fatalities_train = torch.from_numpy(Y_fatalities_train)\n",
                "\n",
                "X_temporal_test = torch.from_numpy(X_temporal_test)\n",
                "X_stable_test = torch.from_numpy(X_stable_test)\n",
                "Y_cases_test = torch.from_numpy(Y_cases_test)\n",
                "Y_fatalities_test = torch.from_numpy(Y_fatalities_test)\n",
                "\n",
                "# Merge two objective values\n",
                "Y_train = torch.cat((Y_cases_train.reshape(14770,1),Y_fatalities_train.reshape(14770,1)),1)\n",
                "Y_test = torch.cat((Y_cases_test.reshape(1640,1),Y_fatalities_test.reshape(1640,1)),1)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(len(X_temporal_train),len(X_temporal_test))"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "# Create train,test loader for training\n",
                "class MyDataset(Dataset):\n",
                "    def __init__(self, data1,data2, labels):\n",
                "        self.trend= data1\n",
                "        self.stable= data2\n",
                "        self.labels = labels  \n",
                "\n",
                "    def __getitem__(self, index):    \n",
                "        trend,stable, labels = self.trend[index], self.stable[index], self.labels[index]\n",
                "        return trend,stable,labels\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.trend) \n",
                "    \n",
                "train_ds = MyDataset(data1 = X_temporal_train,data2 = X_stable_train,labels = Y_train)\n",
                "test_ds =MyDataset(data1 = X_temporal_test,data2 = X_stable_test,labels = Y_test)\n",
                "train_loader = torch.utils.data.DataLoader(train_ds,batch_size = 10,shuffle=False)\n",
                "test_loader = torch.utils.data.DataLoader(test_ds,batch_size = 10,shuffle=False)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Define Model\n",
                "class Net(nn.Module):\n",
                "    def __init__(self):\n",
                "            super(Net, self).__init__()\n",
                "            self.lstm = nn.LSTM(2,32,1,dropout = 0.5)\n",
                "            \n",
                "            self.stable_full = nn.Linear(7,16)\n",
                "            nn.init.kaiming_normal_(self.stable_full.weight)\n",
                "            self.BN1 = nn.BatchNorm1d(16)\n",
                "            self.stable_dropout = nn.Dropout(0.5)\n",
                "            \n",
                "            self.merge_full = nn.Linear(16+13*32,64)# stable:5*16  lstm:13532\n",
                "            nn.init.kaiming_normal_(self.merge_full.weight)\n",
                "            self.BN2 = nn.BatchNorm1d(64)\n",
                "            self.merge_dropout = nn.Dropout(0.3)\n",
                "            self.merge_full2 = nn.Linear(64,2)\n",
                "            nn.init.kaiming_normal_(self.merge_full2.weight)\n",
                "\n",
                "    def reset_hidden(self):\n",
                "        self.hidden = (torch.zeros(self.hidden[0].shape), torch.zeros(self.hidden[1].shape))\n",
                "        \n",
                "    def forward(self, x_trend,x_stable):\n",
                "        batch_size = x_trend.reshape(13,-1,2).size(1)\n",
                "        x_trend = x_trend.reshape(13,batch_size,2)\n",
                "        x_trend, self.hidden = self.lstm(x_trend)\n",
                "        \n",
                "        x_stable = self.stable_dropout(F.relu(self.BN1(self.stable_full(x_stable))))\n",
                "        \n",
                "        s, b, h = x_trend.shape  #(seq, batch, hidden)\n",
                "        x_trend = x_trend.view(b, s*h)\n",
                "        x_merge = torch.cat((x_trend,x_stable),axis = 1)\n",
                "        x_merge = F.relu(self.merge_full2(self.merge_dropout(F.relu(self.BN2(self.merge_full(x_merge))))))\n",
                "        return x_merge"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# Training Settings\n",
                "model = Net()\n",
                "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
                "criterion = nn.MSELoss()"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "# Training process\n",
                "def train_model(epoch):\n",
                "    model.train()\n",
                "    for batch_idx, (trend, stable, target) in enumerate(train_loader):\n",
                "        trend, stable, target = Variable(trend), Variable(stable),Variable(target)\n",
                "        optimizer.zero_grad()\n",
                "        output = model(trend,stable)\n",
                "        loss = criterion(output, target)  \n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        if batch_idx % 300 == 0:\n",
                "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
                "                epoch, batch_idx * len(trend), len(train_loader.dataset),\n",
                "                100. * batch_idx / len(train_loader), loss.data))\n",
                "\n",
                "def test_model(epoch):\n",
                "    model.eval()\n",
                "    test_loss = 0\n",
                "    y_pred = []\n",
                "    y_true = []\n",
                "    for trend, stable, target in test_loader:\n",
                "        trend,stable, target = Variable(trend),Variable(stable),Variable(target)\n",
                "        output = model(trend,stable)\n",
                "        test_loss += criterion(output, target).data\n",
                "        y_pred.append(output)\n",
                "        y_true.append(target)\n",
                "\n",
                "    y_pred = torch.cat(y_pred, dim=0)\n",
                "    y_true = torch.cat(y_true, dim=0)\n",
                "    test_loss = test_loss\n",
                "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
                "    MSE = mean_squared_error(y_true.detach().numpy(), y_pred.detach().numpy())\n",
                "    print('\\nTest set: Average loss: {:.4f}, MSE: {} \\n'.format(\n",
                "        test_loss, MSE \n",
                "        ))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "for epoch in range(1, 21):\n",
                "    train_model(epoch)\n",
                "    test_model(epoch)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# In order to use query function,transform datetime to string\n",
                "def get_str_date(x):\n",
                "    x = str(x)[0:10]\n",
                "    return x\n",
                "\n",
                "scale_train.Date = scale_train.Date.apply(lambda x: get_str_date(x))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "del scale_train['Id']"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "# read test_df and a new train_df\n",
                "test_df = pd.read_csv('/kaggle/input/covid-with-weather-and-population/test_processed.csv')\n",
                "train_df2 =  pd.read_csv('/kaggle/input/covid19-with-population/update_train_processed.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "test_df = test_df.query(\"Date > '2020-04-25'\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# same preprocess as before\n",
                "train_df2.Country.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "train_df2.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "train_df2.replace(np.inf,0,inplace=True)\n",
                "train_df2.UrbanPopRate = train_df2.UrbanPopRate.apply(lambda x:get_percent(x))\n",
                "\n",
                "test_df.Country.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "test_df.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "test_df.replace(np.inf,0,inplace=True)\n",
                "test_df.UrbanPopRate = test_df.UrbanPopRate.apply(lambda x:get_percent(x))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# make train dataframe and test dataframe have same columns\n",
                "test_df['ConfirmedCases'] = np.NAN\n",
                "test_df['Fatalities'] = np.NAN\n",
                "test_df['Id_x'] = 0\n",
                "test_df['Id_y'] = 0\n",
                "test_df = test_df[list(scale_train.columns)]\n",
                "# merge scale_train and test\n",
                "total_df = pd.concat((scale_train,test_df),axis = 0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def get_conf_scaler(country,province):\n",
                "    train_df2_province = train_df2.query(f\"Country == '{country}' and Province =='{province}'\")\n",
                "    train_df2_province_conf = train_df2_province['ConfirmedCases']\n",
                "    train_df2_province_fata = train_df2_province['Fatalities']\n",
                "    province_conf_scaler = preprocessing.MinMaxScaler()\n",
                "    province_fata_scaler = preprocessing.MinMaxScaler()\n",
                "    province_conf_scaler.fit(np.array(train_df2_province_conf).reshape(-1,1))\n",
                "    province_fata_scaler.fit(np.array(train_df2_province_fata).reshape(-1,1))\n",
                "    return province_conf_scaler,province_fata_scaler"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "source": [
                "def get_pred_for_province(country,province):\n",
                "    start_date = datetime.strptime('2020-04-13','%Y-%m-%d')\n",
                "    end_date = datetime.strptime('2020-04-26','%Y-%m-%d')\n",
                "    pred = []\n",
                "    trend_input,stable_input = get_initial_input(country,province,str(start_date)[0:10],str(end_date)[0:10])\n",
                "    for i in range(0,19):\n",
                "        start = str(start_date+timedelta(days = i))[0:10]\n",
                "        end = str(end_date+timedelta(days = i))[0:10]\n",
                "        output,original_output = get_pred(country,province,trend_input,stable_input)\n",
                "        pred.append([end,original_output[0],original_output[1]])\n",
                "        trend_input = trend_input[1:]\n",
                "        output_tensor = torch.as_tensor(output)\n",
                "        new = torch.as_tensor(output_tensor.reshape(1,1,2))\n",
                "        trend_input = torch.cat((trend_input,new),0)\n",
                "    pred_for_province = pd.DataFrame(pred,columns=['Date','confirmed_pred','fata_pred'])\n",
                "    pred_for_province['Province'] = province\n",
                "    pred_for_province['Country'] = country\n",
                "    pred_for_province = pred_for_province[['Country','Province','Date','confirmed_pred','fata_pred']]\n",
                "    return pred_for_province"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "pred_table = pd.DataFrame(columns = ['Country','Province','Date','confirmed_pred','fata_pred'])\n",
                "for country in test_df.Country.unique():\n",
                "    for province in test_df.query(f\"Country == '{country}'\")['Province'].unique():\n",
                "        province_pred = get_pred_for_province(country,province)\n",
                "        pred_table = pd.concat((pred_table,province_pred),0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pred_table"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "original_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "original_train.Date = original_train.Date.apply(lambda x:get_dt(x))\n",
                "original_train.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)\n",
                "for i in range(len(original_train)):\n",
                "    if original_train.Province[i] is np.NaN:\n",
                "        original_train.Province[i] = original_train.Country[i]\n",
                "        \n",
                "for i in range(len(original_train)):\n",
                "    if original_train.Date[i]<datetime.strptime('2020-04-02','%Y-%m-%d') or original_train.Date[i]>datetime.strptime('2020-04-25','%Y-%m-%d'):\n",
                "        original_train.drop(i,inplace=True)\n",
                "        \n",
                "del original_train['Id']\n",
                "#del original_train['Country']\n",
                "\n",
                "original_train.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "#del pred_table['Country']\n",
                "original_train.Date = original_train.Date.apply(lambda x:get_str_date(x))"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "original_test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\n",
                "for i in range(len(original_test)):\n",
                "    if original_test.iloc[i]['Province_State'] is np.NaN:\n",
                "        original_test.iloc[i,1] = original_test.iloc[i,2]\n",
                "        \n",
                "original_test.rename(columns = {'Country_Region':'Country','Province_State':'Province'},inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final = pd.concat([pred_table,original_train],axis = 0,sort = True)\n",
                "final_submit = pd.merge(original_test,final,on = ['Country','Province','Date'],how = 'left')\n",
                "submission = final_submit[['ForecastId','ConfirmedCases','Fatalities']]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "submission"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submission.to_csv('/kaggle/working/submission.csv',index = False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from collections import Counter\n",
                "from datetime import datetime\n",
                "from google.cloud import bigquery\n",
                "import matplotlib.pyplot as plt\n",
                "import os"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n",
                "test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)\n",
                "test.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for i in range(len(train)):\n",
                "    if train.Province[i] is np.NaN:\n",
                "        train.Province[i] = train.Country[i]\n",
                "for i in range(len(test)):\n",
                "    if test.Province[i] is np.NaN:\n",
                "        test.Province[i] = test.Country[i]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train['Days_After_1stJan'] = train.Date.apply(lambda x :get_days(x))\n",
                "test['Days_After_1stJan'] = test.Date.apply(lambda x :get_days(x))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train['Date'] = train.Date.apply(lambda x:get_dt(x))\n",
                "test['Date'] = test.Date.apply(lambda x:get_dt(x))\n",
                "train['Dayofweek'] = train.Date.apply(lambda x:get_dayofweek(x))\n",
                "test['Dayofweek'] = test.Date.apply(lambda x:get_dayofweek(x))\n",
                "train['Month'] = train.Date.apply(lambda x:get_month(x))\n",
                "test['Month'] = test.Date.apply(lambda x:get_month(x))\n",
                "train['Day'] = train.Date.apply(lambda x:get_day(x))\n",
                "test['Day'] = test.Date.apply(lambda x:get_day(x))"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "country_info = pd.read_csv('/kaggle/input/population/population_by_country_2020.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "population = pd.DataFrame(country_info.iloc[:,[0,1,4,5,6,8,9]])\n",
                "population.columns = ['Country','Population','Density','Land_Area','Migrants','MedAge','UrbanPopRate']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for i in range(len(population)):\n",
                "    if np.isnan(population.Migrants[i]):\n",
                "        population.Migrants[i] = np.nanmedian(population.Migrants)\n",
                "    if population.MedAge[i] == 'N.A.':\n",
                "        population.MedAge[i] = 19\n",
                "    if population.UrbanPopRate[i] == 'N.A.':\n",
                "        population.UrbanPopRate[i] = '57%'"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "kosovo = pd.DataFrame([['Kosovo'],[2000700],[168],[10887],[0],[19],['57%']])\n",
                "kosovo = kosovo.T\n",
                "kosovo.columns = population.columns\n",
                "\n",
                "\n",
                "population = population.append(kosovo)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "westbank = pd.DataFrame([['West Bank and Gaza'],[2697687],[485],[5559],[0],[19],['57%']])\n",
                "westbank = westbank.T\n",
                "westbank.columns = population.columns\n",
                "\n",
                "population = population.append(westbank)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "DP = pd.DataFrame(['Diamond Princess',2666,191522,0.01392,2666,19,'100%'])\n",
                "DP = DP.T\n",
                "DP.columns = population.columns\n",
                "\n",
                "population = population.append(DP)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "Shangdan = pd.DataFrame(['MS Zaandam',1432,189618,0.007552,1432,19,'100%'])\n",
                "Shangdan = Shangdan.T\n",
                "Shangdan.columns = population.columns\n",
                "\n",
                "population = population.append(Shangdan)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "Congo = population[population.Country == 'Congo']\n",
                "Congo['Country'] = 'Congo (Brazzaville)'\n",
                "new1 = Congo.copy()\n",
                "Congo['Country'] = 'Congo (Kinshasa)'\n",
                "new2 = Congo.copy()\n",
                "\n",
                "population = population.append(new1)\n",
                "population = population.append(new2)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "population.Country[population.Country == 'United States'] = 'US'\n",
                "population.Country[population.Country == 'Taiwan'] = 'Taiwan*'\n",
                "population.Country[population.Country == 'South Korea'] = 'Korea, South'\n",
                "population.Country[population.Country == 'Cte d\\'Ivoire'] = 'Cote d\\'Ivoire'\n",
                "population.Country[population.Country == 'Czech Republic (Czechia)'] = 'Czechia'\n",
                "population.Country[population.Country == 'Myanmar'] = 'Burma'\n",
                "population.Country[population.Country == 'St. Vincent & Grenadines'] = 'Saint Vincent and the Grenadines'\n",
                "population.Country[population.Country == 'Saint Kitts & Nevis']  = 'Saint Kitts and Nevis'\n",
                "population.Country[population.Country == 'Sao Tome & Principe']  = 'Sao Tome and Principe'"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train = pd.merge(train,population,left_on = 'Country',right_on='Country',how='left')\n",
                "test = pd.merge(test,population,left_on = 'Country',right_on='Country',how = 'left')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "temperature = pd.read_csv('/kaggle/input/weather-data-for-covid19-data-analysis/training_data_with_weather_info_week_4.csv')\n",
                "temperature.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# fill NAN of Province with Country name\n",
                "for i in range(len(temperature)):\n",
                "    if temperature.Province[i] is np.NaN:\n",
                "        temperature.Province[i] = temperature.Country[i]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#delete useless features \n",
                "del temperature['ConfirmedCases']\n",
                "del temperature['Fatalities']\n",
                "del temperature['country+province']\n",
                "del temperature['day_from_jan_first']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#transform date to datetime type\n",
                "temperature.Date = temperature.Date.apply(lambda x:get_dt(x))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "day1 = datetime.strptime('2020-04-09','%Y-%m-%d')\n",
                "day2 = datetime.strptime('2020-04-10','%Y-%m-%d')\n",
                "day3 = datetime.strptime('2020-04-11','%Y-%m-%d')\n",
                "day4 = datetime.strptime('2020-04-12','%Y-%m-%d')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "index_delete = []\n",
                "for i in range(len(train)):\n",
                "    if (train.Date[i] == day1) or (train.Date[i] == day2) or (train.Date[i] == day3) or (train.Date[i] == day4):\n",
                "        index_delete.append(i)\n",
                "        \n",
                "train = (train.drop(index = index_delete)).reset_index(drop = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train = pd.merge(train,temperature,on=['Country','Province','Date'],how='left')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#fill NAN\n",
                "train['Lat'][train['Lat'].isnull()] = np.nanmedian(train['Lat'])\n",
                "train['Long'][train['Long'].isnull()] = np.nanmedian(train['Long'])\n",
                "train['temp'][train['temp'].isnull()] = np.nanmedian(train['temp'])\n",
                "train['min'][train['min'].isnull()] = np.nanmedian(train['min'])\n",
                "train['max'][train['max'].isnull()] = np.nanmedian(train['max'])\n",
                "train['slp'][train['slp'].isnull()] = np.nanmedian(train['slp'])\n",
                "train['dewp'][train['dewp'].isnull()] = np.nanmedian(train['dewp'])\n",
                "train['rh'][train['rh'].isnull()] = np.nanmedian(train['rh'])\n",
                "train['ah'][train['ah'].isnull()] = np.nanmedian(train['ah'])\n",
                "train['stp'][train['stp'].isnull()] = np.nanmedian(train['stp'])\n",
                "train['wdsp'][train['wdsp'].isnull()] = np.nanmedian(train['wdsp'])\n",
                "train['prcp'][train['prcp'].isnull()] = np.nanmedian(train['prcp'])\n",
                "train['fog'][train['fog'].isnull()] = np.nanmedian(train['fog'])"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "!pip install pmdarima"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pmdarima"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#the outlier 'inf' will make the auto_arima come to an error,so it's replaced by 0\n",
                "train.replace(np.inf,0,inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "source": [
                "#Using arima to predict the weather information for future\n",
                "date_pred_df = pd.DataFrame(sorted(list(set(test.Date))),columns=['Date'])\n",
                "date_pred_df = date_pred_df[7:]\n",
                "date_pred_df.reset_index(inplace = True)\n",
                "del date_pred_df['index']\n",
                "\n",
                "nperiods = (datetime.strptime('2020-05-14','%Y-%m-%d')-datetime.strptime('2020-04-08','%Y-%m-%d')).days\n",
                "\n",
                "weather_feature = ['temp', 'min', 'max', 'stp', 'slp', 'dewp', 'rh', 'ah','wdsp', 'prcp', 'fog']\n",
                "weather_pred = pd.DataFrame(columns=['Date','temp', 'min', 'max', 'stp', 'slp', 'dewp', 'rh', 'ah','wdsp', 'prcp', 'fog'])\n",
                "for prov in list(set(train.Province)):\n",
                "    df = train[train.Province == prov]\n",
                "    province_pred = date_pred_df.copy()\n",
                "    for feature in weather_feature:\n",
                "        ts = df[feature]\n",
                "        model = pmdarima.auto_arima(ts)\n",
                "        pred = model.predict(n_periods = nperiods)\n",
                "        province_pred[feature] = pred\n",
                "    province_pred['Province'] = prov\n",
                "    weather_pred = pd.concat([weather_pred,province_pred],axis = 0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for i in range(len(test)):\n",
                "    if test.Date[i]<datetime.strptime('2020-04-09','%Y-%m-%d'):\n",
                "        test.drop(i,inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#get longitude and latitude dataframe and merge it into test df\n",
                "df_longlat = pd.DataFrame(columns = ['Province','Lat','Long'])\n",
                "for i in range(len(train)):\n",
                "    if train.Province[i] not in list(df_longlat.Province):\n",
                "        df_longlat = df_longlat.append(train.iloc[i][['Province','Lat','Long']])\n",
                "        \n",
                "test = pd.merge(test,df_longlat,on = 'Province',how = 'left')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#adding weather feature to test data\n",
                "test = pd.merge(test,weather_pred,on = ['Province','Date'],how = 'left')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "API_beds = pd.read_csv('/kaggle/input/newest-bed-api-for-each-country/Newest_avg_bed_API.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#merge\n",
                "train = pd.merge(train,API_beds,left_on='Country',right_on='Country',how='left')\n",
                "test = pd.merge(test,API_beds,left_on='Country',right_on='Country',how='left')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#fill NAN\n",
                "train.API_beds[train.API_beds.isnull()] = np.nanmedian(train.API_beds)\n",
                "test.API_beds[test.API_beds.isnull()] = np.nanmedian(test.API_beds)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X = train.copy()\n",
                "X_test = test.copy()\n",
                "\n",
                "Province_set = set(X.Province)\n",
                "Country_set = set(X.Country)\n",
                "\n",
                "X = pd.concat([X,pd.get_dummies(X.Country)],axis=1)\n",
                "X_test = pd.concat([X_test,pd.get_dummies(X_test.Country)],axis=1)\n",
                "X = pd.concat([X,pd.get_dummies(X.Province)[Province_set - Country_set]],axis=1)\n",
                "X_test = pd.concat([X_test,pd.get_dummies(X_test.Province)[Province_set - Country_set]],axis=1)\n",
                "\n",
                "y_confirm = X.ConfirmedCases\n",
                "y_fata = X.Fatalities\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "del X['ConfirmedCases']\n",
                "del X['Fatalities']\n",
                "del X['Id_x']\n",
                "del X['Date']\n",
                "del X['Id_y']\n",
                "del X['Province']\n",
                "del X['Country']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ForecastId = X_test.ForecastId"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X['UrbanPopRate'] = X.UrbanPopRate.apply(lambda x:get_percent(x))\n",
                "X_test['UrbanPopRate'] = X_test.UrbanPopRate.apply(lambda x:get_percent(x))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "del X_test['ForecastId']\n",
                "del X_test['Date']\n",
                "del X_test['Province']\n",
                "del X_test['Country']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X = pd.DataFrame(X,dtype=float)\n",
                "X_test = pd.DataFrame(X_test,dtype=float)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X_test.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import xgboost as xgb"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "reg_confirm = xgb.XGBRegressor()\n",
                "reg_confirm.fit(X,y_confirm)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "pred_confirm = reg_confirm.predict(X_test)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "reg_fata = xgb.XGBRegressor()\n",
                "reg_fata.fit(X,y_fata)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "pred_fata = reg_fata.predict(X_test)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "submit = pd.DataFrame(ForecastId)\n",
                "submit['ConfirmedCases']=pred_confirm\n",
                "submit['Fatalities']=pred_fata"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "original_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n",
                "new_test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "original_train.Date = original_train.Date.apply(lambda x:get_dt(x))\n",
                "original_train.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)\n",
                "for i in range(len(original_train)):\n",
                "    if original_train.Province[i] is np.NaN:\n",
                "        original_train.Province[i] = original_train.Country[i]\n",
                "        \n",
                "for i in range(len(original_train)):\n",
                "    if original_train.Date[i]<datetime.strptime('2020-04-02','%Y-%m-%d') or original_train.Date[i]>datetime.strptime('2020-04-08','%Y-%m-%d'):\n",
                "        original_train.drop(i,inplace=True)\n",
                "        \n",
                "del original_train['Id']\n",
                "del original_train['Country']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final = pd.concat([temp_submit_df,original_train],axis = 0)\n",
                "final = final.sort_values(by=['Province','Date'])\n",
                "final_submit = pd.DataFrame(new_test.ForecastId,columns=['ForecastId'])\n",
                "final_submit['Confirmed'] = final.ConfirmedCases.values\n",
                "final_submit['Fatalities'] = final.Fatalities.values"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final_submit"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "final_submit.to_csv('/kaggle/working/submission.csv',index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "%matplotlib inline\n",
                "import numpy as np \n",
                "import pandas as pd \n",
                "from sklearn.model_selection import KFold\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from lightgbm import LGBMClassifier, LGBMRegressor\n",
                "from sklearn.model_selection import cross_validate\n",
                "from tqdm import tqdm_notebook\n",
                "import multiprocessing\n",
                "import matplotlib.pyplot as plt # drawing graph\n",
                "import warnings; warnings.filterwarnings(\"ignore\") \n",
                "import os; os.environ['OMP_NUM_THREADS'] = '4' # speed up using 4 cpu"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def infer_model(df, features, target, n_jobs):\n",
                "    model_class = LGBMRegressor\n",
                "    if len(df[target].value_counts()) == 2:\n",
                "        df[target] = LabelEncoder().fit_transform(df[target])\n",
                "        model_class = LGBMClassifier\n",
                "\n",
                "    categoricals = []\n",
                "    for f in features:\n",
                "        if df[f].dtype == object:\n",
                "            df[f] = LabelEncoder().fit_transform(df[f].apply(str))\n",
                "            categoricals.append(f)\n",
                "\n",
                "    min_child_samples = int(0.01*df.shape[0])\n",
                "\n",
                "    model = model_class(min_child_samples=min_child_samples, n_jobs=n_jobs)\n",
                "\n",
                "    return model, df, categoricals"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data"
            ],
            "source": [
                "# source : https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/\n",
                "df = pd.read_excel('../input/WA_Fn-UseC_-HR-Employee-Attrition.xlsx', sheet_name=0,dtype=dtypes)\n",
                "print(\"Shape of dataframe is: {}\".format(df.shape))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Cheese view of target data\n",
                "labels = 'Leave', 'Stay'\n",
                "colors = ['#ff9999','#99ff99']\n",
                "sizes = [df['Attrition'].value_counts()['Yes'],df['Attrition'].value_counts()['No']]\n",
                "explode = (0.25, 0)\n",
                "fig1, ax1 = plt.subplots()\n",
                "ax1.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.2f%%',\n",
                "        shadow=True, startangle=90)\n",
                "ax1.axis('equal')  \n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "sample_df = df.sample(frac=1, random_state=0)\n",
                "sample_df.sort_values(\"EmployeeNumber\", inplace=True)\n",
                "\n",
                "cv = KFold(n_splits=4, shuffle=False, random_state=0)\n",
                "target = \"Attrition\"\n",
                "features = [col for col in df.columns if col != target]\n",
                "\n",
                "lofo = LOFOImportance(sample_df, features, target, cv=cv, scoring=\"roc_auc\")\n",
                "importance_df= lofo.get_importance()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "plot_importance(importance_df, figsize=(12, 20))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from tpot import TPOTClassifier\n",
                "from sklearn.model_selection import StratifiedKFold,train_test_split\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "import os\n",
                "os.environ['OMP_NUM_THREADS'] = '4'\n",
                "print(os.listdir(\"../input\"))\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "# source : https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/\n",
                "df = pd.read_excel('../input/WA_Fn-UseC_-HR-Employee-Attrition.xlsx', sheet_name=0,dtype=dtypes)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cont=[]\n",
                "cat=[]\n",
                "for key, value in dtypes.items():\n",
                "    if key!='Attrition':\n",
                "        if value == \"int64\":\n",
                "            cont.append(key)\n",
                "        else:\n",
                "            cat.append(key)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head(5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df = pd.get_dummies(df, columns=cat)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df['Attrition']=df.Attrition.eq('Yes').mul(1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(df[cont], df['Attrition'], test_size=0.2, random_state=42)\n",
                "train = pd.concat([X_train, y_train], 1)\n",
                "test = pd.concat([X_test, y_test], 1)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "pipeline_optimizer = TPOTClassifier()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5, n_jobs=-1,random_state=42, verbosity=2, early_stop=5)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "pipeline_optimizer.fit(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "print(pipeline_optimizer.score(X_test, y_test))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.utils import shuffle\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "import warnings; \n",
                "warnings.filterwarnings(\"ignore\") \n",
                "import os \n",
                "os.environ['OMP_NUM_THREADS'] = '8' # speed up using 8 cpu"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "tourney_result = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WDataFiles_Stage1/WNCAATourneyCompactResults.csv')\n",
                "tourney_seed = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WDataFiles_Stage1/WNCAATourneySeeds.csv')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# deleting unnecessary columns\n",
                "tourney_result = tourney_result.drop(['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT'], axis=1)\n",
                "tourney_result"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "tourney_result.rename(columns={'Seed':'WSeed'}, inplace=True)\n",
                "tourney_result = tourney_result.drop('TeamID', axis=1)\n",
                "tourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "tourney_result.rename(columns={'Seed':'LSeed'}, inplace=True)\n",
                "tourney_result = tourney_result.drop('TeamID', axis=1)\n",
                "tourney_result"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "def get_seed(x):\n",
                "    return int(x[1:3])\n",
                "\n",
                "tourney_result['WSeed'] = tourney_result['WSeed'].map(lambda x: get_seed(x))\n",
                "tourney_result['LSeed'] = tourney_result['LSeed'].map(lambda x: get_seed(x))\n",
                "tourney_result"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "season_result = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WDataFiles_Stage1/WRegularSeasonCompactResults.csv')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "season_win_result = season_result[['Season', 'WTeamID', 'WScore']]\n",
                "season_lose_result = season_result[['Season', 'LTeamID', 'LScore']]\n",
                "season_win_result.rename(columns={'WTeamID':'TeamID', 'WScore':'Score'}, inplace=True)\n",
                "season_lose_result.rename(columns={'LTeamID':'TeamID', 'LScore':'Score'}, inplace=True)\n",
                "season_result = pd.concat((season_win_result, season_lose_result)).reset_index(drop=True)\n",
                "season_result"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "season_score = season_result.groupby(['Season', 'TeamID'])['Score'].sum().reset_index()\n",
                "season_score"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tourney_result = pd.merge(tourney_result, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "tourney_result.rename(columns={'Score':'WScoreT'}, inplace=True)\n",
                "tourney_result = tourney_result.drop('TeamID', axis=1)\n",
                "tourney_result = pd.merge(tourney_result, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "tourney_result.rename(columns={'Score':'LScoreT'}, inplace=True)\n",
                "tourney_result = tourney_result.drop('TeamID', axis=1)\n",
                "tourney_result"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tourney_win_result = tourney_result.drop(['Season', 'WTeamID', 'LTeamID'], axis=1)\n",
                "tourney_win_result.rename(columns={'WSeed':'Seed1', 'LSeed':'Seed2', 'WScoreT':'ScoreT1', 'LScoreT':'ScoreT2'}, inplace=True)\n",
                "tourney_win_result"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "tourney_lose_result = tourney_win_result.copy()\n",
                "tourney_lose_result['Seed1'] = tourney_win_result['Seed2']\n",
                "tourney_lose_result['Seed2'] = tourney_win_result['Seed1']\n",
                "tourney_lose_result['ScoreT1'] = tourney_win_result['ScoreT2']\n",
                "tourney_lose_result['ScoreT2'] = tourney_win_result['ScoreT1']\n",
                "tourney_lose_result"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "tourney_win_result['Seed_diff'] = tourney_win_result['Seed1'] - tourney_win_result['Seed2']\n",
                "tourney_win_result['ScoreT_diff'] = tourney_win_result['ScoreT1'] - tourney_win_result['ScoreT2']\n",
                "tourney_lose_result['Seed_diff'] = tourney_lose_result['Seed1'] - tourney_lose_result['Seed2']\n",
                "tourney_lose_result['ScoreT_diff'] = tourney_lose_result['ScoreT1'] - tourney_lose_result['ScoreT2']"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tourney_win_result['result'] = 1\n",
                "tourney_lose_result['result'] = 0\n",
                "tourney_result = pd.concat((tourney_win_result, tourney_lose_result)).reset_index(drop=True)\n",
                "tourney_result"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train = tourney_result.drop('result', axis=1)\n",
                "y_train = tourney_result.result\n",
                "X_train, y_train = shuffle(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "clf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
                "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
                "                      max_samples=None, min_impurity_decrease=0.0,\n",
                "                      min_impurity_split=None, min_samples_leaf=50,\n",
                "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
                "                      n_estimators=500, n_jobs=-1, oob_score=False,\n",
                "                      random_state=50, verbose=1, warm_start=False)\n",
                "\n",
                "clf.fit(X_train, y_train)\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "test_df = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WSampleSubmissionStage1_2020.csv')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "test_df['Season'] = test_df['ID'].map(lambda x: int(x[:4]))\n",
                "test_df['WTeamID'] = test_df['ID'].map(lambda x: int(x[5:9]))\n",
                "test_df['LTeamID'] = test_df['ID'].map(lambda x: int(x[10:14]))\n",
                "test_df"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "test_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "test_df.rename(columns={'Seed':'Seed1'}, inplace=True)\n",
                "test_df = test_df.drop('TeamID', axis=1)\n",
                "test_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "test_df.rename(columns={'Seed':'Seed2'}, inplace=True)\n",
                "test_df = test_df.drop('TeamID', axis=1)\n",
                "test_df = pd.merge(test_df, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "test_df.rename(columns={'Score':'ScoreT1'}, inplace=True)\n",
                "test_df = test_df.drop('TeamID', axis=1)\n",
                "test_df = pd.merge(test_df, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "test_df.rename(columns={'Score':'ScoreT2'}, inplace=True)\n",
                "test_df = test_df.drop('TeamID', axis=1)\n",
                "test_df"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "test_df['Seed1'] = test_df['Seed1'].map(lambda x: get_seed(x))\n",
                "test_df['Seed2'] = test_df['Seed2'].map(lambda x: get_seed(x))\n",
                "test_df['Seed_diff'] = test_df['Seed1'] - test_df['Seed2']\n",
                "test_df['ScoreT_diff'] = test_df['ScoreT1'] - test_df['ScoreT2']\n",
                "test_df = test_df.drop(['ID', 'Pred', 'Season', 'WTeamID', 'LTeamID'], axis=1)\n",
                "test_df"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "test_preds = clf.predict(test_df)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results",
                "process_data"
            ],
            "source": [
                "submission_df = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WSampleSubmissionStage1_2020.csv')\n",
                "submission_df['Pred'] = test_preds\n",
                "submission_df"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "submission_df['Pred'].hist()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submission_df.to_csv('submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "%matplotlib inline\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt # drawing graph\n",
                "import warnings; warnings.filterwarnings(\"ignore\") \n",
                "import os; os.environ['OMP_NUM_THREADS'] = '4' # speed up using 4 cpu\n",
                "from fastai.tabular import *\n",
                "from sklearn.metrics import roc_auc_score\n",
                "from imblearn.over_sampling import SMOTE"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data"
            ],
            "source": [
                "# source : https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/\n",
                "df = pd.read_excel('../input/WA_Fn-UseC_-HR-Employee-Attrition.xlsx', sheet_name=0,dtype=dtypes)\n",
                "print(\"Shape of dataframe is: {}\".format(df.shape))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# preprocessing : categorical encoding\n",
                "df['Attrition']=df.Attrition.eq('Yes').mul(1) # change target from Yes/no to 1/0\n",
                "cont=[]\n",
                "cat=[]\n",
                "for key, value in dtypes.items():\n",
                "    if key!='Attrition':\n",
                "        if value == \"int64\":\n",
                "            cont.append(key)\n",
                "        else:\n",
                "            cat.append(key)\n",
                "df = pd.get_dummies(df, columns=cat)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# get train data\n",
                "col = df.columns\n",
                "cont=[]\n",
                "for i in range(0,len(col)):\n",
                "    if col[i]!='Attrition':\n",
                "        cont.append(col[i])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#save the column name\n",
                "x_col = cont\n",
                "y_col = 'Attrition'\n",
                "\n",
                "X = df.drop('Attrition', axis=1)\n",
                "Y = df.Attrition\n",
                "X_res, Y_res = SMOTE().fit_resample(X, Y)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "smote_df = pd.DataFrame(X_res, columns = x_col)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "smote_df = smote_df.assign(Attrition = Y_res)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "smote_df.Attrition.value_counts()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "smote_df.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "dep_var='Attrition'\n",
                "procs=[ Normalize]\n",
                "data = (TabularList.from_df(smote_df, cont_names=col , procs=procs,)\n",
                "                .split_subsets(train_size=0.8, valid_size=0.2, seed=34)\n",
                "                .label_from_df(cols=dep_var)\n",
                "                .databunch())"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn = tabular_learner(data, layers=[200,100],metrics=accuracy, callback_fns=AUROC)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit(3,lr=1e-3)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit_one_cycle(2,max_lr=1e-6)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "learn.recorder.plot_losses()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import warnings\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from fastai.tabular import *\n",
                "%matplotlib inline\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train = pd.read_csv('../input/hack-ml/Dataset/Train.csv')\n",
                "test = pd.read_csv('../input/hack-ml/Dataset/Test.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('Train set shape:', train.shape)\n",
                "print('Test set shape:', test.shape)\n",
                "print('NaN in Train:',train.isnull().values.any())\n",
                "print('NaN in Test:',test.isnull().values.any())\n",
                "print('Train set overview:')\n",
                "display(train.head())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "f, ax = plt.subplots(figsize=(6, 6))\n",
                "ax = sns.countplot(x=\"netgain\", data=train, label=\"Label count\")\n",
                "sns.despine(bottom=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "del train['id']\n",
                "del test['id']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "dep_var='netgain'\n",
                "cat = ['realtionship_status', 'industry', 'genre', 'targeted_sex', 'airtime', 'airlocation',\n",
                "       'expensive', 'money_back_guarantee']\n",
                "cont = ['average_runtime(minutes_per_week)','ratings']\n",
                "procs= [Categorify,Normalize]\n",
                "\n",
                "inception = TabularList.from_df(test,cat_names=cat, cont_names=cont , procs=procs)\n",
                "data = (TabularList.from_df(train, cat_names=cat, cont_names=cont , procs=procs)\n",
                "                .split_subsets(train_size=0.8, valid_size=0.2, seed=34)\n",
                "                .label_from_df(cols=dep_var)\n",
                "                .add_test(inception)\n",
                "                .databunch())"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn = tabular_learner(data, layers=[200,100],metrics=accuracy)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit(1,lr=1e-3)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit_one_cycle(1,max_lr=1e-8)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "learn.recorder.plot_losses()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "learn.predict(inception[0])[0]"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "%matplotlib inline\n",
                "import pandas as pd\n",
                "from fastai.tabular import *\n",
                "import fastai \n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# check version\n",
                "fastai.__version__"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Feture importance extraction from NN weights\n",
                "def feature_importance(learner): \n",
                "  # based on: https://medium.com/@mp.music93/neural-networks-feature-importance-with-fastai-5c393cf65815\n",
                "    data = learner.data.train_ds.x\n",
                "    cat_names = data.cat_names\n",
                "    cont_names = data.cont_names\n",
                "    loss0=np.array([learner.loss_func(learner.pred_batch(batch=(x,y.to(\"cpu\"))), y.to(\"cpu\")) for x,y in iter(learner.data.valid_dl)]).mean()\n",
                "    fi=dict()\n",
                "    types=[cat_names, cont_names]\n",
                "    for j, t in enumerate(types):\n",
                "      for i, c in enumerate(t):\n",
                "        loss=[]\n",
                "        for x,y in iter(learner.data.valid_dl):\n",
                "          col=x[j][:,i] \n",
                "          idx = torch.randperm(col.nelement())\n",
                "          x[j][:,i] = col.view(-1)[idx].view(col.size())\n",
                "          y=y.to('cpu')\n",
                "          loss.append(learner.loss_func(learner.pred_batch(batch=(x,y)), y))\n",
                "        fi[c]=np.array(loss).mean()-loss0\n",
                "    d = sorted(fi.items(), key=lambda kv: kv[1], reverse=True)\n",
                "    return pd.DataFrame({'cols': [l for l, v in d], 'imp': np.log1p([v for l, v in d])})"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data"
            ],
            "source": [
                "# load all datasets\n",
                "col = [\"age\",\"class of worker\",\"detailed industry recode\",\"detailed occupation recode\",\"education\",\n",
                "       \"wage per hour\",\"enroll in edu inst last wk\",\"marital status\",\"major industry code\",\n",
                "       \"major occupation code\",\"race\",\"hispanic origin\",\"sex\",\"member of a labor union\",\n",
                "       \"reason for unemployment\",\"full or part time employment stat\",\"capital gains\",\"capital losses\",\n",
                "       \"dividends from stocks\",\"tax filer stat\",\"region of previous residence\",\"state of previous residence\",\n",
                "       \"detailed household and family stat\",\"detailed household summary in household\",\"instance weight\",\n",
                "       \"migration code-change in msa\",\"migration code-change in reg\",\"migration code-move within reg\",\n",
                "       \"live in this house 1 year ago\",\"migration prev res in sunbelt\",\"num persons worked for employer\",\n",
                "       \"family members under 18\",\"country of birth father\",\"country of birth mother\",\"country of birth self\",\n",
                "       \"citizenship\",\"own business or self employed\",\"fill inc questionnaire for veteran\\'s admin\",\n",
                "       \"veterans benefits\",\"weeks worked in year\",\"year\",\"income class\"]\n",
                "\n",
                "df = pd.read_csv(\"../input/ml-challenge-week6/census-income.data\", names=col, header=None)\n",
                "print(\"Shape of Train dataframe is: {}\".format(df.shape))\n",
                "print('NaN in Train:',df.isnull().values.any())\n",
                "test = pd.read_csv(\"../input/ml-challenge-week6/census-income.test\", names=col[0:-1], header=None)\n",
                "print(\"Shape of Test dataframe is: {}\".format(test.shape))\n",
                "print('NaN in Test:',test.isnull().values.any())\n",
                "sub = pd.read_csv(\"../input/ml-challenge-week6/sampleSubmission.csv\")\n",
                "zub = sub['index']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# align test target\n",
                "test['income class'] = ' - 50000.'\n",
                "# NaN corrections in test dataset\n",
                "test[\"hispanic origin\"] = test[\"hispanic origin\"].fillna('NA')\n",
                "test[\"state of previous residence\"] = test[\"state of previous residence\"].fillna('?')\n",
                "test['migration code-change in msa'] = test['migration code-change in msa'].fillna('?')\n",
                "test['migration code-change in reg'] = test['migration code-change in reg'].fillna('?')\n",
                "test['migration code-move within reg'] = test['migration code-move within reg'].fillna('?')\n",
                "test['migration prev res in sunbelt'] = test['migration prev res in sunbelt'].fillna('?')\n",
                "test['country of birth father'] = test['country of birth father'].fillna('?')\n",
                "test['country of birth mother'] = test['country of birth mother'].fillna('?')\n",
                "test['country of birth self'] = test['country of birth self'].fillna('?')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# merge train & test dataset\n",
                "df = df.append(test, ignore_index = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# transform target in category type\n",
                "df = df.join(pd.get_dummies(df['income class']))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# parameters for NN model\n",
                "dep_var =  ' 50000+.'\n",
                "procs = [FillMissing, Categorify, Normalize]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# check % positive values in train set\n",
                "df[dep_var].value_counts()[1]/199523"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# split by index\n",
                "idx_test = df.iloc[199523:].index # last N rows\n",
                "idx_val  = df.iloc[159619:199522].index # last 20% of train rows\n",
                "idx_val, idx_test"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# check % positive values in validation set\n",
                "df.loc[idx_val, dep_var].value_counts()[1]/(199522-159619)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# prepare databunch ingestion of test set\n",
                "test = TabularList.from_df(df.loc[idx_test].copy(), path='', cat_names=cat, cont_names=cont)"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "source": [
                "# build NN learner and look at learning rate curve\n",
                "learn = tabular_learner(data, layers=[200,100], metrics=[accuracy, AUROC()],callback_fns=ShowGraph)\n",
                "\n",
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# Initial learning phase using suggested learning rate during 3 cycles\n",
                "lr = 1e-03\n",
                "learn.fit_one_cycle(5, lr)"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "source": [
                "# look again at learning rate curve\n",
                "learn.unfreeze()\n",
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# 2nd learning phase using suggested learning rate during 3 cycles\n",
                "learn.fit_one_cycle(3,max_lr=1e-8)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "process_data"
            ],
            "source": [
                "# predict test classes...\n",
                "probas_test, _ = learn.get_preds(ds_type=DatasetType.Test) # run inference on test\n",
                "probas_test = probas_test[:, 1] # only get probability tensor"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# get tp driver\n",
                "tp = range(0,40000,100) \n",
                "y = []\n",
                "for x in tp:\n",
                "    y.append(0.6432042 + 0.00002635951*x - 9.49824e-10*x**2 + 9.629459e-15*x**3) # from my own analysis...\n",
                "    \n",
                "plt.plot(tp,y);"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('Best amount of true classes should be', np.argmax(y)*100,'with expected AUC arround',y[np.argmax(y)])"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# should have more positive classes, let's align...\n",
                "pivot = .2\n",
                "while len(submission_df[submission_df['income class']>=pivot])< 19100: #(np.argmax(y)*100):\n",
                "    pivot = pivot-.000001\n",
                "correction = .5 - pivot\n",
                "print('Pivot is',pivot,'- tensor correction is', correction)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# apply correction, classify 0/1 and make it int\n",
                "submission_df['income class'] = submission_df['income class'] + correction # tensor correction\n",
                "submission_df['income class'] = submission_df['income class'].apply(np.round)\n",
                "submission_df['income class'] = submission_df['income class'].astype(int)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# check results\n",
                "submission_df.describe()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "\n",
                "submission_df.to_csv('FastAI_v6_corrected.csv', index = False) #"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Here are our NN feature importance \n",
                "fi = feature_importance(learn)\n",
                "fi[:20].plot.barh(x=\"cols\", y=\"imp\", figsize=(10, 10))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt # drawing graph\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train_df = pd.read_csv('/kaggle/input/learn-together/train.csv')\n",
                "print(\"Size of Train dataframe is: {}\".format(train_df.shape))\n",
                "test_df =  pd.read_csv('/kaggle/input/learn-together/test.csv')\n",
                "print(\"Size of Test dataframe is: {}\".format(test_df.shape))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "categories = train_df[target[0]].unique()\n",
                "val = []\n",
                "for i in categories:\n",
                "    temp = len(train_df[train_df[target[0]]==i])\n",
                "    val.append(temp)\n",
                "labels=categories\n",
                "sizes=val\n",
                "colors=['green','red','orange','blue','darkorange','grey','pink']\n",
                "plt.axis('equal')\n",
                "plt.title('target classes distribution')\n",
                "plt.pie(sizes, explode=(0,0,0,0,0,0,0), labels=labels,colors=colors,autopct='%1.2f%%', shadow=True, startangle=90)\n",
                "plt.show()                        "
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "print('We have following categorical features:')\n",
                "print()\n",
                "cat = []\n",
                "cont = []\n",
                "for i in test[1:]:\n",
                "    temp1 = train_df[i].unique()\n",
                "    temp2 = test_df[i].unique()\n",
                "    if len(temp1) == len(temp2):\n",
                "        print(i,':',len(temp1),'unique values')\n",
                "        cat.append(i)\n",
                "    else:\n",
                "        cont.append(i)\n",
                "print()\n",
                "print('And we have',len(cont), 'of following continuous features:') \n",
                "print(cont)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "train_df[cont].hist(bins=20, figsize=(15,15), color = 'orange')\n",
                "plt.suptitle(\"Histogram for each train numeric input variable\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "test_df[cont].hist(bins=20, figsize=(15,15), color = 'darkorange')\n",
                "plt.suptitle(\"Histogram for each test numeric input variable\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "%matplotlib inline\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns # for making plots with seaborn\n",
                "color = sns.color_palette()\n",
                "from PIL import Image\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "import os\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train = pd.read_csv('../input/train.csv')\n",
                "test = pd.read_csv('../input/test.csv')\n",
                "train.head(5)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('size of train data',train.shape)\n",
                "print('size of test data',test.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(12,5))\n",
                "plt.title(\"Distribution of image categories(Target Variable)\")\n",
                "ax = sns.distplot(train[\"category_id\"])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(12,5))\n",
                "plt.title(\"Distribution of train image locations(Train location Variable)\")\n",
                "ax = sns.distplot(train[\"location\"])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(12,5))\n",
                "plt.title(\"Distribution of test image locations(Test location Variable)\")\n",
                "ax = sns.distplot(test[\"location\"])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.FacetGrid(train, hue=\"height\", size=10).map(plt.scatter, \"category_id\", \"location\").add_legend()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "train['date'] = train['date_captured'].str.split('\\s+').str[0]\n",
                "train['time'] = train['date_captured'].str.split('\\s+').str[-1]    \n",
                "train['hour'] = pd.to_numeric(train['time'].str[:2], errors='coerce')\n",
                "sns.FacetGrid(train, hue=\"category_id\", size=10).map(plt.scatter, \"hour\", \"location\").add_legend()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "night = train[(train['hour'] > 19) | (train['hour'] < 7)]\n",
                "day = len(train) - len(night)\n",
                "labels = 'Day', 'Night'\n",
                "sizes = [len(night), day]\n",
                "colors = ['lightcoral', 'lightskyblue']\n",
                "explode = (0.1, 0) \n",
                "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
                "autopct='%1.1f%%', shadow=True, startangle=140)\n",
                "plt.axis('equal')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "# sample night images\n",
                "fig = plt.figure(figsize=(25, 60))\n",
                "imgs = [np.random.choice(night.loc[night['category_id'] == i, 'file_name'], 4) for i in night.category_id.unique()]\n",
                "imgs = [i for j in imgs for i in j]\n",
                "labels = [[i] * 4 for i in train.category_id.unique()]\n",
                "labels = [i for j in labels for i in j]\n",
                "for idx, img in enumerate(imgs):\n",
                "    ax = fig.add_subplot(14, 4, idx + 1, xticks=[], yticks=[])\n",
                "    im = Image.open(\"../input/train_images/\" + img)\n",
                "    plt.imshow(im)\n",
                "    ax.set_title(f'Label: {labels[idx]}')"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "day = train[(train['hour'] < 19) & (train['hour'] > 7)]\n",
                "# sample night images\n",
                "fig = plt.figure(figsize=(25, 60))\n",
                "imgs = [np.random.choice(day.loc[day['category_id'] == i, 'file_name'], 4) for i in day.category_id.unique()]\n",
                "imgs = [i for j in imgs for i in j]\n",
                "labels = [[i] * 4 for i in train.category_id.unique()]\n",
                "labels = [i for j in labels for i in j]\n",
                "for idx, img in enumerate(imgs):\n",
                "    ax = fig.add_subplot(14, 4, idx + 1, xticks=[], yticks=[])\n",
                "    im = Image.open(\"../input/train_images/\" + img)\n",
                "    plt.imshow(im)\n",
                "    ax.set_title(f'Label: {labels[idx]}')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# We import required lib's\n",
                "from fastai.vision import *\n",
                "import os"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# nothing in our folder yet\n",
                "print(os.listdir(\"../working\"))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# move images to working folder\n",
                "!mkdir ../working/dog\n",
                "!mkdir ../working/wolf\n",
                "!cp ../input/dog-v1/* ../working/dog\n",
                "!cp ../input/wolf-v1/* ../working/wolf"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# let's check\n",
                "print(os.listdir(\"../working\"))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "source": [
                "verify_images('../working/wolf', delete=True, max_size=500)\n",
                "verify_images('../working/dog', delete=True, max_size=500)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "classes = ['wolf','dog']\n",
                "np.random.seed(42)\n",
                "data = ImageDataBunch.from_folder('../working', train=\".\", valid_pct=0.2,\n",
                "        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.classes"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data.show_batch(rows=3, figsize=(15,11))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.classes, data.c, len(data.train_ds), len(data.valid_ds)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn = create_cnn(data, models.resnet34, metrics=error_rate)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit_one_cycle(4)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "learn.save('stage-1')"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.unfreeze()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.lr_find()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "learn.recorder.plot()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit_one_cycle(2, max_lr=slice(3e-5,3e-4))"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "learn.save('stage-2')"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "learn.load('stage-2');"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "interp = ClassificationInterpretation.from_learner(learn)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "validate_data"
            ],
            "source": [
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "losses,idxs = interp.top_losses()\n",
                "len(data.valid_ds)==len(losses)==len(idxs)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "interp.plot_top_losses(9, figsize=(15,11))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data.show_batch(rows=3, figsize=(15,11))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.model_selection import train_test_split, StratifiedKFold,KFold\n",
                "from datetime import datetime\n",
                "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\n",
                "from sklearn import metrics\n",
                "from sklearn import preprocessing\n",
                "# Suppr warning\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "import itertools\n",
                "from scipy import interp\n",
                "# Plots\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "from matplotlib import rcParams\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\n",
                "test_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n",
                "train_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\n",
                "test_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n",
                "sample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "train_df = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
                "test_df = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
                "\n",
                "print(\"Train shape : \"+str(train_df.shape))\n",
                "print(\"Test shape  : \"+str(test_df.shape))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#drop sequence...\n",
                "train_df = train_df.reset_index()\n",
                "test_df = test_df.reset_index()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_df['nulls1'] = train_df.isna().sum(axis=1)\n",
                "test_df['nulls1'] = test_df.isna().sum(axis=1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_df = train_df.drop([\"TransactionDT\"], axis = 1)\n",
                "test_df = test_df.drop([\"TransactionDT\"], axis = 1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# KISS\n",
                "train_df = train_df.iloc[:, :53]\n",
                "test_df = test_df.iloc[:, :52]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "del train_transaction, train_identity, test_transaction, test_identity"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
                "us_emails = ['gmail', 'net', 'edu']\n",
                "#https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest_df-579654\n",
                "for c in ['P_emaildomain', 'R_emaildomain']:\n",
                "    train_df[c + '_bin'] = train_df[c].map(emails)\n",
                "    test_df[c + '_bin'] = test_df[c].map(emails)\n",
                "    \n",
                "    train_df[c + '_suffix'] = train_df[c].map(lambda x: str(x).split('.')[-1])\n",
                "    test_df[c + '_suffix'] = test_df[c].map(lambda x: str(x).split('.')[-1])\n",
                "    \n",
                "    train_df[c + '_suffix'] = train_df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
                "    test_df[c + '_suffix'] = test_df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for c1, c2 in train_df.dtypes.reset_index().values:\n",
                "    if c2=='O':\n",
                "        train_df[c1] = train_df[c1].map(lambda x: str(x).lower())\n",
                "        test_df[c1] = test_df[c1].map(lambda x: str(x).lower())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "numerical = [col for col in numerical if col in train_df.columns]\n",
                "categorical = [col for col in categorical if col in train_df.columns]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def nan2mean(df):\n",
                "    for x in list(df.columns.values):\n",
                "        if x in numerical:\n",
                "            #print(\"___________________\"+x)\n",
                "            #print(df[x].isna().sum())\n",
                "            df[x] = df[x].fillna(0)\n",
                "           #print(\"Mean-\"+str(df[x].mean()))\n",
                "    return df\n",
                "train_df=nan2mean(train_df)\n",
                "test_df=nan2mean(test_df)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Label Encoding\n",
                "category_counts = {}\n",
                "for f in categorical:\n",
                "    train_df[f] = train_df[f].replace(\"nan\", \"other\")\n",
                "    train_df[f] = train_df[f].replace(np.nan, \"other\")\n",
                "    test_df[f] = test_df[f].replace(\"nan\", \"other\")\n",
                "    test_df[f] = test_df[f].replace(np.nan, \"other\")\n",
                "    lbl = preprocessing.LabelEncoder()\n",
                "    lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
                "    train_df[f] = lbl.transform(list(train_df[f].values))\n",
                "    test_df[f] = lbl.transform(list(test_df[f].values))\n",
                "    category_counts[f] = len(list(lbl.classes_)) + 1\n",
                "# train_df = train_df.reset_index()\n",
                "# test_df = test_df.reset_index()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "for column in numerical:\n",
                "    scaler = StandardScaler()\n",
                "    if train_df[column].max() > 100 and train_df[column].min() >= 0:\n",
                "        train_df[column] = np.log1p(train_df[column])\n",
                "        test_df[column] = np.log1p(test_df[column])\n",
                "    scaler.fit(np.concatenate([train_df[column].values.reshape(-1,1), test_df[column].values.reshape(-1,1)]))\n",
                "    train_df[column] = scaler.transform(train_df[column].values.reshape(-1,1))\n",
                "    test_df[column] = scaler.transform(test_df[column].values.reshape(-1,1))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "source": [
                "from fastai.tabular import *\n",
                "from sklearn.metrics import roc_auc_score\n",
                "\n",
                "def auroc_score(input, target):\n",
                "    input, target = input.cpu().numpy()[:,1], target.cpu().numpy()\n",
                "    return roc_auc_score(target, input)\n",
                "\n",
                "class AUROC(Callback):\n",
                "    _order = -20 #Needs to run before the recorder\n",
                "\n",
                "    def __init__(self, learn, **kwargs): self.learn = learn\n",
                "    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['AUROC'])\n",
                "    def on_epoch_begin(self, **kwargs): self.output, self.target = [], []\n",
                "    \n",
                "    def on_batch_end(self, last_target, last_output, train, **kwargs):\n",
                "        if not train:\n",
                "            self.output.append(last_output)\n",
                "            self.target.append(last_target)\n",
                "                \n",
                "    def on_epoch_end(self, last_metrics, **kwargs):\n",
                "        if len(self.output) > 0:\n",
                "            output = torch.cat(self.output)\n",
                "            target = torch.cat(self.target)\n",
                "            preds = F.softmax(output, dim=1)\n",
                "            metric = auroc_score(preds, target)\n",
                "            return add_metrics(last_metrics, [metric])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "dep_var='isFraud' \n",
                "procs = [FillMissing, Categorify, Normalize]\n",
                "test_all = TabularList.from_df(test_df, cat_names=categorical,cont_names=numerical,procs=procs)\n",
                "data = (TabularList.from_df(train_df, cat_names=categorical, cont_names=numerical,procs=procs)\n",
                "                           .split_subsets(train_size=0.85, valid_size=0.15, seed=34)\n",
                "                           .label_from_df(cols=dep_var)\n",
                "                           .add_test(test_all)\n",
                "                           .databunch())       "
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn = tabular_learner(data, layers=[200,100],metrics=accuracy, callback_fns=AUROC)\n",
                "#learn = tabular_learner(data, layers=[1000,500,100],emb_drop=0.04,ps=(0.001, 0.01, 0.1),metrics=accuracy, callback_fns=AUROC,wd=1e-2)#.to_fp16()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit(10,lr=1e-2)\n",
                "#learn.fit(30,lr=3e-3)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.unfreeze()\n",
                "#learn.fit_one_cycle(10,max_lr=1e-6)\n",
                "learn.fit_one_cycle(10,max_lr=5e-5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "learn.recorder.plot_losses()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "#learn.freeze()\n",
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.unfreeze()\n",
                "#learn.fit_one_cycle(10,max_lr=1e-6)\n",
                "learn.fit_one_cycle(1,max_lr=1e-8)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "test_pred = learn.get_preds(DatasetType.Test)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "sample_submission.isFraud = test_pred[0][:,1].numpy()\n",
                "sample_submission.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "sample_submission.to_csv('simple_fastai_v3.csv')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "%matplotlib inline\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt \n",
                "#from collections import Counter\n",
                "import networkx as nx \n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "import os\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Create graph from data \n",
                "g = nx.Graph()\n",
                "color_map = []\n",
                "for i in range(0,len(fam)): #len(names)\n",
                "    g.add_node(fam[i], type = 'fam')\n",
                "    for j in ind[i]:\n",
                "        temp = fam[i]+j\n",
                "        g.add_node(temp, type = 'ind')\n",
                "        g.add_edge(fam[i], temp, color='green', weight=1)\n",
                "for n1, attr in g.nodes(data=True):\n",
                "    if attr['type'] == 'fam':\n",
                "        color_map.append('lime')\n",
                "    else: \n",
                "        if attr['type'] == 'ind':\n",
                "            color_map.append('cyan')\n",
                "        else:\n",
                "            color_map.append('red')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Plot the graph\n",
                "plt.figure(3,figsize=(90,90))  \n",
                "edges = g.edges()\n",
                "colors = [g[u][v]['color'] for u,v in edges]\n",
                "nx.draw(g,node_color = color_map, edge_color = colors, with_labels = True)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import os\n",
                "import random\n",
                "import warnings\n",
                "warnings.simplefilter(action='ignore')\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "%matplotlib inline\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC, LinearSVC\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.linear_model import SGDClassifier\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.model_selection import KFold\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import make_scorer, accuracy_score\n",
                "from sklearn.model_selection import RandomizedSearchCV\n",
                "import lightgbm\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.mixture import GaussianMixture\n",
                "from lightgbm import LGBMClassifier\n",
                "from mlxtend.classifier import StackingCVClassifier\n",
                "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "random_state = 1\n",
                "random.seed(random_state)\n",
                "np.random.seed(random_state)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "# Read the data\n",
                "X = pd.read_csv('../input/mh-forest/Forest_Cover_participants_Data/train.csv')\n",
                "X_test_full = pd.read_csv('../input/mh-forest/Forest_Cover_participants_Data/test.csv')\n",
                "\n",
                "col = X.columns\n",
                "newcol = []\n",
                "for i in range(0, len(col)):\n",
                "    temp = col[i]\n",
                "    if temp[-8:] == '(meters)':\n",
                "        #print(temp[:-8])\n",
                "        temp = temp[:-8]\n",
                "    if temp[-9:] == '(degrees)':\n",
                "        #print(temp[:-9])\n",
                "        temp = temp[:-9]\n",
                "    newcol.append(temp)\n",
                "X.columns = newcol\n",
                "\n",
                "col = X_test_full.columns\n",
                "newcol = []\n",
                "for i in range(0, len(col)):\n",
                "    temp = col[i]\n",
                "    if temp[-8:] == '(meters)':\n",
                "        #print(temp[:-8])\n",
                "        temp = temp[:-8]\n",
                "    if temp[-9:] == '(degrees)':\n",
                "        #print(temp[:-9])\n",
                "        temp = temp[:-9]\n",
                "    newcol.append(temp)\n",
                "X_test_full.columns = newcol\n",
                "\n",
                "\n",
                "y = X.Cover_Type\n",
                "X.drop(['Cover_Type'], axis=1, inplace=True)\n",
                "\n",
                "#X.drop(['Id'], axis=1, inplace=True)\n",
                "\n",
                "train_X = X\n",
                "train_y = y"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def WH4(df):\n",
                "    df['Hydro_high'] = df.Vertical_Distance_To_Hydrology.apply(lambda x: x > 3 )\n",
                "    df['Hydro_Euclidean'] = (df['Horizontal_Distance_To_Hydrology']**2 +\n",
                "                            df['Vertical_Distance_To_Hydrology']**2).apply(np.sqrt)\n",
                "    #df.drop(['Vertical_Distance_To_Hydrology'], axis=1, inplace=True)\n",
                "    #df.drop(['Horizontal_Distance_To_Hydrology'], axis=1, inplace=True)\n",
                "    df['Hydro_Fire_road'] = (df.Horizontal_Distance_To_Roadways + df.Horizontal_Distance_To_Fire_Points)/(df.Hydro_Euclidean/20000+1)\n",
                "    df['Hydro_Fire_sum'] = (df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points'])\n",
                "    #df.drop(['Soil_Type15'], axis=1, inplace=True)\n",
                "    #df.drop(['Soil_Type7'], axis=1, inplace=True)\n",
                "    df['Hydro_Elevation_diff'] = (df['Elevation'] - df['Vertical_Distance_To_Hydrology'])\n",
                "    \n",
                "    df['Soil_Type12_32'] = df['Soil_Type_32'] + df['Soil_Type_12']\n",
                "    df['Soil_Type23_22_32_33'] = df['Soil_Type_23'] + df['Soil_Type_22'] + df['Soil_Type_32'] + df['Soil_Type_33']\n",
                "      \n",
                "    df['Hydro_Fire_diff'] = (df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points']).abs()\n",
                "    df['Hydro_Road_sum'] = (df['Horizontal_Distance_To_Hydrology'] +df['Horizontal_Distance_To_Roadways'])\n",
                "    df['Hydro_Road_diff'] = (df['Horizontal_Distance_To_Hydrology'] -df['Horizontal_Distance_To_Roadways']).abs()\n",
                "    df['Road_Fire_sum'] = (df['Horizontal_Distance_To_Roadways'] + df['Horizontal_Distance_To_Fire_Points'])\n",
                "    df['Road_Fire_diff'] = (df['Horizontal_Distance_To_Roadways'] - df['Horizontal_Distance_To_Fire_Points']).abs()\n",
                "    #df.loc[:, :] = np.floor(MinMaxScaler((0, 100)).fit_transform(df))\n",
                "    #df = df.astype('int8')\n",
                "    #df.fillna(0)\n",
                "    \n"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model",
                "process_data"
            ],
            "source": [
                "WH4(X_test_full)\n",
                "WH4(X)\n",
                "\n",
                "\n",
                "gm = GaussianMixture(n_components  = 15)\n",
                "gm.fit(X)\n",
                "X['g_mixture'] = gm.predict(X)\n",
                "X_test_full['g_mixture'] = gm.predict(X_test_full)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X_test_full.shape"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "max_features = min(30, X.columns.size)\n",
                "\n",
                "ab_clf = AdaBoostClassifier(n_estimators=300,\n",
                "                            base_estimator=DecisionTreeClassifier(\n",
                "                                min_samples_leaf=2,\n",
                "                                random_state=random_state),\n",
                "                            random_state=random_state)\n",
                "\n",
                "et_clf = ExtraTreesClassifier(n_estimators=500,\n",
                "                              min_samples_leaf=2,\n",
                "                              min_samples_split=2,\n",
                "                              max_depth=50,\n",
                "                              max_features=max_features,\n",
                "                              random_state=random_state,\n",
                "                              n_jobs=-1)\n",
                "\n",
                "lg_clf = LGBMClassifier(n_estimators=300,\n",
                "                        num_leaves=128,\n",
                "                        verbose=-1,\n",
                "                        random_state=random_state,\n",
                "                        n_jobs=-1)\n",
                "\n",
                "rf_clf = RandomForestClassifier(n_estimators=300,\n",
                "                                random_state=random_state,\n",
                "                                n_jobs=-1)\n",
                "\n",
                "ensemble = [('AdaBoostClassifier', ab_clf),\n",
                "            ('ExtraTreesClassifier', et_clf),\n",
                "            ('LGBMClassifier', lg_clf),\n",
                "            ('RandomForestClassifier', rf_clf)]"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "print('> Cross-validating classifiers')\n",
                "for label, clf in ensemble:\n",
                "    score = cross_val_score(clf, X, train_y,\n",
                "                            cv=5,\n",
                "                            scoring='accuracy',\n",
                "                            verbose=0,\n",
                "                            n_jobs=-1)\n",
                "\n",
                "    print('  -- {: <24} : {:.3f} : {}'.format(label, np.mean(score), np.around(score, 3)))\n",
                "\n",
                "\n",
                "print('> Fitting stack')\n",
                "\n",
                "stack = StackingCVClassifier(classifiers=[ab_clf, et_clf, lg_clf, rf_clf],\n",
                "                             meta_classifier=rf_clf,\n",
                "                             cv=5,\n",
                "                             stratify=True,\n",
                "                             shuffle=True,\n",
                "                             use_probas=True,\n",
                "                             use_features_in_secondary=True,\n",
                "                             verbose=1,\n",
                "                             random_state=random_state,\n",
                "                             n_jobs=-1)\n",
                "\n",
                "stack = stack.fit(X, train_y)\n"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "predictions = stack.predict_proba(X_test_full)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "stack.score(X, train_y)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "sub = pd.read_csv(\"../input/mh-forest/Forest_Cover_participants_Data/sample_submission.csv\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from tqdm import tqdm\n",
                "\n",
                "for i in tqdm(range(0,len(sub))):\n",
                "    sub.iloc[i,0] = predictions[i][0]\n",
                "    sub.iloc[i,1] = predictions[i][1]\n",
                "    sub.iloc[i,2] = predictions[i][2]\n",
                "    sub.iloc[i,3] = predictions[i][3]\n",
                "    sub.iloc[i,4] = predictions[i][4]\n",
                "    sub.iloc[i,5] = predictions[i][5]\n",
                "    sub.iloc[i,6] = predictions[i][6]\n",
                "    "
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "sub.to_csv(\"Tree_version_8a.csv\", index=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "sub.describe()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "%matplotlib inline\n",
                "import pandas as pd\n",
                "import networkx as nx \n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "from IPython.display import Image\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\") \n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "os.environ['OMP_NUM_THREADS'] = '8' "
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "data = pd.read_csv('../input/santa-workshop-tour-2019/family_data.csv')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Drawing Graph\n",
                "plt.figure(3,figsize=(25,25))  \n",
                "edges = g.edges()\n",
                "colors = [g[u][v]['color'] for u,v in edges]\n",
                "nx.draw(g, node_color = color_map, edge_color = colors, with_labels = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# Extract reference graph facts & metrics \n",
                "print('Graph')\n",
                "print('Do we have a fully connected graph? ',nx.is_connected(g))\n",
                "h = g.to_directed()\n",
                "N, K = h.order(), h.size()\n",
                "avg_deg= float(K) / N\n",
                "print (\"# Nodes: \", N)\n",
                "print (\"# Edges: \", K)\n",
                "print (\"Average connectivity degree: \", avg_deg)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "dis = []\n",
                "in_degrees= h.in_degree() \n",
                "for i in in_degrees:\n",
                "    dis.append(i[1])\n",
                "fig = plt.figure(figsize=(20,20));\n",
                "plt.title('Degree Distribution per day');\n",
                "plt.grid(True);\n",
                "plt.xlabel('Days');\n",
                "plt.ylabel('# of Demand');\n",
                "plt.plot(dis[0:99],color='orange',alpha=0.90);"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from fastai.vision import *"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "folder = 'black'\n",
                "file = 'black_bear.txt'"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "folder = 'grizzly'\n",
                "file = 'grizzly_bear.txt'"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "folder = 'teddy'\n",
                "file = 'teddy_bear.txt'"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "# \n",
                "path = Path('/kaggle/working/data/bears')\n",
                "path.mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# kaggle\n",
                "!cp /kaggle/input/* {path}/\n",
                "# tree\n",
                "!apt get tree"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "!tree {path}"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "# \n",
                "dest = path/folder\n",
                "dest.mkdir(parents=True, exist_ok=True)\n",
                "# \n",
                "download_images(path/file, dest, max_pics=200)"
            ]
        },
        {
            "tags": [
                "check_results",
                "validate_data"
            ],
            "source": [
                "# \n",
                "classes = ['black', 'grizzly', 'teddy']\n",
                "# \n",
                "for c in classes:\n",
                "    print(c)\n",
                "    verify_images(path/c, delete=True, max_size=500)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# \n",
                "np.random.seed(42)\n",
                "# train.\n",
                "data = ImageDataBunch.from_folder(path, train=\".\", valid_pct=0.2,\n",
                "        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.classes, data.c, len(data.train_ds), len(data.valid_ds)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.show_batch(rows=3, figsize=(12,8))"
            ]
        },
        {
            "tags": [
                "train_model",
                "transfer_results"
            ],
            "source": [
                "learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n",
                "learn.fit_one_cycle(4)\n",
                "learn.save('stage-1')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_top_losses(9, figsize=(12,8))\n",
                "interp.plot_top_losses(9, figsize=(12,8), heatmap=False)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot()"
            ]
        },
        {
            "tags": [
                "train_model",
                "transfer_results"
            ],
            "source": [
                "learn.unfreeze()\n",
                "learn.fit_one_cycle(2, max_lr=slice(1e-6,3e-4))\n",
                "learn.save('stage-2')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from fastai.widgets import *"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "doc(DatasetFormatter().from_toplosses)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# 25clean\n",
                "idxs = idxs[:25]"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "!tree"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# cleaned.csv \n",
                "!cat {path}/cleaned.csv -n"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data",
                "train_model"
            ],
            "source": [
                "learn_cln = cnn_learner(db, models.resnet34, metrics=error_rate)\n",
                "learn_cln.load('stage-2');\n",
                "ds, idxs = DatasetFormatter().from_similars(learn_cln)\n",
                "len(idxs)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "idxs = idxs[:10]"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "np.random.seed(42)\n",
                "data = ImageDataBunch.from_csv(path, folder=\".\", valid_pct=0.2, csv_labels='cleaned.csv',\n",
                "        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.classes, data.c, len(data.train_ds), len(data.valid_ds)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model",
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n",
                "learn.load('stage-2')\n",
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_top_losses(9, figsize=(12,8))\n",
                "interp.plot_top_losses(9, figsize=(12,8),heatmap=False)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model",
                "visualize_data"
            ],
            "source": [
                "learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n",
                "learn.fit_one_cycle(4)\n",
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_top_losses(9, figsize=(12,8))\n",
                "interp.plot_top_losses(9, figsize=(12,8),heatmap=False)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot()"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "learn.unfreeze()\n",
                "learn.fit_one_cycle(2, max_lr=slice(1e-6,3e-4))\n",
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_top_losses(9, figsize=(12,8))\n",
                "interp.plot_top_losses(9, figsize=(12,8),heatmap=False)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "# library we'll need\n",
                "library(tidyverse)\n",
                "\n",
                "# read in all three datasets (you'll pick one to use later)\n",
                "recpies <- read_csv(\"../input/epirecipes/epi_r.csv\")\n",
                "bikes <- read_csv(\"../input/nyc-east-river-bicycle-crossings/nyc-east-river-bicycle-counts.csv\")\n",
                "weather <- read_csv(\"../input/szeged-weather\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "# We'll need these libraries\n",
                "import numpy as np\n",
                "import pandas as pd \n",
                "from pandas import read_csv\n",
                "\n",
                "# Plotting libraries\n",
                "import seaborn as sns\n",
                "from ggplot import *\n",
                "\n",
                "recipes = read_csv(\"../input/epirecipes/epi_r.csv\")\n",
                "bikes = read_csv(\"../input/nyc-east-river-bicycle-crossings/nyc-east-river-bicycle-counts.csv\")\n",
                "weather = read_csv(\"../input/szeged-weather/weatherHistory.csv\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# quickly clean our dataset\n",
                "recpies <- recpies %>%\n",
                "    filter(calories < 10000) %>% # remove outliers\n",
                "    na.omit() # remove rows with NA values"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "recipes = recipes[recipes['calories'] < 10000].dropna()"
            ]
        },
        {
            "tags": [
                "check_results",
                "validate_data"
            ],
            "source": [
                "# are the ratings all numeric?\n",
                "print(\"Is this variable numeric?\")\n",
                "is.numeric(recpies$rating)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# We'll use the numpy isreal() function\n",
                "# See https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.isreal.html\n",
                "print(\"Is this variable numeric?\")\n",
                "all(recipes['rating'].apply(np.isreal)) # Check that every row is True."
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "print(\"Is this variable only integers?\")\n",
                "\n",
                "all(recipes['rating'] == recipes['rating'].astype(int))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# plot calories by whether or not it's a dessert\n",
                "ggplot(recpies, aes(x = calories, y = dessert)) + # draw a \n",
                "    geom_point()  # add points"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# plot calories by whether or not it's a dessert\n",
                "ggplot(recipes, aes(x='calories', y='dessert')) + geom_point()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.set(style=\"darkgrid\")\n",
                "g = sns.regplot(x=\"calories\", y=\"dessert\", data=recipes, fit_reg=False)\n",
                "g.figure.set_size_inches(8, 8)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# plot & add a regression line\n",
                "ggplot(recpies, aes(x = calories, y = dessert)) + # draw a \n",
                "    geom_point() + # add points\n",
                "    geom_smooth(method = \"glm\", # plot a regression...\n",
                "    method.args = list(family = \"binomial\")) # ...from the binomial family"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ggplot(recipes, aes(x='calories', y='dessert')) + geom_point() + \\\n",
                "stat_smooth(method=\"lm\", color='blue')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.set(style=\"darkgrid\")\n",
                "g = sns.regplot(x=\"calories\", y=\"dessert\", data=recipes, logistic=True)\n",
                "g.figure.set_size_inches(8, 8)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# Extraction of digital\n",
                "bikes['Precipitation'] = bikes['Precipitation'].map(strtonum)\n",
                "# print(bikes['Precipitation'])\n",
                "print(bikes.columns)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ggplot(bikes,aes(x='Low Temp (F)', y='Total')) + geom_point() + \\\n",
                "stat_smooth(method=\"loess\", color='blue')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ggplot(bikes,aes(x='Low Temp (F)', y='Brooklyn Bridge')) + geom_point(color='red') + \\\n",
                "stat_smooth(method=\"loess\", color='violet', se=True)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import os\n",
                "import json\n",
                "\n",
                "import albumentations as albu\n",
                "import cv2\n",
                "import keras\n",
                "from keras import backend as K\n",
                "from keras.models import Model\n",
                "from keras.layers import Input\n",
                "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
                "from keras.layers.pooling import MaxPooling2D\n",
                "from keras.layers.merge import concatenate\n",
                "from keras.losses import binary_crossentropy\n",
                "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "from skimage.exposure import adjust_gamma\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from keras.layers import LeakyReLU\n",
                "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
                "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
                "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
                "from keras import optimizers\n",
                "from keras.legacy import interfaces\n",
                "from keras.utils.generic_utils import get_custom_objects\n",
                "\n",
                "from keras.engine.topology import Input\n",
                "from keras.engine.training import Model\n",
                "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
                "from keras.layers.core import Activation, SpatialDropout2D\n",
                "from keras.layers.merge import concatenate\n",
                "from keras.layers.normalization import BatchNormalization\n",
                "from keras.layers.pooling import MaxPooling2D\n",
                "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add\n",
                "from keras.regularizers import l2\n",
                "from keras.layers.core import Dense, Lambda\n",
                "from keras.layers.merge import concatenate, add\n",
                "from keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\n",
                "from keras.optimizers import SGD\n",
                "from keras.preprocessing.image import ImageDataGenerator"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "!pip install ../input/efficientnet-keras-source-code/repository/qubvel-efficientnet-c993591"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train_df = pd.read_csv('../input/understanding_cloud_organization/train.csv')\n",
                "train_df['ImageId'] = train_df['Image_Label'].apply(lambda x: x.split('_')[0])\n",
                "train_df['ClassId'] = train_df['Image_Label'].apply(lambda x: x.split('_')[1])\n",
                "train_df['hasMask'] = ~ train_df['EncodedPixels'].isna()\n",
                "\n",
                "print(train_df.shape)\n",
                "train_df.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "mask_count_df = train_df.groupby('ImageId').agg(np.sum).reset_index()\n",
                "mask_count_df.sort_values('hasMask', ascending=False, inplace=True)\n",
                "print(mask_count_df.shape)\n",
                "mask_count_df.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "sub_df = pd.read_csv('../input/understanding_cloud_organization/sample_submission.csv')\n",
                "sub_df['ImageId'] = sub_df['Image_Label'].apply(lambda x: x.split('_')[0])\n",
                "test_imgs = pd.DataFrame(sub_df['ImageId'].unique(), columns=['ImageId'])"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "import efficientnet.keras as efn \n",
                "def EfficientUNet(input_shape):\n",
                "    backbone = efn.EfficientNetB4(\n",
                "        weights=None,\n",
                "        include_top=False,\n",
                "        input_shape=input_shape\n",
                "    )\n",
                "    \n",
                "    input = backbone.input\n",
                "    x00 = backbone.input  # (256, 512, 3)\n",
                "    x10 = backbone.get_layer('stem_activation').output  # (128, 256, 4)\n",
                "    x20 = backbone.get_layer('block2d_add').output  # (64, 128, 32)\n",
                "    x30 = backbone.get_layer('block3d_add').output  # (32, 64, 56)\n",
                "    x40 = backbone.get_layer('block5f_add').output  # (16, 32, 160)\n",
                "    x50 = backbone.get_layer('block7b_add').output  # (8, 16, 448)\n",
                "    \n",
                "    x01 = H([x00, U(x10)], 'X01')\n",
                "    x11 = H([x10, U(x20)], 'X11')\n",
                "    x21 = H([x20, U(x30)], 'X21')\n",
                "    x31 = H([x30, U(x40)], 'X31')\n",
                "    x41 = H([x40, U(x50)], 'X41')\n",
                "    \n",
                "    x02 = H([x00, x01, U(x11)], 'X02')\n",
                "    x12 = H([x11, U(x21)], 'X12')\n",
                "    x22 = H([x21, U(x31)], 'X22')\n",
                "    x32 = H([x31, U(x41)], 'X32')\n",
                "    \n",
                "    x03 = H([x00, x01, x02, U(x12)], 'X03')\n",
                "    x13 = H([x12, U(x22)], 'X13')\n",
                "    x23 = H([x22, U(x32)], 'X23')\n",
                "    \n",
                "    x04 = H([x00, x01, x02, x03, U(x13)], 'X04')\n",
                "    x14 = H([x13, U(x23)], 'X14')\n",
                "    \n",
                "    x05 = H([x00, x01, x02, x03, x04, U(x14)], 'X05')\n",
                "    \n",
                "    x_out = Concatenate(name='bridge')([x01, x02, x03, x04, x05])\n",
                "    x_out = Conv2D(4, (3,3), padding=\"same\", name='final_output', activation=\"sigmoid\")(x_out)\n",
                "    \n",
                "    return Model(inputs=input, outputs=x_out)\n",
                "\n",
                "model = EfficientUNet((320, 480 ,3))\n",
                "\n",
                "model.summary()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "minsizes = [20000 ,20000, 22500, 10000]\n"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "sigmoid = lambda x: 1 / (1 + np.exp(-x))"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "check_results",
                "process_data"
            ],
            "source": [
                "sub_df = sub_df[['Image_Label', 'EncodedPixels']]\n",
                "sub_df.to_csv('submission.csv', index=False)\n",
                "display(sub_df.head(10))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "#Q1create two NumPy array by takin user input of data stored in array, check if they have views to same memory, check if elements of arrays are divisible by 3 or not sort 2nd array and find sum of all elements of 1st array\n",
                "import numpy as np\n",
                "inp1 = input(\"Enter first array:\")\n",
                "a = inp1.split()\n",
                "a = [int(i) for i in a]\n",
                "inp2 = input(\"Enter second array:\")\n",
                "b = inp2.split()\n",
                "b = [int(i) for i in b]\n",
                "Arr1 = np.array(a)\n",
                "Arr2 = np.array(b)\n",
                "print(\"Array 1 :\")\n",
                "print(Arr1)\n",
                "print(\"Array 2 :\")\n",
                "print(Arr2)\n",
                "print(\"Do both of these arrays share the same memory :\")\n",
                "print(id(Arr1)==id(Arr2))\n",
                "div1 = Arr1%3==0\n",
                "div2 = Arr2%3==0\n",
                "print(\"elements of array 1 divisible by 3 are :\")\n",
                "print(Arr1[div1])\n",
                "print(\"elements of array 2 divisible by 3 are :\")\n",
                "print(Arr2[div2])\n",
                "print(\"Array 2 after sorting is :\")\n",
                "Arr2.sort()\n",
                "print(Arr2)\n",
                "print(\"Sum of elements of array 1 is :\")\n",
                "print (Arr1.sum())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data",
                "ingest_data"
            ],
            "source": [
                "#Q2 Load the titanic dataset, remove missing values from all attributes, find mean value of first 50 samples, find the mean of the number of male passengers( Sex=1) on the ship, find the highest fare paid by any passenger.\n",
                "import pandas as pd\n",
                "df = pd.read_csv(\"../input/titanic/train_and_test2.csv\")\n",
                "df.head()\n",
                "\n",
                "df.dropna(axis=1, how='all')\n",
                "print(df.head())\n",
                "print(df.shape)\n",
                "\n",
                "print(df[:50].mean())\n",
                "\n",
                "print(df[df['Sex']==1].mean())\n",
                "\n",
                "print(df['Fare'].max())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "#Q3.A student has got the following marks ( English = 86, Maths = 83, Science = 86, History =90, Geography = 88). Wisely choose a graph to represent this data such that it justifies the purpose of data visualization. Highlight the subject in which the student has got least marks. \n",
                "from matplotlib import pyplot as plt\n",
                "slices=[87,83,86,90,88]\n",
                "Subject=['English','Maths','Science','History','Geography']\n",
                "plt.pie(slices,labels=Subject,startangle=90,shadow=True,explode=(0.08,0.5,0.08,0.08,0.08),autopct='%1.1f%%')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#Q4 Load the iris dataset, print the confusion matrix and f1_score as computed on the features.\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.model_selection import cross_val_predict\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import f1_score\n",
                "\n",
                "train = pd.read_csv(\"../input/iris-flower-dataset/IRIS.csv\")\n",
                "\n",
                "\n",
                "X = train.drop(\"species\",axis=1)\n",
                "y = train[\"species\"]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
                "\n",
                "logmodel = LogisticRegression()\n",
                "logmodel.fit(X_train,y_train)\n",
                "\n",
                "predictions = logmodel.predict(X_test)\n",
                "\n",
                "print(\"F1 Score(macro):\",f1_score(y_test, predictions,average='macro'))\n",
                "print(\"F1 Score(micro):\",f1_score(y_test, predictions,average='micro'))\n",
                "print(\"F1 Score(weighted):\",f1_score(y_test, predictions,average='weighted'))\n",
                "print(\"\\nConfusion Matrix(below):\\n\")\n",
                "confusion_matrix(y_test, predictions)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "#Creation of Boolean Array\n",
                "import numpy as np\n",
                "arr1=np.array([1,2,0,True,False],dtype=np.bool)\n",
                "arr1"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "import numpy as np\n",
                "array1 = np.array([0, 10, 20, 40, 60])\n",
                "print(\"Array1: \",array1)\n",
                "array2 = [10, 30, 40]\n",
                "print(\"Array2: \",array2)\n",
                "print(\"Common values between two arrays:\")\n",
                "print(np.intersect1d(array1, array2))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "import numpy as np\n",
                "a=np.array([33,33,13,44,55,66,77,55,12,23,21,34,59])\n",
                "b=np.array([21,34,55,77])\n",
                "x=b.argsort()\n",
                "out=a[b[x[np.searchsorted(b,a,sorter=x)]]!=a]\n",
                "print(out)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "exam_data  = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', 'Matthew', 'Laura'],\n",
                "        'score': [12.5, 9, 16.5, 6.0, 9, 20, np.nan, 5.5],\n",
                "        'attempts': [1, 3, 2, 3, 2, 3, 1, 1],\n",
                "        'qualify': ['yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no']}\n",
                "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
                "\n",
                "df = pd.DataFrame(exam_data , index=labels)\n",
                "print(df)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "exam_data  = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', 'Matthew', 'Laura'],\n",
                "        'score': [12.5, 9, 16.5, 6.0, 9, 20, np.nan, 5.5],\n",
                "        'attempts': [1, 3, 2, 3, 2, 3, 1, 1],\n",
                "        'qualify': ['yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no']}\n",
                "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
                "df = pd.DataFrame(exam_data , index=labels)\n",
                "print(\"Select specific columns and rows:\")\n",
                "print(df.iloc[[1, 3, 5, 6], [1, 3]])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "exam_data  = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', 'Matthew', 'Laura'],\n",
                "        'score': [12.5, 9, 16.5, 6.0, 9, 20, np.nan, 5.5],\n",
                "        'attempts': [1, 3, 2, 3, 2, 3, 1, 1],\n",
                "        'qualify': ['yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no']}\n",
                "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
                "df = pd.DataFrame(exam_data , index=labels)\n",
                "print(\"Rows where score is missing:\")\n",
                "print(df[df['score'].isnull()])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "from matplotlib import pyplot as plt\n",
                "x=['O+','A+','B+','AB+','O-','A-','B-','AB-']\n",
                "y=[9,12,2,10,7,1,4,5]\n",
                "plt.plot(x,y,'r--',x,y,'g^')\n",
                "plt.title('Blood group distribution of 50 patients')\n",
                "plt.ylabel('No of Patients')\n",
                "plt.xlabel('Blood group')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "from matplotlib import pyplot as plt\n",
                "slices=[85,87,92,98,80,83]\n",
                "Subject=['English','Bengali','Hindi','Maths','History','Geography']\n",
                "plt.pie(slices,labels=Subject,startangle=90,shadow=True,explode=(0.08,0.08,0.08,0.08,0.5,0.08),autopct='%1.1f%%')\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "from matplotlib import pyplot as plt\n",
                "heights=[161,150,154,165,168,161,154,162,150,121,162,164,171,165,158,154,156,172,160,170,153,159,161,170,162,165,166,168,165,164,154,152,153,156,158,172,172,161,12,166,161,12,162,167,168,159,158,153,154,159]\n",
                "bins=[150,155,160,165,170]\n",
                "plt.hist(heights,bins,histtype='bar',rwidth=0.5,color='blue')\n",
                "plt.xlabel('Height range')\n",
                "plt.ylabel('No of persons')\n",
                "plt.title(\"Heights Histogram\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#Write a Python program using Scikit-learn to print the keys, number of rows-columns, feature names and the description of the Iris data.\n",
                "import pandas as pd\n",
                "iris_data = pd.read_csv(\"../input/iriscsv/Iris.csv\")\n",
                "print(\"\\nKeys of Iris dataset:\")\n",
                "print(iris_data.keys())\n",
                "print(\"\\nNumber of rows and columns of Iris dataset:\")\n",
                "print(iris_data.shape) "
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "# Write a Python program to get the number of observations, missing values and nan values.\n",
                "import pandas as pd\n",
                "iris = pd.read_csv(\"../input/iriscsv/Iris.csv\")\n",
                "print(iris.info())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#Write a Python program to load the iris data from a given csv file into a dataframe and print the shape of the data, type of the data and first 3 rows.\n",
                "import pandas as pd\n",
                "data = pd.read_csv(\"../input/iriscsv/Iris.csv\")\n",
                "print(\"Shape of the data:\")\n",
                "print(data.shape)\n",
                "print(\"\\nData Type:\")\n",
                "print(type(data))\n",
                "print(\"\\nFirst 3 rows:\")\n",
                "print(data.head(3))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "#Display the Principal components that are calculated on the predictor variables and target variables. \n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "from sklearn.datasets import load_breast_cancer\n",
                "cancer = load_breast_cancer()\n",
                "cancer.keys()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "scaler = StandardScaler()\n",
                "scaler.fit(df)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "scaled_data = scaler.transform(df)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.decomposition import PCA\n",
                "pca = PCA(n_components=2)\n",
                "pca.fit(scaled_data)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x_pca = pca.transform(scaled_data)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "scaled_data.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "x_pca.shape"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#Using scatter plot show where the Principal components lie on the graph. \n",
                "plt.figure(figsize=(8,6))\n",
                "plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='rainbow')\n",
                "plt.xlabel('First principal component')\n",
                "plt.ylabel('Second Principal Component')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#2. Store height of 50 students in inches. Now while the data was beign recorded manually there has been some typing mistake and therefore height of 2 students have been recorded as 172 inch and 2 students have been recorded as 12 inch. Graphically plot and show how you can seggregate correct data from abnormal data.\n",
                "from matplotlib import pyplot as plt\n",
                "heights=[72,71,56,45,67,89,54,58,67,77,77,78,77,73,73,172,72,71,56,45,67,\n",
                "         89,54,58,67,172,77,78,77,73,73,172,12,54,64,75,75,77,88,66,70,12,54,64,75,75,77,88,66,70]\n",
                "def plot_his(heights):\n",
                "    start=min(heights)-min(heights)%10\n",
                "    end=max(heights)+10\n",
                "    bins=list(range(start,end,5))\n",
                "    plt.hist(heights,bins,histtype='bar',rwidth=0.5,color='#FF2400')\n",
                "    plt.xlabel('heights in inches')\n",
                "    plt.ylabel('No. of Students')\n",
                "    plt.title(\"Heights chart\")\n",
                "    plt.show()\n",
                "print(\"Abnormal Data\")\n",
                "plot_his(heights)\n",
                "heights=list(filter(lambda x: not x==172 and not x==12, heights))\n",
                "print(\"Correct Data\")\n",
                "plot_his(heights)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#3. Write a Python program to get the number of observations, missing values and nan values.\n",
                "import pandas as pd\n",
                "iris = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(iris.info())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "import pandas as pd\n",
                "data=pd.read_csv(\"../input/titanic/train_and_test2.csv\")\n",
                "data\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "#Replacing the value of year to 2020\n",
                "thisdict[\"year\"]=2020\n",
                "print(thisdict)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#Removing an item\n",
                "thisdict.pop(\"model\")\n",
                "print(thisdict)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import numpy as np \n",
                "import pandas as pd\n",
                "from nltk.tokenize import word_tokenize\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from nltk.corpus import stopwords\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.linear_model import SGDClassifier\n",
                "from sklearn.model_selection import cross_val_predict\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import roc_auc_score, classification_report, f1_score\n",
                "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "df  = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')"
            ]
        },
        {
            "tags": [
                "train_model",
                "process_data",
                "train_model"
            ],
            "source": [
                "vect = TfidfVectorizer()\n",
                "sklearn_tokenizer = vect.build_tokenizer()\n",
                "stop_words = set(stopwords.words(\"english\"))\n",
                "y = df.target.to_numpy()\n",
                "vect=TfidfVectorizer(tokenizer = sklearn_tokenizer,stop_words='english',ngram_range=(1, 1), norm='l2')\n",
                "clf=SGDClassifier(alpha=0.0001,epsilon=0.1, eta0=0.0,\n",
                "                               l1_ratio=0.1, learning_rate='optimal',\n",
                "                               loss='modified_huber', penalty='l2',class_weight =  'balanced')\n",
                "pp = Pipeline([('vect',vect),('clf',clf )])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "pp.fit(df.question_text.to_numpy(),y)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "test_df =  pd.read_csv('../input/quora-insincere-questions-classification/test.csv')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "test_df['prediction'] = pp.predict(test_df.question_text.to_numpy())\n",
                "test_df[['qid','prediction']].to_csv(\"submission.csv\", index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# Imports\n",
                "\n",
                "# pandas\n",
                "import pandas as pd\n",
                "from pandas import Series,DataFrame\n",
                "\n",
                "# numpy, matplotlib, seaborn\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "sns.set_style('whitegrid')\n",
                "%matplotlib inline\n",
                "\n",
                "# machine learning\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC, LinearSVC\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.naive_bayes import GaussianNB"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "# get titanic & test csv files as a DataFrame\n",
                "titanic_df = pd.read_csv(\"../input/train.csv\")\n",
                "test_df    = pd.read_csv(\"../input/test.csv\")\n",
                "\n",
                "# preview the data\n",
                "titanic_df.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "titanic_df.info()\n",
                "print(\"----------------------------\")\n",
                "test_df.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# drop unnecessary columns, these columns won't be useful in analysis and prediction\n",
                "titanic_df = titanic_df.drop(['PassengerId','Name','Ticket'], axis=1)\n",
                "test_df    = test_df.drop(['Name','Ticket'], axis=1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Fare\n",
                "\n",
                "# only for test_df, since there is a missing \"Fare\" values\n",
                "test_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True)\n",
                "\n",
                "# convert from float to int\n",
                "titanic_df['Fare'] = titanic_df['Fare'].astype(int)\n",
                "test_df['Fare']    = test_df['Fare'].astype(int)\n",
                "\n",
                "# get fare for survived & didn't survive passengers \n",
                "fare_not_survived = titanic_df[\"Fare\"][titanic_df[\"Survived\"] == 0]\n",
                "fare_survived     = titanic_df[\"Fare\"][titanic_df[\"Survived\"] == 1]\n",
                "\n",
                "# get average and std for fare of survived/not survived passengers\n",
                "avgerage_fare = DataFrame([fare_not_survived.mean(), fare_survived.mean()])\n",
                "std_fare      = DataFrame([fare_not_survived.std(), fare_survived.std()])\n",
                "\n",
                "# plot\n",
                "titanic_df['Fare'].plot(kind='hist', figsize=(15,3),bins=100, xlim=(0,50))\n",
                "\n",
                "avgerage_fare.index.names = std_fare.index.names = [\"Survived\"]\n",
                "avgerage_fare.plot(yerr=std_fare,kind='bar',legend=False)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# .... continue with plot Age column\n",
                "\n",
                "# peaks for survived/not survived passengers by their age\n",
                "facet = sns.FacetGrid(titanic_df, hue=\"Survived\",aspect=4)\n",
                "facet.map(sns.kdeplot,'Age',shade= True)\n",
                "facet.set(xlim=(0, titanic_df['Age'].max()))\n",
                "facet.add_legend()\n",
                "\n",
                "# average survived passengers by age\n",
                "fig, axis1 = plt.subplots(1,1,figsize=(18,4))\n",
                "average_age = titanic_df[[\"Age\", \"Survived\"]].groupby(['Age'],as_index=False).mean()\n",
                "sns.barplot(x='Age', y='Survived', data=average_age)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Cabin\n",
                "# It has a lot of NaN values, so it won't cause a remarkable impact on prediction\n",
                "titanic_df.drop(\"Cabin\",axis=1,inplace=True)\n",
                "test_df.drop(\"Cabin\",axis=1,inplace=True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Family\n",
                "\n",
                "# Instead of having two columns Parch & SibSp, \n",
                "# we can have only one column represent if the passenger had any family member aboard or not,\n",
                "# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.\n",
                "titanic_df['Family'] =  titanic_df[\"Parch\"] + titanic_df[\"SibSp\"]\n",
                "titanic_df['Family'].loc[titanic_df['Family'] > 0] = 1\n",
                "titanic_df['Family'].loc[titanic_df['Family'] == 0] = 0\n",
                "\n",
                "test_df['Family'] =  test_df[\"Parch\"] + test_df[\"SibSp\"]\n",
                "test_df['Family'].loc[test_df['Family'] > 0] = 1\n",
                "test_df['Family'].loc[test_df['Family'] == 0] = 0\n",
                "\n",
                "# drop Parch & SibSp\n",
                "titanic_df = titanic_df.drop(['SibSp','Parch'], axis=1)\n",
                "test_df    = test_df.drop(['SibSp','Parch'], axis=1)\n",
                "\n",
                "# plot\n",
                "fig, (axis1,axis2) = plt.subplots(1,2,sharex=True,figsize=(10,5))\n",
                "\n",
                "# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)\n",
                "sns.countplot(x='Family', data=titanic_df, order=[1,0], ax=axis1)\n",
                "\n",
                "# average of survived for those who had/didn't have any family member\n",
                "family_perc = titanic_df[[\"Family\", \"Survived\"]].groupby(['Family'],as_index=False).mean()\n",
                "sns.barplot(x='Family', y='Survived', data=family_perc, order=[1,0], ax=axis2)\n",
                "\n",
                "axis1.set_xticklabels([\"With Family\",\"Alone\"], rotation=0)"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "source": [
                "# Pclass\n",
                "\n",
                "# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])\n",
                "sns.factorplot('Pclass','Survived',order=[1,2,3], data=titanic_df,size=5)\n",
                "\n",
                "# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers\n",
                "pclass_dummies_titanic  = pd.get_dummies(titanic_df['Pclass'])\n",
                "pclass_dummies_titanic.columns = ['Class_1','Class_2','Class_3']\n",
                "pclass_dummies_titanic.drop(['Class_3'], axis=1, inplace=True)\n",
                "\n",
                "pclass_dummies_test  = pd.get_dummies(test_df['Pclass'])\n",
                "pclass_dummies_test.columns = ['Class_1','Class_2','Class_3']\n",
                "pclass_dummies_test.drop(['Class_3'], axis=1, inplace=True)\n",
                "\n",
                "titanic_df.drop(['Pclass'],axis=1,inplace=True)\n",
                "test_df.drop(['Pclass'],axis=1,inplace=True)\n",
                "\n",
                "titanic_df = titanic_df.join(pclass_dummies_titanic)\n",
                "test_df    = test_df.join(pclass_dummies_test)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# define training and testing sets\n",
                "\n",
                "X_train = titanic_df.drop(\"Survived\",axis=1)\n",
                "Y_train = titanic_df[\"Survived\"]\n",
                "X_test  = test_df.drop(\"PassengerId\",axis=1).copy()"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "# Logistic Regression\n",
                "\n",
                "logreg = LogisticRegression()\n",
                "\n",
                "logreg.fit(X_train, Y_train)\n",
                "\n",
                "Y_pred = logreg.predict(X_test)\n",
                "\n",
                "logreg.score(X_train, Y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "Support Vector Machines\n",
                "\n",
                "svc = SVC()\n",
                "\n",
                "svc.fit(X_train, Y_train)\n",
                "\n",
                "Y_pred = svc.predict(X_test)\n",
                "\n",
                "svc.score(X_train, Y_train)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "# Random Forests\n",
                "\n",
                "random_forest = RandomForestClassifier(n_estimators=100)\n",
                "\n",
                "random_forest.fit(X_train, Y_train)\n",
                "\n",
                "Y_pred = random_forest.predict(X_test)\n",
                "\n",
                "random_forest.score(X_train, Y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "knn = KNeighborsClassifier(n_neighbors = 3)\n",
                "\n",
                "knn.fit(X_train, Y_train)\n",
                "\n",
                "Y_pred = knn.predict(X_test)\n",
                "\n",
                "knn.score(X_train, Y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "Gaussian Naive Bayes\n",
                "\n",
                "gaussian = GaussianNB()\n",
                "\n",
                "gaussian.fit(X_train, Y_train)\n",
                "\n",
                "Y_pred = gaussian.predict(X_test)\n",
                "\n",
                "gaussian.score(X_train, Y_train)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# get Correlation Coefficient for each feature using Logistic Regression\n",
                "coeff_df = DataFrame(titanic_df.columns.delete(0))\n",
                "coeff_df.columns = ['Features']\n",
                "coeff_df[\"Coefficient Estimate\"] = pd.Series(logreg.coef_[0])\n",
                "\n",
                "# preview\n",
                "coeff_df"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submission = pd.DataFrame({\n",
                "        \"PassengerId\": test_df[\"PassengerId\"],\n",
                "        \"Survived\": Y_pred\n",
                "    })\n",
                "submission.to_csv('titanic.csv', index=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(\"You've successfully run some Python code\")\n",
                "print(\"Congratulations!\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# You don't need to worry for now about what this code does or how it works. If you're ever curious about the \n",
                "# code behind these exercises, it's available under an open source license here: https://github.com/Kaggle/learntools/\n",
                "# (But if you can understand that code, you'll probably find these lessons boring :)\n",
                "from learntools.core import binder; binder.bind(globals())\n",
                "from learntools.python.ex1 import *\n",
                "print(\"Setup complete! You're ready to start question 0.\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "(5 - 3) // 2"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "8 - 3 * 2 - (1 + 1)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "import random\n",
                "from matplotlib import pyplot as plt\n",
                "from learntools.python.quickdraw import random_category, sample_images_of_category, draw_images_on_subplots\n",
                "\n",
                "## Step 1: Sample some sketches\n",
                "# How many sketches to view - a random number from 2 to 20\n",
                "n = random.randint(2, 20)\n",
                "# Choose a random quickdraw category. (Check out https://quickdraw.withgoogle.com/data for an overview of categories)\n",
                "category = random_category()\n",
                "imgs = sample_images_of_category(n, category)\n",
                "\n",
                "## Step 2: Choose the grid properties\n",
                "######## Your changes should go here ###############\n",
                "rows = n // 8 + min(1, n % 8)\n",
                "cols = min(n, 8)\n",
                "height = rows * 2\n",
                "width = cols * 2\n",
                "\n",
                "## Step 3: Create the grid\n",
                "grid = plt.subplots(rows, cols, figsize=(width, height))\n",
                "\n",
                "## Step 4: Draw the sketches in the grid\n",
                "draw_images_on_subplots(imgs, grid)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "\"\"\"Q1.Store blood groups of 50 different atients and show the no of \n",
                "patients having O- blood grouQ1. Store blood groups of 50 different \n",
                "atients and show the no of patients having O- blood group\"\"\"\n",
                "\n",
                "from matplotlib import pyplot as plt\n",
                "blood_grp=['O+', 'A+', 'B+', 'AB+', 'O-', 'A-', 'B-', 'AB-']\n",
                "patients=[5, 10, 12, 5, 3, 4, 5, 6]\n",
                "colors = ['b','b','b','b','g','b','b','b']\n",
                "plt.bar(blood_grp,patients,color=colors)\n",
                "plt.legend()\n",
                "plt.xlabel('Blood Groups')\n",
                "plt.ylabel('No. of Patients')\n",
                "plt.title('Blood Group Data Set')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "\"\"\"Q2. Store data of marks acquired by a certain student and show \n",
                "them in form of a piechart and slice out the subject having least \n",
                "marks\"\"\"\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "subjects=['English','Bengali','Hindi','Maths','History','Geography']\n",
                "marks = [87,89,93,92,98,95]\n",
                "plt.pie(marks,labels=subjects,startangle=90,shadow=True,\n",
                "        explode=(0.2,0,0,0,0,0),autopct='%1.2f%%')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "\"\"\"Q3.Store data of heights of 50 students with 4 mistakes and plot \n",
                "them in a graph and segregate normal data from abnormal one\"\"\"\n",
                "\n",
                "heights=[72,71,56,45,67,89,54,58,67,77,77,78,77,73,73,172,72,71,56,\n",
                "         45,67,89,54,58,67,172,77,78,77,73,73,172,12,54,64,75,75,77,\n",
                "         88,66,70,12,54,64,75,75,77,88,66,70]\n",
                "def plot_his(heights):\n",
                "    start=min(heights)-min(heights)%10\n",
                "    end=max(heights)+10\n",
                "    bins=list(range(start,end,5))\n",
                "    plt.hist(heights,bins,histtype='bar',rwidth=0.5,color='c')\n",
                "    plt.xlabel('height of students (inches)')\n",
                "    plt.ylabel('No.of Students')\n",
                "    plt.show()\n",
                "plot_his(heights)\n",
                "heights=list(filter(lambda x: not x==172 and not x==12, heights))\n",
                "plot_his(heights)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "#Q1.printing the keys, number of rows-columns, feature names \n",
                "#and the description of the Iris data.\n",
                "import pandas as pd\n",
                "iris_data = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(\"\\nKeys of Iris dataset:\")\n",
                "print(iris_data.keys())\n",
                "print(\"\\nNumber of rows and columns of Iris dataset:\")\n",
                "print(iris_data.shape)\n",
                "print(\"Data type:\")\n",
                "print(type(iris_data))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#Q2.get the number of observations, missing values and nan values.\n",
                "print(\"No.of Observations are:\")\n",
                "print(iris_data.count().sum())\n",
                "print(\"No. of Nan is:\")\n",
                "print(iris_data.isnull().sum())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "#Q3.create a 2-D array with ones on the diagonal and zeros elsewhere.\n",
                "import numpy as np\n",
                "from scipy import sparse\n",
                "eye = np.eye(5)\n",
                "print(\"NumPy array:\\n\", eye)\n",
                "sparse_matrix = sparse.csr_matrix(eye)\n",
                "print(\"\\nSciPy sparse CSR matrix:\\n\", sparse_matrix)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#Q4.load the iris data from a given csv file into a dataframe and \n",
                "#print the shape of the data, type of the data and first 3 rows.\n",
                "print(\"Shape of the data:\")\n",
                "print(iris_data.shape)\n",
                "print(\"\\nData Type:\")\n",
                "print(type(iris_data))\n",
                "print(\"\\nFirst 3 rows:\")\n",
                "print(iris_data.head(3))"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "#Q2.Store height of 50 srudents in inches.Now while the data was being recorded manually there has been some typing mistakes therefore height of two students has been recorded as 172 inches and 2 students recorded as 12 inches. Graphically plot and show how you can seggregate the normal data from the abnormal data.\n",
                "import matplotlib.pyplot as plt\n",
                "heights=[72,71,56,45,67,89,54,58,67,77,77,78,77,73,73,172,72,71,56,\n",
                "         45,67,89,54,58,67,172,77,78,77,73,73,172,12,54,64,75,75,77,\n",
                "         88,66,70,12,54,64,75,75,77,88,66,70]\n",
                "def plot_his(heights):\n",
                "    start=min(heights)-min(heights)%10\n",
                "    end=max(heights)+10\n",
                "    bins=list(range(start,end,5))\n",
                "    plt.hist(heights,bins,histtype='bar',rwidth=0.5,color='c')\n",
                "    plt.xlabel('height of students (inches)')\n",
                "    plt.ylabel('No.of Students')\n",
                "    plt.show()\n",
                "print('Total Data')\n",
                "plot_his(heights)\n",
                "heights=list(filter(lambda x: not x==172 and not x==12, heights))\n",
                "print('Normal Data')\n",
                "plot_his(heights)"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data"
            ],
            "source": [
                "#Q3.get the number of observations, missing values and nan values.\n",
                "test_data = pd.read_csv(\"../input/titanicdataset-traincsv/train.csv\")\n",
                "print(\"No.of Observations are:\")\n",
                "print(test_data.count().sum())\n",
                "print(\"No. of Nan is:\")\n",
                "print(test_data.isnull().sum())"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd "
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "check_results"
            ],
            "source": [
                "train = pd.read_csv('../input/train.csv')\n",
                "test = pd.read_csv('../input/test.csv')\n",
                "\n",
                "X = train.drop([\"label\"], axis=1)\n",
                "X = X.values.astype('int32')\n",
                "print(\"loaded\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#reset\n",
                "X = train.drop([\"label\"], axis=1)\n",
                "X = X.values.astype('int32')\n",
                "\n",
                "# clean data\n",
                "print(X)\n",
                "cut = 2#50\n",
                "X[X <= cut] = 0\n",
                "X[X > cut] = 1\n",
                "print(X)"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "setup_notebook",
                "check_results",
                "train_model"
            ],
            "source": [
                "# create model\n",
                "X1 = np.split(X,2)[0]\n",
                "X2 = np.split(X,2)[1]\n",
                "\n",
                "y1 = np.split(train[\"label\"],2)[0]\n",
                "y2 = np.split(train[\"label\"],2)[1]\n",
                "\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "\n",
                "model = DecisionTreeClassifier()\n",
                "\n",
                "model.fit(X2,y2) # easier to use 2 first than fix indexing\n",
                "\n",
                "p = model.predict(X1)\n",
                "\n",
                "total = 0\n",
                "for i in range(200):\n",
                "    if y1[i] == p[i]:\n",
                "        total += 1\n",
                "\n",
                "print(total/200) # print accuracy"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "check_results"
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "X_train = X\n",
                "X_train = X_train.reshape(X_train.shape[0], 28, 28)\n",
                "\n",
                "print('start')\n",
                "fig, axes = plt.subplots(nrows=2, ncols=8)\n",
                "\n",
                "I = 0\n",
                "for ax in axes.flat[:]:\n",
                "    for i in range(I+1, 500):\n",
                "        if(p[i]) != train[\"label\"][i]:\n",
                "            I = i\n",
                "            break\n",
                "    ax.set_title(p[I]);\n",
                "    ax.set_yticklabels([])\n",
                "    ax.set_xticklabels([])\n",
                "    ax.imshow(X_train[i], cmap=plt.get_cmap('gray'))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model",
                "transfer_results"
            ],
            "source": [
                "model.fit(X,train[\"label\"])\n",
                "predictions = model.predict(test)\n",
                "\n",
                "submissions = pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
                "                         \"Label\": predictions})\n",
                "submissions.to_csv(\"DR.csv\", index=False, header=True)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import os\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "from sklearn.metrics import roc_auc_score\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.feature_selection import VarianceThreshold\n",
                "from sklearn.mixture import GaussianMixture\n",
                "from sklearn.covariance import GraphicalLasso\n",
                "from sklearn.preprocessing import StandardScaler"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "train = pd.read_csv('../input/train.csv')\n",
                "test = pd.read_csv('../input/test.csv')\n",
                "train['wheezy-copper-turtle-magic'] = train['wheezy-copper-turtle-magic'].astype('category')\n",
                "test['wheezy-copper-turtle-magic'] = test['wheezy-copper-turtle-magic'].astype('category')"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "ingest_data",
                "process_data"
            ],
            "source": [
                "magicNum = 131073\n",
                "default_cols = [c for c in train.columns if c not in ['id', 'target','target_pred', 'wheezy-copper-turtle-magic']]\n",
                "cols = [c for c in default_cols]\n",
                "sub = pd.read_csv('../input/sample_submission.csv')\n",
                "sub.to_csv('submission.csv',index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "#1\n",
                "import numpy as np\n",
                "a=np.array([9,5,4,3,2,6])\n",
                "b=np.array([5,8,6,9,2,1])\n",
                "print(\"CHECK IF B HAS SAME VIEWS TO MEMORY IN A\")\n",
                "print(b.base is a)\n",
                "print(\"CHECK IF A HAS SAME VIEWS TO MEMORY IN B\")\n",
                "print(a.base is b)\n",
                "div_by_3=a%3==0\n",
                "div1_by_3=b%3==0\n",
                "print(\"Divisible By 3\")\n",
                "print(a[div_by_3])\n",
                "print(b[div1_by_3])\n",
                "b[::-1].sort()\n",
                "print(\"SECOND ARRAY SORTED\")\n",
                "print(b)\n",
                "print(\"SUM OF ELEMENTS OF FIRST ARRAY\")\n",
                "print(np.sum(a))"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data",
                "process_data"
            ],
            "source": [
                "#2\n",
                "df = pd.read_csv(\"../input/titanic/train_and_test2.csv\")\n",
                "df.head()\n",
                "\n",
                "df.dropna(axis=1, how='all')\n",
                "print(df.head())\n",
                "print(df.shape)\n",
                "\n",
                "df[:50].mean()\n",
                "\n",
                "df[df['Sex']==1].mean()\n",
                "\n",
                "df['Fare'].max()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "#3\n",
                "import matplotlib.pyplot as plt\n",
                "teams = ['English', 'Maths', 'Science ', 'History', 'Geography']\n",
                "slices = [86, 83, 86, 90, 88]\n",
                "colors = ['r', 'y', 'g', 'b','c']\n",
                "plt.pie(slices, labels = teams, colors=colors,\n",
                " startangle=90, shadow = True, explode = (0, 0.5, 0, 0, 0),\n",
                " radius = 1.2, autopct = '%1.1f%%')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "#4\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.model_selection import cross_val_predict\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import f1_score\n",
                "\n",
                "\n",
                "train = pd.read_csv(\"../input/iris-flower-dataset/IRIS.csv\")\n",
                "X= train.drop(\"species\", axis=1)\n",
                "y= train[\"species\"]\n",
                "X_train, X_test,y_train, y_test= train_test_split(X,y, test_size=0.3)\n",
                "\n",
                "logmodel= LogisticRegression()\n",
                "logmodel.fit(X_train, y_train)\n",
                "\n",
                "predictions = logmodel.predict(X_test)\n",
                "\n",
                "print(\"F1 Score(macro):\", f1_score(y_test, predictions, average='macro'))\n",
                "print(\"F1 Score(micro):\", f1_score(y_test, predictions, average='micro'))\n",
                "print(\"F1 Score(weighted):\", f1_score(y_test, predictions, average='weighted'))\n",
                "print(\"\\confusion Matrix(below):\\n\")\n",
                "confusion_matrix(y_test, predictions)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#1\n",
                "import pandas as pd \n",
                "iris_data = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(\"\\nKeys of Iris dataset:\")\n",
                "print(iris_data.keys())\n",
                "print(\"\\nNumber of rows and columns of Iris dataset:\")\n",
                "print(iris_data.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#2\n",
                "import pandas as pd\n",
                "iris = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(iris.info())\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#4\n",
                "import pandas as pd\n",
                "data = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(\"Shape of the data:\")\n",
                "print(data.shape)\n",
                "print(\"\\nData Type:\")\n",
                "print(type(data))\n",
                "print(\"\\nFirst 3 rows:\")\n",
                "print(data.head(3))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "dataset = pd.read_csv(\"../input/wine-customer-segmentation/Wine.csv\")\n",
                "dataset"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "dataset.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X = dataset.iloc[:,0:13]\n",
                "y = dataset.iloc[:, 13]"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "sc = StandardScaler()\n",
                "\n",
                "X_train = sc.fit_transform(X_train)\n",
                "X_test = sc.fit_transform(X_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.decomposition import PCA\n",
                "pca = PCA(n_components = 2)\n",
                "\n",
                "X_train = pca.fit_transform(X_train)\n",
                "X_test = pca.transform(X_test)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X_train"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X_test"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "new_dataset_train = pd.DataFrame(data=X_train, columns=['PC1', 'PC2'])\n",
                "new_dataset_test = pd.DataFrame(data=X_test, columns=['PC1', 'PC2'])\n",
                "# Con-catenating test and train datasets\n",
                "new_dataset = pd.concat([new_dataset_train.reset_index (drop=True), new_dataset_test], axis=1)\n",
                "new_dataset.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "dataset.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "process_data",
                "ingest_data"
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "from sklearn.datasets import load_breast_cancer\n",
                "cancer = load_breast_cancer()\n",
                "df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n",
                "df.head()\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "scaler = StandardScaler()\n",
                "scaler.fit(df)\n",
                "scaled_data = scaler.transform(df)\n",
                "from sklearn.decomposition import PCA\n",
                "pca = PCA(n_components=2)\n",
                "pca.fit(scaled_data)\n",
                "x_pca = pca.transform(scaled_data)\n",
                "plt.figure(figsize=(8,6))\n",
                "plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='rainbow')\n",
                "plt.xlabel('First principal component')\n",
                "plt.ylabel('Second Principal Component')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "setup_notebook"
            ],
            "source": [
                "DATA_PATH = '/kaggle/input/rs6-attrition-predict/'\n",
                "train = pd.read_csv(f'{DATA_PATH}/train.csv')\n",
                "test = pd.read_csv(f'{DATA_PATH}/test.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(train), len(test)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.columns"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.YearsAtCompany.unique()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "for col in category_cols:\n",
                "    nunique_tr = train[col].nunique()\n",
                "    nunique_te = test[col].nunique()\n",
                "    na_tr = len(train.loc[train[col].isna()]) / len(train)\n",
                "    na_te = len(test.loc[test[col].isna()]) / len(test)\n",
                "    print(f'Col name:{col:30}\\tunique cate num in train:{nunique_tr:5}\\tunique cate num in train:{nunique_te:5}\\tnull sample in train:{na_tr:.2f}\\tnull sample in test:{na_te:.2f}')\n",
                "    "
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "for col in digital_cols:\n",
                "    min_tr = train[col].min()\n",
                "    max_tr = train[col].max()\n",
                "    mean_tr = train[col].mean()\n",
                "    median_tr = train[col].median()\n",
                "    std_tr = train[col].std()\n",
                "    \n",
                "    min_te = test[col].min()\n",
                "    max_te = test[col].max()\n",
                "    mean_te = test[col].mean()\n",
                "    median_te = test[col].median()\n",
                "    std_te = test[col].std()\n",
                "    \n",
                "    na_tr = len(train.loc[train[col].isna()]) / len(train)\n",
                "    na_te = len(test.loc[test[col].isna()]) / len(test)\n",
                "    print(f'Col name:{col:30}')\n",
                "    print(f'\\tIn train data: min value:{min_tr:.2f}\\tmax value:{max_tr:.2f}\\tmean value:{mean_tr:.2f}\\tmedian value:{median_tr:.2f}\\tstd value:{std_tr:.2f}\\tnan sample rate:{na_tr:.2f}\\t')\n",
                "    print(f'\\tIn  test data: min value:{min_te:.2f}\\tmax value:{max_te:.2f}\\tmean value:{mean_te:.2f}\\tmedian value:{median_te:.2f}\\tstd value:{std_te:.2f}\\tnan sample rate:{na_te:.2f}\\t')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train[target_col].unique()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import MinMaxScaler\n",
                "\n",
                "sacalar = MinMaxScaler()\n",
                "train_digital = sacalar.fit_transform(train[digital_cols])\n",
                "test_digital = sacalar.transform(test[digital_cols])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
                "\n",
                "train_category, test_category = None, None\n",
                "drop_cols = ['EmployeeNumber', 'Over18', 'StandardHours']\n",
                "for col in [var for var in category_cols if var not in drop_cols]:\n",
                "    lbe, ohe = LabelEncoder(), OneHotEncoder()\n",
                "    \n",
                "    lbe.fit(pd.concat([train[col], test[col]]).values.reshape(-1, 1))\n",
                "    train[col] = lbe.transform(train[col])\n",
                "    test[col] = lbe.transform(test[col])\n",
                "    \n",
                "    ohe.fit(pd.concat([train[col], test[col]]).values.reshape(-1, 1))\n",
                "    oht_train = ohe.transform(train[col].values.reshape(-1, 1)).todense()\n",
                "    oht_test = ohe.transform(test[col].values.reshape(-1, 1)).todense()\n",
                "    \n",
                "    if train_category is None:\n",
                "        train_category = oht_train\n",
                "        test_category = oht_test\n",
                "    else:\n",
                "        train_category = np.hstack((train_category, oht_train))\n",
                "        test_category = np.hstack((test_category, oht_test))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_digital.shape, test_digital.shape, train_category.shape, test_category.shape"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "train_features = np.hstack((train_digital, train_category))\n",
                "test_features = np.hstack((test_digital, test_category))\n",
                "train_features.shape, test_features.shape"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "target_col_dict = {'Yes': 1, 'No': 0}\n",
                "train_labels = train[target_col].map(target_col_dict).values\n",
                "train_labels.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "from sklearn.linear_model import LinearRegression\n",
                "\n",
                "clf = LinearRegression()\n",
                "clf.fit(train_features, train_labels)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "predictions = clf.predict(test_features)\n",
                "predictions.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "predictions.mean()"
            ]
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "source": [
                "sub = test[['user_id']].copy()\n",
                "sub['Attrition'] = predictions\n",
                "sub['Attrition'] = sub['Attrition'].apply(lambda x: x if x >=0 else 0.0005)\n",
                "sub.to_csv('submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "hbfd"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from torchvision.utils import make_grid\n",
                "import torchvision.utils as vutils\n",
                "import matplotlib.animation as animation\n",
                "from IPython.display import HTML\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import copy\n",
                "import math\n",
                "import torch.utils.checkpoint as cp\n",
                "\n",
                "import time\n",
                "import cv2 as cv\n",
                "from tqdm import tqdm_notebook as tqdm\n",
                "import matplotlib.image as mpimg\n",
                "from math import exp\n",
                "\n",
                "import torchvision.transforms.functional as TF\n",
                "from collections import OrderedDict\n"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "x_train_path = '../input/red-sample/train_blur/'\n",
                "y_train_path = '../input/red-sample/train_sharp/'\n",
                "x_test_path = '../input/red-sample/test_blur/'"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "train_x_img_paths = []\n",
                "train_y_img_paths = []\n",
                "test_img_paths = []"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(torch.__version__)"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "#12 16 24\n",
                "block_config =(6, 10, 8)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "IMG_WIDTH = 1280//2\n",
                "IMG_HEIGHT = 720//2\n",
                "latent_size = 200"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "scale_ratio = 2"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "class DeformUnit(nn.Module):\n",
                "    def __init__(self, inc,outc):\n",
                "        super(DeformUnit, self).__init__()\n",
                "        self.conv = DeformConv2d(inc=inc, outc=outc-inc,kernel_size=3, stride=1, padding=1, bias=False, modulation=False)\n",
                "        self.relu = nn.ReLU(inplace=True)\n",
                "\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = torch.cat([x,self.relu(self.conv(x))],1)\n",
                "        return x\n",
                "\n",
                "class DenseDeformUnit(nn.Module):\n",
                "    def __init__(self, inc, f,out):\n",
                "        super(DenseDeformUnit, self).__init__()\n",
                "        print(inc,f)\n",
                "        self.deformunit1 = DeformUnit(inc,inc+f)\n",
                "        inc = inc+f\n",
                "        print(inc,f)\n",
                "        self.deformunit2 = DeformUnit(inc, inc + f)\n",
                "        inc = inc + f\n",
                "        self.deformunit3 = DeformUnit(inc, inc + f)\n",
                "        inc = inc + f\n",
                "        self.deformunit4 = DeformUnit(inc, inc + f)\n",
                "        inc = inc + f\n",
                "        self.deformunit5 = DeformUnit(inc, inc + f)\n",
                "        inc = inc + f\n",
                "        self.deformunit6 = DeformUnit(inc, inc + f)\n",
                "        inc = inc + f\n",
                "        self.conv =  nn.Sequential(\n",
                "            nn.Conv2d(in_channels=inc, out_channels=out, kernel_size=(1, 1), stride=(1, 1)),\n",
                "            nn.ReLU()\n",
                "        )\n",
                "\n",
                "\n",
                "    def forward(self, x):\n",
                "        originalx = x\n",
                "        x = self.deformunit1(x)\n",
                "        x = self.deformunit2(x)\n",
                "        x = self.deformunit3(x)\n",
                "        x = self.deformunit4(x)\n",
                "        x = self.deformunit5(x)\n",
                "        x = self.deformunit6(x)\n",
                "        x = self.conv(x)\n",
                "        x = torch.cat([originalx,x],1)\n",
                "        return x\n",
                "\n",
                "\n",
                "\n",
                "class Deform_UpConv(nn.Sequential):\n",
                "    def __init__(self, num_input_features, num_output_features):\n",
                "        super(Deform_UpConv, self).__init__()\n",
                "        self.add_module('conv', nn.ConvTranspose2d(in_channels=num_input_features, out_channels=num_output_features,kernel_size=(2, 2), stride=(2, 2)))\n",
                "        self.add_module('relu', nn.ReLU(inplace=True))\n",
                "\n",
                "\n",
                "\n",
                "class Deform_DenseNet(nn.Module):\n",
                "\n",
                "    def __init__(self,inc,f,grow,feature_vector = feature_vector):\n",
                "\n",
                "        super(Deform_DenseNet, self).__init__()\n",
                "        out = 4*grow\n",
                "        self.deformdenseunit1 = DenseDeformUnit(inc,f,out)\n",
                "        inc = out+inc+feature_vector[1]\n",
                "        out = out+2*grow\n",
                "        self.deformdenseunit2 = DenseDeformUnit(inc, f,out)\n",
                "        inc = out+inc+feature_vector[2]\n",
                "        out = out+2*grow\n",
                "        self.deformdenseunit3 = DenseDeformUnit(inc, f, out)\n",
                "\n",
                "    def forward(self, x):\n",
                "        features = self.features(x)\n",
                "        #         print(features.shape)\n",
                "        out = F.relu(features, inplace=True)\n",
                "        #         print(out.shape)\n",
                "        return out\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "device = 'cuda'\n",
                "netG = Generator(True,block_config =block_config,num_encoded_channels = num_encoded_channels).to(device)\n",
                "# netG.apply(weights_init)\n",
                "inp = torch.randn(IMG_WIDTH*IMG_HEIGHT*3 * 1).view((-1,3,IMG_HEIGHT,IMG_WIDTH)).to(device)\n",
                "output = netG(inp)\n",
                "print(output.shape)\n",
                "print((output.shape[1]*output.shape[2]*output.shape[3])/(IMG_WIDTH*IMG_HEIGHT*3))\n",
                "encoded_size = output.shape\n",
                "del inp,netG,output\n",
                "torch.cuda.empty_cache()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "feature_vector"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data",
                "visualize_data"
            ],
            "source": [
                "batch_size=1\n",
                "dataset = ImageData()\n",
                "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
                "\n",
                "a = next(iter(dataloader))\n",
                "\n",
                "print(a[1].shape)\n",
                "print(a[0].shape)\n",
                "img1 = a[0][0]\n",
                "img2 = a[1][0]\n",
                "f, axarr = plt.subplots(1,2)\n",
                "axarr[0].imshow(img1.permute(1,2,0))\n",
                "axarr[1].imshow(img2.permute(1,2,0))\n",
                "f.set_figheight(24)\n",
                "f.set_figwidth(24)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# netD = Discriminator().to(device).apply(weights_init)\n",
                "netG = Generator(False,block_config =block_config,num_encoded_channels = num_encoded_channels).to(device)\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def init_weights(m):\n",
                "    if type(m) == nn.Linear:\n",
                "        torch.nn.init.xavier_uniform_(m.weight)\n",
                "        m.bias.data.fill_(0.01)\n",
                "    try:\n",
                "        torch.nn.init.xavier_uniform(m.weight)\n",
                "    except:\n",
                "        _\n",
                "#         print(m)\n",
                "_ = netG.apply(init_weights)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "lr = 0.0002\n",
                "# Initialize BCELoss function\n",
                "criterion = nn.BCELoss()\n",
                "msecriterion = nn.MSELoss()\n",
                "l1criterion = nn.L1Loss()\n",
                "# Establish convention for real and fake labels during training\n",
                "real_label = 1\n",
                "fake_label = 0\n",
                "\n",
                "# Setup Adam optimizers for both G and D\n",
                "# optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
                "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "torch.save(netG, \"netG.model\")"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "netG.eval()\n",
                "valid_batch = next(iter(dataloader))\n",
                "blur_images = valid_batch[0].to(device)\n",
                "output_heatmap = netG(blur_images)\n",
                "rec_img =  (output_heatmap[0].cpu())"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(rec_img.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "rimage = rec_img.detach().permute(1, 2, 0)\n",
                "plt.imshow(rimage)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "rimage = rec_img.detach().permute(1, 2, 0)\n",
                "plt.imshow(rimage)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(rimage.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "%matplotlib inline\n",
                "from copy import deepcopy\n",
                "from collections import OrderedDict\n",
                "import gc\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm_notebook\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.optim import SGD,Adam,lr_scheduler\n",
                "from torch.utils.data import random_split\n",
                "import torchvision\n",
                "from torchvision import transforms,models\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "!tar -zxvf ../input/cifar10-python/cifar-10-python.tar.gz\n"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "batch_size = 128\n",
                "img_size = 28 #224\n",
                "trainset_size = 10000"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "train_transform = transforms.Compose([\n",
                "    transforms.Resize(img_size),\n",
                "#     transforms.RandomHorizontalFlip(p=.40),\n",
                "#     transforms.RandomRotation(30),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
                "\n",
                "test_transform = transforms.Compose([\n",
                "    transforms.Resize(224),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
                "\n",
                "traindata = torchvision.datasets.CIFAR10(root='.', train=True,download=False, transform=train_transform)\n",
                "\n",
                "trainset,valset = random_split(traindata,[trainset_size,50000-trainset_size])\n",
                "\n",
                "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True)\n",
                "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,shuffle=False)\n",
                "\n",
                "testset = torchvision.datasets.CIFAR10(root='.', train=False,download=False, transform=test_transform)\n",
                "testloader = torch.utils.data.DataLoader(testset, batch_size=64,shuffle=False)\n",
                "\n",
                "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from torchvision.utils import make_grid\n",
                "import torchvision.utils as vutils\n",
                "import matplotlib.animation as animation\n",
                "from IPython.display import HTML\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import copy\n",
                "import time\n",
                "import networkx as nx\n",
                "\n",
                "from torch.autograd import Variable\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "\n",
                "device = 'cpu'\n",
                "if torch.cuda.is_available() :\n",
                "    device = 'cuda'"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# custom weights initialization called on netG and netD\n",
                "def weights_init(m):\n",
                "    classname = m.__class__.__name__\n",
                "    if classname.find('Conv') != -1:\n",
                "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
                "    elif classname.find('BatchNorm') != -1:\n",
                "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
                "        nn.init.constant_(m.bias.data, 0)\n",
                "    "
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def get_max_pool_layer(ksize):\n",
                "    return nn.MaxPool2d(ksize).apply(weights_init)\n",
                "\n",
                "\n",
                "def get_avg_pool_layer(ksize):\n",
                "    return nn.AvgPool2d(ksize).apply(weights_init)\n",
                "\n",
                "def get_conv_layer(in_lyr,out_lyr,ksize):\n",
                "    seq_layer = nn.Sequential(\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(in_lyr, out_lyr, ksize),\n",
                "            nn.BatchNorm2d(out_lyr),\n",
                "    )\n",
                "    return seq_layer.apply(weights_init)\n",
                "\n",
                "class depthwise_separable_conv(nn.Module):\n",
                "    def __init__(self, nin, kernels_per_layer, nout):\n",
                "        super(depthwise_separable_conv, self).__init__()\n",
                "        self.depthwise = nn.Conv2d(nin, nin * kernels_per_layer, kernel_size=3, padding=1, groups=nin).apply(weights_init)\n",
                "        self.pointwise = nn.Conv2d(nin * kernels_per_layer, nout, kernel_size=1).apply(weights_init)\n",
                "\n",
                "    def forward(self, x):\n",
                "        out = self.depthwise(x)\n",
                "        out = self.pointwise(out)\n",
                "        return out\n",
                "\n",
                "def get_sep_layer(in_lyr,out_lyr,ksize):\n",
                "    return depthwise_separable_conv(in_lyr,ksize,out_lyr)\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# 1. Conv 3\n",
                "# 2. Conv 5\n",
                "# 3. Sep 3\n",
                "# 4. Sep 5\n",
                "# 5. MaxPool\n",
                "# 6. AvgPool\n",
                "\n",
                "out_features_shape = [10,24,32,64,128]\n",
                "output_size = 10\n",
                "num_actvn_fns = 6\n",
                "\n",
                "def get_new_layer(actvn,x):\n",
                "    ofsindex = 1\n",
                "    actvn = actvn -1\n",
                "#     print(\"Actvn -- \"+str(actvn))\n",
                "    \n",
                "    out_features_shape[ofsindex] = x.shape[1]\n",
                "    \n",
                "    if actvn == 0:\n",
                "        return get_conv_layer(x.shape[1],out_features_shape[ofsindex],3).to(device)\n",
                "    if actvn == 1:\n",
                "        return get_conv_layer(x.shape[1],out_features_shape[ofsindex],5).to(device)\n",
                "    if actvn == 2:\n",
                "        return get_sep_layer(x.shape[1],out_features_shape[ofsindex],3).to(device)\n",
                "    if actvn == 3:\n",
                "        return get_sep_layer(x.shape[1],out_features_shape[ofsindex],5).to(device)\n",
                "    if actvn == 4:\n",
                "        return get_max_pool_layer(3).to(device)\n",
                "    if actvn == 5:\n",
                "        return get_avg_pool_layer(3).to(device)\n",
                "    return get_avg_pool_layer(3).to(device)\n",
                "\n",
                "def get_out_layer(size):\n",
                "    nlayer = nn.Sequential(\n",
                "            nn.Linear(in_features=size[1]*size[2]*size[3], out_features=10),\n",
                "            nn.LogSoftmax(),\n",
                "        )\n",
                "    return nlayer.apply(weights_init).to(device)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def concatzeroes(max_height,max_width,b):\n",
                "    dim = 2\n",
                "    pad_size_1 = int((max_height-b.shape[dim]) / 2)\n",
                "    pad_size_2 = (max_height-b.shape[dim]) - pad_size_1\n",
                "    b = F.pad(input=b, pad=(0, 0, pad_size_1, pad_size_2), mode='constant', value=0)\n",
                "\n",
                "    dim = 3\n",
                "    pad_size_1 = int((max_width-b.shape[dim]) / 2)\n",
                "    pad_size_2 = (max_width-b.shape[dim]) - pad_size_1\n",
                "    b = F.pad(input=b, pad=(pad_size_1, pad_size_2,0,0), mode='constant', value=0)\n",
                "    \n",
                "    return b\n",
                "\n",
                "def downsizetensors(max_height,max_width,b):\n",
                "    size = (max_height,max_width)\n",
                "    return F.interpolate(b, size=size, mode='bilinear', align_corners=False)\n",
                "\n",
                "def concatenate2(a,b,increase_size):\n",
                "    if(increase_size):\n",
                "        max_height = max(a.shape[2],b.shape[2])\n",
                "        max_width = max(a.shape[3],b.shape[3])\n",
                "        a = concatzeroes(max_height,max_width,a)\n",
                "        b = concatzeroes(max_height,max_width,b)\n",
                "        return torch.cat((a, b), 1)\n",
                "    else:\n",
                "        max_height = min(a.shape[2],b.shape[2])\n",
                "        max_width = min(a.shape[3],b.shape[3])\n",
                "        a = downsizetensors(max_height,max_width,a)\n",
                "        b = downsizetensors(max_height,max_width,b)\n",
                "        return torch.cat((a, b), 1)\n",
                "\n",
                "\n",
                "def concatenate3(a,b,c,increase_size):\n",
                "    if(increase_size):\n",
                "        max_height = max(a.shape[2],b.shape[2],c.shape[2])\n",
                "        max_width = max(a.shape[3],b.shape[3],c.shape[3])\n",
                "        a = concatzeroes(max_height,max_width,a)\n",
                "        b = concatzeroes(max_height,max_width,b)\n",
                "        c = concatzeroes(max_height,max_width,c)\n",
                "        return torch.cat((torch.cat((a, b), 1), c), 1)\n",
                "    else:\n",
                "        \n",
                "        max_height = min(a.shape[2],b.shape[2],c.shape[2])\n",
                "        max_width = min(a.shape[3],b.shape[3],c.shape[3])\n",
                "        a = downsizetensors(max_height,max_width,a)\n",
                "        b = downsizetensors(max_height,max_width,b)\n",
                "        c = downsizetensors(max_height,max_width,c)\n",
                "        return torch.cat((torch.cat((a, b), 1), c), 1)"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "num_nodes = 6\n",
                "dag_model = DAG(num_nodes)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "lr = 0.0002\n",
                "# Initialize BCELoss function\n",
                "criterion = nn.NLLLoss()\n",
                "optimizer = None\n",
                "\n",
                "\n",
                "def model_train_loop(cnn_config):\n",
                "    dag_model.config = cnn_config\n",
                "    dag_model.initialize_param_grads()\n",
                "    for epoch in range(10):\n",
                "        # For each batch in the dataloader\n",
                "        for i, data in enumerate(trainloader, 0):\n",
                "            images, labels = data\n",
                "            images, labels = images.to(device),labels.to(device)\n",
                "            output = dag_model(images)\n",
                "            if i==0 and epoch==0:\n",
                "                optimizer = optim.Adam(dag_model.myparameters, lr=lr, betas=(0.5, 0.999))\n",
                "            dag_model.zero_grad()\n",
                "            loss = criterion(output, labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "#         print(loss)\n",
                "#             if i%100==0:\n",
                "#                 print(((torch.sum(torch.argmax(output,1) == labels)).float()/labels.shape[0]).item())\n",
                "#     #             print(loss.item())\n",
                "        \n",
                "    return Variable(((torch.sum(torch.argmax(output,1) == labels)).float()/labels.shape[0]), requires_grad=True)"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "model_train_loop([2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 4, 1, 4, 4, 5, 4])"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from torch.distributions import Categorical\n",
                "from torch.autograd import Variable"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "controller_num_epochs = 1\n",
                "controller_optimizer = None"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model"
            ],
            "source": [
                "controller_model = Controller().to(device)\n",
                "controller_optimizer = optim.Adam(controller_model.parameters(), lr=0.1, betas=(0.5, 0.999))\n",
                "controller_model()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from torchvision.utils import make_grid\n",
                "import torchvision.utils as vutils\n",
                "import matplotlib.animation as animation\n",
                "from IPython.display import HTML\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import copy\n",
                "import time\n",
                "import cv2 as cv\n",
                "from tqdm import tqdm_notebook as tqdm\n",
                "import matplotlib.image as mpimg\n",
                "\n",
                "import torchvision.transforms.functional as TF"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# Code -- https://github.com/alexandru-dinu/cae\n",
                "# DataBase -- https://www.kaggle.com/hsankesara/flickr-image-dataset\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "img_dir = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images/'\n",
                "img_list = os.listdir(img_dir)\n",
                "print(len(img_list))\n",
                "valid_ratio = 0.8"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results",
                "process_data",
                "ingest_data"
            ],
            "source": [
                "class ImageData(Dataset):\n",
                "    def __init__(self,is_train=True):\n",
                "        self.is_train = is_train\n",
                "        self.transform = transforms.Compose([transforms.ToTensor(),])\n",
                "        self.train_index = int(valid_ratio * len(img_list))\n",
                "        self.crop = transforms.CenterCrop((218,178))\n",
                "    def __len__(self):\n",
                "        if self.is_train:\n",
                "            return self.train_index\n",
                "        else:\n",
                "            return len(img_list) - self.train_index -1\n",
                "    def __getitem__(self, index):\n",
                "        if not self.is_train:\n",
                "            index = self.train_index + index\n",
                "#         print(\"hey  \"*4 + str(index))\n",
                "        img = mpimg.imread(img_dir+img_list[index])\n",
                "        img = self.crop(TF.to_pil_image(img))\n",
                "        img = self.transform(img)\n",
                "        img = (img-0.5) /0.5\n",
                "#         img = (img - 255.0) / 255.0\n",
                "        return img"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "batch_size=20\n",
                "dataset = ImageData(is_train=False)\n",
                "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
                "device = 'cuda'"
            ]
        },
        {
            "tags": [
                "check_results",
                "visualize_data"
            ],
            "source": [
                "a = next(iter(dataloader))\n",
                "print(a[0].shape)\n",
                "img = a[15]\n",
                "img = img *0.5 + 0.5\n",
                "plt.imshow(img.permute(1,2,0))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "IMG_WIDTH = 178\n",
                "IMG_HEIGHT = 218\n",
                "latent_size = 200"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "num_images_to_show = 5\n",
                "\n",
                "\n",
                "valid_dataset = ImageData(is_train=False)\n",
                "batch_size = num_images_to_show\n",
                "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
                "valid_batch = next(iter(valid_dataloader)).to(device)\n",
                "valid_batch_1 = next(iter(valid_dataloader)).to(device)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "device"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "\n",
                "f, axarr = plt.subplots(num_images_to_show,4)\n",
                "\n",
                "axarr[0,0].title.set_text('Original \\n Image')\n",
                "axarr[0,1].title.set_text('Reconstructed Image with \\n 43% Compression')\n",
                "axarr[0,2].title.set_text('Reconstructed Image with \\n 68% Compression')\n",
                "axarr[0,3].title.set_text('Reconstructed Image with \\n 84% Compression')\n",
                "\n",
                "for i in range(4):\n",
                "    axarr[0,i].title.set_fontsize(15)\n",
                "\n",
                "for i in range(num_images_to_show):\n",
                "    axarr[i,0].imshow((valid_batch[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5)\n",
                "    axarr[i,1].imshow((reconstructed_img_28[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5)\n",
                "    axarr[i,2].imshow((reconstructed_img_16[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5)\n",
                "    axarr[i,3].imshow((reconstructed_img_8[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5)\n",
                "    f.set_figheight(20)\n",
                "    f.set_figwidth(20)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "plt.savefig('results.png')\n",
                "f.savefig('results.png')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "f, axarr = plt.subplots(2,2)\n",
                "\n",
                "axarr[0,0].title.set_text('Original \\n Image')\n",
                "axarr[0,1].title.set_text('Reconstructed Image with \\n 43% Compression')\n",
                "axarr[1,0].title.set_text('Reconstructed Image with \\n 68% Compression')\n",
                "axarr[1,1].title.set_text('Reconstructed Image with \\n 84% Compression')\n",
                "\n",
                "for i in range(2):\n",
                "    for j in range(2):\n",
                "        axarr[i,j].title.set_fontsize(40)\n",
                "i = 0\n",
                "\n",
                "\n",
                "reimg = (valid_batch_1[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5\n",
                "reimg_28 = (reconstructed_img_28_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\n",
                "reimg_16 = (reconstructed_img_16_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\n",
                "reimg_8 = (reconstructed_img_8_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\n",
                "\n",
                "\n",
                "\n",
                "axarr[0,0].imshow(reimg)\n",
                "axarr[0,1].imshow(reimg_28)\n",
                "axarr[1,0].imshow(reimg_16)\n",
                "axarr[1,1].imshow(reimg_8)\n",
                "f.set_figheight(50)\n",
                "f.set_figwidth(50)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "plt.savefig('results1.png')\n",
                "f.savefig('results1.png')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "reimg = reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2])\n",
                "reimg_28 = reimg_28.view(1,reimg_28.shape[0],reimg_28.shape[1],reimg_28.shape[2])\n",
                "reimg_16 = reimg_16.view(1,reimg_16.shape[0],reimg_16.shape[1],reimg_16.shape[2])\n",
                "reimg_8 = reimg_8.view(1,reimg_8.shape[0],reimg_8.shape[1],reimg_8.shape[2])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "reimg = int(reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2]) * 256)\n",
                "reimg_28 = int(reimg_28.view(1,reimg_28.shape[0],reimg_28.shape[1],reimg_28.shape[2]) *256)\n",
                "reimg_16 = int(reimg_16.view(1,reimg_16.shape[0],reimg_16.shape[1],reimg_16.shape[2]) *256)\n",
                "reimg_8 = int(reimg_8.view(1,reimg_8.shape[0],reimg_8.shape[1],reimg_8.shape[2]) *256)"
            ]
        },
        {
            "tags": [
                "check_results",
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "# import torch\n",
                "# from torch.autograd import Variable\n",
                "\n",
                "# img1 = Variable(torch.rand(1, 1, 256, 256))\n",
                "# img2 = Variable(torch.rand(1, 1, 256, 256))\n",
                "\n",
                "# if torch.cuda.is_available():\n",
                "#     img1 = img1.cuda()\n",
                "#     img2 = img2.cuda()\n",
                "\n",
                "\n",
                "# reimg_28 = reimg_28.view(1,reimg_28.shape[0],reimg_28.shape[1],reimg_28.shape[2])\n",
                "# reimg = reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2])\n",
                "# reimg = reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2])\n",
                "print(ssim(reimg, reimg_28))\n",
                "print(ssim(reimg, reimg_16))\n",
                "print(ssim(reimg, reimg_8))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "reimg.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "%matplotlib inline\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib\n",
                "from sklearn import metrics\n",
                "from sklearn.neighbors import NearestNeighbors\n",
                "\n",
                "from keras.models import Sequential, Model\n",
                "from keras.layers import Dense, Dropout, Convolution2D, MaxPooling2D, Flatten, Input\n",
                "from keras.optimizers import adam\n",
                "from keras.utils.np_utils import to_categorical\n",
                "\n",
                "import seaborn as sns\n",
                "\n",
                "%config InlineBackend.figure_format = 'retina'"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train = pd.read_csv(\"../input/train.csv\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train = train.iloc[:,1:].values\n",
                "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1) #reshape to rectangular\n",
                "X_train = X_train/255 #pixel values are 0 - 255 - this makes puts them in the range 0 - 1\n",
                "\n",
                "y_train = train[\"label\"].values"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "y_ohe = to_categorical(y_train)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model = Sequential()\n",
                "\n",
                "model.add(Convolution2D(20, 5, 5, input_shape = (28, 28, 1), activation=\"relu\"))\n",
                "model.add(MaxPooling2D(pool_size=(2,2)))\n",
                "model.add(Dropout(0.5))\n",
                "\n",
                "model.add(Convolution2D(40, 5, 5, activation=\"relu\"))\n",
                "model.add(MaxPooling2D(pool_size=(2,2)))\n",
                "model.add(Dropout(0.5))\n",
                "\n",
                "model.add(Flatten())\n",
                "model.add(Dense(100, activation = \"relu\"))\n",
                "model.add(Dropout(0.5))\n",
                "model.add(Dense(100, activation = \"relu\"))\n",
                "model.add(Dense(10, activation=\"softmax\"))\n",
                "\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model.compile(loss='categorical_crossentropy', \n",
                "              optimizer = adam(lr=0.001), metrics = [\"accuracy\"])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "hist = model.fit(X_train, y_ohe,\n",
                "          validation_split = 0.05, batch_size = 128, epochs = 8)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "model.save_weights(\"model.h5\")"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "model.load_weights(\"model.h5\")"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "loss_and_metrics = model.evaluate(X_train, y_ohe, batch_size=128)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "#getting the 2D output:\n",
                "output = model.get_layer(\"dense_3\").output\n",
                "extr = Model(model.input, output)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n",
                "\n",
                "X_proj = extr.predict(X_train[:10000])\n",
                "X_proj.shape\n",
                "\n",
                "proj = pd.DataFrame(X_proj[:,:2])\n",
                "proj.columns = [\"comp_1\", \"comp_2\"]\n",
                "proj[\"labels\"] = y_train[:10000]\n",
                "\n",
                "sns.lmplot(\"comp_1\", \"comp_2\",hue = \"labels\", data = proj, fit_reg=False)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "test = pd.read_csv(\"../input/test.csv\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "test = test.iloc[:,:].values\n",
                "test = test.reshape(test.shape[0], 28, 28, 1)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "process_data"
            ],
            "source": [
                "pred = model.predict_classes(test,verbose=0)\n",
                "\n",
                "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(pred)+1)),\n",
                "                         \"Label\": pred})"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submissions.to_csv(\"DR.csv\", index=False, header=True)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# data analysis and wrangling\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import random as rnd\n",
                "# machine learning\n",
                "from sklearn import tree\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn import metrics\n",
                "# visualization\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#read in\n",
                "pd_data = pd.read_csv('../input/weatherAUS.csv')\n",
                "pd_data.shape"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#drop NAN\n",
                "pd_data=pd_data.dropna(how='any')\n",
                "print(pd_data.shape)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#drop something column\n",
                "drop_columns_list = ['WindGustDir', 'WindDir9am', 'WindDir3pm','Date','Location','RISK_MM']\n",
                "pd_data = pd_data.drop(drop_columns_list, axis=1)\n",
                "print(pd_data.shape)\n",
                "pd_data.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#change yes/no to 1/0\n",
                "pd_data['RainToday'].replace({'No':0,'Yes':1},inplace=True)\n",
                "pd_data['RainTomorrow'].replace({'No':0,'Yes':1},inplace=True)\n",
                "pd_data.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#Task: Split the data into train and test\n",
                "train_y = pd_data['RainTomorrow'].head(55000)\n",
                "test_y= pd_data['RainTomorrow'].tail(1420)\n",
                "train_x = pd_data.head(55000).drop(['RainTomorrow'], axis=1)\n",
                "test_x= pd_data.tail(1420).drop(['RainTomorrow'], axis=1)\n",
                "print(train_y.head())\n",
                "print(train_x.head())"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data",
                "setup_notebook"
            ],
            "source": [
                "import graphviz \n",
                "dtree=tree.DecisionTreeClassifier(max_depth=3)\n",
                "dtree=dtree.fit(train_x,train_y)\n",
                "dot_data = tree.export_graphviz(dtree, \n",
                "                filled=True, \n",
                "                feature_names=list(train_x),\n",
                "                class_names=['No rain','rain'],\n",
                "                special_characters=True)\n",
                "graph = graphviz.Source(dot_data)  \n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "graph"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#\n",
                "dtree.feature_importances_\n"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "#\n",
                "predict_y = dtree.predict(test_x)\n",
                "predict_y"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "#\n",
                "from sklearn.metrics import accuracy_score\n",
                "acc_log = dtree.score(train_x, train_y)\n",
                "print('training accuracy: %.5f' % acc_log)\n",
                "x=accuracy_score(test_y, predict_y)\n",
                "print('test accuracy: %.5f' % x)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "#test\n",
                "#\n",
                "#for i in range(400,601,5):    \n",
                "    \"\"\"dtree=tree.DecisionTreeClassifier(min_samples_split=1000,min_samples_leaf =570)\n",
                "    dtree=dtree.fit(train_x,train_y)\n",
                "    predict_y = dtree.predict(test_x)\n",
                "    x=accuracy_score(test_y, predict_y)\n",
                "    print('%d' % i,'test accuracy: %.5f'  %x)\"\"\""
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "#auc\n",
                "fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n",
                "print('max_depth=3 auc: %.5f' % metrics.auc(fpr, tpr))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "tree_train_acc=[]   \n",
                "tree_test_acc=[]    \n",
                "tree_depth=[]       \n",
                "\n",
                "for i in range (2,20):\n",
                "    dtree=tree.DecisionTreeClassifier(max_depth=i)\n",
                "    dtree=dtree.fit(train_x,train_y)\n",
                "    acc_log = dtree.score(train_x, train_y)\n",
                "    print('max_depth=%d ' % i,'training accuracy: %.5f' % acc_log)\n",
                "    \n",
                "    predict_y = dtree.predict(test_x)    \n",
                "    X=accuracy_score(test_y, predict_y)\n",
                "    print('\\t\\ttest accuracy: %.5f' % X)\n",
                "    \n",
                "    tree_train_acc.append(acc_log)\n",
                "    tree_test_acc.append(X)\n",
                "    tree_depth.append(i)\n",
                "    "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.plot(tree_depth,tree_train_acc,'b', label=\"training accuracy\")\n",
                "plt.plot(tree_depth,tree_test_acc,'r', label=\"test accuracy\")\n",
                "plt.ylabel('accuracy (%)')\n",
                "plt.xlabel('max depth ')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "\n",
                "best_depth = tree_depth[tree_test_acc.index(max(tree_test_acc))]\n",
                "print (\"max depth: \", best_depth)\n",
                "print (\"best test accuracy: %.5f\"% max(tree_test_acc))"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n",
                "print('max_depth=7 auc: %.5f' % metrics.auc(fpr, tpr))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "#  cross validation \n",
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.model_selection import train_test_split\n",
                "scores = cross_val_score(dtree,train_x,train_y,cv=5,scoring='accuracy')\n",
                "\n",
                "# \n",
                "print('average of Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "# logistic regression\n",
                "\n",
                "logreg = LogisticRegression()\n",
                "logreg = logreg.fit(train_x, train_y)\n",
                "predict_y = logreg.predict(test_x)\n",
                "acc_log = logreg.score(train_x, train_y)\n",
                "print('training accuracy: %.5f' % acc_log)\n",
                "\n",
                "predict_y =logreg.predict(test_x)\n",
                "X=accuracy_score(test_y, predict_y)\n",
                "print('test accuracy: %.5f' % X)\n",
                "\n",
                "#auc\n",
                "fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n",
                "print('auc: %.5f' % metrics.auc(fpr, tpr))\n",
                "\n",
                "#Cross validation\n",
                "scores = cross_val_score(logreg,train_x,train_y,cv=5,scoring='accuracy')\n",
                "# Cross validation\n",
                "print('average of Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model",
                "check_results"
            ],
            "source": [
                "# Support Vector Machines\n",
                "#\n",
                "'''\n",
                "svc = SVC(gamma='auto',C=0.1,kernel=\"linear\", probability=True)\n",
                "svc.fit(train_x, train_y)\n",
                "predict_y= svc.predict(test_x)\n",
                "acc_svc = svc.score(train_x, train_y)\n",
                "print('training accuracy: %.5f' % acc_svc)\n",
                "\n",
                "predict_y =svc.predict(test_x)\n",
                "X=accuracy_score(test_y, predict_y)\n",
                "print('test accuracy: %.5f' % X)'''"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model",
                "check_results"
            ],
            "source": [
                "# knn\n",
                "\n",
                "knn = KNeighborsClassifier(n_neighbors = 10)\n",
                "knn.fit(train_x, train_y)\n",
                "predict_y = knn.predict(test_x)\n",
                "acc_knn = knn.score(train_x, train_y)\n",
                "print('training accuracy: %.5f' % acc_knn)\n",
                "\n",
                "predict_y =knn.predict(test_x)\n",
                "X=accuracy_score(test_y, predict_y)\n",
                "print('test accuracy: %.5f' % X)\n",
                "\n",
                "#auc\n",
                "fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n",
                "print('auc: %.5f' % metrics.auc(fpr, tpr))\n",
                "\n",
                "#Cross validation\n",
                "scores = cross_val_score(knn,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print('average of Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "# Gaussian Naive Bayes\n",
                "\n",
                "gaussian = GaussianNB()\n",
                "gaussian.fit(train_x, train_y)\n",
                "predict_y = gaussian.predict(test_x)\n",
                "acc_gaussian = gaussian.score(train_x, train_y)\n",
                "print('training accuracy: %.5f' % acc_gaussian)\n",
                "\n",
                "predict_y =gaussian.predict(test_x)\n",
                "X=accuracy_score(test_y, predict_y)\n",
                "print('test accuracy: %.5f' % X)\n",
                "\n",
                "#auc\n",
                "fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n",
                "print('auc: %.5f' % metrics.auc(fpr, tpr))\n",
                "\n",
                "#Cross validation\n",
                "scores = cross_val_score(gaussian,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print('average of Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "rdf = RandomForestClassifier(bootstrap=True, n_estimators=1000, max_depth=7)\n",
                "rdf.fit(train_x, train_y)  "
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "acc_log = rdf.score(train_x, train_y)\n",
                "print('training accuracy: %.5f' % acc_log)\n",
                "\n",
                "predict_y =rdf.predict(test_x)\n",
                "X=accuracy_score(test_y, predict_y)\n",
                "print('test accuracy: %.5f' % X)\n",
                "#auc\n",
                "fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n",
                "print('auc: %.5f' % metrics.auc(fpr, tpr))\n",
                "\n",
                "#Cross validation\n",
                "#\n",
                "'''scores = cross_val_score(rdf,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print(scores)\n",
                "print('Cross validation: %.5f'%scores.mean())'''"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "#Parameters:n_estimators\n",
                "#n_estimators=256\n",
                "\n",
                "from sklearn import model_selection, metrics\n",
                "\n",
                "def scorer(model, X,  train_y):\n",
                "    preds = model.predict(X)\n",
                "    return metrics.accuracy_score( train_y, preds)\n",
                "\n",
                "n_estimators = [1,2,4,8,16,32,64,128, 256]  ## try different n_estimators\n",
                "cv_results = []\n",
                "\n",
                "for estimator in n_estimators:\n",
                "    rf = RandomForestClassifier(n_estimators=estimator)\n",
                "    acc = model_selection.cross_val_score(rf, train_x,  train_y, cv=5, scoring=scorer)\n",
                "    cv_results.append(acc.mean())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "line1= plt.plot(n_estimators, cv_results, 'b', label=\"cross validated accuracy\")\n",
                "plt.ylabel('accuracy')\n",
                "plt.xlabel('n_estimators')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "best_n_estimators = n_estimators[cv_results.index(max(cv_results))]\n",
                "print (\"best_n_estimators: \", best_n_estimators)\n",
                "print (\"best accuracy: \", max(cv_results))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results",
                "process_data"
            ],
            "source": [
                "pd_data = pd.read_csv('../input/weatherAUS.csv')\n",
                "pd_data=pd_data.dropna(how='any')\n",
                "print(pd_data.shape)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "drop_columns_list = ['WindGustDir', 'WindDir9am', 'WindDir3pm','Date','Sunshine','RISK_MM']\n",
                "pd_data = pd_data.drop(drop_columns_list, axis=1)\n",
                "print(pd_data.shape)\n",
                "\n",
                "pd_data['RainToday'].replace({'No':0,'Yes':1},inplace=True)\n",
                "pd_data['RainTomorrow'].replace({'No':0,'Yes':1},inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "groupbyLocation=pd_data.groupby('Location')\n",
                "print(groupbyLocation.size().sort_values(ascending=False))\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "pd_data['Location'] = pd_data['Location'].map( {'Darwin':0,'Perth':1,'Brisbane':2,'MelbourneAirport':3,\n",
                "                                                'PerthAirport':4,'SydneyAirport':5,'Watsonia':6,'Mildura':7,\n",
                "                                                'MountGambier':8,'NorfolkIsland':9,'Cairns':10,'Townsville':11,\n",
                "                                                'WaggaWagga':12,'AliceSprings':13,'Nuriootpa':14,'Hobart':15,\n",
                "                                                'Moree':16,'Melbourne':17,'Portland':18,'Woomera':19,\n",
                "                                                'Sydney':20,'Sale':21,'CoffsHarbour':22,'Williamtown':23,\n",
                "                                                'Canberra':24,'Cobar':25} ).astype(int)\n",
                "train_y=pd_data['RainTomorrow']\n",
                "train_x=pd_data.drop(['RainTomorrow'], axis=1)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "dtree=tree.DecisionTreeClassifier(max_depth=7)\n",
                "dtree=dtree.fit(train_x,train_y)\n",
                "scores = cross_val_score(dtree,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print(scores)\n",
                "print('average of Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "#Logistic Regression\n",
                "logreg = LogisticRegression()\n",
                "logreg.fit(train_x, train_y)\n",
                "\n",
                "#Cross validation\n",
                "scores = cross_val_score(dtree,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print('Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "#knn\n",
                "knn = KNeighborsClassifier(n_neighbors = 10)\n",
                "knn.fit(train_x, train_y)\n",
                "\n",
                "#Cross validation\n",
                "scores = cross_val_score(knn,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print('Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "#Gaussian Naive Bayes\n",
                "gaussian = GaussianNB()\n",
                "gaussian.fit(train_x, train_y)\n",
                "#Cross validation\n",
                "scores = cross_val_score(gaussian,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print('Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# fashion-mnist_test.csv has 10000 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df1 = pd.read_csv('../input/fashion-mnist_test.csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'fashion-mnist_test.csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df1.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotCorrelationMatrix(df1, 196)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotScatterMatrix(df1, 20, 10)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# fashion-mnist_train.csv has 60000 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df2 = pd.read_csv('../input/fashion-mnist_train.csv', delimiter=',', nrows = nRowsRead)\n",
                "df2.dataframeName = 'fashion-mnist_train.csv'\n",
                "nRow, nCol = df2.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df2.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotPerColumnDistribution(df2, 10, 5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotCorrelationMatrix(df2, 196)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotScatterMatrix(df2, 20, 10)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# fashion-mnist_test.csv has 10000 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df1 = pd.read_csv('../input/fashion-mnist_test.csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'fashion-mnist_test.csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df1.head(5)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# fashion-mnist_train.csv has 60000 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df2 = pd.read_csv('../input/fashion-mnist_train.csv', delimiter=',', nrows = nRowsRead)\n",
                "df2.dataframeName = 'fashion-mnist_train.csv'\n",
                "nRow, nCol = df2.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "pip install xmltodict"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "cd ../input/european-soccer-csv-files/"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "#Essential Imports\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import xmltodict\n",
                "import collections"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "#Data comes in a sqlite format, and can be exported in csv format using DB browser (https://sqlitebrowser.org/)\n",
                "\n",
                "#reading data csv files\n",
                "country_df = pd.read_csv('Country.csv')\n",
                "league_df = pd.read_csv('League.csv')\n",
                "match_df = pd.read_csv('Match.csv')\n",
                "player_df = pd.read_csv('Player.csv')\n",
                "player_attr_df = pd.read_csv('Player_Attributes.csv')\n",
                "team_df = pd.read_csv('Team.csv')\n",
                "team_attr_df = pd.read_csv('Team_Attributes.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "country_df.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "type(country_df.name.iloc[0])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "country_df"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "league_df.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "type(league_df.name.iloc[0])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "league_df.head(5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#country_id column drop\n",
                "league_df.drop('country_id', axis= 1, inplace= True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#country df deletion\n",
                "del country_df"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "match_df.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#columns names\n",
                "for i, col in enumerate(match_df.columns):\n",
                "    print(i, col)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "(match_df.country_id == match_df.league_id).all()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#confirmation step\n",
                "cols = np.r_[1, 6, 11:55]\n",
                "match_df.drop(match_df.columns[cols],axis=1, inplace=True)\n",
                "match_df.columns"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#columns names\n",
                "for i, col in enumerate(match_df.columns):\n",
                "    print(i, col)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cols1 = np.r_[0:9, 31:39]\n",
                "match_stats_df = match_df.iloc[:, cols1].reset_index().drop('index', axis= 1)\n",
                "cols2 = np.r_[0, 39:69]\n",
                "match_bets_df = match_df.iloc[:, cols2].reset_index().drop('index', axis= 1)\n",
                "cols3 = np.r_[0, 9:31]\n",
                "match_lineup_df = match_df.iloc[:, cols3].reset_index().drop('index', axis= 1)\n",
                "del match_df"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#change date from string to datetime\n",
                "match_stats_df.date = pd.to_datetime(match_stats_df.date)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#inspecting goal values\n",
                "match_stats_df.goal.unique()[1]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#inspecting possession values\n",
                "match_stats_df.possession.unique()[1]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def parse_goal(goal, home_id, away_id):\n",
                "    '''\n",
                "    The function parses the goal values which is xml text into more convenient tuble\n",
                "    Args:\n",
                "        goal -> xml text with multiple tags and goal info\n",
                "        home_id -> the id of the home team of the match that goal was scored in\n",
                "        away_id -> the id of the away team\n",
                "    Returns:\n",
                "        a tuble of two lists: the first one is the home goals list and the second is the away goals list.\n",
                "        each list consists of a number of tubles correspond to each goal.\n",
                "        tuble format: (time of the goal in mins-int, scorer id-int, assisstant id-int, goal type-string)\n",
                "    '''\n",
                "    if pd.notna(goal):\n",
                "        if xmltodict.parse(goal)['goal'] != None:\n",
                "            goal_dict = xmltodict.parse(goal)['goal']['value']\n",
                "            home_goals = list()\n",
                "            away_goals = list()\n",
                "            if type(goal_dict) == collections.OrderedDict:\n",
                "                goal_dict = [goal_dict]\n",
                "            for g in goal_dict:\n",
                "                try:\n",
                "                    p1 = int(g['player1'])\n",
                "                except:\n",
                "                    p1 = 0\n",
                "                try:\n",
                "                    p2 = int(g['player2'])\n",
                "                except:\n",
                "                    p2 = 0                \n",
                "                g_info = (int(g['elapsed']),p1, p2, g['comment'])\n",
                "                if 'del' not in g.keys():\n",
                "                    if int(g['team']) == home_id:\n",
                "                        home_goals.append(g_info)\n",
                "                    else:\n",
                "                        away_goals.append(g_info)\n",
                "            return home_goals, away_goals"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_stats_df[\"goals_info\"] = match_stats_df.apply(lambda row: parse_goal(row.goal, row.home_team_api_id, row.away_team_api_id), axis= 1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "odd_goals = match_stats_df[match_stats_df.goals_info.isnull() == False]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#confirmation step\n",
                "odd_goals[odd_goals.home_team_goal+odd_goals.away_team_goal != odd_goals.goals_info.apply(lambda x: len(x[0])+len(x[1]))][['goals_info', 'home_team_goal', 'away_team_goal']].head(10)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_stats_df['poss_info'] = match_stats_df.possession.apply(lambda val: parse_poss(val))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "match_stats_df[match_stats_df.goals_info.isnull() == False]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_stats_df.drop(['goal', 'shoton', 'shotoff', 'foulcommit', 'card', 'cross', 'corner', 'possession'], axis=1, inplace= True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_lineup_df.dropna(thresh=20, inplace= True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_lineup_df.reset_index(inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_lineup_df.drop('index', axis=1, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#max number of missing values per lineup\n",
                "match_lineup_df.isnull().sum(axis= 1).max()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_bets_df.dropna(subset= list(match_bets_df.columns).remove('id'), inplace= True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_bets_df.reset_index(inplace= True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_bets_df.drop('index', inplace= True, axis= 1)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "cols = list(match_bets_df.columns)\n",
                "cols.remove('id')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for c in cols:\n",
                "    match_bets_df[c] = match_bets_df[c].apply(lambda x: 1/x)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "match_bets_df"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "player_df.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(type(player_df.player_name[0]))\n",
                "print(type(player_df.birthday[0]))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "player_df.birthday = pd.to_datetime(player_df.birthday)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "player_attr_df.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "player_attr_df.date = pd.to_datetime(player_attr_df.date)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "player_attr_df.describe()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "old_mean = player_attr_df.describe().loc['mean']\n",
                "old_std = player_attr_df.describe().loc['std']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for i in zip(player_attr_df.columns, player_attr_df.dtypes):\n",
                "    if i[1] == 'float64':\n",
                "        player_attr_df[i[0]] = player_attr_df[i[0]].fillna(player_attr_df[i[0]].mean())"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "player_attr_df.isnull().sum()"
            ]
        }
    ]
}