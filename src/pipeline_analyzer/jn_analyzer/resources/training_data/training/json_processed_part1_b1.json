{
    "content": [
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import os\n",
                "print(os.listdir('../input/sasuke'))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "visualize_data"
            ],
            "source": [
                "img = Image.open('../input/sasuke/.jpg')\n",
                "img = img.resize((224,224))\n",
                "plt.imshow(img)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "check_results"
            ],
            "source": [
                "import numpy as np\n",
                "test_x = np.array(img) / 255.0\n",
                "print(test_x.shape)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "test_x = test_x.reshape(1,224,224,3)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "# Import Model\n",
                "#from tensorflow.keras.applications import VGG16\n",
                "#from tensorflow.keras.applications import ResNet101V2\n",
                "from tensorflow.keras.applications import InceptionV3\n",
                "\n",
                "#from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
                "#from tensorflow.keras.applications.resnet import preprocess_input, decode_predictions\n",
                "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
                "\n",
                "# Load Model\n",
                "#model = VGG16(weights='imagenet')\n",
                "#model = ResNet101V2(weights='imagenet')\n",
                "model = InceptionV3(weights='imagenet')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "# model prediction\n",
                "preds = model.predict(test_x)\n",
                "# decode prediction\n",
                "dec_preds =  decode_predictions(preds, top=3)[0]\n",
                "print('Predicted:', dec_preds)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import os\n",
                "print(os.listdir('../input/naruto'))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "visualize_data",
                "process_data"
            ],
            "source": [
                "img = Image.open('../input/naruto/naruto.jpg')\n",
                "img = img.resize((224,224))\n",
                "plt.imshow(img)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "check_results"
            ],
            "source": [
                "import numpy as np\n",
                "test_x = np.array(img) / 255.0\n",
                "print(test_x.shape)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "test_x = test_x.reshape(1,224,224,3)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "# Import Model\n",
                "#from tensorflow.keras.applications import VGG16\n",
                "#from tensorflow.keras.applications import ResNet101V2\n",
                "from tensorflow.keras.applications import InceptionV3\n",
                "\n",
                "#from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
                "#from tensorflow.keras.applications.resnet import preprocess_input, decode_predictions\n",
                "from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n",
                "\n",
                "# Load Model\n",
                "#model = VGG16(weights='imagenet')\n",
                "#model = ResNet101V2(weights='imagenet')\n",
                "model = InceptionV3(weights='imagenet')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "# model prediction\n",
                "preds = model.predict(test_x)\n",
                "# decode prediction\n",
                "dec_preds =  decode_predictions(preds, top=3)[0]\n",
                "print('Predicted:', dec_preds)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import geopandas as gpd\n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import seaborn as sns\n",
                "import folium\n",
                "from folium import plugins"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/new-york-city-airbnb-open-data/')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data=pd.read_csv('../input/new-york-city-airbnb-open-data/AB_NYC_2019.csv')\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data=data.dropna(subset=['name'])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.neighbourhood_group.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(20,6))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "data.neighbourhood_group.value_counts().plot.bar(color=clr,ax=ax[0])\n",
                "ax[0].set_title('The number of rooms in each neighbourhood_group',size=20)\n",
                "ax[0].set_ylabel('rooms',size=18)\n",
                "ax[0].tick_params(axis='x', rotation=360)\n",
                "ax[0].tick_params(labelsize=18)\n",
                "\n",
                "data.groupby(['neighbourhood_group','room_type'])['id'].agg('count').unstack('room_type').plot.bar(ax=ax[1])\n",
                "ax[1].tick_params(axis='x', rotation=360)\n",
                "ax[1].set_title('The number of rooms in each room_type',size=20)\n",
                "ax[1].set_ylabel('rooms',size=18)\n",
                "ax[1].set_xlabel('')\n",
                "ax[1].tick_params(labelsize=18)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.room_type.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(3,1,figsize=(15,36))\n",
                "sns.boxplot(x=\"neighbourhood_group\", y=\"price\", data=data[data.room_type=='Shared room'],ax=ax[0])\n",
                "ax[0].set_title(\"Boxplot of Price for 'Shared room' in each neighbourhood_group\",size=20)\n",
                "\n",
                "sns.boxplot(x=\"neighbourhood_group\", y=\"price\", data=data[data.room_type=='Private room'],ax=ax[1])\n",
                "ax[1].set_title(\"Boxplot of Price for 'Private room' in each neighbourhood_group\",size=20)\n",
                "\n",
                "sns.boxplot(x=\"neighbourhood_group\", y=\"price\", data=data[data.room_type=='Entire home/apt'],ax=ax[2])\n",
                "ax[2].set_title(\"Boxplot of Price for 'Entire home/apt' in each neighbourhood_group\",size=20)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_manha=data[data.neighbourhood_group=='Manhattan']\n",
                "data_manha.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_manha.neighbourhood.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "data_manha.neighbourhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 neighbourhood by the number of rooms\",size=20)\n",
                "ax[0].set_xlabel('rooms',size=18)\n",
                "\n",
                "\n",
                "count=data_manha['neighbourhood'].value_counts()\n",
                "groups=list(data_manha['neighbourhood'].value_counts().index)[:10]\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.15,0.4)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data_manha_65=data_manha[data_manha.price<65]\n",
                "data_manha_65['label']=data_manha_65.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n",
                "data_manha_65.head()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "Long=-73.92\n",
                "Lat=40.86\n",
                "manha_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "manha_rooms_map=plugins.MarkerCluster().add_to(manha_map)\n",
                "for lat,lon,label in zip(data_manha_65.latitude,data_manha_65.longitude,data_manha_65.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(manha_rooms_map)\n",
                "manha_map.add_child(manha_rooms_map)\n",
                "\n",
                "manha_map"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "data_manha_65_80=data_manha.loc[(data_manha['price'] >=65) & (data_manha['price'] <80)]\n",
                "data_manha_65_80['label']=data_manha_65_80.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n",
                "Long=-73.92\n",
                "Lat=40.86\n",
                "manha_65_80_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "manha_65_80_rooms_map=plugins.MarkerCluster().add_to(manha_65_80_map)\n",
                "for lat,lon,label in zip(data_manha_65_80.latitude,data_manha_65_80.longitude,data_manha_65_80.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(manha_65_80_rooms_map)\n",
                "manha_65_80_map.add_child(manha_65_80_rooms_map)\n",
                "\n",
                "manha_65_80_map"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_Brooklyn=data[data.neighbourhood_group=='Brooklyn']\n",
                "data_Brooklyn.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_Brooklyn.neighbourhood.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "data_Brooklyn.neighbourhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 neighbourhood by the number of rooms\",size=20)\n",
                "ax[0].set_xlabel('rooms',size=18)\n",
                "\n",
                "\n",
                "count=data_Brooklyn['neighbourhood'].value_counts()\n",
                "groups=list(data_Brooklyn['neighbourhood'].value_counts().index)[:10]\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.15,0.4)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data_Brooklyn_10_65=data_Brooklyn.loc[(data_Brooklyn['price'] >=10) & (data_Brooklyn['price'] <65)][:2000]\n",
                "data_Brooklyn_10_65['label']=data_Brooklyn_10_65.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n",
                "data_Brooklyn_10_65.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "Long=-73.94\n",
                "Lat=40.72\n",
                "Brooklyn_10_65_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "Brooklyn_10_65_rooms_map=plugins.MarkerCluster().add_to(Brooklyn_10_65_map)\n",
                "for lat,lon,label in zip(data_Brooklyn_10_65.latitude,data_Brooklyn_10_65.longitude,data_Brooklyn_10_65.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(Brooklyn_10_65_rooms_map)\n",
                "Brooklyn_10_65_map.add_child(Brooklyn_10_65_rooms_map)\n",
                "\n",
                "Brooklyn_10_65_map"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_Queens=data[data.neighbourhood_group=='Queens']\n",
                "data_Queens.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_Queens.neighbourhood.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "data_Queens.neighbourhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 neighbourhood by the number of rooms\",size=20)\n",
                "ax[0].set_xlabel('rooms',size=18)\n",
                "\n",
                "\n",
                "count=data_Queens['neighbourhood'].value_counts()\n",
                "groups=list(data_Queens['neighbourhood'].value_counts().index)[:10]\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.15,0.4)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data_Queens_100_1=data_Queens.loc[(data_Queens['price'] <100)][:2000]\n",
                "data_Queens_100_1['label']=data_Queens_100_1.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n",
                "data_Queens_100_1.head()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "Long=-73.80\n",
                "Lat=40.70\n",
                "data_Queens_100_1_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "data_Queens_100_1_rooms_map=plugins.MarkerCluster().add_to(data_Queens_100_1_map)\n",
                "for lat,lon,label in zip(data_Queens_100_1.latitude,data_Queens_100_1.longitude,data_Queens_100_1.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_Queens_100_1_rooms_map)\n",
                "data_Queens_100_1_map.add_child(data_Queens_100_1_rooms_map)\n",
                "\n",
                "data_Queens_100_1_map"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "data_Queens_100_2=data_Queens.loc[(data_Queens['price'] <100)][2000:2800]\n",
                "data_Queens_100_2['label']=data_Queens_100_2.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n",
                "Long=-73.80\n",
                "Lat=40.70\n",
                "data_Queens_100_2_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "data_Queens_100_2_rooms_map=plugins.MarkerCluster().add_to(data_Queens_100_2_map)\n",
                "for lat,lon,label in zip(data_Queens_100_2.latitude,data_Queens_100_2.longitude,data_Queens_100_2.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_Queens_100_2_rooms_map)\n",
                "data_Queens_100_2_map.add_child(data_Queens_100_2_rooms_map)\n",
                "\n",
                "data_Queens_100_2_map"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "data_Queens_100_3=data_Queens.loc[(data_Queens['price'] <100)][2801:-1]\n",
                "data_Queens_100_3['label']=data_Queens_100_3.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n",
                "Long=-73.80\n",
                "Lat=40.70\n",
                "data_Queens_100_3_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "data_Queens_100_3_rooms_map=plugins.MarkerCluster().add_to(data_Queens_100_3_map)\n",
                "for lat,lon,label in zip(data_Queens_100_3.latitude,data_Queens_100_3.longitude,data_Queens_100_3.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_Queens_100_3_rooms_map)\n",
                "data_Queens_100_3_map.add_child(data_Queens_100_3_rooms_map)\n",
                "\n",
                "data_Queens_100_3_map"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(18,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "data.groupby(['host_name'])['number_of_reviews'].agg('sum').sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 host by the number of reviews\",size=20)\n",
                "ax[0].set_xlabel('reviews',size=18)\n",
                "ax[0].set_ylabel('')\n",
                "\n",
                "data.groupby(['host_name'])['price'].agg('mean').sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[1])\n",
                "ax[1].set_title(\"Top 10 host by the average of price for rooms\",size=20)\n",
                "ax[1].set_xlabel('average of price',size=18)\n",
                "ax[1].set_ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,15))\n",
                "corr = data.corr()\n",
                "sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns)\n",
                "plt.title(\"correlation plot\",size=28)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "data_tree=data[['neighbourhood_group','neighbourhood','room_type','minimum_nights','number_of_reviews','price']]\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "labelencoder = LabelEncoder()\n",
                "data_tree['neighbourhood_group_new'] = labelencoder.fit_transform(data_tree['neighbourhood_group'])\n",
                "data_tree['neighbourhood_new'] = labelencoder.fit_transform(data_tree['neighbourhood'])\n",
                "data_tree['room_type_new'] = labelencoder.fit_transform(data_tree['room_type'])\n",
                "data_tree.head()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_tree=data_tree[data_tree.price<=180]\n",
                "data_tree=data_tree[data_tree.price>=90]\n",
                "len(data_tree)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "x_train,x_test,y_train,y_test=train_test_split(data_tree[['neighbourhood_group_new','neighbourhood_new','room_type_new','minimum_nights','number_of_reviews']],data_tree[['price']],test_size=0.1,random_state=0)\n",
                "Reg_tree=DecisionTreeRegressor(criterion='mse',max_depth=3,random_state=0)\n",
                "Reg_tree=Reg_tree.fit(x_train,y_train)\n",
                "y=y_test['price']\n",
                "predict=Reg_tree.predict(x_test)\n",
                "print(\"median absolute deviation (MAD): \",np.mean(abs(np.multiply(np.array(y_test.T-predict),np.array(1/y_test)))))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "transfer_results",
                "ingest_data"
            ],
            "source": [
                "from subprocess import check_call\n",
                "from PIL import Image, ImageDraw, ImageFont\n",
                "from IPython.display import Image as PImage\n",
                "from sklearn.tree import export_graphviz\n",
                "with open(\"tree1.dot\", 'w') as f:\n",
                "     f = export_graphviz(Reg_tree,\n",
                "                              out_file=f,\n",
                "                              max_depth = 3,\n",
                "                              impurity = True,\n",
                "                              feature_names = ['neighbourhood_group_new','neighbourhood_new','room_type_new','minimum_nights','number_of_reviews'],\n",
                "                              rounded = True,\n",
                "                              filled= True )\n",
                "check_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n",
                "img = Image.open(\"tree1.png\")\n",
                "draw = ImageDraw.Draw(img)\n",
                "img.save('sample-out.png')\n",
                "PImage(\"sample-out.png\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os \n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "import folium\n",
                "import datetime\n",
                "import plotly.graph_objs as go\n",
                "import matplotlib.ticker as ticker\n",
                "import matplotlib.animation as animation\n",
                "from IPython.display import HTML"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/novel-corona-virus-2019-dataset')"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data=pd.read_csv('../input/novel-corona-virus-2019-dataset/covid_19_data.csv')\n",
                "data['date']=data.ObservationDate.apply(lambda x:datetime.datetime.strptime(str(x),'%m/%d/%Y').strftime('%Y-%m-%d'))\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data2=pd.read_csv('../input/novel-corona-virus-2019-dataset/COVID19_open_line_list.csv')\n",
                "data2.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "source": [
                "data2=data2.dropna(subset=['wuhan(0)_not_wuhan(1)','latitude'])\n",
                "data2=data2.fillna(value = {'sex' : 'Unknown', 'age' : 'Unknown'})\n",
                "data2=data2.drop(index=data2[data2.sex=='4000'].index)\n",
                "data2=data2.replace(to_replace = 'Female', value ='female')\n",
                "data2=data2.replace(to_replace = 'Male', value ='male')\n",
                "data2['label']=data2.apply(lambda x: ('age:'+str(x['age']),'sex:'+str(x['sex']),'geo_resolution:'+str(x['geo_resolution']),'Confirmed_date:'+str(x['date_confirmation'])),axis=1)\n",
                "plt.figure(figsize=(10,10))\n",
                "sns.barplot(x=data2.isnull().sum().sort_values(ascending=False),y=data2.isnull().sum().sort_values(ascending=False).index)\n",
                "plt.title(\"counts of missing value for COVID19_open_line_list\",size=20)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('The data size is',len(data))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "formated_gdf = data.groupby(['date','Country/Region']).agg('sum')\n",
                "formated_gdf = formated_gdf.reset_index()\n",
                "formated_gdf=formated_gdf.drop(columns='SNo')\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "source": [
                "Confirmed_last=data[data['date']==max(data['date'])].groupby(['Country/Region']).agg('sum').sort_values('Confirmed',ascending=False)[:10]\n",
                "Confirmed_last=Confirmed_last.reset_index()\n",
                "Confirmed_last=Confirmed_last.drop(columns='SNo')\n",
                "Confirmed_last['Recovered rate']=Confirmed_last['Recovered']/Confirmed_last['Confirmed']\n",
                "Confirmed_last.style.background_gradient(cmap='Blues')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = go.Figure(data=[\n",
                "    go.Line(name='Confirmed', x=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['Confirmed']),\n",
                "    go.Line(name='Deaths', x=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['Deaths']),\n",
                "    go.Line(name='Recovered', x=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['Recovered']),\n",
                "])\n",
                "\n",
                "fig.update_layout(\n",
                "    title=\"Number of Confirmed,Recovered,death in china for each day\",\n",
                "    xaxis_title=\"date\",\n",
                "    yaxis_title=\"People\",\n",
                "    font=dict(\n",
                "        family=\"Courier New, monospace\",\n",
                "        size=18,\n",
                "        color=\"#7f7f7f\"\n",
                "    ))\n",
                "fig"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = go.Figure(data=[\n",
                "    go.Line(name='Confirmed', x=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['Confirmed']),\n",
                "    go.Line(name='Deaths', x=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['Deaths']),\n",
                "    go.Line(name='Recovered', x=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['Recovered']),\n",
                "])\n",
                "\n",
                "fig.update_layout(\n",
                "    title=\"Number of Confirmed,Recovered,death in Taiwan for each day\",\n",
                "    xaxis_title=\"date\",\n",
                "    yaxis_title=\"People\",\n",
                "    font=dict(\n",
                "        family=\"Courier New, monospace\",\n",
                "        size=18,\n",
                "        color=\"#7f7f7f\"\n",
                "    ))\n",
                "fig"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "formated_gdf['size'] = formated_gdf['Confirmed'].pow(0.3)\n",
                "\n",
                "fig = px.scatter_geo(formated_gdf, locations=\"Country/Region\", locationmode='country names', \n",
                "                     color=\"Confirmed\", size='size', hover_name=\"Country/Region\", \n",
                "                     range_color= [0, max(formated_gdf['Confirmed'])+2], \n",
                "                     projection=\"natural earth\", animation_frame=\"date\", \n",
                "                     title='Confirmed for each day')\n",
                "fig.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "formated_gdf['size'] = formated_gdf['Deaths'].pow(0.3)\n",
                "\n",
                "fig = px.scatter_geo(formated_gdf, locations=\"Country/Region\", locationmode='country names', \n",
                "                     color=\"Deaths\", size='size', hover_name=\"Country/Region\", \n",
                "                     range_color= [0, max(formated_gdf['Deaths'])+2], \n",
                "                     projection=\"natural earth\", animation_frame=\"date\", \n",
                "                     title='Deaths for each day')\n",
                "fig.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "formated_gdf['size'] = formated_gdf['Recovered'].pow(0.3)\n",
                "\n",
                "fig = px.scatter_geo(formated_gdf, locations=\"Country/Region\", locationmode='country names', \n",
                "                     color=\"Recovered\", size='size', hover_name=\"Country/Region\", \n",
                "                     range_color= [0, max(formated_gdf['Recovered'])+2], \n",
                "                     projection=\"natural earth\", animation_frame=\"date\", \n",
                "                     title='Recovered for each day')\n",
                "fig.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "china=data[data['Country/Region']=='Mainland China']\n",
                "china= china.groupby(['date','Province/State'])['Confirmed', 'Deaths', 'Recovered'].max()\n",
                "china = china.reset_index()\n",
                "china.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "china['Province/State'].unique()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "china_table=china[china['date']==max(china['date'])].sort_values('Confirmed',ascending=False)[:10]\n",
                "china_table=china_table.drop(columns=['date'])\n",
                "china_table['Recovered rate']=china_table['Recovered']/china_table['Confirmed']\n",
                "china_table.style.background_gradient(cmap='Reds')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "colors = dict(zip(\n",
                "    ['Anhui', 'Beijing', 'Chongqing', 'Fujian', 'Gansu', 'Guangdong',\n",
                "       'Guangxi', 'Guizhou', 'Hainan', 'Hebei', 'Heilongjiang', 'Henan',\n",
                "       'Hubei', 'Hunan', 'Inner Mongolia', 'Jiangsu', 'Jiangxi', 'Jilin',\n",
                "       'Liaoning', 'Ningxia', 'Qinghai', 'Shaanxi', 'Shandong',\n",
                "       'Shanghai', 'Shanxi', 'Sichuan', 'Tianjin', 'Tibet', 'Xinjiang',\n",
                "       'Yunnan', 'Zhejiang'],\n",
                "    ['#800000', '#8B0000', '#A52A2A', '#B22222', '#DC143C', '#FF0000', '#FF6347','#FF7F50','#CD5C5C','#F08080',\n",
                "    '#E9967A','#FA8072','#FFA07A','#FF4500','#FF8C00','#FFA500','#FFD700','#B8860B','#DAA520','#EEE8AA',\n",
                "    '#BDB76B','#F0E68C','#808000','#FFFF00','#9ACD32','#556B2F','#6B8E23','#7CFC00','#7FFF00','#ADFF2F',\n",
                "    '#006400']\n",
                "))\n",
                "\n",
                "\n",
                "def race_barchart(date):\n",
                "    dff = china[china['date'].eq(date)].sort_values(by='Confirmed', ascending=True).tail(10)\n",
                "    ax.clear()\n",
                "    ax.barh(dff['Province/State'], dff['Confirmed'], color=[colors[x] for x in dff['Province/State']],height=0.8)\n",
                "    dx = dff['Confirmed'].max() / 200\n",
                "    \n",
                "    for i, (value, name) in enumerate(zip(dff['Confirmed'], dff['Province/State'])):\n",
                "        ax.text(0, i,name+' ',size=16, weight=600, ha='right', va='center') \n",
                "        ax.text(value+dx, i,f'{value:,.0f}',  size=16, ha='left',  va='center') \n",
                "            \n",
                "    ax.text(0.9, 0.2, date, transform=ax.transAxes, color='#777777', size=72, ha='right', weight=1000) \n",
                "    ax.text(0.59, 0.14, 'Total Confirmed:'+str(int(dff['Confirmed'].sum())), transform=ax.transAxes, size=24, color='#000000',ha='left')\n",
                "    ax.tick_params(axis='x', colors='#777777', labelsize=12) \n",
                "    ax.xaxis.set_ticks_position('top') \n",
                "    ax.set_yticks([])\n",
                "    ax.margins(0, 0.01)\n",
                "    ax.grid(which='major', axis='x', linestyle='-') \n",
                "    ax.text(0, 1.15, 'Confirmed for each date in China ',\n",
                "                transform=ax.transAxes, size=24, weight=600, ha='left', va='top') \n",
                "\n",
                "    plt.box(False)\n",
                "    \n",
                "\n",
                "day = list(set(china.date.values))\n",
                "day.sort()\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(16, 9))\n",
                "\n",
                "HTML(animation.FuncAnimation(fig, race_barchart, frames=day).to_jshtml())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = go.Figure(data=[\n",
                "    go.Line(name='Confirmed', x=china[china['Province/State']=='Hubei']['date'], y=china[china['Province/State']=='Hubei']['Confirmed']),\n",
                "    go.Line(name='Deaths', x=china[china['Province/State']=='Hubei']['date'], y=china[china['Province/State']=='Hubei']['Deaths']),\n",
                "    go.Line(name='Recovered', x=china[china['Province/State']=='Hubei']['date'], y=china[china['Province/State']=='Hubei']['Recovered']),\n",
                "])\n",
                "\n",
                "fig.update_layout(\n",
                "    title=\"Number of Confirmed,Recovered,death in Huibel for each day\",\n",
                "    xaxis_title=\"date\",\n",
                "    yaxis_title=\"People\",\n",
                "    font=dict(\n",
                "        family=\"Courier New, monospace\",\n",
                "        size=18,\n",
                "        color=\"#7f7f7f\"\n",
                "    ))\n",
                "fig"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "Italy=data[data['Country/Region']=='Italy']\n",
                "Italy= Italy.groupby(['date'])['Confirmed', 'Deaths', 'Recovered'].max()\n",
                "Italy = Italy.reset_index()\n",
                "Italy.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = go.Figure(data=[\n",
                "    go.Line(name='Confirmed', x=Italy['date'], y=Italy['Confirmed']),\n",
                "    go.Line(name='Deaths', x=Italy['date'], y=Italy['Deaths']),\n",
                "    go.Line(name='Recovered', x=Italy['date'], y=Italy['Recovered']),\n",
                "])\n",
                "\n",
                "fig.update_layout(\n",
                "    title=\"Number of Confirmed,Recovered,death in Italy for each day\",\n",
                "    xaxis_title=\"date\",\n",
                "    yaxis_title=\"People\",\n",
                "    font=dict(\n",
                "        family=\"Courier New, monospace\",\n",
                "        size=18,\n",
                "        color=\"#7f7f7f\"\n",
                "    ))\n",
                "fig"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(20,10))\n",
                "sns.barplot(x=data2.sex.value_counts().sort_values(ascending=False),y=data2.sex.value_counts().sort_values(ascending=False).index,ax=ax[0])\n",
                "ax[0].set_title(\"Number of patient by sex\",size=20)\n",
                "ax[0].set_xlabel('patient',size=18)\n",
                "sns.barplot(x=data2.country.value_counts().sort_values(ascending=False),y=data2.country.value_counts().sort_values(ascending=False).index,ax=ax[1])\n",
                "ax[1].set_title(\"Number of patient by country\",size=20)\n",
                "ax[1].set_xlabel('patient',size=18)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "transfer_results"
            ],
            "source": [
                "incidents=folium.map.FeatureGroup()\n",
                "\n",
                "Lat=data2.latitude.mean()\n",
                "Lon=data2.longitude.mean()\n",
                "from folium import plugins\n",
                "\n",
                "map1=folium.Map([Lat,Lon],zoom_start=3)\n",
                "\n",
                "COVID_map=plugins.MarkerCluster().add_to(map1)\n",
                "for lat,lon,label in zip(data2.latitude,data2.longitude,data2.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(COVID_map)\n",
                "map1.add_child(COVID_map)\n",
                "map1.save(\"COVID\"+\".html\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "HTML('<iframe src=COVID.html width=1000 height=450></iframe>')"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "source": [
                "US=data[data['Country/Region']=='US']\n",
                "US= US.groupby(['date','Province/State'])['Confirmed', 'Deaths', 'Recovered'].max()\n",
                "US =US.reset_index()\n",
                "\n",
                "US_table=US[US['date']==max(US['date'])].sort_values('Confirmed',ascending=False)[:10]\n",
                "US_table=US_table.drop(columns=['date'])\n",
                "US_table['Recovered rate']=US_table['Recovered']/US_table['Confirmed']\n",
                "US_table.style.background_gradient(cmap='Purples')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "import random\n",
                "random.seed( 199 )\n",
                "def randomcolor():\n",
                "    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']\n",
                "    color = \"\"\n",
                "    for i in range(6):\n",
                "        color += colorArr[random.randint(0,14)]\n",
                "    return \"#\"+color\n",
                "color_US=[]\n",
                "for i in range(len(US['Province/State'].unique())):\n",
                "    color_US.append(randomcolor())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "colors_US = dict(zip(\n",
                "    US['Province/State'].unique(),\n",
                "    color_US\n",
                "))\n",
                "\n",
                "\n",
                "def race_barchart_US(date):\n",
                "    dff = US[US['date'].eq(date)].sort_values(by='Confirmed', ascending=True).tail(10)\n",
                "    ax.clear()\n",
                "    ax.barh(dff['Province/State'], dff['Confirmed'], color=[colors_US[x] for x in dff['Province/State']],height=0.8)\n",
                "    dx = dff['Confirmed'].max() / 200\n",
                "    \n",
                "    for i, (value, name) in enumerate(zip(dff['Confirmed'], dff['Province/State'])):\n",
                "        ax.text(0, i,name+' ',size=16, weight=600, ha='right', va='center') \n",
                "        ax.text(value+dx, i,f'{value:,.0f}',  size=16, ha='left',  va='center') \n",
                "            \n",
                "    ax.text(0.9, 0.2, date, transform=ax.transAxes, color='#777777', size=72, ha='right', weight=1000) \n",
                "    ax.text(0.59, 0.14, 'Total Confirmed:'+str(int(dff['Confirmed'].sum())), transform=ax.transAxes, size=24, color='#000000',ha='left')\n",
                "    ax.tick_params(axis='x', colors='#777777', labelsize=12) \n",
                "    ax.xaxis.set_ticks_position('top') \n",
                "    ax.set_yticks([])\n",
                "    ax.margins(0, 0.01)\n",
                "    ax.grid(which='major', axis='x', linestyle='-') \n",
                "    ax.text(0, 1.15, 'Confirmed for each date in US ',\n",
                "                transform=ax.transAxes, size=24, weight=600, ha='left', va='top') \n",
                "\n",
                "    plt.box(False)\n",
                "    \n",
                "\n",
                "day = list(set(US.date.values))\n",
                "day.sort()\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(16, 9))\n",
                "\n",
                "HTML(animation.FuncAnimation(fig, race_barchart_US, frames=day,interval=400).to_jshtml())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = go.Figure(data=[\n",
                "    go.Line(name='Confirmed', x=formated_gdf[formated_gdf['Country/Region']=='US']['date'], y=formated_gdf[formated_gdf['Country/Region']=='US']['Confirmed']),\n",
                "    go.Line(name='Deaths', x=formated_gdf[formated_gdf['Country/Region']=='US']['date'], y=formated_gdf[formated_gdf['Country/Region']=='US']['Deaths']),\n",
                "    go.Line(name='Recovered', x=formated_gdf[formated_gdf['Country/Region']=='US']['date'], y=formated_gdf[formated_gdf['Country/Region']=='US']['Recovered']),\n",
                "])\n",
                "\n",
                "fig.update_layout(\n",
                "    title=\"Number of Confirmed,Recovered,death in US for each day\",\n",
                "    xaxis_title=\"date\",\n",
                "    yaxis_title=\"People\",\n",
                "    font=dict(\n",
                "        family=\"Courier New, monospace\",\n",
                "        size=18,\n",
                "        color=\"#7f7f7f\"\n",
                "    ))\n",
                "fig"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os \n",
                "import seaborn as sns\n",
                "import geopandas as gpd\n",
                "import folium\n",
                "from folium import plugins\n",
                "import datetime\n",
                "import math"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/denver-crime-data')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data=pd.read_csv('../input/denver-crime-data/crime.csv')\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "y=data.isnull().sum().sort_values(ascending=False)[:6].index\n",
                "x=data.isnull().sum().sort_values(ascending=False)[:6]\n",
                "plt.figure(figsize=(8,8))\n",
                "sns.barplot(x,y)\n",
                "plt.title(\"counts of missing value\",size=20)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data=data.dropna(subset=['GEO_LAT','GEO_LON'])\n",
                "data.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data['REPORTED_DATE']=data.REPORTED_DATE.apply(lambda x:datetime.datetime.strptime(x,'%m/%d/%Y %I:%M:%S %p'))\n",
                "data['year']=data.REPORTED_DATE.apply(lambda x:x.strftime('%Y'))\n",
                "data['month']=data.REPORTED_DATE.apply(lambda x:x.strftime('%m'))\n",
                "data['hour']=data.REPORTED_DATE.apply(lambda x:x.strftime('%H'))\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data=data[data.GEO_LAT>39]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "Top10_crime_type=data[data['OFFENSE_CATEGORY_ID'].isin(list(data.OFFENSE_CATEGORY_ID.value_counts()[:10].index[:10]))]\n",
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "y=Top10_crime_type.OFFENSE_CATEGORY_ID.value_counts().index\n",
                "x=Top10_crime_type.OFFENSE_CATEGORY_ID.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"Top 10 crime type by counts\",size=20)\n",
                "ax[0,0].set_xlabel('counts',size=18)\n",
                "ax[0,0].set_ylabel('')\n",
                "\n",
                "\n",
                "Top10_crime_type.groupby(['year','OFFENSE_CATEGORY_ID'])['INCIDENT_ID'].agg('count').unstack('OFFENSE_CATEGORY_ID').plot(ax=ax[0,1])\n",
                "ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "ax[0,1].set_title(\"Top 10 crime type counts by year\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "ax[0,1].set_xlabel('year',size=18)\n",
                "\n",
                "Top10_crime_type.groupby(['month','OFFENSE_CATEGORY_ID'])['INCIDENT_ID'].agg('count').unstack('OFFENSE_CATEGORY_ID').plot(ax=ax[1,0])\n",
                "ax[1,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(-0.25,1))\n",
                "ax[1,0].set_title(\"Top 10 crime type counts by month\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "ax[1,0].set_xlabel('month',size=18)\n",
                "\n",
                "sns.scatterplot(x=\"GEO_LON\", y=\"GEO_LAT\", hue=\"OFFENSE_CATEGORY_ID\",data=Top10_crime_type,ax=ax[1,1])\n",
                "ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "ax[1,1].set_title(\"The distribution of Top 10 crime type\",size=20)\n",
                "ax[1,1].set(xlabel='Longitude', ylabel='LATITUDE')\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_2000=data[:2000]\n",
                "Long=data_2000.GEO_LON.mean()\n",
                "Lat=data_2000.GEO_LAT.mean()\n",
                "data_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "data_crime_map=plugins.MarkerCluster().add_to(data_map)\n",
                "for lat,lon,label in zip(data_2000.GEO_LAT,data_2000.GEO_LON,data_2000.OFFENSE_CATEGORY_ID):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_crime_map)\n",
                "data_map.add_child(data_crime_map)\n",
                "\n",
                "data_map"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "y=data.year.value_counts()\n",
                "x=data.year.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"The number of crimes by year\",size=20)\n",
                "ax[0,0].set_ylabel('counts',size=18)\n",
                "ax[0,0].set_xlabel('')\n",
                "\n",
                "\n",
                "y=data.month.value_counts()\n",
                "x=data.month.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"The number of crimes by month\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "ax[0,1].set_xlabel('')\n",
                "\n",
                "y=data.hour.value_counts()\n",
                "x=data.hour.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"The number of crimes by hour\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "ax[1,0].set_xlabel('')\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "y=data.NEIGHBORHOOD_ID.value_counts()[:10].index\n",
                "x=data.NEIGHBORHOOD_ID.value_counts()[:10]\n",
                "sns.barplot(x=x,y=y,ax=ax[1,1])\n",
                "ax[1,1].set_title(\"The 10 NEIGHBORHOOD_ID by the number of crimes\",size=20)\n",
                "ax[1,1].set_xlabel('counts',size=18)\n",
                "ax[1,1].set_ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "map_all=folium.Map([39.7,-105],zoom_start=12)\n",
                "crime_new=pd.DataFrame({\"Lat\":data['GEO_LAT'],\"Long\":data['GEO_LON']})\n",
                "crime_new=crime_new[:20000]\n",
                "map_all.add_child(plugins.HeatMap(data=crime_new))\n",
                "map_all"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_traf=data[data.OFFENSE_CATEGORY_ID=='traffic-accident']\n",
                "data_traf.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "y=data_traf.year.value_counts()\n",
                "x=data_traf.year.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"The number of traffic-accident by year\",size=20)\n",
                "ax[0,0].set_ylabel('counts',size=18)\n",
                "ax[0,0].set_xlabel('')\n",
                "\n",
                "\n",
                "y=data_traf.month.value_counts()\n",
                "x=data_traf.month.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"The number of traffic-accident by month\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "ax[0,1].set_xlabel('')\n",
                "\n",
                "y=data_traf.hour.value_counts()\n",
                "x=data_traf.hour.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"The number of traffic-accident by hour\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "ax[1,0].set_xlabel('')\n",
                "\n",
                "\n",
                "sns.scatterplot(x=\"GEO_LON\", y=\"GEO_LAT\", hue=\"NEIGHBORHOOD_ID\",data=data_traf,ax=ax[1,1])\n",
                "ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,2.5))\n",
                "ax[1,1].set_title(\"The distribution of traffic-accident\",size=20)\n",
                "ax[1,1].set(xlabel='Longitude', ylabel='LATITUDE')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_IS_TRAFFIC=data[data.IS_TRAFFIC==1]\n",
                "data_IS_TRAFFIC.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_IS_TRAFFIC=data_IS_TRAFFIC[:2000]\n",
                "colors = {'2014' : 'red', '2015' : 'blue','2016' :'green','2017':'brown','2018':'plum','2019':'purple'}\n",
                "Long=data_IS_TRAFFIC.GEO_LON.mean()\n",
                "Lat=data_IS_TRAFFIC.GEO_LAT.mean()\n",
                "data_IS_TRAFFIC_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "for i in range(len(data_IS_TRAFFIC.groupby(['GEO_LAT','GEO_LON'])['INCIDENT_ID'].agg('count').index)):\n",
                "    lat,lon=data_IS_TRAFFIC.groupby(['GEO_LAT','GEO_LON'])['INCIDENT_ID'].agg('count').index[i]\n",
                "    folium.Circle(location=[lat,lon],\n",
                "    popup=data_IS_TRAFFIC.iloc[i]['OFFENSE_TYPE_ID'],\n",
                "    radius=int(data_IS_TRAFFIC.groupby(['GEO_LAT','GEO_LON'])['INCIDENT_ID'].agg('count')[i])*70,\n",
                "    fill=True,\n",
                "    fill_color=colors[data_IS_TRAFFIC['year'].iloc[i]],\n",
                "    fill_opacity=0.7,).add_to(data_IS_TRAFFIC_map)\n",
                "\n",
                "data_IS_TRAFFIC_map"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_IS_TRAFFIC=data[data.IS_TRAFFIC==1]\n",
                "plt.figure(figsize=(20,20))\n",
                "for i in range(6):\n",
                "    traffic=data_IS_TRAFFIC[data_IS_TRAFFIC.year==str(2014+i)]\n",
                "    plt.subplot(3,2,i+1)\n",
                "    plt.scatter('GEO_LON', 'GEO_LAT', data=traffic, c=colors[traffic['year'].iloc[0]])\n",
                "    plt.title(\"The distribution of traffic-accident in \"+str(2014+i),size=20)\n",
                "    plt.xlabel('Longitude')\n",
                "    plt.ylabel('LATITUDE')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_larceny=data[data.OFFENSE_CATEGORY_ID=='larceny']\n",
                "data_larceny.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "y=data_larceny.year.value_counts()\n",
                "x=data_larceny.year.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"The number of larceny by year\",size=20)\n",
                "ax[0,0].set_ylabel('counts',size=18)\n",
                "ax[0,0].set_xlabel('')\n",
                "\n",
                "\n",
                "y=data_larceny.month.value_counts()\n",
                "x=data_larceny.month.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"The number of larceny by month\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "ax[0,1].set_xlabel('')\n",
                "\n",
                "y=data_larceny.hour.value_counts()\n",
                "x=data_larceny.hour.value_counts().index\n",
                "sns.barplot(x=x,y=y,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"The number of larceny by hour\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "ax[1,0].set_xlabel('')\n",
                "\n",
                "\n",
                "sns.scatterplot(x=\"GEO_LON\", y=\"GEO_LAT\", hue=\"NEIGHBORHOOD_ID\",data=data_larceny,ax=ax[1,1])\n",
                "ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,2.5))\n",
                "ax[1,1].set_title(\"The distribution of larency\",size=20)\n",
                "ax[1,1].set(xlabel='Longitude', ylabel='LATITUDE')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def rank_NEIGHBORHOOD(NEIGHBORHOOD_ID):\n",
                "    year=['2014','2015','2016','2017','2018','2019']\n",
                "    B={}\n",
                "    for i in range(len(year)):\n",
                "        A=data[data.year==year[i]]\n",
                "        value=A.groupby(['NEIGHBORHOOD_ID'])['OFFENSE_ID'].agg('count')\n",
                "        rank=A.groupby(['NEIGHBORHOOD_ID'])['OFFENSE_ID'].agg('count').rank(method='min',ascending=False)\n",
                "        new=pd.DataFrame({'rank':rank,'value':value})\n",
                "        B['rank '+year[i]]=str(new[new.index==NEIGHBORHOOD_ID].iloc[0,0])+\"/\"+str(max(rank))\n",
                "        B['value '+year[i]]=str(new[new.index==NEIGHBORHOOD_ID].iloc[0,1])\n",
                "\n",
                "    return B"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def rank_plot(NEIGHBORHOOD_ID):\n",
                "    ID=rank_NEIGHBORHOOD(NEIGHBORHOOD_ID)\n",
                "    y=[]\n",
                "    x=[]\n",
                "    n=[]\n",
                "    for i in range(6):\n",
                "        r1,r2=ID['rank '+str(i+2014)].split('/')\n",
                "        R=float(r1)/float(r2)\n",
                "        R=1-R\n",
                "        y.append(1.5+R*math.sin(0+i*2*math.pi/6))\n",
                "        x.append(1.5+R*math.cos(0+i*2*math.pi/6))\n",
                "        n.append('rank '+str(i+2014)+' '+ID['rank '+str(i+2014)])\n",
                "    \n",
                "    x.append(x[0])\n",
                "    y.append(y[0])\n",
                "    plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "    for i, txt in enumerate(n):\n",
                "        plt.annotate(txt, (x[i], y[i]))\n",
                "        plt.xlim(0.45,2.7)\n",
                "        plt.ylim(0.45,2.7)\n",
                "        plt.fill(x, y,\"plum\")\n",
                "        plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "        plt.title(\"The rank of the number of crime by year in \"+NEIGHBORHOOD_ID,size=18) "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(20,30))\n",
                "plt.subplot(3,2,1)\n",
                "rank_plot('five-points')\n",
                "plt.subplot(3,2,2)\n",
                "rank_plot('stapleton')\n",
                "plt.subplot(3,2,3)\n",
                "rank_plot('cbd')\n",
                "plt.subplot(3,2,4)\n",
                "rank_plot('capitol-hill')\n",
                "plt.subplot(3,2,5)\n",
                "rank_plot('virginia-village')\n",
                "plt.subplot(3,2,6)\n",
                "rank_plot('city-park')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import re #library to clean data\n",
                "import nltk #Natural Language tool kit\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns \n",
                "import os \n",
                "import datetime\n",
                "from nltk.corpus import stopwords #to remove stopword\n",
                "from nltk.stem.porter import PorterStemmer \n",
                "from PIL import Image\n",
                "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/google-play-store-apps')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data=pd.read_csv(\"../input/google-play-store-apps/googleplaystore.csv\")\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.isnull().sum().sort_values(ascending=False)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data_review=pd.read_csv(\"../input/google-play-store-apps/googleplaystore_user_reviews.csv\")\n",
                "data_review.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_review.isnull().sum().sort_values(ascending=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('data size: ',len(data))\n",
                "print('data_review size: ',len(data_review))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data=data.dropna(subset=['Rating','Current Ver','Android Ver','Content Rating'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(25,16))\n",
                "sns.barplot(x=data.Category.value_counts(),y=data.Category.value_counts().index,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"Counts of Category\",size=20)\n",
                "ax[0,0].set_xlabel(\"\")\n",
                "\n",
                "data.Reviews=data.Reviews.astype('int')\n",
                "sns.barplot(x=data.groupby(['Category'])['Reviews'].agg('sum').sort_values(ascending=False),y=data.groupby(['Category'])['Reviews'].agg('sum').sort_values(ascending=False).index,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"Number of reviews by Category\",size=20)\n",
                "ax[0,1].set_ylabel(\"\")\n",
                "\n",
                "data['new_install']=data.Installs.apply(lambda x:x.split('+')[0].strip(',').replace(',',''))\n",
                "data.new_install=data.new_install.astype('int')\n",
                "\n",
                "sns.barplot(x=data.groupby(['Category'])['new_install'].agg('sum').sort_values(ascending=False),y=data.groupby(['Category'])['new_install'].agg('sum').sort_values(ascending=False).index,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"Number of installs by Category\",size=20)\n",
                "ax[1,0].set_ylabel(\"\")\n",
                "ax[1,0].set_xlabel(\"\")\n",
                "\n",
                "sns.boxplot(y=\"Category\",x=\"Rating\",data=data,ax=ax[1,1])\n",
                "ax[1,1].set_ylabel(\"\")\n",
                "ax[1,1].set_title(\"Distribution of rating by Category\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(8,8))\n",
                "corr = data.corr()\n",
                "sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns,annot=True)\n",
                "plt.title(\"correlation plot\",size=28)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(25,16))\n",
                "sns.barplot(x=data.Size.value_counts()[:10],y=data.Size.value_counts()[:10].index,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 Size by counts\",size=20)\n",
                "ax[0].set_xlabel(\"\")\n",
                "\n",
                "sns.boxplot(y=\"Size\",x=\"Rating\",data=data[data.Size.isin(list(data.Size.value_counts()[:10].index))],ax=ax[1])\n",
                "ax[1].set_ylabel(\"\")\n",
                "ax[1].set_title(\"Distribution of rating by Size for Top 10\",size=20)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data['new_Price']=data.Price.apply(lambda x:  x.strip('$') if x!='0' else x.strip(''))\n",
                "data.new_Price=data.new_Price.astype(float)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(25,16))\n",
                "sns.barplot(x=data['Content Rating'].value_counts(),y=data['Content Rating'].value_counts().index,ax=ax[0])\n",
                "ax[0].set_title(\"Counts of Content Rating\",size=20)\n",
                "ax[0].set_xlabel(\"\")\n",
                "\n",
                "sns.barplot(x=data.groupby(['Content Rating'])['new_Price'].agg('sum'),y=data.groupby(['Content Rating'])['new_Price'].agg('sum').index,ax=ax[1])\n",
                "ax[1].set_title(\"Total Price by Content Rating\",size=20)\n",
                "ax[1].set_ylabel(\"\")\n",
                "ax[1].set_xlabel(\"Total Price\")\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_review=data_review.dropna(subset=['Translated_Review'])\n",
                "data_review=data_review.reindex(range(len(data_review)), method='ffill')\n",
                "headline_text_new=[]#Initialize empty array to append clean text\n",
                "for i in range(len(data_review)):\n",
                "    headline=re.sub('[^a-zA-Z]',' ',data_review['Translated_Review'][i]) \n",
                "    headline=headline.lower() #convert to lower case\n",
                "    headline=headline.split() #split to array(default delimiter is \" \")\n",
                "    ps=PorterStemmer() #creating porterStemmer object to take main stem of each word\n",
                "    headline=[ps.stem(word) for word in headline if not word in set(stopwords.words('english'))] #loop for stemming each word  in string array at ith row\n",
                "    headline_text_new.extend(headline)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "wordcloud = WordCloud(background_color=\"black\",max_words=200,max_font_size=40,random_state=10).generate(str(headline_text_new))\n",
                "\n",
                "plt.figure(figsize=(20,15))\n",
                "plt.imshow(wordcloud, interpolation='bilinear')\n",
                "plt.axis(\"off\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import seaborn as sns\n",
                "import math\n",
                "import datetime"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(\"../input/india-trade-data\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data_export=pd.read_csv(\"../input/india-trade-data/2018-2010_export.csv\")\n",
                "data_import=pd.read_csv(\"../input/india-trade-data/2018-2010_import.csv\")\n",
                "data_export.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_export.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_export=data_export.dropna(subset=['value'])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_import.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_import.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_import=data_import.dropna(subset=['value'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(30,30))\n",
                "j=0\n",
                "for i in ['2010','2011','2012','2013','2014','2015','2016','2017','2018']:\n",
                "    j+=1\n",
                "    plt.subplot(3,3,j)\n",
                "    y=data_export[data_export.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10].index\n",
                "    x=data_export[data_export.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10]\n",
                "    sns.barplot(x=x,y=y)\n",
                "    plt.title('Top 10 value of export in '+i,size=24)\n",
                "    plt.xlabel('million US$')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(30,30))\n",
                "j=0\n",
                "for i in ['2010','2011','2012','2013','2014','2015','2016','2017','2018']:\n",
                "    j+=1\n",
                "    plt.subplot(3,3,j)\n",
                "    y=data_import[data_import.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10].index\n",
                "    x=data_import[data_import.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10]\n",
                "    sns.barplot(x=x,y=y)\n",
                "    plt.title('Top 10 value of import in '+i,size=24)\n",
                "    plt.xlabel('million US$')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def rank_export(country):\n",
                "    year=['2010','2011','2012','2013','2014','2015','2016','2017','2018']\n",
                "    B={}\n",
                "    for i in range(len(year)):\n",
                "        A=data_export[data_export.year==int(year[i])]\n",
                "        value=A.groupby(['country'])['value'].agg('sum')\n",
                "        rank=A.groupby(['country'])['value'].agg('sum').rank(method='min',ascending=False)\n",
                "        new=pd.DataFrame({'rank':rank,'value':value})\n",
                "        B['rank '+year[i]]=str(new[new.index==country].iloc[0,0])+\"/\"+str(max(rank))\n",
                "        B['value '+year[i]]=str(new[new.index==country].iloc[0,1])\n",
                "\n",
                "    return B"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def rank_import(country):\n",
                "    year=['2010','2011','2012','2013','2014','2015','2016','2017','2018']\n",
                "    B={}\n",
                "    for i in range(len(year)):\n",
                "        A=data_import[data_import.year==int(year[i])]\n",
                "        value=A.groupby(['country'])['value'].agg('sum')\n",
                "        rank=A.groupby(['country'])['value'].agg('sum').rank(method='min',ascending=False)\n",
                "        new=pd.DataFrame({'rank':rank,'value':value})\n",
                "        B['rank '+year[i]]=str(new[new.index==country].iloc[0,0])+\"/\"+str(max(rank))\n",
                "        B['value '+year[i]]=str(new[new.index==country].iloc[0,1])\n",
                "\n",
                "    return B"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(24,10))\n",
                "plt.subplot(1,2,1)\n",
                "CHINA=rank_export('CHINA P RP')\n",
                "y=[]\n",
                "x=[]\n",
                "n=[]\n",
                "for i in range(9):\n",
                "    r1,r2=CHINA['rank '+str(i+2010)].split('/')\n",
                "    R=float(r1)/float(r2)\n",
                "    R=1-R\n",
                "    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n",
                "    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n",
                "    n.append('rank '+str(i+2010)+' '+CHINA['rank '+str(i+2010)])\n",
                "    \n",
                "x.append(x[0])\n",
                "y.append(y[0])\n",
                "plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "for i, txt in enumerate(n):\n",
                "    plt.annotate(txt, (x[i], y[i]))\n",
                "    plt.fill(x, y,\"coral\")\n",
                "    plt.xlim(0.45,2.7)\n",
                "    plt.ylim(0.45,2.7)\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"CHINA's export rank by year\",size=18)\n",
                "    \n",
                "plt.subplot(1,2,2)\n",
                "CHINA=rank_import('CHINA P RP')\n",
                "y=[]\n",
                "x=[]\n",
                "n=[]\n",
                "for i in range(9):\n",
                "    r1,r2=CHINA['rank '+str(i+2010)].split('/')\n",
                "    R=float(r1)/float(r2)\n",
                "    R=1-R\n",
                "    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n",
                "    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n",
                "    n.append('rank '+str(i+2010)+' '+CHINA['rank '+str(i+2010)])\n",
                "    \n",
                "x.append(x[0])\n",
                "y.append(y[0])\n",
                "plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "for i, txt in enumerate(n):\n",
                "    plt.annotate(txt, (x[i], y[i]))\n",
                "    plt.xlim(0.45,2.7)\n",
                "    plt.ylim(0.45,2.7)\n",
                "    plt.fill(x, y,\"plum\")\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"CHINA's import rank by year\",size=18)   "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(24,10))\n",
                "plt.subplot(1,2,1)\n",
                "USA=rank_export('U S A')\n",
                "y=[]\n",
                "x=[]\n",
                "n=[]\n",
                "for i in range(9):\n",
                "    r1,r2=USA['rank '+str(i+2010)].split('/')\n",
                "    R=float(r1)/float(r2)\n",
                "    R=1-R\n",
                "    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n",
                "    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n",
                "    n.append('rank '+str(i+2010)+' '+USA['rank '+str(i+2010)])\n",
                "    \n",
                "x.append(x[0])\n",
                "y.append(y[0])\n",
                "plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "for i, txt in enumerate(n):\n",
                "    plt.annotate(txt, (x[i], y[i]))\n",
                "    plt.fill(x, y,\"coral\")\n",
                "    plt.xlim(0.45,2.7)\n",
                "    plt.ylim(0.45,2.7)\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"USA's export rank by year\",size=18)\n",
                "    \n",
                "plt.subplot(1,2,2)\n",
                "USA=rank_import('U S A')\n",
                "y=[]\n",
                "x=[]\n",
                "n=[]\n",
                "for i in range(9):\n",
                "    r1,r2=USA['rank '+str(i+2010)].split('/')\n",
                "    R=float(r1)/float(r2)\n",
                "    R=1-R\n",
                "    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n",
                "    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n",
                "    n.append('rank '+str(i+2010)+' '+USA['rank '+str(i+2010)])\n",
                "    \n",
                "x.append(x[0])\n",
                "y.append(y[0])\n",
                "plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "for i, txt in enumerate(n):\n",
                "    plt.annotate(txt, (x[i], y[i]))\n",
                "    plt.xlim(0.45,2.7)\n",
                "    plt.ylim(0.45,2.7)\n",
                "    plt.fill(x, y,\"plum\")\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"USA's import rank by year\",size=18)    \n",
                "    "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(24,10))\n",
                "plt.subplot(1,2,1)\n",
                "ALBANIA=rank_export('ALBANIA')\n",
                "y=[]\n",
                "x=[]\n",
                "n=[]\n",
                "for i in range(9):\n",
                "    r1,r2=ALBANIA['rank '+str(i+2010)].split('/')\n",
                "    R=float(r1)/float(r2)\n",
                "    R=1-R\n",
                "    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n",
                "    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n",
                "    n.append('rank '+str(i+2010)+' '+ALBANIA['rank '+str(i+2010)])\n",
                "    \n",
                "x.append(x[0])\n",
                "y.append(y[0])\n",
                "plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "for i, txt in enumerate(n):\n",
                "    plt.annotate(txt, (x[i], y[i]))\n",
                "    plt.fill(x, y,\"coral\")\n",
                "    plt.xlim(0.45,2.7)\n",
                "    plt.ylim(0.45,2.7)\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"ALBANIA's export rank by year\",size=18)\n",
                "    \n",
                "plt.subplot(1,2,2)\n",
                "ALBANIA=rank_import('ALBANIA')\n",
                "y=[]\n",
                "x=[]\n",
                "n=[]\n",
                "for i in range(9):\n",
                "    r1,r2=ALBANIA['rank '+str(i+2010)].split('/')\n",
                "    R=float(r1)/float(r2)\n",
                "    R=1-R\n",
                "    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n",
                "    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n",
                "    n.append('rank '+str(i+2010)+' '+ALBANIA['rank '+str(i+2010)])\n",
                "    \n",
                "x.append(x[0])\n",
                "y.append(y[0])\n",
                "plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "for i, txt in enumerate(n):\n",
                "    plt.annotate(txt, (x[i], y[i]))\n",
                "    plt.xlim(0.45,2.7)\n",
                "    plt.ylim(0.45,2.7)\n",
                "    plt.fill(x, y,\"plum\")\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"ALBANIA's import rank by year\",size=18)    "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(9,2,figsize=(20,55))\n",
                "for i in range(9):\n",
                "    count=data_export[data_export.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False)\n",
                "    groups=list(data_export[data_export.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False).index[:10])\n",
                "    counts=list(count[:10])\n",
                "    counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "    groups.append('Other')\n",
                "    type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "    clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "    type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[i,0])\n",
                "    ax[i,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "    ax[i,0].set_title(\"Top 10 export of commodity in \"+str(i+2010))\n",
                "    ax[i,0].set_ylabel('')\n",
                "\n",
                "    count=data_import[data_import.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False)\n",
                "    groups=list(data_import[data_import.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False).index[:10])\n",
                "    counts=list(count[:10])\n",
                "    counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "    groups.append('Other')\n",
                "    type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "    clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "    qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[i,1])\n",
                "    ax[i,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "    ax[i,1].set_title(\"Top 10 import of commodity in \"+str(i+2010))\n",
                "    ax[i,1].set_ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(10,2,figsize=(30,55))\n",
                "for i in range(10):\n",
                "    data_export.groupby(['HSCode','year'])['value'].agg(sum).unstack(['HSCode']).iloc[:,i*10:(i+1)*10].plot(ax=ax[i,0])\n",
                "    ax[i,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "    ax[i,0].set_title(\"Export HSCode by year\")\n",
                "    ax[i,0].set_ylabel('values')\n",
                "\n",
                "    data_import.groupby(['HSCode','year'])['value'].agg(sum).unstack(['HSCode']).iloc[:,i*10:(i+1)*10].plot(ax=ax[i,1])\n",
                "    ax[i,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "    ax[i,1].set_title(\"Import HSCode by year\")\n",
                "    ax[i,1].set_ylabel('values')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os \n",
                "import seaborn as sns\n",
                "import geopandas as gpd\n",
                "import folium\n",
                "from folium import plugins\n",
                "import datetime\n",
                "import math"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(\"../input/chicago-food-inspections\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data=pd.read_csv(\"../input/chicago-food-inspections/food-inspections.csv\")\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(data)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(10,10))\n",
                "sns.barplot(x=data.isnull().sum().sort_values(ascending=False),y=data.isnull().sum().sort_values(ascending=False).index)\n",
                "plt.title(\"counts of missing value\",size=20)\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data=data.dropna(subset=['Violations','Facility Type','Latitude','Longitude','AKA Name'])\n",
                "data.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data['year']=data['Inspection Date'].apply(lambda x:x.split('-')[0])\n",
                "data['month']=data['Inspection Date'].apply(lambda x:x.split('-')[1])\n",
                "data['day']=data['Inspection Date'].apply(lambda x:x.split('-')[2].split('T')[0])\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "x=data.year.value_counts().index\n",
                "y=data.year.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"The counts of inspection by year\",size=20)\n",
                "ax[0,0].set_ylabel('counts',size=18)\n",
                "ax[0,0].set_xlabel('')\n",
                "\n",
                "x=data.month.value_counts().index\n",
                "y=data.month.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"The counts of inspection by month\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "ax[0,1].set_xlabel('')\n",
                "\n",
                "x=data.day.value_counts().index\n",
                "y=data.day.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"The counts of inspection by day\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "ax[1,0].set_xlabel('')\n",
                "\n",
                "data.groupby(['year','month'])['Inspection ID'].agg('count').unstack('year').plot(ax=ax[1,1])\n",
                "ax[1,1].set_title(\"The counts of inspection for every month by year\",size=20)\n",
                "ax[1,1].set_ylabel('counts',size=18)\n",
                "ax[1,1].set_xlabel('month')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(15,16))\n",
                "data.Risk.value_counts().plot(kind='bar',color=['red','yellow','green'],ax=ax[0,0])\n",
                "ax[0,0].tick_params(axis='x',labelrotation=360)\n",
                "ax[0,0].set_title(\"The counts of Risk\",size=20)\n",
                "ax[0,0].set_ylabel('counts',size=18)\n",
                "\n",
                "\n",
                "data.groupby(['year','Risk'])['Inspection ID'].agg('count').unstack('Risk').plot(ax=ax[0,1],color=['red','yellow','green'])\n",
                "ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,0.75))\n",
                "ax[0,1].set_title(\"The counts of Risk by year\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "\n",
                "data.groupby(['month','Risk'])['Inspection ID'].agg('count').unstack('Risk').plot(ax=ax[1,0],color=['red','yellow','green'])\n",
                "ax[1,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(-0.25,0.75))\n",
                "ax[1,0].set_title(\"The counts of Risk by month\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "\n",
                "sns.scatterplot(x='Longitude',y='Latitude',hue='Risk' ,data=data, ax=ax[1,1])\n",
                "ax[1,1].set_title(\"The distribution of inspections by risk\",size=20)\n",
                "ax[1,1].set_xlabel('Longitude')\n",
                "ax[1,1].set_ylabel('LATITUDE')\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_risk1=data[data.Risk=='Risk 1 (High)']\n",
                "data_risk1.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "sns.barplot(x=data_risk1['Facility Type'].value_counts()[:10],y=data_risk1['Facility Type'].value_counts()[:10].index,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 Facility Type by the counts of risk 1 \",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=data_risk1.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False)\n",
                "groups=list(data_risk1.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10])\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "ax[1].set_ylabel('')\n",
                "ax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_risk1_2000=data_risk1[:2000]\n",
                "Long=data_risk1_2000.Longitude.mean()\n",
                "Lat=data_risk1_2000.Latitude.mean()\n",
                "risk1_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "risk1_distribution_map=plugins.MarkerCluster().add_to(risk1_map)\n",
                "for lat,lon,label in zip(data_risk1_2000.Latitude,data_risk1_2000.Longitude,data_risk1_2000['AKA Name']):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(risk1_distribution_map)\n",
                "risk1_map.add_child(risk1_distribution_map)\n",
                "\n",
                "risk1_map"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_risk2=data[data.Risk=='Risk 2 (Medium)']\n",
                "data_risk2.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "sns.barplot(x=data_risk2['Facility Type'].value_counts()[:10],y=data_risk2['Facility Type'].value_counts()[:10].index,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 Facility Type by the counts of risk 2 \",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=data_risk2.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False)\n",
                "groups=list(data_risk2.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10])\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "ax[1].set_ylabel('')\n",
                "ax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_risk2_2000=data_risk2[:2000]\n",
                "Long=data_risk2_2000.Longitude.mean()\n",
                "Lat=data_risk2_2000.Latitude.mean()\n",
                "risk2_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "risk2_distribution_map=plugins.MarkerCluster().add_to(risk2_map)\n",
                "for lat,lon,label in zip(data_risk2_2000.Latitude,data_risk2_2000.Longitude,data_risk2_2000['AKA Name']):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(risk2_distribution_map)\n",
                "risk2_map.add_child(risk2_distribution_map)\n",
                "\n",
                "risk2_map"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_risk3=data[data.Risk=='Risk 3 (Low)']\n",
                "data_risk3.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "sns.barplot(x=data_risk3['Facility Type'].value_counts()[:10],y=data_risk3['Facility Type'].value_counts()[:10].index,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 Facility Type by the counts of risk 3 \",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=data_risk3.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False)\n",
                "groups=list(data_risk3.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10])\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "ax[1].set_ylabel('')\n",
                "ax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_risk3_2000=data_risk3[:2000]\n",
                "data_risk3_2000['AKA Name']=data_risk3_2000['AKA Name'].apply(lambda x:x.strip('`').strip())\n",
                "Long=data_risk3_2000.Longitude.mean()\n",
                "Lat=data_risk3_2000.Latitude.mean()\n",
                "risk3_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "risk3_distribution_map=plugins.MarkerCluster().add_to(risk3_map)\n",
                "for lat,lon,label in zip(data_risk3_2000.Latitude,data_risk3_2000.Longitude,data_risk3_2000['AKA Name']):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(risk3_distribution_map)\n",
                "risk3_map.add_child(risk3_distribution_map)\n",
                "\n",
                "risk3_map"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,16))\n",
                "y=data['Facility Type'].value_counts()[:10].index\n",
                "x=data['Facility Type'].value_counts()[:10]\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"Top 10 Facility Type by the counts of inspection \",size=20)\n",
                "ax[0,0].set_xlabel('counts',size=18)\n",
                "ax[0,0].set_ylabel('')\n",
                "\n",
                "sns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=data[data['Facility Type']=='Restaurant'], ax=ax[0,1])\n",
                "ax[0,1].set_title(\"The distribution of inspections for restaurant\",size=20)\n",
                "ax[0,1].set_xlabel('Longitude')\n",
                "ax[0,1].set_ylabel('LATITUDE')\n",
                "\n",
                "sns.scatterplot(x='Longitude',y='Latitude',hue='Risk' ,hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'],data=data[data['Facility Type']=='Grocery Store'], ax=ax[1,0])\n",
                "ax[1,0].set_title(\"The distribution of inspections for Grocery Store\",size=20)\n",
                "ax[1,0].set_xlabel('Longitude')\n",
                "ax[1,0].set_ylabel('LATITUDE')\n",
                "\n",
                "sns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=data[data['Facility Type']=='School'], ax=ax[1,1])\n",
                "ax[1,1].set_title(\"The distribution of inspections for School\",size=20)\n",
                "ax[1,1].set_xlabel('Longitude')\n",
                "ax[1,1].set_ylabel('LATITUDE')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,16))\n",
                "x=data.Results.value_counts().index\n",
                "y=data.Results.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"The counts of Results of inspection \",size=20)\n",
                "ax[0,0].set_ylabel('counts',size=18)\n",
                "ax[0,0].set_xlabel('')\n",
                "\n",
                "data.groupby(['Results','year'])['Inspection ID'].agg('count').unstack('Results').plot(kind='bar',ax=ax[0,1])\n",
                "ax[0,1].tick_params(axis='x',labelrotation=360)\n",
                "ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,0.75))\n",
                "ax[0,1].set_title(\"The counts of results of inspection by year \",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "\n",
                "sns.scatterplot(x='Longitude',y='Latitude',hue='Risk' ,hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'],data=data[data.Results=='Pass'], ax=ax[1,0])\n",
                "ax[1,0].set_title(\"The distribution of result is pass\",size=20)\n",
                "ax[1,0].set_xlabel('Longitude')\n",
                "ax[1,0].set_ylabel('LATITUDE')\n",
                "\n",
                "sns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=data[data.Results=='Fail'], ax=ax[1,1])\n",
                "ax[1,1].set_title(\"The distribution of result is fail\",size=20)\n",
                "ax[1,1].set_xlabel('Longitude')\n",
                "ax[1,1].set_ylabel('LATITUDE')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import re #library to clean data\n",
                "import nltk #Natural Language tool kit\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns \n",
                "import os \n",
                "import datetime\n",
                "from nltk.corpus import stopwords #to remove stopword\n",
                "from nltk.stem.porter import PorterStemmer \n",
                "from PIL import Image\n",
                "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/ireland-historical-news')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "latnigrin=pd.read_csv(\"../input/ireland-historical-news/w3-latnigrin-text.csv\")\n",
                "latnigrin.head()\n",
                "irishtimes=pd.read_csv(\"../input/ireland-historical-news/irishtimes-date-text.csv\")\n",
                "irishtimes.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "irishtimes.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(irishtimes)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "irishtimes['date']=irishtimes.publish_date.apply(lambda x:datetime.datetime.strptime(str(x),'%Y%m%d').strftime('%Y-%m-%d'))\n",
                "irishtimes['year']=irishtimes.date.apply(lambda x:x.split('-')[0])\n",
                "irishtimes['month']=irishtimes.date.apply(lambda x:x.split('-')[1])\n",
                "irishtimes['day']=irishtimes.date.apply(lambda x:x.split('-')[2])\n",
                "irishtimes.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(16,16))\n",
                "Top10_category=irishtimes[irishtimes['headline_category'].isin(list(irishtimes.headline_category.value_counts()[:10].index[:10]))]\n",
                "sns.barplot(y=Top10_category.headline_category.value_counts().index,x=Top10_category.headline_category.value_counts(),ax=ax[0,0])\n",
                "ax[0,0].set_title(\"Top 10 category by counts\",size=20)\n",
                "ax[0,0].set_xlabel('counts',size=18)\n",
                "ax[0,0].set_ylabel('')\n",
                "\n",
                "Top10_category.groupby(['year','headline_category'])['headline_category'].agg('count').unstack('headline_category').plot(ax=ax[0,1])\n",
                "ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "ax[0,1].set_title(\"Top 10 category counts by year\",size=20)\n",
                "ax[0,1].set_ylabel('counts',size=18)\n",
                "ax[0,1].set_xlabel('year',size=18)\n",
                "\n",
                "Top10_category.groupby(['month','headline_category'])['headline_category'].agg('count').unstack('headline_category').plot(ax=ax[1,0])\n",
                "ax[1,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(-0.25,1))\n",
                "ax[1,0].set_title(\"Top 10 category counts by month\",size=20)\n",
                "ax[1,0].set_ylabel('counts',size=18)\n",
                "ax[1,0].set_xlabel('month',size=18)\n",
                "\n",
                "Top10_category.groupby(['day','headline_category'])['headline_category'].agg('count').unstack('headline_category').plot(ax=ax[1,1])\n",
                "ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n",
                "ax[1,1].set_title(\"Top 10 category counts by day\",size=20)\n",
                "ax[1,1].set_ylabel('counts',size=18)\n",
                "ax[1,1].set_xlabel('day',size=18)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(25,25))\n",
                "sns.barplot(x=irishtimes.year.value_counts().index,y=irishtimes.year.value_counts(),ax=ax[0,0])\n",
                "ax[0,0].set_title(\"Bar chart for year\",size=30)\n",
                "ax[0,0].set_xlabel('year',size=20)\n",
                "ax[0,0].set_ylabel('counts',size=20)\n",
                "\n",
                "sns.barplot(x=irishtimes.month.value_counts().index,y=irishtimes.month.value_counts(),ax=ax[0,1])\n",
                "ax[0,1].set_title(\"Bar chart for month\",size=30)\n",
                "ax[0,1].set_xlabel('month',size=20)\n",
                "ax[0,1].set_ylabel('counts',size=20)\n",
                "\n",
                "sns.barplot(x=irishtimes.day.value_counts().index,y=irishtimes.day.value_counts(),ax=ax[1,0])\n",
                "ax[1,0].set_title(\"Bar chart for day\",size=30)\n",
                "ax[1,0].set_xlabel('day',size=20)\n",
                "ax[1,0].set_ylabel('counts',size=20)\n",
                "\n",
                "irishtimes.groupby(['date'])['headline_category'].agg('count').plot(ax=ax[1,1])\n",
                "ax[1,1].set_title(\"Number of news for date\",size=30)\n",
                "ax[1,1].set_xlabel('date',size=20)\n",
                "ax[1,1].set_ylabel('counts',size=20)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "irishtimes_headline_text=irishtimes[:100000]\n",
                "headline_text_new=[]#Initialize empty array to append clean text\n",
                "for i in range(len(irishtimes_headline_text)):\n",
                "\theadline=re.sub('[^a-zA-Z]',' ',irishtimes_headline_text['headline_text'][i]) \n",
                "\theadline=headline.lower() #convert to lower case\n",
                "\theadline=headline.split() #split to array(default delimiter is \" \")\n",
                "\tps=PorterStemmer() #creating porterStemmer object to take main stem of each word\n",
                "\theadline=[ps.stem(word) for word in headline if not word in set(stopwords.words('english'))] #loop for stemming each word  in string array at ith row\n",
                "\theadline_text_new.extend(headline)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "\n",
                "wordcloud = WordCloud(background_color=\"black\",max_words=200,max_font_size=40,random_state=10).generate(str(headline_text_new))\n",
                "\n",
                "plt.figure(figsize=(20,15))\n",
                "plt.imshow(wordcloud, interpolation='bilinear')\n",
                "plt.axis(\"off\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def headline_by_year(year):\n",
                "    headline_text_new=[]#Initialize empty array to append clean text\n",
                "    irishtimes_headline_text=irishtimes[irishtimes.year==str(year)]\n",
                "    headline=None\n",
                "    for i in range(len(irishtimes_headline_text)):\n",
                "        headline=re.sub('[^a-zA-Z]',' ',irishtimes_headline_text['headline_text'][irishtimes_headline_text.index[i]]) \n",
                "        headline=headline.lower() #convert to lower case\n",
                "        headline=headline.split() #split to array(default delimiter is \" \")\n",
                "        ps=PorterStemmer() #creating porterStemmer object to take main stem of each word\n",
                "        headline=[ps.stem(word) for word in headline if not word in set(stopwords.words('english'))] #loop for stemming each word  in string array at ith row\n",
                "        headline_text_new.extend(headline)\n",
                "    wordcloud = WordCloud(background_color=\"black\",random_state=40,max_words=200,max_font_size=40).generate(str(headline_text_new))\n",
                "    plt.figure(figsize=(20,15))\n",
                "    plt.imshow(wordcloud, interpolation='bilinear')\n",
                "    plt.title(\"Wordcloud of headline in \"+str(year),size=20)\n",
                "    plt.axis(\"off\")\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "headline_by_year(1996)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "headline_by_year(2000)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "headline_by_year(2005)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "headline_by_year(2010)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "headline_by_year(2015)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "headline_by_year(2018)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os \n",
                "import seaborn as sns\n",
                "import geopandas as gpd\n",
                "import folium\n",
                "from folium import plugins\n",
                "import datetime\n",
                "import math"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(\"../input/ieee-fraud-detection\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train_identity=pd.read_csv(\"../input/ieee-fraud-detection/train_identity.csv\")\n",
                "train_transaction=pd.read_csv(\"../input/ieee-fraud-detection/train_transaction.csv\")\n",
                "test_transaction=pd.read_csv(\"../input/ieee-fraud-detection/test_transaction.csv\")\n",
                "test_identity=pd.read_csv(\"../input/ieee-fraud-detection/test_identity.csv\")\n",
                "print(\"train_identity_data_size: \",len(train_identity))\n",
                "print(\"train_transaction_data_size: \",len(train_transaction))\n",
                "print(\"test_transaction_data_size: \",len(test_transaction))\n",
                "print(\"test_identity_data_size: \",len(test_identity))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_identity.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(10,10))\n",
                "sns.barplot(x=train_identity.isnull().sum().sort_values(ascending=False),y=train_identity.isnull().sum().sort_values(ascending=False).index)\n",
                "plt.title(\"counts of missing value for train_identity\",size=20)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "train_identity_new=pd.DataFrame(train_identity,columns=['TransactionID','id_01','id_12','id_38','id_37','id_36','id_35','id_15','id_29','id_28','id_11','id_02','DeviceType','id_31','id_17','id_19','id_20'])\n",
                "train_identity_new=train_identity_new.dropna(subset=['id_38','id_37','id_36','id_35','id_15','id_29','id_28','id_11','id_02','DeviceType','id_31','id_17','id_19','id_20'])\n",
                "train_identity_new.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(train_identity_new)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(8,8))\n",
                "corr = train_identity_new.corr()\n",
                "sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns,annot=True)\n",
                "plt.title(\"correlation plot for train_identity_new\",size=28)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "y=train_identity_new.id_01.value_counts().index\n",
                "x=train_identity_new.id_01.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0],orient='h')\n",
                "ax[0,0].set_title(\"Bar chart for id_01\",size=20)\n",
                "ax[0,0].set_xlabel('counts',size=18)\n",
                "ax[0,0].set_ylabel('')\n",
                "\n",
                "y=train_identity_new.id_12.value_counts().index\n",
                "x=train_identity_new.id_12.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"Bar chart for id_12\",size=20)\n",
                "ax[0,1].set_xlabel('counts',size=18)\n",
                "ax[0,1].set_ylabel('')\n",
                "\n",
                "y=train_identity_new.id_38.value_counts().index\n",
                "x=train_identity_new.id_38.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[1,0],order=['T','F'])\n",
                "ax[1,0].set_title(\"Bar chart for id_38\",size=20)\n",
                "ax[1,0].set_xlabel('counts',size=18)\n",
                "ax[1,0].set_ylabel('')\n",
                "\n",
                "y=train_identity_new.id_37.value_counts().index\n",
                "x=train_identity_new.id_37.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[1,1],order=['T','F'])\n",
                "ax[1,1].set_title(\"Bar chart for id_37\",size=20)\n",
                "ax[1,1].set_xlabel('counts',size=18)\n",
                "ax[1,1].set_ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "y=train_identity_new.id_36.value_counts().index\n",
                "x=train_identity_new.id_36.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,0],order=['T','F'])\n",
                "ax[0,0].set_title(\"Bar chart for id_36\",size=20)\n",
                "ax[0,0].set_xlabel('counts',size=18)\n",
                "ax[0,0].set_ylabel('')\n",
                "\n",
                "y=train_identity_new.id_35.value_counts().index\n",
                "x=train_identity_new.id_35.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[0,1],order=['T','F'])\n",
                "ax[0,1].set_title(\"Bar chart for id_35\",size=20)\n",
                "ax[0,1].set_xlabel('counts',size=18)\n",
                "ax[0,1].set_ylabel('')\n",
                "\n",
                "y=train_identity_new.id_15.value_counts().index\n",
                "x=train_identity_new.id_15.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"Bar chart for id_15\",size=20)\n",
                "ax[1,0].set_xlabel('counts',size=18)\n",
                "ax[1,0].set_ylabel('')\n",
                "\n",
                "y=train_identity_new.id_29.value_counts().index\n",
                "x=train_identity_new.id_29.value_counts()\n",
                "sns.barplot(x=x,y=y,ax=ax[1,1])\n",
                "ax[1,1].set_title(\"Bar chart for id_29\",size=20)\n",
                "ax[1,1].set_xlabel('counts',size=18)\n",
                "ax[1,1].set_ylabel('')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "del train_identity\n",
                "del test_identity\n",
                "train_transaction.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(train_transaction)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(18,70))\n",
                "sns.barplot(x=train_transaction.isnull().sum().sort_values(ascending=False),y=train_transaction.isnull().sum().sort_values(ascending=False).index)\n",
                "plt.title(\"counts of missing value for train_transaction\",size=20)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "\n",
                "train_transaction_new=pd.DataFrame(train_transaction,columns=train_transaction.isnull().sum().sort_values()[:250].index)\n",
                "train_transaction_new=train_transaction_new.drop(columns=['TransactionID'])\n",
                "train_transaction_new_label=train_transaction_new.isFraud\n",
                "train_transaction_new=train_transaction_new.drop(columns=['isFraud'])\n",
                "train_transaction_new.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(train_transaction_new)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test_transaction.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(test_transaction)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(18,70))\n",
                "sns.barplot(x=test_transaction.isnull().sum().sort_values(ascending=False),y=test_transaction.isnull().sum().sort_values(ascending=False).index)\n",
                "plt.title(\"counts of missing value for test_transaction\",size=20)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "test_transaction_new=pd.DataFrame(test_transaction,columns=train_transaction.isnull().sum().sort_values()[:250].index)\n",
                "del test_transaction\n",
                "del train_transaction\n",
                "ID=test_transaction_new.TransactionID\n",
                "test_transaction_new=test_transaction_new.drop(columns=['TransactionID','isFraud'])\n",
                "test_transaction_new.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(test_transaction_new)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import LabelEncoder\n",
                "labelencoder = LabelEncoder()\n",
                "for i in list(train_transaction_new.select_dtypes(include=['object']).columns):\n",
                "    test_transaction_new[i] = labelencoder.fit_transform(test_transaction_new[i].astype('str'))\n",
                "    train_transaction_new[i] = labelencoder.fit_transform(train_transaction_new[i].astype('str'))\n",
                "test_transaction_new.ProductCD[:5]\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_transaction_new.ProductCD[:5]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#train_transaction_new=train_transaction_new.fillna(-999)\n",
                "#test_transaction_new=test_transaction_new.fillna(-999)\n",
                "train_transaction_new=train_transaction_new.fillna(train_transaction_new.median())\n",
                "test_transaction_new=test_transaction_new.fillna(train_transaction_new.median())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "from sklearn.linear_model import LogisticRegression  \n",
                "from sklearn.preprocessing import StandardScaler  \n",
                "from sklearn.model_selection import train_test_split\n",
                "X_train,X_test,y_train,y_test=train_test_split(train_transaction_new,train_transaction_new_label,test_size=0.2)\n",
                "del train_transaction_new\n",
                "lr = LogisticRegression(C=0.09,solver='lbfgs')  \n",
                "lr.fit(X_train, y_train)  \n",
                "proba_test = lr.predict_proba(X_test)[:, 1]\n",
                "LR_result=pd.DataFrame({'pred':proba_test,'real':y_test})\n",
                "LR_result['pred_0_1']=LR_result.pred.apply(lambda x:1 if x>=0.5 else 0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('LR_acc: ',sum(LR_result.real==LR_result.pred_0_1)/len(LR_result))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "import lightgbm as lgb  \n",
                "import pickle  \n",
                "from sklearn.metrics import roc_auc_score  \n",
                "lgb_train = lgb.Dataset(X_train, y_train)  \n",
                "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train) \n",
                "params = {  \n",
                "    'boosting_type': 'gbdt',  \n",
                "    'objective': 'binary',  \n",
                "    'metric': {'binary_logloss', 'auc'},  \n",
                "    'num_leaves':240,  \n",
                "    'max_depth': 15,  \n",
                "    'min_data_in_leaf': 100,  \n",
                "    'learning_rate': 0.05,  \n",
                "    'feature_fraction': 0.95,  \n",
                "    'bagging_fraction': 0.95,  \n",
                "    'bagging_freq': 5,  \n",
                "    'lambda_l1': 0,    \n",
                "    'lambda_l2': 0, \n",
                "    'min_gain_to_split': 0.1,  \n",
                "    'verbose': 0,  \n",
                "    'is_unbalance': True  \n",
                "}  "
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "gbm = lgb.train(params,  lgb_train,  \n",
                "                num_boost_round=10000,  \n",
                "                valid_sets=lgb_eval,early_stopping_rounds=500)  "
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "gbm.predict(test_transaction_new[:10], num_iteration=gbm.best_iteration) "
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "LR_TEST=lr.predict_proba(test_transaction_new)[:, 1]\n",
                "LGBM_TEST= gbm.predict(test_transaction_new, num_iteration=gbm.best_iteration) \n",
                "\n",
                "prediction=pd.DataFrame({'TransactionID':ID,'LR_TEST':LR_TEST,'LGBM_TEST':LGBM_TEST})\n",
                "\n",
                "prediction.to_csv('prediction.csv',index=False)\n",
                "\n",
                "submission=pd.DataFrame({'TransactionID':ID,'isFraud':LGBM_TEST})\n",
                "\n",
                "\n",
                "submission.to_csv('submission.csv',index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from keras.utils import np_utils\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Convolution2D,Dense,MaxPool2D,Activation,Dropout,Flatten\n",
                "from keras.layers import GlobalAveragePooling2D\n",
                "from keras.optimizers import Adam\n",
                "from sklearn.model_selection import train_test_split\n",
                "from keras.layers.normalization import BatchNormalization\n",
                "import os \n",
                "import pandas as pd\n",
                "import plotly.graph_objs as go\n",
                "import matplotlib.ticker as ticker\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "import cv2\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/plant-pathology-2020-fgvc7')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results",
                "process_data"
            ],
            "source": [
                "train=pd.read_csv(\"../input/plant-pathology-2020-fgvc7/train.csv\")\n",
                "test=pd.read_csv(\"../input/plant-pathology-2020-fgvc7/test.csv\")\n",
                "train['image_id']=train['image_id']+'.jpg'\n",
                "test['image_id']=test['image_id']+'.jpg'\n",
                "train.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,2,figsize=(20,20))\n",
                "sns.barplot(y=train.healthy.value_counts(),x=train.healthy.value_counts().index,ax=ax[0,0])\n",
                "ax[0,0].set_title(\"Value count for healthy\",size=20)\n",
                "ax[0,0].set_xlabel('healthy',size=18)\n",
                "ax[0,0].set_ylabel('',size=18)\n",
                "\n",
                "sns.barplot(y=train.multiple_diseases.value_counts(),x=train.multiple_diseases.value_counts().index,ax=ax[0,1])\n",
                "ax[0,1].set_title(\"Value count for multiple_diseases\",size=20)\n",
                "ax[0,1].set_xlabel('multiple_diseases',size=18)\n",
                "ax[0,1].set_ylabel('',size=18)\n",
                "\n",
                "sns.barplot(y=train.rust.value_counts(),x=train.rust.value_counts().index,ax=ax[1,0])\n",
                "ax[1,0].set_title(\"Value count for rust\",size=20)\n",
                "ax[1,0].set_xlabel('rust',size=18)\n",
                "ax[1,0].set_ylabel('',size=18)\n",
                "\n",
                "sns.barplot(y=train.scab.value_counts(),x=train.scab.value_counts().index,ax=ax[1,1])\n",
                "ax[1,1].set_title(\"Value count for scab\",size=20)\n",
                "ax[1,1].set_xlabel('healthy',size=18)\n",
                "ax[1,1].set_ylabel('',size=18)\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "img=[]\n",
                "filename=train.image_id\n",
                "for file in filename:\n",
                "    image=cv2.imread(\"../input/plant-pathology-2020-fgvc7/images/\"+file)\n",
                "    res=cv2.resize(image,(256,256))\n",
                "    img.append(res)\n",
                "img=np.array(img)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(img.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,15))\n",
                "for i in range(9):\n",
                "    plt.subplot(3,3,i+1)\n",
                "    plt.imshow(img[i])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_labels = np.float32(train.loc[:, 'healthy':'scab'].values)\n",
                "\n",
                "train, val = train_test_split(train, test_size = 0.15)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "train_datagen = ImageDataGenerator( horizontal_flip=True,\n",
                "    vertical_flip=True,\n",
                "    rotation_range=10,\n",
                "    width_shift_range=0.1,\n",
                "    height_shift_range=0.1,\n",
                "    zoom_range=.1,\n",
                "    fill_mode='nearest',\n",
                "    shear_range=0.1,\n",
                "    rescale=1/255,\n",
                "    brightness_range=[0.5, 1.5])\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train_generator=train_datagen.flow_from_dataframe(train,directory='/kaggle/input/plant-pathology-2020-fgvc7/images/',\n",
                "                                                      target_size=(384,384),\n",
                "                                                      x_col=\"image_id\",\n",
                "                                                      y_col=['healthy','multiple_diseases','rust','scab'],\n",
                "                                                      class_mode='raw',\n",
                "                                                      shuffle=False,\n",
                "                                                       subset='training',\n",
                "                                                      batch_size=32)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "val_generator=train_datagen.flow_from_dataframe(val,directory='/kaggle/input/plant-pathology-2020-fgvc7/images/',\n",
                "                                                      target_size=(384,384),\n",
                "                                                      x_col=\"image_id\",\n",
                "                                                      y_col=['healthy','multiple_diseases','rust','scab'],\n",
                "                                                      class_mode='raw',\n",
                "                                                      shuffle=False,\n",
                "                                                      batch_size=32,\n",
                "                                                  )"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "test_generator=train_datagen.flow_from_dataframe(test,directory='/kaggle/input/plant-pathology-2020-fgvc7/images/',\n",
                "                                                      target_size=(384,384),\n",
                "                                                      x_col=\"image_id\",\n",
                "                                                      y_col=None,\n",
                "                                                      class_mode=None,\n",
                "                                                      shuffle=False,\n",
                "                                                      batch_size=32)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "check_results"
            ],
            "source": [
                "from keras.applications.resnet50 import ResNet50\n",
                "from keras.models import Model\n",
                "import keras\n",
                "from keras import optimizers\n",
                "model_finetuned = ResNet50(include_top=False, weights='imagenet', input_shape=(384,384,3))\n",
                "x = model_finetuned.output\n",
                "x = GlobalAveragePooling2D()(x)\n",
                "x = Dense(128, activation=\"relu\")(x)\n",
                "x = Dense(64, activation=\"relu\")(x)\n",
                "predictions = Dense(4, activation=\"softmax\")(x)\n",
                "model_finetuned = Model(inputs=model_finetuned.input, outputs=predictions)\n",
                "model_finetuned.compile(optimizer='adam',\n",
                "                  loss = 'categorical_crossentropy',\n",
                "                  metrics=['accuracy'])\n",
                "model_finetuned.summary()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from keras.callbacks import ReduceLROnPlateau"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "history_1 = model_finetuned.fit_generator(train_generator,                                    \n",
                "                                  steps_per_epoch=100, \n",
                "                                  epochs=25,validation_data=val_generator,validation_steps=100\n",
                "                                  ,verbose=1,callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.3,patience=3, min_lr=0.000001)],use_multiprocessing=False,\n",
                "               shuffle=True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = go.Figure(data=[\n",
                "    go.Line(name='train_acc', x=history_1.epoch, y=history_1.history['accuracy']),\n",
                "    go.Line(name='Val_acc', x=history_1.epoch, y=history_1.history['val_accuracy'])])\n",
                "\n",
                "fig.update_layout(\n",
                "    title=\"Accuracy\",\n",
                "    xaxis_title=\"epoch\",\n",
                "    yaxis_title=\"accuracy\",\n",
                "    font=dict(\n",
                "        family=\"Courier New, monospace\",\n",
                "        size=18,\n",
                "        color=\"#7f7f7f\"\n",
                "    ))\n",
                "fig"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "SUB_PATH = \"../input/plant-pathology-2020-fgvc7/sample_submission.csv\"\n",
                "\n",
                "sub = pd.read_csv(SUB_PATH)\n",
                "probs_RESNET = model_finetuned.predict(test_generator, verbose=1)\n",
                "sub.loc[:, 'healthy':] = probs_RESNET\n",
                "sub.to_csv('submission_RESNET.csv', index=False)\n",
                "sub.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data"
            ],
            "source": [
                "import os\n",
                "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
                "import sys\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow as tf\n",
                "import seaborn as sns\n",
                "from PIL import Image\n",
                "from sklearn.model_selection import train_test_split\n",
                "train_dir = '../input/severstal-steel-defect-detection/' \n",
                "train_image_dir = os.path.join(train_dir, 'train_images') \n",
                "train = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
                "train['ClassId_EncodedPixels'] = train.apply(lambda row: (row['ClassId'], row['EncodedPixels']), axis = 1)\n",
                "grouped_EncodedPixels = train.groupby('ImageId')['ClassId_EncodedPixels'].apply(list)\n",
                "img_h=256\n",
                "img_w=256\n",
                "k_size=3\n",
                "batch_size=10\n",
                "epochs=1\n",
                "train=train.dropna(subset=['EncodedPixels'])\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def rle2mask(rle,img):\n",
                "\twidth=img.shape[0]\n",
                "\tlength=img.shape[1]\n",
                "\tmask= np.zeros(width*length).astype(np.uint8)\n",
                "\trle=rle.split()\n",
                "\tstarts = rle[0::2]\n",
                "\tlengths = rle[1::2]\n",
                "\tfor i in range(len(starts)):\n",
                "\t\tmask[int(starts[i]):(int(starts[i])+int(lengths[i]))]=1\n",
                "\treturn np.flipud(np.rot90(mask.reshape(length, width), k=1 ) )\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(len(train.ImageId))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(10,10))\n",
                "sns.barplot(x=train.ClassId.value_counts().index,y=train.ClassId.value_counts())\n",
                "plt.ylabel('')\n",
                "plt.xlabel('ClassId')\n",
                "plt.title(\"Number of images for each class\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "fig=plt.figure(figsize=(20,10))\n",
                "col=2\n",
                "row=5\n",
                "for i in range(1,11):\n",
                "\tfig.add_subplot(row,col,i)\n",
                "\tGraph=train['ImageId'][i]\n",
                "\timg_new=cv2.imread(\"../input/severstal-steel-defect-detection/train_images/\"+Graph)\n",
                "\timg_new= cv2.cvtColor(img_new,cv2.COLOR_BGR2RGB)\n",
                "\tmask = rle2mask(train['EncodedPixels'].iloc[i], img_new)\n",
                "\timg_new[mask==1,0] = 255\n",
                "\tplt.imshow(img_new)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def mask2rle(img):\n",
                "\timg_flt= img.T.flatten()\n",
                "\timg_flt= np.concatenate([[0],img_flt,[0]]) \n",
                "\truns = np.where(img_flt[1:] != img_flt[:-1])[0]  \n",
                "\truns[1::2] -= runs[::2]\n",
                "\treturn ' '.join(str(x) for x in runs)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def ResUNet(img_h,img_w):\n",
                "\tf=[16,32,64,128,256]\n",
                "\tinputs=Input((img_h,img_w,1))\n",
                "\n",
                "\te0=inputs\n",
                "\te1=stem(e0,f[0])\n",
                "\te2=residual_block(e1,f[1],strides=2)\n",
                "\te3=residual_block(e2,f[2],strides=2)\n",
                "\te4=residual_block(e3,f[3],strides=2)\n",
                "\te5=residual_block(e4,f[4],strides=2)\n",
                "\n",
                "\tb0=conv_block(e5,f[4],strides=1)\n",
                "\tb1=conv_block(b0,f[4],strides=1)\n",
                "\n",
                "\tu1=upsample_concat_block(b1,e4)\n",
                "\td1=residual_block(u1,f[4])\n",
                "\n",
                "\tu2=upsample_concat_block(d1,e3)\n",
                "\td2=residual_block(u2,f[3])\n",
                "\n",
                "\tu3=upsample_concat_block(d2,e2)\n",
                "\td3=residual_block(u3,f[2])\n",
                "\n",
                "\tu4=upsample_concat_block(d3,e1)\n",
                "\td4=residual_block(u4,f[1])\n",
                "\n",
                "\toutputs=tf.keras.layers.Conv2D(4,(1,1),padding='same',activation='sigmoid')(d4)\n",
                "\tmodel=tf.keras.models.Model(inputs,outputs)\n",
                "\treturn model\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def dsc(y_true, y_pred):\n",
                "    smooth = 1.\n",
                "    y_true_f = tf.keras.layers.Flatten()(y_true)\n",
                "    y_pred_f = tf.keras.layers.Flatten()(y_pred)\n",
                "    intersection = reduce_sum(y_true_f * y_pred_f)\n",
                "    score = (2. * intersection + smooth) / (reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth)\n",
                "    return score\n",
                "\n",
                "def dice_loss(y_true, y_pred):\n",
                "    loss = 1 - dsc(y_true, y_pred)\n",
                "    return loss\n",
                "\n",
                "def bce_dice_loss(y_true, y_pred):\n",
                "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
                "    return loss\n",
                "def tversky(y_true, y_pred, smooth=1e-6):\n",
                "    y_true_pos = tf.keras.layers.Flatten()(y_true)\n",
                "    y_pred_pos = tf.keras.layers.Flatten()(y_pred)\n",
                "    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)\n",
                "    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n",
                "    false_pos = tf.reduce_sum((1-y_true_pos)*y_pred_pos)\n",
                "    alpha = 0.7\n",
                "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
                "\n",
                "def tversky_loss(y_true, y_pred):\n",
                "    return 1 - tversky(y_true,y_pred)\n",
                "\n",
                "def focal_tversky_loss(y_true,y_pred):\n",
                "    pt_1 = tversky(y_true, y_pred)\n",
                "    gamma = 0.75\n",
                "    return tf.keras.backend.pow((1-pt_1), gamma)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model = ResUNet(img_h=img_h, img_w=img_w)\n",
                "adam = tf.keras.optimizers.Adam(lr = 0.05, epsilon = 0.1)\n",
                "model.compile(optimizer=adam, loss=focal_tversky_loss, metrics=[tversky])"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import seaborn as sns\n",
                "import math\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(\"../input/house-prices-advanced-regression-techniques\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n",
                "test=pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "train"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(8,8))\n",
                "train.isnull().sum().sort_values(ascending=False)[:19].sort_values().plot.barh(color='plum')\n",
                "plt.title('counts of missing value in the train data',size=20)\n",
                "plt.xlabel('counts')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,15))\n",
                "corr = train.corr()\n",
                "sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns)\n",
                "plt.title(\"correlation plot\",size=28)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "train.MSSubClass.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 MSSubClass by counts\",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=train.MSSubClass.value_counts()\n",
                "groups=list(train.MSSubClass.value_counts().index)[:10]\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.15,0.8)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(2,1,figsize=(15,15))\n",
                "sns.boxplot(x=\"MSSubClass\", y=\"SalePrice\", data=train,ax=ax[0])\n",
                "ax[0].set_title(\"Boxplot of Price for MSSubClass\",size=20)\n",
                "\n",
                "train=train[train.SalePrice<=400000]\n",
                "sns.boxplot(x=\"MSSubClass\", y=\"SalePrice\", data=train,ax=ax[1])\n",
                "ax[1].set_title(\"Boxplot of Price for MSSubClass(price<=400000)\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "train.MSZoning.value_counts().sort_values(ascending=False).plot(kind='bar',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"bar chart for MSZoning\",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "ax[0].tick_params(axis='x',rotation=360)\n",
                "\n",
                "sns.boxplot(x=\"MSZoning\", y=\"SalePrice\", data=train,ax=ax[1])\n",
                "ax[1].set_title(\"Boxplot of Price for MSZoning\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "train.Neighborhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 Neighborhood by counts\",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=train.Neighborhood.value_counts()\n",
                "groups=list(train.Neighborhood.value_counts().index)[:10]\n",
                "counts=list(count[:10])\n",
                "counts.append(count.agg(sum)-count[:10].agg('sum'))\n",
                "groups.append('Other')\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.15,0.8)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(figsize=(25,15))\n",
                "sns.boxplot(x=\"Neighborhood\", y=\"SalePrice\", data=train,ax=ax)\n",
                "ax.set_title(\"Boxplot of Price for Neighborhood\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "train.groupby(['OverallQual'])['Id'].agg('count').plot(kind='bar',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 OverallQual by counts\",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=train.groupby(['OverallQual'])['Id'].agg('count')\n",
                "groups=list(train.groupby(['OverallQual'])['Id'].agg('count').index)\n",
                "counts=list(count)\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=(\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.20,0.8)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(figsize=(25,15))\n",
                "sns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=train,ax=ax)\n",
                "ax.set_title(\"Boxplot of Price for OverallQual\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(1,2,figsize=(15,8))\n",
                "clr = (\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "train.groupby(['OverallCond'])['Id'].agg('count').plot(kind='bar',color=clr,ax=ax[0])\n",
                "ax[0].set_title(\"Top 10 OverallCond by counts\",size=20)\n",
                "ax[0].set_xlabel('counts',size=18)\n",
                "\n",
                "\n",
                "count=train.groupby(['OverallCond'])['Id'].agg('count')\n",
                "groups=list(train.groupby(['OverallCond'])['Id'].agg('count').index)\n",
                "counts=list(count)\n",
                "type_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\n",
                "clr1=(\"blue\", \"forestgreen\", \"gold\", \"red\", \"purple\",'cadetblue','hotpink','orange','darksalmon','brown')\n",
                "qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.20,0.8)) \n",
                "plt.subplots_adjust(wspace =0.5, hspace =0)\n",
                "plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(figsize=(25,15))\n",
                "sns.boxplot(x=\"OverallCond\", y=\"SalePrice\", data=train,ax=ax)\n",
                "ax.set_title(\"Boxplot of Price for OverallCond\",size=20)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "data_tree=train[['MSSubClass','MSZoning','Neighborhood','OverallQual','OverallCond','SalePrice']]\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "labelencoder = LabelEncoder()\n",
                "data_tree['MSZoning_new'] = labelencoder.fit_transform(data_tree['MSZoning'])\n",
                "data_tree['Neighborhood_new'] = labelencoder.fit_transform(data_tree['Neighborhood'])\n",
                "data_tree.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "x_train,x_test,y_train,y_test=train_test_split(data_tree[['MSSubClass','MSZoning_new','Neighborhood_new','OverallQual','OverallCond']],data_tree[['SalePrice']],test_size=0.1,random_state=300)\n",
                "tree=DecisionTreeRegressor(criterion='mse',max_depth=4,random_state=0)\n",
                "tree=tree.fit(x_train,y_train)\n",
                "y=y_test['SalePrice']\n",
                "predict=tree.predict(x_test)\n",
                "print(np.mean(abs(np.multiply(np.array(y_test.T-predict),np.array(1/y_test)))))"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_tree_for_test=test[['Id','MSSubClass','MSZoning','Neighborhood','OverallQual','OverallCond']]\n",
                "data_tree_for_test.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_tree_for_test[data_tree_for_test.MSZoning.isnull()==True]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_tree_for_test.MSZoning.value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_tree_for_test.MSZoning[[455,756,790,1444]]='RL'"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import LabelEncoder\n",
                "labelencoder = LabelEncoder()\n",
                "data_tree_for_test['MSZoning_new'] = labelencoder.fit_transform(data_tree_for_test['MSZoning'])\n",
                "data_tree_for_test['Neighborhood_new'] = labelencoder.fit_transform(data_tree_for_test['Neighborhood'])\n",
                "data_tree_for_test.head()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "predict_test=tree.predict(data_tree_for_test[['MSSubClass','MSZoning_new','Neighborhood_new','OverallQual','OverallCond']])\n",
                "submit=pd.DataFrame({'Id':data_tree_for_test.Id,'SalePrice':predict_test})\n",
                "submit.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submit.to_csv('submission.csv',index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import lxml\n",
                "import os\n",
                "import urllib\n",
                "import sys\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import cv2\n",
                "import csv\n",
                "import multiprocessing\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir('../input/iwildcam-2019-fgvc6')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train=pd.read_csv(\"../input/iwildcam-2019-fgvc6/train.csv\")\n",
                "test=pd.read_csv(\"../input/iwildcam-2019-fgvc6/test.csv\")\n",
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(os.listdir('../input/iwildcam-2019-fgvc6/train_images'))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(train.id)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "img=[]\n",
                "filename=train.id[:10000]\n",
                "label=train.category_id[:10000]\n",
                "for file in filename:\n",
                "    image=cv2.imread(\"../input/iwildcam-2019-fgvc6/train_images/\"+file+'.jpg')\n",
                "    res=cv2.resize(image,(32,32))\n",
                "    img.append(res)\n",
                "img=np.array(img)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,15))\n",
                "for i in range(9):\n",
                "    plt.subplot(3,3,i+1)\n",
                "    plt.imshow(img[i])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "np.random.seed(921)\n",
                "from keras.utils import np_utils\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Convolution2D,Dense,MaxPool2D,Activation,Dropout,Flatten\n",
                "from keras.optimizers import Adam\n",
                "from sklearn.model_selection import train_test_split\n",
                "from keras.layers.normalization import BatchNormalization\n",
                "X_train,X_test,y_train,y_test=train_test_split(img,label,test_size=0.2)\n",
                "del img\n",
                "y_train=y_train.astype(int)\n",
                "y_test=y_test.astype(int)\n",
                "y_train=np.array(y_train).reshape(-1,1)\n",
                "y_test=np.array(y_test).reshape(-1,1)\n",
                "X_train=X_train.reshape(-1,32,32,3)/255 #Normalize\n",
                "X_test=X_test.reshape(-1,32,32,3)/255\n",
                "y_train=np_utils.to_categorical(y_train,num_classes=max(label)+1)\n",
                "y_test=np_utils.to_categorical(y_test,num_classes=max(label)+1)"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model=Sequential()\n",
                "model.add(Convolution2D(filters=32,kernel_size=(3,3),input_shape=(32,32,3),padding='same'))\n",
                "model.add(BatchNormalization())\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(rate=0.35))\n",
                "\n",
                "model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n",
                "\n",
                "model.add(Convolution2D(filters=64,kernel_size=(3,3),padding='same'))\n",
                "model.add(BatchNormalization())\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(rate=0.45))\n",
                "\n",
                "model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n",
                "\n",
                "model.add(Flatten())\n",
                "\n",
                "model.add(Dense(1024,activation='relu'))\n",
                "model.add(BatchNormalization())\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(rate=0.75))\n",
                "\n",
                "model.add(Dense(max(label)+1,activation='softmax'))\n",
                "\n",
                "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
                "\n",
                "train_history=model.fit(X_train,y_train,validation_split=0.2,epochs=20,batch_size=128,verbose=1)\n",
                "accuracy=model.evaluate(X_test,y_test,verbose=1)\n",
                "print(\"test accuracy:\",accuracy[1])#accuracy for test set\n",
                "\n",
                "\n",
                "\n",
                "def show_train_history(train_history,train,validation):\n",
                "\tplt.plot(train_history.history[train])\n",
                "\tplt.plot(train_history.history[validation])\n",
                "\tplt.title('Train History')\n",
                "\tplt.ylabel('train')\n",
                "\tplt.xlabel('Epoch')\n",
                "\tplt.legend(['train','validation'],loc='upper left')\n",
                "\tplt.show()\n",
                "\n",
                "show_train_history(train_history,'acc','val_acc') #acc:accuracy for training set. val_acc:accuracy for validation."
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "prediction=model.predict_classes(X_test)\n",
                "print(prediction[0:10])"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "img_test=[]\n",
                "filename_test=test.id[:10000]\n",
                "for file in filename_test:\n",
                "    image=cv2.imread(\"../input/iwildcam-2019-fgvc6/test_images/\"+file+'.jpg')\n",
                "    res=cv2.resize(image,(32,32))\n",
                "    img_test.append(res)\n",
                "img_test=np.array(img_test)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "prediction=model.predict_classes(img_test)\n",
                "print(prediction[0:10])"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submit=pd.DataFrame({'Id':filename_test,'Predicted':prediction})\n",
                "submit.to_csv('submission.csv',index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt \n",
                "import seaborn as sns\n",
                "import os\n",
                "import geopandas as gpd\n",
                "import folium\n",
                "from folium import plugins\n",
                "import datetime \n",
                "import re"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(\"../input/autotel-shared-car-locations\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "check_results"
            ],
            "source": [
                "data=pd.read_csv(\"../input/autotel-shared-car-locations/sample_table.csv\")\n",
                "data.head()\n",
                "data=data[data['total_cars']>0]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data['date']=data.timestamp.apply(lambda x:x.split(' ')[0])\n",
                "pd.to_datetime(data['date'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(12,12))\n",
                "y=data.groupby('date')['total_cars'].agg('sum').index\n",
                "x=data.groupby('date')['total_cars'].agg('sum')\n",
                "sns.barplot(x=x,y=y)\n",
                "plt.title(\"total_car in the data by date\",size=24)\n",
                "plt.xlabel('cars')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "index=data.groupby('date')['timestamp'].agg('count').index\n",
                "plt.figure(figsize=(15,100))\n",
                "for i in range(len(index)):\n",
                "    plt.subplot(15,2,i+1)\n",
                "    sns.scatterplot(x='latitude',y='longitude',alpha=0.01,data=data[data.date==index[i]])\n",
                "    plt.title('total_car '+index[i],size=20)\n",
                "    plt.xlim(32,32.17)\n",
                "    plt.ylim(34.74,34.85)\n",
                "    "
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data.carsList.apply(lambda x:x.strip('[]'))\n",
                "data['carsList']=data.carsList.apply(lambda x:x.strip('[]'))\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_37=data[data.carsList.apply(lambda x:re.match('37',x)!=None)]\n",
                "data_37_new=data_37[data.date==data_37.date.unique()[0]].drop_duplicates(subset=['longitude'])\n",
                "for i in range(1,len(data_37.date.unique())):\n",
                "    data_37_concate=data_37[data.date==data_37.date.unique()[i]].drop_duplicates(subset=['longitude'])\n",
                "    data_37_new=pd.concat([data_37_new,data_37_concate],axis=0)\n",
                "\n",
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_37_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "data_37_cars_map=plugins.MarkerCluster().add_to(data_37_map)\n",
                "for lat,lon,label in zip(data_37_new.latitude,data_37_new.longitude,data_37_new.timestamp):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_37_cars_map)\n",
                "data_37_map.add_child(data_37_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_37=data[data.carsList.apply(lambda x:re.match('37',x)!=None)]\n",
                "data_37_new=data_37[data.date==data_37.date.unique()[0]].drop_duplicates(subset=['latitude'])\n",
                "for i in range(1,len(data_37.date.unique())):\n",
                "    data_37_concate=data_37[data.date==data_37.date.unique()[i]].drop_duplicates(subset=['latitude'])\n",
                "    data_37_new=pd.concat([data_37_new,data_37_concate],axis=0)\n",
                "\n",
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_37_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "data_37_cars_map=plugins.MarkerCluster().add_to(data_37_map)\n",
                "for lat,lon,label in zip(data_37_new.latitude,data_37_new.longitude,data_37_new.timestamp):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_37_cars_map)\n",
                "data_37_map.add_child(data_37_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_total_cars_1=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[:2000]\n",
                "data_total_cars_1.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_1_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_1['label']=data_total_cars_1.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_1_cars_map=plugins.MarkerCluster().add_to(data_total_cars_1_map)\n",
                "for lat,lon,label in zip(data_total_cars_1.latitude,data_total_cars_1.longitude,data_total_cars_1.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_1_cars_map)\n",
                "data_total_cars_1_map.add_child(data_total_cars_1_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_total_cars_2=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[2000:4000]\n",
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_2_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_2['label']=data_total_cars_2.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_2_cars_map=plugins.MarkerCluster().add_to(data_total_cars_2_map)\n",
                "for lat,lon,label in zip(data_total_cars_2.latitude,data_total_cars_2.longitude,data_total_cars_2.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_2_cars_map)\n",
                "data_total_cars_2_map.add_child(data_total_cars_2_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_total_cars_3=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[4000:6000]\n",
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_3_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_3['label']=data_total_cars_3.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_3_cars_map=plugins.MarkerCluster().add_to(data_total_cars_3_map)\n",
                "for lat,lon,label in zip(data_total_cars_3.latitude,data_total_cars_3.longitude,data_total_cars_3.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_3_cars_map)\n",
                "data_total_cars_3_map.add_child(data_total_cars_3_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_total_cars_4=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[6000:8000]\n",
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_4_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_4['label']=data_total_cars_4.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_4_cars_map=plugins.MarkerCluster().add_to(data_total_cars_4_map)\n",
                "for lat,lon,label in zip(data_total_cars_4.latitude,data_total_cars_4.longitude,data_total_cars_4.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_4_cars_map)\n",
                "data_total_cars_4_map.add_child(data_total_cars_4_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_total_cars_5=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[8000:-1]\n",
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_5_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_5['label']=data_total_cars_5.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_5_cars_map=plugins.MarkerCluster().add_to(data_total_cars_5_map)\n",
                "for lat,lon,label in zip(data_total_cars_5.latitude,data_total_cars_5.longitude,data_total_cars_5.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_5_cars_map)\n",
                "data_total_cars_5_map.add_child(data_total_cars_5_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_total_cars_6=data[data.total_cars==2].drop_duplicates(subset=['latitude'])\n",
                "data_total_cars_6.head()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_6_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_6['label']=data_total_cars_6.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_6_cars_map=plugins.MarkerCluster().add_to(data_total_cars_6_map)\n",
                "for lat,lon,label in zip(data_total_cars_6.latitude,data_total_cars_6.longitude,data_total_cars_6.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_6_cars_map)\n",
                "data_total_cars_6_map.add_child(data_total_cars_6_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_total_cars_7=data[data.total_cars>=3].drop_duplicates(subset=['latitude'])\n",
                "data_total_cars_7.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "Long=34.78\n",
                "Lat=32.05\n",
                "data_total_cars_7_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "data_total_cars_7['label']=data_total_cars_7.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n",
                "\n",
                "data_total_cars_7_cars_map=plugins.MarkerCluster().add_to(data_total_cars_7_map)\n",
                "for lat,lon,label in zip(data_total_cars_7.latitude,data_total_cars_7.longitude,data_total_cars_7.label):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_7_cars_map)\n",
                "data_total_cars_7_map.add_child(data_total_cars_7_cars_map)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import lxml\n",
                "import os\n",
                "import urllib\n",
                "import sys\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import cv2\n",
                "import csv\n",
                "import multiprocessing\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "##https://bulkresizephotos.com/zh-tw <- This website can change your image to 32*32 pixels\n",
                "new_train=pd.read_csv('../input/landmark-id-from-0-to-499/new_train_id0_499.csv')\n",
                "filename=os.listdir(\"../input/graph-id0-499/landgraphnew_0_499\")\n",
                "filename.sort(key=lambda x:int(x[:-4]))\n",
                "img=[]\n",
                "for file in filename:\n",
                "\timg.append(np.array(Image.open(\"../input/graph-id0-499/landgraphnew_0_499/\"+file)))\n",
                "img=np.array(img)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "new_train.groupby(['landmark_id']).agg('count').sort_values(by='id',ascending=False).style.background_gradient(cmap='Blues')\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "np.random.seed(1337)\n",
                "from keras.utils import np_utils\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Convolution2D,Dense,MaxPool2D,Activation,Dropout,Flatten\n",
                "from keras.optimizers import Adam\n",
                "from sklearn.model_selection import train_test_split\n",
                "from keras.layers.normalization import BatchNormalization\n",
                "X_train,X_test,y_train,y_test=train_test_split(img,new_train['landmark_id'],test_size=0.2)\n",
                "y_train=y_train.astype(int)\n",
                "y_test=y_test.astype(int)\n",
                "y_train=np.array(y_train).reshape(-1,1)\n",
                "y_test=np.array(y_test).reshape(-1,1)\n",
                "X_train=X_train.reshape(-1,32,32,3)/255 #Normalize\n",
                "X_test=X_test.reshape(-1,32,32,3)/255\n",
                "y_train=np_utils.to_categorical(y_train,num_classes=500)\n",
                "y_test=np_utils.to_categorical(y_test,num_classes=500)#landmark_id is from 0 to 499"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model=Sequential()\n",
                "model.add(Convolution2D(filters=32,kernel_size=(3,3),input_shape=(32,32,3),padding='same'))\n",
                "model.add(BatchNormalization())\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(rate=0.35))\n",
                "\n",
                "model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n",
                "\n",
                "model.add(Convolution2D(filters=64,kernel_size=(3,3),padding='same'))\n",
                "model.add(BatchNormalization())\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(rate=0.45))\n",
                "\n",
                "model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n",
                "\n",
                "model.add(Flatten())\n",
                "\n",
                "model.add(Dense(1024,activation='relu'))\n",
                "model.add(BatchNormalization())\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(rate=0.75))\n",
                "\n",
                "model.add(Dense(500,activation='softmax'))\n",
                "\n",
                "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
                "\n",
                "train_history=model.fit(X_train,y_train,validation_split=0.2,epochs=100,batch_size=128,verbose=1)\n",
                "accuracy=model.evaluate(X_test,y_test,verbose=1)\n",
                "print(\"test accuracy:\",accuracy[1])#accuracy for test set\n",
                "\n",
                "\n",
                "\n",
                "def show_train_history(train_history,train,validation):\n",
                "\tplt.plot(train_history.history[train])\n",
                "\tplt.plot(train_history.history[validation])\n",
                "\tplt.title('Train History')\n",
                "\tplt.ylabel('train')\n",
                "\tplt.xlabel('Epoch')\n",
                "\tplt.legend(['train','validation'],loc='upper left')\n",
                "\tplt.show()\n",
                "\n",
                "show_train_history(train_history,'acc','val_acc') #acc:accuracy for training set. val_acc:accuracy for validation."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from matplotlib.pyplot import figure, show\n",
                "from matplotlib.ticker import MaxNLocator\n",
                "import os \n",
                "import time\n",
                "from matplotlib import cm\n",
                "import pylab as pl"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data",
                "process_data"
            ],
            "source": [
                "data=pd.read_csv(\"../input/crimes-in-boston/crime.csv\",encoding=\"gbk\")\n",
                "data=data.loc[(data['Lat']>35)&(data['Long']< -60)] #remove NA from 'Lat' and 'Long'\n",
                "data=data.dropna(subset=[\"STREET\"])\n",
                "columns=['OFFENSE_CODE_GROUP']\n",
                "for j in columns:\n",
                "\tprint(j,data[j].unique())\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(data.isnull().sum())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "count=data['OFFENSE_CODE_GROUP'].value_counts()\n",
                "groups=list(data['OFFENSE_CODE_GROUP'].value_counts().index)[:9]\n",
                "counts=list(count[:9])\n",
                "counts.append(count.agg(sum)-count[:9].agg('sum'))\n",
                "\n",
                "groups.append('other_type')\n",
                "type_dict={\"group\":groups,\n",
                "          \"counts\":counts}\n",
                "type_dict=pd.DataFrame(type_dict)\n",
                "qx = type_dict.plot(kind='pie', figsize=(10,7), y='counts', labels=groups,\n",
                "             autopct='%1.1f%%', pctdistance=0.9, radius=1.2)\n",
                "plt.legend(loc=0, bbox_to_anchor=(0.95,0.6)) \n",
                "\n",
                "plt.title('Top 10 for crime type', weight='bold', size=14,y=1.08)\n",
                "plt.axis('equal')\n",
                "plt.ylabel('')\n",
                "plt.show()\n",
                "plt.clf()\n",
                "plt.close()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(data['OFFENSE_CODE_GROUP'].value_counts())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.bar(index.value_counts().index,index.value_counts())\n",
                "plt.xlabel(\"Crime type\")\n",
                "plt.ylabel(\"Count\")\n",
                "plt.title(\"Counting for Crime type\")\n",
                "plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "Crime_year=pd.Index(data['YEAR'])\n",
                "ax =figure().gca()\n",
                "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
                "ax.bar(Crime_year.value_counts().index,Crime_year.value_counts())\n",
                "plt.xlabel(\"Year\")\n",
                "plt.ylabel(\"Count\")\n",
                "plt.title(\"Counting the number for Crime (Year)\")\n",
                "plt.show(ax)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "\n",
                "data.groupby(['YEAR','MONTH'])['OFFENSE_CODE_GROUP'].agg('count').unstack('YEAR')\n",
                "fig,ax=plt.subplots(figsize=(15,6))\n",
                "data.groupby(['MONTH','YEAR'])['OFFENSE_CODE_GROUP'].agg('count').unstack().plot(ax=ax)\n",
                "plt.title(\"Counting the number for Crime (month)\")\n",
                "plt.grid(True)\n",
                "\n",
                "data.groupby(['YEAR','MONTH'])['OFFENSE_CODE_GROUP'].agg('count').unstack('YEAR')\n",
                "fig,ax=plt.subplots(figsize=(18,12))\n",
                "data.groupby(['MONTH','YEAR'])['OFFENSE_CODE_GROUP'].agg('count').unstack().plot(kind='bar',ax=ax)\n",
                "pl.xticks(rotation=360)\n",
                "plt.title(\"Counting the number for Crime (month)\")\n",
                "plt.grid(True)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "import geopandas as gpd\n",
                "import folium\n",
                "\n",
                "\n",
                "incidents=folium.map.FeatureGroup()\n",
                "\n",
                "#for lat,lon, in zip(data.Lat,data.Long):\n",
                "#\tincidents.add_child(folium.CircleMarker([lat,lon],radius=7,color='yellow',fill=True,fill_color='red',fill_opacity=0.4))\n",
                "\n",
                "Lat=42.3\n",
                "Lon=-71.1\n",
                "#boston_map=folium.Map([Lat,Lon],zoom_start=12)\n",
                "#boston_map.add_child(incidents)\n",
                "#boston_map.save(\"mymap.html\")\n",
                "\n",
                "from folium import plugins\n",
                "\n",
                "data1=data[data['YEAR']==2015][0:2000]\n",
                "filename=\"Crime2015\"\n",
                "boston_map=folium.Map([Lat,Lon],zoom_start=12)\n",
                "incidents2=plugins.MarkerCluster().add_to(boston_map)\n",
                "for lat,lon,label in zip(data1.Lat,data1.Long,data1.OFFENSE_CODE_GROUP):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(incidents2)\n",
                "boston_map.add_child(incidents2)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data1=data[data['YEAR']==2016][0:2000]\n",
                "filename=\"Crime2016\"\n",
                "boston_map=folium.Map([Lat,Lon],zoom_start=12)\n",
                "incidents2=plugins.MarkerCluster().add_to(boston_map)\n",
                "for lat,lon,label in zip(data1.Lat,data1.Long,data1.OFFENSE_CODE_GROUP):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(incidents2)\n",
                "boston_map.add_child(incidents2)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data1=data[data['YEAR']==2017][0:2000]\n",
                "filename=\"Crime2017\"\n",
                "boston_map=folium.Map([Lat,Lon],zoom_start=12)\n",
                "incidents2=plugins.MarkerCluster().add_to(boston_map)\n",
                "for lat,lon,label in zip(data1.Lat,data1.Long,data1.OFFENSE_CODE_GROUP):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(incidents2)\n",
                "boston_map.add_child(incidents2)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data1=data[data['YEAR']==2018][0:2000]\n",
                "filename=\"Crime2018\"\n",
                "boston_map=folium.Map([Lat,Lon],zoom_start=12)\n",
                "incidents2=plugins.MarkerCluster().add_to(boston_map)\n",
                "for lat,lon,label in zip(data1.Lat,data1.Long,data1.OFFENSE_CODE_GROUP):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(incidents2)\n",
                "boston_map.add_child(incidents2)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os\n",
                "import seaborn as sns\n",
                "import math\n",
                "import datetime"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(\"../input/pga-tour-20102018-data/\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data_2019=pd.read_csv(\"../input/pga-tour-20102018-data/2019_data.csv\")\n",
                "data_2019.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(data_2019)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_2019.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data_2019['year']=data_2019.Date.apply(lambda x:x.split('-')[0])\n",
                "data_2019['month']=data_2019.Date.apply(lambda x:x.split('-')[1])\n",
                "data_2019['day']=data_2019.Date.apply(lambda x:x.split('-')[2])\n",
                "data_2019.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_2019.Statistic.unique()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "type_stat=pd.DataFrame({'type':data_2019.Statistic.value_counts().sort_values(ascending=False)[:10].index,'value':data_2019.Statistic.value_counts().sort_values(ascending=False)[:10].values})\n",
                "type_stat"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "\n",
                "qx = type_stat.plot(kind='pie', figsize=(10,7), y='value', labels=list(type_stat.type),autopct='%1.1f%%', pctdistance=0.9, radius=1.2)\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.10,1.0)) \n",
                "plt.title('Top 10 Statistic by number', weight='bold', size=14,y=1.08)\n",
                "plt.axis('equal')\n",
                "plt.ylabel('')\n",
                "plt.show()\n",
                "plt.clf()\n",
                "plt.close()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "DD=data_2019[data_2019.Variable=='Driving Distance - (TOTAL DISTANCE)'].iloc[:,[0,4]]\n",
                "DD['Value_new']=DD['Value'].apply(lambda x:''.join(x.split(',')))\n",
                "DD.Value_new=DD.Value_new.astype(int)\n",
                "DD=DD.drop(columns='Value')\n",
                "plt.figure(figsize=(22,15))\n",
                "plt.subplot(2,2,1)\n",
                "DD.groupby('Player Name')['Value_new'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='darkorange')\n",
                "plt.title(\"Top 10 player by Total Driving Distance\",size=20)\n",
                "plt.xlabel('meters')\n",
                "plt.ylabel('')\n",
                "plt.subplot(2,2,2)\n",
                "DD=data_2019[data_2019.Variable=='Driving Distance - (AVG.)'].iloc[:,[0,4]]\n",
                "DD.Value=DD.Value.astype(float)\n",
                "DD.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='navy')\n",
                "plt.title(\"Top 10 player by Average Driving Distance\",size=20)\n",
                "plt.xlabel('meters')\n",
                "plt.ylabel('')\n",
                "plt.subplot(2,2,3)\n",
                "DD=data_2019[data_2019.Variable=='Driving Distance - (ROUNDS)'].iloc[:,[0,4]]\n",
                "DD.Value=DD.Value.astype(float)\n",
                "DD.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='purple')\n",
                "plt.title(\"Top 10 player by Rounds Driving Distance\",size=20)\n",
                "plt.xlabel('meters')\n",
                "plt.ylabel('')\n",
                "plt.subplot(2,2,4)\n",
                "DD=data_2019[data_2019.Variable=='Driving Distance - (TOTAL DRIVES)'].iloc[:,[0,4]]\n",
                "DD.Value=DD.Value.astype(float)\n",
                "DD.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh()\n",
                "plt.title(\"Top 10 player by TOTAL DRIVES Driving Distance\",size=20)\n",
                "plt.xlabel('meters')\n",
                "plt.ylabel('')\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "DD=data_2019[data_2019.Variable=='Driving Distance - (TOTAL DISTANCE)'].iloc[:,[0,4,6]]\n",
                "DD['Value_new']=DD['Value'].apply(lambda x:''.join(x.split(',')))\n",
                "DD.Value_new=DD.Value_new.astype(int)\n",
                "DD=DD.drop(columns='Value')\n",
                "plt.figure(figsize=(22,22))\n",
                "for i in range(len(DD.month.unique())):\n",
                "    DD_month=DD[DD.month==DD.month.unique()[i]]\n",
                "    plt.subplot(4,2,i+1)\n",
                "    DD_month.groupby('Player Name')['Value_new'].max().sort_values(ascending=False)[:10].sort_values().plot(kind='barh')\n",
                "    plt.title(\"Top 10 player by Total Driving Distance in month\"+DD.month.unique()[i],size=20)\n",
                "    plt.xlabel('meters')\n",
                "    plt.ylabel('')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "TE=data_2019[data_2019.Variable=='Total Eagles - (ROUNDS)'].iloc[:,[0,4]]\n",
                "plt.figure(figsize=(22,15))\n",
                "\n",
                "plt.subplot(1,2,1)\n",
                "TE.Value=TE.Value.astype(int)\n",
                "TE.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='firebrick')\n",
                "plt.title(\"Top 10 player by Total Eagles(Rounds)\",size=20)\n",
                "plt.xlabel('counts')\n",
                "plt.ylabel('')\n",
                "\n",
                "plt.subplot(1,2,2)\n",
                "TE=data_2019[data_2019.Variable=='Total Eagles - (TOTAL)'].iloc[:,[0,4]]\n",
                "TE=TE.dropna(subset=['Value'])\n",
                "TE.Value=TE.Value.astype(int)\n",
                "TE.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='cyan')\n",
                "plt.xticks(np.linspace(0, 20, 5))\n",
                "plt.title(\"Top 10 player by Total Eagles(Total)\",size=20)\n",
                "plt.xlabel('counts')\n",
                "plt.ylabel('')\n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def TOTAvgRank_DD_DAP_TE_TB(player,month):\n",
                "    Ranktype=['Driving Distance - (TOTAL DRIVES)','Driving Accuracy Percentage - (%)','Total Eagles - (TOTAL)','Total Birdies - (TOTAL)']\n",
                "    B={}\n",
                "    for i in range(len(Ranktype)):\n",
                "        A=data_2019[data_2019.Variable==Ranktype[i]]\n",
                "        A=A[A.month==month]\n",
                "        A.dropna(subset=['Value'])\n",
                "        A.Value=A.Value.astype(float)\n",
                "        value=A.groupby(['Player Name'])['Value'].agg('mean')\n",
                "        rank=A.groupby(['Player Name'])['Value'].agg('mean').rank(method='min',ascending=False)\n",
                "        new=pd.DataFrame({'rank':rank,'value':value})\n",
                "        B['type'+str(i)]=str(Ranktype[i])\n",
                "        B['rank'+str(i)]=str(new[new.index==player].iloc[0,0])+\"/\"+str(max(rank))\n",
                "        B['value'+str(i)]=str(new[new.index==player].iloc[0,1])\n",
                "\n",
                "    return B\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(25,35))\n",
                "j=1\n",
                "for k in ['01','02','03','04','05','06','07','08']:\n",
                "    Aaron=TOTAvgRank_DD_DAP_TE_TB('Aaron Baddeley',str(k))\n",
                "    y=[]\n",
                "    x=[]\n",
                "    n=[]\n",
                "    for i in range(4):\n",
                "        r1,r2=Aaron['rank'+str(i)].split('/')\n",
                "        R=float(r1)/float(r2)\n",
                "        R=1-R\n",
                "        y.append(1.5+R*math.sin(math.pi/4+i*math.pi/2))\n",
                "        x.append(1.5+R*math.cos(math.pi/4+i*math.pi/2))\n",
                "        n.append(Aaron['type'+str(i)]+\" rank: \"+Aaron['rank'+str(i)])\n",
                "\n",
                "    x.append(x[0])\n",
                "    y.append(y[0])\n",
                "    plt.subplot(4,2,j)\n",
                "    plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "    for i, txt in enumerate(n):\n",
                "        plt.annotate(txt, (x[i]-0.2, y[i]))\n",
                "    plt.xlim(0.5,2.5)\n",
                "    plt.ylim(0.5,2.5)\n",
                "    plt.fill(x, y,\"coral\")\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"Aaron Baddeley's performance in month \"+str(k),size=18)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(25,35))\n",
                "j=1\n",
                "for k in ['01','02','03','04','05','06','07','08']:\n",
                "    Sungjae=TOTAvgRank_DD_DAP_TE_TB('Sungjae Im',str(k))\n",
                "    y=[]\n",
                "    x=[]\n",
                "    n=[]\n",
                "    for i in range(4):\n",
                "        r1,r2=Sungjae['rank'+str(i)].split('/')\n",
                "        R=float(r1)/float(r2)\n",
                "        R=1-R\n",
                "        y.append(1.5+R*math.sin(math.pi/4+i*math.pi/2))\n",
                "        x.append(1.5+R*math.cos(math.pi/4+i*math.pi/2))\n",
                "        n.append(Sungjae['type'+str(i)]+\" rank: \"+Sungjae['rank'+str(i)])\n",
                "\n",
                "    x.append(x[0])\n",
                "    y.append(y[0])\n",
                "    plt.subplot(4,2,j)\n",
                "    plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n",
                "    for i, txt in enumerate(n):\n",
                "        plt.annotate(txt, (x[i]-0.2, y[i]))\n",
                "    plt.xlim(0.5,2.5)\n",
                "    plt.ylim(0.5,2.5)\n",
                "    plt.fill(x, y,\"greenyellow\")\n",
                "    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n",
                "    plt.title(\"Sungjae Im's performance in month \"+str(k),size=18)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_2019[data_2019.Statistic=='Official World Golf Ranking'].Variable.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "OWGR_TP=data_2019[data_2019.Variable=='Official World Golf Ranking - (TOTAL POINTS)']\n",
                "OWGR_TP.Value=OWGR_TP.Value.astype(float)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2,figsize=(18,8))\n",
                "\n",
                "OWGR_TP.groupby('Player Name')['Value'].mean().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color='brown',ax=axes[0])\n",
                "axes[0].set_title('Top 10 player by Average of Total Points',size=20)\n",
                "axes[0].set_xlabel('points')\n",
                "axes[0].set_ylabel('')\n",
                "\n",
                "OWGE_TP10=OWGR_TP[OWGR_TP['Player Name'].isin(list(OWGR_TP.groupby('Player Name')['Value'].mean().sort_values(ascending=False)[:10].index[:10]))]\n",
                "OWGE_TP10.groupby(['Player Name','Date'])['Value'].agg('mean').unstack('Player Name').plot(ax=axes[1])\n",
                "axes[1].set_title('Official World Golf Ranking - (TOTAL POINTS)',size=20)\n",
                "axes[1].set_ylabel('Total points')\n",
                "axes[1].set_xlim([0,30])\n",
                "plt.xticks([0,5,10,15,20,25,29], ['2019-01-27','2019-03-03','2019-04-07','2019-05-12','2019-06-16','2019-07-21','2019-08-18'], rotation=0)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "import os \n",
                "import seaborn as sns\n",
                "import pylab as pl\n",
                "import geopandas as gpd\n",
                "import folium\n",
                "from folium import plugins\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(os.listdir(\"../input/city-lines/\"))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "cities=pd.read_csv(\"../input/city-lines/cities.csv\")\n",
                "stations=pd.read_csv(\"../input/city-lines/stations.csv\")\n",
                "tracks=pd.read_csv(\"../input/city-lines/tracks.csv\")\n",
                "lines=pd.read_csv(\"../input/city-lines/lines.csv\")\n",
                "track_lines=pd.read_csv(\"../input/city-lines/track_lines.csv\")\n",
                "print(\"cities size:\",len(cities))\n",
                "print(\"stations size:\",len(stations))\n",
                "print(\"tracks size:\",len(tracks))\n",
                "print(\"lines size:\",len(lines))\n",
                "print(\"track_lines size:\",len(track_lines))"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "stations=stations.dropna(subset=['closure','name','opening'])\n",
                "stations=stations[stations.closure>=9999]\n",
                "stations=stations[stations.opening>0]\n",
                "stations=stations[stations.opening<=2030]\n",
                "stations.columns=['id','stations_name','geometry','buildstart','opening','closure','city_id']\n",
                "stations['Long']=stations['geometry'].apply(lambda x: x.split('POINT(')[1].split(' ')[0])\n",
                "stations['Lat']=stations['geometry'].apply(lambda x: x.split('POINT(')[1].split(' ')[1].split(')')[0])\n",
                "id_country=pd.DataFrame({'city_id':cities.id,'country':cities.country,'name':cities.name})\n",
                "\n",
                "stations=pd.merge(stations,id_country)\n",
                "stations.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(18, 6))\n",
                "plt.subplot(1, 2, 1) \n",
                "stations.name.value_counts()[:10].sort_values().plot.barh()\n",
                "plt.title(\"Top 10 city by stations\",size=18)\n",
                "plt.xlabel(\"stations\")\n",
                "plt.subplot(1, 2, 2) \n",
                "stations.country.value_counts()[:10].sort_values().plot.barh()\n",
                "plt.title(\"Top 10 country by stations\",size=18)\n",
                "plt.xlabel(\"stations\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig,ax=plt.subplots(figsize=(10,6))\n",
                "stations.groupby(['opening','country'])['stations_name'].agg('count').unstack().plot(ax=ax)\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.05,1.1))\n",
                "plt.xlabel('')\n",
                "plt.ylabel('stations')\n",
                "plt.title(\"The number of opening stations by year (all country)\",size=18)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(35,55))\n",
                "for i in range(len(stations.country.unique())):\n",
                "    plt.subplot(9,3,i+1)\n",
                "    stations[stations.country==stations.country.unique()[i]].name.value_counts().sort_values().plot.barh()\n",
                "    plt.xlabel('stations')\n",
                "    plt.title(\"The number of stations in \"+stations.country.unique()[i],size=20)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "map_all=folium.Map()\n",
                "stations_new=pd.DataFrame({\"Lat\":stations['Lat'],\"Long\":stations['Long']})\n",
                "map_all.add_child(plugins.HeatMap(data=stations_new))\n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tokyo_lines=lines[lines.city_id==114]\n",
                "tokyo_track_lines=track_lines[track_lines.city_id==114]\n",
                "tokyo_tracks=tracks[tracks.city_id==114].drop(columns=['buildstart','opening','closure','city_id'])\n",
                "tokyo_tracks.columns=['section_id','geometry','length']\n",
                "tokyo_track_lines=pd.merge(tokyo_track_lines,tokyo_tracks)\n",
                "tokyo_track_lines=tokyo_track_lines.drop(columns=['id','created_at','updated_at','city_id'])\n",
                "tokyo_track_lines.columns=['section_id','id','geometry','length']\n",
                "tokyo_lines=pd.merge(tokyo_track_lines,tokyo_lines)\n",
                "tokyo_stations=stations[stations['city_id']==114]\n",
                "tokyo_stations.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "x=[]\n",
                "y=[]\n",
                "z=[]\n",
                "for i in range(len(tokyo_lines)):\n",
                "    sp=tokyo_lines.iloc[i].geometry.split('(')[1].split(')')[0].split(',')\n",
                "    for j in range(len(sp)):\n",
                "        x.append(sp[j].split(' ')[0])\n",
                "        y.append(sp[j].split(' ')[1])\n",
                "        z.append(tokyo_lines.url_name[i])\n",
                "fix=pd.DataFrame({'x':x,'y':y,'z':z})\n",
                "fix['x']=fix['x'].astype(float)\n",
                "fix['y']=fix['y'].astype(float)\n",
                "plt.figure(figsize=(25, 25))\n",
                "plt.subplot(2, 2, 1) \n",
                "ax=sns.scatterplot(x=\"x\", y=\"y\", hue=\"z\",data=fix)\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.05,0.6))\n",
                "plt.title(\"lines for Tokyo\",size=20)\n",
                "ax.get_legend().remove()\n",
                "ax.set(xlabel='Longitude', ylabel='LATITUDE')\n",
                "plt.subplot(2,2,2)\n",
                "(tokyo_lines.groupby(['url_name'])['length'].sum()/1000).sort_values(ascending= False)[:10].sort_values().plot.barh()\n",
                "plt.ylabel(' ')\n",
                "plt.xlabel('length(km)')\n",
                "plt.title(\"Top 10 track by length\",size=20)\n",
                "plt.subplot(2,2,3)\n",
                "tokyo_stations.groupby(['opening'])['id'].agg('count').plot()\n",
                "plt.xlabel(' ')\n",
                "plt.ylabel('stations')\n",
                "plt.title(\"Number of opening stations by year\",size=20)\n",
                "plt.subplot(2,2,4)\n",
                "tokyo_lines.name.value_counts()[:10].sort_values().plot.barh()\n",
                "plt.xlabel('counts')\n",
                "plt.title(\"Top 10 line by number\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "\n",
                "stations_tokyo_2000=stations[stations['city_id']==114][0:2000]\n",
                "Long=139.75\n",
                "Lat=35.67\n",
                "tokyo_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "tokyo_stations_map=plugins.MarkerCluster().add_to(tokyo_map)\n",
                "for lat,lon,label in zip(stations_tokyo_2000.Lat,stations_tokyo_2000.Long,stations_tokyo_2000.stations_name):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(tokyo_stations_map)\n",
                "tokyo_map.add_child(tokyo_stations_map)\n",
                "\n",
                "tokyo_map\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "osaka_lines=lines[lines.city_id==91]\n",
                "osaka_track_lines=track_lines[track_lines.city_id==91]\n",
                "osaka_tracks=tracks[tracks.city_id==91].drop(columns=['buildstart','opening','closure','city_id'])\n",
                "osaka_tracks.columns=['section_id','geometry','length']\n",
                "osaka_track_lines=pd.merge(osaka_track_lines,osaka_tracks)\n",
                "osaka_track_lines=osaka_track_lines.drop(columns=['id','created_at','updated_at','city_id'])\n",
                "osaka_track_lines.columns=['section_id','id','geometry','length']\n",
                "osaka_lines=pd.merge(osaka_track_lines,osaka_lines)\n",
                "osaka_stations=stations[stations['city_id']==91]\n",
                "osaka_stations.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "x=[]\n",
                "y=[]\n",
                "z=[]\n",
                "for i in range(len(osaka_lines)):\n",
                "    sp=osaka_lines.iloc[i].geometry.split('(')[1].split(')')[0].split(',')\n",
                "    for j in range(len(sp)):\n",
                "        x.append(sp[j].split(' ')[0])\n",
                "        y.append(sp[j].split(' ')[1])\n",
                "        z.append(osaka_lines.url_name[i])\n",
                "fix=pd.DataFrame({'x':x,'y':y,'z':z})\n",
                "fix['x']=fix['x'].astype(float)\n",
                "fix['y']=fix['y'].astype(float)\n",
                "plt.figure(figsize=(27, 27))\n",
                "plt.subplot(2, 2, 1) \n",
                "ax=sns.scatterplot(x=\"x\", y=\"y\", hue=\"z\",data=fix)\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.05,0.6))\n",
                "plt.title(\"lines for osaka\",size=20)\n",
                "ax.get_legend().remove()\n",
                "ax.set(xlabel='Longitude', ylabel='LATITUDE')\n",
                "plt.subplot(2,2,2)\n",
                "(osaka_lines.groupby(['url_name'])['length'].sum()/1000).sort_values(ascending= False)[:10].sort_values().plot.barh()\n",
                "plt.ylabel(' ')\n",
                "plt.xlabel('length(km)')\n",
                "plt.title(\"Top 10 track by length\",size=20)\n",
                "plt.subplot(2,2,3)\n",
                "osaka_stations.groupby(['opening'])['id'].agg('count').plot()\n",
                "plt.xlabel(' ')\n",
                "plt.ylabel('stations')\n",
                "plt.title(\"Number of opening stations by year\",size=20)\n",
                "plt.subplot(2,2,4)\n",
                "osaka_lines.name.value_counts()[:10].sort_values().plot.barh()\n",
                "plt.xlabel('counts')\n",
                "plt.title(\"Top 10 line by number\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "\n",
                "stations_osaka=stations[stations['city_id']==91]\n",
                "Long=135.5\n",
                "Lat=34.53\n",
                "osaka_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "osaka_stations_map=plugins.MarkerCluster().add_to(osaka_map)\n",
                "for lat,lon,label in zip(stations_osaka.Lat,stations_osaka.Long,stations_osaka.stations_name):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(osaka_stations_map)\n",
                "osaka_map.add_child(osaka_stations_map)\n",
                "\n",
                "osaka_map"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "New_York_lines=lines[lines.city_id==206]\n",
                "New_York_track_lines=track_lines[track_lines.city_id==206]\n",
                "New_York_tracks=tracks[tracks.city_id==206].drop(columns=['buildstart','opening','closure','city_id'])\n",
                "New_York_tracks.columns=['section_id','geometry','length']\n",
                "New_York_track_lines=pd.merge(New_York_track_lines,New_York_tracks)\n",
                "New_York_track_lines=New_York_track_lines.drop(columns=['id','created_at','updated_at','city_id'])\n",
                "New_York_track_lines.columns=['section_id','id','geometry','length']\n",
                "New_York_lines=pd.merge(New_York_track_lines,New_York_lines)\n",
                "New_York_stations=stations[stations['city_id']==206]\n",
                "New_York_stations.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "x=[]\n",
                "y=[]\n",
                "z=[]\n",
                "for i in range(len(New_York_lines)):\n",
                "    sp=New_York_lines.iloc[i].geometry.split('(')[1].split(')')[0].split(',')\n",
                "    for j in range(len(sp)):\n",
                "        x.append(sp[j].split(' ')[0])\n",
                "        y.append(sp[j].split(' ')[1])\n",
                "        z.append(New_York_lines.url_name[i])\n",
                "fix=pd.DataFrame({'x':x,'y':y,'z':z})\n",
                "fix['x']=fix['x'].astype(float)\n",
                "fix['y']=fix['y'].astype(float)\n",
                "plt.figure(figsize=(27, 27))\n",
                "plt.subplot(2, 2, 1) \n",
                "ax=sns.scatterplot(x=\"x\", y=\"y\", hue=\"z\",data=fix)\n",
                "plt.legend(loc=0, bbox_to_anchor=(1.05,0.6))\n",
                "plt.title(\"lines for New_York\",size=20)\n",
                "ax.get_legend().remove()\n",
                "ax.set(xlabel='Longitude', ylabel='LATITUDE')\n",
                "plt.subplot(2,2,2)\n",
                "(New_York_lines.groupby(['url_name'])['length'].sum()/1000).sort_values(ascending= False)[:10].sort_values().plot.barh()\n",
                "plt.ylabel(' ')\n",
                "plt.xlabel('length(km)')\n",
                "plt.title(\"Top 10 track by length\",size=20)\n",
                "plt.subplot(2,2,3)\n",
                "New_York_stations.groupby(['opening'])['id'].agg('count').plot()\n",
                "plt.xlabel(' ')\n",
                "plt.ylabel('stations')\n",
                "plt.title(\"Number of opening stations by year\",size=20)\n",
                "plt.subplot(2,2,4)\n",
                "New_York_lines.name.value_counts()[:10].sort_values().plot.barh()\n",
                "plt.xlabel('counts')\n",
                "plt.title(\"Top 10 line by number\",size=20)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "stations_New_York=stations[stations['city_id']==206]\n",
                "Long=-73.97\n",
                "Lat=40.78\n",
                "New_York_map=folium.Map([Lat,Long],zoom_start=12)\n",
                "\n",
                "New_York_stations_map=plugins.MarkerCluster().add_to(New_York_map)\n",
                "for lat,lon,label in zip(stations_New_York.Lat,stations_New_York.Long,stations_New_York.stations_name):\n",
                "    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(New_York_stations_map)\n",
                "New_York_map.add_child(New_York_stations_map)\n",
                "\n",
                "New_York_map"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import tensorflow as tf\n",
                "print(tf.__version__)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# Import Libraries\n",
                "import numpy as np\n",
                "\n",
                "import tensorflow as tf\n",
                "import tensorflow.keras as keras\n",
                "from tensorflow.keras.datasets import mnist\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
                "\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "# Load Dataset\n",
                "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(x_train.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "# Show image of training data\n",
                "plt.figure(figsize = (10, 10)) # set size of figure 10x10\n",
                "rand_indexes = np.random.randint(0, x_train.shape[0], 8) # select 8 digits(0~9) randomly \n",
                "print(rand_indexes)\n",
                "for index,im_index in enumerate(rand_indexes):\n",
                "    plt.subplot(4, 4, index+1)\n",
                "    plt.imshow(x_train[im_index], cmap = 'gray', interpolation = 'none')\n",
                "    plt.title('Class %d' % y_train[im_index])\n",
                "plt.tight_layout()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# data convert to floating point\n",
                "#x_train = x_train.astype('float32') / 255\n",
                "x_train = x_train / 255.0\n",
                "#x_test = x_test.astype('float32') / 255\n",
                "x_test = x_test / 255.0\n",
                "print('x_train shape:', x_train.shape)\n",
                "print(x_train.shape[0], 'train samples')\n",
                "print(x_test.shape[0], 'test samples')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(x_test.shape)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "img_rows, img_cols = 28, 28 # input image dimensions\n",
                "\n",
                "if keras.backend.image_data_format() == 'channels_first':\n",
                "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
                "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
                "    input_shape = (1, img_rows, img_cols)\n",
                "else:\n",
                "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
                "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
                "    input_shape = (img_rows, img_cols, 1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "y_train = keras.utils.to_categorical(y_train)\n",
                "y_test = keras.utils.to_categorical(y_test)\n",
                "num_classes = y_train.shape[1]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(y_train.shape)\n",
                "print(y_test.shape)\n",
                "print(num_classes)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "# Build Model\n",
                "model = Sequential()\n",
                "# 1st Conv layer\n",
                "model.add(Conv2D(32, kernel_size = (3, 3), padding = 'same', activation = 'relu', input_shape = input_shape))\n",
                "model.add(MaxPool2D(pool_size = (2, 2)))\n",
                "model.add(Dropout(0.25))\n",
                "# 2nd Conv layer        \n",
                "model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n",
                "model.add(MaxPool2D(pool_size = (2, 2)))\n",
                "model.add(Dropout(0.25))\n",
                "# Fully Connected layer        \n",
                "model.add(Flatten())\n",
                "model.add(Dense(128, activation = 'relu'))\n",
                "model.add(Dropout(0.2))\n",
                "model.add(Dense(num_classes, activation = 'softmax'))\n",
                "\n",
                "model.summary()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# Compile Model\n",
                "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# Train Model\n",
                "num_epochs = 30\n",
                "history = model.fit(x_train, y_train, batch_size = 128, epochs = num_epochs, verbose = 1, validation_data=(x_test, y_test));"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "# Save Model\n",
                "model.save('mnist_cnn.h5')"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "# Evaluate Model\n",
                "score = model.evaluate(x_test, y_test, verbose = 0)\n",
                "print('Test loss: ', score[0])\n",
                "print('Test accuracy: ', score[1])"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "# Show Train History\n",
                "keys=history.history.keys()\n",
                "print(keys)\n",
                "\n",
                "def show_train_history(hisData,train,test): \n",
                "    plt.plot(hisData.history[train])\n",
                "    plt.plot(hisData.history[test])\n",
                "    plt.title('Training History')\n",
                "    plt.ylabel(train)\n",
                "    plt.xlabel('Epoch')\n",
                "    plt.legend(['train', 'test'], loc='upper left')\n",
                "    plt.show()\n",
                "\n",
                "show_train_history(history, 'loss', 'val_loss')\n",
                "show_train_history(history, 'accuracy', 'val_accuracy')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "import math\n",
                "import csv\n",
                "import sys\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "df = pd.read_csv(\"../input/raw-benford-numbers-edited/Raw Benford Numbers.csv\", index_col = 'Unnamed: 0')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#an example of some of our data\n",
                "df.tail()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "## Remove all \"nan\" values for blank cells\n",
                "all = []\n",
                "for x in df:\n",
                "    all.append(df[x].values)\n",
                "\n",
                "cData = []\n",
                "for i in range(0,3):\n",
                "    cleanedList = [x for x in all[i] if str(x) != 'nan']\n",
                "    cData.append(cleanedList)\n",
                "finalData = []\n",
                "for z in range(len(cData)):\n",
                "    for y in cData[z]:\n",
                "        finalData.append(y)\n",
                "#finalData is the cleaned data without nan values, we still have to clear the trailing decimal points and zeroes\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#this for and if loop evaluates if the value ends with '.0' and if it does, the last two digits are removed\n",
                "for val in range(len(finalData)):\n",
                "    if str(finalData[val])[-2:] == '.0':\n",
                "        finalData[val] = str(finalData[val])[:-2]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(finalData)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "second_Digit = secondDigit(finalData)\n",
                "third_Digit = thirdDigit(finalData)\n",
                "first_Digit = firstDigit(finalData)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "Benford_percentiles = pd.DataFrame({\n",
                "    'First Digit Expected': [0, .301, .176, .125, .097, .079, .067, .058, .051, .046],\n",
                "    'Second Digit Expected': [.12, .114, .109, .104, .100, .097, .093, .090, .088, .085],\n",
                "    'Third Digit Expected': [.102, .101, .101, .101, .100, .100, .099, .099, .099, .098]\n",
                "                                    })"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "Benford_percentiles"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#This is just a quick script to seperate out the expected first digit from the rest of the Data Frame as these are on a\n",
                "#scale of 1-9 instead of 0-9 like the second and third digits\n",
                "First_digit_benfords = []\n",
                "for x in Benford_percentiles['First Digit Expected']:\n",
                "    if x > 0:\n",
                "        First_digit_benfords.append(x)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "first_digit_percentile = [(x / sum(first_Digit)) for x in first_Digit]\n",
                "fig = plt.figure()\n",
                "ax = fig.add_axes([0,0,1,1])\n",
                "index = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
                "x = np.arange(len(index))\n",
                "plt.xticks(x, index)\n",
                "ax.plot(x, First_digit_benfords, label= 'Actual')\n",
                "ax.plot(x, first_digit_percentile, label= 'Expected')\n",
                "plt.title('First Digit; Expected values are in blue, actual are in orange')\n",
                "plt.show"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "second_Digit_percentile = [(x / sum(second_Digit)) for x in second_Digit]\n",
                "fig = plt.figure()\n",
                "ax = fig.add_axes([0,0,1,1])\n",
                "index = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
                "x = np.arange(len(index))\n",
                "plt.xticks(x, index)\n",
                "ax.plot(x, Benford_percentiles['Second Digit Expected'])\n",
                "ax.plot(x, second_Digit_percentile)\n",
                "plt.title('Second Digit; Expected values are in blue, actual are in orange')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "third_Digit_percentile = [(x / sum(third_Digit)) for x in third_Digit]\n",
                "fig = plt.figure()\n",
                "ax = fig.add_axes([0,0,1,1])\n",
                "index = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
                "x = np.arange(len(index))\n",
                "plt.xticks(x, index)\n",
                "ax.plot(x, Benford_percentiles['Third Digit Expected'])\n",
                "ax.plot(x, third_Digit_percentile)\n",
                "plt.title('Third Digit; Expected values are in blue, actual are in orange')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#reinitialize these variables before I alter them just to keep the code clean\n",
                "first_digit_percentile = [(x / sum(first_Digit)) for x in first_Digit]\n",
                "first_digit_percentile.insert(0, 0)\n",
                "index = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
                "\n",
                "final_series = {'First Digit Test': first_digit_percentile,\n",
                "                'Second Digit Test': second_Digit_percentile,\n",
                "                'Third Digit Test': third_Digit_percentile}\n",
                "digit_df = pd.DataFrame(data=final_series, index=index)\n",
                "final_df = pd.merge(digit_df, Benford_percentiles, on=digit_df.index, how='outer')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final_df"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "final_df.plot(kind='box', rot=-30)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('Skew\\n', final_df.skew(), '\\nKurtosis:\\n', final_df.kurt())"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final_df.std()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def myDataFrame(sample, digit_test, expected):\n",
                "    difference = []\n",
                "    for x in range(len(digit_test)):\n",
                "        difference.append(digit_test[x]-expected[x])\n",
                "    if len(sample) < 10:\n",
                "        sample.insert(0, 0)\n",
                "    output = pd.DataFrame({\n",
                "        'Sample Count': sample, \n",
                "        'Digit Test (%)': digit_test, \n",
                "        'Expected Values (%)': expected, \n",
                "        'Difference (%)': difference\n",
                "                        })\n",
                "    return output"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#This runs the function we created before on each of my different tests\n",
                "first_digit_df = myDataFrame(first_Digit, final_df['First Digit Test'], final_df['First Digit Expected'])\n",
                "second_digit_df = myDataFrame(second_Digit, final_df['Second Digit Test'], final_df['Second Digit Expected'])\n",
                "third_digit_df = myDataFrame(third_Digit, final_df['Third Digit Test'], final_df['Third Digit Expected'])\n",
                "\n",
                "#now I convert the numbers into percentage values to make my data tables more readable\n",
                "for x in [first_digit_df, second_digit_df, third_digit_df]:\n",
                "    for y in ['Digit Test (%)', 'Expected Values (%)', 'Difference (%)']:\n",
                "        x[y] = round(x[y].apply(lambda i: i*100), 2)\n",
                "\n",
                "#Below is an example of the completed data table\n",
                "first_digit_df"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "last_two = last_two_digit_test(finalData)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "index = []\n",
                "for x in range(100):\n",
                "    index.append(x)\n",
                "\n",
                "#Plotting\n",
                "fig = plt.figure()\n",
                "ax = fig.add_axes([0,0,1,1])\n",
                "width = .35\n",
                "x = np.arange(len(index))\n",
                "plt.xticks(x, index)\n",
                "ax.bar(x - width/2, last_two, width= width)\n",
                "plt.title('Last Two Digit Count')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "last_two_df = pd.DataFrame(last_two, index=index)\n",
                "print([ j for (i,j) in zip(last_two, index) if i >= 4 ])"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#We create new variables called 'significant numbers' which we will alter in order to deliver us just the significant digits\n",
                "\n",
                "first_digit_significant_numbers = first_digit_df.copy()\n",
                "second_digit_significant_numbers = second_digit_df.copy()\n",
                "third_digit_significant_numbers = third_digit_df.copy()\n",
                "\n",
                "#This for loop adds the significant digit to a list called 'temp' when the difference of the number is greater than 5%\n",
                "temp = []\n",
                "for x in [first_digit_significant_numbers, second_digit_significant_numbers, third_digit_significant_numbers]:\n",
                "    temp.append(x.index.where(x['Difference (%)'] > 5).dropna())\n",
                "\n",
                "#Now we convert the list into a cleaner version which will be easier to work with and put it in a dictionary\n",
                "significant_numbers = {\n",
                "    'First Digit Significant Values' : temp[0].astype(int).values,\n",
                "    'Second Digit Significant Values' : temp[1].astype(int).values,\n",
                "    'Third Digit Significant Values' : temp[2].astype(int).values\n",
                "}\n",
                "#here is an example of what those numbers are\n",
                "significant_numbers"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#This cell changes all of my integers into strings which are easier to slice in order to locate the sections of the 990 that\n",
                "#might contain fraudulent data\n",
                "finalData = [str(x) for x in finalData]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#these lines below actually are finding the numbers in our entire dataset where the digit at some given position\n",
                "#matches the number and the position that we are looking for\n",
                "first_numbers = [x for x in finalData if x[:1] in significant_numbers['First Digit Significant Values'].astype(str)]\n",
                "second_numbers = [x for x in finalData if x[1:2] in significant_numbers['Second Digit Significant Values'].astype(str)]\n",
                "third_numbers = [x for x in finalData if x[2:3] in significant_numbers['Third Digit Significant Values'].astype(str)]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(first_numbers, second_numbers, third_numbers)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def intersection(lst1, lst2): \n",
                "    lst3 = [value for value in lst1 if value in lst2] \n",
                "    return lst3 "
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "inter_one_two = intersection(first_numbers, second_numbers)\n",
                "inter_one_three = intersection(first_numbers, third_numbers)\n",
                "inter_two_three = intersection(second_numbers, third_numbers)\n",
                "interfinal = intersection(inter_one_two, third_numbers)\n",
                "print(inter_one_two)\n",
                "print(inter_one_three)\n",
                "print(inter_two_three)\n",
                "print(interfinal)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "in_first = set(inter_one_two)\n",
                "in_second = set(inter_two_three)\n",
                "\n",
                "in_second_but_not_in_first = in_second - in_first\n",
                "\n",
                "result = inter_one_two + list(in_second_but_not_in_first)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(result)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#delete all values that arent on our list 'result'\n",
                "all_to_investigate = df[df.isin(result)]\n",
                "\n",
                "#remove every column that is entirely 'NaN' values\n",
                "all_to_investigate = all_to_investigate.dropna(axis='columns', how='all')\n",
                "\n",
                "#remove every row that is entirely 'NaN' values\n",
                "all_to_investigate = all_to_investigate.dropna(axis='index', how='all')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "all_to_investigate"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "to_drop = ['revenue less expenses', 'Total expenses']\n",
                "final_list = all_to_investigate.drop(to_drop)\n",
                "final_list"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final_list"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "transfer_results",
                "visualize_data",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output.\n",
                "\n",
                "\n",
                "train_df = pd.read_csv('../input/train.csv')\n",
                "test_df = pd.read_csv('../input/test.csv')\n",
                "# combine = [train_df, test_df]\n",
                "\n",
                "train_df['color'] = train_df['color'].map({'clear':0, 'white':1, 'green':2, 'blood':3, 'blue':4, 'black':5})\n",
                "import seaborn as sns\n",
                "sns.set()\n",
                "sns.pairplot(train_df, hue=\"type\")\n",
                "\n",
                "#print(combine)\n",
                "#print(test_df.columns.values)\n",
                "# train_df.head()\n",
                "\n",
                "# train_df.info()\n",
                "# print('_'*40)\n",
                "# test_df.info()\n",
                "\n",
                "#What is the distribution of numerical feature values across the samples?\n",
                "# train_df.describe()\n",
                "\n",
                "#What is the distribution of categorical features?\n",
                "# train_df.describe(include=['O'])\n",
                "\n",
                "\n",
                "# submission = pd.DataFrame({\n",
                "#         \"id\": test_df[\"id\"],\n",
                "#         \"type\": \"Ghoul\"\n",
                "#     })\n",
                "print(submission)\n",
                "submission.to_csv('../output/submission.csv', index=False)\n",
                "submission.to_csv('submission.csv', index=False)\n",
                "\n",
                "\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# modules we'll use\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# helpful modules\n",
                "import fuzzywuzzy\n",
                "from fuzzywuzzy import process\n",
                "import chardet\n",
                "\n",
                "# set seed for reproducibility\n",
                "np.random.seed(0)"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data"
            ],
            "source": [
                "# look at the first ten thousand bytes to guess the character encoding\n",
                "with open(\"../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", 'rb') as rawdata:\n",
                "    result = chardet.detect(rawdata.read(100000))\n",
                "\n",
                "# check what the character encoding might be\n",
                "print(result)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "# read in our dat\n",
                "suicide_attacks = pd.read_csv(\"../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", \n",
                "                              encoding='Windows-1252')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# get all the unique values in the 'City' column\n",
                "cities = suicide_attacks['City'].unique()\n",
                "\n",
                "# sort them alphabetically and then take a closer look\n",
                "cities.sort()\n",
                "cities"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# convert to lower case\n",
                "suicide_attacks['City'] = suicide_attacks['City'].str.lower()\n",
                "# remove trailing white spaces\n",
                "suicide_attacks['City'] = suicide_attacks['City'].str.strip()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# Your turn! Take a look at all the unique values in the \"Province\" column. \n",
                "# Then convert the column to lowercase and remove any trailing white spaces\n",
                "\n",
                "# get all the unique values in the 'City' column\n",
                "provinces = suicide_attacks['Province'].unique()\n",
                "\n",
                "# sort them alphabetically and then take a closer look\n",
                "provinces.sort()\n",
                "provinces"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# convert to lower case\n",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.lower()\n",
                "# remove trailing white spaces\n",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.strip()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# get all the unique values in the 'City' column\n",
                "cities = suicide_attacks['City'].unique()\n",
                "\n",
                "# sort them alphabetically and then take a closer look\n",
                "cities.sort()\n",
                "cities"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# get the top 10 closest matches to \"d.i khan\"\n",
                "matches = fuzzywuzzy.process.extract(\"d.i khan\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
                "\n",
                "# take a look at them\n",
                "matches"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# function to replace rows in the provided column of the provided dataframe\n",
                "# that match the provided string above the provided ratio with the provided string\n",
                "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n",
                "    # get a list of unique strings\n",
                "    strings = df[column].unique()\n",
                "    \n",
                "    # get the top 10 closest matches to our input string\n",
                "    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n",
                "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n",
                "\n",
                "    # only get matches with a ratio > 90\n",
                "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n",
                "\n",
                "    # get the rows of all the close matches in our dataframe\n",
                "    rows_with_matches = df[column].isin(close_matches)\n",
                "\n",
                "    # replace all rows with close matches with the input matches \n",
                "    df.loc[rows_with_matches, column] = string_to_match\n",
                "    \n",
                "    # let us know the function's done\n",
                "    print(\"All done!\")"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# get all the unique values in the 'City' column\n",
                "cities = suicide_attacks['City'].unique()\n",
                "\n",
                "# sort them alphabetically and then take a closer look\n",
                "cities.sort()\n",
                "cities"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# Your turn! It looks like 'kuram agency' and 'kurram agency' should\n",
                "# be the same city. Correct the dataframe so that they are.\n",
                "\n",
                "\n",
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"kuram agency\")\n",
                "\n",
                "# get all the unique values in the 'City' column\n",
                "cities = suicide_attacks['City'].unique()\n",
                "\n",
                "# sort them alphabetically and then take a closer look\n",
                "cities.sort()\n",
                "cities"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "# modules we'll use\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# for Box-Cox Transformation\n",
                "from scipy import stats\n",
                "\n",
                "# for min_max scaling\n",
                "from mlxtend.preprocessing import minmax_scaling\n",
                "\n",
                "# plotting modules\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# read in all our data\n",
                "kickstarters_2017 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201801.csv\")\n",
                "\n",
                "# set seed for reproducibility\n",
                "np.random.seed(0)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# generate 1000 data points randomly drawn from an exponential distribution\n",
                "original_data = np.random.exponential(size = 1000)\n",
                "\n",
                "# mix-max scale the data between 0 and 1\n",
                "scaled_data = minmax_scaling(original_data, columns = [0])\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(original_data, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(scaled_data, ax=ax[1])\n",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# normalize the exponential data with boxcox\n",
                "normalized_data = stats.boxcox(original_data)\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(original_data, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(normalized_data[0], ax=ax[1])\n",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# select the usd_goal_real column\n",
                "usd_goal = kickstarters_2017.usd_goal_real\n",
                "\n",
                "# scale the goals from 0 to 1\n",
                "scaled_data = minmax_scaling(usd_goal, columns = [0])\n",
                "\n",
                "# plot the original & scaled data together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(scaled_data, ax=ax[1])\n",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Your turn! \n",
                "\n",
                "# We just scaled the \"usd_goal_real\" column. What about the \"goal\" column?\n",
                "\n",
                "# select the usd_goal_real column\n",
                "goal = kickstarters_2017.goal\n",
                "\n",
                "# scale the goals from 0 to 1\n",
                "scaled_data = minmax_scaling(goal, columns = [0])\n",
                "\n",
                "# plot the original & scaled data together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(kickstarters_2017.goal, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(scaled_data, ax=ax[1])\n",
                "ax[1].set_title(\"Scaled data\")\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# get the index of all positive pledges (Box-Cox only takes postive values)\n",
                "index_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0\n",
                "\n",
                "# get only positive pledges (using their indexes)\n",
                "positive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]\n",
                "\n",
                "# normalize the pledges (w/ Box-Cox)\n",
                "normalized_pledges = stats.boxcox(positive_pledges)[0]\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(positive_pledges, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(normalized_pledges, ax=ax[1])\n",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Your turn! \n",
                "# We looked as the usd_pledged_real column. What about the \"pledged\" column? Does it have the same info?\n",
                "\n",
                "# get the index of all positive pledges (Box-Cox only takes postive values)\n",
                "index_of_pledged = kickstarters_2017.pledged > 0\n",
                "\n",
                "# get only positive pledges (using their indexes)\n",
                "positive_pledges = kickstarters_2017.pledged.loc[index_of_pledged]\n",
                "\n",
                "# normalize the pledges (w/ Box-Cox)\n",
                "normalized_pledges = stats.boxcox(positive_pledges)[0]\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(positive_pledges, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(normalized_pledges, ax=ax[1])\n",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# modules we'll use\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "# for Box-Cox Transformation\n",
                "from scipy import stats\n",
                "\n",
                "# for min_max scaling\n",
                "from mlxtend.preprocessing import minmax_scaling\n",
                "\n",
                "# plotting modules\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# read in all our data\n",
                "kickstarters_2017 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201801.csv\")\n",
                "\n",
                "# set seed for reproducibility\n",
                "np.random.seed(0)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# generate 1000 data points randomly drawn from an exponential distribution\n",
                "original_data = np.random.exponential(size = 1000)\n",
                "\n",
                "# mix-max scale the data between 0 and 1\n",
                "scaled_data = minmax_scaling(original_data, columns = [0])\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(original_data, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(scaled_data, ax=ax[1])\n",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# normalize the exponential data with boxcox\n",
                "normalized_data = stats.boxcox(original_data)\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(original_data, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(normalized_data[0], ax=ax[1])\n",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# select the usd_goal_real column\n",
                "usd_goal = kickstarters_2017.usd_goal_real\n",
                "\n",
                "# scale the goals from 0 to 1\n",
                "scaled_data = minmax_scaling(usd_goal, columns = [0])\n",
                "\n",
                "# plot the original & scaled data together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(scaled_data, ax=ax[1])\n",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Your turn! \n",
                "\n",
                "# We just scaled the \"usd_goal_real\" column. What about the \"goal\" column?\n",
                "\n",
                "usd_goal = kickstarters_2017.goal\n",
                "\n",
                "# scale the goals from 0 to 1\n",
                "scaled_data = minmax_scaling(usd_goal, columns = [0])\n",
                "\n",
                "# plot the original & scaled data together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(kickstarters_2017.goal, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(scaled_data, ax=ax[1])\n",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# get the index of all positive pledges (Box-Cox only takes postive values)\n",
                "index_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0\n",
                "\n",
                "# get only positive pledges (using their indexes)\n",
                "positive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]\n",
                "\n",
                "# normalize the pledges (w/ Box-Cox)\n",
                "normalized_pledges = stats.boxcox(positive_pledges)[0]\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(positive_pledges, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(normalized_pledges, ax=ax[1])\n",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Your turn! \n",
                "# We looked as the usd_pledged_real column. What about the \"pledged\" column? Does it have the same info?\n",
                "\n",
                "index_of_positive_pledges = kickstarters_2017.pledged > 0\n",
                "\n",
                "# get only positive pledges (using their indexes)\n",
                "positive_pledges = kickstarters_2017.pledged.loc[index_of_positive_pledges]\n",
                "\n",
                "# normalize the pledges (w/ Box-Cox)\n",
                "normalized_pledges = stats.boxcox(positive_pledges)[0]\n",
                "\n",
                "# plot both together to compare\n",
                "fig, ax=plt.subplots(1,2)\n",
                "sns.distplot(positive_pledges, ax=ax[0])\n",
                "ax[0].set_title(\"Original Data\")\n",
                "sns.distplot(normalized_pledges, ax=ax[1])\n",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# books.csv has 13719 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df1 = pd.read_csv('../input/books.csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'books.csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df1.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotCorrelationMatrix(df1, 8)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotScatterMatrix(df1, 18, 10)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import joblib\n",
                "import lightgbm as lgb\n",
                "import time\n",
                "import pickle\n",
                "import math\n",
                "import string\n",
                "import datetime\n",
                "from sklearn.model_selection import KFold\n",
                "from sklearn.metrics import f1_score, mean_squared_error\n",
                "import glob\n",
                "import json\n",
                "\n",
                "def challenge_data_conversion(challenge_data):\n",
                "    output = []\n",
                "    output.append(challenge_data['id'])\n",
                "    output.append(1 if len(challenge_data['winners']) > 0 else 0)\n",
                "    output.append(len(challenge_data['winners']))\n",
                "    \n",
                "    return output\n",
                "\n",
                "def data_conversion(training_file_path):\n",
                "    data_df = pd.DataFrame(columns=['id', 'hasWinner', 'numOfWinners'])\n",
                "    file_list = []\n",
                "    extensions = [\"json\"]\n",
                "    for extension in extensions:\n",
                "        file_glob = glob.glob(training_file_path+\"/*.\"+extension)\n",
                "        file_list.extend(file_glob)\n",
                "    print(str(len(file_list))+' files')\n",
                "        \n",
                "    for file_path in file_list:\n",
                "        with open(file_path,'r') as f:\n",
                "            data_dict = json.load(f)\n",
                "        for challenge_data in data_dict:\n",
                "            #try:\n",
                "            data_df.loc[len(data_df)] = challenge_data_conversion(challenge_data)\n",
                "            #except:\n",
                "            #    print(challenge_data_conversion(challenge_data))\n",
                "            \n",
                "            \n",
                "    return data_df\n",
                "\n",
                "test_data = data_conversion('../input/challenge-health-notification-test-data/')\n",
                "reg_output = pd.read_csv('../input/challenge-health-notification-reg-output/lightgbm_numOfWinners_prediction.csv')\n",
                "cls_output = pd.read_csv('../input/challenge-health-notification-cls-output/lightgbm_hasWinner_prediction.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('f1 score:')\n",
                "print(f1_score(test_data['hasWinner'].astype(\"int\").values, cls_output['hasWinner'].values))\n",
                "\n",
                "print('')\n",
                "\n",
                "print('mean squared error:')\n",
                "print(mean_squared_error(test_data['numOfWinners'].values, reg_output['numOfWinners'].values))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import cv2\n",
                "import json\n",
                "import os\n",
                "import matplotlib\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.patches import Polygon\n",
                "from matplotlib.collections import PatchCollection\n",
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "import seaborn as sns"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "DATASET_DIR = '/kaggle/input/pku-autonomous-driving/'\n",
                "JSON_DIR = os.path.join(DATASET_DIR, 'car_models_json')\n",
                "NUM_IMG_SAMPLES = 10 # The number of image samples used for visualization"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "df = pd.read_csv(os.path.join(DATASET_DIR, 'train.csv'))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "image_ids = np.array(df['ImageId'])\n",
                "prediction_strings = np.array(df['PredictionString'])\n",
                "prediction_strings = [\n",
                "    np.array(prediction_string.split(' ')).astype(np.float32).reshape(-1, 7) \\\n",
                "    for prediction_string in prediction_strings\n",
                "]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('Image ID:', image_ids[0])\n",
                "print('Annotations:\\n', prediction_strings[0])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "models_map = dict((y, x) for x, y in models.items())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cars = []\n",
                "for prediction_string in prediction_strings:\n",
                "    for car in prediction_string:\n",
                "        cars.append(car)\n",
                "cars = np.array(cars)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "unique, counts = np.unique(cars[..., 0].astype(np.uint8), return_counts=True)\n",
                "all_model_types = zip(unique, counts)\n",
                "\n",
                "for i, model_type in enumerate(all_model_types):\n",
                "    print('{}.\\t Model type: {:<22} | {} cars'.format(i, models_map[model_type[0]], model_type[1]))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def plot_figures(\n",
                "    sizes,\n",
                "    pie_title,\n",
                "    start_angle,\n",
                "    bar_title,\n",
                "    bar_ylabel,\n",
                "    labels,\n",
                "    explode,\n",
                "    colors=None,\n",
                "):\n",
                "    fig, ax = plt.subplots(figsize=(14, 14))\n",
                "\n",
                "    y_pos = np.arange(len(labels))\n",
                "    barlist = ax.bar(y_pos, sizes, align='center')\n",
                "    ax.set_xticks(y_pos, labels)\n",
                "    ax.set_ylabel(bar_ylabel)\n",
                "    ax.set_title(bar_title)\n",
                "    if colors is not None:\n",
                "        for idx, item in enumerate(barlist):\n",
                "            item.set_color(colors[idx])\n",
                "\n",
                "    def autolabel(rects):\n",
                "        \"\"\"\n",
                "        Attach a text label above each bar displaying its height\n",
                "        \"\"\"\n",
                "        for rect in rects:\n",
                "            height = rect.get_height()\n",
                "            ax.text(\n",
                "                rect.get_x() + rect.get_width()/2., height,\n",
                "                '%d' % int(height),\n",
                "                ha='center', va='bottom', fontweight='bold'\n",
                "            )\n",
                "\n",
                "    autolabel(barlist)\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(14, 14))\n",
                "    \n",
                "    pielist = ax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=False, startangle=start_angle, counterclock=False)\n",
                "    ax.axis('equal')\n",
                "    ax.set_title(pie_title)\n",
                "    if colors is not None:\n",
                "        for idx, item in enumerate(pielist[0]):\n",
                "            item.set_color(colors[idx])\n",
                "\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plot_figures(\n",
                "    counts,\n",
                "    pie_title='The percentage of the number of cars of each model type',\n",
                "    start_angle=170,\n",
                "    bar_title='Distribution of cars of each model type',\n",
                "    bar_ylabel='Frequency',\n",
                "    labels=[label for label in unique],\n",
                "    explode=np.zeros(len(unique))\n",
                ")"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "# Get all json files\n",
                "files = [file for file in os.listdir(JSON_DIR) if os.path.isfile(os.path.join(JSON_DIR, file))]\n",
                "\n",
                "# For each json file, plot figure\n",
                "for file in files:\n",
                "    model_path = os.path.join(JSON_DIR, file)\n",
                "    with open(model_path) as src:\n",
                "        data = json.load(src)\n",
                "        car_type = data['car_type']\n",
                "        faces = data['faces']\n",
                "        vertices = np.array(data['vertices'])\n",
                "        triangles = np.array(faces) - 1\n",
                "\n",
                "        fig = plt.figure(figsize=(16, 5))\n",
                "        ax11 = fig.add_subplot(1, 2, 1, projection='3d')\n",
                "        ax11.set_title('Model: {} | Type: {}'.format(file.split('.')[0], car_type))\n",
                "        ax11.set_xlim([-2, 3])\n",
                "        ax11.set_ylim([-3, 2])\n",
                "        ax11.set_zlim([0, 3])\n",
                "        ax11.view_init(30, -50)\n",
                "        ax11.plot_trisurf(vertices[:,0], vertices[:,2], triangles, -vertices[:,1], shade=True, color='lime')\n",
                "        \n",
                "        ax12 = fig.add_subplot(1, 2, 2, projection='3d')\n",
                "        ax12.set_title('Model: {} | Type: {}'.format(file.split('.')[0], car_type))\n",
                "        ax12.set_xlim([-2, 3])\n",
                "        ax12.set_ylim([-3, 2])\n",
                "        ax12.set_zlim([0, 3])\n",
                "        ax12.view_init(30, 40)\n",
                "        ax12.plot_trisurf(vertices[:,0], vertices[:,2], triangles, -vertices[:,1], shade=True, color='lime')"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "def show_samples(samples):\n",
                "    for sample in samples:\n",
                "        fig, ax = plt.subplots(figsize=(18, 16))\n",
                "        \n",
                "        # Get image\n",
                "        img_path = os.path.join(DATASET_DIR, 'train_images', '{}.{}'.format(sample, 'jpg'))\n",
                "        img = cv2.imread(img_path, 1)\n",
                "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
                "\n",
                "        # Get corresponding mask\n",
                "        mask_path = os.path.join(DATASET_DIR, 'train_masks', '{}.{}'.format(sample, 'jpg'))\n",
                "        mask = cv2.imread(mask_path, 0)\n",
                "\n",
                "        patches = []\n",
                "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
                "        for contour in contours:\n",
                "            poly_patch = Polygon(contour.reshape(-1, 2), closed=True, linewidth=2, edgecolor='r', facecolor='r', fill=True)\n",
                "            patches.append(poly_patch)\n",
                "        p = PatchCollection(patches, match_original=True, cmap=matplotlib.cm.jet, alpha=0.3)\n",
                "\n",
                "        ax.imshow(img/255)\n",
                "        ax.set_title(sample)\n",
                "        ax.add_collection(p)\n",
                "        ax.set_xticklabels([])\n",
                "        ax.set_yticklabels([])\n",
                "        plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# Randomly select samples\n",
                "samples = image_ids[np.random.choice(image_ids.shape[0], NUM_IMG_SAMPLES, replace=False)]\n",
                "\n",
                "# Show images and corresponding masks of too-far-away (not of interest) cars\n",
                "show_samples(samples)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "lens = [len(str2coords(s)) for s in train['PredictionString']]\n",
                "\n",
                "plt.figure(figsize=(15,6))\n",
                "sns.countplot(lens);\n",
                "plt.xlabel('Number of cars in image');"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "points_df = pd.DataFrame()\n",
                "for col in ['x', 'y', 'z', 'yaw', 'pitch', 'roll']:\n",
                "    arr = []\n",
                "    for ps in train['PredictionString']:\n",
                "        coords = str2coords(ps)\n",
                "        arr += [c[col] for c in coords]\n",
                "    points_df[col] = arr"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,6))\n",
                "sns.distplot(points_df['x'], bins=500);\n",
                "plt.xlabel('x')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,6))\n",
                "sns.distplot(points_df['y'], bins=500);\n",
                "plt.xlabel('y')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,6))\n",
                "sns.distplot(points_df['z'], bins=500);\n",
                "plt.xlabel('z')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,6))\n",
                "sns.distplot(points_df['yaw'], bins=500);\n",
                "plt.xlabel('yaw')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,6))\n",
                "sns.distplot(points_df['pitch'], bins=500);\n",
                "plt.xlabel('pitch')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def rotate(x, angle):\n",
                "    x = x + angle\n",
                "    x = x - (x + np.pi) // (2 * np.pi) * 2 * np.pi\n",
                "    return x\n",
                "\n",
                "plt.figure(figsize=(15,6))\n",
                "sns.distplot(points_df['roll'].map(lambda x: rotate(x, np.pi)), bins=500);\n",
                "plt.xlabel('roll rotated by pi')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def get_img_coords(input_item, input_type=str, output_z=False):\n",
                "    '''\n",
                "    Input is a PredictionString (e.g. from train dataframe)\n",
                "    Output is two arrays:\n",
                "        xs: x coordinates in the image (row)\n",
                "        ys: y coordinates in the image (column)\n",
                "    '''\n",
                "    if input_type == str:\n",
                "        coords = str2coords(input_item)\n",
                "    else:\n",
                "        coords = input_item\n",
                "    \n",
                "    xs = [c['x'] for c in coords]\n",
                "    ys = [c['y'] for c in coords]\n",
                "    zs = [c['z'] for c in coords]\n",
                "    P = np.array(list(zip(xs, ys, zs))).T\n",
                "    img_p = np.dot(camera_matrix, P).T\n",
                "    img_p[:, 0] /= img_p[:, 2]\n",
                "    img_p[:, 1] /= img_p[:, 2]\n",
                "    img_xs = img_p[:, 0]\n",
                "    img_ys = img_p[:, 1]\n",
                "    img_zs = img_p[:, 2] # z = Distance from the camera\n",
                "    if output_z:\n",
                "        return img_xs, img_ys, img_zs\n",
                "    return img_xs, img_ys\n",
                "\n",
                "plt.figure(figsize=(14,14))\n",
                "plt.imshow(imread(PATH + 'train_images/' + train['ImageId'][2217] + '.jpg'))\n",
                "plt.scatter(*get_img_coords(train['PredictionString'][2217]), color='red', s=100);"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "xs, ys = [], []\n",
                "\n",
                "for ps in train['PredictionString']:\n",
                "    x, y = get_img_coords(ps)\n",
                "    xs += list(x)\n",
                "    ys += list(y)\n",
                "\n",
                "plt.figure(figsize=(18,18))\n",
                "plt.imshow(imread(PATH + 'train_images/' + train['ImageId'][2217] + '.jpg'), alpha=0.3)\n",
                "plt.scatter(xs, ys, color='red', s=10, alpha=0.2);"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "n_rows = 6\n",
                "\n",
                "for idx in range(n_rows):\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(20,20))\n",
                "    img = imread(PATH + 'train_images/' + train['ImageId'].iloc[idx] + '.jpg')\n",
                "    axes[0].imshow(img)\n",
                "    img_vis = visualize(img, str2coords(train['PredictionString'].iloc[idx]))\n",
                "    axes[1].imshow(img_vis)\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "import random\n",
                "sample_index_list = random.sample(list(range(len(points_df))), 5000)\n",
                "v = np.vstack([points_df['x'][sample_index_list], points_df['y'][sample_index_list], \n",
                "               points_df['z'][sample_index_list], points_df['yaw'][sample_index_list], \n",
                "               points_df['pitch'][sample_index_list], points_df['roll'][sample_index_list]])\n",
                "CM = np.corrcoef(v)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(7, 7))\n",
                "im = ax.imshow(CM)\n",
                "ax.set_xticks(np.arange(6))\n",
                "ax.set_yticks(np.arange(6))\n",
                "ax.set_xticklabels(['x', 'y', 'z', 'yaw', 'pitch', 'roll'])\n",
                "ax.set_yticklabels(['x', 'y', 'z', 'yaw', 'pitch', 'roll'])\n",
                "for i in range(6):\n",
                "    for j in range(6):\n",
                "        text = ax.text(j, i, round(CM[i, j], 2),\n",
                "                       ha=\"center\", va=\"center\", color=\"w\")\n",
                "fig.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "coords = []\n",
                "for sample_index in sample_index_list:\n",
                "    coord = {}\n",
                "    coord['x'] = points_df['x'][sample_index] \n",
                "    coord['y'] = points_df['y'][sample_index] \n",
                "    coord['z'] = points_df['z'][sample_index]\n",
                "    coords.append(coord)\n",
                "img_x_list, img_y_list, img_z_list = get_img_coords(coords, input_type=list, output_z=True)\n",
                "\n",
                "v = np.vstack([points_df['x'][sample_index_list], points_df['y'][sample_index_list], \n",
                "               points_df['z'][sample_index_list], img_x_list, img_y_list, img_z_list])\n",
                "CM = np.corrcoef(v)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(7, 7))\n",
                "im = ax.imshow(CM)\n",
                "ax.set_xticks(np.arange(6))\n",
                "ax.set_yticks(np.arange(6))\n",
                "ax.set_xticklabels(['x', 'y', 'z', 'img_x', 'img_y', 'img_z'])\n",
                "ax.set_yticklabels(['x', 'y', 'z', 'img_x', 'img_y', 'img_z'])\n",
                "for i in range(6):\n",
                "    for j in range(6):\n",
                "        text = ax.text(j, i, round(CM[i, j], 2),\n",
                "                       ha=\"center\", va=\"center\", color=\"w\")\n",
                "fig.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "mask_path = os.path.join(DATASET_DIR, 'train_masks', '{}.{}'.format(image_ids[0], 'jpg'))\n",
                "mask_accru = cv2.imread(mask_path, 0).astype(np.int) / 255\n",
                "for id in image_ids[1:]:\n",
                "    mask_path = os.path.join(DATASET_DIR, 'train_masks', '{}.{}'.format(id, 'jpg'))\n",
                "    try:\n",
                "        mask = cv2.imread(mask_path, 0).astype(np.int) / 255\n",
                "        mask_accru = np.add(mask_accru, mask)\n",
                "    except:\n",
                "        pass\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(18, 16))\n",
                "ax.set_title('mask distribution')\n",
                "im = ax.imshow(mask_accru)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import os\n",
                "from sklearn import preprocessing\n",
                "import lightgbm as lgb\n",
                "from sklearn.model_selection import KFold, StratifiedKFold\n",
                "from sklearn.model_selection import train_test_split\n",
                "from bayes_opt import BayesianOptimization\n",
                "import seaborn as sns\n",
                "from sklearn import metrics\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import mean_squared_error\n",
                "from sklearn.metrics import roc_auc_score\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.metrics import recall_score\n",
                "from sklearn.metrics import f1_score\n",
                "from sklearn.metrics import auc\n",
                "from sklearn.metrics import precision_score\n",
                "from sklearn.metrics import roc_curve\n",
                "from scipy.interpolate import interp1d\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.metrics import confusion_matrix"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train_id=pd.read_csv('../input/train_identity.csv')\n",
                "test_id=pd.read_csv('../input/test_identity.csv')\n",
                "train_id.head()\n",
                "#print(train_id.describe())\n",
                "#print(train_id.isnull().sum())"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train_data=pd.read_csv('../input/train_transaction.csv')\n",
                "test_data=pd.read_csv('../input/test_transaction.csv')\n",
                "#print(train_data.describe())\n",
                "#print(train_data.isnull().sum())"
            ]
        },
        {
            "tags": [
                "process_data",
                "setup_notebook"
            ],
            "source": [
                "import gc\n",
                "train = train_data.merge(train_id, how='left', on='TransactionID')\n",
                "test = test_data.merge(test_id, how='left',on='TransactionID')\n",
                "del train_id; gc.collect()\n",
                "del test_id; gc.collect()\n",
                "del train_data; gc.collect()\n",
                "del test_data; gc.collect()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "index_pro=train['P_emaildomain']=='protonmail.com'\n",
                "index_pro_data=train['P_emaildomain'][index_pro]\n",
                "train['isprotonmail']=index_pro_data\n",
                "del index_pro\n",
                "del index_pro_data\n",
                "index_pro=test['P_emaildomain']=='protonmail.com'\n",
                "index_pro_data=test['P_emaildomain'][index_pro]\n",
                "test['isprotonmail']=index_pro_data\n",
                "del index_pro\n",
                "del index_pro_data\n",
                "\n",
                "train.loc[ (train.isprotonmail.isnull()), 'isprotonmail' ] = 0\n",
                "train['isprotonmail'].describe(include=\"all\")\n",
                "\n",
                "test.loc[ (test.isprotonmail.isnull()), 'isprotonmail' ] = 0\n",
                "test['isprotonmail'].describe(include=\"all\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "\n",
                "'''cols_to_drop=['V108', 'id_22', 'id_26', 'id_25', 'C3', 'V14', 'id_24', 'id_07', 'V121', 'V65', 'V305', 'V311', 'V286', 'V111', 'V300', 'V115', 'V113', 'V301',  'V25', 'V123', 'V118', 'V109', 'id_23', 'V112', 'V114', 'V120', 'V88', 'V107', 'V117', 'V122', 'V116', 'id_08', 'id_27', 'id_21', 'V110', 'V119']\n",
                "\n",
                "train = train.drop(cols_to_drop, axis=1)\n",
                "test = test.drop(cols_to_drop, axis=1)'''"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "training_missing = train.isna().sum(axis=0) / train.shape[0] \n",
                "test_missing = test.isna().sum(axis=0) / test.shape[0] \n",
                "change = (training_missing / test_missing).sort_values(ascending=False)\n",
                "change = change[change<1e6] # remove the divide by zero errors\n",
                "train_more=change[change>4].reset_index()\n",
                "train_more.columns=['train_more_id','rate']\n",
                "test_more=change[change<0.4].reset_index()\n",
                "test_more.columns=['test_more_id','rate']\n",
                "train_more_id=train_more['train_more_id'].values\n",
                "train[train_more_id].head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig, axs = plt.subplots(ncols=2)\n",
                "\n",
                "train_vals = train[\"V80\"].fillna(-999)\n",
                "test_vals = test[test[\"TransactionDT\"]>2.5e7][\"V80\"].fillna(-999) # values following the shift\n",
                "\n",
                "\n",
                "axs[0].hist(train_vals, alpha=0.5, normed=True, bins=25)\n",
                "    \n",
                "axs[1].hist(test_vals, alpha=0.5, normed=True, bins=25)\n",
                "\n",
                "\n",
                "fig.set_size_inches(7,3)\n",
                "plt.tight_layout()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
                "us_emails = ['gmail', 'net', 'edu']\n",
                "for c in ['P_emaildomain', 'R_emaildomain']:\n",
                "    train[c + '_bin'] = train[c].map(emails)\n",
                "    test[c + '_bin'] = test[c].map(emails)\n",
                "    \n",
                "    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n",
                "    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n",
                "    \n",
                "    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
                "    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import Imputer\n",
                "imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
                "test[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']]=imp.fit_transform(test[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']])\n",
                "train[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']]=imp.fit_transform(train[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "isna = train.isna().sum(axis=1)\n",
                "isna_test = test.isna().sum(axis=1)\n",
                "train['isna']=train.isna().sum(axis=1)\n",
                "test['isna']=test.isna().sum(axis=1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "test[test.TransactionDT>2.5e7][train_more_id]=imp.fit_transform(test[test.TransactionDT>2.5e7][train_more_id])\n",
                "train[train_more_id]=imp.fit_transform(train[train_more_id])"
            ]
        },
        {
            "tags": [
                "process_data",
                "setup_notebook"
            ],
            "source": [
                "import datetime\n",
                "START_DATE = '2017-12-01'\n",
                "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
                "train['TransactionDT'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
                "test['TransactionDT'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
                "\n",
                "train['hour'] = train['TransactionDT'].dt.hour\n",
                "test['hour'] = test['TransactionDT'].dt.hour\n",
                "\n",
                "train['month'] = train['TransactionDT'].dt.month\n",
                "test['month'] = test['TransactionDT'].dt.month\n",
                "#print(train['TransactionDT'])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x=test['hour']<7\n",
                "x1=test['hour']>19\n",
                "x.astype(int)\n",
                "x1.astype(int)\n",
                "x2=np.add(x,x1)\n",
                "x2.astype(int)\n",
                "test['night']=x2.astype(int)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x=train['hour']<7\n",
                "x1=train['hour']>19\n",
                "x.astype(int)\n",
                "x1.astype(int)\n",
                "x2=np.add(x,x1)\n",
                "x2.astype(int)\n",
                "train['night']=x2.astype(int)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train=train.drop('TransactionDT',axis=1)\n",
                "test=test.drop('TransactionDT',axis=1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for c1, c2 in train.dtypes.reset_index().values:\n",
                "    if c2=='O':\n",
                "        train[c1] = train[c1].map(lambda x: labels[str(x).lower()])\n",
                "        test[c1] = test[c1].map(lambda x: labels[str(x).lower()])\n",
                "train.fillna(-999, inplace=True)\n",
                "test.fillna(-999, inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train = train.fillna(-999)\n",
                "test = test.fillna(-999)\n",
                "for f in test.columns:\n",
                "    if train[f].dtype=='object' or test[f].dtype=='object': \n",
                "        print(f)\n",
                "        lbl = preprocessing.LabelEncoder()\n",
                "        lbl.fit(list(train[f].values) + list(test[f].values))\n",
                "        train[f] = lbl.transform(list(train[f].values))\n",
                "        test[f] = lbl.transform(list(test[f].values)) "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train['TransactionAmt']=np.log(train['TransactionAmt'])\n",
                "test['TransactionAmt']=np.log(test['TransactionAmt'])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "'''columns_np=train.columns.values\n",
                "columns_list=list(train)\n",
                "train_12=train[train.month==12]\n",
                "train_1=train[train.month==1]\n",
                "train_2=train[train.month==2]\n",
                "train_3=train[train.month==3]\n",
                "train_4=train[train.month==4]\n",
                "train_5=train[train.month==5]\n",
                "train_6=train[train.month==6]'''\n",
                "'''\n",
                "test_7=test[test.month==7]\n",
                "test_8=test[test.month==8]\n",
                "test_9=test[test.month==9]\n",
                "test_10=test[test.month==10]\n",
                "test_11=test[test.month==11]\n",
                "test_12=test[test.month==12]'''"
            ]
        },
        {
            "tags": [
                "process_data",
                "setup_notebook"
            ],
            "source": [
                "'''from imblearn.under_sampling import RandomUnderSampler\n",
                "ran=RandomUnderSampler(return_indices=True) ##intialize to return indices of dropped rows\n",
                "train_12,y_train12,dropped12 = ran.fit_sample(train_12,train_12['isFraud'])\n",
                "train_1,y_train1,dropped1 = ran.fit_sample(train_1,train_1['isFraud'])\n",
                "train_2,y_train2,dropped2 = ran.fit_sample(train_2,train_2['isFraud'])\n",
                "train_3,y_train3,dropped3 = ran.fit_sample(train_3,train_3['isFraud'])\n",
                "train_4,y_train4,dropped4 = ran.fit_sample(train_4,train_4['isFraud'])\n",
                "train_5,y_train5,dropped5 = ran.fit_sample(train_5,train_5['isFraud'])\n",
                "train_6,y_train6,dropped6 = ran.fit_sample(train_6,train_6['isFraud'])\n",
                "del train\n",
                "del y_train12\n",
                "del y_train1\n",
                "del y_train2\n",
                "del y_train3\n",
                "del y_train4\n",
                "del y_train5\n",
                "del y_train6'''"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "'''train_12=pd.DataFrame(np.array(train_12))\n",
                "train_1=pd.DataFrame(np.array(train_1))\n",
                "train_2=pd.DataFrame(np.array(train_2))\n",
                "train_3=pd.DataFrame(np.array(train_3))\n",
                "train_4=pd.DataFrame(np.array(train_4))\n",
                "train_5=pd.DataFrame(np.array(train_5))\n",
                "train_6=pd.DataFrame(np.array(train_6))\n",
                "train=pd.concat([train_12,train_1,train_2,train_3,train_4,train_5,train_6],axis=0)\n",
                "train.columns=columns_list\n",
                "del train_1\n",
                "del train_2\n",
                "del train_12\n",
                "del train_3\n",
                "del train_4\n",
                "del train_5\n",
                "del train_6'''"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "'''train=pd.DataFrame(np.array(train))\n",
                "train.columns=columns_list'''"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "y_train=train['isFraud']\n",
                "train=train.drop('isFraud',axis=1)\n",
                "train=train.drop('month',axis=1)\n",
                "test=test.drop('month',axis=1)\n",
                "X_test = test.copy()\n",
                "del test; gc.collect()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from imblearn.over_sampling import RandomOverSampler\n",
                "ros = RandomOverSampler(random_state=0)\n",
                "train_data, y_train = ros.fit_sample(train, y_train)\n",
                "del train; gc.collect()'''\n",
                "from imblearn.over_sampling import SMOTE\n",
                "smote = SMOTE(frac=0.1, random_state=1)\n",
                "train_data, y_train = smote.fit_sample(train, y_train)'''\n",
                "\n",
                "'''from imblearn.under_sampling import RandomUnderSampler\n",
                "ran=RandomUnderSampler(return_indices=True) ##intialize to return indices of dropped rows\n",
                "train_data,y_train,dropped = ran.fit_sample(train,y_train)\n",
                "del train; gc.collect()'''"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(y_train)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cols_to_drop=['V108', 'id_22', 'id_26', 'id_25', 'C3', 'V14', 'id_24', 'id_07', 'V121', 'V65', 'V305', 'V311', 'V286', 'V111', 'V300', 'V115', 'V113', 'V301',  'V25', 'V123', 'V118', 'V109', 'id_23', 'V112', 'V114', 'V120', 'V88', 'V107', 'V117', 'V122', 'V116', 'id_08', 'id_27', 'id_21', 'V110', 'V119']\n",
                "\n",
                "train = train.drop(cols_to_drop, axis=1)\n",
                "X_test = X_test.drop(cols_to_drop, axis=1)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "process_data"
            ],
            "source": [
                "def fast_auc(y_true, y_prob):\n",
                "    \"\"\"\n",
                "    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n",
                "    \"\"\"\n",
                "    y_true = np.asarray(y_true)\n",
                "    y_true = y_true[np.argsort(y_prob)]\n",
                "    nfalse = 0\n",
                "    auc = 0\n",
                "    n = len(y_true)\n",
                "    for i in range(n):\n",
                "        y_i = y_true[i]\n",
                "        nfalse += (1 - y_i)\n",
                "        auc += y_i * nfalse\n",
                "    auc /= (nfalse * (n - nfalse))\n",
                "    return auc\n",
                "\n",
                "\n",
                "def eval_auc(y_true, y_pred):\n",
                "    \"\"\"\n",
                "    Fast auc eval function for lgb.\n",
                "    \"\"\"\n",
                "    return 'auc', fast_auc(y_true, y_pred), True\n",
                "\n",
                "\n",
                "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
                "    \"\"\"\n",
                "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
                "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
                "    \"\"\"\n",
                "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
                "    return np.log(maes.map(lambda x: max(x, floor))).mean()\n",
                "def train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n",
                "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n",
                "    \"\"\"\n",
                "    A function to train a variety of classification models.\n",
                "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
                "    \n",
                "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
                "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
                "    :params: y - target\n",
                "    :params: folds - folds to split data\n",
                "    :params: model_type - type of model to use\n",
                "    :params: eval_metric - metric to use\n",
                "    :params: columns - columns to use. If None - use all columns\n",
                "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
                "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
                "    \n",
                "    \"\"\"\n",
                "    columns = X.columns if columns is None else columns\n",
                "    n_splits = folds.n_splits if splits is None else n_folds\n",
                "    X_test = X_test[columns]\n",
                "    \n",
                "    # to set up scoring parameters\n",
                "    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n",
                "                        'catboost_metric_name': 'AUC',\n",
                "                        'sklearn_scoring_function': metrics.roc_auc_score},\n",
                "                    }\n",
                "    \n",
                "    result_dict = {}\n",
                "    if averaging == 'usual':\n",
                "        # out-of-fold predictions on train data\n",
                "        oof = np.zeros((len(X), 1))\n",
                "\n",
                "        # averaged predictions on train data\n",
                "        prediction = np.zeros((len(X_test), 1))\n",
                "        \n",
                "    elif averaging == 'rank':\n",
                "        # out-of-fold predictions on train data\n",
                "        oof = np.zeros((len(X), 1))\n",
                "\n",
                "        # averaged predictions on train data\n",
                "        prediction = np.zeros((len(X_test), 1))\n",
                "\n",
                "    \n",
                "    # list of scores on folds\n",
                "    scores = []\n",
                "    feature_importance = pd.DataFrame()\n",
                "    \n",
                "    # split and train on folds\n",
                "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
                "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
                "        if type(X) == np.ndarray:\n",
                "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
                "            y_train, y_valid = y[train_index], y[valid_index]\n",
                "        else:\n",
                "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
                "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
                "            \n",
                "        if model_type == 'lgb':\n",
                "            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n",
                "            model.fit(X_train, y_train, \n",
                "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
                "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
                "            \n",
                "            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n",
                "            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n",
                "            \n",
                "        if model_type == 'xgb':\n",
                "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
                "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
                "\n",
                "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
                "            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n",
                "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
                "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
                "        \n",
                "        if model_type == 'sklearn':\n",
                "            model = model\n",
                "            model.fit(X_train, y_train)\n",
                "            \n",
                "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
                "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
                "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
                "            print('')\n",
                "            \n",
                "            y_pred = model.predict_proba(X_test)\n",
                "        \n",
                "        if model_type == 'cat':\n",
                "            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
                "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
                "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
                "\n",
                "            y_pred_valid = model.predict(X_valid)\n",
                "            y_pred = model.predict(X_test)\n",
                "        \n",
                "        if averaging == 'usual':\n",
                "            \n",
                "            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n",
                "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
                "            \n",
                "            prediction += y_pred.reshape(-1, 1)\n",
                "\n",
                "        elif averaging == 'rank':\n",
                "                                  \n",
                "            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n",
                "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
                "                                  \n",
                "            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n",
                "        \n",
                "        if model_type == 'lgb' and plot_feature_importance:\n",
                "            # feature importance\n",
                "            fold_importance = pd.DataFrame()\n",
                "            fold_importance[\"feature\"] = columns\n",
                "            fold_importance[\"importance\"] = model.feature_importances_\n",
                "            fold_importance[\"fold\"] = fold_n + 1\n",
                "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
                "\n",
                "    prediction /= n_splits\n",
                "    \n",
                "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
                "    \n",
                "    result_dict['oof'] = oof\n",
                "    result_dict['prediction'] = prediction\n",
                "    result_dict['scores'] = scores\n",
                "    \n",
                "    if model_type == 'lgb':\n",
                "        if plot_feature_importance:\n",
                "            feature_importance[\"importance\"] /= n_splits\n",
                "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
                "                by=\"importance\", ascending=False)[:50].index\n",
                "\n",
                "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
                "\n",
                "            plt.figure(figsize=(16, 12));\n",
                "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
                "            plt.title('LGB Features (avg over folds)');\n",
                "            \n",
                "            result_dict['feature_importance'] = feature_importance\n",
                "            result_dict['top_columns'] = cols\n",
                "        \n",
                "    return result_dict"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.model_selection import TimeSeriesSplit\n",
                "n_fold = 7\n",
                "folds = TimeSeriesSplit(n_splits=n_fold)\n",
                "folds = KFold(n_splits=5)"
            ]
        },
        {
            "tags": [
                "train_model",
                "setup_notebook"
            ],
            "source": [
                "import time\n",
                "params = {'num_leaves': 256,\n",
                "          'min_child_samples': 79,\n",
                "          'objective': 'binary',\n",
                "          'max_depth': 13,\n",
                "          'learning_rate': 0.03,\n",
                "          \"boosting_type\": \"gbdt\",\n",
                "          \"subsample_freq\": 3,\n",
                "          \"subsample\": 0.9,\n",
                "          \"bagging_seed\": 11,\n",
                "          \"metric\": 'auc',\n",
                "          \"verbosity\": -1,\n",
                "          'reg_alpha': 0.3,\n",
                "          'reg_lambda': 0.3,\n",
                "          'colsample_bytree': 0.9,\n",
                "          # 'categorical_feature': cat_cols\n",
                "          'tree_method':'gpu_hist'\n",
                "          }\n",
                "result_dict_lgb = train_model_classification(X=train, X_test=X_test, y=y_train, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n",
                "                                             verbose=500, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "transfer_results",
                "process_data"
            ],
            "source": [
                "sample_submission = pd.read_csv('../input/sample_submission.csv')\n",
                "sample_submission['isFraud'] = result_dict_lgb['prediction'] \n",
                "sample_submission.to_csv('submission_IEEE_.csv',index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import datetime\n",
                "import gc\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import lightgbm as lgb\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import mean_squared_error\n",
                "import time\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "np.random.seed(4590)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "df_train = pd.read_csv('../input/elo-merchant-category-recommendation/train.csv')\n",
                "df_test = pd.read_csv('../input/elo-merchant-category-recommendation/test.csv')\n",
                "df_hist_trans = pd.read_csv('../input/elo-merchant-category-recommendation/historical_transactions.csv')\n",
                "df_new_merchant_trans = pd.read_csv('../input/elo-merchant-category-recommendation/new_merchant_transactions.csv')\n",
                "df_merchants = pd.read_csv('../input/elo-merchant-category-recommendation/merchants.csv')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "gdf = df_hist_trans.groupby(\"card_id\")\n",
                "print(type(gdf))\n",
                "gdf = gdf.agg({'merchant_category_id':['min']}).reset_index()\n",
                "print(type(gdf))\n",
                "gdf.columns = [\"card_id\", \"merchant_category_id\"]\n",
                "df_train = pd.merge(df_train, gdf, on=\"card_id\", how=\"left\")\n",
                "df_test = pd.merge(df_test, gdf, on=\"card_id\", how=\"left\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "gdf = df_hist_trans.groupby(\"card_id\")\n",
                "print(type(gdf))\n",
                "gdf = gdf.agg({'merchant_category_id':['max']}).reset_index()\n",
                "print(type(gdf))\n",
                "gdf.columns = [\"card_id\", \"max_merchant_category_id\"]\n",
                "df_train = pd.merge(df_train, gdf, on=\"card_id\", how=\"left\")\n",
                "df_test = pd.merge(df_test, gdf, on=\"card_id\", how=\"left\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for df in [df_hist_trans,df_new_merchant_trans]:\n",
                "    df['category_2'].fillna(1.0,inplace=True)\n",
                "    df['category_3'].fillna('A',inplace=True)\n",
                "    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for df in [df_hist_trans,df_new_merchant_trans]:\n",
                "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
                "    df['year'] = df['purchase_date'].dt.year\n",
                "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
                "    df['month'] = df['purchase_date'].dt.month\n",
                "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
                "    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n",
                "    df['hour'] = df['purchase_date'].dt.hour\n",
                "    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n",
                "    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n",
                "    #https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/73244\n",
                "    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n",
                "    df['month_diff'] += df['month_lag']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "aggs = {}\n",
                "for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
                "    aggs[col] = ['nunique']\n",
                "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
                "aggs['installments'] = ['sum','max','min','mean','var']\n",
                "aggs['purchase_date'] = ['max','min']\n",
                "aggs['month_lag'] = ['max','min','mean','var']\n",
                "aggs['month_diff'] = ['mean']\n",
                "aggs['weekend'] = ['sum', 'mean']\n",
                "aggs['category_1'] = ['sum', 'mean']\n",
                "aggs['card_id'] = ['size']\n",
                "\n",
                "for col in ['category_2','category_3']:\n",
                "    df_new_merchant_trans[col+'_mean'] = df_new_merchant_trans.groupby([col])['purchase_amount'].transform('mean')\n",
                "    aggs[col+'_mean'] = ['mean']\n",
                "    \n",
                "new_columns = get_new_columns('new_hist',aggs)\n",
                "df_hist_trans_group = df_new_merchant_trans.groupby('card_id').agg(aggs)\n",
                "df_hist_trans_group.columns = new_columns\n",
                "df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
                "df_hist_trans_group['new_hist_purchase_date_diff'] = (df_hist_trans_group['new_hist_purchase_date_max'] - df_hist_trans_group['new_hist_purchase_date_min']).dt.days\n",
                "df_hist_trans_group['new_hist_purchase_date_average'] = df_hist_trans_group['new_hist_purchase_date_diff']/df_hist_trans_group['new_hist_card_id_size']\n",
                "df_hist_trans_group['new_hist_purchase_date_uptonow'] = (datetime.datetime.today() - df_hist_trans_group['new_hist_purchase_date_max']).dt.days\n",
                "df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n",
                "df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n",
                "del df_hist_trans_group;gc.collect()\n",
                "del df_new_merchant_trans;gc.collect()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_train.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "aggs = {}\n",
                "for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
                "    aggs[col] = ['nunique']\n",
                "\n",
                "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
                "aggs['installments'] = ['sum','max','min','mean','var']\n",
                "aggs['purchase_date'] = ['max','min']\n",
                "aggs['month_lag'] = ['max','min','mean','var']\n",
                "aggs['month_diff'] = ['mean']\n",
                "aggs['authorized_flag'] = ['sum', 'mean']\n",
                "aggs['weekend'] = ['sum', 'mean']\n",
                "aggs['category_1'] = ['sum', 'mean']\n",
                "aggs['card_id'] = ['size']\n",
                "\n",
                "for col in ['category_2','category_3']:\n",
                "    df_hist_trans[col+'_mean'] = df_hist_trans.groupby([col])['purchase_amount'].transform('mean')\n",
                "    aggs[col+'_mean'] = ['mean']    \n",
                "\n",
                "new_columns = get_new_columns('hist',aggs)\n",
                "df_hist_trans_group = df_hist_trans.groupby('card_id').agg(aggs)\n",
                "df_hist_trans_group.columns = new_columns\n",
                "df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
                "df_hist_trans_group['hist_purchase_date_diff'] = (df_hist_trans_group['hist_purchase_date_max'] - df_hist_trans_group['hist_purchase_date_min']).dt.days\n",
                "df_hist_trans_group['hist_purchase_date_average'] = df_hist_trans_group['hist_purchase_date_diff']/df_hist_trans_group['hist_card_id_size']\n",
                "df_hist_trans_group['hist_purchase_date_uptonow'] = (datetime.datetime.today() - df_hist_trans_group['hist_purchase_date_max']).dt.days\n",
                "df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n",
                "df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n",
                "del df_hist_trans_group;gc.collect()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "del df_hist_trans;gc.collect()\n",
                "#del df_new_merchant_trans;gc.collect()\n",
                "df_train.head(5)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import Imputer\n",
                "imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
                "df_merchants['avg_sales_lag3']=imp.fit(pd.DataFrame(df_merchants['avg_sales_lag3'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['avg_sales_lag3'].values.reshape(-1,1)))\n",
                "df_merchants['avg_sales_lag6']=imp.fit(pd.DataFrame(df_merchants['avg_sales_lag6'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['avg_sales_lag6'].values.reshape(-1,1)))\n",
                "df_merchants['avg_sales_lag12']=imp.fit(pd.DataFrame(df_merchants['avg_sales_lag12'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['avg_sales_lag12'].values.reshape(-1,1)))\n",
                "df_merchants['category_2']=imp.fit(pd.DataFrame(df_merchants['category_2'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['category_2'].values.reshape(-1,1)))\n",
                "df_merchants.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "aggs={}\n",
                "for col in ['avg_sales_lag3','avg_purchases_lag3','active_months_lag3','avg_sales_lag6','avg_purchases_lag6','active_months_lag6','avg_sales_lag12','avg_purchases_lag12','active_months_lag12','numerical_1','numerical_2']:\n",
                "    aggs[col]= ['mean']\n",
                "    \n",
                "new_columns= get_new_columns('merchants',aggs)\n",
                "df_merchants_group = df_merchants.groupby('merchant_category_id').agg(aggs)\n",
                "df_merchants_group.columns = new_columns\n",
                "df_merchants_group.reset_index(drop=False,inplace=True)\n",
                "df_train=df_train.merge(df_merchants_group,on='merchant_category_id',how='left')\n",
                "df_test=df_test.merge(df_merchants_group.reset_index(),on='merchant_category_id',how='left')\n",
                "df_train.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "df_merchants['max_merchant_category_id']=df_merchants['merchant_category_id']\n",
                "#df_test['max_merchant_category_id']=df_test['merchant_category_id']\n",
                "\n",
                "aggs={}\n",
                "for col in ['avg_sales_lag3','avg_purchases_lag3','active_months_lag3','avg_sales_lag6','avg_purchases_lag6','active_months_lag6','avg_sales_lag12','avg_purchases_lag12','active_months_lag12','numerical_1','numerical_2']:\n",
                "    aggs[col]= ['mean']\n",
                "    \n",
                "new_columns= get_new_columns('max_merchants',aggs)\n",
                "df_merchants_group = df_merchants.groupby('max_merchant_category_id').agg(aggs)\n",
                "df_merchants_group.columns = new_columns\n",
                "df_merchants_group.reset_index(drop=False,inplace=True)\n",
                "df_train=df_train.merge(df_merchants_group,on='max_merchant_category_id',how='left')\n",
                "df_test=df_test.merge(df_merchants_group.reset_index(),on='max_merchant_category_id',how='left')\n",
                "df_train.head()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "df_train['outliers'] = 0\n",
                "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
                "df_train['outliers'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for df in [df_train,df_test]:\n",
                "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
                "    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n",
                "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
                "    df['month'] = df['first_active_month'].dt.month\n",
                "    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n",
                "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
                "    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
                "    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n",
                "                     'new_hist_purchase_date_min']:\n",
                "        df[f] = df[f].astype(np.int64) * 1e-9\n",
                "    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n",
                "    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
                "\n",
                "for f in ['feature_1','feature_2','feature_3']:\n",
                "    order_label = df_train.groupby([f])['outliers'].mean()\n",
                "    df_train[f] = df_train[f].map(order_label)\n",
                "    df_test[f] = df_test[f].map(order_label)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n",
                "target = df_train['target']\n",
                "del df_train['target']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "features = [c for c in df_train.columns if c not in ['card_id', 'first_active_month']]\n",
                "categorical_feats = [c for c in features if 'feature_' in c]"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "param = {'num_leaves': 31,\n",
                "         'min_data_in_leaf': 30, \n",
                "         'objective':'binary',\n",
                "         'max_depth': 6,\n",
                "         'learning_rate': 0.01,\n",
                "         \"boosting\": \"rf\",\n",
                "         \"feature_fraction\": 0.9,\n",
                "         \"bagging_freq\": 1,\n",
                "         \"bagging_fraction\": 0.9 ,\n",
                "         \"bagging_seed\": 11,\n",
                "         \"metric\": 'binary_logloss',\n",
                "         \"lambda_l1\": 0.1,\n",
                "         \"verbosity\": -1,\n",
                "         \"random_state\": 2333}"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "from sklearn.model_selection import StratifiedKFold, KFold\n",
                "from sklearn.metrics import mean_squared_error\n",
                "from sklearn.metrics import log_loss\n",
                "%%time\n",
                "folds = KFold(n_splits=5, shuffle=True, random_state=15)\n",
                "oof = np.zeros(len(df_train))\n",
                "predictions = np.zeros(len(df_test))\n",
                "feature_importance_df = pd.DataFrame()\n",
                "\n",
                "start = time.time()\n",
                "\n",
                "\n",
                "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, target.values)):\n",
                "    print(\"fold n°{}\".format(fold_))\n",
                "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n",
                "    val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n",
                "\n",
                "    num_round = 10000\n",
                "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n",
                "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
                "    \n",
                "    fold_importance_df = pd.DataFrame()\n",
                "    fold_importance_df[\"feature\"] = features\n",
                "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
                "    fold_importance_df[\"fold\"] = fold_ + 1\n",
                "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
                "    \n",
                "    predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) / folds.n_splits\n",
                "\n",
                "print(\"CV score: {:<8.5f}\".format(log_loss(target, oof)))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "outlier_id = pd.DataFrame(df_outlier_prob.sort_values(by='target',ascending = False).head(25000)['card_id'])"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "check_results"
            ],
            "source": [
                "best_submission = pd.read_csv('../input/finaldata/submission_ashish.csv')\n",
                "most_likely_liers = best_submission.merge(outlier_id,how='right')\n",
                "most_likely_liers.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "%%time\n",
                "for card_id in most_likely_liers['card_id']:\n",
                "    model_without_outliers.loc[model_without_outliers['card_id']==card_id,'target']\\\n",
                "    = most_likely_liers.loc[most_likely_liers['card_id']==card_id,'target'].values"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "model_without_outliers.to_csv(\"combining_submission.csv\", index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import gc\n",
                "import os\n",
                "import pickle\n",
                "import random\n",
                "import re\n",
                "import sklearn.metrics\n",
                "import sklearn.model_selection\n",
                "import time\n",
                "import warnings\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from collections import OrderedDict\n",
                "from tqdm import tqdm, tqdm_notebook\n",
                "import functools\n",
                "import multiprocessing\n",
                "import sklearn.preprocessing\n",
                "import unicodedata\n",
                "import copy\n",
                "import time\n",
                "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.utils.data"
            ]
        },
        {
            "tags": [
                "check_results",
                "setup_notebook"
            ],
            "source": [
                "KAGGLE_RUN = (not os.path.exists('/opt/conda/home/.history'))\n",
                "if KAGGLE_RUN: print('Kaggle run')\n",
                "\n",
                "pd.options.display.float_format = '{:.6f}'.format"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "def tf_seed_everything(seed):\n",
                "    import tensorflow as tf\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    tf.set_random_seed(seed)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def f1_curve(target, preds, t_min=0.01, t_max=0.99, steps=99):\n",
                "    curve = {}\n",
                "    for t in np.linspace(t_min, t_max, steps):\n",
                "        with warnings.catch_warnings():\n",
                "            warnings.simplefilter(\"ignore\")\n",
                "            curve[t] = sklearn.metrics.f1_score(target, preds >= t)\n",
                "    return pd.Series(curve).sort_index()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "def torch_seed(seed):\n",
                "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "def torch_gc():\n",
                "    import gc\n",
                "    gc.collect()\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def keras_rnn_init(m):\n",
                "    for name, param in m.named_parameters():\n",
                "        if 'weight_ih' in name: torch.nn.init.xavier_uniform_(param)\n",
                "        if 'weight_hh' in name: torch.nn.init.orthogonal_(param)\n",
                "        if 'bias_' in name: torch.nn.init.constant_(param, 0)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "%%time\n",
                "torch_seed(42)\n",
                "\n",
                "qd = QuoraData()\n",
                "qd.convert_start(['glove', 'wiki'])\n",
                "\n",
                "prep = QuoraPreprocessor()\n",
                "input_df = qd.read_input()\n",
                "input_df['question_text'] = prep.transform(input_df.question_text)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "%%time\n",
                "qfe = QuoraFeatureExtractor(num_words=95000, max_len=70)\n",
                "input_X = qfe.fit_transform(input_df.question_text, fit_mask=input_df.target.notnull().values)\n",
                "print({ k: v.shape for (k, v) in input_X.items() })"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "%%time\n",
                "fold = Fold(input_df, input_X, seed=1005); torch_seed(fold.seed)\n",
                "learn1 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "%%time\n",
                "fold = Fold(input_df, input_X, seed=555); torch_seed(fold.seed)\n",
                "learn2 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "%%time\n",
                "fold = Fold(input_df, input_X, seed=1111); torch_seed(fold.seed)\n",
                "learn3 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "%%time\n",
                "fold = Fold(input_df, input_X, seed=1111); torch_seed(fold.seed)\n",
                "learn3 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "%%time\n",
                "fold = Fold(input_df, input_X, seed=3333); torch_seed(fold.seed)\n",
                "learn4 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "transfer_results"
            ],
            "source": [
                "%%time\n",
                "test_idx = fold.test_idx\n",
                "hold_idx = fold.holdout_idx\n",
                "hold_y = input_df.loc[fold.holdout_idx, 'target']\n",
                "ensemble_hold = []\n",
                "ensemble_test = []\n",
                "for li, learn in enumerate([learn1, learn2, learn3, learn4]):\n",
                "    for ep in [5]:\n",
                "        if ep >= len(learn.epoch_params): continue\n",
                "        learn.restore_epoch(ep)\n",
                "        ensemble_hold.append(learn.predict(idx=hold_idx))\n",
                "        f1 = f1_curve(hold_y, ensemble_hold[-1]).max()\n",
                "        print('learn%d ep%d %.6f' % (li+1, ep, f1))\n",
                "        ensemble_test.append(learn.predict(idx=test_idx))\n",
                "\n",
                "ensemble_test_s = pd.Series(np.mean(ensemble_test, axis=0), index=fold.data.loc[test_idx, 'qid'])\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "score = f1_curve(hold_y, np.mean(ensemble_hold, axis=0))\n",
                "thresh = score.idxmax()\n",
                "print('F1score=',score.max())\n",
                "\n",
                "\n",
                "\n",
                "submission_df = pd.read_csv('../input/sample_submission.csv')\n",
                "submission_df['prediction'] = (ensemble_test_s.loc[submission_df.qid] >= thresh).astype(int).values\n",
                "submission_df.to_csv('submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "from tqdm import tqdm"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "# Read dataset, divide it into train and test set\n",
                "dataset = pd.read_csv(\"/kaggle/input/iris/Iris.csv\")\n",
                "dataset.loc[dataset[\"Species\"] == \"Iris-setosa\",\"Species\"] = 1\n",
                "dataset.loc[dataset[\"Species\"] == \"Iris-versicolor\",\"Species\"] = 2\n",
                "dataset.loc[dataset[\"Species\"] == \"Iris-virginica\",\"Species\"] = 3\n",
                "dataset = dataset.to_numpy()\n",
                "np.random.shuffle(dataset)\n",
                "\n",
                "dataset = np.asarray(dataset,dtype = np.float64)\n",
                "len_dataset = dataset.shape[0]\n",
                "\n",
                "train = dataset[:int(0.75*len_dataset),:]\n",
                "X = train[:,1:-1]\n",
                "Y = train[:,-1]\n",
                "Y = pd.get_dummies(Y)\n",
                "Y = Y.to_numpy()\n",
                "op_neurons=len(Y[0])\n",
                "N,p = X.shape\n",
                "\n",
                "test = dataset[int(0.75*len_dataset):,:]\n",
                "Xt = test[:,1:-1]\n",
                "Yt = test[:,-1]\n",
                "Yt = pd.get_dummies(Yt)\n",
                "Yt = Yt.to_numpy()\n",
                "Nt,pt = Xt.shape"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "# dictionary\n",
                "# Implementation of Network using Gradient Descent\n",
                "epochs = 10000\n",
                "alpha = 0.1\n",
                "costs = []\n",
                "for num in tqdm(range(epochs)):\n",
                "    #Forward Propogation\n",
                "    a1 = np.dot(parameters[\"W1\"],X.T) + parameters[\"b1\"]\n",
                "    h1 = sigmoid(a1)\n",
                "    a2 = np.dot(parameters[\"W2\"],h1) + parameters[\"b2\"]\n",
                "    h2 = sigmoid(a2)\n",
                "    a3 = np.dot(parameters[\"W3\"],h2) + parameters[\"b3\"]\n",
                "    h3 = softmax(a3)\n",
                "    # Gradients for Backpropogation\n",
                "    \n",
                "    dL_da3 = -( Y.T - h3)\n",
                "    dL_dW3 = (1/N)*np.dot(dL_da3 , h2.T)\n",
                "    dL_db3 = (1/N)*(np.sum(dL_da3,axis=1,keepdims = True))\n",
                "    \n",
                "    dL_dh2 = np.dot(parameters[\"W3\"].T , dL_da3)\n",
                "    dL_da2 = np.multiply(dL_dh2,sigmoid_derivative(a2))\n",
                "    dL_dW2 = (1/N)*np.dot(dL_da2 , h1.T)\n",
                "    dL_db2 = (1/N)*(np.sum(dL_da2,axis=1,keepdims = True))\n",
                "    \n",
                "    dL_dh1 = np.dot(parameters[\"W2\"].T , dL_da2)\n",
                "    dL_da1 = np.multiply(dL_dh1,sigmoid_derivative(a1))\n",
                "    dL_dW1 = (1/N)*np.dot(dL_da1 , X)\n",
                "    dL_db1 = (1/N)*(np.sum(dL_da1,axis = 1,keepdims = True))\n",
                "    \n",
                "    # GD Updates\n",
                "    parameters[\"W3\"] = parameters[\"W3\"] - (alpha)*dL_dW3\n",
                "    parameters[\"b3\"] = parameters[\"b3\"] - (alpha)*dL_db3\n",
                "    parameters[\"W2\"] = parameters[\"W2\"] - (alpha)*dL_dW2\n",
                "    parameters[\"b2\"] = parameters[\"b2\"] - (alpha)*dL_db2\n",
                "    parameters[\"W1\"] = parameters[\"W1\"] - (alpha)*dL_dW1\n",
                "    parameters[\"b1\"] = parameters[\"b1\"] - (alpha)*dL_db1\n",
                "    costs.append(compute_cost(h3.T,Y))\n",
                "plt.plot(costs)\n",
                "print(\"Training Cost\",costs[-1])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "# Libraries\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "\n",
                "# set seed for reproducibility\n",
                "np.random.seed(0)\n",
                "\n",
                "#Data\n",
                "tumor_data = pd.read_csv('../input/data.csv')\n",
                "tumor_data.sample(5)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(\"The dataset has %d rows and %d columns\" % (tumor_data.shape[0], tumor_data.shape[1]))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tumor_data.describe(include='all')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tumor_data.dtypes"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tumor_data['Unnamed: 32'].sample(8)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "missing_values = tumor_data['Unnamed: 32'].isnull().sum()\n",
                "number_of_rows = tumor_data['Unnamed: 32'].shape[0]\n",
                "if missing_values == number_of_rows:\n",
                "    print('The whole \\'Unnamed: 32\\' column has empty values.')\n",
                "else:\n",
                "    print('There are non-empty values in the \\'Unnamed: 32\\' column.')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tumor_data.drop(['Unnamed: 32'], axis= 1, inplace = True)\n",
                "tumor_data.columns"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tumor_data.isna().sum()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tumor_data['target'] = tumor_data['diagnosis'].replace({'B': 1, 'M': 0})\n",
                "# Let's show if the convertion was rightly done\n",
                "tumor_data[['id', 'diagnosis', 'target']].sample(5)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tumor_data.drop(['diagnosis'], axis = 1, inplace = True)\n",
                "tumor_data.columns"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tumor_data.drop(['id'], axis = 1, inplace=True)\n",
                "tumor_data.columns"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.pairplot(\n",
                "    tumor_data,\n",
                "    vars = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean'],\n",
                "    hue = 'target'\n",
                ")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# First, we make sure that the graphic is crearly visible\n",
                "plt.figure(figsize = (30, 20))\n",
                "# And now, draw the heatmap\n",
                "sns.heatmap(tumor_data.corr(), cmap = \"RdBu_r\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Independent variables\n",
                "X = tumor_data.drop(['target'], axis = 1)\n",
                "# Dependent variable\n",
                "Y = tumor_data['target']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "from sklearn.svm import SVC\n",
                "svc_model = SVC()\n",
                "svc_model.fit(X_train, Y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "Y_predicted = svc_model.predict(X_test)\n",
                "Y_predicted"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "cm = confusion_matrix(Y_test, Y_predicted)\n",
                "sns.heatmap(cm, annot = True, cmap=\"Blues\")"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "print(classification_report(Y_test, Y_predicted))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tumor_data.describe()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "min_train = X_train.min()\n",
                "range_train = (X_train - min_train).max()\n",
                "X_train_scaled = (X_train - min_train)/range_train\n",
                "X_train_scaled.describe()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig = plt.figure(figsize = (20, 5))\n",
                "ax1 = fig.add_subplot(121)\n",
                "ax2 = fig.add_subplot(122)\n",
                "ax1.set_title('Values without normalization')\n",
                "ax2.set_title('Values with normalization')\n",
                "sns.scatterplot(x = X_train['texture_mean'], y = X_train['area_mean'], hue = Y_train, ax = ax1)\n",
                "sns.scatterplot(x = X_train_scaled['texture_mean'], y = X_train_scaled['area_mean'], hue = Y_train, ax = ax2)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "# Train the model again\n",
                "svc_model.fit(X_train_scaled, Y_train)\n",
                "# Create scaled test data\n",
                "X_test_scaled = (X_test - X_test.min())/(X_test - X_test.min()).max()\n",
                "# Calculate new predictions\n",
                "Y_predicted = svc_model.predict(X_test_scaled)\n",
                "# Draw confusion matrix\n",
                "cm = confusion_matrix(Y_test, Y_predicted)\n",
                "sns.heatmap(cm, annot = True, cmap='Blues')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "print(classification_report(Y_test, Y_predicted))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "check_results"
            ],
            "source": [
                "# We can automate the refinement of C and gamma using the GridSearchCV library\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "C_values =  [0.1, 1, 10, 100, 1000]\n",
                "gamma_values = [1, 0.1, 0.01, 0.001]\n",
                "grid = GridSearchCV(SVC(), {'C': C_values, 'gamma': gamma_values, 'kernel': ['rbf']}, refit = True, verbose = 4)\n",
                "# Find best pair of C and gamma values\n",
                "grid.fit(X_train_scaled, Y_train)\n",
                "grid.best_params_"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data",
                "check_results"
            ],
            "source": [
                "# We can use the optimized grid object directly to get predictions\n",
                "grid_predicted = grid.predict(X_test_scaled)\n",
                "\n",
                "cm = confusion_matrix(Y_test, grid_predicted)\n",
                "sns.heatmap(cm, annot = True, cmap = 'Blues')\n",
                "print(classification_report(Y_test, grid_predicted))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#1 Write a python program using Scikit_learn to print the keys, number of rows-columns , feature names and the description of the iris data.\n",
                "import pandas as pd\n",
                "iris_data = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(\"\\nKeys of Iris dataset:\")\n",
                "print(iris_data.keys())\n",
                "print(\"\\nNumber of rows and columns of Iris dataset:\")\n",
                "print(iris_data.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#2 Write a python program to get the number of observations,missing values and nan values.\n",
                "import pandas as pd\n",
                "iris = pd.read_csv(\"../input/daily-sun-spot-data-1818-to-2019/sunspot_data.csv\")\n",
                "print(iris.info())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "check_results"
            ],
            "source": [
                "#3 Write a python program to create a 2-D array with ones on the diagonal and zeroes elsewhere.\n",
                "import numpy as np\n",
                "from scipy import sparse\n",
                "eye = np.eye(6)\n",
                "print(\"NumPy array:\\n\", eye)\n",
                "sparse_matrix = sparse.csr_matrix(eye)\n",
                "print(\"\\nSciPy sparse CSR matrix:\\n\", sparse_matrix)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#4 Write a python program to load the iris data framefrom a given csv file into a dataframe and print the shape of the data, type of the data and first 3 rows.\n",
                "import pandas as pd\n",
                "data = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(\"Shape of the data:\")\n",
                "print(data.shape)\n",
                "print(\"\\nData Type:\")\n",
                "print(type(data))\n",
                "print(\"\\nFirst 3 rows:\")\n",
                "print(data.head(3))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "#Q.1 Write a Pandas program to get the powers of an array values element-wise.\n",
                "import pandas as pd\n",
                "df = pd.DataFrame({\"a\":[2,3,5,7,11,13,17],\"b\":[19,23,29,31,37,41,42],\"c\":[21,26,34,38,48,54,59]});\n",
                "print(df)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#Q.3 Write a panda sprogram to get the first three rows of a given DataFrame.\n",
                "import pandas as pd\n",
                "df = pd.read_csv(\"../input/prediction-of-asteroid-diameter/Asteroid.csv\");\n",
                "row = df[0:3];\n",
                "print(row)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#Q.4 Write a pandas program to select the specified columns and rows from a given DataFrame.\n",
                "import pandas as pd\n",
                "df = pd.read_csv(\"../input/prediction-of-asteroid-diameter/Asteroid.csv\");\n",
                "print(df.iloc[1:10,3:5])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#Q.5 write a pandas program to select the rows where the score is mising.\n",
                "import pandas as pd\n",
                "df = pd.read_csv(\"../input/prediction-of-asteroid-diameter/Asteroid.csv\");\n",
                "print(df[df[:].isnull()])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "import pandas as pd\n",
                "data = pd.read_csv(\"../input/titanic/train_and_test2.csv\")\n",
                "data\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "# 1.Create 1D,2D and boolean array using numpy.\n",
                "#1D array\n",
                "import numpy as np\n",
                "a = np.array([1,1,2,3,5,8,13,21,34,55,79])\n",
                "print(a)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "#2D array\n",
                "import numpy as np\n",
                "b = [2,3,5,7],[73,37,19,17],[101,111,153,59]\n",
                "c = np.array(b)\n",
                "print(c)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "#boolean array\n",
                "import numpy as np\n",
                "d = np.array([1,1,.5,0,0.66], dtype=bool)\n",
                "print(d)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "# 2. Extract the odd numbers from a 2D array using numpy package.\n",
                "import numpy as np\n",
                "b = np.array([[2,3,5], [1,6,11]], np.int32)\n",
                "c = (b[b%2!=0])\n",
                "print(c)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# 4. How to get the common items between two python numpy arrays?\n",
                "import numpy as np\n",
                "a1 = np.array([2,3,5,7,11])\n",
                "print(a1)\n",
                "a2 = [1,2,3,4,5,6,7,8,9]\n",
                "print(a2)\n",
                "print(\"common values between two arrays are\")\n",
                "print(np.intersect1d(a1,a2))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tupple1 = (1,1,2,3,5,8,21)\n",
                "print(tupple1)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "from sklearn.utils import shuffle\n",
                "import torch\n",
                "from torch import nn\n",
                "from torch.utils.data import Dataset\n",
                "import torch.nn.functional as F\n",
                "from torch.autograd import Variable\n",
                "from sklearn.metrics import mean_squared_error\n",
                "from datetime import datetime\n",
                "from datetime import timedelta\n",
                "from sklearn import preprocessing\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train = pd.read_csv('/kaggle/input/covid19-with-population/update_train_processed.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# The character ' will make later query function report an error,so it's replaced by a space\n",
                "train.Country.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "train.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "\n",
                "# There are few infinite values in the weather data,it will cause the training loss become NAN.Since the amount of np.inf is very few,it's simply replace by 0.\n",
                "train.replace(np.inf,0,inplace=True)\n",
                "\n",
                "# Transform percentage data to float\n",
                "def get_percent(x):\n",
                "    x = str(x)\n",
                "    x = x.strip('%')\n",
                "    x = int(x)/100\n",
                "    return x\n",
                "\n",
                "train.UrbanPopRate = train.UrbanPopRate.apply(lambda x:get_percent(x))\n",
                "\n",
                "# Transform date type\n",
                "def get_dt(x):\n",
                "    return datetime.strptime(x,'%Y-%m-%d')\n",
                "\n",
                "train.Date = train.Date.apply(lambda x:get_dt(x))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for index,row in train.iterrows():\n",
                "    if train.iloc[index].Province == train.iloc[index - 1].Province and train.iloc[index].ConfirmedCases < train.iloc[index-1].ConfirmedCases:\n",
                "        train.iloc[index,4] = train.iloc[index-1,4]\n",
                "    if train.iloc[index].Province == train.iloc[index - 1].Province and train.iloc[index].Fatalities < train.iloc[index-1].Fatalities:\n",
                "        train.iloc[index,5] = train.iloc[index-1,5]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_exam = train[['Country','Province','Date','ConfirmedCases','Fatalities']]\n",
                "diff_df = pd.DataFrame(columns = ['Country','Province','Date','ConfirmedCases','Fatalities'])\n",
                "for country in train_exam.Country.unique():\n",
                "    for province in train_exam[train_exam.Country == country].Province.unique():\n",
                "        province_df = train_exam.query(f\"Country == '{country}' and Province == '{province}'\")\n",
                "        conf = province_df.ConfirmedCases\n",
                "        fata = province_df.Fatalities\n",
                "        diff_conf = conf.diff()\n",
                "        diff_fata = fata.diff()\n",
                "        province_df.ConfirmedCases = diff_conf\n",
                "        province_df.Fatalities = diff_fata\n",
                "        diff_df = pd.concat([diff_df,province_df],0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(sum(diff_df.ConfirmedCases < 0),sum(diff_df.Fatalities<0))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "pd.set_option('mode.chained_assignment', None)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "scale_train = pd.DataFrame(columns = ['Id_x', 'Province', 'Country', 'Date', 'ConfirmedCases', 'Fatalities',\n",
                "       'Days_After_1stJan', 'Dayofweek', 'Month', 'Day', 'Population',\n",
                "       'Density', 'Land_Area', 'Migrants', 'MedAge', 'UrbanPopRate', 'Id_y',\n",
                "       'Lat', 'Long', 'temp', 'min', 'max', 'stp', 'slp', 'dewp', 'rh', 'ah',\n",
                "       'wdsp', 'prcp', 'fog', 'API_beds'])\n",
                "for country in train.Country.unique():\n",
                "    for province in train.query(f\"Country=='{country}'\").Province.unique():\n",
                "        province_df = train.query(f\"Country=='{country}' and Province=='{province}'\")\n",
                "        province_confirm = province_df.ConfirmedCases\n",
                "        province_fatalities = province_df.Fatalities\n",
                "        province_confirm = np.array(province_confirm).reshape(-1,1)\n",
                "        province_fatalities = np.array(province_confirm).reshape(-1,1)\n",
                "        scaler1= preprocessing.MinMaxScaler()\n",
                "        scaled_confirm = scaler1.fit_transform(province_confirm)\n",
                "        scaler2 = preprocessing.MinMaxScaler()\n",
                "        scaled_fata = scaler2.fit_transform(province_fatalities)\n",
                "        province_df['ConfirmedCases'] = scaled_confirm\n",
                "        province_df['Fatalities'] = scaled_fata\n",
                "        scale_train = pd.concat((scale_train,province_df),axis = 0,sort=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "trend_df = pd.DataFrame(columns={\"infection_trend\",\"fatality_trend\",\"quarantine_trend\",\"school_trend\",\"total_population\",\"expected_cases\",\"expected_fatalities\"})\n",
                "\n",
                "train_df = scale_train\n",
                "days_in_sequence = 14\n",
                "\n",
                "trend_list = []\n",
                "\n",
                "with tqdm(total=len(list(train_df.Country.unique()))) as pbar:\n",
                "    for country in train_df.Country.unique():\n",
                "        for province in train_df.query(f\"Country=='{country}'\").Province.unique():\n",
                "            province_df = train_df.query(f\"Country=='{country}' and Province=='{province}'\")\n",
                "            \n",
                "\n",
                "            for i in range(0,len(province_df)):\n",
                "                if i+days_in_sequence<=len(province_df):\n",
                "                    #prepare all the trend inputs\n",
                "                    infection_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].ConfirmedCases.values]\n",
                "                    fatality_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Fatalities.values]\n",
                "\n",
                "                    #preparing all the stable inputs\n",
                "                    days_after_1stJan = float(province_df.iloc[i].Days_After_1stJan)\n",
                "                    dayofweek = float(province_df.iloc[i].Dayofweek)\n",
                "                    month = float(province_df.iloc[i].Month)\n",
                "                    day= float(province_df.iloc[i].Day)\n",
                "                    population = float(province_df.iloc[i].Population)\n",
                "                    density = float(province_df.iloc[i].Density)\n",
                "                    land_area = float(province_df.iloc[i].Land_Area)\n",
                "                    migrants = float(province_df.iloc[i].Migrants)\n",
                "                    medage = float(province_df.iloc[i].MedAge)\n",
                "                    urbanpoprate = float(province_df.iloc[i].UrbanPopRate)\n",
                "                    beds = float(province_df.iloc[i].API_beds)\n",
                "\n",
                "                    #True cases in i+days_in_sequence-1 day\n",
                "                    expected_cases = float(province_df.iloc[i+days_in_sequence-1].ConfirmedCases)\n",
                "                    expected_fatalities = float(province_df.iloc[i+days_in_sequence-1].Fatalities)\n",
                "\n",
                "                    trend_list.append({\"infection_trend\":infection_trend,\n",
                "                                     \"fatality_trend\":fatality_trend,\n",
                "                                     \"stable_inputs\":[population,density,land_area,migrants,medage,urbanpoprate,beds],\n",
                "                                     \"expected_cases\":expected_cases,\n",
                "                                     \"expected_fatalities\":expected_fatalities})\n",
                "        pbar.update(1)\n",
                "trend_df = pd.DataFrame(trend_list)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "trend_df[\"temporal_inputs\"] = [np.asarray([trends[\"infection_trend\"],trends[\"fatality_trend\"]]) for idx,trends in trend_df.iterrows()]\n",
                "trend_df = shuffle(trend_df)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "trend_df.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Only keeping 25 sequences where the number of cases stays at 0, as there were way too many of these samples in our dataset.\n",
                "i=0\n",
                "temp_df = pd.DataFrame()\n",
                "for idx,row in trend_df.iterrows():\n",
                "    if sum(row.infection_trend)>0:\n",
                "        temp_df = temp_df.append(row)\n",
                "    else:\n",
                "        if i<25:\n",
                "            temp_df = temp_df.append(row)\n",
                "            i+=1\n",
                "trend_df = temp_df"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "sequence_length = 13\n",
                "training_percentage = 0.9\n",
                "# The purpose of '-2'and'+2' is to make the number of samples in the training test set divisible by batchsize\n",
                "training_item_count = int(len(trend_df)*training_percentage)\n",
                "validation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\n",
                "training_df = trend_df[:training_item_count-2]\n",
                "validation_df = trend_df[training_item_count+2:]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[\"temporal_inputs\"].values]),(training_item_count-2,2,sequence_length)),(0,2,1) )).astype(np.float32)\n",
                "X_stable_train = np.asarray([np.asarray(x) for x in training_df[\"stable_inputs\"]]).astype(np.float32)\n",
                "Y_cases_train = np.asarray([np.asarray(x) for x in training_df[\"expected_cases\"]]).astype(np.float32)\n",
                "Y_fatalities_train = np.asarray([np.asarray(x) for x in training_df[\"expected_fatalities\"]]).astype(np.float32)\n",
                "\n",
                "X_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in validation_df[\"temporal_inputs\"]]),(validation_item_count-2,2,sequence_length)),(0,2,1)) ).astype(np.float32)\n",
                "X_stable_test = np.asarray([np.asarray(x) for x in validation_df[\"stable_inputs\"]]).astype(np.float32)\n",
                "Y_cases_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_cases\"]]).astype(np.float32)\n",
                "Y_fatalities_test = np.asarray([np.asarray(x) for x in validation_df[\"expected_fatalities\"]]).astype(np.float32)\n",
                "\n",
                "# Transform to tensor type\n",
                "X_temporal_train = torch.from_numpy(X_temporal_train)\n",
                "X_stable_train = torch.from_numpy(X_stable_train)\n",
                "Y_cases_train = torch.from_numpy(Y_cases_train)\n",
                "Y_fatalities_train = torch.from_numpy(Y_fatalities_train)\n",
                "\n",
                "X_temporal_test = torch.from_numpy(X_temporal_test)\n",
                "X_stable_test = torch.from_numpy(X_stable_test)\n",
                "Y_cases_test = torch.from_numpy(Y_cases_test)\n",
                "Y_fatalities_test = torch.from_numpy(Y_fatalities_test)\n",
                "\n",
                "# Merge two objective values\n",
                "Y_train = torch.cat((Y_cases_train.reshape(14770,1),Y_fatalities_train.reshape(14770,1)),1)\n",
                "Y_test = torch.cat((Y_cases_test.reshape(1640,1),Y_fatalities_test.reshape(1640,1)),1)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(len(X_temporal_train),len(X_temporal_test))"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "# Create train,test loader for training\n",
                "class MyDataset(Dataset):\n",
                "    def __init__(self, data1,data2, labels):\n",
                "        self.trend= data1\n",
                "        self.stable= data2\n",
                "        self.labels = labels  \n",
                "\n",
                "    def __getitem__(self, index):    \n",
                "        trend,stable, labels = self.trend[index], self.stable[index], self.labels[index]\n",
                "        return trend,stable,labels\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.trend) \n",
                "    \n",
                "train_ds = MyDataset(data1 = X_temporal_train,data2 = X_stable_train,labels = Y_train)\n",
                "test_ds =MyDataset(data1 = X_temporal_test,data2 = X_stable_test,labels = Y_test)\n",
                "train_loader = torch.utils.data.DataLoader(train_ds,batch_size = 10,shuffle=False)\n",
                "test_loader = torch.utils.data.DataLoader(test_ds,batch_size = 10,shuffle=False)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Define Model\n",
                "class Net(nn.Module):\n",
                "    def __init__(self):\n",
                "            super(Net, self).__init__()\n",
                "            self.lstm = nn.LSTM(2,32,1,dropout = 0.5)\n",
                "            \n",
                "            self.stable_full = nn.Linear(7,16)\n",
                "            nn.init.kaiming_normal_(self.stable_full.weight)\n",
                "            self.BN1 = nn.BatchNorm1d(16)\n",
                "            self.stable_dropout = nn.Dropout(0.5)\n",
                "            \n",
                "            self.merge_full = nn.Linear(16+13*32,64)# stable:（5*16）  lstm:（13，5，32）\n",
                "            nn.init.kaiming_normal_(self.merge_full.weight)\n",
                "            self.BN2 = nn.BatchNorm1d(64)\n",
                "            self.merge_dropout = nn.Dropout(0.3)\n",
                "            self.merge_full2 = nn.Linear(64,2)\n",
                "            nn.init.kaiming_normal_(self.merge_full2.weight)\n",
                "\n",
                "    def reset_hidden(self):\n",
                "        self.hidden = (torch.zeros(self.hidden[0].shape), torch.zeros(self.hidden[1].shape))\n",
                "        \n",
                "    def forward(self, x_trend,x_stable):\n",
                "        batch_size = x_trend.reshape(13,-1,2).size(1)\n",
                "        x_trend = x_trend.reshape(13,batch_size,2)\n",
                "        x_trend, self.hidden = self.lstm(x_trend)\n",
                "        \n",
                "        x_stable = self.stable_dropout(F.relu(self.BN1(self.stable_full(x_stable))))\n",
                "        \n",
                "        s, b, h = x_trend.shape  #(seq, batch, hidden)\n",
                "        x_trend = x_trend.view(b, s*h)\n",
                "        x_merge = torch.cat((x_trend,x_stable),axis = 1)\n",
                "        x_merge = F.relu(self.merge_full2(self.merge_dropout(F.relu(self.BN2(self.merge_full(x_merge))))))\n",
                "        return x_merge"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# Training Settings\n",
                "model = Net()\n",
                "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
                "criterion = nn.MSELoss()"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "# Training process\n",
                "def train_model(epoch):\n",
                "    model.train()\n",
                "    for batch_idx, (trend, stable, target) in enumerate(train_loader):\n",
                "        trend, stable, target = Variable(trend), Variable(stable),Variable(target)\n",
                "        optimizer.zero_grad()\n",
                "        output = model(trend,stable)\n",
                "        loss = criterion(output, target)  \n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        if batch_idx % 300 == 0:\n",
                "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
                "                epoch, batch_idx * len(trend), len(train_loader.dataset),\n",
                "                100. * batch_idx / len(train_loader), loss.data))\n",
                "\n",
                "def test_model(epoch):\n",
                "    model.eval()\n",
                "    test_loss = 0\n",
                "    y_pred = []\n",
                "    y_true = []\n",
                "    for trend, stable, target in test_loader:\n",
                "        trend,stable, target = Variable(trend),Variable(stable),Variable(target)\n",
                "        output = model(trend,stable)\n",
                "        test_loss += criterion(output, target).data\n",
                "        y_pred.append(output)\n",
                "        y_true.append(target)\n",
                "\n",
                "    y_pred = torch.cat(y_pred, dim=0)\n",
                "    y_true = torch.cat(y_true, dim=0)\n",
                "    test_loss = test_loss\n",
                "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
                "    MSE = mean_squared_error(y_true.detach().numpy(), y_pred.detach().numpy())\n",
                "    print('\\nTest set: Average loss: {:.4f}, MSE: {} \\n'.format(\n",
                "        test_loss, MSE \n",
                "        ))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "for epoch in range(1, 21):\n",
                "    train_model(epoch)\n",
                "    test_model(epoch)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# In order to use query function,transform datetime to string\n",
                "def get_str_date(x):\n",
                "    x = str(x)[0:10]\n",
                "    return x\n",
                "\n",
                "scale_train.Date = scale_train.Date.apply(lambda x: get_str_date(x))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "del scale_train['Id']"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "# read test_df and a new train_df\n",
                "test_df = pd.read_csv('/kaggle/input/covid-with-weather-and-population/test_processed.csv')\n",
                "train_df2 =  pd.read_csv('/kaggle/input/covid19-with-population/update_train_processed.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "test_df = test_df.query(\"Date > '2020-04-25'\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# same preprocess as before\n",
                "train_df2.Country.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "train_df2.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "train_df2.replace(np.inf,0,inplace=True)\n",
                "train_df2.UrbanPopRate = train_df2.UrbanPopRate.apply(lambda x:get_percent(x))\n",
                "\n",
                "test_df.Country.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "test_df.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "test_df.replace(np.inf,0,inplace=True)\n",
                "test_df.UrbanPopRate = test_df.UrbanPopRate.apply(lambda x:get_percent(x))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# make train dataframe and test dataframe have same columns\n",
                "test_df['ConfirmedCases'] = np.NAN\n",
                "test_df['Fatalities'] = np.NAN\n",
                "test_df['Id_x'] = 0\n",
                "test_df['Id_y'] = 0\n",
                "test_df = test_df[list(scale_train.columns)]\n",
                "# merge scale_train and test\n",
                "total_df = pd.concat((scale_train,test_df),axis = 0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def get_conf_scaler(country,province):\n",
                "    train_df2_province = train_df2.query(f\"Country == '{country}' and Province =='{province}'\")\n",
                "    train_df2_province_conf = train_df2_province['ConfirmedCases']\n",
                "    train_df2_province_fata = train_df2_province['Fatalities']\n",
                "    province_conf_scaler = preprocessing.MinMaxScaler()\n",
                "    province_fata_scaler = preprocessing.MinMaxScaler()\n",
                "    province_conf_scaler.fit(np.array(train_df2_province_conf).reshape(-1,1))\n",
                "    province_fata_scaler.fit(np.array(train_df2_province_fata).reshape(-1,1))\n",
                "    return province_conf_scaler,province_fata_scaler"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "source": [
                "def get_pred_for_province(country,province):\n",
                "    start_date = datetime.strptime('2020-04-13','%Y-%m-%d')\n",
                "    end_date = datetime.strptime('2020-04-26','%Y-%m-%d')\n",
                "    pred = []\n",
                "    trend_input,stable_input = get_initial_input(country,province,str(start_date)[0:10],str(end_date)[0:10])\n",
                "    for i in range(0,19):\n",
                "        start = str(start_date+timedelta(days = i))[0:10]\n",
                "        end = str(end_date+timedelta(days = i))[0:10]\n",
                "        output,original_output = get_pred(country,province,trend_input,stable_input)\n",
                "        pred.append([end,original_output[0],original_output[1]])\n",
                "        trend_input = trend_input[1:]\n",
                "        output_tensor = torch.as_tensor(output)\n",
                "        new = torch.as_tensor(output_tensor.reshape(1,1,2))\n",
                "        trend_input = torch.cat((trend_input,new),0)\n",
                "    pred_for_province = pd.DataFrame(pred,columns=['Date','confirmed_pred','fata_pred'])\n",
                "    pred_for_province['Province'] = province\n",
                "    pred_for_province['Country'] = country\n",
                "    pred_for_province = pred_for_province[['Country','Province','Date','confirmed_pred','fata_pred']]\n",
                "    return pred_for_province"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "pred_table = pd.DataFrame(columns = ['Country','Province','Date','confirmed_pred','fata_pred'])\n",
                "for country in test_df.Country.unique():\n",
                "    for province in test_df.query(f\"Country == '{country}'\")['Province'].unique():\n",
                "        province_pred = get_pred_for_province(country,province)\n",
                "        pred_table = pd.concat((pred_table,province_pred),0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pred_table"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "original_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "original_train.Date = original_train.Date.apply(lambda x:get_dt(x))\n",
                "original_train.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)\n",
                "for i in range(len(original_train)):\n",
                "    if original_train.Province[i] is np.NaN:\n",
                "        original_train.Province[i] = original_train.Country[i]\n",
                "        \n",
                "for i in range(len(original_train)):\n",
                "    if original_train.Date[i]<datetime.strptime('2020-04-02','%Y-%m-%d') or original_train.Date[i]>datetime.strptime('2020-04-25','%Y-%m-%d'):\n",
                "        original_train.drop(i,inplace=True)\n",
                "        \n",
                "del original_train['Id']\n",
                "#del original_train['Country']\n",
                "\n",
                "original_train.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n",
                "#del pred_table['Country']\n",
                "original_train.Date = original_train.Date.apply(lambda x:get_str_date(x))"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "original_test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\n",
                "for i in range(len(original_test)):\n",
                "    if original_test.iloc[i]['Province_State'] is np.NaN:\n",
                "        original_test.iloc[i,1] = original_test.iloc[i,2]\n",
                "        \n",
                "original_test.rename(columns = {'Country_Region':'Country','Province_State':'Province'},inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final = pd.concat([pred_table,original_train],axis = 0,sort = True)\n",
                "final_submit = pd.merge(original_test,final,on = ['Country','Province','Date'],how = 'left')\n",
                "submission = final_submit[['ForecastId','ConfirmedCases','Fatalities']]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "submission"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submission.to_csv('/kaggle/working/submission.csv',index = False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from collections import Counter\n",
                "from datetime import datetime\n",
                "from google.cloud import bigquery\n",
                "import matplotlib.pyplot as plt\n",
                "import os"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n",
                "test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)\n",
                "test.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for i in range(len(train)):\n",
                "    if train.Province[i] is np.NaN:\n",
                "        train.Province[i] = train.Country[i]\n",
                "for i in range(len(test)):\n",
                "    if test.Province[i] is np.NaN:\n",
                "        test.Province[i] = test.Country[i]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train['Days_After_1stJan'] = train.Date.apply(lambda x :get_days(x))\n",
                "test['Days_After_1stJan'] = test.Date.apply(lambda x :get_days(x))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train['Date'] = train.Date.apply(lambda x:get_dt(x))\n",
                "test['Date'] = test.Date.apply(lambda x:get_dt(x))\n",
                "train['Dayofweek'] = train.Date.apply(lambda x:get_dayofweek(x))\n",
                "test['Dayofweek'] = test.Date.apply(lambda x:get_dayofweek(x))\n",
                "train['Month'] = train.Date.apply(lambda x:get_month(x))\n",
                "test['Month'] = test.Date.apply(lambda x:get_month(x))\n",
                "train['Day'] = train.Date.apply(lambda x:get_day(x))\n",
                "test['Day'] = test.Date.apply(lambda x:get_day(x))"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "country_info = pd.read_csv('/kaggle/input/population/population_by_country_2020.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "population = pd.DataFrame(country_info.iloc[:,[0,1,4,5,6,8,9]])\n",
                "population.columns = ['Country','Population','Density','Land_Area','Migrants','MedAge','UrbanPopRate']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for i in range(len(population)):\n",
                "    if np.isnan(population.Migrants[i]):\n",
                "        population.Migrants[i] = np.nanmedian(population.Migrants)\n",
                "    if population.MedAge[i] == 'N.A.':\n",
                "        population.MedAge[i] = 19\n",
                "    if population.UrbanPopRate[i] == 'N.A.':\n",
                "        population.UrbanPopRate[i] = '57%'"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "kosovo = pd.DataFrame([['Kosovo'],[2000700],[168],[10887],[0],[19],['57%']])\n",
                "kosovo = kosovo.T\n",
                "kosovo.columns = population.columns\n",
                "\n",
                "\n",
                "population = population.append(kosovo)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "westbank = pd.DataFrame([['West Bank and Gaza'],[2697687],[485],[5559],[0],[19],['57%']])\n",
                "westbank = westbank.T\n",
                "westbank.columns = population.columns\n",
                "\n",
                "population = population.append(westbank)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "DP = pd.DataFrame(['Diamond Princess',2666,191522,0.01392,2666,19,'100%'])\n",
                "DP = DP.T\n",
                "DP.columns = population.columns\n",
                "\n",
                "population = population.append(DP)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "Shangdan = pd.DataFrame(['MS Zaandam',1432,189618,0.007552,1432,19,'100%'])\n",
                "Shangdan = Shangdan.T\n",
                "Shangdan.columns = population.columns\n",
                "\n",
                "population = population.append(Shangdan)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "Congo = population[population.Country == 'Congo']\n",
                "Congo['Country'] = 'Congo (Brazzaville)'\n",
                "new1 = Congo.copy()\n",
                "Congo['Country'] = 'Congo (Kinshasa)'\n",
                "new2 = Congo.copy()\n",
                "\n",
                "population = population.append(new1)\n",
                "population = population.append(new2)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "population.Country[population.Country == 'United States'] = 'US'\n",
                "population.Country[population.Country == 'Taiwan'] = 'Taiwan*'\n",
                "population.Country[population.Country == 'South Korea'] = 'Korea, South'\n",
                "population.Country[population.Country == 'Côte d\\'Ivoire'] = 'Cote d\\'Ivoire'\n",
                "population.Country[population.Country == 'Czech Republic (Czechia)'] = 'Czechia'\n",
                "population.Country[population.Country == 'Myanmar'] = 'Burma'\n",
                "population.Country[population.Country == 'St. Vincent & Grenadines'] = 'Saint Vincent and the Grenadines'\n",
                "population.Country[population.Country == 'Saint Kitts & Nevis']  = 'Saint Kitts and Nevis'\n",
                "population.Country[population.Country == 'Sao Tome & Principe']  = 'Sao Tome and Principe'"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train = pd.merge(train,population,left_on = 'Country',right_on='Country',how='left')\n",
                "test = pd.merge(test,population,left_on = 'Country',right_on='Country',how = 'left')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "temperature = pd.read_csv('/kaggle/input/weather-data-for-covid19-data-analysis/training_data_with_weather_info_week_4.csv')\n",
                "temperature.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# fill NAN of Province with Country name\n",
                "for i in range(len(temperature)):\n",
                "    if temperature.Province[i] is np.NaN:\n",
                "        temperature.Province[i] = temperature.Country[i]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#delete useless features \n",
                "del temperature['ConfirmedCases']\n",
                "del temperature['Fatalities']\n",
                "del temperature['country+province']\n",
                "del temperature['day_from_jan_first']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#transform date to datetime type\n",
                "temperature.Date = temperature.Date.apply(lambda x:get_dt(x))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "day1 = datetime.strptime('2020-04-09','%Y-%m-%d')\n",
                "day2 = datetime.strptime('2020-04-10','%Y-%m-%d')\n",
                "day3 = datetime.strptime('2020-04-11','%Y-%m-%d')\n",
                "day4 = datetime.strptime('2020-04-12','%Y-%m-%d')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "index_delete = []\n",
                "for i in range(len(train)):\n",
                "    if (train.Date[i] == day1) or (train.Date[i] == day2) or (train.Date[i] == day3) or (train.Date[i] == day4):\n",
                "        index_delete.append(i)\n",
                "        \n",
                "train = (train.drop(index = index_delete)).reset_index(drop = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train = pd.merge(train,temperature,on=['Country','Province','Date'],how='left')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#fill NAN\n",
                "train['Lat'][train['Lat'].isnull()] = np.nanmedian(train['Lat'])\n",
                "train['Long'][train['Long'].isnull()] = np.nanmedian(train['Long'])\n",
                "train['temp'][train['temp'].isnull()] = np.nanmedian(train['temp'])\n",
                "train['min'][train['min'].isnull()] = np.nanmedian(train['min'])\n",
                "train['max'][train['max'].isnull()] = np.nanmedian(train['max'])\n",
                "train['slp'][train['slp'].isnull()] = np.nanmedian(train['slp'])\n",
                "train['dewp'][train['dewp'].isnull()] = np.nanmedian(train['dewp'])\n",
                "train['rh'][train['rh'].isnull()] = np.nanmedian(train['rh'])\n",
                "train['ah'][train['ah'].isnull()] = np.nanmedian(train['ah'])\n",
                "train['stp'][train['stp'].isnull()] = np.nanmedian(train['stp'])\n",
                "train['wdsp'][train['wdsp'].isnull()] = np.nanmedian(train['wdsp'])\n",
                "train['prcp'][train['prcp'].isnull()] = np.nanmedian(train['prcp'])\n",
                "train['fog'][train['fog'].isnull()] = np.nanmedian(train['fog'])"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "!pip install pmdarima"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pmdarima"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#the outlier 'inf' will make the auto_arima come to an error,so it's replaced by 0\n",
                "train.replace(np.inf,0,inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "source": [
                "#Using arima to predict the weather information for future\n",
                "date_pred_df = pd.DataFrame(sorted(list(set(test.Date))),columns=['Date'])\n",
                "date_pred_df = date_pred_df[7:]\n",
                "date_pred_df.reset_index(inplace = True)\n",
                "del date_pred_df['index']\n",
                "\n",
                "nperiods = (datetime.strptime('2020-05-14','%Y-%m-%d')-datetime.strptime('2020-04-08','%Y-%m-%d')).days\n",
                "\n",
                "weather_feature = ['temp', 'min', 'max', 'stp', 'slp', 'dewp', 'rh', 'ah','wdsp', 'prcp', 'fog']\n",
                "weather_pred = pd.DataFrame(columns=['Date','temp', 'min', 'max', 'stp', 'slp', 'dewp', 'rh', 'ah','wdsp', 'prcp', 'fog'])\n",
                "for prov in list(set(train.Province)):\n",
                "    df = train[train.Province == prov]\n",
                "    province_pred = date_pred_df.copy()\n",
                "    for feature in weather_feature:\n",
                "        ts = df[feature]\n",
                "        model = pmdarima.auto_arima(ts)\n",
                "        pred = model.predict(n_periods = nperiods)\n",
                "        province_pred[feature] = pred\n",
                "    province_pred['Province'] = prov\n",
                "    weather_pred = pd.concat([weather_pred,province_pred],axis = 0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for i in range(len(test)):\n",
                "    if test.Date[i]<datetime.strptime('2020-04-09','%Y-%m-%d'):\n",
                "        test.drop(i,inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#get longitude and latitude dataframe and merge it into test df\n",
                "df_longlat = pd.DataFrame(columns = ['Province','Lat','Long'])\n",
                "for i in range(len(train)):\n",
                "    if train.Province[i] not in list(df_longlat.Province):\n",
                "        df_longlat = df_longlat.append(train.iloc[i][['Province','Lat','Long']])\n",
                "        \n",
                "test = pd.merge(test,df_longlat,on = 'Province',how = 'left')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#adding weather feature to test data\n",
                "test = pd.merge(test,weather_pred,on = ['Province','Date'],how = 'left')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "API_beds = pd.read_csv('/kaggle/input/newest-bed-api-for-each-country/Newest_avg_bed_API.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#merge\n",
                "train = pd.merge(train,API_beds,left_on='Country',right_on='Country',how='left')\n",
                "test = pd.merge(test,API_beds,left_on='Country',right_on='Country',how='left')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#fill NAN\n",
                "train.API_beds[train.API_beds.isnull()] = np.nanmedian(train.API_beds)\n",
                "test.API_beds[test.API_beds.isnull()] = np.nanmedian(test.API_beds)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X = train.copy()\n",
                "X_test = test.copy()\n",
                "\n",
                "Province_set = set(X.Province)\n",
                "Country_set = set(X.Country)\n",
                "\n",
                "X = pd.concat([X,pd.get_dummies(X.Country)],axis=1)\n",
                "X_test = pd.concat([X_test,pd.get_dummies(X_test.Country)],axis=1)\n",
                "X = pd.concat([X,pd.get_dummies(X.Province)[Province_set - Country_set]],axis=1)\n",
                "X_test = pd.concat([X_test,pd.get_dummies(X_test.Province)[Province_set - Country_set]],axis=1)\n",
                "\n",
                "y_confirm = X.ConfirmedCases\n",
                "y_fata = X.Fatalities\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "del X['ConfirmedCases']\n",
                "del X['Fatalities']\n",
                "del X['Id_x']\n",
                "del X['Date']\n",
                "del X['Id_y']\n",
                "del X['Province']\n",
                "del X['Country']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ForecastId = X_test.ForecastId"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X['UrbanPopRate'] = X.UrbanPopRate.apply(lambda x:get_percent(x))\n",
                "X_test['UrbanPopRate'] = X_test.UrbanPopRate.apply(lambda x:get_percent(x))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "del X_test['ForecastId']\n",
                "del X_test['Date']\n",
                "del X_test['Province']\n",
                "del X_test['Country']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X = pd.DataFrame(X,dtype=float)\n",
                "X_test = pd.DataFrame(X_test,dtype=float)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X_test.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import xgboost as xgb"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "reg_confirm = xgb.XGBRegressor()\n",
                "reg_confirm.fit(X,y_confirm)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "pred_confirm = reg_confirm.predict(X_test)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "reg_fata = xgb.XGBRegressor()\n",
                "reg_fata.fit(X,y_fata)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "pred_fata = reg_fata.predict(X_test)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "submit = pd.DataFrame(ForecastId)\n",
                "submit['ConfirmedCases']=pred_confirm\n",
                "submit['Fatalities']=pred_fata"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "original_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n",
                "new_test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "original_train.Date = original_train.Date.apply(lambda x:get_dt(x))\n",
                "original_train.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)\n",
                "for i in range(len(original_train)):\n",
                "    if original_train.Province[i] is np.NaN:\n",
                "        original_train.Province[i] = original_train.Country[i]\n",
                "        \n",
                "for i in range(len(original_train)):\n",
                "    if original_train.Date[i]<datetime.strptime('2020-04-02','%Y-%m-%d') or original_train.Date[i]>datetime.strptime('2020-04-08','%Y-%m-%d'):\n",
                "        original_train.drop(i,inplace=True)\n",
                "        \n",
                "del original_train['Id']\n",
                "del original_train['Country']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final = pd.concat([temp_submit_df,original_train],axis = 0)\n",
                "final = final.sort_values(by=['Province','Date'])\n",
                "final_submit = pd.DataFrame(new_test.ForecastId,columns=['ForecastId'])\n",
                "final_submit['Confirmed'] = final.ConfirmedCases.values\n",
                "final_submit['Fatalities'] = final.Fatalities.values"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final_submit"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "final_submit.to_csv('/kaggle/working/submission.csv',index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "%matplotlib inline\n",
                "import numpy as np \n",
                "import pandas as pd \n",
                "from sklearn.model_selection import KFold\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from lightgbm import LGBMClassifier, LGBMRegressor\n",
                "from sklearn.model_selection import cross_validate\n",
                "from tqdm import tqdm_notebook\n",
                "import multiprocessing\n",
                "import matplotlib.pyplot as plt # drawing graph\n",
                "import warnings; warnings.filterwarnings(\"ignore\") \n",
                "import os; os.environ['OMP_NUM_THREADS'] = '4' # speed up using 4 cpu"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def infer_model(df, features, target, n_jobs):\n",
                "    model_class = LGBMRegressor\n",
                "    if len(df[target].value_counts()) == 2:\n",
                "        df[target] = LabelEncoder().fit_transform(df[target])\n",
                "        model_class = LGBMClassifier\n",
                "\n",
                "    categoricals = []\n",
                "    for f in features:\n",
                "        if df[f].dtype == object:\n",
                "            df[f] = LabelEncoder().fit_transform(df[f].apply(str))\n",
                "            categoricals.append(f)\n",
                "\n",
                "    min_child_samples = int(0.01*df.shape[0])\n",
                "\n",
                "    model = model_class(min_child_samples=min_child_samples, n_jobs=n_jobs)\n",
                "\n",
                "    return model, df, categoricals"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data"
            ],
            "source": [
                "# source : https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/\n",
                "df = pd.read_excel('../input/WA_Fn-UseC_-HR-Employee-Attrition.xlsx', sheet_name=0,dtype=dtypes)\n",
                "print(\"Shape of dataframe is: {}\".format(df.shape))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Cheese view of target data\n",
                "labels = 'Leave', 'Stay'\n",
                "colors = ['#ff9999','#99ff99']\n",
                "sizes = [df['Attrition'].value_counts()['Yes'],df['Attrition'].value_counts()['No']]\n",
                "explode = (0.25, 0)\n",
                "fig1, ax1 = plt.subplots()\n",
                "ax1.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.2f%%',\n",
                "        shadow=True, startangle=90)\n",
                "ax1.axis('equal')  \n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "sample_df = df.sample(frac=1, random_state=0)\n",
                "sample_df.sort_values(\"EmployeeNumber\", inplace=True)\n",
                "\n",
                "cv = KFold(n_splits=4, shuffle=False, random_state=0)\n",
                "target = \"Attrition\"\n",
                "features = [col for col in df.columns if col != target]\n",
                "\n",
                "lofo = LOFOImportance(sample_df, features, target, cv=cv, scoring=\"roc_auc\")\n",
                "importance_df= lofo.get_importance()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "plot_importance(importance_df, figsize=(12, 20))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from tpot import TPOTClassifier\n",
                "from sklearn.model_selection import StratifiedKFold,train_test_split\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "import os\n",
                "os.environ['OMP_NUM_THREADS'] = '4'\n",
                "print(os.listdir(\"../input\"))\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "# source : https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/\n",
                "df = pd.read_excel('../input/WA_Fn-UseC_-HR-Employee-Attrition.xlsx', sheet_name=0,dtype=dtypes)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cont=[]\n",
                "cat=[]\n",
                "for key, value in dtypes.items():\n",
                "    if key!='Attrition':\n",
                "        if value == \"int64\":\n",
                "            cont.append(key)\n",
                "        else:\n",
                "            cat.append(key)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head(5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df = pd.get_dummies(df, columns=cat)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df['Attrition']=df.Attrition.eq('Yes').mul(1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(df[cont], df['Attrition'], test_size=0.2, random_state=42)\n",
                "train = pd.concat([X_train, y_train], 1)\n",
                "test = pd.concat([X_test, y_test], 1)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "pipeline_optimizer = TPOTClassifier()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5, n_jobs=-1,random_state=42, verbosity=2, early_stop=5)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "pipeline_optimizer.fit(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "print(pipeline_optimizer.score(X_test, y_test))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.utils import shuffle\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "import warnings; \n",
                "warnings.filterwarnings(\"ignore\") \n",
                "import os \n",
                "os.environ['OMP_NUM_THREADS'] = '8' # speed up using 8 cpu"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "tourney_result = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WDataFiles_Stage1/WNCAATourneyCompactResults.csv')\n",
                "tourney_seed = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WDataFiles_Stage1/WNCAATourneySeeds.csv')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# deleting unnecessary columns\n",
                "tourney_result = tourney_result.drop(['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT'], axis=1)\n",
                "tourney_result"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "tourney_result.rename(columns={'Seed':'WSeed'}, inplace=True)\n",
                "tourney_result = tourney_result.drop('TeamID', axis=1)\n",
                "tourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "tourney_result.rename(columns={'Seed':'LSeed'}, inplace=True)\n",
                "tourney_result = tourney_result.drop('TeamID', axis=1)\n",
                "tourney_result"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "def get_seed(x):\n",
                "    return int(x[1:3])\n",
                "\n",
                "tourney_result['WSeed'] = tourney_result['WSeed'].map(lambda x: get_seed(x))\n",
                "tourney_result['LSeed'] = tourney_result['LSeed'].map(lambda x: get_seed(x))\n",
                "tourney_result"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "season_result = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WDataFiles_Stage1/WRegularSeasonCompactResults.csv')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "season_win_result = season_result[['Season', 'WTeamID', 'WScore']]\n",
                "season_lose_result = season_result[['Season', 'LTeamID', 'LScore']]\n",
                "season_win_result.rename(columns={'WTeamID':'TeamID', 'WScore':'Score'}, inplace=True)\n",
                "season_lose_result.rename(columns={'LTeamID':'TeamID', 'LScore':'Score'}, inplace=True)\n",
                "season_result = pd.concat((season_win_result, season_lose_result)).reset_index(drop=True)\n",
                "season_result"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "season_score = season_result.groupby(['Season', 'TeamID'])['Score'].sum().reset_index()\n",
                "season_score"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tourney_result = pd.merge(tourney_result, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "tourney_result.rename(columns={'Score':'WScoreT'}, inplace=True)\n",
                "tourney_result = tourney_result.drop('TeamID', axis=1)\n",
                "tourney_result = pd.merge(tourney_result, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "tourney_result.rename(columns={'Score':'LScoreT'}, inplace=True)\n",
                "tourney_result = tourney_result.drop('TeamID', axis=1)\n",
                "tourney_result"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tourney_win_result = tourney_result.drop(['Season', 'WTeamID', 'LTeamID'], axis=1)\n",
                "tourney_win_result.rename(columns={'WSeed':'Seed1', 'LSeed':'Seed2', 'WScoreT':'ScoreT1', 'LScoreT':'ScoreT2'}, inplace=True)\n",
                "tourney_win_result"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "tourney_lose_result = tourney_win_result.copy()\n",
                "tourney_lose_result['Seed1'] = tourney_win_result['Seed2']\n",
                "tourney_lose_result['Seed2'] = tourney_win_result['Seed1']\n",
                "tourney_lose_result['ScoreT1'] = tourney_win_result['ScoreT2']\n",
                "tourney_lose_result['ScoreT2'] = tourney_win_result['ScoreT1']\n",
                "tourney_lose_result"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "tourney_win_result['Seed_diff'] = tourney_win_result['Seed1'] - tourney_win_result['Seed2']\n",
                "tourney_win_result['ScoreT_diff'] = tourney_win_result['ScoreT1'] - tourney_win_result['ScoreT2']\n",
                "tourney_lose_result['Seed_diff'] = tourney_lose_result['Seed1'] - tourney_lose_result['Seed2']\n",
                "tourney_lose_result['ScoreT_diff'] = tourney_lose_result['ScoreT1'] - tourney_lose_result['ScoreT2']"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "tourney_win_result['result'] = 1\n",
                "tourney_lose_result['result'] = 0\n",
                "tourney_result = pd.concat((tourney_win_result, tourney_lose_result)).reset_index(drop=True)\n",
                "tourney_result"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train = tourney_result.drop('result', axis=1)\n",
                "y_train = tourney_result.result\n",
                "X_train, y_train = shuffle(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "clf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
                "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
                "                      max_samples=None, min_impurity_decrease=0.0,\n",
                "                      min_impurity_split=None, min_samples_leaf=50,\n",
                "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
                "                      n_estimators=500, n_jobs=-1, oob_score=False,\n",
                "                      random_state=50, verbose=1, warm_start=False)\n",
                "\n",
                "clf.fit(X_train, y_train)\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "test_df = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WSampleSubmissionStage1_2020.csv')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "test_df['Season'] = test_df['ID'].map(lambda x: int(x[:4]))\n",
                "test_df['WTeamID'] = test_df['ID'].map(lambda x: int(x[5:9]))\n",
                "test_df['LTeamID'] = test_df['ID'].map(lambda x: int(x[10:14]))\n",
                "test_df"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "test_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "test_df.rename(columns={'Seed':'Seed1'}, inplace=True)\n",
                "test_df = test_df.drop('TeamID', axis=1)\n",
                "test_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "test_df.rename(columns={'Seed':'Seed2'}, inplace=True)\n",
                "test_df = test_df.drop('TeamID', axis=1)\n",
                "test_df = pd.merge(test_df, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "test_df.rename(columns={'Score':'ScoreT1'}, inplace=True)\n",
                "test_df = test_df.drop('TeamID', axis=1)\n",
                "test_df = pd.merge(test_df, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n",
                "test_df.rename(columns={'Score':'ScoreT2'}, inplace=True)\n",
                "test_df = test_df.drop('TeamID', axis=1)\n",
                "test_df"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "test_df['Seed1'] = test_df['Seed1'].map(lambda x: get_seed(x))\n",
                "test_df['Seed2'] = test_df['Seed2'].map(lambda x: get_seed(x))\n",
                "test_df['Seed_diff'] = test_df['Seed1'] - test_df['Seed2']\n",
                "test_df['ScoreT_diff'] = test_df['ScoreT1'] - test_df['ScoreT2']\n",
                "test_df = test_df.drop(['ID', 'Pred', 'Season', 'WTeamID', 'LTeamID'], axis=1)\n",
                "test_df"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "test_preds = clf.predict(test_df)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results",
                "process_data"
            ],
            "source": [
                "submission_df = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WSampleSubmissionStage1_2020.csv')\n",
                "submission_df['Pred'] = test_preds\n",
                "submission_df"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "submission_df['Pred'].hist()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submission_df.to_csv('submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "%matplotlib inline\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt # drawing graph\n",
                "import warnings; warnings.filterwarnings(\"ignore\") \n",
                "import os; os.environ['OMP_NUM_THREADS'] = '4' # speed up using 4 cpu\n",
                "from fastai.tabular import *\n",
                "from sklearn.metrics import roc_auc_score\n",
                "from imblearn.over_sampling import SMOTE"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data"
            ],
            "source": [
                "# source : https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/\n",
                "df = pd.read_excel('../input/WA_Fn-UseC_-HR-Employee-Attrition.xlsx', sheet_name=0,dtype=dtypes)\n",
                "print(\"Shape of dataframe is: {}\".format(df.shape))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# preprocessing : categorical encoding\n",
                "df['Attrition']=df.Attrition.eq('Yes').mul(1) # change target from Yes/no to 1/0\n",
                "cont=[]\n",
                "cat=[]\n",
                "for key, value in dtypes.items():\n",
                "    if key!='Attrition':\n",
                "        if value == \"int64\":\n",
                "            cont.append(key)\n",
                "        else:\n",
                "            cat.append(key)\n",
                "df = pd.get_dummies(df, columns=cat)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# get train data\n",
                "col = df.columns\n",
                "cont=[]\n",
                "for i in range(0,len(col)):\n",
                "    if col[i]!='Attrition':\n",
                "        cont.append(col[i])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#save the column name\n",
                "x_col = cont\n",
                "y_col = 'Attrition'\n",
                "\n",
                "X = df.drop('Attrition', axis=1)\n",
                "Y = df.Attrition\n",
                "X_res, Y_res = SMOTE().fit_resample(X, Y)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "smote_df = pd.DataFrame(X_res, columns = x_col)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "smote_df = smote_df.assign(Attrition = Y_res)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "smote_df.Attrition.value_counts()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "smote_df.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "dep_var='Attrition'\n",
                "procs=[ Normalize]\n",
                "data = (TabularList.from_df(smote_df, cont_names=col , procs=procs,)\n",
                "                .split_subsets(train_size=0.8, valid_size=0.2, seed=34)\n",
                "                .label_from_df(cols=dep_var)\n",
                "                .databunch())"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn = tabular_learner(data, layers=[200,100],metrics=accuracy, callback_fns=AUROC)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit(3,lr=1e-3)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit_one_cycle(2,max_lr=1e-6)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "learn.recorder.plot_losses()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import warnings\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from fastai.tabular import *\n",
                "%matplotlib inline\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train = pd.read_csv('../input/hack-ml/Dataset/Train.csv')\n",
                "test = pd.read_csv('../input/hack-ml/Dataset/Test.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('Train set shape:', train.shape)\n",
                "print('Test set shape:', test.shape)\n",
                "print('NaN in Train:',train.isnull().values.any())\n",
                "print('NaN in Test:',test.isnull().values.any())\n",
                "print('Train set overview:')\n",
                "display(train.head())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "f, ax = plt.subplots(figsize=(6, 6))\n",
                "ax = sns.countplot(x=\"netgain\", data=train, label=\"Label count\")\n",
                "sns.despine(bottom=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "del train['id']\n",
                "del test['id']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "dep_var='netgain'\n",
                "cat = ['realtionship_status', 'industry', 'genre', 'targeted_sex', 'airtime', 'airlocation',\n",
                "       'expensive', 'money_back_guarantee']\n",
                "cont = ['average_runtime(minutes_per_week)','ratings']\n",
                "procs= [Categorify,Normalize]\n",
                "\n",
                "inception = TabularList.from_df(test,cat_names=cat, cont_names=cont , procs=procs)\n",
                "data = (TabularList.from_df(train, cat_names=cat, cont_names=cont , procs=procs)\n",
                "                .split_subsets(train_size=0.8, valid_size=0.2, seed=34)\n",
                "                .label_from_df(cols=dep_var)\n",
                "                .add_test(inception)\n",
                "                .databunch())"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn = tabular_learner(data, layers=[200,100],metrics=accuracy)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit(1,lr=1e-3)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit_one_cycle(1,max_lr=1e-8)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "learn.recorder.plot_losses()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "learn.predict(inception[0])[0]"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "%matplotlib inline\n",
                "import pandas as pd\n",
                "from fastai.tabular import *\n",
                "import fastai \n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# check version\n",
                "fastai.__version__"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Feture importance extraction from NN weights\n",
                "def feature_importance(learner): \n",
                "  # based on: https://medium.com/@mp.music93/neural-networks-feature-importance-with-fastai-5c393cf65815\n",
                "    data = learner.data.train_ds.x\n",
                "    cat_names = data.cat_names\n",
                "    cont_names = data.cont_names\n",
                "    loss0=np.array([learner.loss_func(learner.pred_batch(batch=(x,y.to(\"cpu\"))), y.to(\"cpu\")) for x,y in iter(learner.data.valid_dl)]).mean()\n",
                "    fi=dict()\n",
                "    types=[cat_names, cont_names]\n",
                "    for j, t in enumerate(types):\n",
                "      for i, c in enumerate(t):\n",
                "        loss=[]\n",
                "        for x,y in iter(learner.data.valid_dl):\n",
                "          col=x[j][:,i] \n",
                "          idx = torch.randperm(col.nelement())\n",
                "          x[j][:,i] = col.view(-1)[idx].view(col.size())\n",
                "          y=y.to('cpu')\n",
                "          loss.append(learner.loss_func(learner.pred_batch(batch=(x,y)), y))\n",
                "        fi[c]=np.array(loss).mean()-loss0\n",
                "    d = sorted(fi.items(), key=lambda kv: kv[1], reverse=True)\n",
                "    return pd.DataFrame({'cols': [l for l, v in d], 'imp': np.log1p([v for l, v in d])})"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data"
            ],
            "source": [
                "# load all datasets\n",
                "col = [\"age\",\"class of worker\",\"detailed industry recode\",\"detailed occupation recode\",\"education\",\n",
                "       \"wage per hour\",\"enroll in edu inst last wk\",\"marital status\",\"major industry code\",\n",
                "       \"major occupation code\",\"race\",\"hispanic origin\",\"sex\",\"member of a labor union\",\n",
                "       \"reason for unemployment\",\"full or part time employment stat\",\"capital gains\",\"capital losses\",\n",
                "       \"dividends from stocks\",\"tax filer stat\",\"region of previous residence\",\"state of previous residence\",\n",
                "       \"detailed household and family stat\",\"detailed household summary in household\",\"instance weight\",\n",
                "       \"migration code-change in msa\",\"migration code-change in reg\",\"migration code-move within reg\",\n",
                "       \"live in this house 1 year ago\",\"migration prev res in sunbelt\",\"num persons worked for employer\",\n",
                "       \"family members under 18\",\"country of birth father\",\"country of birth mother\",\"country of birth self\",\n",
                "       \"citizenship\",\"own business or self employed\",\"fill inc questionnaire for veteran\\'s admin\",\n",
                "       \"veterans benefits\",\"weeks worked in year\",\"year\",\"income class\"]\n",
                "\n",
                "df = pd.read_csv(\"../input/ml-challenge-week6/census-income.data\", names=col, header=None)\n",
                "print(\"Shape of Train dataframe is: {}\".format(df.shape))\n",
                "print('NaN in Train:',df.isnull().values.any())\n",
                "test = pd.read_csv(\"../input/ml-challenge-week6/census-income.test\", names=col[0:-1], header=None)\n",
                "print(\"Shape of Test dataframe is: {}\".format(test.shape))\n",
                "print('NaN in Test:',test.isnull().values.any())\n",
                "sub = pd.read_csv(\"../input/ml-challenge-week6/sampleSubmission.csv\")\n",
                "zub = sub['index']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# align test target\n",
                "test['income class'] = ' - 50000.'\n",
                "# NaN corrections in test dataset\n",
                "test[\"hispanic origin\"] = test[\"hispanic origin\"].fillna('NA')\n",
                "test[\"state of previous residence\"] = test[\"state of previous residence\"].fillna('?')\n",
                "test['migration code-change in msa'] = test['migration code-change in msa'].fillna('?')\n",
                "test['migration code-change in reg'] = test['migration code-change in reg'].fillna('?')\n",
                "test['migration code-move within reg'] = test['migration code-move within reg'].fillna('?')\n",
                "test['migration prev res in sunbelt'] = test['migration prev res in sunbelt'].fillna('?')\n",
                "test['country of birth father'] = test['country of birth father'].fillna('?')\n",
                "test['country of birth mother'] = test['country of birth mother'].fillna('?')\n",
                "test['country of birth self'] = test['country of birth self'].fillna('?')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# merge train & test dataset\n",
                "df = df.append(test, ignore_index = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# transform target in category type\n",
                "df = df.join(pd.get_dummies(df['income class']))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# parameters for NN model\n",
                "dep_var =  ' 50000+.'\n",
                "procs = [FillMissing, Categorify, Normalize]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# check % positive values in train set\n",
                "df[dep_var].value_counts()[1]/199523"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# split by index\n",
                "idx_test = df.iloc[199523:].index # last N rows\n",
                "idx_val  = df.iloc[159619:199522].index # last 20% of train rows\n",
                "idx_val, idx_test"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# check % positive values in validation set\n",
                "df.loc[idx_val, dep_var].value_counts()[1]/(199522-159619)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# prepare databunch ingestion of test set\n",
                "test = TabularList.from_df(df.loc[idx_test].copy(), path='', cat_names=cat, cont_names=cont)"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "source": [
                "# build NN learner and look at learning rate curve\n",
                "learn = tabular_learner(data, layers=[200,100], metrics=[accuracy, AUROC()],callback_fns=ShowGraph)\n",
                "\n",
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# Initial learning phase using suggested learning rate during 3 cycles\n",
                "lr = 1e-03\n",
                "learn.fit_one_cycle(5, lr)"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "source": [
                "# look again at learning rate curve\n",
                "learn.unfreeze()\n",
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# 2nd learning phase using suggested learning rate during 3 cycles\n",
                "learn.fit_one_cycle(3,max_lr=1e-8)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "process_data"
            ],
            "source": [
                "# predict test classes...\n",
                "probas_test, _ = learn.get_preds(ds_type=DatasetType.Test) # run inference on test\n",
                "probas_test = probas_test[:, 1] # only get probability tensor"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# get tp driver\n",
                "tp = range(0,40000,100) \n",
                "y = []\n",
                "for x in tp:\n",
                "    y.append(0.6432042 + 0.00002635951*x - 9.49824e-10*x**2 + 9.629459e-15*x**3) # from my own analysis...\n",
                "    \n",
                "plt.plot(tp,y);"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('Best amount of true classes should be', np.argmax(y)*100,'with expected AUC arround',y[np.argmax(y)])"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# should have more positive classes, let's align...\n",
                "pivot = .2\n",
                "while len(submission_df[submission_df['income class']>=pivot])< 19100: #(np.argmax(y)*100):\n",
                "    pivot = pivot-.000001\n",
                "correction = .5 - pivot\n",
                "print('Pivot is',pivot,'- tensor correction is', correction)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# apply correction, classify 0/1 and make it int\n",
                "submission_df['income class'] = submission_df['income class'] + correction # tensor correction\n",
                "submission_df['income class'] = submission_df['income class'].apply(np.round)\n",
                "submission_df['income class'] = submission_df['income class'].astype(int)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# check results\n",
                "submission_df.describe()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "\n",
                "submission_df.to_csv('FastAI_v6_corrected.csv', index = False) #"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Here are our NN feature importance \n",
                "fi = feature_importance(learn)\n",
                "fi[:20].plot.barh(x=\"cols\", y=\"imp\", figsize=(10, 10))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt # drawing graph\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train_df = pd.read_csv('/kaggle/input/learn-together/train.csv')\n",
                "print(\"Size of Train dataframe is: {}\".format(train_df.shape))\n",
                "test_df =  pd.read_csv('/kaggle/input/learn-together/test.csv')\n",
                "print(\"Size of Test dataframe is: {}\".format(test_df.shape))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "categories = train_df[target[0]].unique()\n",
                "val = []\n",
                "for i in categories:\n",
                "    temp = len(train_df[train_df[target[0]]==i])\n",
                "    val.append(temp)\n",
                "labels=categories\n",
                "sizes=val\n",
                "colors=['green','red','orange','blue','darkorange','grey','pink']\n",
                "plt.axis('equal')\n",
                "plt.title('target classes distribution')\n",
                "plt.pie(sizes, explode=(0,0,0,0,0,0,0), labels=labels,colors=colors,autopct='%1.2f%%', shadow=True, startangle=90)\n",
                "plt.show()                        "
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "print('We have following categorical features:')\n",
                "print()\n",
                "cat = []\n",
                "cont = []\n",
                "for i in test[1:]:\n",
                "    temp1 = train_df[i].unique()\n",
                "    temp2 = test_df[i].unique()\n",
                "    if len(temp1) == len(temp2):\n",
                "        print(i,':',len(temp1),'unique values')\n",
                "        cat.append(i)\n",
                "    else:\n",
                "        cont.append(i)\n",
                "print()\n",
                "print('And we have',len(cont), 'of following continuous features:') \n",
                "print(cont)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "train_df[cont].hist(bins=20, figsize=(15,15), color = 'orange')\n",
                "plt.suptitle(\"Histogram for each train numeric input variable\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "test_df[cont].hist(bins=20, figsize=(15,15), color = 'darkorange')\n",
                "plt.suptitle(\"Histogram for each test numeric input variable\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "%matplotlib inline\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns # for making plots with seaborn\n",
                "color = sns.color_palette()\n",
                "from PIL import Image\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "import os\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train = pd.read_csv('../input/train.csv')\n",
                "test = pd.read_csv('../input/test.csv')\n",
                "train.head(5)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('size of train data',train.shape)\n",
                "print('size of test data',test.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(12,5))\n",
                "plt.title(\"Distribution of image categories(Target Variable)\")\n",
                "ax = sns.distplot(train[\"category_id\"])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(12,5))\n",
                "plt.title(\"Distribution of train image locations(Train location Variable)\")\n",
                "ax = sns.distplot(train[\"location\"])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(12,5))\n",
                "plt.title(\"Distribution of test image locations(Test location Variable)\")\n",
                "ax = sns.distplot(test[\"location\"])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.FacetGrid(train, hue=\"height\", size=10).map(plt.scatter, \"category_id\", \"location\").add_legend()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "train['date'] = train['date_captured'].str.split('\\s+').str[0]\n",
                "train['time'] = train['date_captured'].str.split('\\s+').str[-1]    \n",
                "train['hour'] = pd.to_numeric(train['time'].str[:2], errors='coerce')\n",
                "sns.FacetGrid(train, hue=\"category_id\", size=10).map(plt.scatter, \"hour\", \"location\").add_legend()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "night = train[(train['hour'] > 19) | (train['hour'] < 7)]\n",
                "day = len(train) - len(night)\n",
                "labels = 'Day', 'Night'\n",
                "sizes = [len(night), day]\n",
                "colors = ['lightcoral', 'lightskyblue']\n",
                "explode = (0.1, 0) \n",
                "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
                "autopct='%1.1f%%', shadow=True, startangle=140)\n",
                "plt.axis('equal')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "# sample night images\n",
                "fig = plt.figure(figsize=(25, 60))\n",
                "imgs = [np.random.choice(night.loc[night['category_id'] == i, 'file_name'], 4) for i in night.category_id.unique()]\n",
                "imgs = [i for j in imgs for i in j]\n",
                "labels = [[i] * 4 for i in train.category_id.unique()]\n",
                "labels = [i for j in labels for i in j]\n",
                "for idx, img in enumerate(imgs):\n",
                "    ax = fig.add_subplot(14, 4, idx + 1, xticks=[], yticks=[])\n",
                "    im = Image.open(\"../input/train_images/\" + img)\n",
                "    plt.imshow(im)\n",
                "    ax.set_title(f'Label: {labels[idx]}')"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "day = train[(train['hour'] < 19) & (train['hour'] > 7)]\n",
                "# sample night images\n",
                "fig = plt.figure(figsize=(25, 60))\n",
                "imgs = [np.random.choice(day.loc[day['category_id'] == i, 'file_name'], 4) for i in day.category_id.unique()]\n",
                "imgs = [i for j in imgs for i in j]\n",
                "labels = [[i] * 4 for i in train.category_id.unique()]\n",
                "labels = [i for j in labels for i in j]\n",
                "for idx, img in enumerate(imgs):\n",
                "    ax = fig.add_subplot(14, 4, idx + 1, xticks=[], yticks=[])\n",
                "    im = Image.open(\"../input/train_images/\" + img)\n",
                "    plt.imshow(im)\n",
                "    ax.set_title(f'Label: {labels[idx]}')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# We import required lib's\n",
                "from fastai.vision import *\n",
                "import os"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# nothing in our folder yet\n",
                "print(os.listdir(\"../working\"))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# move images to working folder\n",
                "!mkdir ../working/dog\n",
                "!mkdir ../working/wolf\n",
                "!cp ../input/dog-v1/* ../working/dog\n",
                "!cp ../input/wolf-v1/* ../working/wolf"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# let's check\n",
                "print(os.listdir(\"../working\"))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "source": [
                "verify_images('../working/wolf', delete=True, max_size=500)\n",
                "verify_images('../working/dog', delete=True, max_size=500)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "classes = ['wolf','dog']\n",
                "np.random.seed(42)\n",
                "data = ImageDataBunch.from_folder('../working', train=\".\", valid_pct=0.2,\n",
                "        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.classes"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data.show_batch(rows=3, figsize=(15,11))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.classes, data.c, len(data.train_ds), len(data.valid_ds)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn = create_cnn(data, models.resnet34, metrics=error_rate)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit_one_cycle(4)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "learn.save('stage-1')"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.unfreeze()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.lr_find()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "learn.recorder.plot()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit_one_cycle(2, max_lr=slice(3e-5,3e-4))"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "learn.save('stage-2')"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "learn.load('stage-2');"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "interp = ClassificationInterpretation.from_learner(learn)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "validate_data"
            ],
            "source": [
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "losses,idxs = interp.top_losses()\n",
                "len(data.valid_ds)==len(losses)==len(idxs)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "interp.plot_top_losses(9, figsize=(15,11))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data.show_batch(rows=3, figsize=(15,11))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.model_selection import train_test_split, StratifiedKFold,KFold\n",
                "from datetime import datetime\n",
                "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\n",
                "from sklearn import metrics\n",
                "from sklearn import preprocessing\n",
                "# Suppr warning\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "import itertools\n",
                "from scipy import interp\n",
                "# Plots\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "from matplotlib import rcParams\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\n",
                "test_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n",
                "train_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\n",
                "test_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n",
                "sample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "train_df = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n",
                "test_df = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n",
                "\n",
                "print(\"Train shape : \"+str(train_df.shape))\n",
                "print(\"Test shape  : \"+str(test_df.shape))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#drop sequence...\n",
                "train_df = train_df.reset_index()\n",
                "test_df = test_df.reset_index()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_df['nulls1'] = train_df.isna().sum(axis=1)\n",
                "test_df['nulls1'] = test_df.isna().sum(axis=1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_df = train_df.drop([\"TransactionDT\"], axis = 1)\n",
                "test_df = test_df.drop([\"TransactionDT\"], axis = 1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# KISS\n",
                "train_df = train_df.iloc[:, :53]\n",
                "test_df = test_df.iloc[:, :52]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "del train_transaction, train_identity, test_transaction, test_identity"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
                "us_emails = ['gmail', 'net', 'edu']\n",
                "#https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest_df-579654\n",
                "for c in ['P_emaildomain', 'R_emaildomain']:\n",
                "    train_df[c + '_bin'] = train_df[c].map(emails)\n",
                "    test_df[c + '_bin'] = test_df[c].map(emails)\n",
                "    \n",
                "    train_df[c + '_suffix'] = train_df[c].map(lambda x: str(x).split('.')[-1])\n",
                "    test_df[c + '_suffix'] = test_df[c].map(lambda x: str(x).split('.')[-1])\n",
                "    \n",
                "    train_df[c + '_suffix'] = train_df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
                "    test_df[c + '_suffix'] = test_df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for c1, c2 in train_df.dtypes.reset_index().values:\n",
                "    if c2=='O':\n",
                "        train_df[c1] = train_df[c1].map(lambda x: str(x).lower())\n",
                "        test_df[c1] = test_df[c1].map(lambda x: str(x).lower())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "numerical = [col for col in numerical if col in train_df.columns]\n",
                "categorical = [col for col in categorical if col in train_df.columns]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def nan2mean(df):\n",
                "    for x in list(df.columns.values):\n",
                "        if x in numerical:\n",
                "            #print(\"___________________\"+x)\n",
                "            #print(df[x].isna().sum())\n",
                "            df[x] = df[x].fillna(0)\n",
                "           #print(\"Mean-\"+str(df[x].mean()))\n",
                "    return df\n",
                "train_df=nan2mean(train_df)\n",
                "test_df=nan2mean(test_df)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Label Encoding\n",
                "category_counts = {}\n",
                "for f in categorical:\n",
                "    train_df[f] = train_df[f].replace(\"nan\", \"other\")\n",
                "    train_df[f] = train_df[f].replace(np.nan, \"other\")\n",
                "    test_df[f] = test_df[f].replace(\"nan\", \"other\")\n",
                "    test_df[f] = test_df[f].replace(np.nan, \"other\")\n",
                "    lbl = preprocessing.LabelEncoder()\n",
                "    lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
                "    train_df[f] = lbl.transform(list(train_df[f].values))\n",
                "    test_df[f] = lbl.transform(list(test_df[f].values))\n",
                "    category_counts[f] = len(list(lbl.classes_)) + 1\n",
                "# train_df = train_df.reset_index()\n",
                "# test_df = test_df.reset_index()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "for column in numerical:\n",
                "    scaler = StandardScaler()\n",
                "    if train_df[column].max() > 100 and train_df[column].min() >= 0:\n",
                "        train_df[column] = np.log1p(train_df[column])\n",
                "        test_df[column] = np.log1p(test_df[column])\n",
                "    scaler.fit(np.concatenate([train_df[column].values.reshape(-1,1), test_df[column].values.reshape(-1,1)]))\n",
                "    train_df[column] = scaler.transform(train_df[column].values.reshape(-1,1))\n",
                "    test_df[column] = scaler.transform(test_df[column].values.reshape(-1,1))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "source": [
                "from fastai.tabular import *\n",
                "from sklearn.metrics import roc_auc_score\n",
                "\n",
                "def auroc_score(input, target):\n",
                "    input, target = input.cpu().numpy()[:,1], target.cpu().numpy()\n",
                "    return roc_auc_score(target, input)\n",
                "\n",
                "class AUROC(Callback):\n",
                "    _order = -20 #Needs to run before the recorder\n",
                "\n",
                "    def __init__(self, learn, **kwargs): self.learn = learn\n",
                "    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['AUROC'])\n",
                "    def on_epoch_begin(self, **kwargs): self.output, self.target = [], []\n",
                "    \n",
                "    def on_batch_end(self, last_target, last_output, train, **kwargs):\n",
                "        if not train:\n",
                "            self.output.append(last_output)\n",
                "            self.target.append(last_target)\n",
                "                \n",
                "    def on_epoch_end(self, last_metrics, **kwargs):\n",
                "        if len(self.output) > 0:\n",
                "            output = torch.cat(self.output)\n",
                "            target = torch.cat(self.target)\n",
                "            preds = F.softmax(output, dim=1)\n",
                "            metric = auroc_score(preds, target)\n",
                "            return add_metrics(last_metrics, [metric])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "dep_var='isFraud' \n",
                "procs = [FillMissing, Categorify, Normalize]\n",
                "test_all = TabularList.from_df(test_df, cat_names=categorical,cont_names=numerical,procs=procs)\n",
                "data = (TabularList.from_df(train_df, cat_names=categorical, cont_names=numerical,procs=procs)\n",
                "                           .split_subsets(train_size=0.85, valid_size=0.15, seed=34)\n",
                "                           .label_from_df(cols=dep_var)\n",
                "                           .add_test(test_all)\n",
                "                           .databunch())       "
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn = tabular_learner(data, layers=[200,100],metrics=accuracy, callback_fns=AUROC)\n",
                "#learn = tabular_learner(data, layers=[1000,500,100],emb_drop=0.04,ps=(0.001, 0.01, 0.1),metrics=accuracy, callback_fns=AUROC,wd=1e-2)#.to_fp16()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.fit(10,lr=1e-2)\n",
                "#learn.fit(30,lr=3e-3)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.unfreeze()\n",
                "#learn.fit_one_cycle(10,max_lr=1e-6)\n",
                "learn.fit_one_cycle(10,max_lr=5e-5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "learn.recorder.plot_losses()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "#learn.freeze()\n",
                "learn.lr_find()\n",
                "learn.recorder.plot(suggestion=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "learn.unfreeze()\n",
                "#learn.fit_one_cycle(10,max_lr=1e-6)\n",
                "learn.fit_one_cycle(1,max_lr=1e-8)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "test_pred = learn.get_preds(DatasetType.Test)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "sample_submission.isFraud = test_pred[0][:,1].numpy()\n",
                "sample_submission.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "sample_submission.to_csv('simple_fastai_v3.csv')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "%matplotlib inline\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt \n",
                "#from collections import Counter\n",
                "import networkx as nx \n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "import os\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Create graph from data \n",
                "g = nx.Graph()\n",
                "color_map = []\n",
                "for i in range(0,len(fam)): #len(names)\n",
                "    g.add_node(fam[i], type = 'fam')\n",
                "    for j in ind[i]:\n",
                "        temp = fam[i]+j\n",
                "        g.add_node(temp, type = 'ind')\n",
                "        g.add_edge(fam[i], temp, color='green', weight=1)\n",
                "for n1, attr in g.nodes(data=True):\n",
                "    if attr['type'] == 'fam':\n",
                "        color_map.append('lime')\n",
                "    else: \n",
                "        if attr['type'] == 'ind':\n",
                "            color_map.append('cyan')\n",
                "        else:\n",
                "            color_map.append('red')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Plot the graph\n",
                "plt.figure(3,figsize=(90,90))  \n",
                "edges = g.edges()\n",
                "colors = [g[u][v]['color'] for u,v in edges]\n",
                "nx.draw(g,node_color = color_map, edge_color = colors, with_labels = True)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import os\n",
                "import random\n",
                "import warnings\n",
                "warnings.simplefilter(action='ignore')\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "%matplotlib inline\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC, LinearSVC\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.linear_model import SGDClassifier\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.model_selection import KFold\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import make_scorer, accuracy_score\n",
                "from sklearn.model_selection import RandomizedSearchCV\n",
                "import lightgbm\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.mixture import GaussianMixture\n",
                "from lightgbm import LGBMClassifier\n",
                "from mlxtend.classifier import StackingCVClassifier\n",
                "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "random_state = 1\n",
                "random.seed(random_state)\n",
                "np.random.seed(random_state)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "# Read the data\n",
                "X = pd.read_csv('../input/mh-forest/Forest_Cover_participants_Data/train.csv')\n",
                "X_test_full = pd.read_csv('../input/mh-forest/Forest_Cover_participants_Data/test.csv')\n",
                "\n",
                "col = X.columns\n",
                "newcol = []\n",
                "for i in range(0, len(col)):\n",
                "    temp = col[i]\n",
                "    if temp[-8:] == '(meters)':\n",
                "        #print(temp[:-8])\n",
                "        temp = temp[:-8]\n",
                "    if temp[-9:] == '(degrees)':\n",
                "        #print(temp[:-9])\n",
                "        temp = temp[:-9]\n",
                "    newcol.append(temp)\n",
                "X.columns = newcol\n",
                "\n",
                "col = X_test_full.columns\n",
                "newcol = []\n",
                "for i in range(0, len(col)):\n",
                "    temp = col[i]\n",
                "    if temp[-8:] == '(meters)':\n",
                "        #print(temp[:-8])\n",
                "        temp = temp[:-8]\n",
                "    if temp[-9:] == '(degrees)':\n",
                "        #print(temp[:-9])\n",
                "        temp = temp[:-9]\n",
                "    newcol.append(temp)\n",
                "X_test_full.columns = newcol\n",
                "\n",
                "\n",
                "y = X.Cover_Type\n",
                "X.drop(['Cover_Type'], axis=1, inplace=True)\n",
                "\n",
                "#X.drop(['Id'], axis=1, inplace=True)\n",
                "\n",
                "train_X = X\n",
                "train_y = y"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def WH4(df):\n",
                "    df['Hydro_high'] = df.Vertical_Distance_To_Hydrology.apply(lambda x: x > 3 )\n",
                "    df['Hydro_Euclidean'] = (df['Horizontal_Distance_To_Hydrology']**2 +\n",
                "                            df['Vertical_Distance_To_Hydrology']**2).apply(np.sqrt)\n",
                "    #df.drop(['Vertical_Distance_To_Hydrology'], axis=1, inplace=True)\n",
                "    #df.drop(['Horizontal_Distance_To_Hydrology'], axis=1, inplace=True)\n",
                "    df['Hydro_Fire_road'] = (df.Horizontal_Distance_To_Roadways + df.Horizontal_Distance_To_Fire_Points)/(df.Hydro_Euclidean/20000+1)\n",
                "    df['Hydro_Fire_sum'] = (df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points'])\n",
                "    #df.drop(['Soil_Type15'], axis=1, inplace=True)\n",
                "    #df.drop(['Soil_Type7'], axis=1, inplace=True)\n",
                "    df['Hydro_Elevation_diff'] = (df['Elevation'] - df['Vertical_Distance_To_Hydrology'])\n",
                "    \n",
                "    df['Soil_Type12_32'] = df['Soil_Type_32'] + df['Soil_Type_12']\n",
                "    df['Soil_Type23_22_32_33'] = df['Soil_Type_23'] + df['Soil_Type_22'] + df['Soil_Type_32'] + df['Soil_Type_33']\n",
                "      \n",
                "    df['Hydro_Fire_diff'] = (df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points']).abs()\n",
                "    df['Hydro_Road_sum'] = (df['Horizontal_Distance_To_Hydrology'] +df['Horizontal_Distance_To_Roadways'])\n",
                "    df['Hydro_Road_diff'] = (df['Horizontal_Distance_To_Hydrology'] -df['Horizontal_Distance_To_Roadways']).abs()\n",
                "    df['Road_Fire_sum'] = (df['Horizontal_Distance_To_Roadways'] + df['Horizontal_Distance_To_Fire_Points'])\n",
                "    df['Road_Fire_diff'] = (df['Horizontal_Distance_To_Roadways'] - df['Horizontal_Distance_To_Fire_Points']).abs()\n",
                "    #df.loc[:, :] = np.floor(MinMaxScaler((0, 100)).fit_transform(df))\n",
                "    #df = df.astype('int8')\n",
                "    #df.fillna(0)\n",
                "    \n"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model",
                "process_data"
            ],
            "source": [
                "WH4(X_test_full)\n",
                "WH4(X)\n",
                "\n",
                "\n",
                "gm = GaussianMixture(n_components  = 15)\n",
                "gm.fit(X)\n",
                "X['g_mixture'] = gm.predict(X)\n",
                "X_test_full['g_mixture'] = gm.predict(X_test_full)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X_test_full.shape"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "max_features = min(30, X.columns.size)\n",
                "\n",
                "ab_clf = AdaBoostClassifier(n_estimators=300,\n",
                "                            base_estimator=DecisionTreeClassifier(\n",
                "                                min_samples_leaf=2,\n",
                "                                random_state=random_state),\n",
                "                            random_state=random_state)\n",
                "\n",
                "et_clf = ExtraTreesClassifier(n_estimators=500,\n",
                "                              min_samples_leaf=2,\n",
                "                              min_samples_split=2,\n",
                "                              max_depth=50,\n",
                "                              max_features=max_features,\n",
                "                              random_state=random_state,\n",
                "                              n_jobs=-1)\n",
                "\n",
                "lg_clf = LGBMClassifier(n_estimators=300,\n",
                "                        num_leaves=128,\n",
                "                        verbose=-1,\n",
                "                        random_state=random_state,\n",
                "                        n_jobs=-1)\n",
                "\n",
                "rf_clf = RandomForestClassifier(n_estimators=300,\n",
                "                                random_state=random_state,\n",
                "                                n_jobs=-1)\n",
                "\n",
                "ensemble = [('AdaBoostClassifier', ab_clf),\n",
                "            ('ExtraTreesClassifier', et_clf),\n",
                "            ('LGBMClassifier', lg_clf),\n",
                "            ('RandomForestClassifier', rf_clf)]"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "print('> Cross-validating classifiers')\n",
                "for label, clf in ensemble:\n",
                "    score = cross_val_score(clf, X, train_y,\n",
                "                            cv=5,\n",
                "                            scoring='accuracy',\n",
                "                            verbose=0,\n",
                "                            n_jobs=-1)\n",
                "\n",
                "    print('  -- {: <24} : {:.3f} : {}'.format(label, np.mean(score), np.around(score, 3)))\n",
                "\n",
                "\n",
                "print('> Fitting stack')\n",
                "\n",
                "stack = StackingCVClassifier(classifiers=[ab_clf, et_clf, lg_clf, rf_clf],\n",
                "                             meta_classifier=rf_clf,\n",
                "                             cv=5,\n",
                "                             stratify=True,\n",
                "                             shuffle=True,\n",
                "                             use_probas=True,\n",
                "                             use_features_in_secondary=True,\n",
                "                             verbose=1,\n",
                "                             random_state=random_state,\n",
                "                             n_jobs=-1)\n",
                "\n",
                "stack = stack.fit(X, train_y)\n"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "predictions = stack.predict_proba(X_test_full)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "stack.score(X, train_y)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "sub = pd.read_csv(\"../input/mh-forest/Forest_Cover_participants_Data/sample_submission.csv\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from tqdm import tqdm\n",
                "\n",
                "for i in tqdm(range(0,len(sub))):\n",
                "    sub.iloc[i,0] = predictions[i][0]\n",
                "    sub.iloc[i,1] = predictions[i][1]\n",
                "    sub.iloc[i,2] = predictions[i][2]\n",
                "    sub.iloc[i,3] = predictions[i][3]\n",
                "    sub.iloc[i,4] = predictions[i][4]\n",
                "    sub.iloc[i,5] = predictions[i][5]\n",
                "    sub.iloc[i,6] = predictions[i][6]\n",
                "    "
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "sub.to_csv(\"Tree_version_8a.csv\", index=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "sub.describe()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "%matplotlib inline\n",
                "import pandas as pd\n",
                "import networkx as nx \n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm\n",
                "from IPython.display import Image\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\") \n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "os.environ['OMP_NUM_THREADS'] = '8' "
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "data = pd.read_csv('../input/santa-workshop-tour-2019/family_data.csv')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Drawing Graph\n",
                "plt.figure(3,figsize=(25,25))  \n",
                "edges = g.edges()\n",
                "colors = [g[u][v]['color'] for u,v in edges]\n",
                "nx.draw(g, node_color = color_map, edge_color = colors, with_labels = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# Extract reference graph facts & metrics \n",
                "print('Graph')\n",
                "print('Do we have a fully connected graph? ',nx.is_connected(g))\n",
                "h = g.to_directed()\n",
                "N, K = h.order(), h.size()\n",
                "avg_deg= float(K) / N\n",
                "print (\"# Nodes: \", N)\n",
                "print (\"# Edges: \", K)\n",
                "print (\"Average connectivity degree: \", avg_deg)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "dis = []\n",
                "in_degrees= h.in_degree() \n",
                "for i in in_degrees:\n",
                "    dis.append(i[1])\n",
                "fig = plt.figure(figsize=(20,20));\n",
                "plt.title('Degree Distribution per day');\n",
                "plt.grid(True);\n",
                "plt.xlabel('Days');\n",
                "plt.ylabel('# of Demand');\n",
                "plt.plot(dis[0:99],color='orange',alpha=0.90);"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from fastai.vision import *"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "folder = 'black'\n",
                "file = 'black_bear.txt'"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "folder = 'grizzly'\n",
                "file = 'grizzly_bear.txt'"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "folder = 'teddy'\n",
                "file = 'teddy_bear.txt'"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "# 设置所有数据存放的根目录\n",
                "path = Path('/kaggle/working/data/bears')\n",
                "path.mkdir(parents=True, exist_ok=True)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# 在kaggle中，将导入的数据，从只读目录复制到工作区可写目录下\n",
                "!cp /kaggle/input/* {path}/\n",
                "# 安装tree命令\n",
                "!apt get tree"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "!tree {path}"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "# 创建各个类别图像下载的目录\n",
                "dest = path/folder\n",
                "dest.mkdir(parents=True, exist_ok=True)\n",
                "# 下载图像\n",
                "download_images(path/file, dest, max_pics=200)"
            ]
        },
        {
            "tags": [
                "check_results",
                "validate_data"
            ],
            "source": [
                "# 创建类别\n",
                "classes = ['black', 'grizzly', 'teddy']\n",
                "# 删除不能被打开的错误图像\n",
                "for c in classes:\n",
                "    print(c)\n",
                "    verify_images(path/c, delete=True, max_size=500)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# 置固定的随机数种子，保证每次创建相同的验证集，以便调整超参数\n",
                "np.random.seed(42)\n",
                "# 默认训练集会在train目录下查找。用.设置为当前目录，并且划分验证集\n",
                "data = ImageDataBunch.from_folder(path, train=\".\", valid_pct=0.2,\n",
                "        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.classes, data.c, len(data.train_ds), len(data.valid_ds)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.show_batch(rows=3, figsize=(12,8))"
            ]
        },
        {
            "tags": [
                "train_model",
                "transfer_results"
            ],
            "source": [
                "learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n",
                "learn.fit_one_cycle(4)\n",
                "learn.save('stage-1')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_top_losses(9, figsize=(12,8))\n",
                "interp.plot_top_losses(9, figsize=(12,8), heatmap=False)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot()"
            ]
        },
        {
            "tags": [
                "train_model",
                "transfer_results"
            ],
            "source": [
                "learn.unfreeze()\n",
                "learn.fit_one_cycle(2, max_lr=slice(1e-6,3e-4))\n",
                "learn.save('stage-2')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from fastai.widgets import *"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "doc(DatasetFormatter().from_toplosses)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# 取前25个进行clean\n",
                "idxs = idxs[:25]"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "!tree"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# 查看cleaned.csv 里面保存的是被清理之后的正确标签\n",
                "!cat {path}/cleaned.csv -n"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data",
                "train_model"
            ],
            "source": [
                "learn_cln = cnn_learner(db, models.resnet34, metrics=error_rate)\n",
                "learn_cln.load('stage-2');\n",
                "ds, idxs = DatasetFormatter().from_similars(learn_cln)\n",
                "len(idxs)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "idxs = idxs[:10]"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "np.random.seed(42)\n",
                "data = ImageDataBunch.from_csv(path, folder=\".\", valid_pct=0.2, csv_labels='cleaned.csv',\n",
                "        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.classes, data.c, len(data.train_ds), len(data.valid_ds)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model",
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n",
                "learn.load('stage-2')\n",
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_top_losses(9, figsize=(12,8))\n",
                "interp.plot_top_losses(9, figsize=(12,8),heatmap=False)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model",
                "visualize_data"
            ],
            "source": [
                "learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n",
                "learn.fit_one_cycle(4)\n",
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_top_losses(9, figsize=(12,8))\n",
                "interp.plot_top_losses(9, figsize=(12,8),heatmap=False)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model"
            ],
            "source": [
                "learn.lr_find()\n",
                "learn.recorder.plot()"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "learn.unfreeze()\n",
                "learn.fit_one_cycle(2, max_lr=slice(1e-6,3e-4))\n",
                "interp = ClassificationInterpretation.from_learner(learn)\n",
                "interp.plot_top_losses(9, figsize=(12,8))\n",
                "interp.plot_top_losses(9, figsize=(12,8),heatmap=False)\n",
                "interp.plot_confusion_matrix()"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "# library we'll need\n",
                "library(tidyverse)\n",
                "\n",
                "# read in all three datasets (you'll pick one to use later)\n",
                "recpies <- read_csv(\"../input/epirecipes/epi_r.csv\")\n",
                "bikes <- read_csv(\"../input/nyc-east-river-bicycle-crossings/nyc-east-river-bicycle-counts.csv\")\n",
                "weather <- read_csv(\"../input/szeged-weather\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "# We'll need these libraries\n",
                "import numpy as np\n",
                "import pandas as pd \n",
                "from pandas import read_csv\n",
                "\n",
                "# Plotting libraries\n",
                "import seaborn as sns\n",
                "from ggplot import *\n",
                "\n",
                "recipes = read_csv(\"../input/epirecipes/epi_r.csv\")\n",
                "bikes = read_csv(\"../input/nyc-east-river-bicycle-crossings/nyc-east-river-bicycle-counts.csv\")\n",
                "weather = read_csv(\"../input/szeged-weather/weatherHistory.csv\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# quickly clean our dataset\n",
                "recpies <- recpies %>%\n",
                "    filter(calories < 10000) %>% # remove outliers\n",
                "    na.omit() # remove rows with NA values"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "recipes = recipes[recipes['calories'] < 10000].dropna()"
            ]
        },
        {
            "tags": [
                "check_results",
                "validate_data"
            ],
            "source": [
                "# are the ratings all numeric?\n",
                "print(\"Is this variable numeric?\")\n",
                "is.numeric(recpies$rating)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# We'll use the numpy isreal() function\n",
                "# See https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.isreal.html\n",
                "print(\"Is this variable numeric?\")\n",
                "all(recipes['rating'].apply(np.isreal)) # Check that every row is True."
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "print(\"Is this variable only integers?\")\n",
                "\n",
                "all(recipes['rating'] == recipes['rating'].astype(int))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# plot calories by whether or not it's a dessert\n",
                "ggplot(recpies, aes(x = calories, y = dessert)) + # draw a \n",
                "    geom_point()  # add points"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# plot calories by whether or not it's a dessert\n",
                "ggplot(recipes, aes(x='calories', y='dessert')) + geom_point()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.set(style=\"darkgrid\")\n",
                "g = sns.regplot(x=\"calories\", y=\"dessert\", data=recipes, fit_reg=False)\n",
                "g.figure.set_size_inches(8, 8)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# plot & add a regression line\n",
                "ggplot(recpies, aes(x = calories, y = dessert)) + # draw a \n",
                "    geom_point() + # add points\n",
                "    geom_smooth(method = \"glm\", # plot a regression...\n",
                "    method.args = list(family = \"binomial\")) # ...from the binomial family"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ggplot(recipes, aes(x='calories', y='dessert')) + geom_point() + \\\n",
                "stat_smooth(method=\"lm\", color='blue')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.set(style=\"darkgrid\")\n",
                "g = sns.regplot(x=\"calories\", y=\"dessert\", data=recipes, logistic=True)\n",
                "g.figure.set_size_inches(8, 8)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# Extraction of digital\n",
                "bikes['Precipitation'] = bikes['Precipitation'].map(strtonum)\n",
                "# print(bikes['Precipitation'])\n",
                "print(bikes.columns)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ggplot(bikes,aes(x='Low Temp (°F)', y='Total')) + geom_point() + \\\n",
                "stat_smooth(method=\"loess\", color='blue')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "ggplot(bikes,aes(x='Low Temp (°F)', y='Brooklyn Bridge')) + geom_point(color='red') + \\\n",
                "stat_smooth(method=\"loess\", color='violet', se=True)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import os\n",
                "import json\n",
                "\n",
                "import albumentations as albu\n",
                "import cv2\n",
                "import keras\n",
                "from keras import backend as K\n",
                "from keras.models import Model\n",
                "from keras.layers import Input\n",
                "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
                "from keras.layers.pooling import MaxPooling2D\n",
                "from keras.layers.merge import concatenate\n",
                "from keras.losses import binary_crossentropy\n",
                "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "from skimage.exposure import adjust_gamma\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from keras.layers import LeakyReLU\n",
                "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
                "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
                "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
                "from keras import optimizers\n",
                "from keras.legacy import interfaces\n",
                "from keras.utils.generic_utils import get_custom_objects\n",
                "\n",
                "from keras.engine.topology import Input\n",
                "from keras.engine.training import Model\n",
                "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
                "from keras.layers.core import Activation, SpatialDropout2D\n",
                "from keras.layers.merge import concatenate\n",
                "from keras.layers.normalization import BatchNormalization\n",
                "from keras.layers.pooling import MaxPooling2D\n",
                "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add\n",
                "from keras.regularizers import l2\n",
                "from keras.layers.core import Dense, Lambda\n",
                "from keras.layers.merge import concatenate, add\n",
                "from keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\n",
                "from keras.optimizers import SGD\n",
                "from keras.preprocessing.image import ImageDataGenerator"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "!pip install ../input/efficientnet-keras-source-code/repository/qubvel-efficientnet-c993591"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train_df = pd.read_csv('../input/understanding_cloud_organization/train.csv')\n",
                "train_df['ImageId'] = train_df['Image_Label'].apply(lambda x: x.split('_')[0])\n",
                "train_df['ClassId'] = train_df['Image_Label'].apply(lambda x: x.split('_')[1])\n",
                "train_df['hasMask'] = ~ train_df['EncodedPixels'].isna()\n",
                "\n",
                "print(train_df.shape)\n",
                "train_df.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "mask_count_df = train_df.groupby('ImageId').agg(np.sum).reset_index()\n",
                "mask_count_df.sort_values('hasMask', ascending=False, inplace=True)\n",
                "print(mask_count_df.shape)\n",
                "mask_count_df.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "sub_df = pd.read_csv('../input/understanding_cloud_organization/sample_submission.csv')\n",
                "sub_df['ImageId'] = sub_df['Image_Label'].apply(lambda x: x.split('_')[0])\n",
                "test_imgs = pd.DataFrame(sub_df['ImageId'].unique(), columns=['ImageId'])"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "import efficientnet.keras as efn \n",
                "def EfficientUNet(input_shape):\n",
                "    backbone = efn.EfficientNetB4(\n",
                "        weights=None,\n",
                "        include_top=False,\n",
                "        input_shape=input_shape\n",
                "    )\n",
                "    \n",
                "    input = backbone.input\n",
                "    x00 = backbone.input  # (256, 512, 3)\n",
                "    x10 = backbone.get_layer('stem_activation').output  # (128, 256, 4)\n",
                "    x20 = backbone.get_layer('block2d_add').output  # (64, 128, 32)\n",
                "    x30 = backbone.get_layer('block3d_add').output  # (32, 64, 56)\n",
                "    x40 = backbone.get_layer('block5f_add').output  # (16, 32, 160)\n",
                "    x50 = backbone.get_layer('block7b_add').output  # (8, 16, 448)\n",
                "    \n",
                "    x01 = H([x00, U(x10)], 'X01')\n",
                "    x11 = H([x10, U(x20)], 'X11')\n",
                "    x21 = H([x20, U(x30)], 'X21')\n",
                "    x31 = H([x30, U(x40)], 'X31')\n",
                "    x41 = H([x40, U(x50)], 'X41')\n",
                "    \n",
                "    x02 = H([x00, x01, U(x11)], 'X02')\n",
                "    x12 = H([x11, U(x21)], 'X12')\n",
                "    x22 = H([x21, U(x31)], 'X22')\n",
                "    x32 = H([x31, U(x41)], 'X32')\n",
                "    \n",
                "    x03 = H([x00, x01, x02, U(x12)], 'X03')\n",
                "    x13 = H([x12, U(x22)], 'X13')\n",
                "    x23 = H([x22, U(x32)], 'X23')\n",
                "    \n",
                "    x04 = H([x00, x01, x02, x03, U(x13)], 'X04')\n",
                "    x14 = H([x13, U(x23)], 'X14')\n",
                "    \n",
                "    x05 = H([x00, x01, x02, x03, x04, U(x14)], 'X05')\n",
                "    \n",
                "    x_out = Concatenate(name='bridge')([x01, x02, x03, x04, x05])\n",
                "    x_out = Conv2D(4, (3,3), padding=\"same\", name='final_output', activation=\"sigmoid\")(x_out)\n",
                "    \n",
                "    return Model(inputs=input, outputs=x_out)\n",
                "\n",
                "model = EfficientUNet((320, 480 ,3))\n",
                "\n",
                "model.summary()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "minsizes = [20000 ,20000, 22500, 10000]\n"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "sigmoid = lambda x: 1 / (1 + np.exp(-x))"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "check_results",
                "process_data"
            ],
            "source": [
                "sub_df = sub_df[['Image_Label', 'EncodedPixels']]\n",
                "sub_df.to_csv('submission.csv', index=False)\n",
                "display(sub_df.head(10))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "#Q1create two NumPy array by takin user input of data stored in array, check if they have views to same memory, check if elements of arrays are divisible by 3 or not sort 2nd array and find sum of all elements of 1st array\n",
                "import numpy as np\n",
                "inp1 = input(\"Enter first array:\")\n",
                "a = inp1.split()\n",
                "a = [int(i) for i in a]\n",
                "inp2 = input(\"Enter second array:\")\n",
                "b = inp2.split()\n",
                "b = [int(i) for i in b]\n",
                "Arr1 = np.array(a)\n",
                "Arr2 = np.array(b)\n",
                "print(\"Array 1 :\")\n",
                "print(Arr1)\n",
                "print(\"Array 2 :\")\n",
                "print(Arr2)\n",
                "print(\"Do both of these arrays share the same memory :\")\n",
                "print(id(Arr1)==id(Arr2))\n",
                "div1 = Arr1%3==0\n",
                "div2 = Arr2%3==0\n",
                "print(\"elements of array 1 divisible by 3 are :\")\n",
                "print(Arr1[div1])\n",
                "print(\"elements of array 2 divisible by 3 are :\")\n",
                "print(Arr2[div2])\n",
                "print(\"Array 2 after sorting is :\")\n",
                "Arr2.sort()\n",
                "print(Arr2)\n",
                "print(\"Sum of elements of array 1 is :\")\n",
                "print (Arr1.sum())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data",
                "ingest_data"
            ],
            "source": [
                "#Q2 Load the titanic dataset, remove missing values from all attributes, find mean value of first 50 samples, find the mean of the number of male passengers( Sex=1) on the ship, find the highest fare paid by any passenger.\n",
                "import pandas as pd\n",
                "df = pd.read_csv(\"../input/titanic/train_and_test2.csv\")\n",
                "df.head()\n",
                "\n",
                "df.dropna(axis=1, how='all')\n",
                "print(df.head())\n",
                "print(df.shape)\n",
                "\n",
                "print(df[:50].mean())\n",
                "\n",
                "print(df[df['Sex']==1].mean())\n",
                "\n",
                "print(df['Fare'].max())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "#Q3.A student has got the following marks ( English = 86, Maths = 83, Science = 86, History =90, Geography = 88). Wisely choose a graph to represent this data such that it justifies the purpose of data visualization. Highlight the subject in which the student has got least marks. \n",
                "from matplotlib import pyplot as plt\n",
                "slices=[87,83,86,90,88]\n",
                "Subject=['English','Maths','Science','History','Geography']\n",
                "plt.pie(slices,labels=Subject,startangle=90,shadow=True,explode=(0.08,0.5,0.08,0.08,0.08),autopct='%1.1f%%')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#Q4 Load the iris dataset, print the confusion matrix and f1_score as computed on the features.\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.model_selection import cross_val_predict\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import f1_score\n",
                "\n",
                "train = pd.read_csv(\"../input/iris-flower-dataset/IRIS.csv\")\n",
                "\n",
                "\n",
                "X = train.drop(\"species\",axis=1)\n",
                "y = train[\"species\"]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
                "\n",
                "logmodel = LogisticRegression()\n",
                "logmodel.fit(X_train,y_train)\n",
                "\n",
                "predictions = logmodel.predict(X_test)\n",
                "\n",
                "print(\"F1 Score(macro):\",f1_score(y_test, predictions,average='macro'))\n",
                "print(\"F1 Score(micro):\",f1_score(y_test, predictions,average='micro'))\n",
                "print(\"F1 Score(weighted):\",f1_score(y_test, predictions,average='weighted'))\n",
                "print(\"\\nConfusion Matrix(below):\\n\")\n",
                "confusion_matrix(y_test, predictions)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "#Creation of Boolean Array\n",
                "import numpy as np\n",
                "arr1=np.array([1,2,0,True,False],dtype=np.bool)\n",
                "arr1"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "import numpy as np\n",
                "array1 = np.array([0, 10, 20, 40, 60])\n",
                "print(\"Array1: \",array1)\n",
                "array2 = [10, 30, 40]\n",
                "print(\"Array2: \",array2)\n",
                "print(\"Common values between two arrays:\")\n",
                "print(np.intersect1d(array1, array2))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "import numpy as np\n",
                "a=np.array([33,33,13,44,55,66,77,55,12,23,21,34,59])\n",
                "b=np.array([21,34,55,77])\n",
                "x=b.argsort()\n",
                "out=a[b[x[np.searchsorted(b,a,sorter=x)]]!=a]\n",
                "print(out)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "exam_data  = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', 'Matthew', 'Laura'],\n",
                "        'score': [12.5, 9, 16.5, 6.0, 9, 20, np.nan, 5.5],\n",
                "        'attempts': [1, 3, 2, 3, 2, 3, 1, 1],\n",
                "        'qualify': ['yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no']}\n",
                "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
                "\n",
                "df = pd.DataFrame(exam_data , index=labels)\n",
                "print(df)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "exam_data  = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', 'Matthew', 'Laura'],\n",
                "        'score': [12.5, 9, 16.5, 6.0, 9, 20, np.nan, 5.5],\n",
                "        'attempts': [1, 3, 2, 3, 2, 3, 1, 1],\n",
                "        'qualify': ['yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no']}\n",
                "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
                "df = pd.DataFrame(exam_data , index=labels)\n",
                "print(\"Select specific columns and rows:\")\n",
                "print(df.iloc[[1, 3, 5, 6], [1, 3]])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "exam_data  = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', 'Matthew', 'Laura'],\n",
                "        'score': [12.5, 9, 16.5, 6.0, 9, 20, np.nan, 5.5],\n",
                "        'attempts': [1, 3, 2, 3, 2, 3, 1, 1],\n",
                "        'qualify': ['yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no']}\n",
                "labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
                "df = pd.DataFrame(exam_data , index=labels)\n",
                "print(\"Rows where score is missing:\")\n",
                "print(df[df['score'].isnull()])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "from matplotlib import pyplot as plt\n",
                "x=['O+','A+','B+','AB+','O-','A-','B-','AB-']\n",
                "y=[9,12,2,10,7,1,4,5]\n",
                "plt.plot(x,y,'r--',x,y,'g^')\n",
                "plt.title('Blood group distribution of 50 patients')\n",
                "plt.ylabel('No of Patients')\n",
                "plt.xlabel('Blood group')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "from matplotlib import pyplot as plt\n",
                "slices=[85,87,92,98,80,83]\n",
                "Subject=['English','Bengali','Hindi','Maths','History','Geography']\n",
                "plt.pie(slices,labels=Subject,startangle=90,shadow=True,explode=(0.08,0.08,0.08,0.08,0.5,0.08),autopct='%1.1f%%')\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "from matplotlib import pyplot as plt\n",
                "heights=[161,150,154,165,168,161,154,162,150,121,162,164,171,165,158,154,156,172,160,170,153,159,161,170,162,165,166,168,165,164,154,152,153,156,158,172,172,161,12,166,161,12,162,167,168,159,158,153,154,159]\n",
                "bins=[150,155,160,165,170]\n",
                "plt.hist(heights,bins,histtype='bar',rwidth=0.5,color='blue')\n",
                "plt.xlabel('Height range')\n",
                "plt.ylabel('No of persons')\n",
                "plt.title(\"Heights Histogram\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#Write a Python program using Scikit-learn to print the keys, number of rows-columns, feature names and the description of the Iris data.\n",
                "import pandas as pd\n",
                "iris_data = pd.read_csv(\"../input/iriscsv/Iris.csv\")\n",
                "print(\"\\nKeys of Iris dataset:\")\n",
                "print(iris_data.keys())\n",
                "print(\"\\nNumber of rows and columns of Iris dataset:\")\n",
                "print(iris_data.shape) "
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "# Write a Python program to get the number of observations, missing values and nan values.\n",
                "import pandas as pd\n",
                "iris = pd.read_csv(\"../input/iriscsv/Iris.csv\")\n",
                "print(iris.info())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#Write a Python program to load the iris data from a given csv file into a dataframe and print the shape of the data, type of the data and first 3 rows.\n",
                "import pandas as pd\n",
                "data = pd.read_csv(\"../input/iriscsv/Iris.csv\")\n",
                "print(\"Shape of the data:\")\n",
                "print(data.shape)\n",
                "print(\"\\nData Type:\")\n",
                "print(type(data))\n",
                "print(\"\\nFirst 3 rows:\")\n",
                "print(data.head(3))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "#Display the Principal components that are calculated on the predictor variables and target variables. \n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "from sklearn.datasets import load_breast_cancer\n",
                "cancer = load_breast_cancer()\n",
                "cancer.keys()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "scaler = StandardScaler()\n",
                "scaler.fit(df)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "scaled_data = scaler.transform(df)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.decomposition import PCA\n",
                "pca = PCA(n_components=2)\n",
                "pca.fit(scaled_data)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x_pca = pca.transform(scaled_data)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "scaled_data.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "x_pca.shape"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#Using scatter plot show where the Principal components lie on the graph. \n",
                "plt.figure(figsize=(8,6))\n",
                "plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='rainbow')\n",
                "plt.xlabel('First principal component')\n",
                "plt.ylabel('Second Principal Component')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#2. Store height of 50 students in inches. Now while the data was beign recorded manually there has been some typing mistake and therefore height of 2 students have been recorded as 172 inch and 2 students have been recorded as 12 inch. Graphically plot and show how you can seggregate correct data from abnormal data.\n",
                "from matplotlib import pyplot as plt\n",
                "heights=[72,71,56,45,67,89,54,58,67,77,77,78,77,73,73,172,72,71,56,45,67,\n",
                "         89,54,58,67,172,77,78,77,73,73,172,12,54,64,75,75,77,88,66,70,12,54,64,75,75,77,88,66,70]\n",
                "def plot_his(heights):\n",
                "    start=min(heights)-min(heights)%10\n",
                "    end=max(heights)+10\n",
                "    bins=list(range(start,end,5))\n",
                "    plt.hist(heights,bins,histtype='bar',rwidth=0.5,color='#FF2400')\n",
                "    plt.xlabel('heights in inches')\n",
                "    plt.ylabel('No. of Students')\n",
                "    plt.title(\"Heights chart\")\n",
                "    plt.show()\n",
                "print(\"Abnormal Data\")\n",
                "plot_his(heights)\n",
                "heights=list(filter(lambda x: not x==172 and not x==12, heights))\n",
                "print(\"Correct Data\")\n",
                "plot_his(heights)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#3. Write a Python program to get the number of observations, missing values and nan values.\n",
                "import pandas as pd\n",
                "iris = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(iris.info())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "import pandas as pd\n",
                "data=pd.read_csv(\"../input/titanic/train_and_test2.csv\")\n",
                "data\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "#Replacing the value of year to 2020\n",
                "thisdict[\"year\"]=2020\n",
                "print(thisdict)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#Removing an item\n",
                "thisdict.pop(\"model\")\n",
                "print(thisdict)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import numpy as np \n",
                "import pandas as pd\n",
                "from nltk.tokenize import word_tokenize\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from nltk.corpus import stopwords\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.linear_model import SGDClassifier\n",
                "from sklearn.model_selection import cross_val_predict\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import roc_auc_score, classification_report, f1_score\n",
                "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "df  = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')"
            ]
        },
        {
            "tags": [
                "train_model",
                "process_data",
                "train_model"
            ],
            "source": [
                "vect = TfidfVectorizer()\n",
                "sklearn_tokenizer = vect.build_tokenizer()\n",
                "stop_words = set(stopwords.words(\"english\"))\n",
                "y = df.target.to_numpy()\n",
                "vect=TfidfVectorizer(tokenizer = sklearn_tokenizer,stop_words='english',ngram_range=(1, 1), norm='l2')\n",
                "clf=SGDClassifier(alpha=0.0001,epsilon=0.1, eta0=0.0,\n",
                "                               l1_ratio=0.1, learning_rate='optimal',\n",
                "                               loss='modified_huber', penalty='l2',class_weight =  'balanced')\n",
                "pp = Pipeline([('vect',vect),('clf',clf )])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "pp.fit(df.question_text.to_numpy(),y)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "test_df =  pd.read_csv('../input/quora-insincere-questions-classification/test.csv')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "test_df['prediction'] = pp.predict(test_df.question_text.to_numpy())\n",
                "test_df[['qid','prediction']].to_csv(\"submission.csv\", index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# Imports\n",
                "\n",
                "# pandas\n",
                "import pandas as pd\n",
                "from pandas import Series,DataFrame\n",
                "\n",
                "# numpy, matplotlib, seaborn\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "sns.set_style('whitegrid')\n",
                "%matplotlib inline\n",
                "\n",
                "# machine learning\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC, LinearSVC\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.naive_bayes import GaussianNB"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "# get titanic & test csv files as a DataFrame\n",
                "titanic_df = pd.read_csv(\"../input/train.csv\")\n",
                "test_df    = pd.read_csv(\"../input/test.csv\")\n",
                "\n",
                "# preview the data\n",
                "titanic_df.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "titanic_df.info()\n",
                "print(\"----------------------------\")\n",
                "test_df.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# drop unnecessary columns, these columns won't be useful in analysis and prediction\n",
                "titanic_df = titanic_df.drop(['PassengerId','Name','Ticket'], axis=1)\n",
                "test_df    = test_df.drop(['Name','Ticket'], axis=1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Fare\n",
                "\n",
                "# only for test_df, since there is a missing \"Fare\" values\n",
                "test_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True)\n",
                "\n",
                "# convert from float to int\n",
                "titanic_df['Fare'] = titanic_df['Fare'].astype(int)\n",
                "test_df['Fare']    = test_df['Fare'].astype(int)\n",
                "\n",
                "# get fare for survived & didn't survive passengers \n",
                "fare_not_survived = titanic_df[\"Fare\"][titanic_df[\"Survived\"] == 0]\n",
                "fare_survived     = titanic_df[\"Fare\"][titanic_df[\"Survived\"] == 1]\n",
                "\n",
                "# get average and std for fare of survived/not survived passengers\n",
                "avgerage_fare = DataFrame([fare_not_survived.mean(), fare_survived.mean()])\n",
                "std_fare      = DataFrame([fare_not_survived.std(), fare_survived.std()])\n",
                "\n",
                "# plot\n",
                "titanic_df['Fare'].plot(kind='hist', figsize=(15,3),bins=100, xlim=(0,50))\n",
                "\n",
                "avgerage_fare.index.names = std_fare.index.names = [\"Survived\"]\n",
                "avgerage_fare.plot(yerr=std_fare,kind='bar',legend=False)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# .... continue with plot Age column\n",
                "\n",
                "# peaks for survived/not survived passengers by their age\n",
                "facet = sns.FacetGrid(titanic_df, hue=\"Survived\",aspect=4)\n",
                "facet.map(sns.kdeplot,'Age',shade= True)\n",
                "facet.set(xlim=(0, titanic_df['Age'].max()))\n",
                "facet.add_legend()\n",
                "\n",
                "# average survived passengers by age\n",
                "fig, axis1 = plt.subplots(1,1,figsize=(18,4))\n",
                "average_age = titanic_df[[\"Age\", \"Survived\"]].groupby(['Age'],as_index=False).mean()\n",
                "sns.barplot(x='Age', y='Survived', data=average_age)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Cabin\n",
                "# It has a lot of NaN values, so it won't cause a remarkable impact on prediction\n",
                "titanic_df.drop(\"Cabin\",axis=1,inplace=True)\n",
                "test_df.drop(\"Cabin\",axis=1,inplace=True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Family\n",
                "\n",
                "# Instead of having two columns Parch & SibSp, \n",
                "# we can have only one column represent if the passenger had any family member aboard or not,\n",
                "# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.\n",
                "titanic_df['Family'] =  titanic_df[\"Parch\"] + titanic_df[\"SibSp\"]\n",
                "titanic_df['Family'].loc[titanic_df['Family'] > 0] = 1\n",
                "titanic_df['Family'].loc[titanic_df['Family'] == 0] = 0\n",
                "\n",
                "test_df['Family'] =  test_df[\"Parch\"] + test_df[\"SibSp\"]\n",
                "test_df['Family'].loc[test_df['Family'] > 0] = 1\n",
                "test_df['Family'].loc[test_df['Family'] == 0] = 0\n",
                "\n",
                "# drop Parch & SibSp\n",
                "titanic_df = titanic_df.drop(['SibSp','Parch'], axis=1)\n",
                "test_df    = test_df.drop(['SibSp','Parch'], axis=1)\n",
                "\n",
                "# plot\n",
                "fig, (axis1,axis2) = plt.subplots(1,2,sharex=True,figsize=(10,5))\n",
                "\n",
                "# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)\n",
                "sns.countplot(x='Family', data=titanic_df, order=[1,0], ax=axis1)\n",
                "\n",
                "# average of survived for those who had/didn't have any family member\n",
                "family_perc = titanic_df[[\"Family\", \"Survived\"]].groupby(['Family'],as_index=False).mean()\n",
                "sns.barplot(x='Family', y='Survived', data=family_perc, order=[1,0], ax=axis2)\n",
                "\n",
                "axis1.set_xticklabels([\"With Family\",\"Alone\"], rotation=0)"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "source": [
                "# Pclass\n",
                "\n",
                "# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])\n",
                "sns.factorplot('Pclass','Survived',order=[1,2,3], data=titanic_df,size=5)\n",
                "\n",
                "# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers\n",
                "pclass_dummies_titanic  = pd.get_dummies(titanic_df['Pclass'])\n",
                "pclass_dummies_titanic.columns = ['Class_1','Class_2','Class_3']\n",
                "pclass_dummies_titanic.drop(['Class_3'], axis=1, inplace=True)\n",
                "\n",
                "pclass_dummies_test  = pd.get_dummies(test_df['Pclass'])\n",
                "pclass_dummies_test.columns = ['Class_1','Class_2','Class_3']\n",
                "pclass_dummies_test.drop(['Class_3'], axis=1, inplace=True)\n",
                "\n",
                "titanic_df.drop(['Pclass'],axis=1,inplace=True)\n",
                "test_df.drop(['Pclass'],axis=1,inplace=True)\n",
                "\n",
                "titanic_df = titanic_df.join(pclass_dummies_titanic)\n",
                "test_df    = test_df.join(pclass_dummies_test)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# define training and testing sets\n",
                "\n",
                "X_train = titanic_df.drop(\"Survived\",axis=1)\n",
                "Y_train = titanic_df[\"Survived\"]\n",
                "X_test  = test_df.drop(\"PassengerId\",axis=1).copy()"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "# Logistic Regression\n",
                "\n",
                "logreg = LogisticRegression()\n",
                "\n",
                "logreg.fit(X_train, Y_train)\n",
                "\n",
                "Y_pred = logreg.predict(X_test)\n",
                "\n",
                "logreg.score(X_train, Y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "Support Vector Machines\n",
                "\n",
                "svc = SVC()\n",
                "\n",
                "svc.fit(X_train, Y_train)\n",
                "\n",
                "Y_pred = svc.predict(X_test)\n",
                "\n",
                "svc.score(X_train, Y_train)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "# Random Forests\n",
                "\n",
                "random_forest = RandomForestClassifier(n_estimators=100)\n",
                "\n",
                "random_forest.fit(X_train, Y_train)\n",
                "\n",
                "Y_pred = random_forest.predict(X_test)\n",
                "\n",
                "random_forest.score(X_train, Y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "knn = KNeighborsClassifier(n_neighbors = 3)\n",
                "\n",
                "knn.fit(X_train, Y_train)\n",
                "\n",
                "Y_pred = knn.predict(X_test)\n",
                "\n",
                "knn.score(X_train, Y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "Gaussian Naive Bayes\n",
                "\n",
                "gaussian = GaussianNB()\n",
                "\n",
                "gaussian.fit(X_train, Y_train)\n",
                "\n",
                "Y_pred = gaussian.predict(X_test)\n",
                "\n",
                "gaussian.score(X_train, Y_train)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# get Correlation Coefficient for each feature using Logistic Regression\n",
                "coeff_df = DataFrame(titanic_df.columns.delete(0))\n",
                "coeff_df.columns = ['Features']\n",
                "coeff_df[\"Coefficient Estimate\"] = pd.Series(logreg.coef_[0])\n",
                "\n",
                "# preview\n",
                "coeff_df"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submission = pd.DataFrame({\n",
                "        \"PassengerId\": test_df[\"PassengerId\"],\n",
                "        \"Survived\": Y_pred\n",
                "    })\n",
                "submission.to_csv('titanic.csv', index=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(\"You've successfully run some Python code\")\n",
                "print(\"Congratulations!\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# You don't need to worry for now about what this code does or how it works. If you're ever curious about the \n",
                "# code behind these exercises, it's available under an open source license here: https://github.com/Kaggle/learntools/\n",
                "# (But if you can understand that code, you'll probably find these lessons boring :)\n",
                "from learntools.core import binder; binder.bind(globals())\n",
                "from learntools.python.ex1 import *\n",
                "print(\"Setup complete! You're ready to start question 0.\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "(5 - 3) // 2"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "8 - 3 * 2 - (1 + 1)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "import random\n",
                "from matplotlib import pyplot as plt\n",
                "from learntools.python.quickdraw import random_category, sample_images_of_category, draw_images_on_subplots\n",
                "\n",
                "## Step 1: Sample some sketches\n",
                "# How many sketches to view - a random number from 2 to 20\n",
                "n = random.randint(2, 20)\n",
                "# Choose a random quickdraw category. (Check out https://quickdraw.withgoogle.com/data for an overview of categories)\n",
                "category = random_category()\n",
                "imgs = sample_images_of_category(n, category)\n",
                "\n",
                "## Step 2: Choose the grid properties\n",
                "######## Your changes should go here ###############\n",
                "rows = n // 8 + min(1, n % 8)\n",
                "cols = min(n, 8)\n",
                "height = rows * 2\n",
                "width = cols * 2\n",
                "\n",
                "## Step 3: Create the grid\n",
                "grid = plt.subplots(rows, cols, figsize=(width, height))\n",
                "\n",
                "## Step 4: Draw the sketches in the grid\n",
                "draw_images_on_subplots(imgs, grid)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "\"\"\"Q1.Store blood groups of 50 different atients and show the no of \n",
                "patients having O- blood grouQ1. Store blood groups of 50 different \n",
                "atients and show the no of patients having O- blood group\"\"\"\n",
                "\n",
                "from matplotlib import pyplot as plt\n",
                "blood_grp=['O+', 'A+', 'B+', 'AB+', 'O-', 'A-', 'B-', 'AB-']\n",
                "patients=[5, 10, 12, 5, 3, 4, 5, 6]\n",
                "colors = ['b','b','b','b','g','b','b','b']\n",
                "plt.bar(blood_grp,patients,color=colors)\n",
                "plt.legend()\n",
                "plt.xlabel('Blood Groups')\n",
                "plt.ylabel('No. of Patients')\n",
                "plt.title('Blood Group Data Set')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "\"\"\"Q2. Store data of marks acquired by a certain student and show \n",
                "them in form of a piechart and slice out the subject having least \n",
                "marks\"\"\"\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "subjects=['English','Bengali','Hindi','Maths','History','Geography']\n",
                "marks = [87,89,93,92,98,95]\n",
                "plt.pie(marks,labels=subjects,startangle=90,shadow=True,\n",
                "        explode=(0.2,0,0,0,0,0),autopct='%1.2f%%')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "\"\"\"Q3.Store data of heights of 50 students with 4 mistakes and plot \n",
                "them in a graph and segregate normal data from abnormal one\"\"\"\n",
                "\n",
                "heights=[72,71,56,45,67,89,54,58,67,77,77,78,77,73,73,172,72,71,56,\n",
                "         45,67,89,54,58,67,172,77,78,77,73,73,172,12,54,64,75,75,77,\n",
                "         88,66,70,12,54,64,75,75,77,88,66,70]\n",
                "def plot_his(heights):\n",
                "    start=min(heights)-min(heights)%10\n",
                "    end=max(heights)+10\n",
                "    bins=list(range(start,end,5))\n",
                "    plt.hist(heights,bins,histtype='bar',rwidth=0.5,color='c')\n",
                "    plt.xlabel('height of students (inches)')\n",
                "    plt.ylabel('No.of Students')\n",
                "    plt.show()\n",
                "plot_his(heights)\n",
                "heights=list(filter(lambda x: not x==172 and not x==12, heights))\n",
                "plot_his(heights)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "#Q1.printing the keys, number of rows-columns, feature names \n",
                "#and the description of the Iris data.\n",
                "import pandas as pd\n",
                "iris_data = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(\"\\nKeys of Iris dataset:\")\n",
                "print(iris_data.keys())\n",
                "print(\"\\nNumber of rows and columns of Iris dataset:\")\n",
                "print(iris_data.shape)\n",
                "print(\"Data type:\")\n",
                "print(type(iris_data))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#Q2.get the number of observations, missing values and nan values.\n",
                "print(\"No.of Observations are:\")\n",
                "print(iris_data.count().sum())\n",
                "print(\"No. of Nan is:\")\n",
                "print(iris_data.isnull().sum())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "#Q3.create a 2-D array with ones on the diagonal and zeros elsewhere.\n",
                "import numpy as np\n",
                "from scipy import sparse\n",
                "eye = np.eye(5)\n",
                "print(\"NumPy array:\\n\", eye)\n",
                "sparse_matrix = sparse.csr_matrix(eye)\n",
                "print(\"\\nSciPy sparse CSR matrix:\\n\", sparse_matrix)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#Q4.load the iris data from a given csv file into a dataframe and \n",
                "#print the shape of the data, type of the data and first 3 rows.\n",
                "print(\"Shape of the data:\")\n",
                "print(iris_data.shape)\n",
                "print(\"\\nData Type:\")\n",
                "print(type(iris_data))\n",
                "print(\"\\nFirst 3 rows:\")\n",
                "print(iris_data.head(3))"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "#Q2.Store height of 50 srudents in inches.Now while the data was being recorded manually there has been some typing mistakes therefore height of two students has been recorded as 172 inches and 2 students recorded as 12 inches. Graphically plot and show how you can seggregate the normal data from the abnormal data.\n",
                "import matplotlib.pyplot as plt\n",
                "heights=[72,71,56,45,67,89,54,58,67,77,77,78,77,73,73,172,72,71,56,\n",
                "         45,67,89,54,58,67,172,77,78,77,73,73,172,12,54,64,75,75,77,\n",
                "         88,66,70,12,54,64,75,75,77,88,66,70]\n",
                "def plot_his(heights):\n",
                "    start=min(heights)-min(heights)%10\n",
                "    end=max(heights)+10\n",
                "    bins=list(range(start,end,5))\n",
                "    plt.hist(heights,bins,histtype='bar',rwidth=0.5,color='c')\n",
                "    plt.xlabel('height of students (inches)')\n",
                "    plt.ylabel('No.of Students')\n",
                "    plt.show()\n",
                "print('Total Data')\n",
                "plot_his(heights)\n",
                "heights=list(filter(lambda x: not x==172 and not x==12, heights))\n",
                "print('Normal Data')\n",
                "plot_his(heights)"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data"
            ],
            "source": [
                "#Q3.get the number of observations, missing values and nan values.\n",
                "test_data = pd.read_csv(\"../input/titanicdataset-traincsv/train.csv\")\n",
                "print(\"No.of Observations are:\")\n",
                "print(test_data.count().sum())\n",
                "print(\"No. of Nan is:\")\n",
                "print(test_data.isnull().sum())"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd "
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "check_results"
            ],
            "source": [
                "train = pd.read_csv('../input/train.csv')\n",
                "test = pd.read_csv('../input/test.csv')\n",
                "\n",
                "X = train.drop([\"label\"], axis=1)\n",
                "X = X.values.astype('int32')\n",
                "print(\"loaded\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#reset\n",
                "X = train.drop([\"label\"], axis=1)\n",
                "X = X.values.astype('int32')\n",
                "\n",
                "# clean data\n",
                "print(X)\n",
                "cut = 2#50\n",
                "X[X <= cut] = 0\n",
                "X[X > cut] = 1\n",
                "print(X)"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "setup_notebook",
                "check_results",
                "train_model"
            ],
            "source": [
                "# create model\n",
                "X1 = np.split(X,2)[0]\n",
                "X2 = np.split(X,2)[1]\n",
                "\n",
                "y1 = np.split(train[\"label\"],2)[0]\n",
                "y2 = np.split(train[\"label\"],2)[1]\n",
                "\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "\n",
                "model = DecisionTreeClassifier()\n",
                "\n",
                "model.fit(X2,y2) # easier to use 2 first than fix indexing\n",
                "\n",
                "p = model.predict(X1)\n",
                "\n",
                "total = 0\n",
                "for i in range(200):\n",
                "    if y1[i] == p[i]:\n",
                "        total += 1\n",
                "\n",
                "print(total/200) # print accuracy"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "check_results"
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "X_train = X\n",
                "X_train = X_train.reshape(X_train.shape[0], 28, 28)\n",
                "\n",
                "print('start')\n",
                "fig, axes = plt.subplots(nrows=2, ncols=8)\n",
                "\n",
                "I = 0\n",
                "for ax in axes.flat[:]:\n",
                "    for i in range(I+1, 500):\n",
                "        if(p[i]) != train[\"label\"][i]:\n",
                "            I = i\n",
                "            break\n",
                "    ax.set_title(p[I]);\n",
                "    ax.set_yticklabels([])\n",
                "    ax.set_xticklabels([])\n",
                "    ax.imshow(X_train[i], cmap=plt.get_cmap('gray'))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model",
                "transfer_results"
            ],
            "source": [
                "model.fit(X,train[\"label\"])\n",
                "predictions = model.predict(test)\n",
                "\n",
                "submissions = pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
                "                         \"Label\": predictions})\n",
                "submissions.to_csv(\"DR.csv\", index=False, header=True)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import os\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "from sklearn.metrics import roc_auc_score\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.feature_selection import VarianceThreshold\n",
                "from sklearn.mixture import GaussianMixture\n",
                "from sklearn.covariance import GraphicalLasso\n",
                "from sklearn.preprocessing import StandardScaler"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "train = pd.read_csv('../input/train.csv')\n",
                "test = pd.read_csv('../input/test.csv')\n",
                "train['wheezy-copper-turtle-magic'] = train['wheezy-copper-turtle-magic'].astype('category')\n",
                "test['wheezy-copper-turtle-magic'] = test['wheezy-copper-turtle-magic'].astype('category')"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "ingest_data",
                "process_data"
            ],
            "source": [
                "magicNum = 131073\n",
                "default_cols = [c for c in train.columns if c not in ['id', 'target','target_pred', 'wheezy-copper-turtle-magic']]\n",
                "cols = [c for c in default_cols]\n",
                "sub = pd.read_csv('../input/sample_submission.csv')\n",
                "sub.to_csv('submission.csv',index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "#1\n",
                "import numpy as np\n",
                "a=np.array([9,5,4,3,2,6])\n",
                "b=np.array([5,8,6,9,2,1])\n",
                "print(\"CHECK IF B HAS SAME VIEWS TO MEMORY IN A\")\n",
                "print(b.base is a)\n",
                "print(\"CHECK IF A HAS SAME VIEWS TO MEMORY IN B\")\n",
                "print(a.base is b)\n",
                "div_by_3=a%3==0\n",
                "div1_by_3=b%3==0\n",
                "print(\"Divisible By 3\")\n",
                "print(a[div_by_3])\n",
                "print(b[div1_by_3])\n",
                "b[::-1].sort()\n",
                "print(\"SECOND ARRAY SORTED\")\n",
                "print(b)\n",
                "print(\"SUM OF ELEMENTS OF FIRST ARRAY\")\n",
                "print(np.sum(a))"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data",
                "process_data"
            ],
            "source": [
                "#2\n",
                "df = pd.read_csv(\"../input/titanic/train_and_test2.csv\")\n",
                "df.head()\n",
                "\n",
                "df.dropna(axis=1, how='all')\n",
                "print(df.head())\n",
                "print(df.shape)\n",
                "\n",
                "df[:50].mean()\n",
                "\n",
                "df[df['Sex']==1].mean()\n",
                "\n",
                "df['Fare'].max()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "#3\n",
                "import matplotlib.pyplot as plt\n",
                "teams = ['English', 'Maths', 'Science ', 'History', 'Geography']\n",
                "slices = [86, 83, 86, 90, 88]\n",
                "colors = ['r', 'y', 'g', 'b','c']\n",
                "plt.pie(slices, labels = teams, colors=colors,\n",
                " startangle=90, shadow = True, explode = (0, 0.5, 0, 0, 0),\n",
                " radius = 1.2, autopct = '%1.1f%%')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "#4\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.model_selection import cross_val_predict\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import f1_score\n",
                "\n",
                "\n",
                "train = pd.read_csv(\"../input/iris-flower-dataset/IRIS.csv\")\n",
                "X= train.drop(\"species\", axis=1)\n",
                "y= train[\"species\"]\n",
                "X_train, X_test,y_train, y_test= train_test_split(X,y, test_size=0.3)\n",
                "\n",
                "logmodel= LogisticRegression()\n",
                "logmodel.fit(X_train, y_train)\n",
                "\n",
                "predictions = logmodel.predict(X_test)\n",
                "\n",
                "print(\"F1 Score(macro):\", f1_score(y_test, predictions, average='macro'))\n",
                "print(\"F1 Score(micro):\", f1_score(y_test, predictions, average='micro'))\n",
                "print(\"F1 Score(weighted):\", f1_score(y_test, predictions, average='weighted'))\n",
                "print(\"\\confusion Matrix(below):\\n\")\n",
                "confusion_matrix(y_test, predictions)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#1\n",
                "import pandas as pd \n",
                "iris_data = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(\"\\nKeys of Iris dataset:\")\n",
                "print(iris_data.keys())\n",
                "print(\"\\nNumber of rows and columns of Iris dataset:\")\n",
                "print(iris_data.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#2\n",
                "import pandas as pd\n",
                "iris = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(iris.info())\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#4\n",
                "import pandas as pd\n",
                "data = pd.read_csv(\"../input/iris-dataset/iris.data.csv\")\n",
                "print(\"Shape of the data:\")\n",
                "print(data.shape)\n",
                "print(\"\\nData Type:\")\n",
                "print(type(data))\n",
                "print(\"\\nFirst 3 rows:\")\n",
                "print(data.head(3))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "dataset = pd.read_csv(\"../input/wine-customer-segmentation/Wine.csv\")\n",
                "dataset"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "dataset.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X = dataset.iloc[:,0:13]\n",
                "y = dataset.iloc[:, 13]"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "sc = StandardScaler()\n",
                "\n",
                "X_train = sc.fit_transform(X_train)\n",
                "X_test = sc.fit_transform(X_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.decomposition import PCA\n",
                "pca = PCA(n_components = 2)\n",
                "\n",
                "X_train = pca.fit_transform(X_train)\n",
                "X_test = pca.transform(X_test)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X_train"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X_test"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "new_dataset_train = pd.DataFrame(data=X_train, columns=['PC1', 'PC2'])\n",
                "new_dataset_test = pd.DataFrame(data=X_test, columns=['PC1', 'PC2'])\n",
                "# Con-catenating test and train datasets\n",
                "new_dataset = pd.concat([new_dataset_train.reset_index (drop=True), new_dataset_test], axis=1)\n",
                "new_dataset.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "dataset.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "process_data",
                "ingest_data"
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "from sklearn.datasets import load_breast_cancer\n",
                "cancer = load_breast_cancer()\n",
                "df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n",
                "df.head()\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "scaler = StandardScaler()\n",
                "scaler.fit(df)\n",
                "scaled_data = scaler.transform(df)\n",
                "from sklearn.decomposition import PCA\n",
                "pca = PCA(n_components=2)\n",
                "pca.fit(scaled_data)\n",
                "x_pca = pca.transform(scaled_data)\n",
                "plt.figure(figsize=(8,6))\n",
                "plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='rainbow')\n",
                "plt.xlabel('First principal component')\n",
                "plt.ylabel('Second Principal Component')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "setup_notebook"
            ],
            "source": [
                "DATA_PATH = '/kaggle/input/rs6-attrition-predict/'\n",
                "train = pd.read_csv(f'{DATA_PATH}/train.csv')\n",
                "test = pd.read_csv(f'{DATA_PATH}/test.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(train), len(test)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.columns"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.YearsAtCompany.unique()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "for col in category_cols:\n",
                "    nunique_tr = train[col].nunique()\n",
                "    nunique_te = test[col].nunique()\n",
                "    na_tr = len(train.loc[train[col].isna()]) / len(train)\n",
                "    na_te = len(test.loc[test[col].isna()]) / len(test)\n",
                "    print(f'Col name:{col:30}\\tunique cate num in train:{nunique_tr:5}\\tunique cate num in train:{nunique_te:5}\\tnull sample in train:{na_tr:.2f}\\tnull sample in test:{na_te:.2f}')\n",
                "    "
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "for col in digital_cols:\n",
                "    min_tr = train[col].min()\n",
                "    max_tr = train[col].max()\n",
                "    mean_tr = train[col].mean()\n",
                "    median_tr = train[col].median()\n",
                "    std_tr = train[col].std()\n",
                "    \n",
                "    min_te = test[col].min()\n",
                "    max_te = test[col].max()\n",
                "    mean_te = test[col].mean()\n",
                "    median_te = test[col].median()\n",
                "    std_te = test[col].std()\n",
                "    \n",
                "    na_tr = len(train.loc[train[col].isna()]) / len(train)\n",
                "    na_te = len(test.loc[test[col].isna()]) / len(test)\n",
                "    print(f'Col name:{col:30}')\n",
                "    print(f'\\tIn train data: min value:{min_tr:.2f}\\tmax value:{max_tr:.2f}\\tmean value:{mean_tr:.2f}\\tmedian value:{median_tr:.2f}\\tstd value:{std_tr:.2f}\\tnan sample rate:{na_tr:.2f}\\t')\n",
                "    print(f'\\tIn  test data: min value:{min_te:.2f}\\tmax value:{max_te:.2f}\\tmean value:{mean_te:.2f}\\tmedian value:{median_te:.2f}\\tstd value:{std_te:.2f}\\tnan sample rate:{na_te:.2f}\\t')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train[target_col].unique()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import MinMaxScaler\n",
                "\n",
                "sacalar = MinMaxScaler()\n",
                "train_digital = sacalar.fit_transform(train[digital_cols])\n",
                "test_digital = sacalar.transform(test[digital_cols])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
                "\n",
                "train_category, test_category = None, None\n",
                "drop_cols = ['EmployeeNumber', 'Over18', 'StandardHours']\n",
                "for col in [var for var in category_cols if var not in drop_cols]:\n",
                "    lbe, ohe = LabelEncoder(), OneHotEncoder()\n",
                "    \n",
                "    lbe.fit(pd.concat([train[col], test[col]]).values.reshape(-1, 1))\n",
                "    train[col] = lbe.transform(train[col])\n",
                "    test[col] = lbe.transform(test[col])\n",
                "    \n",
                "    ohe.fit(pd.concat([train[col], test[col]]).values.reshape(-1, 1))\n",
                "    oht_train = ohe.transform(train[col].values.reshape(-1, 1)).todense()\n",
                "    oht_test = ohe.transform(test[col].values.reshape(-1, 1)).todense()\n",
                "    \n",
                "    if train_category is None:\n",
                "        train_category = oht_train\n",
                "        test_category = oht_test\n",
                "    else:\n",
                "        train_category = np.hstack((train_category, oht_train))\n",
                "        test_category = np.hstack((test_category, oht_test))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_digital.shape, test_digital.shape, train_category.shape, test_category.shape"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "train_features = np.hstack((train_digital, train_category))\n",
                "test_features = np.hstack((test_digital, test_category))\n",
                "train_features.shape, test_features.shape"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "target_col_dict = {'Yes': 1, 'No': 0}\n",
                "train_labels = train[target_col].map(target_col_dict).values\n",
                "train_labels.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "from sklearn.linear_model import LinearRegression\n",
                "\n",
                "clf = LinearRegression()\n",
                "clf.fit(train_features, train_labels)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "predictions = clf.predict(test_features)\n",
                "predictions.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "predictions.mean()"
            ]
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "source": [
                "sub = test[['user_id']].copy()\n",
                "sub['Attrition'] = predictions\n",
                "sub['Attrition'] = sub['Attrition'].apply(lambda x: x if x >=0 else 0.0005)\n",
                "sub.to_csv('submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "hbfd"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from torchvision.utils import make_grid\n",
                "import torchvision.utils as vutils\n",
                "import matplotlib.animation as animation\n",
                "from IPython.display import HTML\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import copy\n",
                "import math\n",
                "import torch.utils.checkpoint as cp\n",
                "\n",
                "import time\n",
                "import cv2 as cv\n",
                "from tqdm import tqdm_notebook as tqdm\n",
                "import matplotlib.image as mpimg\n",
                "from math import exp\n",
                "\n",
                "import torchvision.transforms.functional as TF\n",
                "from collections import OrderedDict\n"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "x_train_path = '../input/red-sample/train_blur/'\n",
                "y_train_path = '../input/red-sample/train_sharp/'\n",
                "x_test_path = '../input/red-sample/test_blur/'"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "train_x_img_paths = []\n",
                "train_y_img_paths = []\n",
                "test_img_paths = []"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(torch.__version__)"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "#12 16 24\n",
                "block_config =(6, 10, 8)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "IMG_WIDTH = 1280//2\n",
                "IMG_HEIGHT = 720//2\n",
                "latent_size = 200"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "scale_ratio = 2"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "class DeformUnit(nn.Module):\n",
                "    def __init__(self, inc,outc):\n",
                "        super(DeformUnit, self).__init__()\n",
                "        self.conv = DeformConv2d(inc=inc, outc=outc-inc,kernel_size=3, stride=1, padding=1, bias=False, modulation=False)\n",
                "        self.relu = nn.ReLU(inplace=True)\n",
                "\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = torch.cat([x,self.relu(self.conv(x))],1)\n",
                "        return x\n",
                "\n",
                "class DenseDeformUnit(nn.Module):\n",
                "    def __init__(self, inc, f,out):\n",
                "        super(DenseDeformUnit, self).__init__()\n",
                "        print(inc,f)\n",
                "        self.deformunit1 = DeformUnit(inc,inc+f)\n",
                "        inc = inc+f\n",
                "        print(inc,f)\n",
                "        self.deformunit2 = DeformUnit(inc, inc + f)\n",
                "        inc = inc + f\n",
                "        self.deformunit3 = DeformUnit(inc, inc + f)\n",
                "        inc = inc + f\n",
                "        self.deformunit4 = DeformUnit(inc, inc + f)\n",
                "        inc = inc + f\n",
                "        self.deformunit5 = DeformUnit(inc, inc + f)\n",
                "        inc = inc + f\n",
                "        self.deformunit6 = DeformUnit(inc, inc + f)\n",
                "        inc = inc + f\n",
                "        self.conv =  nn.Sequential(\n",
                "            nn.Conv2d(in_channels=inc, out_channels=out, kernel_size=(1, 1), stride=(1, 1)),\n",
                "            nn.ReLU()\n",
                "        )\n",
                "\n",
                "\n",
                "    def forward(self, x):\n",
                "        originalx = x\n",
                "        x = self.deformunit1(x)\n",
                "        x = self.deformunit2(x)\n",
                "        x = self.deformunit3(x)\n",
                "        x = self.deformunit4(x)\n",
                "        x = self.deformunit5(x)\n",
                "        x = self.deformunit6(x)\n",
                "        x = self.conv(x)\n",
                "        x = torch.cat([originalx,x],1)\n",
                "        return x\n",
                "\n",
                "\n",
                "\n",
                "class Deform_UpConv(nn.Sequential):\n",
                "    def __init__(self, num_input_features, num_output_features):\n",
                "        super(Deform_UpConv, self).__init__()\n",
                "        self.add_module('conv', nn.ConvTranspose2d(in_channels=num_input_features, out_channels=num_output_features,kernel_size=(2, 2), stride=(2, 2)))\n",
                "        self.add_module('relu', nn.ReLU(inplace=True))\n",
                "\n",
                "\n",
                "\n",
                "class Deform_DenseNet(nn.Module):\n",
                "\n",
                "    def __init__(self,inc,f,grow,feature_vector = feature_vector):\n",
                "\n",
                "        super(Deform_DenseNet, self).__init__()\n",
                "        out = 4*grow\n",
                "        self.deformdenseunit1 = DenseDeformUnit(inc,f,out)\n",
                "        inc = out+inc+feature_vector[1]\n",
                "        out = out+2*grow\n",
                "        self.deformdenseunit2 = DenseDeformUnit(inc, f,out)\n",
                "        inc = out+inc+feature_vector[2]\n",
                "        out = out+2*grow\n",
                "        self.deformdenseunit3 = DenseDeformUnit(inc, f, out)\n",
                "\n",
                "    def forward(self, x):\n",
                "        features = self.features(x)\n",
                "        #         print(features.shape)\n",
                "        out = F.relu(features, inplace=True)\n",
                "        #         print(out.shape)\n",
                "        return out\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "device = 'cuda'\n",
                "netG = Generator(True,block_config =block_config,num_encoded_channels = num_encoded_channels).to(device)\n",
                "# netG.apply(weights_init)\n",
                "inp = torch.randn(IMG_WIDTH*IMG_HEIGHT*3 * 1).view((-1,3,IMG_HEIGHT,IMG_WIDTH)).to(device)\n",
                "output = netG(inp)\n",
                "print(output.shape)\n",
                "print((output.shape[1]*output.shape[2]*output.shape[3])/(IMG_WIDTH*IMG_HEIGHT*3))\n",
                "encoded_size = output.shape\n",
                "del inp,netG,output\n",
                "torch.cuda.empty_cache()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "feature_vector"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data",
                "visualize_data"
            ],
            "source": [
                "batch_size=1\n",
                "dataset = ImageData()\n",
                "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
                "\n",
                "a = next(iter(dataloader))\n",
                "\n",
                "print(a[1].shape)\n",
                "print(a[0].shape)\n",
                "img1 = a[0][0]\n",
                "img2 = a[1][0]\n",
                "f, axarr = plt.subplots(1,2)\n",
                "axarr[0].imshow(img1.permute(1,2,0))\n",
                "axarr[1].imshow(img2.permute(1,2,0))\n",
                "f.set_figheight(24)\n",
                "f.set_figwidth(24)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# netD = Discriminator().to(device).apply(weights_init)\n",
                "netG = Generator(False,block_config =block_config,num_encoded_channels = num_encoded_channels).to(device)\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def init_weights(m):\n",
                "    if type(m) == nn.Linear:\n",
                "        torch.nn.init.xavier_uniform_(m.weight)\n",
                "        m.bias.data.fill_(0.01)\n",
                "    try:\n",
                "        torch.nn.init.xavier_uniform(m.weight)\n",
                "    except:\n",
                "        _\n",
                "#         print(m)\n",
                "_ = netG.apply(init_weights)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "lr = 0.0002\n",
                "# Initialize BCELoss function\n",
                "criterion = nn.BCELoss()\n",
                "msecriterion = nn.MSELoss()\n",
                "l1criterion = nn.L1Loss()\n",
                "# Establish convention for real and fake labels during training\n",
                "real_label = 1\n",
                "fake_label = 0\n",
                "\n",
                "# Setup Adam optimizers for both G and D\n",
                "# optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
                "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "torch.save(netG, \"netG.model\")"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "netG.eval()\n",
                "valid_batch = next(iter(dataloader))\n",
                "blur_images = valid_batch[0].to(device)\n",
                "output_heatmap = netG(blur_images)\n",
                "rec_img =  (output_heatmap[0].cpu())"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(rec_img.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "rimage = rec_img.detach().permute(1, 2, 0)\n",
                "plt.imshow(rimage)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "rimage = rec_img.detach().permute(1, 2, 0)\n",
                "plt.imshow(rimage)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(rimage.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "%matplotlib inline\n",
                "from copy import deepcopy\n",
                "from collections import OrderedDict\n",
                "import gc\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm import tqdm_notebook\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.optim import SGD,Adam,lr_scheduler\n",
                "from torch.utils.data import random_split\n",
                "import torchvision\n",
                "from torchvision import transforms,models\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "!tar -zxvf ../input/cifar10-python/cifar-10-python.tar.gz\n"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "batch_size = 128\n",
                "img_size = 28 #224\n",
                "trainset_size = 10000"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "train_transform = transforms.Compose([\n",
                "    transforms.Resize(img_size),\n",
                "#     transforms.RandomHorizontalFlip(p=.40),\n",
                "#     transforms.RandomRotation(30),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
                "\n",
                "test_transform = transforms.Compose([\n",
                "    transforms.Resize(224),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
                "\n",
                "traindata = torchvision.datasets.CIFAR10(root='.', train=True,download=False, transform=train_transform)\n",
                "\n",
                "trainset,valset = random_split(traindata,[trainset_size,50000-trainset_size])\n",
                "\n",
                "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True)\n",
                "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,shuffle=False)\n",
                "\n",
                "testset = torchvision.datasets.CIFAR10(root='.', train=False,download=False, transform=test_transform)\n",
                "testloader = torch.utils.data.DataLoader(testset, batch_size=64,shuffle=False)\n",
                "\n",
                "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from torchvision.utils import make_grid\n",
                "import torchvision.utils as vutils\n",
                "import matplotlib.animation as animation\n",
                "from IPython.display import HTML\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import copy\n",
                "import time\n",
                "import networkx as nx\n",
                "\n",
                "from torch.autograd import Variable\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "\n",
                "device = 'cpu'\n",
                "if torch.cuda.is_available() :\n",
                "    device = 'cuda'"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# custom weights initialization called on netG and netD\n",
                "def weights_init(m):\n",
                "    classname = m.__class__.__name__\n",
                "    if classname.find('Conv') != -1:\n",
                "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
                "    elif classname.find('BatchNorm') != -1:\n",
                "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
                "        nn.init.constant_(m.bias.data, 0)\n",
                "    "
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def get_max_pool_layer(ksize):\n",
                "    return nn.MaxPool2d(ksize).apply(weights_init)\n",
                "\n",
                "\n",
                "def get_avg_pool_layer(ksize):\n",
                "    return nn.AvgPool2d(ksize).apply(weights_init)\n",
                "\n",
                "def get_conv_layer(in_lyr,out_lyr,ksize):\n",
                "    seq_layer = nn.Sequential(\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(in_lyr, out_lyr, ksize),\n",
                "            nn.BatchNorm2d(out_lyr),\n",
                "    )\n",
                "    return seq_layer.apply(weights_init)\n",
                "\n",
                "class depthwise_separable_conv(nn.Module):\n",
                "    def __init__(self, nin, kernels_per_layer, nout):\n",
                "        super(depthwise_separable_conv, self).__init__()\n",
                "        self.depthwise = nn.Conv2d(nin, nin * kernels_per_layer, kernel_size=3, padding=1, groups=nin).apply(weights_init)\n",
                "        self.pointwise = nn.Conv2d(nin * kernels_per_layer, nout, kernel_size=1).apply(weights_init)\n",
                "\n",
                "    def forward(self, x):\n",
                "        out = self.depthwise(x)\n",
                "        out = self.pointwise(out)\n",
                "        return out\n",
                "\n",
                "def get_sep_layer(in_lyr,out_lyr,ksize):\n",
                "    return depthwise_separable_conv(in_lyr,ksize,out_lyr)\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# 1. Conv 3\n",
                "# 2. Conv 5\n",
                "# 3. Sep 3\n",
                "# 4. Sep 5\n",
                "# 5. MaxPool\n",
                "# 6. AvgPool\n",
                "\n",
                "out_features_shape = [10,24,32,64,128]\n",
                "output_size = 10\n",
                "num_actvn_fns = 6\n",
                "\n",
                "def get_new_layer(actvn,x):\n",
                "    ofsindex = 1\n",
                "    actvn = actvn -1\n",
                "#     print(\"Actvn -- \"+str(actvn))\n",
                "    \n",
                "    out_features_shape[ofsindex] = x.shape[1]\n",
                "    \n",
                "    if actvn == 0:\n",
                "        return get_conv_layer(x.shape[1],out_features_shape[ofsindex],3).to(device)\n",
                "    if actvn == 1:\n",
                "        return get_conv_layer(x.shape[1],out_features_shape[ofsindex],5).to(device)\n",
                "    if actvn == 2:\n",
                "        return get_sep_layer(x.shape[1],out_features_shape[ofsindex],3).to(device)\n",
                "    if actvn == 3:\n",
                "        return get_sep_layer(x.shape[1],out_features_shape[ofsindex],5).to(device)\n",
                "    if actvn == 4:\n",
                "        return get_max_pool_layer(3).to(device)\n",
                "    if actvn == 5:\n",
                "        return get_avg_pool_layer(3).to(device)\n",
                "    return get_avg_pool_layer(3).to(device)\n",
                "\n",
                "def get_out_layer(size):\n",
                "    nlayer = nn.Sequential(\n",
                "            nn.Linear(in_features=size[1]*size[2]*size[3], out_features=10),\n",
                "            nn.LogSoftmax(),\n",
                "        )\n",
                "    return nlayer.apply(weights_init).to(device)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def concatzeroes(max_height,max_width,b):\n",
                "    dim = 2\n",
                "    pad_size_1 = int((max_height-b.shape[dim]) / 2)\n",
                "    pad_size_2 = (max_height-b.shape[dim]) - pad_size_1\n",
                "    b = F.pad(input=b, pad=(0, 0, pad_size_1, pad_size_2), mode='constant', value=0)\n",
                "\n",
                "    dim = 3\n",
                "    pad_size_1 = int((max_width-b.shape[dim]) / 2)\n",
                "    pad_size_2 = (max_width-b.shape[dim]) - pad_size_1\n",
                "    b = F.pad(input=b, pad=(pad_size_1, pad_size_2,0,0), mode='constant', value=0)\n",
                "    \n",
                "    return b\n",
                "\n",
                "def downsizetensors(max_height,max_width,b):\n",
                "    size = (max_height,max_width)\n",
                "    return F.interpolate(b, size=size, mode='bilinear', align_corners=False)\n",
                "\n",
                "def concatenate2(a,b,increase_size):\n",
                "    if(increase_size):\n",
                "        max_height = max(a.shape[2],b.shape[2])\n",
                "        max_width = max(a.shape[3],b.shape[3])\n",
                "        a = concatzeroes(max_height,max_width,a)\n",
                "        b = concatzeroes(max_height,max_width,b)\n",
                "        return torch.cat((a, b), 1)\n",
                "    else:\n",
                "        max_height = min(a.shape[2],b.shape[2])\n",
                "        max_width = min(a.shape[3],b.shape[3])\n",
                "        a = downsizetensors(max_height,max_width,a)\n",
                "        b = downsizetensors(max_height,max_width,b)\n",
                "        return torch.cat((a, b), 1)\n",
                "\n",
                "\n",
                "def concatenate3(a,b,c,increase_size):\n",
                "    if(increase_size):\n",
                "        max_height = max(a.shape[2],b.shape[2],c.shape[2])\n",
                "        max_width = max(a.shape[3],b.shape[3],c.shape[3])\n",
                "        a = concatzeroes(max_height,max_width,a)\n",
                "        b = concatzeroes(max_height,max_width,b)\n",
                "        c = concatzeroes(max_height,max_width,c)\n",
                "        return torch.cat((torch.cat((a, b), 1), c), 1)\n",
                "    else:\n",
                "        \n",
                "        max_height = min(a.shape[2],b.shape[2],c.shape[2])\n",
                "        max_width = min(a.shape[3],b.shape[3],c.shape[3])\n",
                "        a = downsizetensors(max_height,max_width,a)\n",
                "        b = downsizetensors(max_height,max_width,b)\n",
                "        c = downsizetensors(max_height,max_width,c)\n",
                "        return torch.cat((torch.cat((a, b), 1), c), 1)"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "num_nodes = 6\n",
                "dag_model = DAG(num_nodes)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "lr = 0.0002\n",
                "# Initialize BCELoss function\n",
                "criterion = nn.NLLLoss()\n",
                "optimizer = None\n",
                "\n",
                "\n",
                "def model_train_loop(cnn_config):\n",
                "    dag_model.config = cnn_config\n",
                "    dag_model.initialize_param_grads()\n",
                "    for epoch in range(10):\n",
                "        # For each batch in the dataloader\n",
                "        for i, data in enumerate(trainloader, 0):\n",
                "            images, labels = data\n",
                "            images, labels = images.to(device),labels.to(device)\n",
                "            output = dag_model(images)\n",
                "            if i==0 and epoch==0:\n",
                "                optimizer = optim.Adam(dag_model.myparameters, lr=lr, betas=(0.5, 0.999))\n",
                "            dag_model.zero_grad()\n",
                "            loss = criterion(output, labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "#         print(loss)\n",
                "#             if i%100==0:\n",
                "#                 print(((torch.sum(torch.argmax(output,1) == labels)).float()/labels.shape[0]).item())\n",
                "#     #             print(loss.item())\n",
                "        \n",
                "    return Variable(((torch.sum(torch.argmax(output,1) == labels)).float()/labels.shape[0]), requires_grad=True)"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "model_train_loop([2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 4, 1, 4, 4, 5, 4])"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from torch.distributions import Categorical\n",
                "from torch.autograd import Variable"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "controller_num_epochs = 1\n",
                "controller_optimizer = None"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model"
            ],
            "source": [
                "controller_model = Controller().to(device)\n",
                "controller_optimizer = optim.Adam(controller_model.parameters(), lr=0.1, betas=(0.5, 0.999))\n",
                "controller_model()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "from torchvision.utils import make_grid\n",
                "import torchvision.utils as vutils\n",
                "import matplotlib.animation as animation\n",
                "from IPython.display import HTML\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import copy\n",
                "import time\n",
                "import cv2 as cv\n",
                "from tqdm import tqdm_notebook as tqdm\n",
                "import matplotlib.image as mpimg\n",
                "\n",
                "import torchvision.transforms.functional as TF"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# Code -- https://github.com/alexandru-dinu/cae\n",
                "# DataBase -- https://www.kaggle.com/hsankesara/flickr-image-dataset\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "img_dir = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images/'\n",
                "img_list = os.listdir(img_dir)\n",
                "print(len(img_list))\n",
                "valid_ratio = 0.8"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results",
                "process_data",
                "ingest_data"
            ],
            "source": [
                "class ImageData(Dataset):\n",
                "    def __init__(self,is_train=True):\n",
                "        self.is_train = is_train\n",
                "        self.transform = transforms.Compose([transforms.ToTensor(),])\n",
                "        self.train_index = int(valid_ratio * len(img_list))\n",
                "        self.crop = transforms.CenterCrop((218,178))\n",
                "    def __len__(self):\n",
                "        if self.is_train:\n",
                "            return self.train_index\n",
                "        else:\n",
                "            return len(img_list) - self.train_index -1\n",
                "    def __getitem__(self, index):\n",
                "        if not self.is_train:\n",
                "            index = self.train_index + index\n",
                "#         print(\"hey  \"*4 + str(index))\n",
                "        img = mpimg.imread(img_dir+img_list[index])\n",
                "        img = self.crop(TF.to_pil_image(img))\n",
                "        img = self.transform(img)\n",
                "        img = (img-0.5) /0.5\n",
                "#         img = (img - 255.0) / 255.0\n",
                "        return img"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "batch_size=20\n",
                "dataset = ImageData(is_train=False)\n",
                "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
                "device = 'cuda'"
            ]
        },
        {
            "tags": [
                "check_results",
                "visualize_data"
            ],
            "source": [
                "a = next(iter(dataloader))\n",
                "print(a[0].shape)\n",
                "img = a[15]\n",
                "img = img *0.5 + 0.5\n",
                "plt.imshow(img.permute(1,2,0))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "IMG_WIDTH = 178\n",
                "IMG_HEIGHT = 218\n",
                "latent_size = 200"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "num_images_to_show = 5\n",
                "\n",
                "\n",
                "valid_dataset = ImageData(is_train=False)\n",
                "batch_size = num_images_to_show\n",
                "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
                "valid_batch = next(iter(valid_dataloader)).to(device)\n",
                "valid_batch_1 = next(iter(valid_dataloader)).to(device)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "device"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "\n",
                "f, axarr = plt.subplots(num_images_to_show,4)\n",
                "\n",
                "axarr[0,0].title.set_text('Original \\n Image')\n",
                "axarr[0,1].title.set_text('Reconstructed Image with \\n 43% Compression')\n",
                "axarr[0,2].title.set_text('Reconstructed Image with \\n 68% Compression')\n",
                "axarr[0,3].title.set_text('Reconstructed Image with \\n 84% Compression')\n",
                "\n",
                "for i in range(4):\n",
                "    axarr[0,i].title.set_fontsize(15)\n",
                "\n",
                "for i in range(num_images_to_show):\n",
                "    axarr[i,0].imshow((valid_batch[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5)\n",
                "    axarr[i,1].imshow((reconstructed_img_28[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5)\n",
                "    axarr[i,2].imshow((reconstructed_img_16[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5)\n",
                "    axarr[i,3].imshow((reconstructed_img_8[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5)\n",
                "    f.set_figheight(20)\n",
                "    f.set_figwidth(20)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "plt.savefig('results.png')\n",
                "f.savefig('results.png')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "f, axarr = plt.subplots(2,2)\n",
                "\n",
                "axarr[0,0].title.set_text('Original \\n Image')\n",
                "axarr[0,1].title.set_text('Reconstructed Image with \\n 43% Compression')\n",
                "axarr[1,0].title.set_text('Reconstructed Image with \\n 68% Compression')\n",
                "axarr[1,1].title.set_text('Reconstructed Image with \\n 84% Compression')\n",
                "\n",
                "for i in range(2):\n",
                "    for j in range(2):\n",
                "        axarr[i,j].title.set_fontsize(40)\n",
                "i = 0\n",
                "\n",
                "\n",
                "reimg = (valid_batch_1[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5\n",
                "reimg_28 = (reconstructed_img_28_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\n",
                "reimg_16 = (reconstructed_img_16_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\n",
                "reimg_8 = (reconstructed_img_8_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\n",
                "\n",
                "\n",
                "\n",
                "axarr[0,0].imshow(reimg)\n",
                "axarr[0,1].imshow(reimg_28)\n",
                "axarr[1,0].imshow(reimg_16)\n",
                "axarr[1,1].imshow(reimg_8)\n",
                "f.set_figheight(50)\n",
                "f.set_figwidth(50)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "plt.savefig('results1.png')\n",
                "f.savefig('results1.png')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "reimg = reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2])\n",
                "reimg_28 = reimg_28.view(1,reimg_28.shape[0],reimg_28.shape[1],reimg_28.shape[2])\n",
                "reimg_16 = reimg_16.view(1,reimg_16.shape[0],reimg_16.shape[1],reimg_16.shape[2])\n",
                "reimg_8 = reimg_8.view(1,reimg_8.shape[0],reimg_8.shape[1],reimg_8.shape[2])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "reimg = int(reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2]) * 256)\n",
                "reimg_28 = int(reimg_28.view(1,reimg_28.shape[0],reimg_28.shape[1],reimg_28.shape[2]) *256)\n",
                "reimg_16 = int(reimg_16.view(1,reimg_16.shape[0],reimg_16.shape[1],reimg_16.shape[2]) *256)\n",
                "reimg_8 = int(reimg_8.view(1,reimg_8.shape[0],reimg_8.shape[1],reimg_8.shape[2]) *256)"
            ]
        },
        {
            "tags": [
                "check_results",
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "# import torch\n",
                "# from torch.autograd import Variable\n",
                "\n",
                "# img1 = Variable(torch.rand(1, 1, 256, 256))\n",
                "# img2 = Variable(torch.rand(1, 1, 256, 256))\n",
                "\n",
                "# if torch.cuda.is_available():\n",
                "#     img1 = img1.cuda()\n",
                "#     img2 = img2.cuda()\n",
                "\n",
                "\n",
                "# reimg_28 = reimg_28.view(1,reimg_28.shape[0],reimg_28.shape[1],reimg_28.shape[2])\n",
                "# reimg = reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2])\n",
                "# reimg = reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2])\n",
                "print(ssim(reimg, reimg_28))\n",
                "print(ssim(reimg, reimg_16))\n",
                "print(ssim(reimg, reimg_8))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "reimg.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "%matplotlib inline\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib\n",
                "from sklearn import metrics\n",
                "from sklearn.neighbors import NearestNeighbors\n",
                "\n",
                "from keras.models import Sequential, Model\n",
                "from keras.layers import Dense, Dropout, Convolution2D, MaxPooling2D, Flatten, Input\n",
                "from keras.optimizers import adam\n",
                "from keras.utils.np_utils import to_categorical\n",
                "\n",
                "import seaborn as sns\n",
                "\n",
                "%config InlineBackend.figure_format = 'retina'"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train = pd.read_csv(\"../input/train.csv\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train = train.iloc[:,1:].values\n",
                "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1) #reshape to rectangular\n",
                "X_train = X_train/255 #pixel values are 0 - 255 - this makes puts them in the range 0 - 1\n",
                "\n",
                "y_train = train[\"label\"].values"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "y_ohe = to_categorical(y_train)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model = Sequential()\n",
                "\n",
                "model.add(Convolution2D(20, 5, 5, input_shape = (28, 28, 1), activation=\"relu\"))\n",
                "model.add(MaxPooling2D(pool_size=(2,2)))\n",
                "model.add(Dropout(0.5))\n",
                "\n",
                "model.add(Convolution2D(40, 5, 5, activation=\"relu\"))\n",
                "model.add(MaxPooling2D(pool_size=(2,2)))\n",
                "model.add(Dropout(0.5))\n",
                "\n",
                "model.add(Flatten())\n",
                "model.add(Dense(100, activation = \"relu\"))\n",
                "model.add(Dropout(0.5))\n",
                "model.add(Dense(100, activation = \"relu\"))\n",
                "model.add(Dense(10, activation=\"softmax\"))\n",
                "\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model.compile(loss='categorical_crossentropy', \n",
                "              optimizer = adam(lr=0.001), metrics = [\"accuracy\"])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "hist = model.fit(X_train, y_ohe,\n",
                "          validation_split = 0.05, batch_size = 128, epochs = 8)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "model.save_weights(\"model.h5\")"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "model.load_weights(\"model.h5\")"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "loss_and_metrics = model.evaluate(X_train, y_ohe, batch_size=128)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "#getting the 2D output:\n",
                "output = model.get_layer(\"dense_3\").output\n",
                "extr = Model(model.input, output)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n",
                "\n",
                "X_proj = extr.predict(X_train[:10000])\n",
                "X_proj.shape\n",
                "\n",
                "proj = pd.DataFrame(X_proj[:,:2])\n",
                "proj.columns = [\"comp_1\", \"comp_2\"]\n",
                "proj[\"labels\"] = y_train[:10000]\n",
                "\n",
                "sns.lmplot(\"comp_1\", \"comp_2\",hue = \"labels\", data = proj, fit_reg=False)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "test = pd.read_csv(\"../input/test.csv\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "test = test.iloc[:,:].values\n",
                "test = test.reshape(test.shape[0], 28, 28, 1)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "process_data"
            ],
            "source": [
                "pred = model.predict_classes(test,verbose=0)\n",
                "\n",
                "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(pred)+1)),\n",
                "                         \"Label\": pred})"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submissions.to_csv(\"DR.csv\", index=False, header=True)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# data analysis and wrangling\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import random as rnd\n",
                "# machine learning\n",
                "from sklearn import tree\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn import metrics\n",
                "# visualization\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#read in\n",
                "pd_data = pd.read_csv('../input/weatherAUS.csv')\n",
                "pd_data.shape"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#drop NAN\n",
                "pd_data=pd_data.dropna(how='any')\n",
                "print(pd_data.shape)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#drop something column\n",
                "drop_columns_list = ['WindGustDir', 'WindDir9am', 'WindDir3pm','Date','Location','RISK_MM']\n",
                "pd_data = pd_data.drop(drop_columns_list, axis=1)\n",
                "print(pd_data.shape)\n",
                "pd_data.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#change yes/no to 1/0\n",
                "pd_data['RainToday'].replace({'No':0,'Yes':1},inplace=True)\n",
                "pd_data['RainTomorrow'].replace({'No':0,'Yes':1},inplace=True)\n",
                "pd_data.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#Task: Split the data into train and test\n",
                "train_y = pd_data['RainTomorrow'].head(55000)\n",
                "test_y= pd_data['RainTomorrow'].tail(1420)\n",
                "train_x = pd_data.head(55000).drop(['RainTomorrow'], axis=1)\n",
                "test_x= pd_data.tail(1420).drop(['RainTomorrow'], axis=1)\n",
                "print(train_y.head())\n",
                "print(train_x.head())"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data",
                "setup_notebook"
            ],
            "source": [
                "import graphviz \n",
                "dtree=tree.DecisionTreeClassifier(max_depth=3)\n",
                "dtree=dtree.fit(train_x,train_y)\n",
                "dot_data = tree.export_graphviz(dtree, \n",
                "                filled=True, \n",
                "                feature_names=list(train_x),\n",
                "                class_names=['No rain','rain'],\n",
                "                special_characters=True)\n",
                "graph = graphviz.Source(dot_data)  \n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "graph"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#不同資料與結果的關聯性\n",
                "dtree.feature_importances_\n"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "#把訓練好的模型套用到測試數據\n",
                "predict_y = dtree.predict(test_x)\n",
                "predict_y"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "#計算訓練數據與測試數據的正確率\n",
                "from sklearn.metrics import accuracy_score\n",
                "acc_log = dtree.score(train_x, train_y)\n",
                "print('training accuracy: %.5f' % acc_log)\n",
                "x=accuracy_score(test_y, predict_y)\n",
                "print('test accuracy: %.5f' % x)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "#test\n",
                "#測試不同的參數，發現並沒有太大改變\n",
                "#for i in range(400,601,5):    \n",
                "    \"\"\"dtree=tree.DecisionTreeClassifier(min_samples_split=1000,min_samples_leaf =570)\n",
                "    dtree=dtree.fit(train_x,train_y)\n",
                "    predict_y = dtree.predict(test_x)\n",
                "    x=accuracy_score(test_y, predict_y)\n",
                "    print('%d' % i,'test accuracy: %.5f'  %x)\"\"\""
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "#auc\n",
                "fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n",
                "print('max_depth=3 auc: %.5f' % metrics.auc(fpr, tpr))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "tree_train_acc=[]   \n",
                "tree_test_acc=[]    \n",
                "tree_depth=[]       \n",
                "\n",
                "for i in range (2,20):\n",
                "    dtree=tree.DecisionTreeClassifier(max_depth=i)\n",
                "    dtree=dtree.fit(train_x,train_y)\n",
                "    acc_log = dtree.score(train_x, train_y)\n",
                "    print('max_depth=%d ' % i,'training accuracy: %.5f' % acc_log)\n",
                "    \n",
                "    predict_y = dtree.predict(test_x)    \n",
                "    X=accuracy_score(test_y, predict_y)\n",
                "    print('\\t\\ttest accuracy: %.5f' % X)\n",
                "    \n",
                "    tree_train_acc.append(acc_log)\n",
                "    tree_test_acc.append(X)\n",
                "    tree_depth.append(i)\n",
                "    "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.plot(tree_depth,tree_train_acc,'b', label=\"training accuracy\")\n",
                "plt.plot(tree_depth,tree_test_acc,'r', label=\"test accuracy\")\n",
                "plt.ylabel('accuracy (%)')\n",
                "plt.xlabel('max depth ')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "\n",
                "best_depth = tree_depth[tree_test_acc.index(max(tree_test_acc))]\n",
                "print (\"max depth: \", best_depth)\n",
                "print (\"best test accuracy: %.5f\"% max(tree_test_acc))"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n",
                "print('max_depth=7 auc: %.5f' % metrics.auc(fpr, tpr))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "# 交叉驗證 cross validation \n",
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.model_selection import train_test_split\n",
                "scores = cross_val_score(dtree,train_x,train_y,cv=5,scoring='accuracy')\n",
                "\n",
                "# 計算平均值與標準差\n",
                "print('average of Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "# logistic regression\n",
                "\n",
                "logreg = LogisticRegression()\n",
                "logreg = logreg.fit(train_x, train_y)\n",
                "predict_y = logreg.predict(test_x)\n",
                "acc_log = logreg.score(train_x, train_y)\n",
                "print('training accuracy: %.5f' % acc_log)\n",
                "\n",
                "predict_y =logreg.predict(test_x)\n",
                "X=accuracy_score(test_y, predict_y)\n",
                "print('test accuracy: %.5f' % X)\n",
                "\n",
                "#auc\n",
                "fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n",
                "print('auc: %.5f' % metrics.auc(fpr, tpr))\n",
                "\n",
                "#Cross validation\n",
                "scores = cross_val_score(logreg,train_x,train_y,cv=5,scoring='accuracy')\n",
                "# 計算Cross validation的平均值與標準差\n",
                "print('average of Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model",
                "check_results"
            ],
            "source": [
                "# Support Vector Machines\n",
                "#運算時間太長\n",
                "'''\n",
                "svc = SVC(gamma='auto',C=0.1,kernel=\"linear\", probability=True)\n",
                "svc.fit(train_x, train_y)\n",
                "predict_y= svc.predict(test_x)\n",
                "acc_svc = svc.score(train_x, train_y)\n",
                "print('training accuracy: %.5f' % acc_svc)\n",
                "\n",
                "predict_y =svc.predict(test_x)\n",
                "X=accuracy_score(test_y, predict_y)\n",
                "print('test accuracy: %.5f' % X)'''"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model",
                "check_results"
            ],
            "source": [
                "# knn\n",
                "\n",
                "knn = KNeighborsClassifier(n_neighbors = 10)\n",
                "knn.fit(train_x, train_y)\n",
                "predict_y = knn.predict(test_x)\n",
                "acc_knn = knn.score(train_x, train_y)\n",
                "print('training accuracy: %.5f' % acc_knn)\n",
                "\n",
                "predict_y =knn.predict(test_x)\n",
                "X=accuracy_score(test_y, predict_y)\n",
                "print('test accuracy: %.5f' % X)\n",
                "\n",
                "#auc\n",
                "fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n",
                "print('auc: %.5f' % metrics.auc(fpr, tpr))\n",
                "\n",
                "#Cross validation\n",
                "scores = cross_val_score(knn,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print('average of Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "# Gaussian Naive Bayes\n",
                "\n",
                "gaussian = GaussianNB()\n",
                "gaussian.fit(train_x, train_y)\n",
                "predict_y = gaussian.predict(test_x)\n",
                "acc_gaussian = gaussian.score(train_x, train_y)\n",
                "print('training accuracy: %.5f' % acc_gaussian)\n",
                "\n",
                "predict_y =gaussian.predict(test_x)\n",
                "X=accuracy_score(test_y, predict_y)\n",
                "print('test accuracy: %.5f' % X)\n",
                "\n",
                "#auc\n",
                "fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n",
                "print('auc: %.5f' % metrics.auc(fpr, tpr))\n",
                "\n",
                "#Cross validation\n",
                "scores = cross_val_score(gaussian,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print('average of Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "rdf = RandomForestClassifier(bootstrap=True, n_estimators=1000, max_depth=7)\n",
                "rdf.fit(train_x, train_y)  "
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "acc_log = rdf.score(train_x, train_y)\n",
                "print('training accuracy: %.5f' % acc_log)\n",
                "\n",
                "predict_y =rdf.predict(test_x)\n",
                "X=accuracy_score(test_y, predict_y)\n",
                "print('test accuracy: %.5f' % X)\n",
                "#auc\n",
                "fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n",
                "print('auc: %.5f' % metrics.auc(fpr, tpr))\n",
                "\n",
                "#Cross validation\n",
                "#運算時間太長\n",
                "'''scores = cross_val_score(rdf,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print(scores)\n",
                "print('Cross validation: %.5f'%scores.mean())'''"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "#Parameters:決定最佳的n_estimators\n",
                "#目前只算到n_estimators=256，太大需要時間過長\n",
                "\n",
                "from sklearn import model_selection, metrics\n",
                "\n",
                "def scorer(model, X,  train_y):\n",
                "    preds = model.predict(X)\n",
                "    return metrics.accuracy_score( train_y, preds)\n",
                "\n",
                "n_estimators = [1,2,4,8,16,32,64,128, 256]  ## try different n_estimators\n",
                "cv_results = []\n",
                "\n",
                "for estimator in n_estimators:\n",
                "    rf = RandomForestClassifier(n_estimators=estimator)\n",
                "    acc = model_selection.cross_val_score(rf, train_x,  train_y, cv=5, scoring=scorer)\n",
                "    cv_results.append(acc.mean())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "line1= plt.plot(n_estimators, cv_results, 'b', label=\"cross validated accuracy\")\n",
                "plt.ylabel('accuracy')\n",
                "plt.xlabel('n_estimators')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "best_n_estimators = n_estimators[cv_results.index(max(cv_results))]\n",
                "print (\"best_n_estimators: \", best_n_estimators)\n",
                "print (\"best accuracy: \", max(cv_results))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results",
                "process_data"
            ],
            "source": [
                "pd_data = pd.read_csv('../input/weatherAUS.csv')\n",
                "pd_data=pd_data.dropna(how='any')\n",
                "print(pd_data.shape)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "drop_columns_list = ['WindGustDir', 'WindDir9am', 'WindDir3pm','Date','Sunshine','RISK_MM']\n",
                "pd_data = pd_data.drop(drop_columns_list, axis=1)\n",
                "print(pd_data.shape)\n",
                "\n",
                "pd_data['RainToday'].replace({'No':0,'Yes':1},inplace=True)\n",
                "pd_data['RainTomorrow'].replace({'No':0,'Yes':1},inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "groupbyLocation=pd_data.groupby('Location')\n",
                "print(groupbyLocation.size().sort_values(ascending=False))\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "pd_data['Location'] = pd_data['Location'].map( {'Darwin':0,'Perth':1,'Brisbane':2,'MelbourneAirport':3,\n",
                "                                                'PerthAirport':4,'SydneyAirport':5,'Watsonia':6,'Mildura':7,\n",
                "                                                'MountGambier':8,'NorfolkIsland':9,'Cairns':10,'Townsville':11,\n",
                "                                                'WaggaWagga':12,'AliceSprings':13,'Nuriootpa':14,'Hobart':15,\n",
                "                                                'Moree':16,'Melbourne':17,'Portland':18,'Woomera':19,\n",
                "                                                'Sydney':20,'Sale':21,'CoffsHarbour':22,'Williamtown':23,\n",
                "                                                'Canberra':24,'Cobar':25} ).astype(int)\n",
                "train_y=pd_data['RainTomorrow']\n",
                "train_x=pd_data.drop(['RainTomorrow'], axis=1)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "dtree=tree.DecisionTreeClassifier(max_depth=7)\n",
                "dtree=dtree.fit(train_x,train_y)\n",
                "scores = cross_val_score(dtree,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print(scores)\n",
                "print('average of Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "#Logistic Regression\n",
                "logreg = LogisticRegression()\n",
                "logreg.fit(train_x, train_y)\n",
                "\n",
                "#Cross validation\n",
                "scores = cross_val_score(dtree,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print('Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "#knn\n",
                "knn = KNeighborsClassifier(n_neighbors = 10)\n",
                "knn.fit(train_x, train_y)\n",
                "\n",
                "#Cross validation\n",
                "scores = cross_val_score(knn,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print('Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "#Gaussian Naive Bayes\n",
                "gaussian = GaussianNB()\n",
                "gaussian.fit(train_x, train_y)\n",
                "#Cross validation\n",
                "scores = cross_val_score(gaussian,train_x,train_y,cv=5,scoring='accuracy')\n",
                "print('Cross validation: %.5f'%scores.mean())\n",
                "print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# fashion-mnist_test.csv has 10000 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df1 = pd.read_csv('../input/fashion-mnist_test.csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'fashion-mnist_test.csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df1.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotCorrelationMatrix(df1, 196)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotScatterMatrix(df1, 20, 10)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# fashion-mnist_train.csv has 60000 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df2 = pd.read_csv('../input/fashion-mnist_train.csv', delimiter=',', nrows = nRowsRead)\n",
                "df2.dataframeName = 'fashion-mnist_train.csv'\n",
                "nRow, nCol = df2.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df2.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotPerColumnDistribution(df2, 10, 5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotCorrelationMatrix(df2, 196)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotScatterMatrix(df2, 20, 10)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# fashion-mnist_test.csv has 10000 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df1 = pd.read_csv('../input/fashion-mnist_test.csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'fashion-mnist_test.csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df1.head(5)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# fashion-mnist_train.csv has 60000 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df2 = pd.read_csv('../input/fashion-mnist_train.csv', delimiter=',', nrows = nRowsRead)\n",
                "df2.dataframeName = 'fashion-mnist_train.csv'\n",
                "nRow, nCol = df2.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "pip install xmltodict"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "source": [
                "cd ../input/european-soccer-csv-files/"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "#Essential Imports\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import xmltodict\n",
                "import collections"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "#Data comes in a sqlite format, and can be exported in csv format using DB browser (https://sqlitebrowser.org/)\n",
                "\n",
                "#reading data csv files\n",
                "country_df = pd.read_csv('Country.csv')\n",
                "league_df = pd.read_csv('League.csv')\n",
                "match_df = pd.read_csv('Match.csv')\n",
                "player_df = pd.read_csv('Player.csv')\n",
                "player_attr_df = pd.read_csv('Player_Attributes.csv')\n",
                "team_df = pd.read_csv('Team.csv')\n",
                "team_attr_df = pd.read_csv('Team_Attributes.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "country_df.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "type(country_df.name.iloc[0])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "country_df"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "league_df.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "type(league_df.name.iloc[0])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "league_df.head(5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#country_id column drop\n",
                "league_df.drop('country_id', axis= 1, inplace= True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#country df deletion\n",
                "del country_df"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "match_df.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#columns names\n",
                "for i, col in enumerate(match_df.columns):\n",
                "    print(i, col)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "(match_df.country_id == match_df.league_id).all()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#confirmation step\n",
                "cols = np.r_[1, 6, 11:55]\n",
                "match_df.drop(match_df.columns[cols],axis=1, inplace=True)\n",
                "match_df.columns"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#columns names\n",
                "for i, col in enumerate(match_df.columns):\n",
                "    print(i, col)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cols1 = np.r_[0:9, 31:39]\n",
                "match_stats_df = match_df.iloc[:, cols1].reset_index().drop('index', axis= 1)\n",
                "cols2 = np.r_[0, 39:69]\n",
                "match_bets_df = match_df.iloc[:, cols2].reset_index().drop('index', axis= 1)\n",
                "cols3 = np.r_[0, 9:31]\n",
                "match_lineup_df = match_df.iloc[:, cols3].reset_index().drop('index', axis= 1)\n",
                "del match_df"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#change date from string to datetime\n",
                "match_stats_df.date = pd.to_datetime(match_stats_df.date)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#inspecting goal values\n",
                "match_stats_df.goal.unique()[1]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#inspecting possession values\n",
                "match_stats_df.possession.unique()[1]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def parse_goal(goal, home_id, away_id):\n",
                "    '''\n",
                "    The function parses the goal values which is xml text into more convenient tuble\n",
                "    Args:\n",
                "        goal -> xml text with multiple tags and goal info\n",
                "        home_id -> the id of the home team of the match that goal was scored in\n",
                "        away_id -> the id of the away team\n",
                "    Returns:\n",
                "        a tuble of two lists: the first one is the home goals list and the second is the away goals list.\n",
                "        each list consists of a number of tubles correspond to each goal.\n",
                "        tuble format: (time of the goal in mins-int, scorer id-int, assisstant id-int, goal type-string)\n",
                "    '''\n",
                "    if pd.notna(goal):\n",
                "        if xmltodict.parse(goal)['goal'] != None:\n",
                "            goal_dict = xmltodict.parse(goal)['goal']['value']\n",
                "            home_goals = list()\n",
                "            away_goals = list()\n",
                "            if type(goal_dict) == collections.OrderedDict:\n",
                "                goal_dict = [goal_dict]\n",
                "            for g in goal_dict:\n",
                "                try:\n",
                "                    p1 = int(g['player1'])\n",
                "                except:\n",
                "                    p1 = 0\n",
                "                try:\n",
                "                    p2 = int(g['player2'])\n",
                "                except:\n",
                "                    p2 = 0                \n",
                "                g_info = (int(g['elapsed']),p1, p2, g['comment'])\n",
                "                if 'del' not in g.keys():\n",
                "                    if int(g['team']) == home_id:\n",
                "                        home_goals.append(g_info)\n",
                "                    else:\n",
                "                        away_goals.append(g_info)\n",
                "            return home_goals, away_goals"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_stats_df[\"goals_info\"] = match_stats_df.apply(lambda row: parse_goal(row.goal, row.home_team_api_id, row.away_team_api_id), axis= 1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "odd_goals = match_stats_df[match_stats_df.goals_info.isnull() == False]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#confirmation step\n",
                "odd_goals[odd_goals.home_team_goal+odd_goals.away_team_goal != odd_goals.goals_info.apply(lambda x: len(x[0])+len(x[1]))][['goals_info', 'home_team_goal', 'away_team_goal']].head(10)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_stats_df['poss_info'] = match_stats_df.possession.apply(lambda val: parse_poss(val))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "match_stats_df[match_stats_df.goals_info.isnull() == False]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_stats_df.drop(['goal', 'shoton', 'shotoff', 'foulcommit', 'card', 'cross', 'corner', 'possession'], axis=1, inplace= True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_lineup_df.dropna(thresh=20, inplace= True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_lineup_df.reset_index(inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_lineup_df.drop('index', axis=1, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#max number of missing values per lineup\n",
                "match_lineup_df.isnull().sum(axis= 1).max()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_bets_df.dropna(subset= list(match_bets_df.columns).remove('id'), inplace= True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_bets_df.reset_index(inplace= True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "match_bets_df.drop('index', inplace= True, axis= 1)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "cols = list(match_bets_df.columns)\n",
                "cols.remove('id')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for c in cols:\n",
                "    match_bets_df[c] = match_bets_df[c].apply(lambda x: 1/x)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "match_bets_df"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "player_df.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(type(player_df.player_name[0]))\n",
                "print(type(player_df.birthday[0]))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "player_df.birthday = pd.to_datetime(player_df.birthday)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "player_attr_df.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "player_attr_df.date = pd.to_datetime(player_attr_df.date)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "player_attr_df.describe()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "old_mean = player_attr_df.describe().loc['mean']\n",
                "old_std = player_attr_df.describe().loc['std']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for i in zip(player_attr_df.columns, player_attr_df.dtypes):\n",
                "    if i[1] == 'float64':\n",
                "        player_attr_df[i[0]] = player_attr_df[i[0]].fillna(player_attr_df[i[0]].mean())"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "player_attr_df.isnull().sum()"
            ]
        }
    ]
}