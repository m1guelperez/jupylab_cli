content,tag,content_original
"ASSIGN = dict((y, x) for x, y in models.items())",0,"['models_map = dict((y, x) for x, y in models.items())']"
"def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test): ASSIGN=normalize_X(ASSIGN) ASSIGN=normalize_X(ASSIGN) ASSIGN=normalize_X(ASSIGN) ASSIGN = shuffle(ASSIGN, random_state=42) ASSIGN = shuffle(ASSIGN, random_state=42) ASSIGN = shuffle(ASSIGN, random_state=42) return X_train,X_val,X_test,y_train,y_val,y_test",0,"['def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test):\n', '    X_train=normalize_X(X_train)\n', '    X_val=normalize_X(X_val)\n', '    X_test=normalize_X(X_test)\n', '    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n', '    X_val, y_val = shuffle(X_val, y_val, random_state=42)\n', '    X_test, y_test = shuffle(X_test, y_test, random_state=42)\n', '    return X_train,X_val,X_test,y_train,y_val,y_test']"
ASSIGN = ASSIGN.T,0,['artime = artime.T']
SETUP,0,"['import pandas as pd\n', 'import numpy as np\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', 'from matplotlib.pyplot import figure, show\n', 'from matplotlib.ticker import MaxNLocator\n', 'import os \n', 'import time\n', 'from matplotlib import cm\n', 'import pylab as pl']"
"final.drop(['First','Last','Name','Last Name'],axis = 1,inplace = True)",0,"[""final.drop(['First','Last','Name','Last Name'],axis = 1,inplace = True)""]"
"CHECKPOINT test_df['Seed1'] = test_df['Seed1'].map(lambda x: get_seed(x)) test_df['Seed2'] = test_df['Seed2'].map(lambda x: get_seed(x)) test_df['Seed_diff'] = test_df['Seed1'] - test_df['Seed2'] test_df['ScoreT_diff'] = test_df['ScoreT1'] - test_df['ScoreT2'] ASSIGN = ASSIGN.drop(['ID', 'Pred', 'Season', 'WTeamID', 'LTeamID'], axis=1) test_df",0,"[""test_df['Seed1'] = test_df['Seed1'].map(lambda x: get_seed(x))\n"", ""test_df['Seed2'] = test_df['Seed2'].map(lambda x: get_seed(x))\n"", ""test_df['Seed_diff'] = test_df['Seed1'] - test_df['Seed2']\n"", ""test_df['ScoreT_diff'] = test_df['ScoreT1'] - test_df['ScoreT2']\n"", ""test_df = test_df.drop(['ID', 'Pred', 'Season', 'WTeamID', 'LTeamID'], axis=1)\n"", 'test_df']"
"CHECKPOINT ASSIGN = svm.SVC(C = 1,degree = 2, kernel = 'poly',class_weight = 'balanced',gamma = 'scale') ASSIGN = cross_val_score(svc,x_train,x_test,ASSIGN=10) Accuracy3 = ASSIGN.mean() Accuracy.append(Accuracy3) print(ASSIGN) print(ASSIGN.mean())",0,"[""SVM_all = svm.SVC(C = 1,degree = 2, kernel = 'poly',class_weight = 'balanced',gamma = 'scale')\n"", 'cv = cross_val_score(svc,x_train,x_test,cv=10)\n', 'Accuracy3 = cv.mean()\n', 'Accuracy.append(Accuracy3)\n', 'print(cv)\n', 'print(cv.mean())']"
"ASSIGN = pd.get_dummies(ASSIGN, columns = [""Cabin_final""],prefix=""Cabin_"")",0,"['final = pd.get_dummies(final, columns = [""Cabin_final""],prefix=""Cabin_"")']"
"plt.bar(parts,damage_per_strike,color='red') plt.title(""Parts Damage per strike in the Aircraft"") plt.xticks(rotation='vertical')",1,"[""plt.bar(parts,damage_per_strike,color='red')\n"", 'plt.title(""Parts Damage per strike in the Aircraft"")\n', ""plt.xticks(rotation='vertical')""]"
"sns.catplot(x='Year', y='Arson', data=cbdr,height = 5, aspect = 4)",1,"[""sns.catplot(x='Year', y='Arson', data=cbdr,height = 5, aspect = 4)""]"
df_all.sample(n=5),0,['df_all.sample(n=5)']
"CHECKPOINT ''' ASSIGN = SVC(gamma='auto',C=0.1,kernel=""linear"", probability=True) ASSIGN.fit(train_x, train_y) ASSIGN= svc.predict(test_x) ASSIGN = svc.score(train_x, train_y) print('training accuracy: %.5f' % ASSIGN) ASSIGN =svc.predict(test_x) X=accuracy_score(test_y, ASSIGN) print('test accuracy: %.5f' % X)'''",0,"['# Support Vector Machines\n', '#運算時間太長\n', ""'''\n"", 'svc = SVC(gamma=\'auto\',C=0.1,kernel=""linear"", probability=True)\n', 'svc.fit(train_x, train_y)\n', 'predict_y= svc.predict(test_x)\n', 'acc_svc = svc.score(train_x, train_y)\n', ""print('training accuracy: %.5f' % acc_svc)\n"", '\n', 'predict_y =svc.predict(test_x)\n', 'X=accuracy_score(test_y, predict_y)\n', ""print('test accuracy: %.5f' % X)'''""]"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path""); print(ASSIGN.iloc[1:10,3:5])",0,"['#Q.4 Write a pandas program to select the specified columns and rows from a given DataFrame.\n', 'import pandas as pd\n', 'df = pd.read_csv(""../input/prediction-of-asteroid-diameter/Asteroid.csv"");\n', 'print(df.iloc[1:10,3:5])']"
"plt.figure(figsize = [12,5]) sns.distplot(data[data['Clicked']==0]['Salary'], label = 'Clicked==0') sns.distplot(data[data['Clicked']==1]['Salary'], label = 'Clicked==1') plt.legend() plt.show()",1,"['plt.figure(figsize = [12,5])\n', ""sns.distplot(data[data['Clicked']==0]['Salary'], label = 'Clicked==0')\n"", ""sns.distplot(data[data['Clicked']==1]['Salary'], label = 'Clicked==1')\n"", 'plt.legend()\n', 'plt.show()']"
"sns.lineplot(x = 'GRE Score', y = 'CGPA', data = df)",1,"[""sns.lineplot(x = 'GRE Score', y = 'CGPA', data = df)""]"
"ASSIGN = 'Leave', 'Stay' ASSIGN = [' ASSIGN = [df['Attrition'].value_counts()['Yes'],df['Attrition'].value_counts()['No']] ASSIGN = (0.25, 0) fig1, ax1 = plt.subplots() ax1.pie(ASSIGN, ASSIGN=ASSIGN, ASSIGN=ASSIGN, ASSIGN=ASSIGN, autopct='%1.2f%%', ASSIGN=True, startangle=90) ax1.axis('equal') plt.show()",1,"['# Cheese view of target data\n', ""labels = 'Leave', 'Stay'\n"", ""colors = ['#ff9999','#99ff99']\n"", ""sizes = [df['Attrition'].value_counts()['Yes'],df['Attrition'].value_counts()['No']]\n"", 'explode = (0.25, 0)\n', 'fig1, ax1 = plt.subplots()\n', ""ax1.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.2f%%',\n"", '        shadow=True, startangle=90)\n', ""ax1.axis('equal')  \n"", 'plt.show()']"
"original_train.Date = original_train.Date.apply(lambda x:get_dt(x)) original_train.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True) for i in range(len(original_train)): if original_train.Province[i] is np.NaN: original_train.Province[i] = original_train.Country[i] for i in range(len(original_train)): if original_train.Date[i]<datetime.strptime('2020-04-02','%Y-%m-%d') or original_train.Date[i]>datetime.strptime('2020-04-08','%Y-%m-%d'): original_train.drop(i,inplace=True) del original_train['Id'] del original_train['Country']",0,"['original_train.Date = original_train.Date.apply(lambda x:get_dt(x))\n', ""original_train.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)\n"", 'for i in range(len(original_train)):\n', '    if original_train.Province[i] is np.NaN:\n', '        original_train.Province[i] = original_train.Country[i]\n', '        \n', 'for i in range(len(original_train)):\n', ""    if original_train.Date[i]<datetime.strptime('2020-04-02','%Y-%m-%d') or original_train.Date[i]>datetime.strptime('2020-04-08','%Y-%m-%d'):\n"", '        original_train.drop(i,inplace=True)\n', '        \n', ""del original_train['Id']\n"", ""del original_train['Country']""]"
"ASSIGN = pd.get_dummies(ASSIGN, columns=cat)",0,"['df = pd.get_dummies(df, columns=cat)']"
'''train=pd.DataFrame(np.array(train)) train.columns=columns_list''',0,"[""'''train=pd.DataFrame(np.array(train))\n"", ""train.columns=columns_list'''""]"
"sns.jointplot(x = 'TOEFL Score', y = 'CGPA', data=df)",1,"[""sns.jointplot(x = 'TOEFL Score', y = 'CGPA', data=df)""]"
"matplotlib.rcParams['figure.figsize'] = (10.0, 10.0) ASSIGN = extr.predict(X_train[:10000]) X_proj.shape ASSIGN = pd.DataFrame(X_proj[:,:2]) ASSIGN.columns = [""comp_1"", ""comp_2""] ASSIGN = y_train[:10000] sns.lmplot(""comp_1"", ""comp_2"",hue = ""labels"", data = ASSIGN, fit_reg=False)",1,"[""matplotlib.rcParams['figure.figsize'] = (10.0, 10.0)\n"", '\n', 'X_proj = extr.predict(X_train[:10000])\n', 'X_proj.shape\n', '\n', 'proj = pd.DataFrame(X_proj[:,:2])\n', 'proj.columns = [""comp_1"", ""comp_2""]\n', 'proj[""labels""] = y_train[:10000]\n', '\n', 'sns.lmplot(""comp_1"", ""comp_2"",hue = ""labels"", data = proj, fit_reg=False)']"
"CHECKPOINT print('Image ID:', image_ids[0]) print('Annotations:\n', prediction_strings[0])",0,"[""print('Image ID:', image_ids[0])\n"", ""print('Annotations:\\n', prediction_strings[0])""]"
"ASSIGN=data['OFFENSE_CODE_GROUP'].value_counts() ASSIGN=list(data['OFFENSE_CODE_GROUP'].value_counts().index)[:9] ASSIGN=list(count[:9]) ASSIGN.append(ASSIGN.agg(sum)-ASSIGN[:9].agg('sum')) ASSIGN.append('other_type') ASSIGN={""group"":groups, ""ASSIGN"":ASSIGN} ASSIGN=pd.DataFrame(ASSIGN) ASSIGN = type_dict.plot(kind='pie', figsize=(10,7), y='counts', labels=groups, ASSIGN='%1.1f%%', pctdistance=0.9, radius=1.2) plt.legend(loc=0, bbox_to_anchor=(0.95,0.6)) plt.title('Top 10 for crime type', weight='bold', size=14,y=1.08) plt.axis('equal') plt.ylabel('') plt.show() plt.clf() plt.close()",1,"[""count=data['OFFENSE_CODE_GROUP'].value_counts()\n"", ""groups=list(data['OFFENSE_CODE_GROUP'].value_counts().index)[:9]\n"", 'counts=list(count[:9])\n', ""counts.append(count.agg(sum)-count[:9].agg('sum'))\n"", '\n', ""groups.append('other_type')\n"", 'type_dict={""group"":groups,\n', '          ""counts"":counts}\n', 'type_dict=pd.DataFrame(type_dict)\n', ""qx = type_dict.plot(kind='pie', figsize=(10,7), y='counts', labels=groups,\n"", ""             autopct='%1.1f%%', pctdistance=0.9, radius=1.2)\n"", 'plt.legend(loc=0, bbox_to_anchor=(0.95,0.6)) \n', '\n', ""plt.title('Top 10 for crime type', weight='bold', size=14,y=1.08)\n"", ""plt.axis('equal')\n"", ""plt.ylabel('')\n"", 'plt.show()\n', 'plt.clf()\n', 'plt.close()']"
df1.head(5),0,['df1.head(5)']
"for epoch in range(1, 21): train_model(epoch) test_model(epoch)",0,"['for epoch in range(1, 21):\n', '    train_model(epoch)\n', '    test_model(epoch)']"
"ASSIGN = pd.DataFrame(index=np.arange(0, total_images), columns=[""path"", ""target""]) ASSIGN = 0 for n in range(len(folder)): ASSIGN = folder[n] ASSIGN = os.path.join(base_path,class_id) ASSIGN = os.listdir(final_path) for m in range(len(ASSIGN)): ASSIGN = subfiles[m] ASSIGN.iloc[ASSIGN][""path""] = os.path.join(ASSIGN,ASSIGN) ASSIGN.iloc[ASSIGN][""target""] = ASSIGN ASSIGN += 1 ASSIGN.head()",0,"['data = pd.DataFrame(index=np.arange(0, total_images), columns=[""path"", ""target""])\n', '\n', 'k = 0\n', 'for n in range(len(folder)):\n', '    class_id = folder[n]\n', '    final_path = os.path.join(base_path,class_id) \n', '    subfiles = os.listdir(final_path)\n', '    for m in range(len(subfiles)):\n', '      image_path = subfiles[m]\n', '      data.iloc[k][""path""] = os.path.join(final_path,image_path)\n', '      data.iloc[k][""target""] = class_id\n', '      k += 1  \n', '\n', 'data.head()']"
match_stats_df.goal.unique()[1],0,"['#inspecting goal values\n', 'match_stats_df.goal.unique()[1]']"
"CHECKPOINT ASSIGN=-73.80 ASSIGN=40.70 ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(data_Queens_100_1_map) for lat,lon,label in zip(data_Queens_100_1.latitude,data_Queens_100_1.longitude,data_Queens_100_1.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN) data_Queens_100_1_map",1,"['Long=-73.80\n', 'Lat=40.70\n', 'data_Queens_100_1_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'data_Queens_100_1_rooms_map=plugins.MarkerCluster().add_to(data_Queens_100_1_map)\n', 'for lat,lon,label in zip(data_Queens_100_1.latitude,data_Queens_100_1.longitude,data_Queens_100_1.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_Queens_100_1_rooms_map)\n', 'data_Queens_100_1_map.add_child(data_Queens_100_1_rooms_map)\n', '\n', 'data_Queens_100_1_map']"
"CHECKPOINT ASSIGN = Sequential() ASSIGN.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3))) ASSIGN.add(MaxPooling2D((2, 2))) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) ASSIGN.add(MaxPooling2D((2, 2))) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) ASSIGN.add(MaxPooling2D((2, 2))) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) ASSIGN.add(MaxPooling2D((2, 2))) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Flatten()) ASSIGN.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) ASSIGN.add(Dropout(0.5)) ASSIGN.add(Dense(1, activation='sigmoid')) print(ASSIGN.summary()) input_and_run2(ASSIGN,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.001,num_epochs=200)",0,"['##BUILDING THE MODEL 1\n', '\n', 'model4 = Sequential()#add model layers\n', '\n', ""model4.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)))\n"", 'model4.add(MaxPooling2D((2, 2)))\n', 'model4.add(Dropout(0.2))\n', ""model4.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n"", 'model4.add(MaxPooling2D((2, 2)))\n', 'model4.add(Dropout(0.2))\n', ""model4.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n"", 'model4.add(MaxPooling2D((2, 2)))\n', 'model4.add(Dropout(0.2))\n', ""model4.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n"", 'model4.add(MaxPooling2D((2, 2)))\n', 'model4.add(Dropout(0.2))\n', 'model4.add(Flatten())\n', ""model4.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n"", 'model4.add(Dropout(0.5))\n', ""model4.add(Dense(1, activation='sigmoid'))\n"", '\n', 'print(model4.summary())\n', 'input_and_run2(model4,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.001,num_epochs=200)']"
"class MyDataset(Dataset): def __init__(self, data1,data2, labels): self.trend= data1 self.stable= data2 self.labels = labels def __getitem__(self, index): ASSIGN = self.trend[index], self.stable[index], self.labels[index] return trend,stable,labels def __len__(self): return len(self.trend) ASSIGN = MyDataset(data1 = X_temporal_train,data2 = X_stable_train,labels = Y_train) ASSIGN =MyDataset(data1 = X_temporal_test,data2 = X_stable_test,labels = Y_test) ASSIGN = torch.utils.data.DataLoader(train_ds,batch_size = 10,shuffle=False) ASSIGN = torch.utils.data.DataLoader(test_ds,batch_size = 10,shuffle=False)",0,"['# Create train,test loader for training\n', 'class MyDataset(Dataset):\n', '    def __init__(self, data1,data2, labels):\n', '        self.trend= data1\n', '        self.stable= data2\n', '        self.labels = labels  \n', '\n', '    def __getitem__(self, index):    \n', '        trend,stable, labels = self.trend[index], self.stable[index], self.labels[index]\n', '        return trend,stable,labels\n', '\n', '    def __len__(self):\n', '        return len(self.trend) \n', '    \n', 'train_ds = MyDataset(data1 = X_temporal_train,data2 = X_stable_train,labels = Y_train)\n', 'test_ds =MyDataset(data1 = X_temporal_test,data2 = X_stable_test,labels = Y_test)\n', 'train_loader = torch.utils.data.DataLoader(train_ds,batch_size = 10,shuffle=False)\n', 'test_loader = torch.utils.data.DataLoader(test_ds,batch_size = 10,shuffle=False)']"
"CHECKPOINT ASSIGN = model.fit(maxlag=2,method='cmle') print(f'Lag: {ASSIGN.k_ar}') print(f'Coefficients:\n{ASSIGN.params}')",0,"[""ARfit = model.fit(maxlag=2,method='cmle')\n"", ""print(f'Lag: {ARfit.k_ar}')\n"", ""print(f'Coefficients:\\n{ARfit.params}')""]"
ASSIGN = to_categorical(y_train),0,['y_ohe = to_categorical(y_train)']
"ASSIGN=[] for dir1 in dirs: ASSIGN=os.listdir(r""..path""+dir1) for file in ASSIGN: ASSIGN=pd.read_csv(""..path""+dir1+""path""+file,quotechar='""',delimiter=""|"") ASSIGN.append(ASSIGN.values)",0,"['#storing all the files from every directory\n', 'li=[]\n', 'for dir1 in dirs:\n', '    files=os.listdir(r""../input/zomato_data/""+dir1)\n', '    #reading each file from list of files from previous step and creating pandas data fame    \n', '    for file in files:\n', '        \n', '        df_file=pd.read_csv(""../input/zomato_data/""+dir1+""/""+file,quotechar=\'""\',delimiter=""|"")\n', '#appending the dataframe into a list\n', '        li.append(df_file.values)\n', '    \n', '    ']"
"ASSIGN = 131073 ASSIGN = [c for c in train.columns if c not in ['id', 'target','target_pred', 'wheezy-copper-turtle-magic']] ASSIGN = [c for c in default_cols] ASSIGN = pd.read_csv('..path') ASSIGN.to_csv('submission.csv',index=False)",0,"['magicNum = 131073\n', ""default_cols = [c for c in train.columns if c not in ['id', 'target','target_pred', 'wheezy-copper-turtle-magic']]\n"", 'cols = [c for c in default_cols]\n', ""sub = pd.read_csv('../input/sample_submission.csv')\n"", ""sub.to_csv('submission.csv',index=False)""]"
"CHECKPOINT class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv1d(1, 128, 80, 4) self.bn1 = nn.BatchNorm1d(128) self.pool1 = nn.MaxPool1d(4) self.conv2 = nn.Conv1d(128, 128, 3) self.bn2 = nn.BatchNorm1d(128) self.pool2 = nn.MaxPool1d(4) self.conv3 = nn.Conv1d(128, 256, 3) self.bn3 = nn.BatchNorm1d(256) self.pool3 = nn.MaxPool1d(4) self.conv4 = nn.Conv1d(256, 512, 3) self.bn4 = nn.BatchNorm1d(512) self.pool4 = nn.MaxPool1d(4) self.avgPool = nn.AvgPool1d(30) self.fc1 = nn.Linear(512, 10) def forward(self, x): ASSIGN = self.conv1(ASSIGN) ASSIGN = F.relu(self.bn1(ASSIGN)) ASSIGN = self.pool1(ASSIGN) ASSIGN = self.conv2(ASSIGN) ASSIGN = F.relu(self.bn2(ASSIGN)) ASSIGN = self.pool2(ASSIGN) ASSIGN = self.conv3(ASSIGN) ASSIGN = F.relu(self.bn3(ASSIGN)) ASSIGN = self.pool3(ASSIGN) ASSIGN = self.conv4(ASSIGN) ASSIGN = F.relu(self.bn4(ASSIGN)) ASSIGN = self.pool4(ASSIGN) ASSIGN = self.avgPool(ASSIGN) ASSIGN = ASSIGN.permute(0, 2, 1) ASSIGN = self.fc1(ASSIGN) return F.log_softmax(ASSIGN, dim = 2) ASSIGN = Net() ASSIGN.to(device) print(ASSIGN)",0,"['class Net(nn.Module):\n', '    def __init__(self):\n', '        super(Net, self).__init__()\n', '        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n', '        self.bn1 = nn.BatchNorm1d(128)\n', '        self.pool1 = nn.MaxPool1d(4)\n', '        self.conv2 = nn.Conv1d(128, 128, 3)\n', '        self.bn2 = nn.BatchNorm1d(128)\n', '        self.pool2 = nn.MaxPool1d(4)\n', '        self.conv3 = nn.Conv1d(128, 256, 3)\n', '        self.bn3 = nn.BatchNorm1d(256)\n', '        self.pool3 = nn.MaxPool1d(4)\n', '        self.conv4 = nn.Conv1d(256, 512, 3)\n', '        self.bn4 = nn.BatchNorm1d(512)\n', '        self.pool4 = nn.MaxPool1d(4)\n', '        self.avgPool = nn.AvgPool1d(30) #input should be 512x30 so this outputs a 512x1\n', '        self.fc1 = nn.Linear(512, 10)\n', '        \n', '    def forward(self, x):\n', '        x = self.conv1(x)\n', '        x = F.relu(self.bn1(x))\n', '        x = self.pool1(x)\n', '        x = self.conv2(x)\n', '        x = F.relu(self.bn2(x))\n', '        x = self.pool2(x)\n', '        x = self.conv3(x)\n', '        x = F.relu(self.bn3(x))\n', '        x = self.pool3(x)\n', '        x = self.conv4(x)\n', '        x = F.relu(self.bn4(x))\n', '        x = self.pool4(x)\n', '        x = self.avgPool(x)\n', '        x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n', '        x = self.fc1(x)\n', '        return F.log_softmax(x, dim = 2)\n', '\n', 'model = Net()\n', 'model.to(device)\n', 'print(model)']"
"ASSIGN=plt.subplots(2,2,figsize=(20,20)) ASSIGN=data.year.value_counts().index ASSIGN=data.year.value_counts() sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,0]) ax[0,0].set_title(""The counts of inspection by year"",size=20) ax[0,0].set_ylabel('counts',size=18) ax[0,0].set_xlabel('') ASSIGN=data.month.value_counts().index ASSIGN=data.month.value_counts() sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,1]) ax[0,1].set_title(""The counts of inspection by month"",size=20) ax[0,1].set_ylabel('counts',size=18) ax[0,1].set_xlabel('') ASSIGN=data.day.value_counts().index ASSIGN=data.day.value_counts() sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[1,0]) ax[1,0].set_title(""The counts of inspection by day"",size=20) ax[1,0].set_ylabel('counts',size=18) ax[1,0].set_xlabel('') data.groupby(['year','month'])['Inspection ID'].agg('count').unstack('year').plot(ax=ax[1,1]) ax[1,1].set_title(""The counts of inspection for every month by year"",size=20) ax[1,1].set_ylabel('counts',size=18) ax[1,1].set_xlabel('month')",1,"['fig,ax=plt.subplots(2,2,figsize=(20,20))\n', 'x=data.year.value_counts().index\n', 'y=data.year.value_counts()\n', 'sns.barplot(x=x,y=y,ax=ax[0,0])\n', 'ax[0,0].set_title(""The counts of inspection by year"",size=20)\n', ""ax[0,0].set_ylabel('counts',size=18)\n"", ""ax[0,0].set_xlabel('')\n"", '\n', 'x=data.month.value_counts().index\n', 'y=data.month.value_counts()\n', 'sns.barplot(x=x,y=y,ax=ax[0,1])\n', 'ax[0,1].set_title(""The counts of inspection by month"",size=20)\n', ""ax[0,1].set_ylabel('counts',size=18)\n"", ""ax[0,1].set_xlabel('')\n"", '\n', 'x=data.day.value_counts().index\n', 'y=data.day.value_counts()\n', 'sns.barplot(x=x,y=y,ax=ax[1,0])\n', 'ax[1,0].set_title(""The counts of inspection by day"",size=20)\n', ""ax[1,0].set_ylabel('counts',size=18)\n"", ""ax[1,0].set_xlabel('')\n"", '\n', ""data.groupby(['year','month'])['Inspection ID'].agg('count').unstack('year').plot(ax=ax[1,1])\n"", 'ax[1,1].set_title(""The counts of inspection for every month by year"",size=20)\n', ""ax[1,1].set_ylabel('counts',size=18)\n"", ""ax[1,1].set_xlabel('month')""]"
"CHECKPOINT ASSIGN = ASSIGN.drop(['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT'], axis=1) tourney_result",0,"['# deleting unnecessary columns\n', ""tourney_result = tourney_result.drop(['DayNum', 'WScore', 'LScore', 'WLoc', 'NumOT'], axis=1)\n"", 'tourney_result']"
data.isnull().sum().sort_values(ascending=False),0,['data.isnull().sum().sort_values(ascending=False)']
"ASSIGN = range(0,11,2) ASSIGN = range(1,12, 2) ASSIGN = draw_df[draw_df.index.isin(evens)] ASSIGN = draw_df[draw_df.index.isin(odds)] ASSIGN = [ast.literal_eval(pts) for pts in df1.drawing.values] ASSIGN = [ast.literal_eval(pts) for pts in df2.drawing.values] ASSIGN = df2.word.tolist() for i, example in enumerate(ASSIGN): plt.figure(figsize=(6,3)) for x,y in example: plt.subplot(1,2,1) plt.plot(x, y, marker='.') plt.axis('off') for x,y, in ASSIGN[i]: plt.subplot(1,2,2) plt.plot(x, y, marker='.') plt.axis('off') ASSIGN = labels[i] plt.title(ASSIGN, fontsize=10) plt.show()",1,"['evens = range(0,11,2)\n', 'odds = range(1,12, 2)\n', '# We have drawing images, 2 per label, consecutively\n', 'df1 = draw_df[draw_df.index.isin(evens)]\n', 'df2 = draw_df[draw_df.index.isin(odds)]\n', '\n', 'example1s = [ast.literal_eval(pts) for pts in df1.drawing.values]\n', 'example2s = [ast.literal_eval(pts) for pts in df2.drawing.values]\n', 'labels = df2.word.tolist()\n', '\n', 'for i, example in enumerate(example1s):\n', '    plt.figure(figsize=(6,3))\n', '    \n', '    for x,y in example:\n', '        plt.subplot(1,2,1)\n', ""        plt.plot(x, y, marker='.')\n"", ""        plt.axis('off')\n"", '\n', '    for x,y, in example2s[i]:\n', '        plt.subplot(1,2,2)\n', ""        plt.plot(x, y, marker='.')\n"", ""        plt.axis('off')\n"", '        label = labels[i]\n', '        plt.title(label, fontsize=10)\n', '\n', '    plt.show()  ']"
SETUP,0,['from sklearn.model_selection import GridSearchCV']
SETUP,0,"['import os\n', '\n', 'import numpy as np\n', 'import pandas as pd']"
"ASSIGN = [], [] for ps in train['PredictionString']: ASSIGN = get_img_coords(ps) xs += list(x) ys += list(y) plt.figure(figsize=(18,18)) plt.imshow(imread(PATH + 'train_imagespath' + train['ImageId'][2217] + '.jpg'), alpha=0.3) plt.scatter(ASSIGN, color='red', s=10, alpha=0.2);",1,"['xs, ys = [], []\n', '\n', ""for ps in train['PredictionString']:\n"", '    x, y = get_img_coords(ps)\n', '    xs += list(x)\n', '    ys += list(y)\n', '\n', 'plt.figure(figsize=(18,18))\n', ""plt.imshow(imread(PATH + 'train_images/' + train['ImageId'][2217] + '.jpg'), alpha=0.3)\n"", ""plt.scatter(xs, ys, color='red', s=10, alpha=0.2);""]"
"ASSIGN = Lasso() ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]} ASSIGN = GridSearchCV(l, parameters, scoring='neg_mean_squared_error', cv = 100) ASSIGN.fit(x_t, Y_t) ASSIGN.best_params_",0,"['l = Lasso()\n', '\n', ""parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n"", '\n', ""lr = GridSearchCV(l, parameters, scoring='neg_mean_squared_error', cv = 100)\n"", '\n', 'lr.fit(x_t, Y_t)\n', 'lr.best_params_']"
"ASSIGN = Net() ASSIGN = torch.optim.Adam(model.parameters(),lr=0.001) ASSIGN = nn.MSELoss()",0,"['# Training Settings\n', 'model = Net()\n', 'optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n', 'criterion = nn.MSELoss()']"
"CHECKPOINT ASSIGN = model15.predict(x_es) ASSIGN = mean_squared_error(Y_es, y_pred15, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, ASSIGN)))",0,"['y_pred15 = model15.predict(x_es)\n', '\n', 'val = mean_squared_error(Y_es, y_pred15, squared=False)\n', 'val15 = str(round(val, 4))\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred15)))\n""]"
CHECKPOINT data_mat.dtypes,0,['data_mat.dtypes']
headline_by_year(2015),0,['headline_by_year(2015)']
os.chdir('path') os.listdir(),0,"[""os.chdir('/kaggle/input/novel-corona-virus-2019-dataset/')\n"", 'os.listdir()']"
"CHECKPOINT ASSIGN = season_result.groupby(['Season', 'TeamID'])['Score'].sum().reset_index() season_score",0,"[""season_score = season_result.groupby(['Season', 'TeamID'])['Score'].sum().reset_index()\n"", 'season_score']"
len(irishtimes),0,['len(irishtimes)']
"SETUP ASSIGN = train_data.merge(train_id, how='left', on='TransactionID') ASSIGN = test_data.merge(test_id, how='left',on='TransactionID') del train_id; gc.collect() del test_id; gc.collect() del train_data; gc.collect() del test_data; gc.collect()",0,"['import gc\n', ""train = train_data.merge(train_id, how='left', on='TransactionID')\n"", ""test = test_data.merge(test_id, how='left',on='TransactionID')\n"", 'del train_id; gc.collect()\n', 'del test_id; gc.collect()\n', 'del train_data; gc.collect()\n', 'del test_data; gc.collect()']"
"ASSIGN = feature_importance(learn) ASSIGN[:20].plot.barh(x=""cols"", y=""imp"", figsize=(10, 10))",1,"['# Here are our NN feature importance \n', 'fi = feature_importance(learn)\n', 'fi[:20].plot.barh(x=""cols"", y=""imp"", figsize=(10, 10))']"
"ASSIGN=plt.subplots(2,2,figsize=(20,20)) sns.barplot(y=train.healthy.value_counts(),x=train.healthy.value_counts().index,ax=ax[0,0]) ax[0,0].set_title(""Value count for healthy"",size=20) ax[0,0].set_xlabel('healthy',size=18) ax[0,0].set_ylabel('',size=18) sns.barplot(y=train.multiple_diseases.value_counts(),x=train.multiple_diseases.value_counts().index,ax=ax[0,1]) ax[0,1].set_title(""Value count for multiple_diseases"",size=20) ax[0,1].set_xlabel('multiple_diseases',size=18) ax[0,1].set_ylabel('',size=18) sns.barplot(y=train.rust.value_counts(),x=train.rust.value_counts().index,ax=ax[1,0]) ax[1,0].set_title(""Value count for rust"",size=20) ax[1,0].set_xlabel('rust',size=18) ax[1,0].set_ylabel('',size=18) sns.barplot(y=train.scab.value_counts(),x=train.scab.value_counts().index,ax=ax[1,1]) ax[1,1].set_title(""Value count for scab"",size=20) ax[1,1].set_xlabel('healthy',size=18) ax[1,1].set_ylabel('',size=18)",1,"['fig,ax=plt.subplots(2,2,figsize=(20,20))\n', 'sns.barplot(y=train.healthy.value_counts(),x=train.healthy.value_counts().index,ax=ax[0,0])\n', 'ax[0,0].set_title(""Value count for healthy"",size=20)\n', ""ax[0,0].set_xlabel('healthy',size=18)\n"", ""ax[0,0].set_ylabel('',size=18)\n"", '\n', 'sns.barplot(y=train.multiple_diseases.value_counts(),x=train.multiple_diseases.value_counts().index,ax=ax[0,1])\n', 'ax[0,1].set_title(""Value count for multiple_diseases"",size=20)\n', ""ax[0,1].set_xlabel('multiple_diseases',size=18)\n"", ""ax[0,1].set_ylabel('',size=18)\n"", '\n', 'sns.barplot(y=train.rust.value_counts(),x=train.rust.value_counts().index,ax=ax[1,0])\n', 'ax[1,0].set_title(""Value count for rust"",size=20)\n', ""ax[1,0].set_xlabel('rust',size=18)\n"", ""ax[1,0].set_ylabel('',size=18)\n"", '\n', 'sns.barplot(y=train.scab.value_counts(),x=train.scab.value_counts().index,ax=ax[1,1])\n', 'ax[1,1].set_title(""Value count for scab"",size=20)\n', ""ax[1,1].set_xlabel('healthy',size=18)\n"", ""ax[1,1].set_ylabel('',size=18)\n""]"
ASSIGN=data[data.Risk=='Risk 1 (High)'] ASSIGN.head(),0,"[""data_risk1=data[data.Risk=='Risk 1 (High)']\n"", 'data_risk1.head()']"
SETUP,0,['from keras.callbacks import ReduceLROnPlateau']
"learn.fit(3,lr=1e-3)",0,"['learn.fit(3,lr=1e-3)']"
"CHECKPOINT ASSIGN = DecisionTreeRegressor() ASSIGN.fit(x_t, Y_t) ASSIGN = model18.score(x_es,Y_es) print(ASSIGN*100,'%')",0,"['model18 = DecisionTreeRegressor()\n', 'model18.fit(x_t, Y_t)\n', '\n', 'accuracy18 = model18.score(x_es,Y_es)\n', ""print(accuracy18*100,'%')""]"
"CHECKPOINT ASSIGN = 10000 ASSIGN = 0.1 ASSIGN = [] for num in tqdm(range(ASSIGN)): ASSIGN = np.dot(parameters[""W1""],X.T) + parameters[""b1""] ASSIGN = sigmoid(a1) ASSIGN = np.dot(parameters[""W2""],h1) + parameters[""b2""] ASSIGN = sigmoid(a2) ASSIGN = np.dot(parameters[""W3""],h2) + parameters[""b3""] ASSIGN = softmax(a3) ASSIGN = -( Y.T - h3) ASSIGN = (1path)*np.dot(dL_da3 , h2.T) ASSIGN = (1path)*(np.sum(dL_da3,axis=1,keepdims = True)) ASSIGN = np.dot(parameters[""W3""].T , dL_da3) ASSIGN = np.multiply(dL_dh2,sigmoid_derivative(a2)) ASSIGN = (1path)*np.dot(dL_da2 , h1.T) ASSIGN = (1path)*(np.sum(dL_da2,axis=1,keepdims = True)) ASSIGN = np.dot(parameters[""W2""].T , dL_da2) ASSIGN = np.multiply(dL_dh1,sigmoid_derivative(a1)) ASSIGN = (1path)*np.dot(dL_da1 , X) ASSIGN = (1path)*(np.sum(dL_da1,axis = 1,keepdims = True)) parameters[""W3""] = parameters[""W3""] - (ASSIGN)*ASSIGN parameters[""b3""] = parameters[""b3""] - (ASSIGN)*ASSIGN parameters[""W2""] = parameters[""W2""] - (ASSIGN)*ASSIGN parameters[""b2""] = parameters[""b2""] - (ASSIGN)*ASSIGN parameters[""W1""] = parameters[""W1""] - (ASSIGN)*ASSIGN parameters[""b1""] = parameters[""b1""] - (ASSIGN)*ASSIGN ASSIGN.append(compute_cost(ASSIGN.T,Y)) plt.plot(ASSIGN) print(,ASSIGN[-1])",1,"['# dictionary\n', '# Implementation of Network using Gradient Descent\n', 'epochs = 10000\n', 'alpha = 0.1\n', 'costs = []\n', 'for num in tqdm(range(epochs)):\n', '    #Forward Propogation\n', '    a1 = np.dot(parameters[""W1""],X.T) + parameters[""b1""]\n', '    h1 = sigmoid(a1)\n', '    a2 = np.dot(parameters[""W2""],h1) + parameters[""b2""]\n', '    h2 = sigmoid(a2)\n', '    a3 = np.dot(parameters[""W3""],h2) + parameters[""b3""]\n', '    h3 = softmax(a3)\n', '    # Gradients for Backpropogation\n', '    \n', '    dL_da3 = -( Y.T - h3)\n', '    dL_dW3 = (1/N)*np.dot(dL_da3 , h2.T)\n', '    dL_db3 = (1/N)*(np.sum(dL_da3,axis=1,keepdims = True))\n', '    \n', '    dL_dh2 = np.dot(parameters[""W3""].T , dL_da3)\n', '    dL_da2 = np.multiply(dL_dh2,sigmoid_derivative(a2))\n', '    dL_dW2 = (1/N)*np.dot(dL_da2 , h1.T)\n', '    dL_db2 = (1/N)*(np.sum(dL_da2,axis=1,keepdims = True))\n', '    \n', '    dL_dh1 = np.dot(parameters[""W2""].T , dL_da2)\n', '    dL_da1 = np.multiply(dL_dh1,sigmoid_derivative(a1))\n', '    dL_dW1 = (1/N)*np.dot(dL_da1 , X)\n', '    dL_db1 = (1/N)*(np.sum(dL_da1,axis = 1,keepdims = True))\n', '    \n', '    # GD Updates\n', '    parameters[""W3""] = parameters[""W3""] - (alpha)*dL_dW3\n', '    parameters[""b3""] = parameters[""b3""] - (alpha)*dL_db3\n', '    parameters[""W2""] = parameters[""W2""] - (alpha)*dL_dW2\n', '    parameters[""b2""] = parameters[""b2""] - (alpha)*dL_db2\n', '    parameters[""W1""] = parameters[""W1""] - (alpha)*dL_dW1\n', '    parameters[""b1""] = parameters[""b1""] - (alpha)*dL_db1\n', '    costs.append(compute_cost(h3.T,Y))\n', 'plt.plot(costs)\n', 'print(""Training Cost"",costs[-1])']"
"final.replace(to_replace = [ ' Don', ' Rev', ' Dr', ' Mme', ' Major', ' Sir', ' Col', ' Capt',' Jonkheer'], value = ' Honorary(M)', inplace = True) final.replace(to_replace = [ ' Ms', ' Lady', ' Mlle',' the Countess', ' Dona'], value = ' Honorary(F)', inplace = True)",0,"[""final.replace(to_replace = [ ' Don', ' Rev', ' Dr', ' Mme',\n"", ""        ' Major', ' Sir', ' Col', ' Capt',' Jonkheer'], value = ' Honorary(M)', inplace = True)\n"", ""final.replace(to_replace = [ ' Ms', ' Lady', ' Mlle',' the Countess', ' Dona'], value = ' Honorary(F)', inplace = True)""]"
"ASSIGN=np.load(X_TRAIN_FILE) ASSIGN=a.f.arr_0 ASSIGN=np.load(Y_TRAIN_FILE) ASSIGN=a.f.arr_0 ASSIGN=np.load(X_TEST_FILE) ASSIGN=a ASSIGN=np.load(Y_TEST_FILE) ASSIGN=a ASSIGN, X_val, ASSIGN, Y_val = train_test_split(ASSIGN, ASSIGN, test_size=0.25, random_state=42)",0,"['a=np.load(X_TRAIN_FILE)\n', 'X_train=a.f.arr_0\n', 'a=np.load(Y_TRAIN_FILE)\n', 'Y_train=a.f.arr_0\n', 'a=np.load(X_TEST_FILE)\n', 'X_test=a\n', 'a=np.load(Y_TEST_FILE)\n', 'Y_test=a\n', '\n', 'X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)\n']"
"ASSIGN = np.r_[0:9, 31:39] ASSIGN = match_df.iloc[:, cols1].reset_index().drop('index', axis= 1) ASSIGN = np.r_[0, 39:69] ASSIGN = match_df.iloc[:, cols2].reset_index().drop('index', axis= 1) ASSIGN = np.r_[0, 9:31] ASSIGN = match_df.iloc[:, cols3].reset_index().drop('index', axis= 1) del match_df",0,"['cols1 = np.r_[0:9, 31:39]\n', ""match_stats_df = match_df.iloc[:, cols1].reset_index().drop('index', axis= 1)\n"", 'cols2 = np.r_[0, 39:69]\n', ""match_bets_df = match_df.iloc[:, cols2].reset_index().drop('index', axis= 1)\n"", 'cols3 = np.r_[0, 9:31]\n', ""match_lineup_df = match_df.iloc[:, cols3].reset_index().drop('index', axis= 1)\n"", 'del match_df']"
SETUP,0,"['from mpl_toolkits.mplot3d import Axes3D\n', 'from sklearn.preprocessing import StandardScaler\n', 'import matplotlib.pyplot as plt # plotting\n', 'import numpy as np # linear algebra\n', 'import os # accessing directory structure\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n']"
len(y_train),0,['len(y_train)']
"ASSIGN=plt.subplots(1,2,figsize=(20,6)) ASSIGN = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",'cadetblue','hotpink','orange','darksalmon','brown') data.neighbourhood_group.value_counts().plot.bar(color=ASSIGN,ax=ax[0]) ax[0].set_title('The number of rooms in each neighbourhood_group',size=20) ax[0].set_ylabel('rooms',size=18) ax[0].tick_params(axis='x', rotation=360) ax[0].tick_params(labelsize=18) data.groupby(['neighbourhood_group','room_type'])['id'].agg('count').unstack('room_type').plot.bar(ax=ax[1]) ax[1].tick_params(axis='x', rotation=360) ax[1].set_title('The number of rooms in each room_type',size=20) ax[1].set_ylabel('rooms',size=18) ax[1].set_xlabel('') ax[1].tick_params(labelsize=18)",1,"['fig,ax=plt.subplots(1,2,figsize=(20,6))\n', 'clr = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",\'cadetblue\',\'hotpink\',\'orange\',\'darksalmon\',\'brown\')\n', 'data.neighbourhood_group.value_counts().plot.bar(color=clr,ax=ax[0])\n', ""ax[0].set_title('The number of rooms in each neighbourhood_group',size=20)\n"", ""ax[0].set_ylabel('rooms',size=18)\n"", ""ax[0].tick_params(axis='x', rotation=360)\n"", 'ax[0].tick_params(labelsize=18)\n', '\n', ""data.groupby(['neighbourhood_group','room_type'])['id'].agg('count').unstack('room_type').plot.bar(ax=ax[1])\n"", ""ax[1].tick_params(axis='x', rotation=360)\n"", ""ax[1].set_title('The number of rooms in each room_type',size=20)\n"", ""ax[1].set_ylabel('rooms',size=18)\n"", ""ax[1].set_xlabel('')\n"", 'ax[1].tick_params(labelsize=18)']"
"CHECKPOINT ASSIGN = MultinomialNB(alpha = 1,fit_prior = True) Estimator.append(('ASSIGN',MultinomialNB(alpha = 1,fit_prior = True))) ASSIGN = cross_val_score(mnb,x_train,x_test,ASSIGN=10) Accuracy5 = ASSIGN.mean() Accuracy.append(Accuracy5) print(ASSIGN) print(ASSIGN.mean())",0,"['mnb = MultinomialNB(alpha = 1,fit_prior = True)\n', ""Estimator.append(('mnb',MultinomialNB(alpha = 1,fit_prior = True)))\n"", 'cv = cross_val_score(mnb,x_train,x_test,cv=10)\n', 'Accuracy5 = cv.mean()\n', 'Accuracy.append(Accuracy5)\n', 'print(cv)\n', 'print(cv.mean())']"
"logisticRegr.fit(X_train_pca, y_train)",0,"['logisticRegr.fit(X_train_pca, y_train)']"
data_features[(data_features['BsmtFinType1'].isnull() & data_features['BsmtCond'].notnull())],0,"[""data_features[(data_features['BsmtFinType1'].isnull() & data_features['BsmtCond'].notnull())]\n"", '#We detect that all bsmt variables have 79 common missing values. Obviously these data mean there are no basements in these houses.\n', '#So we use the NONE to fill them like the garage variables.']"
df.head(),0,['df.head()']
"SETUP '''reg_score=[] for j in range(1000): ASSIGN =train_test_split(x,y,random_state=j,test_size=0.1) ASSIGN=LinearRegression().fit(x_train,y_train) reg_score.append(ASSIGN.score(x_test,y_test)) K=reg_score.index(np.max(reg_score))",0,"[""'''reg_score=[]\n"", 'import numpy as np\n', 'for j in range(1000):\n', '    x_train,x_test,y_train,y_test =train_test_split(x,y,random_state=j,test_size=0.1)\n', '    lr=LinearRegression().fit(x_train,y_train)\n', '    reg_score.append(lr.score(x_test,y_test))\n', 'K=reg_score.index(np.max(reg_score))\n', ""#K=353'''""]"
"X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)",0,"['X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)']"
CHECKPOINT for i in range(len(predictions1)): print(f),0,"['for i in range(len(predictions1)):\n', '    print(f""predicted={predictions1[i]}, expected={test[\'Total\'][i]}"")']"
"SETUP CHECKPOINT print(check_output([, ]).decode()) ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path') ASSIGN['color'] = ASSIGN['color'].map({'clear':0, 'white':1, 'green':2, 'blood':3, 'blue':4, 'black':5}) sns.set() sns.pairplot(ASSIGN, hue=""type"") print(submission) submission.to_csv('..path', index=False) submission.to_csv('submission.csv', index=False)",1,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n', '\n', 'from subprocess import check_output\n', 'print(check_output([""ls"", ""../input""]).decode(""utf8""))\n', '\n', '# Any results you write to the current directory are saved as output.\n', '\n', '\n', ""train_df = pd.read_csv('../input/train.csv')\n"", ""test_df = pd.read_csv('../input/test.csv')\n"", '# combine = [train_df, test_df]\n', '\n', ""train_df['color'] = train_df['color'].map({'clear':0, 'white':1, 'green':2, 'blood':3, 'blue':4, 'black':5})\n"", 'import seaborn as sns\n', 'sns.set()\n', 'sns.pairplot(train_df, hue=""type"")\n', '\n', '#print(combine)\n', '#print(test_df.columns.values)\n', '# train_df.head()\n', '\n', '# train_df.info()\n', ""# print('_'*40)\n"", '# test_df.info()\n', '\n', '#What is the distribution of numerical feature values across the samples?\n', '# train_df.describe()\n', '\n', '#What is the distribution of categorical features?\n', ""# train_df.describe(include=['O'])\n"", '\n', '\n', '# submission = pd.DataFrame({\n', '#         ""id"": test_df[""id""],\n', '#         ""type"": ""Ghoul""\n', '#     })\n', 'print(submission)\n', ""submission.to_csv('../output/submission.csv', index=False)\n"", ""submission.to_csv('submission.csv', index=False)\n"", '\n', '\n']"
"SETUP CHECKPOINT ASSIGN = SVR(C=1, epsilon=0.1) ASSIGN.fit(x_train,Y_train) ASSIGN = ASSIGN.score(x_test,Y_test) print(ASSIGN*100,'%')",0,"['from sklearn.svm import SVR\n', 'model12 = SVR(C=1, epsilon=0.1)\n', 'model12.fit(x_train,Y_train)\n', '\n', 'model12 = model12.score(x_test,Y_test)\n', ""print(model12*100,'%')""]"
len(train_identity_new),0,['len(train_identity_new)']
ASSIGN = cleaning_test.isnull().sum().sort_values(ascending = False).head(29).index.values.tolist() ASSIGN = cleaning_test[list_miss_test].describe().columns.values.tolist() cleaning_test[ASSIGN] = cleaning_test[ASSIGN].fillna(cleaning_test[ASSIGN].median()),0,"['# Cleaning testset in numerical\n', 'list_miss_test = cleaning_test.isnull().sum().sort_values(ascending = False).head(29).index.values.tolist()\n', 'list_numeric = cleaning_test[list_miss_test].describe().columns.values.tolist()\n', '# handling with median\n', 'cleaning_test[list_numeric] = cleaning_test[list_numeric].fillna(cleaning_test[list_numeric].median())']"
"ASSIGN=pd.DataFrame(train_transaction,columns=train_transaction.isnull().sum().sort_values()[:250].index) ASSIGN=ASSIGN.drop(columns=['TransactionID']) ASSIGN=train_transaction_new.isFraud ASSIGN=ASSIGN.drop(columns=['isFraud']) ASSIGN.head()",0,"['\n', 'train_transaction_new=pd.DataFrame(train_transaction,columns=train_transaction.isnull().sum().sort_values()[:250].index)\n', ""train_transaction_new=train_transaction_new.drop(columns=['TransactionID'])\n"", 'train_transaction_new_label=train_transaction_new.isFraud\n', ""train_transaction_new=train_transaction_new.drop(columns=['isFraud'])\n"", 'train_transaction_new.head()']"
"SETUP ASSIGN = PCA(svd_solver='randomized', random_state=42)",0,"['#Improting the PCA module\n', 'from sklearn.decomposition import PCA\n', ""pca = PCA(svd_solver='randomized', random_state=42)""]"
"CHECKPOINT ASSIGN=history.history.ASSIGN() print(ASSIGN) def show_train_history(hisData,train,test): plt.plot(hisData.history[train]) plt.plot(hisData.history[test]) plt.title('Training History') plt.ylabel(train) plt.xlabel('Epoch') plt.legend(['train', 'test'], loc='upper left') plt.show() show_train_history(history, 'loss', 'val_loss') show_train_history(history, 'accuracy', 'val_accuracy')",1,"['# Show Train History\n', 'keys=history.history.keys()\n', 'print(keys)\n', '\n', 'def show_train_history(hisData,train,test): \n', '    plt.plot(hisData.history[train])\n', '    plt.plot(hisData.history[test])\n', ""    plt.title('Training History')\n"", '    plt.ylabel(train)\n', ""    plt.xlabel('Epoch')\n"", ""    plt.legend(['train', 'test'], loc='upper left')\n"", '    plt.show()\n', '\n', ""show_train_history(history, 'loss', 'val_loss')\n"", ""show_train_history(history, 'accuracy', 'val_accuracy')""]"
ASSIGN = pd.read_csv('path') ASSIGN = pd.read_csv('path'),0,"[""train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n"", ""test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')""]"
SETUP,0,['from sklearn.metrics import confusion_matrix']
"plt.figure(3,figsize=(90,90)) ASSIGN = g.ASSIGN() ASSIGN = [g[u][v]['color'] for u,v in edges] nx.draw(g,node_color = color_map, edge_color = ASSIGN, with_labels = True) plt.show()",1,"['# Plot the graph\n', 'plt.figure(3,figsize=(90,90))  \n', 'edges = g.edges()\n', ""colors = [g[u][v]['color'] for u,v in edges]\n"", 'nx.draw(g,node_color = color_map, edge_color = colors, with_labels = True)\n', 'plt.show()']"
SETUP,0,"['import os\n', 'import re\n', 'from glob import glob\n', 'from tqdm import tqdm\n', 'import numpy as np\n', 'import pandas as pd\n', 'import ast\n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline']"
"SETUP CHECKPOINT estimator.append(('MNB',MultinomialNB())) ASSIGN = cross_val_score(MNB,x_train,x_test,ASSIGN=10) ASSIGN = cv.mean() accuracy.append(ASSIGN) print(ASSIGN) print(ASSIGN.mean())",0,"['MNB = MultinomialNB()\n', ""estimator.append(('MNB',MultinomialNB()))\n"", 'cv = cross_val_score(MNB,x_train,x_test,cv=10)\n', 'accuracy6 = cv.mean()\n', 'accuracy.append(accuracy6)\n', 'print(cv)\n', 'print(cv.mean())']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['import numpy as np \n', 'import pandas as pd\n', 'from nltk.tokenize import word_tokenize\n', 'from sklearn.feature_extraction.text import TfidfVectorizer\n', 'from nltk.corpus import stopwords\n', 'from sklearn.pipeline import Pipeline\n', 'from sklearn.linear_model import SGDClassifier\n', 'from sklearn.model_selection import cross_val_predict\n', 'from sklearn.model_selection import StratifiedKFold\n', 'from sklearn.metrics import roc_auc_score, classification_report, f1_score\n', 'from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))']"
"ASSIGN = tree_depth[tree_test_acc.index(max(tree_test_acc))] print (""max depth: "", ASSIGN) print (""best test accuracy: %.5f""% max(tree_test_acc))",0,"['\n', 'best_depth = tree_depth[tree_test_acc.index(max(tree_test_acc))]\n', 'print (""max depth: "", best_depth)\n', 'print (""best test accuracy: %.5f""% max(tree_test_acc))']"
len(train.id),0,['len(train.id)']
"def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow): ASSIGN = df.ASSIGN() ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]] nRow, nCol = ASSIGN.shape ASSIGN = list(df) ASSIGN = (nCol + nGraphPerRow - 1) path plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k') for i in range(min(nCol, nGraphShown)): plt.subplot(ASSIGN, nGraphPerRow, i + 1) ASSIGN = df.iloc[:, i] if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)): ASSIGN = columnDf.value_counts() ASSIGN.plot.bar() else: ASSIGN.hist() plt.ylabel('counts') plt.xticks(rotation = 90) plt.title(f'{ASSIGN[i]} (column {i})') plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0) plt.show()",1,"['# Distribution graphs (histogram/bar graph) of column data\n', 'def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n', '    nunique = df.nunique()\n', '    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n', '    nRow, nCol = df.shape\n', '    columnNames = list(df)\n', '    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n', ""    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n"", '    for i in range(min(nCol, nGraphShown)):\n', '        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n', '        columnDf = df.iloc[:, i]\n', '        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n', '            valueCounts = columnDf.value_counts()\n', '            valueCounts.plot.bar()\n', '        else:\n', '            columnDf.hist()\n', ""        plt.ylabel('counts')\n"", '        plt.xticks(rotation = 90)\n', ""        plt.title(f'{columnNames[i]} (column {i})')\n"", '    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n', '    plt.show()\n']"
"ASSIGN = {} for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']: ASSIGN = ['nunique'] ASSIGN['purchase_amount'] = ['sum','max','min','mean','var'] ASSIGN = ['sum','max','min','mean','var'] ASSIGN['purchase_date'] = ['max','min'] ASSIGN['month_lag'] = ['max','min','mean','var'] ASSIGN['month_diff'] = ['mean'] ASSIGN['authorized_flag'] = ['sum', 'mean'] ASSIGN = ['sum', 'mean'] ASSIGN['category_1'] = ['sum', 'mean'] ASSIGN['card_id'] = ['size'] for col in ['category_2','category_3']: df_hist_trans[col+'_mean'] = df_hist_trans.groupby([col])['purchase_amount'].transform('mean') ASSIGN[col+'_mean'] = ['mean'] ASSIGN = get_new_columns('hist',aggs) ASSIGN = df_hist_trans.groupby('card_id').agg(aggs) ASSIGN.columns = ASSIGN ASSIGN.reset_index(drop=False,inplace=True) ASSIGN['hist_purchase_date_diff'] = (ASSIGN['hist_purchase_date_max'] - ASSIGN['hist_purchase_date_min']).dt.days ASSIGN['hist_purchase_date_average'] = ASSIGN['hist_purchase_date_diff']path['hist_card_id_size'] ASSIGN['hist_purchase_date_uptonow'] = (datetime.datetime.today() - ASSIGN['hist_purchase_date_max']).dt.days ASSIGN = ASSIGN.merge(df_hist_trans_group,on='card_id',how='left') ASSIGN = ASSIGN.merge(df_hist_trans_group,on='card_id',how='left') del ASSIGN;gc.collect()",0,"['aggs = {}\n', ""for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n"", ""    aggs[col] = ['nunique']\n"", '\n', ""aggs['purchase_amount'] = ['sum','max','min','mean','var']\n"", ""aggs['installments'] = ['sum','max','min','mean','var']\n"", ""aggs['purchase_date'] = ['max','min']\n"", ""aggs['month_lag'] = ['max','min','mean','var']\n"", ""aggs['month_diff'] = ['mean']\n"", ""aggs['authorized_flag'] = ['sum', 'mean']\n"", ""aggs['weekend'] = ['sum', 'mean']\n"", ""aggs['category_1'] = ['sum', 'mean']\n"", ""aggs['card_id'] = ['size']\n"", '\n', ""for col in ['category_2','category_3']:\n"", ""    df_hist_trans[col+'_mean'] = df_hist_trans.groupby([col])['purchase_amount'].transform('mean')\n"", ""    aggs[col+'_mean'] = ['mean']    \n"", '\n', ""new_columns = get_new_columns('hist',aggs)\n"", ""df_hist_trans_group = df_hist_trans.groupby('card_id').agg(aggs)\n"", 'df_hist_trans_group.columns = new_columns\n', 'df_hist_trans_group.reset_index(drop=False,inplace=True)\n', ""df_hist_trans_group['hist_purchase_date_diff'] = (df_hist_trans_group['hist_purchase_date_max'] - df_hist_trans_group['hist_purchase_date_min']).dt.days\n"", ""df_hist_trans_group['hist_purchase_date_average'] = df_hist_trans_group['hist_purchase_date_diff']/df_hist_trans_group['hist_card_id_size']\n"", ""df_hist_trans_group['hist_purchase_date_uptonow'] = (datetime.datetime.today() - df_hist_trans_group['hist_purchase_date_max']).dt.days\n"", ""df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n"", ""df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n"", 'del df_hist_trans_group;gc.collect()']"
doc(DatasetFormatter().from_toplosses),0,['doc(DatasetFormatter().from_toplosses)']
"def show_samples(samples): for sample in samples: ASSIGN = plt.subplots(figsize=(18, 16)) ASSIGN = os.path.join(DATASET_DIR, 'train_images', '{}.{}'.format(sample, 'jpg')) ASSIGN = cv2.imread(img_path, 1) ASSIGN = cv2.cvtColor(ASSIGN, cv2.COLOR_BGR2RGB) ASSIGN = os.path.join(DATASET_DIR, 'train_masks', '{}.{}'.format(sample, 'jpg')) ASSIGN = cv2.imread(mask_path, 0) ASSIGN = [] ASSIGN = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE) for contour in contours: ASSIGN = Polygon(contour.reshape(-1, 2), closed=True, linewidth=2, edgecolor='r', facecolor='r', fill=True) ASSIGN.append(ASSIGN) ASSIGN = PatchCollection(patches, match_original=True, cmap=matplotlib.cm.jet, alpha=0.3) ax.imshow(imgpath) ax.set_title(sample) ax.add_collection(ASSIGN) ax.set_xticklabels([]) ax.set_yticklabels([]) plt.show()",1,"['def show_samples(samples):\n', '    for sample in samples:\n', '        fig, ax = plt.subplots(figsize=(18, 16))\n', '        \n', '        # Get image\n', ""        img_path = os.path.join(DATASET_DIR, 'train_images', '{}.{}'.format(sample, 'jpg'))\n"", '        img = cv2.imread(img_path, 1)\n', '        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n', '\n', '        # Get corresponding mask\n', ""        mask_path = os.path.join(DATASET_DIR, 'train_masks', '{}.{}'.format(sample, 'jpg'))\n"", '        mask = cv2.imread(mask_path, 0)\n', '\n', '        patches = []\n', '        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n', '        for contour in contours:\n', ""            poly_patch = Polygon(contour.reshape(-1, 2), closed=True, linewidth=2, edgecolor='r', facecolor='r', fill=True)\n"", '            patches.append(poly_patch)\n', '        p = PatchCollection(patches, match_original=True, cmap=matplotlib.cm.jet, alpha=0.3)\n', '\n', '        ax.imshow(img/255)\n', '        ax.set_title(sample)\n', '        ax.add_collection(p)\n', '        ax.set_xticklabels([])\n', '        ax.set_yticklabels([])\n', '        plt.show()']"
SETUP,0,"['# data analysis and wrangling\n', 'import pandas as pd\n', 'import numpy as np\n', 'import random as rnd\n', '# machine learning\n', 'from sklearn import tree\n', 'from sklearn.linear_model import LogisticRegression\n', 'from sklearn.svm import SVC\n', 'from sklearn.neighbors import KNeighborsClassifier\n', 'from sklearn.naive_bayes import GaussianNB\n', 'from sklearn import metrics\n', '# visualization\n', 'import seaborn as sns\n', 'import matplotlib.pyplot as plt']"
"ASSIGN=tree.predict(data_tree_for_test[['MSSubClass','MSZoning_new','Neighborhood_new','OverallQual','OverallCond']]) ASSIGN=pd.DataFrame({'Id':data_tree_for_test.Id,'SalePrice':predict_test}) ASSIGN.head()",0,"[""predict_test=tree.predict(data_tree_for_test[['MSSubClass','MSZoning_new','Neighborhood_new','OverallQual','OverallCond']])\n"", ""submit=pd.DataFrame({'Id':data_tree_for_test.Id,'SalePrice':predict_test})\n"", 'submit.head()']"
SETUP,0,['import xgboost as xgb']
"np.random.seed(42) ASSIGN = ImageDataBunch.from_csv(path, folder=""."", valid_pct=0.2, csv_labels='cleaned.csv', ASSIGN=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)",0,"['np.random.seed(42)\n', 'data = ImageDataBunch.from_csv(path, folder=""."", valid_pct=0.2, csv_labels=\'cleaned.csv\',\n', '        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)']"
smote_df.Attrition.value_counts(),0,['smote_df.Attrition.value_counts()']
"plt.figure(figsize=(50,60)) ASSIGN=Basemap(width=120000,height=900000,projection=""lcc"",resolution=""l"",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77) ASSIGN.drawcountries() ASSIGN.drawmapboundary(color=' ASSIGN.drawcoastlines() ASSIGN=np.array(df_plot_top[""lng""]) ASSIGN=np.array(df_plot_top[""ASSIGN""]) ASSIGN=np.array(df_plot_top[""COUNT""]) ASSIGN=np.array(df_plot_top[""CITY""]) ASSIGN=map(lg,lat) ASSIGN=df_plot_top[""COUNT""].apply(lambda x: int(x)path) plt.scatter(ASSIGN,s=ASSIGN,marker=""o"",c=ASSIGN) for a,b ,c,d in zip(ASSIGN,ASSIGN,ASSIGN): plt.text(a,b,c,fontsize=30,color=""r"") plt.title(""TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO"",fontsize=30,color='RED')",1,"['#lets plot with the city names inside the map corresponding to the cities exact co-ordinates which we received from google api ,here marker color will be different as per marker size\n', '#plt.subplots(figsize=(20,50))\n', 'plt.figure(figsize=(50,60))\n', 'map=Basemap(width=120000,height=900000,projection=""lcc"",resolution=""l"",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n', 'map.drawcountries()\n', ""map.drawmapboundary(color='#f2f2f2')\n"", '\n', 'map.drawcoastlines()\n', '\n', '\n', '\n', 'lg=np.array(df_plot_top[""lng""])\n', 'lat=np.array(df_plot_top[""lat""])\n', 'pt=np.array(df_plot_top[""COUNT""])\n', 'city_name=np.array(df_plot_top[""CITY""])\n', '\n', 'x,y=map(lg,lat)\n', '\n', '#using lambda function to create different sizes of marker as per thecount \n', '\n', 'p_s=df_plot_top[""COUNT""].apply(lambda x: int(x)/2)\n', '\n', '#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is different.\n', 'plt.scatter(x,y,s=p_s,marker=""o"",c=p_s)\n', '\n', 'for a,b ,c,d in zip(x,y,city_name,pt):\n', '    #plt.text takes x position , y position ,text ,font size and color as arguments\n', '    plt.text(a,b,c,fontsize=30,color=""r"")\n', '   \n', '    \n', '    \n', 'plt.title(""TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO"",fontsize=30,color=\'RED\')']"
ASSIGN = build_model(),0,['model = build_model()']
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") print() print(ASSIGN.shape) print() print(type(ASSIGN)) print() print(ASSIGN.head(3))",0,"['#4 Write a python program to load the iris data framefrom a given csv file into a dataframe and print the shape of the data, type of the data and first 3 rows.\n', 'import pandas as pd\n', 'data = pd.read_csv(""../input/iris-dataset/iris.data.csv"")\n', 'print(""Shape of the data:"")\n', 'print(data.shape)\n', 'print(""\\nData Type:"")\n', 'print(type(data))\n', 'print(""\\nFirst 3 rows:"")\n', 'print(data.head(3))']"
CHECKPOINT ASSIGN=pd_data.groupby('Location') print(ASSIGN.size().sort_values(ascending=False)),0,"[""groupbyLocation=pd_data.groupby('Location')\n"", 'print(groupbyLocation.size().sort_values(ascending=False))\n']"
"ASSIGN = proba[:, 1] ASSIGN = roc_curve(test_Y, pos_probs) plt.plot([0, 1], [0, 1], linestyle='--', label='No Skill') plt.plot(fpr, tpr, marker='.', label='Logistic') plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.legend() plt.show()",1,"['# retrieve just the probabilities for the positive class\n', 'pos_probs = proba[:, 1]\n', '\n', '# calculate roc curve for model\n', 'fpr, tpr, thresholds = roc_curve(test_Y, pos_probs)\n', '\n', '# plot no skill roc curve\n', ""plt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n"", '# plot model roc curve\n', ""plt.plot(fpr, tpr, marker='.', label='Logistic')\n"", '# axis labels\n', ""plt.xlabel('False Positive Rate')\n"", ""plt.ylabel('True Positive Rate')\n"", '# show the legend\n', 'plt.legend()\n', '# show the plot\n', 'plt.show()']"
"sns.catplot(x='Year', y='Kidnapping and Abduction', data=total ,height = 5, aspect = 4,kind = 'bar')",1,"[""sns.catplot(x='Year', y='Kidnapping and Abduction', data=total ,height = 5, aspect = 4,kind = 'bar')""]"
"class ImageData(Dataset): def __init__(self,is_train=True): self.is_train = is_train self.transform = transforms.Compose([transforms.ToTensor(),]) self.train_index = int(valid_ratio * len(img_list)) self.crop = transforms.CenterCrop((218,178)) def __len__(self): if self.is_train: return self.train_index else: return len(img_list) - self.train_index -1 def __getitem__(self, index): if not self.is_train: ASSIGN = self.train_index + ASSIGN ASSIGN = mpimg.imread(img_dir+img_list[index]) ASSIGN = self.crop(TF.to_pil_image(ASSIGN)) ASSIGN = self.transform(ASSIGN) ASSIGN = (ASSIGN-0.5) path return img",0,"['class ImageData(Dataset):\n', '    def __init__(self,is_train=True):\n', '        self.is_train = is_train\n', '        self.transform = transforms.Compose([transforms.ToTensor(),])\n', '        self.train_index = int(valid_ratio * len(img_list))\n', '        self.crop = transforms.CenterCrop((218,178))\n', '    def __len__(self):\n', '        if self.is_train:\n', '            return self.train_index\n', '        else:\n', '            return len(img_list) - self.train_index -1\n', '    def __getitem__(self, index):\n', '        if not self.is_train:\n', '            index = self.train_index + index\n', '#         print(""hey  ""*4 + str(index))\n', '        img = mpimg.imread(img_dir+img_list[index])\n', '        img = self.crop(TF.to_pil_image(img))\n', '        img = self.transform(img)\n', '        img = (img-0.5) /0.5\n', '#         img = (img - 255.0) / 255.0\n', '        return img']"
"SETUP ASSIGN=reg.predict(x_test) r2_score(y_test,ASSIGN)",0,"['y_pred=reg.predict(x_test)\n', 'from sklearn.metrics import r2_score\n', 'r2_score(y_test,y_pred)']"
SETUP,0,"['import torch\n', 'import torch.nn as nn\n', 'import torch.nn.functional as F\n', 'import torch.optim as optim\n', 'from torch.utils.data import Dataset, DataLoader\n', 'from torchvision import transforms\n', 'from torchvision.utils import make_grid\n', 'import torchvision.utils as vutils\n', 'import matplotlib.animation as animation\n', 'from IPython.display import HTML\n', '\n', 'import matplotlib.pyplot as plt\n', 'from PIL import Image\n', 'import numpy as np\n', 'import pandas as pd\n', 'import copy\n', 'import time\n', 'import networkx as nx\n', '\n', 'from torch.autograd import Variable\n']"
del train_identity del test_identity train_transaction.head(),0,"['del train_identity\n', 'del test_identity\n', 'train_transaction.head()']"
del temperature['ConfirmedCases'] del temperature['Fatalities'] del temperature['country+province'] del temperature['day_from_jan_first'],0,"['#delete useless features \n', ""del temperature['ConfirmedCases']\n"", ""del temperature['Fatalities']\n"", ""del temperature['country+province']\n"", ""del temperature['day_from_jan_first']""]"
"plt.bar(damage_x,damage_y) plt.title(""Parts Damaged in the Aircraft"") plt.xticks(rotation='vertical')",1,"['plt.bar(damage_x,damage_y)\n', 'plt.title(""Parts Damaged in the Aircraft"")\n', ""plt.xticks(rotation='vertical')""]"
"ASSIGN = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', max_features=25000) ASSIGN.fit(trainDF['review']) ASSIGN = tfidf_vect.transform(train_x) ASSIGN = tfidf_vect.transform(val_x)",0,"['# word level tf-idf\n', ""tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=25000)\n"", ""tfidf_vect.fit(trainDF['review'])\n"", 'xtrain_tfidf =  tfidf_vect.transform(train_x)\n', 'xval_tfidf = tfidf_vect.transform(val_x)']"
8 - 3 * 2 - (1 + 1),0,['8 - 3 * 2 - (1 + 1)']
"CHECKPOINT ASSIGN = pd.read_csv('path') print(ASSIGN.iloc[0, :])",0,"[""csvData = pd.read_csv('/kaggle/input/urbansound8k/UrbanSound8K.csv')\n"", 'print(csvData.iloc[0, :])']"
SETUP,0,"['%matplotlib inline\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import matplotlib.pyplot as plt # drawing graph\n', 'import warnings; warnings.filterwarnings(""ignore"") \n', ""import os; os.environ['OMP_NUM_THREADS'] = '4' # speed up using 4 cpu\n"", 'from fastai.tabular import *\n', 'from sklearn.metrics import roc_auc_score\n', 'from imblearn.over_sampling import SMOTE']"
"ASSIGN=[] ASSIGN=test.id[:10000] for file in ASSIGN: ASSIGN=cv2.imread(""..path""+file+'.jpg') ASSIGN=cv2.resize(image,(32,32)) ASSIGN.append(ASSIGN) ASSIGN=np.array(ASSIGN)",0,"['img_test=[]\n', 'filename_test=test.id[:10000]\n', 'for file in filename_test:\n', '    image=cv2.imread(""../input/iwildcam-2019-fgvc6/test_images/""+file+\'.jpg\')\n', '    res=cv2.resize(image,(32,32))\n', '    img_test.append(res)\n', 'img_test=np.array(img_test)']"
os.listdir('..path'),0,"[""os.listdir('../input/google-play-store-apps')""]"
"ASSIGN = 32 ASSIGN = model.fit_generator( datagen_train.flow(x_train, y_train, ASSIGN=32), ASSIGN= 50, ASSIGN=datagen_test.flow(x_test, y_test, batch_size=32), ASSIGN = 1, ASSIGN= int(len(x_train)path), ASSIGN= int(len(x_test)path) )",0,"['## finetuning\n', '\n', 'batch_size = 32\n', '\n', 'history = model.fit_generator(\n', '    datagen_train.flow(x_train, y_train, batch_size=32),\n', '    epochs= 50,\n', '    validation_data=datagen_test.flow(x_test, y_test, batch_size=32),\n', '    verbose = 1,\n', '    #callbacks=callbacks,\n', '    steps_per_epoch=  int(len(x_train)//batch_size),\n', '    validation_steps= int(len(x_test)// batch_size)\n', ')']"
ASSIGN = [x for x in finalData if x[:1] in significant_numbers['First Digit Significant Values'].astype(str)] ASSIGN = [x for x in finalData if x[1:2] in significant_numbers['Second Digit Significant Values'].astype(str)] ASSIGN = [x for x in finalData if x[2:3] in significant_numbers['Third Digit Significant Values'].astype(str)],0,"['#these lines below actually are finding the numbers in our entire dataset where the digit at some given position\n', '#matches the number and the position that we are looking for\n', ""first_numbers = [x for x in finalData if x[:1] in significant_numbers['First Digit Significant Values'].astype(str)]\n"", ""second_numbers = [x for x in finalData if x[1:2] in significant_numbers['Second Digit Significant Values'].astype(str)]\n"", ""third_numbers = [x for x in finalData if x[2:3] in significant_numbers['Third Digit Significant Values'].astype(str)]""]"
for i in range(len(population)): if np.isnan(population.Migrants[i]): population.Migrants[i] = np.nanmedian(population.Migrants) if population.MedAge[i] == 'N.A.': population.MedAge[i] = 19 if population.UrbanPopRate[i] == 'N.A.': population.UrbanPopRate[i] = '57%',0,"['for i in range(len(population)):\n', '    if np.isnan(population.Migrants[i]):\n', '        population.Migrants[i] = np.nanmedian(population.Migrants)\n', ""    if population.MedAge[i] == 'N.A.':\n"", '        population.MedAge[i] = 19\n', ""    if population.UrbanPopRate[i] == 'N.A.':\n"", ""        population.UrbanPopRate[i] = '57%'""]"
ASSIGN = ASSIGN.fillna(ASSIGN.median()),0,"['final[""Fare""] = final[""Fare""].fillna(final[""Fare""].median())\n']"
"ASSIGN = optim.SGD(model.parameters(), lr=0.001) ASSIGN = MultiStepLR(optimizer, milestones=[2, 4])",0,"['optimizer = optim.SGD(model.parameters(), lr=0.001)\n', 'scheduler = MultiStepLR(optimizer, milestones=[2, 4])']"
SETUP,0,"['import numpy as np\n', 'import pandas as pd \n', 'import matplotlib.pyplot as plt\n', 'import os\n', 'import seaborn as sns\n', 'import math\n']"
ASSIGN = set(inter_one_two) ASSIGN = set(inter_two_three) ASSIGN = in_second - in_first ASSIGN = inter_one_two + list(in_second_but_not_in_first),0,"['in_first = set(inter_one_two)\n', 'in_second = set(inter_two_three)\n', '\n', 'in_second_but_not_in_first = in_second - in_first\n', '\n', 'result = inter_one_two + list(in_second_but_not_in_first)']"
zomato.info(),0,['zomato.info()']
"plt.figure(figsize=(15,15)) for i in range(9): plt.subplot(3,3,i+1) plt.imshow(img[i])",1,"['plt.figure(figsize=(15,15))\n', 'for i in range(9):\n', '    plt.subplot(3,3,i+1)\n', '    plt.imshow(img[i])']"
"match_bets_df.dropna(subset= list(match_bets_df.columns).remove('id'), inplace= True)",0,"[""match_bets_df.dropna(subset= list(match_bets_df.columns).remove('id'), inplace= True)""]"
"ASSIGN = LogisticRegression(solver=""lbfgs"").fit(train_embeddings, train_Y)",0,"['clf = LogisticRegression(solver=""lbfgs"").fit(train_embeddings, train_Y)']"
ASSIGN = X_train.min() ASSIGN = (X_train - min_train).max() ASSIGN = (X_train - min_train)path ASSIGN.describe(),0,"['min_train = X_train.min()\n', 'range_train = (X_train - min_train).max()\n', 'X_train_scaled = (X_train - min_train)/range_train\n', 'X_train_scaled.describe()']"
"train_df['bboxs'] = train_df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=','))",0,"[""train_df['bboxs'] = train_df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=','))""]"
"ASSIGN = pd.concat([temp_submit_df,original_train],axis = 0) ASSIGN = ASSIGN.sort_values(by=['Province','Date']) ASSIGN = pd.DataFrame(new_test.ForecastId,columns=['ForecastId']) ASSIGN['Confirmed'] = ASSIGN.ConfirmedCases.values ASSIGN['Fatalities'] = ASSIGN.Fatalities.values",0,"['final = pd.concat([temp_submit_df,original_train],axis = 0)\n', ""final = final.sort_values(by=['Province','Date'])\n"", ""final_submit = pd.DataFrame(new_test.ForecastId,columns=['ForecastId'])\n"", ""final_submit['Confirmed'] = final.ConfirmedCases.values\n"", ""final_submit['Fatalities'] = final.Fatalities.values""]"
ASSIGN = pd.read_csv('path') ASSIGN = pd.read_csv('path'),0,"[""original_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')\n"", ""new_test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')""]"
"df_final[""CITY""].unique()",0,"['#lets count how many unique cities are there \n', '\n', 'df_final[""CITY""].unique()']"
df['image_path'] = '..path' + df['id'].astype(str) + '.jpg',0,"[""df['image_path'] = '../input/train/' + df['id'].astype(str) + '.jpg'""]"
"ASSIGN = final.copy() ASSIGN = pd.concat([df['Survived'],result['Survived']],axis = 0) ASSIGN = pd.concat([ASSIGN,sur],axis = 1)",0,"['correlation = final.copy()\n', ""sur = pd.concat([df['Survived'],result['Survived']],axis = 0)\n"", 'correlation = pd.concat([correlation,sur],axis = 1)']"
data.room_type.unique(),0,['data.room_type.unique()']
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") print(ASSIGN.info())",0,"['#2 Write a python program to get the number of observations,missing values and nan values.\n', 'import pandas as pd\n', 'iris = pd.read_csv(""../input/daily-sun-spot-data-1818-to-2019/sunspot_data.csv"")\n', 'print(iris.info())']"
ASSIGN = predict(test_loader),0,['predictions = predict(test_loader)']
"ASSIGN=plt.subplots(10,2,figsize=(30,55)) for i in range(10): data_export.groupby(['HSCode','year'])['value'].agg(sum).unstack(['HSCode']).iloc[:,i*10:(i+1)*10].plot(ax=ax[i,0]) ax[i,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1)) ax[i,0].set_title(""Export HSCode by year"") ax[i,0].set_ylabel('values') data_import.groupby(['HSCode','year'])['value'].agg(sum).unstack(['HSCode']).iloc[:,i*10:(i+1)*10].plot(ax=ax[i,1]) ax[i,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1)) ax[i,1].set_title(""Import HSCode by year"") ax[i,1].set_ylabel('values')",1,"['fig,ax=plt.subplots(10,2,figsize=(30,55))\n', 'for i in range(10):\n', ""    data_export.groupby(['HSCode','year'])['value'].agg(sum).unstack(['HSCode']).iloc[:,i*10:(i+1)*10].plot(ax=ax[i,0])\n"", '    ax[i,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n', '    ax[i,0].set_title(""Export HSCode by year"")\n', ""    ax[i,0].set_ylabel('values')\n"", '\n', ""    data_import.groupby(['HSCode','year'])['value'].agg(sum).unstack(['HSCode']).iloc[:,i*10:(i+1)*10].plot(ax=ax[i,1])\n"", '    ax[i,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n', '    ax[i,1].set_title(""Import HSCode by year"")\n', ""    ax[i,1].set_ylabel('values')""]"
SETUP,0,"['import matplotlib.pyplot as plt\n', 'import pandas as pd\n', 'import numpy as np']"
SETUP ASSIGN = StandardScaler() ASSIGN = sc.fit_transform(ASSIGN) ASSIGN = sc.fit_transform(ASSIGN),0,"['from sklearn.preprocessing import StandardScaler\n', 'sc = StandardScaler()\n', '\n', 'X_train = sc.fit_transform(X_train)\n', 'X_test = sc.fit_transform(X_test)']"
"ASSIGN= total.groupby(['Year'])['Murder'].sum().reset_index().sort_values(by='Murder',ascending=False) ASSIGN.head(15).style.background_gradient(cmap='Reds')",0,"[""s= total.groupby(['Year'])['Murder'].sum().reset_index().sort_values(by='Murder',ascending=False)\n"", ""s.head(15).style.background_gradient(cmap='Reds')""]"
"plt.figure(figsize=(30,30)) sns.heatmap(correlation.corr(), annot=True, linewidth=0.5, cmap='coolwarm')",1,"['plt.figure(figsize=(30,30))\n', ""sns.heatmap(correlation.corr(), annot=True, linewidth=0.5, cmap='coolwarm')""]"
ASSIGN=pd.DataFrame(df_np),0,"['#creating final dataframe from the numpy array\n', 'df_final=pd.DataFrame(df_np)']"
"CHECKPOINT def plotCorrelationMatrix(df, graphWidth): ASSIGN = df.dataframeName ASSIGN = ASSIGN.dropna('columns') ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]] if ASSIGN.shape[1] < 2: print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2') return ASSIGN = df.ASSIGN() plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k') ASSIGN = plt.matshow(corr, fignum = 1) plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90) plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns) plt.gca().xaxis.tick_bottom() plt.colorbar(ASSIGN) plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15) plt.show()",1,"['# Correlation matrix\n', 'def plotCorrelationMatrix(df, graphWidth):\n', '    filename = df.dataframeName\n', ""    df = df.dropna('columns') # drop columns with NaN\n"", '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n', '    if df.shape[1] < 2:\n', ""        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n"", '        return\n', '    corr = df.corr()\n', ""    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n"", '    corrMat = plt.matshow(corr, fignum = 1)\n', '    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n', '    plt.yticks(range(len(corr.columns)), corr.columns)\n', '    plt.gca().xaxis.tick_bottom()\n', '    plt.colorbar(corrMat)\n', ""    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n"", '    plt.show()\n']"
train.head(),0,['train.head()']
"r2_score(y_test,y_predict)",0,"['r2_score(y_test,y_predict)']"
"SETUP ASSIGN = LinearRegression() ASSIGN.fit(train_features, train_labels)",0,"['from sklearn.linear_model import LinearRegression\n', '\n', 'clf = LinearRegression()\n', 'clf.fit(train_features, train_labels)']"
"plt.figure(figsize = (7,6)) ASSIGN = plt.bar(top10_deaths.keys(), top10_deaths.values()) plt.xlabel('Country') plt.ylabel('Count') plt.title('Highest Death Cases in 10 countries') plt.xticks(list(top10_deaths.keys()), rotation = 90) for bar in ASSIGN: ASSIGN = bar.get_height() plt.text(bar.get_x(), ASSIGN + .005, ASSIGN, rotation = 5) plt.show()",1,"['plt.figure(figsize = (7,6))\n', 'bars = plt.bar(top10_deaths.keys(), top10_deaths.values())\n', ""plt.xlabel('Country')\n"", ""plt.ylabel('Count')\n"", ""plt.title('Highest Death Cases in 10 countries')\n"", 'plt.xticks(list(top10_deaths.keys()), rotation = 90)\n', 'for bar in bars:\n', '    yval = bar.get_height()\n', '    plt.text(bar.get_x(), yval + .005, yval, rotation = 5)\n', 'plt.show()']"
CHECKPOINT zomato_en.columns,0,['zomato_en.columns']
"CHECKPOINT ASSIGN=np.array(lst) ASSIGN=np.array(y) np.savez(""CAT_DOG_X_train"",ASSIGN) np.savez(""CAT_DOG_Y_train"",ASSIGN) print(ASSIGN.shape) print(ASSIGN.shape)",0,"['X_train=np.array(lst)\n', 'Y_train=np.array(y)\n', 'np.savez(""CAT_DOG_X_train"",X_train)\n', 'np.savez(""CAT_DOG_Y_train"",Y_train)\n', 'print(X_train.shape)\n', 'print(Y_train.shape)\n']"
"ASSIGN = df.columns ASSIGN=[] for i in range(0,len(ASSIGN)): if ASSIGN[i]!='Attrition': ASSIGN.append(ASSIGN[i])",0,"['# get train data\n', 'col = df.columns\n', 'cont=[]\n', 'for i in range(0,len(col)):\n', ""    if col[i]!='Attrition':\n"", '        cont.append(col[i])']"
ASSIGN = pd.read_csv('path') ASSIGN = pd.read_csv('path'),0,"[""trainset = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n"", ""testset = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')""]"
"df_merchants['max_merchant_category_id']=df_merchants['merchant_category_id'] ASSIGN={} for col in ['avg_sales_lag3','avg_purchases_lag3','active_months_lag3','avg_sales_lag6','avg_purchases_lag6','active_months_lag6','avg_sales_lag12','avg_purchases_lag12','active_months_lag12','numerical_1','numerical_2']: SLICE= ['mean'] ASSIGN= get_new_columns('max_merchants',aggs) ASSIGN = df_merchants.groupby('max_merchant_category_id').agg(aggs) ASSIGN.columns = ASSIGN ASSIGN.reset_index(drop=False,inplace=True) ASSIGN=ASSIGN.merge(df_merchants_group,on='max_merchant_category_id',how='left') ASSIGN=ASSIGN.merge(df_merchants_group.reset_index(),on='max_merchant_category_id',how='left') ASSIGN.head()",0,"[""df_merchants['max_merchant_category_id']=df_merchants['merchant_category_id']\n"", ""#df_test['max_merchant_category_id']=df_test['merchant_category_id']\n"", '\n', 'aggs={}\n', ""for col in ['avg_sales_lag3','avg_purchases_lag3','active_months_lag3','avg_sales_lag6','avg_purchases_lag6','active_months_lag6','avg_sales_lag12','avg_purchases_lag12','active_months_lag12','numerical_1','numerical_2']:\n"", ""    aggs[col]= ['mean']\n"", '    \n', ""new_columns= get_new_columns('max_merchants',aggs)\n"", ""df_merchants_group = df_merchants.groupby('max_merchant_category_id').agg(aggs)\n"", 'df_merchants_group.columns = new_columns\n', 'df_merchants_group.reset_index(drop=False,inplace=True)\n', ""df_train=df_train.merge(df_merchants_group,on='max_merchant_category_id',how='left')\n"", ""df_test=df_test.merge(df_merchants_group.reset_index(),on='max_merchant_category_id',how='left')\n"", 'df_train.head()']"
"ASSIGN = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'} ASSIGN = ['gmail', 'net', 'edu'] for c in ['P_emaildomain', 'R_emaildomain']: train[c + '_bin'] = train[c].map(ASSIGN) test[c + '_bin'] = test[c].map(ASSIGN) train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1]) test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1]) train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in ASSIGN else 'us') test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in ASSIGN else 'us')",0,"[""emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n"", ""us_emails = ['gmail', 'net', 'edu']\n"", ""for c in ['P_emaildomain', 'R_emaildomain']:\n"", ""    train[c + '_bin'] = train[c].map(emails)\n"", ""    test[c + '_bin'] = test[c].map(emails)\n"", '    \n', ""    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n"", ""    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n"", '    \n', ""    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n"", ""    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')""]"
"SETUP CHECKPOINT ASSIGN = SVR(C=1, epsilon=0.1) ASSIGN.fit(x_t,Y_t) ASSIGN = ASSIGN.score(x_es,Y_es) print(ASSIGN*100,'%')",0,"['from sklearn.svm import SVR\n', 'model20 = SVR(C=1, epsilon=0.1)\n', 'model20.fit(x_t,Y_t)\n', '\n', 'model20 = model20.score(x_es,Y_es)\n', ""print(model20*100,'%')""]"
"ASSIGN = kickstarters_2017.pledged > 0 ASSIGN = kickstarters_2017.pledged.loc[index_of_positive_pledges] ASSIGN = stats.boxcox(positive_pledges)[0] ASSIGN=plt.subplots(1,2) sns.distplot(ASSIGN, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Normalized data"")",1,"['# Your turn! \n', '# We looked as the usd_pledged_real column. What about the ""pledged"" column? Does it have the same info?\n', '\n', 'index_of_positive_pledges = kickstarters_2017.pledged > 0\n', '\n', '# get only positive pledges (using their indexes)\n', 'positive_pledges = kickstarters_2017.pledged.loc[index_of_positive_pledges]\n', '\n', '# normalize the pledges (w/ Box-Cox)\n', 'normalized_pledges = stats.boxcox(positive_pledges)[0]\n', '\n', '# plot both together to compare\n', 'fig, ax=plt.subplots(1,2)\n', 'sns.distplot(positive_pledges, ax=ax[0])\n', 'ax[0].set_title(""Original Data"")\n', 'sns.distplot(normalized_pledges, ax=ax[1])\n', 'ax[1].set_title(""Normalized data"")']"
"sample_submission.to_csv(""submission.csv"", index=False)",0,"['sample_submission.to_csv(""submission.csv"", index=False)']"
type(country_df.name.iloc[0]),0,['type(country_df.name.iloc[0])']
"ASSIGN=df_final[""CITY""].value_counts().sort_values(ascending=True)",0,"['#creating pandas series to hold the citynames and corresponding count of restuarnats in ascending order\n', 'li2=df_final[""CITY""].value_counts().sort_values(ascending=True)']"
"SETUP ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"")",0,"['import pandas as pd\n', 'sample_submission = pd.read_csv(""../input/house-prices-advanced-regression-techniques/sample_submission.csv"")\n', 'test = pd.read_csv(""../input/house-prices-advanced-regression-techniques/test.csv"")\n', 'train = pd.read_csv(""../input/house-prices-advanced-regression-techniques/train.csv"")']"
"SETUP np.random.seed(1337) X_train,X_test,y_train,y_test=train_test_split(img,new_train['landmark_id'],test_size=0.2) ASSIGN=ASSIGN.astype(int) ASSIGN=ASSIGN.astype(int) ASSIGN=np.array(ASSIGN).reshape(-1,1) ASSIGN=np.array(ASSIGN).reshape(-1,1) ASSIGN=ASSIGN.reshape(-1,32,32,3)path ASSIGN=ASSIGN.reshape(-1,32,32,3)path ASSIGN=np_utils.to_categorical(ASSIGN,num_classes=500) ASSIGN=np_utils.to_categorical(ASSIGN,num_classes=500)",0,"['np.random.seed(1337)\n', 'from keras.utils import np_utils\n', 'from keras.models import Sequential\n', 'from keras.layers import Convolution2D,Dense,MaxPool2D,Activation,Dropout,Flatten\n', 'from keras.optimizers import Adam\n', 'from sklearn.model_selection import train_test_split\n', 'from keras.layers.normalization import BatchNormalization\n', ""X_train,X_test,y_train,y_test=train_test_split(img,new_train['landmark_id'],test_size=0.2)\n"", 'y_train=y_train.astype(int)\n', 'y_test=y_test.astype(int)\n', 'y_train=np.array(y_train).reshape(-1,1)\n', 'y_test=np.array(y_test).reshape(-1,1)\n', 'X_train=X_train.reshape(-1,32,32,3)/255 #Normalize\n', 'X_test=X_test.reshape(-1,32,32,3)/255\n', 'y_train=np_utils.to_categorical(y_train,num_classes=500)\n', 'y_test=np_utils.to_categorical(y_test,num_classes=500)#landmark_id is from 0 to 499']"
"ASSIGN = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270] ASSIGN = RandomForestRegressor() ASSIGN = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]} ASSIGN = GridSearchCV(rf, parameters,scoring='neg_mean_squared_error', cv=10) ASSIGN.fit(x_train, Y_train) ASSIGN.best_params_",0,"['n_estimators = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]\n', '\n', 'rf = RandomForestRegressor()\n', '\n', ""parameters = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}\n"", '\n', ""rfr = GridSearchCV(rf, parameters,scoring='neg_mean_squared_error', cv=10)\n"", '\n', 'rfr.fit(x_train, Y_train)\n', '\n', 'rfr.best_params_']"
"match_lineup_df.dropna(thresh=20, inplace= True)",0,"['match_lineup_df.dropna(thresh=20, inplace= True)']"
ASSIGN = 1 ASSIGN = None,0,"['controller_num_epochs = 1\n', 'controller_optimizer = None']"
"SETUP ASSIGN=[161,150,154,165,168,161,154,162,150,121,162,164,171,165,158,154,156,172,160,170,153,159,161,170,162,165,166,168,165,164,154,152,153,156,158,172,172,161,12,166,161,12,162,167,168,159,158,153,154,159] ASSIGN=[150,155,160,165,170] plt.hist(ASSIGN,ASSIGN,histtype='bar',rwidth=0.5,color='blue') plt.xlabel('Height range') plt.ylabel('No of persons') plt.title(""Heights Histogram"") plt.show()",1,"['from matplotlib import pyplot as plt\n', 'heights=[161,150,154,165,168,161,154,162,150,121,162,164,171,165,158,154,156,172,160,170,153,159,161,170,162,165,166,168,165,164,154,152,153,156,158,172,172,161,12,166,161,12,162,167,168,159,158,153,154,159]\n', 'bins=[150,155,160,165,170]\n', ""plt.hist(heights,bins,histtype='bar',rwidth=0.5,color='blue')\n"", ""plt.xlabel('Height range')\n"", ""plt.ylabel('No of persons')\n"", 'plt.title(""Heights Histogram"")\n', 'plt.show()']"
"ASSIGN = CountVectorizer(analyzer='word', token_pattern=r'\w{1,}') ASSIGN.fit(df['review']) ASSIGN = count_vect.transform(X_train) ASSIGN = count_vect.transform(X_val) ASSIGN = count_vect.transform(test_df['review'])",0,"['# create a count vectorizer object \n', ""count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n"", ""count_vect.fit(df['review'])\n"", '\n', '# transform the training and validation data using count vectorizer object\n', 'xtrain_count =  count_vect.transform(X_train)\n', 'xvalid_count =  count_vect.transform(X_val)\n', ""xtest_count = count_vect.transform(test_df['review'])""]"
"model.compile(optimizer='adam', ASSIGN='categorical_crossentropy', ASSIGN=['accuracy'])",0,"[""model.compile(optimizer='adam',\n"", ""              loss='categorical_crossentropy',\n"", ""              metrics=['accuracy'])""]"
SETUP,0,"['#generalization and overfitting\n', 'import numpy as np\n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline  ']"
"ASSIGN=pd.read_csv(""..path"") ASSIGN.head()",0,"['data_review=pd.read_csv(""../input/google-play-store-apps/googleplaystore_user_reviews.csv"")\n', 'data_review.head()']"
SETUP,0,"['import os\n', 'import json\n', '\n', 'import albumentations as albu\n', 'import cv2\n', 'import keras\n', 'from keras import backend as K\n', 'from keras.models import Model\n', 'from keras.layers import Input\n', 'from keras.layers.convolutional import Conv2D, Conv2DTranspose\n', 'from keras.layers.pooling import MaxPooling2D\n', 'from keras.layers.merge import concatenate\n', 'from keras.losses import binary_crossentropy\n', 'from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n', 'from skimage.exposure import adjust_gamma\n', 'import matplotlib.pyplot as plt\n', 'import numpy as np\n', 'import pandas as pd\n', 'from tqdm import tqdm\n', 'from sklearn.model_selection import train_test_split\n', 'from keras.layers import LeakyReLU\n', 'from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n', 'from keras.layers import Conv2D, Concatenate, MaxPooling2D\n', 'from keras.layers import UpSampling2D, Dropout, BatchNormalization\n', 'from keras import optimizers\n', 'from keras.legacy import interfaces\n', 'from keras.utils.generic_utils import get_custom_objects\n', '\n', 'from keras.engine.topology import Input\n', 'from keras.engine.training import Model\n', 'from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n', 'from keras.layers.core import Activation, SpatialDropout2D\n', 'from keras.layers.merge import concatenate\n', 'from keras.layers.normalization import BatchNormalization\n', 'from keras.layers.pooling import MaxPooling2D\n', 'from keras.layers import Input,Dropout,BatchNormalization,Activation,Add\n', 'from keras.regularizers import l2\n', 'from keras.layers.core import Dense, Lambda\n', 'from keras.layers.merge import concatenate, add\n', 'from keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, Permute\n', 'from keras.optimizers import SGD\n', 'from keras.preprocessing.image import ImageDataGenerator']"
"def plotScatterMatrix(df, plotSize, textSize): ASSIGN = ASSIGN.select_dtypes(include =[np.number]) ASSIGN = ASSIGN.dropna('columns') ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]] ASSIGN = list(df) if len(ASSIGN) > 10: ASSIGN = ASSIGN[:10] ASSIGN = ASSIGN[columnNames] ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde') ASSIGN = df.corr().values for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)): ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize) plt.suptitle('Scatter and Density Plot') plt.show()",1,"['# Scatter and density plots\n', 'def plotScatterMatrix(df, plotSize, textSize):\n', '    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n', '    # Remove rows and columns that would lead to df being singular\n', ""    df = df.dropna('columns')\n"", '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n', '    columnNames = list(df)\n', '    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n', '        columnNames = columnNames[:10]\n', '    df = df[columnNames]\n', ""    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n"", '    corrs = df.corr().values\n', '    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n', ""        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n"", ""    plt.suptitle('Scatter and Density Plot')\n"", '    plt.show()\n']"
data_2019.isnull().sum(),0,['data_2019.isnull().sum()']
"def top_3_accuracy(x,y): ASSIGN = top_k_categorical_accuracy(x,y, 3) return t3 ASSIGN = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, ASSIGN=1, mode='auto', min_delta=0.005, cooldown=5, min_lr=0) ASSIGN = EarlyStopping(monitor='val_loss', mode='auto', patience=2,verbose=0) model.compile(loss='categorical_crossentropy', ASSIGN='adam', ASSIGN=['accuracy', top_3_accuracy]) model.summary() model.fit(x=X_train, y=y_train, ASSIGN = 1000, ASSIGN = 25, ASSIGN = (X_val, y_val), ASSIGN = 1)",0,"['def top_3_accuracy(x,y): \n', '    t3 = top_k_categorical_accuracy(x,y, 3)\n', '    return t3\n', '\n', ""reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, \n"", ""                                   verbose=1, mode='auto', min_delta=0.005, cooldown=5, min_lr=0)\n"", '\n', ""earlystop = EarlyStopping(monitor='val_loss', mode='auto', patience=2,verbose=0) \n"", '\n', '#callbacks = [reduceLROnPlat, earlystop]\n', '#callbacks = earlystop\n', '\n', ""model.compile(loss='categorical_crossentropy',\n"", ""              optimizer='adam',\n"", ""              metrics=['accuracy', top_3_accuracy])\n"", '\n', 'model.summary()\n', '\n', '# model.fit(x=X_train, y=y_train,\n', '#           batch_size = 1000,\n', '#           epochs = 100,\n', '#           validation_data = (X_val, y_val),\n', '#           callbacks = callbacks,\n', '#           verbose = 1)\n', 'model.fit(x=X_train, y=y_train,\n', '          batch_size = 1000,\n', '          epochs = 25,\n', '          validation_data = (X_val, y_val),\n', '          verbose = 1)\n']"
"ASSIGN=ASSIGN.drop('TransactionDT',axis=1) ASSIGN=ASSIGN.drop('TransactionDT',axis=1)",0,"[""train=train.drop('TransactionDT',axis=1)\n"", ""test=test.drop('TransactionDT',axis=1)""]"
SETUP,0,"['import torch\n', 'import torch.nn as nn\n', 'import torch.nn.functional as F\n', 'import torch.optim as optim\n', 'from torchvision import datasets, transforms\n', 'from torch.utils.data import Dataset\n', 'import torchaudio\n', 'import pandas as pd\n', 'import numpy as np']"
"GNB.fit(x_train,x_test) GNB.score(y_train,y_test)",0,"['GNB.fit(x_train,x_test)\n', 'GNB.score(y_train,y_test)']"
CHECKPOINT tf.dtypes,0,['tf.dtypes']
"ASSIGN = pd.DataFrame(columns = ['Province','Lat','Long']) for i in range(len(train)): if train.Province[i] not in list(ASSIGN.Province): ASSIGN = ASSIGN.append(train.iloc[i][['Province','Lat','Long']]) ASSIGN = pd.merge(ASSIGN,df_longlat,on = 'Province',how = 'left')",0,"['#get longitude and latitude dataframe and merge it into test df\n', ""df_longlat = pd.DataFrame(columns = ['Province','Lat','Long'])\n"", 'for i in range(len(train)):\n', '    if train.Province[i] not in list(df_longlat.Province):\n', ""        df_longlat = df_longlat.append(train.iloc[i][['Province','Lat','Long']])\n"", '        \n', ""test = pd.merge(test,df_longlat,on = 'Province',how = 'left')""]"
"def rank_plot(NEIGHBORHOOD_ID): ID=rank_NEIGHBORHOOD(NEIGHBORHOOD_ID) ASSIGN=[] ASSIGN=[] ASSIGN=[] for i in range(6): r1,r2=ID['rank '+str(i+2014)].split('path') R=float(r1)path(r2) R=1-R ASSIGN.append(1.5+R*math.sin(0+i*2*math.pipath)) ASSIGN.append(1.5+R*math.cos(0+i*2*math.pipath)) ASSIGN.append('rank '+str(i+2014)+' '+ID['rank '+str(i+2014)]) ASSIGN.append(ASSIGN[0]) ASSIGN.append(ASSIGN[0]) plt.plot(ASSIGN,ASSIGN, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2) for i, txt in enumerate(ASSIGN): plt.annotate(txt, (ASSIGN[i], ASSIGN[i])) plt.xlim(0.45,2.7) plt.ylim(0.45,2.7) plt.fill(ASSIGN, ASSIGN,""plum"") plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2) plt.title(""The rank of the number of crime by year in ""+NEIGHBORHOOD_ID,size=18)",1,"['def rank_plot(NEIGHBORHOOD_ID):\n', '    ID=rank_NEIGHBORHOOD(NEIGHBORHOOD_ID)\n', '    y=[]\n', '    x=[]\n', '    n=[]\n', '    for i in range(6):\n', ""        r1,r2=ID['rank '+str(i+2014)].split('/')\n"", '        R=float(r1)/float(r2)\n', '        R=1-R\n', '        y.append(1.5+R*math.sin(0+i*2*math.pi/6))\n', '        x.append(1.5+R*math.cos(0+i*2*math.pi/6))\n', ""        n.append('rank '+str(i+2014)+' '+ID['rank '+str(i+2014)])\n"", '    \n', '    x.append(x[0])\n', '    y.append(y[0])\n', ""    plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n"", '    for i, txt in enumerate(n):\n', '        plt.annotate(txt, (x[i], y[i]))\n', '        plt.xlim(0.45,2.7)\n', '        plt.ylim(0.45,2.7)\n', '        plt.fill(x, y,""plum"")\n', ""        plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n"", '        plt.title(""The rank of the number of crime by year in ""+NEIGHBORHOOD_ID,size=18) ']"
"data_features[(data_features['BsmtCond'].isnull() & data_features['BsmtFinType1'].notnull())][['BsmtQual','BsmtFinType1','BsmtFinType2']]",0,"[""data_features[(data_features['BsmtCond'].isnull() & data_features['BsmtFinType1'].notnull())][['BsmtQual','BsmtFinType1','BsmtFinType2']]""]"
"SETUP ASSIGN = set(tb.State) ASSIGN = {} for state in ASSIGN: ASSIGN = stats.linregress(tb.Year[tb.State == state], tb['Never smoked'][tb.State == state]) ASSIGN[state] = slope ASSIGN = pd.DataFrame([slope_dict]).transpose() ASSIGN.columns = ['slope'] ASSIGN.sort(columns = 'slope', ascending = True, inplace = True)",0,"['from scipy import stats\n', '\n', 'states = set(tb.State)\n', '\n', 'slope_dict = {}\n', '\n', 'for state in states:\n', ""    slope, intercept, r_value, p_value, std_err = stats.linregress(tb.Year[tb.State == state], tb['Never smoked'][tb.State == state])\n"", '    slope_dict[state] = slope\n', '    \n', 'slope_df = pd.DataFrame([slope_dict]).transpose()\n', ""slope_df.columns = ['slope']\n"", ""slope_df.sort(columns = 'slope', ascending = True, inplace = True)""]"
"CHECKPOINT ASSIGN = train[""label""] ASSIGN = train.drop(labels = [""label""], axis =1) del train print('x train datasets shape: ',ASSIGN.shape) print('y label datasets shape: ',ASSIGN.shape)",0,"['y_label = train[""label""]\n', 'x_train = train.drop(labels = [""label""], axis =1)\n', 'del train\n', ""print('x train datasets shape: ',x_train.shape)\n"", ""print('y label datasets shape: ',y_label.shape)""]"
"learn.unfreeze() learn.fit_one_cycle(1,max_lr=1e-8)",0,"['learn.unfreeze()\n', '#learn.fit_one_cycle(10,max_lr=1e-6)\n', 'learn.fit_one_cycle(1,max_lr=1e-8)']"
"lineplot(count_sales_group, title = 'Of All Games Produced Did Every Region Sell Every Game?\n\nGames for Sale by Region by Year', ylabel ='Count', legendsize = 8, legendloc = 'upper left')",1,"[""lineplot(count_sales_group, title = 'Of All Games Produced Did Every Region Sell Every Game?\\n\\nGames for Sale by Region by Year', ylabel ='Count', legendsize = 8, legendloc = 'upper left')""]"
"SETUP CHECKPOINT ASSIGN = X ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], 28, 28) print('start') ASSIGN = plt.subplots(nrows=2, ncols=8) ASSIGN = 0 for ax in axes.flat[:]: for i in range(ASSIGN+1, 500): if(p[i]) != train[""label""][i]: ASSIGN = i break ax.set_title(p[ASSIGN]); ax.set_yticklabels([]) ax.set_xticklabels([]) ax.imshow(ASSIGN[i], cmap=plt.get_cmap('gray'))",1,"['import matplotlib.pyplot as plt\n', '%matplotlib inline\n', '\n', 'X_train = X\n', 'X_train = X_train.reshape(X_train.shape[0], 28, 28)\n', '\n', ""print('start')\n"", 'fig, axes = plt.subplots(nrows=2, ncols=8)\n', '\n', 'I = 0\n', 'for ax in axes.flat[:]:\n', '    for i in range(I+1, 500):\n', '        if(p[i]) != train[""label""][i]:\n', '            I = i\n', '            break\n', '    ax.set_title(p[I]);\n', '    ax.set_yticklabels([])\n', '    ax.set_xticklabels([])\n', ""    ax.imshow(X_train[i], cmap=plt.get_cmap('gray'))""]"
CHECKPOINT soundFormatted.shape,0,['soundFormatted.shape']
"plotPerColumnDistribution(df1, 10, 5)",1,"['plotPerColumnDistribution(df1, 10, 5)']"
"CHECKPOINT ASSIGN = [(x path(first_Digit)) for x in first_Digit] ASSIGN = plt.figure() ASSIGN = fig.add_axes([0,0,1,1]) ASSIGN = ['1', '2', '3', '4', '5', '6', '7', '8', '9'] ASSIGN = np.arange(len(index)) plt.xticks(ASSIGN, ASSIGN) ASSIGN.plot(ASSIGN, First_digit_benfords, label= 'Actual') ASSIGN.plot(ASSIGN, ASSIGN, label= 'Expected') plt.title('First Digit; Expected values are in blue, actual are in orange') plt.show",1,"['first_digit_percentile = [(x / sum(first_Digit)) for x in first_Digit]\n', 'fig = plt.figure()\n', 'ax = fig.add_axes([0,0,1,1])\n', ""index = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\n"", 'x = np.arange(len(index))\n', 'plt.xticks(x, index)\n', ""ax.plot(x, First_digit_benfords, label= 'Actual')\n"", ""ax.plot(x, first_digit_percentile, label= 'Expected')\n"", ""plt.title('First Digit; Expected values are in blue, actual are in orange')\n"", 'plt.show']"
ASSIGN = reg_confirm.predict(X_test),0,['pred_confirm = reg_confirm.predict(X_test)']
"CHECKPOINT print('LR_acc: ',sum(LR_result.real==LR_result.pred_0_1)path(LR_result))",0,"[""print('LR_acc: ',sum(LR_result.real==LR_result.pred_0_1)/len(LR_result))""]"
SETUP,0,"['import numpy as np\n', 'import pandas as pd\n', 'import matplotlib.pyplot as plt']"
"SETUP ASSIGN = load_breast_cancer() ASSIGN = pd.DataFrame(cancer['data'],columns=cancer['feature_names']) ASSIGN.head() ASSIGN = StandardScaler() ASSIGN.fit(ASSIGN) ASSIGN = scaler.transform(df) ASSIGN = PCA(n_components=2) ASSIGN.fit(ASSIGN) ASSIGN = pca.transform(scaled_data) plt.figure(figsize=(8,6)) plt.scatter(ASSIGN[:,0],ASSIGN[:,1],c=ASSIGN['target'],cmap='rainbow') plt.xlabel('First principal component') plt.ylabel('Second Principal Component')",1,"['import matplotlib.pyplot as plt\n', 'import pandas as pd\n', 'from sklearn.datasets import load_breast_cancer\n', 'cancer = load_breast_cancer()\n', ""df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n"", 'df.head()\n', 'from sklearn.preprocessing import StandardScaler\n', 'scaler = StandardScaler()\n', 'scaler.fit(df)\n', 'scaled_data = scaler.transform(df)\n', 'from sklearn.decomposition import PCA\n', 'pca = PCA(n_components=2)\n', 'pca.fit(scaled_data)\n', 'x_pca = pca.transform(scaled_data)\n', 'plt.figure(figsize=(8,6))\n', ""plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='rainbow')\n"", ""plt.xlabel('First principal component')\n"", ""plt.ylabel('Second Principal Component')""]"
learn.lr_find(),0,['learn.lr_find()']
data.isnull().sum(),0,['data.isnull().sum()']
"SETUP CHECKPOINT ASSIGN=train_test_split(data_tree[['MSSubClass','MSZoning_new','Neighborhood_new','OverallQual','OverallCond']],data_tree[['SalePrice']],test_size=0.1,random_state=300) ASSIGN=DecisionTreeRegressor(criterion='mse',max_depth=4,random_state=0) ASSIGN=ASSIGN.fit(x_train,y_train) ASSIGN=y_test['SalePrice'] ASSIGN=tree.ASSIGN(x_test) print(np.mean(abs(np.multiply(np.array(y_test.T-ASSIGN),np.array(1path)))))",0,"['from sklearn.model_selection import train_test_split\n', 'from sklearn.tree import DecisionTreeRegressor\n', ""x_train,x_test,y_train,y_test=train_test_split(data_tree[['MSSubClass','MSZoning_new','Neighborhood_new','OverallQual','OverallCond']],data_tree[['SalePrice']],test_size=0.1,random_state=300)\n"", ""tree=DecisionTreeRegressor(criterion='mse',max_depth=4,random_state=0)\n"", 'tree=tree.fit(x_train,y_train)\n', ""y=y_test['SalePrice']\n"", 'predict=tree.predict(x_test)\n', 'print(np.mean(abs(np.multiply(np.array(y_test.T-predict),np.array(1/y_test)))))']"
"CHECKPOINT print(classification_report(y_test, y_pred_gs1))",0,"['#evaluate gs results\n', 'print(classification_report(y_test, y_pred_gs1))']"
ASSIGN = ASSIGN[:25],0,"['# 取前25个进行clean\n', 'idxs = idxs[:25]']"
"CHECKPOINT ASSIGN = model8.predict(x_test) ASSIGN = mean_squared_error(Y_test, y_pred8, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, ASSIGN)))",0,"['y_pred8 = model8.predict(x_test)\n', '\n', 'val = mean_squared_error(Y_test, y_pred8, squared=False)\n', 'val8 = str(round(val, 4))\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred8)))\n""]"
"data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtExposure'].isnull())][ ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]",0,"[""data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtExposure'].isnull())][\n"", ""    ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]""]"
data.neighbourhood_group.unique(),0,['data.neighbourhood_group.unique()']
"ASSIGN = data_features.loc[data_features['Fence'].notnull(),'Fence'] data_features.loc[data_features['Fence'].notnull(),'Fence'].value_counts()",0,"[""a = data_features.loc[data_features['Fence'].notnull(),'Fence']\n"", ""data_features.loc[data_features['Fence'].notnull(),'Fence'].value_counts()""]"
data.tail(),0,['data.tail()']
SLICE=np.log(SLICE) SLICE=np.log(SLICE),0,"[""train['TransactionAmt']=np.log(train['TransactionAmt'])\n"", ""test['TransactionAmt']=np.log(test['TransactionAmt'])""]"
"CHECKPOINT class DeformUnit(nn.Module): def __init__(self, inc,outc): super(DeformUnit, self).__init__() self.conv = DeformConv2d(inc=inc, outc=outc-inc,kernel_size=3, stride=1, padding=1, bias=False, modulation=False) self.relu = nn.ReLU(inplace=True) def forward(self, x): ASSIGN = torch.cat([ASSIGN,self.relu(self.conv(ASSIGN))],1) return x class DenseDeformUnit(nn.Module): def __init__(self, inc, f,out): super(DenseDeformUnit, self).__init__() print(inc,f) self.deformunit1 = DeformUnit(inc,inc+f) ASSIGN = ASSIGN+f print(ASSIGN,f) self.deformunit2 = DeformUnit(ASSIGN, ASSIGN + f) ASSIGN = ASSIGN + f self.deformunit3 = DeformUnit(ASSIGN, ASSIGN + f) ASSIGN = ASSIGN + f self.deformunit4 = DeformUnit(ASSIGN, ASSIGN + f) ASSIGN = ASSIGN + f self.deformunit5 = DeformUnit(ASSIGN, ASSIGN + f) ASSIGN = ASSIGN + f self.deformunit6 = DeformUnit(ASSIGN, ASSIGN + f) ASSIGN = ASSIGN + f self.conv = nn.Sequential( nn.Conv2d(in_channels=ASSIGN, out_channels=out, kernel_size=(1, 1), stride=(1, 1)), nn.ReLU() ) def forward(self, ASSIGN): ASSIGN = x ASSIGN = self.deformunit1(ASSIGN) ASSIGN = self.deformunit2(ASSIGN) ASSIGN = self.deformunit3(ASSIGN) ASSIGN = self.deformunit4(ASSIGN) ASSIGN = self.deformunit5(ASSIGN) ASSIGN = self.deformunit6(ASSIGN) ASSIGN = self.conv(ASSIGN) ASSIGN = torch.cat([originalx,ASSIGN],1) return x class Deform_UpConv(nn.Sequential): def __init__(self, num_input_features, num_output_features): super(Deform_UpConv, self).__init__() self.add_module('conv', nn.ConvTranspose2d(in_channels=num_input_features, out_channels=num_output_features,kernel_size=(2, 2), stride=(2, 2))) self.add_module('relu', nn.ReLU(inplace=True)) class Deform_DenseNet(nn.Module): def __init__(self,ASSIGN,f,grow,feature_vector = feature_vector): super(Deform_DenseNet, self).__init__() ASSIGN = 4*grow self.deformdenseunit1 = DenseDeformUnit(ASSIGN,f,ASSIGN) ASSIGN = out+ASSIGN+feature_vector[1] ASSIGN = ASSIGN+2*grow self.deformdenseunit2 = DenseDeformUnit(ASSIGN, f,ASSIGN) ASSIGN = out+ASSIGN+feature_vector[2] ASSIGN = ASSIGN+2*grow self.deformdenseunit3 = DenseDeformUnit(ASSIGN, f, ASSIGN) def forward(self, ASSIGN): ASSIGN = self.ASSIGN(x) ASSIGN = F.relu(features, inplace=True) return out",0,"['class DeformUnit(nn.Module):\n', '    def __init__(self, inc,outc):\n', '        super(DeformUnit, self).__init__()\n', '        self.conv = DeformConv2d(inc=inc, outc=outc-inc,kernel_size=3, stride=1, padding=1, bias=False, modulation=False)\n', '        self.relu = nn.ReLU(inplace=True)\n', '\n', '\n', '    def forward(self, x):\n', '        x = torch.cat([x,self.relu(self.conv(x))],1)\n', '        return x\n', '\n', 'class DenseDeformUnit(nn.Module):\n', '    def __init__(self, inc, f,out):\n', '        super(DenseDeformUnit, self).__init__()\n', '        print(inc,f)\n', '        self.deformunit1 = DeformUnit(inc,inc+f)\n', '        inc = inc+f\n', '        print(inc,f)\n', '        self.deformunit2 = DeformUnit(inc, inc + f)\n', '        inc = inc + f\n', '        self.deformunit3 = DeformUnit(inc, inc + f)\n', '        inc = inc + f\n', '        self.deformunit4 = DeformUnit(inc, inc + f)\n', '        inc = inc + f\n', '        self.deformunit5 = DeformUnit(inc, inc + f)\n', '        inc = inc + f\n', '        self.deformunit6 = DeformUnit(inc, inc + f)\n', '        inc = inc + f\n', '        self.conv =  nn.Sequential(\n', '            nn.Conv2d(in_channels=inc, out_channels=out, kernel_size=(1, 1), stride=(1, 1)),\n', '            nn.ReLU()\n', '        )\n', '\n', '\n', '    def forward(self, x):\n', '        originalx = x\n', '        x = self.deformunit1(x)\n', '        x = self.deformunit2(x)\n', '        x = self.deformunit3(x)\n', '        x = self.deformunit4(x)\n', '        x = self.deformunit5(x)\n', '        x = self.deformunit6(x)\n', '        x = self.conv(x)\n', '        x = torch.cat([originalx,x],1)\n', '        return x\n', '\n', '\n', '\n', 'class Deform_UpConv(nn.Sequential):\n', '    def __init__(self, num_input_features, num_output_features):\n', '        super(Deform_UpConv, self).__init__()\n', ""        self.add_module('conv', nn.ConvTranspose2d(in_channels=num_input_features, out_channels=num_output_features,kernel_size=(2, 2), stride=(2, 2)))\n"", ""        self.add_module('relu', nn.ReLU(inplace=True))\n"", '\n', '\n', '\n', 'class Deform_DenseNet(nn.Module):\n', '\n', '    def __init__(self,inc,f,grow,feature_vector = feature_vector):\n', '\n', '        super(Deform_DenseNet, self).__init__()\n', '        out = 4*grow\n', '        self.deformdenseunit1 = DenseDeformUnit(inc,f,out)\n', '        inc = out+inc+feature_vector[1]\n', '        out = out+2*grow\n', '        self.deformdenseunit2 = DenseDeformUnit(inc, f,out)\n', '        inc = out+inc+feature_vector[2]\n', '        out = out+2*grow\n', '        self.deformdenseunit3 = DenseDeformUnit(inc, f, out)\n', '\n', '    def forward(self, x):\n', '        features = self.features(x)\n', '        #         print(features.shape)\n', '        out = F.relu(features, inplace=True)\n', '        #         print(out.shape)\n', '        return out\n']"
"ASSIGN = pd.read_csv('..path', encoding='latin-1') ASSIGN.head()",0,"['#Load dataset\n', ""data = pd.read_csv('../input/facebook-ads-2/Facebook_Ads_2.csv', encoding='latin-1')\n"", 'data.head()']"
"CHECKPOINT ASSIGN = data.drop(['Chance of Admit '], axis = 1) x",0,"[""x = data.drop(['Chance of Admit '], axis = 1)\n"", 'x']"
"ASSIGN = Ivis(embedding_dims=2, model='maaten', ASSIGN=15, n_epochs_without_progress=5, ASSIGN=0.95, ASSIGN=0) ASSIGN.fit(train_X, train_Y.values)",0,"[""ivis = Ivis(embedding_dims=2, model='maaten',\n"", '            k=15, n_epochs_without_progress=5,\n', '            supervision_weight=0.95,\n', '            verbose=0)\n', 'ivis.fit(train_X, train_Y.values)']"
val_counts_image_id.head(),0,['val_counts_image_id.head()']
"CHECKPOINT ASSIGN = ASSIGN.values.reshape(-1, 28, 28, 1) print('x train datasets shape: ',ASSIGN.shape)",0,"['x_train = x_train.values.reshape(-1, 28, 28, 1)\n', ""print('x train datasets shape: ',x_train.shape)""]"
"ASSIGN = nn.Sequential(nn.Linear(255, 1024), nn.ReLU(), nn.Linear(1024, 512), nn.ReLU(), nn.Linear(512, 120), nn.LogSoftmax(dim=1)) model.ASSIGN = ASSIGN",0,"['classifier = nn.Sequential(nn.Linear(255, 1024),\n', '                           nn.ReLU(),\n', '                           nn.Linear(1024, 512),\n', '                           nn.ReLU(),\n', '                           nn.Linear(512, 120),\n', '                           nn.LogSoftmax(dim=1))\n', '# Replace default classifier with new classifier\n', 'model.classifier = classifier']"
ASSIGN = pd.read_csv('path'),0,"[""train_df = pd.read_csv('/kaggle/input/global-wheat-detection/train.csv')""]"
ASSIGN=data.fillna(0) ASSIGN.head(),0,"['data2=data.fillna(0)\n', 'data2.head()']"
cbd.head(),0,['cbd.head()']
plt.savefig('results.png') f.savefig('results.png'),0,"[""plt.savefig('results.png')\n"", ""f.savefig('results.png')""]"
"CHECKPOINT ASSIGN = os.listdir(""..path"") print(ASSIGN)",0,"['folder = os.listdir(""../input/lung-colon-sobel/trainable_sobel"")\n', 'print(folder)']"
SETUP ASSIGN = StandardScaler() ASSIGN.fit(df),0,"['from sklearn.preprocessing import StandardScaler\n', 'scaler = StandardScaler()\n', 'scaler.fit(df)']"
"CHECKPOINT ASSIGN = DecisionTreeRegressor() ASSIGN.fit(x_train, Y_train) ASSIGN = model8.score(x_test,Y_test) print(ASSIGN*100,'%')",0,"['model8 = DecisionTreeRegressor()\n', 'model8.fit(x_train, Y_train)\n', '\n', 'accuracy8 = model8.score(x_test,Y_test)\n', ""print(accuracy8*100,'%')""]"
"def predict(test_loader): model.eval() ASSIGN = [] with torch.no_grad(): for images, _ in test_loader: ASSIGN = ASSIGN.to(device) ASSIGN = model(images) ASSIGN = torch.exp(output) ASSIGN = ps.topk(1, dim=1) ASSIGN += [int(i) for i in list(top_class.data.cpu().numpy())] return predictions",0,"['def predict(test_loader):\n', '    model.eval()\n', '    \n', '    predictions = []\n', '    with torch.no_grad():\n', '        for images, _ in test_loader:\n', '            images = images.to(device)\n', '            output = model(images)\n', '            ps = torch.exp(output)\n', '            top_p, top_class = ps.topk(1, dim=1)\n', '            predictions += [int(i) for i in list(top_class.data.cpu().numpy())]\n', '        \n', '    return predictions']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
df.isnull().sum(),0,['df.isnull().sum()']
"ASSIGN = pd.read_csv(""..path"") ASSIGN.sample(n=5)",0,"['df_train = pd.read_csv(""../input/train_users_2.csv"")\n', 'df_train.sample(n=5) ']"
""""""" ASSIGN = [650,700,750,800,850] ASSIGN = ['gini'] ASSIGN = [4,5] ASSIGN = [5,6] ASSIGN = [False,True] ASSIGN = [False,True] ASSIGN = ['balanced_subsample'] ASSIGN = ['log2'] ASSIGN = RandomForestClassifier() ASSIGN = {'n_estimators': [650,700,750,800,850],'criterion': ['gini'],'max_depth':[5,6],'min_samples_split':[4,5],'bootstrap':[False,True] ,'ASSIGN':[False,True],'ASSIGN':['balanced_subsample'],'ASSIGN':['log2']} ASSIGN = GridSearchCV(rF, parameters, scoring='accuracy' ,cv =5) ASSIGN.fit(x_train,x_test) ASSIGN.best_params_ """"""",0,"['""""""\n', 'n_estimators = [650,700,750,800,850]\n', ""criterion = ['gini']\n"", 'max_depth = [4,5]\n', 'min_samples_split = [5,6]\n', 'bootstrap = [False,True]\n', 'oob_score = [False,True]\n', ""class_weight = ['balanced_subsample']\n"", ""max_features = ['log2']\n"", '\n', 'rF = RandomForestClassifier()\n', '\n', ""parameters = {'n_estimators': [650,700,750,800,850],'criterion': ['gini'],'max_depth':[5,6],'min_samples_split':[4,5],'bootstrap':[False,True]\n"", ""              ,'oob_score':[False,True],'class_weight':['balanced_subsample'],'max_features':['log2']}\n"", '\n', ""RClassifier = GridSearchCV(rF, parameters, scoring='accuracy' ,cv =5)\n"", 'RClassifier.fit(x_train,x_test)\n', 'RClassifier.best_params_\n', '""""""']"
"''' ASSIGN = [150,200,250,300,350] ASSIGN = [.01,.1] ASSIGN = [.05,.1] ASSIGN = [3,4,5] ASSIGN = [9,10,11] ASSIGN = ['exponential'] ASSIGN = ['auto'] ASSIGN = GradientBoostingClassifier() ASSIGN = {'n_estimators': [150,200,250,300,350],'loss': ['exponential'],'max_features':['auto'],'learning_rate':[.01,.1],'subsample':[.05,.1], 'ASSIGN':[3,4,5],'ASSIGN':[9,10,11]} ASSIGN = GridSearchCV(GB, parameters, scoring='accuracy' ,cv =5) ASSIGN.fit(x_train, x_test) ASSIGN.best_params_ '''",0,"[""'''\n"", 'n_estimators = [150,200,250,300,350]\n', 'learning_rate = [.01,.1]\n', 'subsample = [.05,.1]\n', 'min_samples_split = [3,4,5]\n', 'max_depth = [9,10,11]\n', ""loss = ['exponential']\n"", ""max_features = ['auto']\n"", '\n', 'GB = GradientBoostingClassifier()\n', '\n', ""parameters = {'n_estimators': [150,200,250,300,350],'loss': ['exponential'],'max_features':['auto'],'learning_rate':[.01,.1],'subsample':[.05,.1],\n"", ""             'min_samples_split':[3,4,5],'max_depth':[9,10,11]}\n"", '\n', ""GBClassifier = GridSearchCV(GB, parameters, scoring='accuracy' ,cv =5)\n"", 'GBClassifier.fit(x_train, x_test)\n', 'GBClassifier.best_params_\n', ""'''""]"
CHECKPOINT scaled_data.shape,0,['scaled_data.shape']
"CHECKPOINT X_test,X_valid,y_test,y_valid = train_test_split(X_test_sub, y_test_sub, test_size=0.5, random_state=0 , shuffle =False) print(X_test.shape) print(X_valid.shape)",0,"['X_test,X_valid,y_test,y_valid = train_test_split(X_test_sub, y_test_sub, test_size=0.5, random_state=0 , shuffle =False)\n', 'print(X_test.shape)\n', 'print(X_valid.shape)']"
ASSIGN = last_two_digit_test(finalData),0,['last_two = last_two_digit_test(finalData)']
"CHECKPOINT ASSIGN = linear_model.ElasticNet(alpha=0.001) ASSIGN.fit(X_train,y_train) ASSIGN = model6.score(X_test,y_test) print(ASSIGN*100,'%')",0,"['model6 = linear_model.ElasticNet(alpha=0.001)\n', 'model6.fit(X_train,y_train)\n', '\n', 'accuracy6 = model6.score(X_test,y_test)\n', ""print(accuracy6*100,'%')""]"
"ASSIGN = pd.read_csv(""..path"") ASSIGN.sample(n=5)",0,"['df_test = pd.read_csv(""../input/test_users.csv"")\n', 'df_test.sample(n=5)']"
"submit.to_csv('submission.csv',index=False)",0,"[""submit.to_csv('submission.csv',index=False)""]"
final.Ticket.value_counts(),0,['final.Ticket.value_counts()']
"ASSIGN=lines[lines.city_id==91] ASSIGN=track_lines[track_lines.city_id==91] ASSIGN=tracks[tracks.city_id==91].drop(columns=['buildstart','opening','closure','city_id']) ASSIGN.columns=['section_id','geometry','length'] ASSIGN=pd.merge(ASSIGN,osaka_tracks) ASSIGN=ASSIGN.drop(columns=['id','created_at','updated_at','city_id']) ASSIGN.columns=['section_id','id','geometry','length'] ASSIGN=pd.merge(osaka_track_lines,ASSIGN) ASSIGN=stations[stations['city_id']==91] ASSIGN.head()",0,"['osaka_lines=lines[lines.city_id==91]\n', 'osaka_track_lines=track_lines[track_lines.city_id==91]\n', ""osaka_tracks=tracks[tracks.city_id==91].drop(columns=['buildstart','opening','closure','city_id'])\n"", ""osaka_tracks.columns=['section_id','geometry','length']\n"", 'osaka_track_lines=pd.merge(osaka_track_lines,osaka_tracks)\n', ""osaka_track_lines=osaka_track_lines.drop(columns=['id','created_at','updated_at','city_id'])\n"", ""osaka_track_lines.columns=['section_id','id','geometry','length']\n"", 'osaka_lines=pd.merge(osaka_track_lines,osaka_lines)\n', ""osaka_stations=stations[stations['city_id']==91]\n"", 'osaka_stations.head()']"
CHECKPOINT for col in digital_cols: ASSIGN = train[col].min() ASSIGN = train[col].max() ASSIGN = train[col].mean() ASSIGN = train[col].median() ASSIGN = train[col].std() ASSIGN = test[col].min() ASSIGN = test[col].max() ASSIGN = test[col].mean() ASSIGN = test[col].median() ASSIGN = test[col].std() ASSIGN = len(train.loc[train[col].isna()]) path(train) ASSIGN = len(test.loc[test[col].isna()]) path(test) print(f'Col name:{col:30}') print(f'\tIn train data: min value:{ASSIGN:.2f}\tmax value:{ASSIGN:.2f}\tmean value:{ASSIGN:.2f}\tmedian value:{ASSIGN:.2f}\tstd value:{ASSIGN:.2f}\tnan sample rate:{ASSIGN:.2f}\t') print(f'\tIn test data: min value:{ASSIGN:.2f}\tmax value:{ASSIGN:.2f}\tmean value:{ASSIGN:.2f}\tmedian value:{ASSIGN:.2f}\tstd value:{ASSIGN:.2f}\tnan sample rate:{ASSIGN:.2f}\t'),0,"['for col in digital_cols:\n', '    min_tr = train[col].min()\n', '    max_tr = train[col].max()\n', '    mean_tr = train[col].mean()\n', '    median_tr = train[col].median()\n', '    std_tr = train[col].std()\n', '    \n', '    min_te = test[col].min()\n', '    max_te = test[col].max()\n', '    mean_te = test[col].mean()\n', '    median_te = test[col].median()\n', '    std_te = test[col].std()\n', '    \n', '    na_tr = len(train.loc[train[col].isna()]) / len(train)\n', '    na_te = len(test.loc[test[col].isna()]) / len(test)\n', ""    print(f'Col name:{col:30}')\n"", ""    print(f'\\tIn train data: min value:{min_tr:.2f}\\tmax value:{max_tr:.2f}\\tmean value:{mean_tr:.2f}\\tmedian value:{median_tr:.2f}\\tstd value:{std_tr:.2f}\\tnan sample rate:{na_tr:.2f}\\t')\n"", ""    print(f'\\tIn  test data: min value:{min_te:.2f}\\tmax value:{max_te:.2f}\\tmean value:{mean_te:.2f}\\tmedian value:{median_te:.2f}\\tstd value:{std_te:.2f}\\tnan sample rate:{na_te:.2f}\\t')""]"
CHECKPOINT feature_vector,0,['feature_vector']
ASSIGN = pd.read_csv('..path'),0,"[""test_df = pd.read_csv('../input/test.csv')""]"
ASSIGN = pd.read_csv('..path'),0,"[""menu = pd.read_csv('../input/menu.csv')""]"
"os.listdir(""..path"")",0,"['os.listdir(""../input/house-prices-advanced-regression-techniques"")']"
"ivis.save_model('ivis-supervised-fraud', overwrite=True)",0,"[""ivis.save_model('ivis-supervised-fraud', overwrite=True)""]"
"SETUP CHECKPOINT warnings.filterwarnings(""ignore"") for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'from sklearn.preprocessing import LabelEncoder\n', 'from sklearn.model_selection import train_test_split, StratifiedKFold,KFold\n', 'from datetime import datetime\n', 'from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\n', 'from sklearn import metrics\n', 'from sklearn import preprocessing\n', '# Suppr warning\n', 'import warnings\n', 'warnings.filterwarnings(""ignore"")\n', 'import itertools\n', 'from scipy import interp\n', '# Plots\n', 'import seaborn as sns\n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline\n', 'from matplotlib import rcParams\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# Any results you write to the current directory are saved as output.']"
"ASSIGN = lambda x: int(x.replace(',', '')) if type(x) == np.str and x != np.nan else x zomato.votes = zomato.votes.astype('int') zomato['approx_cost_for_2_people'] = zomato['approx_cost_for_2_people'].apply(ASSIGN)",0,"[""remove_comma = lambda x: int(x.replace(',', '')) if type(x) == np.str and x != np.nan else x \n"", ""zomato.votes = zomato.votes.astype('int')\n"", ""zomato['approx_cost_for_2_people'] = zomato['approx_cost_for_2_people'].apply(remove_comma)""]"
"CHECKPOINT ASSIGN=np.array([5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]) ASSIGN=np.array([2,5,6,10,80]) ASSIGN=[m for m, val in enumerate(a1) if val in set(a2)] ASSIGN=np.delete(a1,temp) print(,ASSIGN) print(,ASSIGN) print(,ASSIGN) print(,ASSIGN)",0,"['a1=np.array([5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100])\n', 'a2=np.array([2,5,6,10,80])\n', 'temp=[m for m, val in enumerate(a1) if val in set(a2)]\n', 'new_arr=np.delete(a1,temp)\n', 'print(""ARRAY 1:"",a1)\n', 'print(""ARRAY 2:"",a2)\n', 'print(""NEW ARRAY:"",new_arr)\n', 'print(""ARRAY 2:"",a2)']"
"CHECKPOINT ASSIGN = DecisionTreeClassifier(criterion = ""entropy"", random_state = 100,max_depth = 3) ASSIGN = ASSIGN.fit(X_train,y_train) ASSIGN = clf.predict(X_test) print(,metrics.accuracy_score(y_test, ASSIGN)) print(, classification_report(y_test, ASSIGN))",0,"['# Create Decision Tree classifer object\n', 'clf = DecisionTreeClassifier(criterion = ""entropy"", random_state = 100,max_depth = 3)\n', '\n', '# Train Decision Tree Classifer\n', 'clf = clf.fit(X_train,y_train)\n', '\n', '#Predict the response for test dataset\n', 'y_pred = clf.predict(X_test)\n', '\n', '# Model Accuracy, how often is the classifier correct?\n', 'print(""Accuracy:"",metrics.accuracy_score(y_test, y_pred))\n', 'print(""Report : "",  classification_report(y_test, y_pred)) ']"
"ASSIGN=32 ASSIGN = model.fit_generator( data_gen(X_train, target_label_map, ASSIGN, augment=True), ASSIGN=data_gen(X_valid, target_label_map, batch_size), ASSIGN=50, ASSIGN = 1, ASSIGN= int(len(X_train)path), ASSIGN= int(len(X_valid)path) )",0,"['batch_size=32\n', 'history = model.fit_generator(\n', '    data_gen(X_train, target_label_map, batch_size, augment=True),\n', '    validation_data=data_gen(X_valid, target_label_map, batch_size),\n', '    epochs=50, \n', '    verbose = 1,\n', '    #callbacks=callbacks,\n', '    steps_per_epoch=  int(len(X_train)//batch_size),\n', '    validation_steps= int(len(X_valid)// batch_size)\n', ')']"
SETUP,0,"['import numpy as np\n', 'import pandas as pd\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', '%matplotlib inline']"
"CHECKPOINT model4.save(""model4.h5"") print()",0,"['model4.save(""model4.h5"")\n', 'print(""Saved model to disk"")']"
"sns.countplot(top_species) plt.title(""Top Species That Impact with Aircraft"") plt.xticks(rotation='vertical')",1,"['sns.countplot(top_species)\n', 'plt.title(""Top Species That Impact with Aircraft"")\n', ""plt.xticks(rotation='vertical')""]"
"ASSIGN=pd.DataFrame({'Students':['A','B','C','D'],'Subjects':['DSD','AIpath','COA','ALGORITHMS'],'Marks':[50,90,80,80]})",0,"[""df=pd.DataFrame({'Students':['A','B','C','D'],'Subjects':['DSD','AI/ML','COA','ALGORITHMS'],'Marks':[50,90,80,80]})""]"
ASSIGN = pd.read_csv('..path'),0,"[""data = pd.read_csv('../input/santa-workshop-tour-2019/family_data.csv')""]"
"ASSIGN = pd.DataFrame(index=np.arange(0, total_images), columns=[""path"", ""target""]) ASSIGN = 0 for n in range(len(folder)): ASSIGN = folder[n] ASSIGN = os.path.join(base_path,class_id) ASSIGN = os.listdir(final_path) for m in range(len(ASSIGN)): ASSIGN = subfiles[m] ASSIGN.iloc[ASSIGN][""path""] = os.path.join(ASSIGN,ASSIGN) ASSIGN.iloc[ASSIGN][""target""] = ASSIGN ASSIGN += 1 ASSIGN.head()",0,"['data = pd.DataFrame(index=np.arange(0, total_images), columns=[""path"", ""target""])\n', '\n', 'k = 0\n', 'for n in range(len(folder)):\n', '    class_id = folder[n]\n', '    final_path = os.path.join(base_path,class_id) \n', '    subfiles = os.listdir(final_path)\n', '    for m in range(len(subfiles)):\n', '      image_path = subfiles[m]\n', '      data.iloc[k][""path""] = os.path.join(final_path,image_path)\n', '      data.iloc[k][""target""] = class_id\n', '      k += 1  \n', '\n', 'data.head()']"
"def save_model(model, epoch, loss): ASSIGN = os.path.join(args.intermediate_path, ""model_{}_epoch{}_loss{:.4f}.pth"" .format(args.seed, epoch, loss)) torch.save(model.state_dict(), os.path.join(ASSIGN))",0,"['def save_model(model, epoch, loss):\n', '    model_file = os.path.join(args.intermediate_path,\n', '                              ""model_{}_epoch{}_loss{:.4f}.pth""\n', '                              .format(args.seed, epoch, loss))\n', '    torch.save(model.state_dict(), os.path.join(model_file))\n']"
"ASSIGN=pd.DataFrame(ASSIGN.values, columns =[""NAME"",""PRICE"",""CUSINE_CATEGORY"",""CITY"",""REGION"",""URL"",""PAGE NO"",""CUSINE TYPE"",""TIMING"",""RATING_TYPE"",""RATING"",""VOTES""])",0,"['#adding the header columns\n', 'df_final=pd.DataFrame(df_final.values, columns =[""NAME"",""PRICE"",""CUSINE_CATEGORY"",""CITY"",""REGION"",""URL"",""PAGE NO"",""CUSINE TYPE"",""TIMING"",""RATING_TYPE"",""RATING"",""VOTES""])']"
data_manha.neighbourhood.unique(),0,['data_manha.neighbourhood.unique()']
"ASSIGN = nx.Graph() ASSIGN = [] for i in range(0,len(fam)): ASSIGN.add_node(fam[i], type = 'fam') for j in ind[i]: ASSIGN = fam[i]+j ASSIGN.add_node(ASSIGN, type = 'ind') ASSIGN.add_edge(fam[i], ASSIGN, color='green', weight=1) for n1, attr in ASSIGN.nodes(data=True): if attr['type'] == 'fam': ASSIGN.append('lime') else: if attr['type'] == 'ind': ASSIGN.append('cyan') else: ASSIGN.append('red')",1,"['# Create graph from data \n', 'g = nx.Graph()\n', 'color_map = []\n', 'for i in range(0,len(fam)): #len(names)\n', ""    g.add_node(fam[i], type = 'fam')\n"", '    for j in ind[i]:\n', '        temp = fam[i]+j\n', ""        g.add_node(temp, type = 'ind')\n"", ""        g.add_edge(fam[i], temp, color='green', weight=1)\n"", 'for n1, attr in g.nodes(data=True):\n', ""    if attr['type'] == 'fam':\n"", ""        color_map.append('lime')\n"", '    else: \n', ""        if attr['type'] == 'ind':\n"", ""            color_map.append('cyan')\n"", '        else:\n', ""            color_map.append('red')""]"
CHECKPOINT print(os.listdir('..path')),0,"[""print(os.listdir('../input'))""]"
ASSIGN = 6 ASSIGN = DAG(num_nodes),0,"['num_nodes = 6\n', 'dag_model = DAG(num_nodes)']"
ASSIGN = stack.predict_proba(X_test_full),0,['predictions = stack.predict_proba(X_test_full)']
ASSIGN=df.columns,0,['cols=df.columns']
"CHECKPOINT ASSIGN = tempData ASSIGN = torch.zeros([32000, 1]) ASSIGN[:32000] = ASSIGN[::5] soundFormatted.shape",0,"['soundData = tempData\n', 'soundFormatted = torch.zeros([32000, 1])\n', 'soundFormatted[:32000] = soundData[::5] #take every fifth sample of soundData\n', 'soundFormatted.shape']"
"death.columns = ['ds','y'] ASSIGN = pd.to_datetime(ASSIGN)",0,"[""death.columns = ['ds','y']\n"", ""death['ds'] = pd.to_datetime(death['ds'])""]"
CHECKPOINT pred_table,0,['pred_table']
"CHECKPOINT ASSIGN = pd.merge(ASSIGN, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left') ASSIGN.rename(columns={'Seed':'WSeed'}, inplace=True) ASSIGN = ASSIGN.drop('TeamID', axis=1) ASSIGN = pd.merge(ASSIGN, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left') ASSIGN.rename(columns={'Seed':'LSeed'}, inplace=True) ASSIGN = ASSIGN.drop('TeamID', axis=1) tourney_result",0,"[""tourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n"", ""tourney_result.rename(columns={'Seed':'WSeed'}, inplace=True)\n"", ""tourney_result = tourney_result.drop('TeamID', axis=1)\n"", ""tourney_result = pd.merge(tourney_result, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n"", ""tourney_result.rename(columns={'Seed':'LSeed'}, inplace=True)\n"", ""tourney_result = tourney_result.drop('TeamID', axis=1)\n"", 'tourney_result']"
"gbm.predict(test_transaction_new[:10], num_iteration=gbm.best_iteration)",0,"['gbm.predict(test_transaction_new[:10], num_iteration=gbm.best_iteration) ']"
ASSIGN = model.predict_proba(test_x),0,['predict_y = model.predict_proba(test_x)']
"for i in zip(player_attr_df.columns, player_attr_df.dtypes): if i[1] == 'float64': player_attr_df[i[0]] = player_attr_df[i[0]].fillna(player_attr_df[i[0]].mean())",0,"['for i in zip(player_attr_df.columns, player_attr_df.dtypes):\n', ""    if i[1] == 'float64':\n"", '        player_attr_df[i[0]] = player_attr_df[i[0]].fillna(player_attr_df[i[0]].mean())']"
"CHECKPOINT ASSIGN = DecisionTreeClassifier(random_state = 5) estimator.append(('ASSIGN',DecisionTreeClassifier(random_state = 5))) ASSIGN = cross_val_score(DT,x_train,x_test,ASSIGN=10) ASSIGN = cv.mean() accuracy.append(ASSIGN) print(ASSIGN) print(ASSIGN.mean())",0,"['DT = DecisionTreeClassifier(random_state = 5)\n', ""estimator.append(('DT',DecisionTreeClassifier(random_state = 5)))\n"", 'cv = cross_val_score(DT,x_train,x_test,cv=10)\n', 'accuracy4 = cv.mean()\n', 'accuracy.append(accuracy4)\n', 'print(cv)\n', 'print(cv.mean())']"
CHECKPOINT print(rec_img.shape),0,['print(rec_img.shape)']
del train['id'] del test['id'],0,"[""del train['id']\n"", ""del test['id']""]"
"SETUP CHECKPOINT estimator.append(('XGB', XGBClassifier(random_state = 5))) ASSIGN = cross_val_score(XGB,x_train,x_test,ASSIGN=10) ASSIGN = cv.mean() accuracy.append(ASSIGN) print(ASSIGN) print(ASSIGN.mean())",0,"['XGB = XGBClassifier(random_state = 5)\n', ""estimator.append(('XGB', XGBClassifier(random_state = 5)))\n"", 'cv = cross_val_score(XGB,x_train,x_test,cv=10)\n', 'accuracy9 = cv.mean()\n', 'accuracy.append(accuracy9)\n', 'print(cv)\n', 'print(cv.mean())']"
CHECKPOINT print(X_train.shape) print(Y_train.shape) print(X_test.shape) print(Y_test.shape),0,"['print(X_train.shape)\n', 'print(Y_train.shape)\n', 'print(X_test.shape)\n', 'print(Y_test.shape)']"
"SETUP if ""models"" in pathlib.Path.cwd().parts: while ""models"" in pathlib.Path.cwd().parts: os.chdir('..') elif not pathlib.Path('models').exists():",0,"['import os\n', 'import pathlib\n', '\n', ""# Clone the tensorflow models repository if it doesn't already exist\n"", 'if ""models"" in pathlib.Path.cwd().parts:\n', '    while ""models"" in pathlib.Path.cwd().parts:\n', ""        os.chdir('..')\n"", ""elif not pathlib.Path('models').exists():\n"", '    !git clone --depth 1 https://github.com/tensorflow/models']"
"plt.figure(figsize=(20,30)) plt.subplot(3,2,1) rank_plot('five-points') plt.subplot(3,2,2) rank_plot('stapleton') plt.subplot(3,2,3) rank_plot('cbd') plt.subplot(3,2,4) rank_plot('capitol-hill') plt.subplot(3,2,5) rank_plot('virginia-village') plt.subplot(3,2,6) rank_plot('city-park')",1,"['plt.figure(figsize=(20,30))\n', 'plt.subplot(3,2,1)\n', ""rank_plot('five-points')\n"", 'plt.subplot(3,2,2)\n', ""rank_plot('stapleton')\n"", 'plt.subplot(3,2,3)\n', ""rank_plot('cbd')\n"", 'plt.subplot(3,2,4)\n', ""rank_plot('capitol-hill')\n"", 'plt.subplot(3,2,5)\n', ""rank_plot('virginia-village')\n"", 'plt.subplot(3,2,6)\n', ""rank_plot('city-park')""]"
"def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow): ASSIGN = df.ASSIGN() ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]] nRow, nCol = ASSIGN.shape ASSIGN = list(df) ASSIGN = (nCol + nGraphPerRow - 1) path plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k') for i in range(min(nCol, nGraphShown)): plt.subplot(ASSIGN, nGraphPerRow, i + 1) ASSIGN = df.iloc[:, i] if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)): ASSIGN = columnDf.value_counts() ASSIGN.plot.bar() else: ASSIGN.hist() plt.ylabel('counts') plt.xticks(rotation = 90) plt.title(f'{ASSIGN[i]} (column {i})') plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0) plt.show()",1,"['# Distribution graphs (histogram/bar graph) of column data\n', 'def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n', '    nunique = df.nunique()\n', '    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n', '    nRow, nCol = df.shape\n', '    columnNames = list(df)\n', '    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n', ""    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n"", '    for i in range(min(nCol, nGraphShown)):\n', '        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n', '        columnDf = df.iloc[:, i]\n', '        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n', '            valueCounts = columnDf.value_counts()\n', '            valueCounts.plot.bar()\n', '        else:\n', '            columnDf.hist()\n', ""        plt.ylabel('counts')\n"", '        plt.xticks(rotation = 90)\n', ""        plt.title(f'{columnNames[i]} (column {i})')\n"", '    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n', '    plt.show()\n']"
"SETUP CHECKPOINT ASSIGN = svc_model.predict(X_test) print(classification_report(y_test, ASSIGN))",0,"['#Evaluate model\n', 'y_pred = svc_model.predict(X_test)\n', 'from sklearn.metrics import classification_report, confusion_matrix\n', 'print(classification_report(y_test, y_pred))']"
"ASSIGN=ASSIGN.dropna(subset=['closure','name','opening']) ASSIGN=ASSIGN[ASSIGN.closure>=9999] ASSIGN=ASSIGN[ASSIGN.opening>0] ASSIGN=ASSIGN[ASSIGN.opening<=2030] ASSIGN.columns=['id','stations_name','geometry','buildstart','opening','closure','city_id'] SLICE=SLICE.apply(lambda x: x.split('POINT(')[1].split(' ')[0]) SLICE=SLICE.apply(lambda x: x.split('POINT(')[1].split(' ')[1].split(')')[0]) ASSIGN=pd.DataFrame({'city_id':cities.id,'country':cities.country,'name':cities.name}) ASSIGN=pd.merge(ASSIGN,id_country) ASSIGN.head()",0,"[""stations=stations.dropna(subset=['closure','name','opening'])\n"", 'stations=stations[stations.closure>=9999]\n', 'stations=stations[stations.opening>0]\n', 'stations=stations[stations.opening<=2030]\n', ""stations.columns=['id','stations_name','geometry','buildstart','opening','closure','city_id']\n"", ""stations['Long']=stations['geometry'].apply(lambda x: x.split('POINT(')[1].split(' ')[0])\n"", ""stations['Lat']=stations['geometry'].apply(lambda x: x.split('POINT(')[1].split(' ')[1].split(')')[0])\n"", ""id_country=pd.DataFrame({'city_id':cities.id,'country':cities.country,'name':cities.name})\n"", '\n', 'stations=pd.merge(stations,id_country)\n', 'stations.head()']"
"sns.catplot(x='Year', y='Other Crimes Against SCs', data=total ,height = 5, aspect = 4,kind = 'bar')",1,"[""sns.catplot(x='Year', y='Other Crimes Against SCs', data=total ,height = 5, aspect = 4,kind = 'bar')""]"
"ASSIGN=pd.DataFrame.from_dict(dc,orient=""index"",columns=[""CITY"",""COUNT""])",0,"['#creating another data frame from the above dictionary\n', 'df_map=pd.DataFrame.from_dict(dc,orient=""index"",columns=[""CITY"",""COUNT""])']"
SETUP,0,"['import numpy as np\n', 'import pandas as pd\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', '%matplotlib inline']"
"ASSIGN = pd.read_csv(""..path"") ASSIGN  = pd.read_csv(""..path"") ASSIGN.head()",0,"['# get titanic & test csv files as a DataFrame\n', 'titanic_df = pd.read_csv(""../input/train.csv"")\n', 'test_df    = pd.read_csv(""../input/test.csv"")\n', '\n', '# preview the data\n', 'titanic_df.head()']"
SETUP,0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', '# source https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/dicom.ipynb#scrollTo=WodUv8O1VKmr']"
SETUP CHECKPOINT print(os.listdir('..path')),0,"['# General Imports\n', 'import copy\n', 'import os\n', 'import numpy as np\n', 'import pandas as pd\n', 'from sklearn.feature_extraction.text import TfidfVectorizer\n', 'from sklearn.model_selection import train_test_split\n', '\n', '# Pytorch Imports\n', 'import torch\n', 'from torch import nn\n', 'import torch.nn.functional as F\n', 'from torch.autograd import Variable\n', 'import torchvision.transforms as transforms\n', 'from torch.utils.data import TensorDataset, DataLoader\n', '\n', ""print(os.listdir('../input/'))""]"
data.iloc[0:10],0,['data.iloc[0:10]#selecting 6 cloumns and 10 rows']
"matriz_deconfusao(cm = reg_cm, normalize  = False, ASSIGN = ['Positivo - COVID-19', 'Não Infectado'], ASSIGN    = ""Matriz de Confusão"")",1,"['matriz_deconfusao(cm = reg_cm, normalize  = False,\n', ""                      target_names = ['Positivo - COVID-19', 'Não Infectado'],\n"", '                      title        = ""Matriz de Confusão"")']"
"ASSIGN = df['Chance of Admit '] sns.distplot(ASSIGN , kde= True,rug = False, bins = 30)",1,"[""x = df['Chance of Admit ']\n"", 'sns.distplot(x , kde= True,rug = False, bins = 30)']"
CHECKPOINT data.shape,0,['data.shape']
"final['Ticket_length'] = np.where(((final.Ticket_length == 3) | (final.Ticket_length == 4) | (final.Ticket_length == 5)),4,final.Ticket_length) final['Ticket_length'] = np.where(((final.Ticket_length == 6)),5,final.Ticket_length) final['Ticket_length'] = np.where(((final.Ticket_length == 7) | (final.Ticket_length == 8) | (final.Ticket_length == 9) | (final.Ticket_length == 10) | (final.Ticket_length == 13) | (final.Ticket_length == 17)| (final.Ticket_length == 16)| (final.Ticket_length == 13)| (final.Ticket_length == 12) | (final.Ticket_length == 15) | (final.Ticket_length == 11)| (final.Ticket_length == 18)),12,final.Ticket_length)",0,"[""final['Ticket_length'] = np.where(((final.Ticket_length == 3) | (final.Ticket_length == 4) | (final.Ticket_length == 5)),4,final.Ticket_length)\n"", '\n', ""final['Ticket_length'] = np.where(((final.Ticket_length == 6)),5,final.Ticket_length)\n"", '\n', ""final['Ticket_length'] = np.where(((final.Ticket_length == 7) | (final.Ticket_length == 8) | (final.Ticket_length == 9) | (final.Ticket_length == 10) | (final.Ticket_length == 13)\n"", '                                 | (final.Ticket_length == 17)| (final.Ticket_length == 16)| (final.Ticket_length == 13)| (final.Ticket_length == 12) | (final.Ticket_length == 15)\n', '                                 | (final.Ticket_length == 11)| (final.Ticket_length == 18)),12,final.Ticket_length)\n']"
"CHECKPOINT print(, len(data)) print(, len(data[data['Retire']==1])) print(, len(data[data['Retire']==0]))",0,"['#Retirement\n', 'print(""The total number of customers in this dataset is: "", len(data))\n', 'print(""Out of those customers, the number of retired customers is: "", len(data[data[\'Retire\']==1]))\n', 'print(""Out of those customers, the number of not retired customers is: "", len(data[data[\'Retire\']==0]))']"
"ASSIGN = pd.DataFrame({'breed': df['breed'].value_counts().index, 'instances': df['breed'].value_counts().values}) ASSIGN = ASSIGN.sort_values(by=['breed']) ASSIGN.head()",0,"[""temp = pd.DataFrame({'breed': df['breed'].value_counts().index, 'instances': df['breed'].value_counts().values})\n"", ""temp = temp.sort_values(by=['breed'])\n"", 'temp.head()']"
ASSIGN=ASSIGN[ASSIGN.price<=180] ASSIGN=ASSIGN[ASSIGN.price>=90] len(ASSIGN),0,"['data_tree=data_tree[data_tree.price<=180]\n', 'data_tree=data_tree[data_tree.price>=90]\n', 'len(data_tree)']"
"CHECKPOINT def replace_matches_in_column(df, column, string_to_match, min_ratio = 90): ASSIGN = df[column].unique() ASSIGN = fuzzywuzzy.process.extract(string_to_match, strings, ASSIGN=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio) ASSIGN = [matches[0] for matches in matches if matches[1] >= min_ratio] ASSIGN = df[column].isin(close_matches) df.loc[ASSIGN, column] = string_to_match print()",0,"['# function to replace rows in the provided column of the provided dataframe\n', '# that match the provided string above the provided ratio with the provided string\n', 'def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):\n', '    # get a list of unique strings\n', '    strings = df[column].unique()\n', '    \n', '    # get the top 10 closest matches to our input string\n', '    matches = fuzzywuzzy.process.extract(string_to_match, strings, \n', '                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n', '\n', '    # only get matches with a ratio > 90\n', '    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]\n', '\n', '    # get the rows of all the close matches in our dataframe\n', '    rows_with_matches = df[column].isin(close_matches)\n', '\n', '    # replace all rows with close matches with the input matches \n', '    df.loc[rows_with_matches, column] = string_to_match\n', '    \n', ""    # let us know the function's done\n"", '    print(""All done!"")']"
"ASSIGN = covid_data[covid_data['Countrypath'] == 'Italy'] ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for dat in ASSIGN['ObservationDate'].unique(): ASSIGN = italy_data[italy_data['ObservationDate'] == dat] ASSIGN = sub['Confirmed'].sum() ASSIGN = sub['Deaths'].sum() ASSIGN = sub['Recovered'].sum() ASSIGN.append(dat) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN = pd.Series(ASSIGN) ASSIGN =pd.Series(ASSIGN) ASSIGN = pd.Series(ASSIGN) ASSIGN = pd.Series(ASSIGN) ASSIGN = [date.min(), date[len(date)path], date.max()] plt.figure(figsize=(8,8)) plt.plot(ASSIGN, ASSIGN, color = 'yellow') plt.plot(ASSIGN, ASSIGN, color = 'red') plt.plot(ASSIGN, ASSIGN, color = 'green') plt.xticks(ASSIGN, ASSIGN) plt.xlabel('Date') plt.ylabel('Cummulative Count cases') plt.title('Trend Curve of Confirmed Cases in Italy') plt.legend(['Confirmed', 'Death', 'Recovered']) plt.show()",1,"[""italy_data = covid_data[covid_data['Country/Region'] == 'Italy']\n"", 'date = []\n', 'c = []\n', 'd = []\n', 'r = []\n', ""for dat in italy_data['ObservationDate'].unique():\n"", ""    sub = italy_data[italy_data['ObservationDate'] == dat]\n"", ""    confirm = sub['Confirmed'].sum()\n"", ""    death = sub['Deaths'].sum()\n"", ""    recover = sub['Recovered'].sum()\n"", '    date.append(dat)\n', '    c.append(confirm)\n', '    d.append(death)\n', '    r.append(recover)\n', '\n', 'date = pd.Series(date)\n', 'c  =pd.Series(c)\n', 'd = pd.Series(d)\n', 'r = pd.Series(r)\n', '\n', 't = [date.min(), date[len(date)//2], date.max()]\n', 'plt.figure(figsize=(8,8))\n', ""plt.plot(date, c, color = 'yellow')\n"", ""plt.plot(date, d, color = 'red')\n"", ""plt.plot(date, r, color = 'green')\n"", 'plt.xticks(t, t)\n', ""plt.xlabel('Date')\n"", ""plt.ylabel('Cummulative Count cases')\n"", ""plt.title('Trend Curve of Confirmed Cases in Italy')\n"", ""plt.legend(['Confirmed', 'Death', 'Recovered'])\n"", 'plt.show()']"
"SETUP warnings.filterwarnings(""ignore"")",0,"['#For data processing\n', 'import pandas as pd\n', 'import numpy as np\n', '\n', '#For visualizations\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', '\n', '#For ignoring warnings\n', 'import warnings\n', 'warnings.filterwarnings(""ignore"")']"
SETUP,0,"['import numpy as np\n', 'import pandas as pd \n', 'import geopandas as gpd\n', 'import matplotlib.pyplot as plt\n', 'import os\n', 'import seaborn as sns\n', 'import folium\n', 'from folium import plugins']"
"SETUP ASSIGN = Fold(input_df, input_X, seed=3333); torch_seed(ASSIGN.seed) ASSIGN = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)",0,"['%%time\n', 'fold = Fold(input_df, input_X, seed=3333); torch_seed(fold.seed)\n', 'learn4 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)']"
test.head(),0,['test.head()']
SETUP,0,"['import seaborn as sns\n', 'import matplotlib.pyplot as plt\n', 'import math\n', 'import csv\n', 'import sys\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n']"
CHECKPOINT ASSIGN = LabelEncoder() data['target_label'] = ASSIGN.fit_transform(data['target']) ASSIGN = ASSIGN.sample(frac=1).reset_index(drop=True) data,0,"['# creating instance of labelencoder\n', 'labelencoder = LabelEncoder()\n', '# Assigning numerical values and storing in another column\n', ""data['target_label'] = labelencoder.fit_transform(data['target'])\n"", 'data = data.sample(frac=1).reset_index(drop=True)\n', 'data']"
"ASSIGN=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[6000:8000] ASSIGN=34.78 ASSIGN=32.05 ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN['label']=ASSIGN.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1) ASSIGN=plugins.MarkerCluster().add_to(data_total_cars_4_map) for lat,lon,label in zip(ASSIGN.latitude,ASSIGN.longitude,ASSIGN.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN)",1,"[""data_total_cars_4=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[6000:8000]\n"", 'Long=34.78\n', 'Lat=32.05\n', 'data_total_cars_4_map=folium.Map([Lat,Long],zoom_start=12)\n', ""data_total_cars_4['label']=data_total_cars_4.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n"", '\n', 'data_total_cars_4_cars_map=plugins.MarkerCluster().add_to(data_total_cars_4_map)\n', 'for lat,lon,label in zip(data_total_cars_4.latitude,data_total_cars_4.longitude,data_total_cars_4.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_4_cars_map)\n', 'data_total_cars_4_map.add_child(data_total_cars_4_cars_map)\n', '\n']"
"ASSIGN = (data_features['BsmtFinType1'].notnull() & data_features['BsmtExposure'].isnull()) data_features.loc[ASSIGN,'BsmtExposure'] = 'No'",0,"[""condition2 = (data_features['BsmtFinType1'].notnull() & data_features['BsmtExposure'].isnull())\n"", ""data_features.loc[condition2,'BsmtExposure'] = 'No'""]"
CHECKPOINT df_final,0,"['#displaying the dataframe\n', 'df_final']"
ASSIGN = match_stats_df[match_stats_df.goals_info.isnull() == False],0,['odd_goals = match_stats_df[match_stats_df.goals_info.isnull() == False]']
total.isnull().sum(),0,['total.isnull().sum()']
"ASSIGN = {'num_leaves': 31, 'min_data_in_leaf': 30, 'objective':'binary', 'max_depth': 6, 'learning_rate': 0.01, ""boosting"": ""rf"", ""feature_fraction"": 0.9, ""bagging_freq"": 1, ""bagging_fraction"": 0.9 , ""bagging_seed"": 11, ""metric"": 'binary_logloss', ""lambda_l1"": 0.1, ""verbosity"": -1, ""random_state"": 2333}",0,"[""param = {'num_leaves': 31,\n"", ""         'min_data_in_leaf': 30, \n"", ""         'objective':'binary',\n"", ""         'max_depth': 6,\n"", ""         'learning_rate': 0.01,\n"", '         ""boosting"": ""rf"",\n', '         ""feature_fraction"": 0.9,\n', '         ""bagging_freq"": 1,\n', '         ""bagging_fraction"": 0.9 ,\n', '         ""bagging_seed"": 11,\n', '         ""metric"": \'binary_logloss\',\n', '         ""lambda_l1"": 0.1,\n', '         ""verbosity"": -1,\n', '         ""random_state"": 2333}']"
SETUP,0,"['#Import packages\n', 'import pandas as pd\n', 'import numpy as np\n', 'import seaborn as sns\n', 'import matplotlib.pyplot as plt']"
CHECKPOINT cbs.shape,0,['cbs.shape']
"sub_df.to_csv('my_submission.csv', index=False)",0,"[""sub_df.to_csv('my_submission.csv', index=False)""]"
ASSIGN = [str(x) for x in ASSIGN],0,"['#This cell changes all of my integers into strings which are easier to slice in order to locate the sections of the 990 that\n', '#might contain fraudulent data\n', 'finalData = [str(x) for x in finalData]']"
"CHECKPOINT SETUP ASSIGN = pd.read_csv(""..path"") dataset",0,"['import numpy as np\n', 'import matplotlib.pyplot as plt\n', 'import pandas as pd\n', 'dataset = pd.read_csv(""../input/wine-customer-segmentation/Wine.csv"")\n', 'dataset']"
"plotScatterMatrix(df1, 18, 10)",1,"['plotScatterMatrix(df1, 18, 10)']"
""""""" ASSIGN = [0.01,0.1, 1, 10,50, 100] ASSIGN = ['l2'] ASSIGN = ['newton-cg','lbfgs','liblinear'] ASSIGN = ['dict','balanced','None'] ASSIGN = [900,1000,1100,1200] ASSIGN = LogisticRegression() ASSIGN = {'C': [0.01,0.1, 1, 10,50, 100],'penalty' : ['l2'],'solver' : ['newton-cg','lbfgs','liblinear'],'class_weight':['dict','balanced','None'],'max_iter':[900,1000,1100,1200]} ASSIGN = GridSearchCV(Log, parameters, scoring='accuracy',cv =10) ASSIGN.fit(x_train, x_test) ASSIGN.best_params_ """"""",0,"['""""""\n', 'C = [0.01,0.1, 1, 10,50, 100]\n', ""penalty = ['l2']\n"", ""solver = ['newton-cg','lbfgs','liblinear']\n"", ""class_weight = ['dict','balanced','None']\n"", 'max_iter = [900,1000,1100,1200]\n', '\n', 'Log = LogisticRegression()\n', '\n', ""parameters = {'C': [0.01,0.1, 1, 10,50, 100],'penalty' : ['l2'],'solver' : ['newton-cg','lbfgs','liblinear'],'class_weight':['dict','balanced','None'],'max_iter':[900,1000,1100,1200]}\n"", '\n', ""log_regressor = GridSearchCV(Log, parameters, scoring='accuracy',cv =10)\n"", 'log_regressor.fit(x_train, x_test)\n', 'log_regressor.best_params_\n', '""""""']"
"SETUP CHECKPOINT X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42) print(X_train.shape,y_train.shape) print(X_test.shape,y_test.shape)",0,"['from sklearn.model_selection import train_test_split\n', 'X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n', 'print(X_train.shape,y_train.shape)\n', 'print(X_test.shape,y_test.shape)']"
"CHECKPOINT ASSIGN = (deaths_dict['Italy']path)*100 print('Death Percentage in Italy: ', ASSIGN) ASSIGN = (deaths_dict['Mainland China']path)*100 print('Death Percentage in China: ', ASSIGN) print(total_deaths)",0,"[""italian_death_perc = (deaths_dict['Italy']/total_deaths)*100\n"", ""print('Death Percentage in Italy: ', italian_death_perc)\n"", '\n', ""china_death_perc = (deaths_dict['Mainland China']/total_deaths)*100\n"", ""print('Death Percentage in China: ', china_death_perc)\n"", '\n', 'print(total_deaths)']"
"ASSIGN=plt.subplots(2,2,figsize=(20,20)) ASSIGN=data_larceny.year.value_counts() ASSIGN=data_larceny.year.value_counts().index sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,0]) ax[0,0].set_title(""The number of larceny by year"",size=20) ax[0,0].set_ylabel('counts',size=18) ax[0,0].set_xlabel('') ASSIGN=data_larceny.month.value_counts() ASSIGN=data_larceny.month.value_counts().index sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,1]) ax[0,1].set_title(""The number of larceny by month"",size=20) ax[0,1].set_ylabel('counts',size=18) ax[0,1].set_xlabel('') ASSIGN=data_larceny.hour.value_counts() ASSIGN=data_larceny.hour.value_counts().index sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[1,0]) ax[1,0].set_title(""The number of larceny by hour"",size=20) ax[1,0].set_ylabel('counts',size=18) ax[1,0].set_xlabel('') sns.scatterplot(ASSIGN=""GEO_LON"", ASSIGN=""GEO_LAT"", hue=""NEIGHBORHOOD_ID"",data=data_larceny,ax=ax[1,1]) ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,2.5)) ax[1,1].set_title(""The distribution of larency"",size=20) ax[1,1].set(xlabel='Longitude', ylabel='LATITUDE')",1,"['fig,ax=plt.subplots(2,2,figsize=(20,20))\n', 'y=data_larceny.year.value_counts()\n', 'x=data_larceny.year.value_counts().index\n', 'sns.barplot(x=x,y=y,ax=ax[0,0])\n', 'ax[0,0].set_title(""The number of larceny by year"",size=20)\n', ""ax[0,0].set_ylabel('counts',size=18)\n"", ""ax[0,0].set_xlabel('')\n"", '\n', '\n', 'y=data_larceny.month.value_counts()\n', 'x=data_larceny.month.value_counts().index\n', 'sns.barplot(x=x,y=y,ax=ax[0,1])\n', 'ax[0,1].set_title(""The number of larceny by month"",size=20)\n', ""ax[0,1].set_ylabel('counts',size=18)\n"", ""ax[0,1].set_xlabel('')\n"", '\n', 'y=data_larceny.hour.value_counts()\n', 'x=data_larceny.hour.value_counts().index\n', 'sns.barplot(x=x,y=y,ax=ax[1,0])\n', 'ax[1,0].set_title(""The number of larceny by hour"",size=20)\n', ""ax[1,0].set_ylabel('counts',size=18)\n"", ""ax[1,0].set_xlabel('')\n"", '\n', '\n', 'sns.scatterplot(x=""GEO_LON"", y=""GEO_LAT"", hue=""NEIGHBORHOOD_ID"",data=data_larceny,ax=ax[1,1])\n', 'ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,2.5))\n', 'ax[1,1].set_title(""The distribution of larency"",size=20)\n', ""ax[1,1].set(xlabel='Longitude', ylabel='LATITUDE')""]"
"ASSIGN = pd.DataFrame({ 'First Digit Expected': [0, .301, .176, .125, .097, .079, .067, .058, .051, .046], 'Second Digit Expected': [.12, .114, .109, .104, .100, .097, .093, .090, .088, .085], 'Third Digit Expected': [.102, .101, .101, .101, .100, .100, .099, .099, .099, .098] })",0,"['Benford_percentiles = pd.DataFrame({\n', ""    'First Digit Expected': [0, .301, .176, .125, .097, .079, .067, .058, .051, .046],\n"", ""    'Second Digit Expected': [.12, .114, .109, .104, .100, .097, .093, .090, .088, .085],\n"", ""    'Third Digit Expected': [.102, .101, .101, .101, .100, .100, .099, .099, .099, .098]\n"", '                                    })']"
"ASSIGN = Image.open('..path') ASSIGN = ASSIGN.resize((224,224)) plt.imshow(ASSIGN)",1,"[""img = Image.open('../input/sasuke/.jpg')\n"", 'img = img.resize((224,224))\n', 'plt.imshow(img)']"
CHECKPOINT ASSIGN = data_features.isnull().sum().sort_values(ascending=False) ASSIGN = ASSIGN[ASSIGN>0] data_features_na,0,"['data_features_na = data_features.isnull().sum().sort_values(ascending=False)\n', 'data_features_na = data_features_na[data_features_na>0]\n', 'data_features_na']"
"SETUP CHECKPOINT ASSIGN = pd.DataFrame({""a"":[2,3,5,7,11,13,17],""b"":[19,23,29,31,37,41,42],""c"":[21,26,34,38,48,54,59]}); print(ASSIGN)",0,"['#Q.1 Write a Pandas program to get the powers of an array values element-wise.\n', 'import pandas as pd\n', 'df = pd.DataFrame({""a"":[2,3,5,7,11,13,17],""b"":[19,23,29,31,37,41,42],""c"":[21,26,34,38,48,54,59]});\n', 'print(df)']"
"CHECKPOINT ASSIGN = ['WindGustDir', 'WindDir9am', 'WindDir3pm','Date','Sunshine','RISK_MM'] ASSIGN = ASSIGN.drop(drop_columns_list, axis=1) print(ASSIGN.shape) ASSIGN['RainToday'].replace({'No':0,'Yes':1},inplace=True) ASSIGN['RainTomorrow'].replace({'No':0,'Yes':1},inplace=True)",0,"[""drop_columns_list = ['WindGustDir', 'WindDir9am', 'WindDir3pm','Date','Sunshine','RISK_MM']\n"", 'pd_data = pd_data.drop(drop_columns_list, axis=1)\n', 'print(pd_data.shape)\n', '\n', ""pd_data['RainToday'].replace({'No':0,'Yes':1},inplace=True)\n"", ""pd_data['RainTomorrow'].replace({'No':0,'Yes':1},inplace=True)""]"
mnist.head(),0,['mnist.head()']
"sample_submission.isFraud = test_pred[0][:,1].numpy() sample_submission.head()",0,"['sample_submission.isFraud = test_pred[0][:,1].numpy()\n', 'sample_submission.head()']"
ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path'),0,"[""tourney_result = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WDataFiles_Stage1/WNCAATourneyCompactResults.csv')\n"", ""tourney_seed = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WDataFiles_Stage1/WNCAATourneySeeds.csv')""]"
CHECKPOINT match_bets_df,0,['match_bets_df']
"ASSIGN=pd.DataFrame(ASSIGN.values, columns =[""NAME"",""PRICE"",""CUSINE_CATEGORY"",""CITY"",""REGION"",""URL"",""PAGE NO"",""CUSINE TYPE"",""TIMING"",""RATING_TYPE"",""RATING"",""VOTES""])",0,"['#adding the header columns\n', 'df_final=pd.DataFrame(df_final.values, columns =[""NAME"",""PRICE"",""CUSINE_CATEGORY"",""CITY"",""REGION"",""URL"",""PAGE NO"",""CUSINE TYPE"",""TIMING"",""RATING_TYPE"",""RATING"",""VOTES""])\n']"
"ASSIGN = transforms.Compose([ transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) ASSIGN = transforms.Compose([ transforms.Resize(224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) ASSIGN = torchvision.datasets.CIFAR10(root='.', train=True,download=False, transform=train_transform) ASSIGN = random_split(traindata,[trainset_size,50000-trainset_size]) ASSIGN = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True) ASSIGN = torch.utils.data.DataLoader(valset, batch_size=batch_size,shuffle=False) ASSIGN = torchvision.datasets.CIFAR10(root='.', train=False,download=False, transform=test_transform) ASSIGN = torch.utils.data.DataLoader(testset, batch_size=64,shuffle=False) ASSIGN = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')",0,"['train_transform = transforms.Compose([\n', '    transforms.Resize(img_size),\n', '#     transforms.RandomHorizontalFlip(p=.40),\n', '#     transforms.RandomRotation(30),\n', '    transforms.ToTensor(),\n', '    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n', '\n', 'test_transform = transforms.Compose([\n', '    transforms.Resize(224),\n', '    transforms.ToTensor(),\n', '    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n', '\n', ""traindata = torchvision.datasets.CIFAR10(root='.', train=True,download=False, transform=train_transform)\n"", '\n', 'trainset,valset = random_split(traindata,[trainset_size,50000-trainset_size])\n', '\n', 'trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True)\n', 'valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,shuffle=False)\n', '\n', ""testset = torchvision.datasets.CIFAR10(root='.', train=False,download=False, transform=test_transform)\n"", 'testloader = torch.utils.data.DataLoader(testset, batch_size=64,shuffle=False)\n', '\n', ""classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n""]"
"plt.figure(figsize=(30,30)) ASSIGN=0 for i in ['2010','2011','2012','2013','2014','2015','2016','2017','2018']: ASSIGN+=1 plt.subplot(3,3,ASSIGN) ASSIGN=data_import[data_import.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10].index ASSIGN=data_import[data_import.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10] sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN) plt.title('Top 10 value of import in '+i,size=24) plt.xlabel('million US$')",1,"['plt.figure(figsize=(30,30))\n', 'j=0\n', ""for i in ['2010','2011','2012','2013','2014','2015','2016','2017','2018']:\n"", '    j+=1\n', '    plt.subplot(3,3,j)\n', ""    y=data_import[data_import.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10].index\n"", ""    x=data_import[data_import.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10]\n"", '    sns.barplot(x=x,y=y)\n', ""    plt.title('Top 10 value of import in '+i,size=24)\n"", ""    plt.xlabel('million US$')""]"
train.head(),0,['train.head()']
sub_df.head(),0,['sub_df.head()']
SETUP,0,"['import pandas as pd\n', 'import numpy as np\n', 'from tqdm import tqdm\n', 'from sklearn.utils import shuffle\n', 'import torch\n', 'from torch import nn\n', 'from torch.utils.data import Dataset\n', 'import torch.nn.functional as F\n', 'from torch.autograd import Variable\n', 'from sklearn.metrics import mean_squared_error\n', 'from datetime import datetime\n', 'from datetime import timedelta\n', 'from sklearn import preprocessing\n']"
"learn.fit(10,lr=1e-2)",0,"['learn.fit(10,lr=1e-2)\n', '#learn.fit(30,lr=3e-3)']"
"ASSIGN = ASSIGN.view(1,ASSIGN.shape[0],ASSIGN.shape[1],ASSIGN.shape[2]) ASSIGN = ASSIGN.view(1,ASSIGN.shape[0],ASSIGN.shape[1],ASSIGN.shape[2]) ASSIGN = ASSIGN.view(1,ASSIGN.shape[0],ASSIGN.shape[1],ASSIGN.shape[2]) ASSIGN = ASSIGN.view(1,ASSIGN.shape[0],ASSIGN.shape[1],ASSIGN.shape[2])",0,"['reimg = reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2])\n', 'reimg_28 = reimg_28.view(1,reimg_28.shape[0],reimg_28.shape[1],reimg_28.shape[2])\n', 'reimg_16 = reimg_16.view(1,reimg_16.shape[0],reimg_16.shape[1],reimg_16.shape[2])\n', 'reimg_8 = reimg_8.view(1,reimg_8.shape[0],reimg_8.shape[1],reimg_8.shape[2])']"
"SETUP ASSIGN = Fold(input_df, input_X, seed=555); torch_seed(ASSIGN.seed) ASSIGN = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)",0,"['%%time\n', 'fold = Fold(input_df, input_X, seed=555); torch_seed(fold.seed)\n', 'learn2 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)']"
CHECKPOINT scaled_data.shape,0,['scaled_data.shape']
"plt.figure(figsize=(15,6)) sns.distplot(points_df['pitch'], bins=500); plt.xlabel('pitch') plt.show()",1,"['plt.figure(figsize=(15,6))\n', ""sns.distplot(points_df['pitch'], bins=500);\n"", ""plt.xlabel('pitch')\n"", 'plt.show()']"
CHECKPOINT print(X_train.shape) print(Y_train.shape) print(X_val.shape) print(Y_val.shape) print(X_test.shape) print(Y_test.shape),0,"['print(X_train.shape)\n', 'print(Y_train.shape)\n', 'print(X_val.shape)\n', 'print(Y_val.shape)\n', 'print(X_test.shape)\n', 'print(Y_test.shape)']"
"ASSIGN = pd.DataFrame() for col in ['x', 'y', 'z', 'yaw', 'pitch', 'roll']: ASSIGN = [] for ps in train['PredictionString']: ASSIGN = str2coords(ps) ASSIGN += [c[col] for c in ASSIGN] ASSIGN[col] = ASSIGN",0,"['points_df = pd.DataFrame()\n', ""for col in ['x', 'y', 'z', 'yaw', 'pitch', 'roll']:\n"", '    arr = []\n', ""    for ps in train['PredictionString']:\n"", '        coords = str2coords(ps)\n', '        arr += [c[col] for c in coords]\n', '    points_df[col] = arr']"
"''' ASSIGN = [4,5,6] ASSIGN = [1,1.5,2.0,2.5,3] ASSIGN = [.6,.8,1,1.2] ASSIGN = [.6,.8,1,1.2] ASSIGN = [.5,.01] ASSIGN = [5,6,7,8] ASSIGN = XGBClassifier() ASSIGN = {'min_child_weight': [4,5,6],'gamma': [1,1.5,2.0,2.5,3],'subsample':[.6,.8,1,1.2],'colsample_bytree':[.6,.8,1,1.2], 'ASSIGN':[.5,.01],'ASSIGN':[5,6,7,8]} ASSIGN = GridSearchCV(XB, parameters, scoring='accuracy' ,cv =5) ASSIGN.fit(x_train, x_test) ASSIGN.best_params_ '''",0,"[""'''\n"", 'min_child_weight = [4,5,6]\n', 'gamma = [1,1.5,2.0,2.5,3]\n', 'subsample = [.6,.8,1,1.2]\n', 'colsample_bytree = [.6,.8,1,1.2]\n', 'eta = [.5,.01]\n', '\n', 'max_depth = [5,6,7,8]\n', '\n', 'XB = XGBClassifier()\n', '\n', ""parameters = {'min_child_weight': [4,5,6],'gamma': [1,1.5,2.0,2.5,3],'subsample':[.6,.8,1,1.2],'colsample_bytree':[.6,.8,1,1.2],\n"", ""             'eta':[.5,.01],'max_depth':[5,6,7,8]}\n"", '\n', ""XBClassifier = GridSearchCV(XB, parameters, scoring='accuracy' ,cv =5)\n"", 'XBClassifier.fit(x_train, x_test)\n', 'XBClassifier.best_params_\n', ""'''""]"
"ASSIGN = train['Id'] ASSIGN = test['Id'] train.drop(""Id"", axis = 1, inplace = True) test.drop(""Id"", axis = 1, inplace = True)",0,"[""#Save the 'Id' column\n"", ""train_ID = train['Id']\n"", ""test_ID = test['Id']\n"", '\n', ""#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n"", 'train.drop(""Id"", axis = 1, inplace = True)\n', 'test.drop(""Id"", axis = 1, inplace = True)']"
"sns.catplot(x='Year', y='Total Atrocities', data=total ,height = 5, aspect = 4,kind = 'bar')",1,"[""sns.catplot(x='Year', y='Total Atrocities', data=total ,height = 5, aspect = 4,kind = 'bar')""]"
ASSIGN=data[data.neighbourhood_group=='Queens'] ASSIGN.head(),0,"[""data_Queens=data[data.neighbourhood_group=='Queens']\n"", 'data_Queens.head()']"
CHECKPOINT final_submit,0,['final_submit']
"DTree.fit(x_train,y_train) ASSIGN=DTree.predict(x_test)",0,"['DTree.fit(x_train,y_train)\n', 'y_predict=DTree.predict(x_test)']"
CHECKPOINT ASSIGN = ASSIGN.fillna(-999) ASSIGN = ASSIGN.fillna(-999) for f in ASSIGN.columns: if ASSIGN[f].dtype=='object' or ASSIGN[f].dtype=='object': print(f) ASSIGN = preprocessing.LabelEncoder() ASSIGN.fit(list(ASSIGN[f].values) + list(ASSIGN[f].values)) ASSIGN = lbl.transform(list(ASSIGN.values)) ASSIGN = lbl.transform(list(ASSIGN.values)),0,"['train = train.fillna(-999)\n', 'test = test.fillna(-999)\n', 'for f in test.columns:\n', ""    if train[f].dtype=='object' or test[f].dtype=='object': \n"", '        print(f)\n', '        lbl = preprocessing.LabelEncoder()\n', '        lbl.fit(list(train[f].values) + list(test[f].values))\n', '        train[f] = lbl.transform(list(train[f].values))\n', '        test[f] = lbl.transform(list(test[f].values)) ']"
"CHECKPOINT ASSIGN = logisticRegr.ASSIGN(X_test_pca, y_test) print(ASSIGN)",0,"['score = logisticRegr.score(X_test_pca, y_test)\n', 'print(score)']"
ASSIGN = X.UrbanPopRate.apply(lambda x:get_percent(x)) X_test['UrbanPopRate'] = X_test.UrbanPopRate.apply(lambda x:get_percent(x)),0,"[""X['UrbanPopRate'] = X.UrbanPopRate.apply(lambda x:get_percent(x))\n"", ""X_test['UrbanPopRate'] = X_test.UrbanPopRate.apply(lambda x:get_percent(x))""]"
"SETUP CHECKPOINT def squeeze_excitation_layer(x, out_dim, ratio = 4, concate = True): ''' SE module performs inter-channel weighting. ''' ASSIGN = GlobalAveragePooling2D()(x) ASSIGN = Dense(units=out_dim path)(squeeze) ASSIGN = Activation('relu')(ASSIGN) ASSIGN = Dense(units=out_dim)(ASSIGN) ASSIGN = Activation('sigmoid')(ASSIGN) print(ASSIGN.shape) ASSIGN = Reshape((1, 1, out_dim))(ASSIGN) ASSIGN = multiply([x, excitation]) if concate: ASSIGN = concatenate([ASSIGN, x],axis=3) return scale",0,"['from keras.layers import Activation, Reshape, Lambda, dot, add\n', 'from keras.layers import Conv1D, Conv2D, Conv3D\n', 'from keras.layers import MaxPool1D,GlobalAveragePooling2D,Dense,multiply,Activation,concatenate\n', 'from keras import backend as K\n', '\n', '\n', 'def squeeze_excitation_layer(x, out_dim, ratio = 4, concate = True):\n', ""    '''\n"", '    SE module performs inter-channel weighting.\n', ""    '''\n"", '    squeeze = GlobalAveragePooling2D()(x)\n', '\n', '    excitation = Dense(units=out_dim //ratio)(squeeze)\n', ""    excitation = Activation('relu')(excitation)\n"", '    excitation = Dense(units=out_dim)(excitation)\n', ""    excitation = Activation('sigmoid')(excitation)\n"", '    print(excitation.shape)\n', '    excitation = Reshape((1, 1, out_dim))(excitation)\n', '\n', '    scale = multiply([x, excitation])\n', '\n', '    if concate:\n', '        scale = concatenate([scale, x],axis=3)\n', '    return scale']"
"DT.fit(x_train,x_test) DT.score(y_train,y_test)",0,"['DT.fit(x_train,x_test)\n', 'DT.score(y_train,y_test)']"
data.isnull().sum(),0,['data.isnull().sum()']
"ASSIGN = (data_features['MasVnrType'].isnull() & data_features['MasVnrArea'].notnull()) data_features.loc[ASSIGN,['MasVnrType','MasVnrArea']]",0,"[""condition6 = (data_features['MasVnrType'].isnull() & data_features['MasVnrArea'].notnull())\n"", ""data_features.loc[condition6,['MasVnrType','MasVnrArea']]""]"
"ASSIGN = pd.read_csv(""..path"")",0,"['train = pd.read_csv(""../input/train.csv"")']"
learn.unfreeze() learn.lr_find() learn.recorder.plot(suggestion=True),1,"['# look again at learning rate curve\n', 'learn.unfreeze()\n', 'learn.lr_find()\n', 'learn.recorder.plot(suggestion=True)']"
"CHECKPOINT ASSIGN = go.Figure(data=[ go.Line(name='Confirmed', x=china[china['Provincepath']=='Hubei']['date'], y=china[china['Provincepath']=='Hubei']['Confirmed']), go.Line(name='Deaths', x=china[china['Provincepath']=='Hubei']['date'], y=china[china['Provincepath']=='Hubei']['Deaths']), go.Line(name='Recovered', x=china[china['Provincepath']=='Hubei']['date'], y=china[china['Provincepath']=='Hubei']['Recovered']), ]) ASSIGN.update_layout( ASSIGN=""Number of Confirmed,Recovered,death in Huibel for each day"", ASSIGN=""date"", ASSIGN=""People"", ASSIGN=dict( ASSIGN=""Courier New, monospace"", ASSIGN=18, ASSIGN=""#7f7f7f"" )) fig",1,"['fig = go.Figure(data=[\n', ""    go.Line(name='Confirmed', x=china[china['Province/State']=='Hubei']['date'], y=china[china['Province/State']=='Hubei']['Confirmed']),\n"", ""    go.Line(name='Deaths', x=china[china['Province/State']=='Hubei']['date'], y=china[china['Province/State']=='Hubei']['Deaths']),\n"", ""    go.Line(name='Recovered', x=china[china['Province/State']=='Hubei']['date'], y=china[china['Province/State']=='Hubei']['Recovered']),\n"", '])\n', '\n', 'fig.update_layout(\n', '    title=""Number of Confirmed,Recovered,death in Huibel for each day"",\n', '    xaxis_title=""date"",\n', '    yaxis_title=""People"",\n', '    font=dict(\n', '        family=""Courier New, monospace"",\n', '        size=18,\n', '        color=""#7f7f7f""\n', '    ))\n', 'fig']"
"CHECKPOINT plt.figure(figsize = (10, 10)) ASSIGN = np.random.randint(0, x_train.shape[0], 8) print(ASSIGN) for index,im_index in enumerate(ASSIGN): plt.subplot(4, 4, index+1) plt.imshow(x_train[im_index], cmap = 'gray', interpolation = 'none') plt.title('Class %d' % y_train[im_index]) plt.tight_layout()",1,"['# Show image of training data\n', 'plt.figure(figsize = (10, 10)) # set size of figure 10x10\n', 'rand_indexes = np.random.randint(0, x_train.shape[0], 8) # select 8 digits(0~9) randomly \n', 'print(rand_indexes)\n', 'for index,im_index in enumerate(rand_indexes):\n', '    plt.subplot(4, 4, index+1)\n', ""    plt.imshow(x_train[im_index], cmap = 'gray', interpolation = 'none')\n"", ""    plt.title('Class %d' % y_train[im_index])\n"", 'plt.tight_layout()']"
SETUP,0,['%matplotlib inline']
ASSIGN=ASSIGN.dropna(subset=['value']),0,"[""data_export=data_export.dropna(subset=['value'])""]"
SETUP,0,"['from PIL import Image\n', 'import matplotlib.pyplot as plt']"
CHECKPOINT for i in cat_to_names: ASSIGN == '11': print(cat_to_names['11']),0,"['for i in cat_to_names:\n', ""    if i == '11':\n"", ""        print(cat_to_names['11'])""]"
"CHECKPOINT ASSIGN = pd.merge(ASSIGN, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left') ASSIGN.rename(columns={'Score':'WScoreT'}, inplace=True) ASSIGN = ASSIGN.drop('TeamID', axis=1) ASSIGN = pd.merge(ASSIGN, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left') ASSIGN.rename(columns={'Score':'LScoreT'}, inplace=True) ASSIGN = ASSIGN.drop('TeamID', axis=1) tourney_result",0,"[""tourney_result = pd.merge(tourney_result, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n"", ""tourney_result.rename(columns={'Score':'WScoreT'}, inplace=True)\n"", ""tourney_result = tourney_result.drop('TeamID', axis=1)\n"", ""tourney_result = pd.merge(tourney_result, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n"", ""tourney_result.rename(columns={'Score':'LScoreT'}, inplace=True)\n"", ""tourney_result = tourney_result.drop('TeamID', axis=1)\n"", 'tourney_result']"
final['Ticket_length'].value_counts(),0,"[""final['Ticket_length'].value_counts()""]"
data2.head(),0,['data2.head()']
"SETUP warnings.filterwarnings(""ignore"")",0,"['import pandas as pd\n', 'import numpy as np\n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline\n', 'import warnings\n', 'import seaborn as sns\n', 'warnings.filterwarnings(""ignore"")']"
"plotCorrelationMatrix(df1, 8)",1,"['plotCorrelationMatrix(df1, 8)']"
"ASSIGN = get_data_loaders(TRAIN_DATA_PATH, VAL_DATA_PATH)",0,"['train_loader, val_loader, train_class_names, val_class_names = get_data_loaders(TRAIN_DATA_PATH, VAL_DATA_PATH)']"
ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path') ASSIGN.head(5),0,"[""train = pd.read_csv('../input/train.csv')\n"", ""test = pd.read_csv('../input/test.csv')\n"", 'train.head(5)']"
"DD=data_2019[data_2019.Variable=='Driving Distance - (TOTAL DISTANCE)'].iloc[:,[0,4,6]] DD['Value_new']=DD['Value'].apply(lambda x:''.join(x.split(','))) DD.Value_new=DD.Value_new.astype(int) DD=DD.drop(columns='Value') plt.figure(figsize=(22,22)) for i in range(len(DD.month.unique())): ASSIGN=DD[DD.month==DD.month.unique()[i]] plt.subplot(4,2,i+1) ASSIGN.groupby('Player Name')['Value_new'].max().sort_values(ascending=False)[:10].sort_values().plot(kind='barh') plt.title(""Top 10 player by Total Driving Distance in month""+DD.month.unique()[i],size=20) plt.xlabel('meters') plt.ylabel('')",1,"[""DD=data_2019[data_2019.Variable=='Driving Distance - (TOTAL DISTANCE)'].iloc[:,[0,4,6]]\n"", ""DD['Value_new']=DD['Value'].apply(lambda x:''.join(x.split(',')))\n"", 'DD.Value_new=DD.Value_new.astype(int)\n', ""DD=DD.drop(columns='Value')\n"", 'plt.figure(figsize=(22,22))\n', 'for i in range(len(DD.month.unique())):\n', '    DD_month=DD[DD.month==DD.month.unique()[i]]\n', '    plt.subplot(4,2,i+1)\n', ""    DD_month.groupby('Player Name')['Value_new'].max().sort_values(ascending=False)[:10].sort_values().plot(kind='barh')\n"", '    plt.title(""Top 10 player by Total Driving Distance in month""+DD.month.unique()[i],size=20)\n', ""    plt.xlabel('meters')\n"", ""    plt.ylabel('')""]"
"CHECKPOINT ASSIGN = rdf.score(train_x, train_y) print('training accuracy: %.5f' % ASSIGN) ASSIGN =rdf.predict(test_x) X=accuracy_score(test_y, ASSIGN) print('test accuracy: %.5f' % X) ASSIGN = metrics.roc_curve(test_y, predict_y, pos_label=1) print('auc: %.5f' % metrics.auc(fpr, tpr)) '''scores = cross_val_score(rdf,train_x,train_y,cv=5,scoring='accuracy') print(scores) print('Cross validation: %.5f'%scores.mean())'''",0,"['acc_log = rdf.score(train_x, train_y)\n', ""print('training accuracy: %.5f' % acc_log)\n"", '\n', 'predict_y =rdf.predict(test_x)\n', 'X=accuracy_score(test_y, predict_y)\n', ""print('test accuracy: %.5f' % X)\n"", '#auc\n', 'fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n', ""print('auc: %.5f' % metrics.auc(fpr, tpr))\n"", '\n', '#Cross validation\n', '#運算時間太長\n', ""'''scores = cross_val_score(rdf,train_x,train_y,cv=5,scoring='accuracy')\n"", 'print(scores)\n', ""print('Cross validation: %.5f'%scores.mean())'''""]"
"def feature_importance(learner): ASSIGN = learner.ASSIGN.train_ds.x ASSIGN = data.ASSIGN ASSIGN = data.ASSIGN ASSIGN=np.array([learner.loss_func(learner.pred_batch(batch=(x,y.to(""cpu""))), y.to(""cpu"")) for x,y in iter(learner.data.valid_dl)]).mean() ASSIGN=dict() ASSIGN=[cat_names, cont_names] for j, t in enumerate(ASSIGN): for i, c in enumerate(t): ASSIGN=[] for x,y in iter(learner.ASSIGN.valid_dl): ASSIGN=x[j][:,i] ASSIGN = torch.randperm(col.nelement()) x[j][:,i] = ASSIGN.view(-1)[ASSIGN].view(ASSIGN.size()) ASSIGN=ASSIGN.to('cpu') ASSIGN.append(learner.loss_func(learner.pred_batch(batch=(x,ASSIGN)), ASSIGN)) SLICE=np.array(ASSIGN).mean()-ASSIGN ASSIGN = sorted(fi.items(), key=lambda kv: kv[1], reverse=True) return pd.DataFrame({'cols': [l for l, v in ASSIGN], 'imp': np.log1p([v for l, v in ASSIGN])})",0,"['# Feture importance extraction from NN weights\n', 'def feature_importance(learner): \n', '  # based on: https://medium.com/@mp.music93/neural-networks-feature-importance-with-fastai-5c393cf65815\n', '    data = learner.data.train_ds.x\n', '    cat_names = data.cat_names\n', '    cont_names = data.cont_names\n', '    loss0=np.array([learner.loss_func(learner.pred_batch(batch=(x,y.to(""cpu""))), y.to(""cpu"")) for x,y in iter(learner.data.valid_dl)]).mean()\n', '    fi=dict()\n', '    types=[cat_names, cont_names]\n', '    for j, t in enumerate(types):\n', '      for i, c in enumerate(t):\n', '        loss=[]\n', '        for x,y in iter(learner.data.valid_dl):\n', '          col=x[j][:,i] \n', '          idx = torch.randperm(col.nelement())\n', '          x[j][:,i] = col.view(-1)[idx].view(col.size())\n', ""          y=y.to('cpu')\n"", '          loss.append(learner.loss_func(learner.pred_batch(batch=(x,y)), y))\n', '        fi[c]=np.array(loss).mean()-loss0\n', '    d = sorted(fi.items(), key=lambda kv: kv[1], reverse=True)\n', ""    return pd.DataFrame({'cols': [l for l, v in d], 'imp': np.log1p([v for l, v in d])})""]"
"ASSIGN == ""__main__"": ASSIGN = 100 ASSIGN = np.linspace(0, 6*np.pi, N) ASSIGN = np.sin(X) plt.plot(ASSIGN, ASSIGN) plt.show() for deg in (5, 6, 7, 8, 9): fit_and_display(ASSIGN, ASSIGN, 10, deg) plot_train_vs_test_curves(ASSIGN, ASSIGN)",1,"['if __name__ == ""__main__"":\n', '    # make up some data and plot it\n', '    N = 100\n', '    X = np.linspace(0, 6*np.pi, N)\n', '    Y = np.sin(X)\n', '\n', '    plt.plot(X, Y)\n', '    plt.show()\n', '\n', '    for deg in (5, 6, 7, 8, 9):\n', '        fit_and_display(X, Y, 10, deg)\n', '    plot_train_vs_test_curves(X, Y)']"
"SETUP def EfficientUNet(input_shape): ASSIGN = efn.EfficientNetB4( ASSIGN=None, ASSIGN=False, ASSIGN=ASSIGN ) ASSIGN = backbone.ASSIGN ASSIGN = backbone.input ASSIGN = backbone.get_layer('stem_activation').output ASSIGN = backbone.get_layer('block2d_add').output ASSIGN = backbone.get_layer('block3d_add').output ASSIGN = backbone.get_layer('block5f_add').output ASSIGN = backbone.get_layer('block7b_add').output ASSIGN = H([x00, U(x10)], 'X01') ASSIGN = H([x10, U(x20)], 'X11') ASSIGN = H([x20, U(x30)], 'X21') ASSIGN = H([x30, U(x40)], 'X31') ASSIGN = H([x40, U(x50)], 'X41') ASSIGN = H([x00, x01, U(x11)], 'X02') ASSIGN = H([x11, U(x21)], 'X12') ASSIGN = H([x21, U(x31)], 'X22') ASSIGN = H([x31, U(x41)], 'X32') ASSIGN = H([x00, x01, x02, U(x12)], 'X03') ASSIGN = H([x12, U(x22)], 'X13') ASSIGN = H([x22, U(x32)], 'X23') ASSIGN = H([x00, x01, x02, x03, U(x13)], 'X04') ASSIGN = H([x13, U(x23)], 'X14') ASSIGN = H([x00, x01, x02, x03, x04, U(x14)], 'X05') ASSIGN = Concatenate(name='bridge')([x01, x02, x03, x04, x05]) ASSIGN = Conv2D(4, (3,3), padding=""same"", name='final_output', activation=""sigmoid"")(ASSIGN) return Model(inputs=ASSIGN, outputs=ASSIGN) ASSIGN = EfficientUNet((320, 480 ,3)) ASSIGN.summary()",0,"['import efficientnet.keras as efn \n', 'def EfficientUNet(input_shape):\n', '    backbone = efn.EfficientNetB4(\n', '        weights=None,\n', '        include_top=False,\n', '        input_shape=input_shape\n', '    )\n', '    \n', '    input = backbone.input\n', '    x00 = backbone.input  # (256, 512, 3)\n', ""    x10 = backbone.get_layer('stem_activation').output  # (128, 256, 4)\n"", ""    x20 = backbone.get_layer('block2d_add').output  # (64, 128, 32)\n"", ""    x30 = backbone.get_layer('block3d_add').output  # (32, 64, 56)\n"", ""    x40 = backbone.get_layer('block5f_add').output  # (16, 32, 160)\n"", ""    x50 = backbone.get_layer('block7b_add').output  # (8, 16, 448)\n"", '    \n', ""    x01 = H([x00, U(x10)], 'X01')\n"", ""    x11 = H([x10, U(x20)], 'X11')\n"", ""    x21 = H([x20, U(x30)], 'X21')\n"", ""    x31 = H([x30, U(x40)], 'X31')\n"", ""    x41 = H([x40, U(x50)], 'X41')\n"", '    \n', ""    x02 = H([x00, x01, U(x11)], 'X02')\n"", ""    x12 = H([x11, U(x21)], 'X12')\n"", ""    x22 = H([x21, U(x31)], 'X22')\n"", ""    x32 = H([x31, U(x41)], 'X32')\n"", '    \n', ""    x03 = H([x00, x01, x02, U(x12)], 'X03')\n"", ""    x13 = H([x12, U(x22)], 'X13')\n"", ""    x23 = H([x22, U(x32)], 'X23')\n"", '    \n', ""    x04 = H([x00, x01, x02, x03, U(x13)], 'X04')\n"", ""    x14 = H([x13, U(x23)], 'X14')\n"", '    \n', ""    x05 = H([x00, x01, x02, x03, x04, U(x14)], 'X05')\n"", '    \n', ""    x_out = Concatenate(name='bridge')([x01, x02, x03, x04, x05])\n"", '    x_out = Conv2D(4, (3,3), padding=""same"", name=\'final_output\', activation=""sigmoid"")(x_out)\n', '    \n', '    return Model(inputs=input, outputs=x_out)\n', '\n', 'model = EfficientUNet((320, 480 ,3))\n', '\n', 'model.summary()']"
"gs.fit(X_train, y_train)",0,"['#fit gs\n', 'gs.fit(X_train, y_train)']"
"CHECKPOINT ASSIGN = model4.predict(X_test) ASSIGN= mean_squared_error(y_test, y_pred4, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ASSIGN)))",0,"['y_pred4 = model4.predict(X_test)\n', '\n', 'val= mean_squared_error(y_test, y_pred4, squared=False)\n', 'val4 = str(round(val, 4))\n', '\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred4)))\n""]"
"sns.catplot(x = 'Sex',y='Survived',hue = 'Pclass', kind = 'bar', data = df1, col = 'Pclass', color = 'purple')",1,"[""sns.catplot(x = 'Sex',y='Survived',hue = 'Pclass', kind = 'bar', data = df1, col = 'Pclass', color = 'purple')""]"
"SETUP ASSIGN = {'num_leaves': 256, 'min_child_samples': 79, 'objective': 'binary', 'max_depth': 13, 'learning_rate': 0.03, ""boosting_type"": ""gbdt"", ""subsample_freq"": 3, ""subsample"": 0.9, ""bagging_seed"": 11, ""metric"": 'auc', ""verbosity"": -1, 'reg_alpha': 0.3, 'reg_lambda': 0.3, 'colsample_bytree': 0.9, 'tree_method':'gpu_hist' } ASSIGN = train_model_classification(X=train, X_test=X_test, y=y_train, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True, ASSIGN=500, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1)",0,"['import time\n', ""params = {'num_leaves': 256,\n"", ""          'min_child_samples': 79,\n"", ""          'objective': 'binary',\n"", ""          'max_depth': 13,\n"", ""          'learning_rate': 0.03,\n"", '          ""boosting_type"": ""gbdt"",\n', '          ""subsample_freq"": 3,\n', '          ""subsample"": 0.9,\n', '          ""bagging_seed"": 11,\n', '          ""metric"": \'auc\',\n', '          ""verbosity"": -1,\n', ""          'reg_alpha': 0.3,\n"", ""          'reg_lambda': 0.3,\n"", ""          'colsample_bytree': 0.9,\n"", ""          # 'categorical_feature': cat_cols\n"", ""          'tree_method':'gpu_hist'\n"", '          }\n', ""result_dict_lgb = train_model_classification(X=train, X_test=X_test, y=y_train, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n"", ""                                             verbose=500, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1)""]"
"ASSIGN=train_datagen.flow_from_dataframe(val,directory='path', ASSIGN=(384,384), ASSIGN=""image_id"", ASSIGN=['healthy','multiple_diseases','rust','scab'], ASSIGN='raw', ASSIGN=False, ASSIGN=32, )",0,"[""val_generator=train_datagen.flow_from_dataframe(val,directory='/kaggle/input/plant-pathology-2020-fgvc7/images/',\n"", '                                                      target_size=(384,384),\n', '                                                      x_col=""image_id"",\n', ""                                                      y_col=['healthy','multiple_diseases','rust','scab'],\n"", ""                                                      class_mode='raw',\n"", '                                                      shuffle=False,\n', '                                                      batch_size=32,\n', '                                                  )']"
"SETUP CHECKPOINT estimator.append(('GNB',GaussianNB())) ASSIGN = cross_val_score(GNB,x_train,x_test,ASSIGN=10) ASSIGN = cv.mean() accuracy.append(ASSIGN) print(ASSIGN) print(ASSIGN.mean())",0,"['GNB = GaussianNB()\n', ""estimator.append(('GNB',GaussianNB()))\n"", 'cv = cross_val_score(GNB,x_train,x_test,cv=10)\n', 'accuracy5 = cv.mean()\n', 'accuracy.append(accuracy5)\n', 'print(cv)\n', 'print(cv.mean())']"
"CHECKPOINT ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path') ASSIGN = train.drop([""label""], axis=1) ASSIGN = ASSIGN.values.astype('int32') print()",0,"[""train = pd.read_csv('../input/train.csv')\n"", ""test = pd.read_csv('../input/test.csv')\n"", '\n', 'X = train.drop([""label""], axis=1)\n', ""X = X.values.astype('int32')\n"", 'print(""loaded"")']"
"CHECKPOINT def train(epochs): ASSIGN = [] ASSIGN = [] for epoch in range(epochs): ASSIGN = 0 ASSIGN = 0 ASSIGN = 0 model.train() ASSIGN = 0 for inputs, labels in train_loader: ASSIGN = inputs.to(device), labels.to(device) optimizer.zero_grad() ASSIGN = model.forward(inputs) ASSIGN = criterion(output, labels) ASSIGN.backward() optimizer.step() ASSIGN += ASSIGN.item() ASSIGN += 1 model.eval() ASSIGN = 0 with torch.no_grad(): for ASSIGN in val_loader: ASSIGN = inputs.to(device), labels.to(device) ASSIGN = model.forward(inputs) ASSIGN = criterion(output, labels) ASSIGN += ASSIGN.item() ASSIGN = torch.exp(ASSIGN) ASSIGN = output.topk(1, dim=1) ASSIGN = top_class == labels.view(*top_class.shape) ASSIGN += torch.mean(ASSIGN.type(torch.FloatTensor)).item() ASSIGN += 1 ASSIGN = train_losspath(train_loader) ASSIGN = val_losspath(val_loader) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) print('Accuracy: ', accuracypath(val_loader)) print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(epoch, ASSIGN, ASSIGN)) return train_losses, valid_losses",0,"['def train(epochs):\n', '    train_losses = []\n', '    valid_losses = []\n', '    \n', '    for epoch in range(epochs):\n', '        train_loss = 0\n', '        val_loss = 0\n', '        accuracy = 0\n', '\n', '        # Training the model\n', '        model.train()\n', '        counter = 0\n', '        for inputs, labels in train_loader:\n', '            # Move to device\n', '            inputs, labels = inputs.to(device), labels.to(device)\n', '            # Clear optimizers\n', '            optimizer.zero_grad()\n', '            # Forward pass\n', '            output = model.forward(inputs)\n', '            # Loss\n', '            loss = criterion(output, labels)\n', '            # Calculate gradients (backpropogation)\n', '            loss.backward()\n', '            # Adjust parameters based on gradients\n', '            optimizer.step()\n', ""            # Add the loss to the training set's rnning loss\n"", '            train_loss += loss.item()\n', '\n', '            # Print the progress of our training\n', '            counter += 1\n', '            #print(counter, ""/"", len(train_loader))\n', '\n', '            # Evaluating the model\n', '        model.eval()\n', '        counter = 0\n', '        # Tell torch not to calculate gradients\n', '        with torch.no_grad():\n', '            for inputs, labels in val_loader:\n', '                # Move to device\n', '                inputs, labels = inputs.to(device), labels.to(device)\n', '                # Forward pass\n', '                output = model.forward(inputs)\n', '                # Calculate Loss\n', '                valloss = criterion(output, labels)\n', ""                # Add loss to the validation set's running loss\n"", '                val_loss += valloss.item()\n', '\n', '                # Since our model outputs a LogSoftmax, find the real \n', '                # percentages by reversing the log function\n', '                output = torch.exp(output)\n', '                # Get the top class of the output\n', '                top_p, top_class = output.topk(1, dim=1)\n', '                # See how many of the classes were correct?\n', '                equals = top_class == labels.view(*top_class.shape)\n', '                # Calculate the mean (get the accuracy for this batch)\n', '                # and add it to the running accuracy for this epoch\n', '                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n', '\n', '                # Print the progress of our evaluation\n', '                counter += 1\n', '                #print(counter, ""/"", len(val_loader))\n', '\n', '        # Get the average loss for the entire epoch\n', '        train_loss = train_loss/len(train_loader)\n', '        valid_loss = val_loss/len(val_loader)\n', '\n', '        train_losses.append(train_loss)\n', '        valid_losses.append(valid_loss)\n', '        # Print out the information\n', ""        print('Accuracy: ', accuracy/len(val_loader))\n"", ""        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n"", '\n', '    return train_losses, valid_losses\n']"
"ASSIGN = list(pd.DataFrame(X_train).columns) ASSIGN = pd.DataFrame({'PC1':pca_last.components_[0],'PC2':pca_last.components_[1], 'Feature':colnames}) ASSIGN.head()",0,"['colnames = list(pd.DataFrame(X_train).columns)\n', ""pcs_df = pd.DataFrame({'PC1':pca_last.components_[0],'PC2':pca_last.components_[1], 'Feature':colnames})\n"", 'pcs_df.head()']"
CHECKPOINT ASSIGN = svc_model.predict(X_test) Y_predicted,0,"['Y_predicted = svc_model.predict(X_test)\n', 'Y_predicted']"
SETUP,0,"['import pandas as pd\n', 'import numpy as np\n', 'import matplotlib.pyplot as plt']"
"def CalculateSSE(original_Y, predicted_Y): ASSIGN=0.0 for i in range(0, original_Y.size): ASSIGN += (original_Y[i]-predicted_Y[i])**2 ASSIGN = theSSEpath return theSSE",0,"['def CalculateSSE(original_Y, predicted_Y):\n', '    theSSE=0.0\n', '    \n', '    for i in range(0, original_Y.size):\n', '        theSSE += (original_Y[i]-predicted_Y[i])**2\n', '        \n', '    theSSE = theSSE/2\n', '    \n', '    return theSSE']"
"sns.catplot(x='Year', y='Prevention of atrocities (POA) Act', data=cbdr,height = 5, aspect = 4)",1,"[""sns.catplot(x='Year', y='Prevention of atrocities (POA) Act', data=cbdr,height = 5, aspect = 4)""]"
"sns.catplot(x='Year', y='Other Crimes Against SCs', data=cbdr,height = 5, aspect = 4)",1,"[""sns.catplot(x='Year', y='Other Crimes Against SCs', data=cbdr,height = 5, aspect = 4)""]"
SETUP,0,['import pandas as pd']
ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path'),0,"[""train = pd.read_csv('../input/hack-ml/Dataset/Train.csv')\n"", ""test = pd.read_csv('../input/hack-ml/Dataset/Test.csv')""]"
"sns.jointplot(x=df1['Age'], y=df1['SibSp'], kind = 'kde')",1,"[""sns.jointplot(x=df1['Age'], y=df1['SibSp'], kind = 'kde')""]"
"SETUP CHECKPOINT ASSIGN = cross_val_score(SVC,x_train,x_test,ASSIGN=10) ASSIGN = cv.mean() accuracy.append(ASSIGN) print(ASSIGN) print(ASSIGN.mean())",0,"['SVC = LinearSVC()\n', ""#estimator.append(('LSVC',LinearSVC()))\n"", 'cv = cross_val_score(SVC,x_train,x_test,cv=10)\n', 'accuracy2 = cv.mean()\n', 'accuracy.append(accuracy2)\n', 'print(cv)\n', 'print(cv.mean())\n']"
ASSIGN = pd.read_csv('..path').fillna(0) ASSIGN.head(),0,"[""train = pd.read_csv('../input/web-traffic-time-series-forecasting/train_1.csv').fillna(0)\n"", 'train.head()']"
"ASSIGN = SVM_all.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('DecisionTrees(HT).csv',index = False)",0,"['model14pred = SVM_all.predict(y_train)\n', ""submission14 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission14['PassengerId'] = result['PassengerId']\n"", ""submission14['Survived'] = model14pred\n"", ""submission14.to_csv('DecisionTrees(HT).csv',index = False)""]"
"ASSIGN='isFraud' ASSIGN = [FillMissing, Categorify, Normalize] ASSIGN = TabularList.from_df(test_df, cat_names=categorical,cont_names=numerical,procs=procs) ASSIGN = (TabularList.from_df(train_df, cat_names=categorical, cont_names=numerical,procs=procs) .split_subsets(train_size=0.85, valid_size=0.15, seed=34) .label_from_df(cols=ASSIGN) .add_test(ASSIGN) .databunch())",0,"[""dep_var='isFraud' \n"", 'procs = [FillMissing, Categorify, Normalize]\n', 'test_all = TabularList.from_df(test_df, cat_names=categorical,cont_names=numerical,procs=procs)\n', 'data = (TabularList.from_df(train_df, cat_names=categorical, cont_names=numerical,procs=procs)\n', '                           .split_subsets(train_size=0.85, valid_size=0.15, seed=34)\n', '                           .label_from_df(cols=dep_var)\n', '                           .add_test(test_all)\n', '                           .databunch())       ']"
"SETUP CHECKPOINT ASSIGN = ['AR(1)','AR(2)','AR(11)'] ASSIGN = [predictions1, predictions2, predictions11] for i in range(3): ASSIGN = mean_squared_error(test['Total'], preds[i]) print(f'{ASSIGN[i]} Error: {ASSIGN:11.10}')",0,"['from sklearn.metrics import mean_squared_error\n', '\n', ""labels = ['AR(1)','AR(2)','AR(11)']\n"", 'preds = [predictions1, predictions2, predictions11]  # these are variables, not strings!\n', '\n', 'for i in range(3):\n', ""    error = mean_squared_error(test['Total'], preds[i])\n"", ""    print(f'{labels[i]} Error: {error:11.10}')""]"
"ASSIGN = VotingClassifier(estimators = estimator, voting ='soft') ASSIGN.fit(x_train, x_test) ASSIGN = vot_soft.predict(y_train) ASSIGN.score(y_train,y_test)",0,"[""vot_soft = VotingClassifier(estimators = estimator, voting ='soft') \n"", 'vot_soft.fit(x_train, x_test) \n', 'y_pred = vot_soft.predict(y_train)\n', 'vot_soft.score(y_train,y_test)']"
"CHECKPOINT ASSIGN=np.min(X) ASSIGN=np.max(X) ASSIGN=np.min(Y) ASSIGN=np.max(Y) print(, ASSIGN, ASSIGN) print(, ASSIGN, ASSIGN) plt.xlim(0,ASSIGN+1) plt.ylim(0,ASSIGN+1) plt.scatter(X,Y, 10, color = 'blue') plt.show()",1,"['# Setting up some global values and also plotting the data to get a feel for it.\n', '\n', 'Xmin=np.min(X)\n', 'Xmax=np.max(X)\n', 'Ymin=np.min(Y)\n', 'Ymax=np.max(Y)\n', 'print(""X min & max:"", Xmin, Xmax)\n', 'print(""Y min & max:"", Ymin, Ymax)\n', '\n', '\n', '# plt.xlim(Xmin-1,Xmax+1)\n', 'plt.xlim(0,Xmax+1)\n', 'plt.ylim(0,Ymax+1)\n', '\n', ""plt.scatter(X,Y, 10, color = 'blue')\n"", 'plt.show()']"
"submission.to_csv('..path', index=False)",0,"[""submission.to_csv('../working/submission.csv', index=False)""]"
CHECKPOINT ASSIGN = suicide_attacks['Province'].unique() ASSIGN.sort() provinces,0,"['# Your turn! Take a look at all the unique values in the ""Province"" column. \n', '# Then convert the column to lowercase and remove any trailing white spaces\n', '\n', ""# get all the unique values in the 'City' column\n"", ""provinces = suicide_attacks['Province'].unique()\n"", '\n', '# sort them alphabetically and then take a closer look\n', 'provinces.sort()\n', 'provinces']"
"def ResUNet(img_h,img_w): ASSIGN=[16,32,64,128,256] ASSIGN=Input((img_h,img_w,1)) ASSIGN=inputs ASSIGN=stem(e0,f[0]) ASSIGN=residual_block(e1,f[1],strides=2) ASSIGN=residual_block(e2,f[2],strides=2) ASSIGN=residual_block(e3,f[3],strides=2) ASSIGN=residual_block(e4,f[4],strides=2) ASSIGN=conv_block(e5,f[4],strides=1) ASSIGN=conv_block(b0,f[4],strides=1) ASSIGN=upsample_concat_block(b1,e4) ASSIGN=residual_block(u1,f[4]) ASSIGN=upsample_concat_block(d1,e3) ASSIGN=residual_block(u2,f[3]) ASSIGN=upsample_concat_block(d2,e2) ASSIGN=residual_block(u3,f[2]) ASSIGN=upsample_concat_block(d3,e1) ASSIGN=residual_block(u4,f[1]) ASSIGN=tf.keras.layers.Conv2D(4,(1,1),padding='same',activation='sigmoid')(d4) ASSIGN=tf.keras.models.Model(inputs,outputs) return model",0,"['def ResUNet(img_h,img_w):\n', '\tf=[16,32,64,128,256]\n', '\tinputs=Input((img_h,img_w,1))\n', '\n', '\te0=inputs\n', '\te1=stem(e0,f[0])\n', '\te2=residual_block(e1,f[1],strides=2)\n', '\te3=residual_block(e2,f[2],strides=2)\n', '\te4=residual_block(e3,f[3],strides=2)\n', '\te5=residual_block(e4,f[4],strides=2)\n', '\n', '\tb0=conv_block(e5,f[4],strides=1)\n', '\tb1=conv_block(b0,f[4],strides=1)\n', '\n', '\tu1=upsample_concat_block(b1,e4)\n', '\td1=residual_block(u1,f[4])\n', '\n', '\tu2=upsample_concat_block(d1,e3)\n', '\td2=residual_block(u2,f[3])\n', '\n', '\tu3=upsample_concat_block(d2,e2)\n', '\td3=residual_block(u3,f[2])\n', '\n', '\tu4=upsample_concat_block(d3,e1)\n', '\td4=residual_block(u4,f[1])\n', '\n', ""\toutputs=tf.keras.layers.Conv2D(4,(1,1),padding='same',activation='sigmoid')(d4)\n"", '\tmodel=tf.keras.models.Model(inputs,outputs)\n', '\treturn model\n']"
"data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtQual'].isnull())][ ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]",0,"[""data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtQual'].isnull())][\n"", ""    ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]""]"
"plt.figure(figsize=(14,9)) sns.heatmap(relevant.isnull(), cbar=False, yticklabels=False)",1,"['plt.figure(figsize=(14,9))\n', 'sns.heatmap(relevant.isnull(), cbar=False, yticklabels=False)']"
"ASSIGN = rf.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('RandomForest(HT).csv',index = False)",0,"['model16pred = rf.predict(y_train)\n', ""submission16 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission16['PassengerId'] = result['PassengerId']\n"", ""submission16['Survived'] = model16pred\n"", ""submission16.to_csv('RandomForest(HT).csv',index = False)""]"
ASSIGN = data.nunique()[data.nunique()<5].keys().tolist() ASSIGN = all_cat_var[:-1],0,"['# Getting the categorical variables \n', 'all_cat_var = data.nunique()[data.nunique()<5].keys().tolist()\n', '\n', '# Getting the categorical variables without churn\n', 'cat_var = all_cat_var[:-1]']"
ASSIGN=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[:2000] ASSIGN.head(),0,"[""data_total_cars_1=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[:2000]\n"", 'data_total_cars_1.head()']"
"ASSIGN = dict(zip( US['Provincepath'].unique(), color_US )) def race_barchart_US(date): ASSIGN = US[US['date'].eq(date)].sort_values(by='Confirmed', ascending=True).tail(10) ax.clear() ax.barh(ASSIGN['Provincepath'], ASSIGN['Confirmed'], color=[ASSIGN[x] for x in ASSIGN['Provincepath']],height=0.8) ASSIGN = dff['Confirmed'].max() path for i, (value, name) in enumerate(zip(ASSIGN['Confirmed'], ASSIGN['Provincepath'])): ax.text(0, i,name+' ',size=16, weight=600, ha='right', va='center') ax.text(value+ASSIGN, i,f'{value:,.0f}', size=16, ha='left', va='center') ax.text(0.9, 0.2, date, transform=ax.transAxes, color=' ax.text(0.59, 0.14, 'Total Confirmed:'+str(int(ASSIGN['Confirmed'].sum())), transform=ax.transAxes, size=24, color=' ax.tick_params(axis='x', colors=' ax.xaxis.set_ticks_position('top') ax.set_yticks([]) ax.margins(0, 0.01) ax.grid(which='major', axis='x', linestyle='-') ax.text(0, 1.15, 'Confirmed for each date in US ', ASSIGN=ax.transAxes, size=24, weight=600, ha='left', va='top') plt.box(False) ASSIGN = list(set(US.date.values)) ASSIGN.sort() ASSIGN = plt.subplots(figsize=(16, 9)) HTML(animation.FuncAnimation(fig, race_barchart_US, frames=ASSIGN,interval=400).to_jshtml())",1,"['colors_US = dict(zip(\n', ""    US['Province/State'].unique(),\n"", '    color_US\n', '))\n', '\n', '\n', 'def race_barchart_US(date):\n', ""    dff = US[US['date'].eq(date)].sort_values(by='Confirmed', ascending=True).tail(10)\n"", '    ax.clear()\n', ""    ax.barh(dff['Province/State'], dff['Confirmed'], color=[colors_US[x] for x in dff['Province/State']],height=0.8)\n"", ""    dx = dff['Confirmed'].max() / 200\n"", '    \n', ""    for i, (value, name) in enumerate(zip(dff['Confirmed'], dff['Province/State'])):\n"", ""        ax.text(0, i,name+' ',size=16, weight=600, ha='right', va='center') \n"", ""        ax.text(value+dx, i,f'{value:,.0f}',  size=16, ha='left',  va='center') \n"", '            \n', ""    ax.text(0.9, 0.2, date, transform=ax.transAxes, color='#777777', size=72, ha='right', weight=1000) \n"", ""    ax.text(0.59, 0.14, 'Total Confirmed:'+str(int(dff['Confirmed'].sum())), transform=ax.transAxes, size=24, color='#000000',ha='left')\n"", ""    ax.tick_params(axis='x', colors='#777777', labelsize=12) \n"", ""    ax.xaxis.set_ticks_position('top') \n"", '    ax.set_yticks([])\n', '    ax.margins(0, 0.01)\n', ""    ax.grid(which='major', axis='x', linestyle='-') \n"", ""    ax.text(0, 1.15, 'Confirmed for each date in US ',\n"", ""                transform=ax.transAxes, size=24, weight=600, ha='left', va='top') \n"", '\n', '    plt.box(False)\n', '    \n', '\n', 'day = list(set(US.date.values))\n', 'day.sort()\n', '\n', 'fig, ax = plt.subplots(figsize=(16, 9))\n', '\n', 'HTML(animation.FuncAnimation(fig, race_barchart_US, frames=day,interval=400).to_jshtml())']"
models.keys(),0,['models.keys()']
ASSIGN = scaler.transform(CC),0,['scaled_data = scaler.transform(CC)']
"CHECKPOINT ASSIGN = ASSIGN[pd.notnull(ASSIGN['TotalCharges'])] print(,ASSIGN['TotalCharges'].isna().sum()) ASSIGN = pd.to_numeric(ASSIGN,errors='coerce')",0,"['# Removing the missing values\n', ""data = data[pd.notnull(data['TotalCharges'])]\n"", 'print(""Number of null values in total charges:"",data[\'TotalCharges\'].isna().sum())\n', ""data['TotalCharges'] = pd.to_numeric(data['TotalCharges'],errors='coerce')""]"
"ASSIGN = AR(artime['Total']) ASSIGN = model.fit(maxlag=8) ASSIGN = ARfit.predict(start=len(artime), end=len(artime)+20).rename('Forecast') artime['Total'].plot(legend=True) ASSIGN.plot(legend=True, grid=True, figsize=(12,6));",1,"[""model = AR(artime['Total'])\n"", '\n', '# Next, fit the model\n', 'ARfit = model.fit(maxlag=8)\n', '\n', '# Make predictions\n', ""fcast = ARfit.predict(start=len(artime), end=len(artime)+20).rename('Forecast')\n"", '\n', '# Plot the results\n', ""artime['Total'].plot(legend=True)\n"", 'fcast.plot(legend=True, grid=True, figsize=(12,6));']"
"lineplot(total_sales_group, title = 'Sales by Year', ylabel ='Sales (In Millions)', legendsize = 8)",1,"[""lineplot(total_sales_group, title = 'Sales by Year', ylabel ='Sales (In Millions)', legendsize = 8)""]"
"CHECKPOINT ASSIGN = GaussianNB() ASSIGN.fit(train_x, train_y) ASSIGN = cross_val_score(gaussian,train_x,train_y,cv=5,scoring='accuracy') print('Cross validation: %.5f'%ASSIGN.mean()) print('standard deviation of Cross validation: %.5f'%ASSIGN.std(ddof=1))",0,"['#Gaussian Naive Bayes\n', 'gaussian = GaussianNB()\n', 'gaussian.fit(train_x, train_y)\n', '#Cross validation\n', ""scores = cross_val_score(gaussian,train_x,train_y,cv=5,scoring='accuracy')\n"", ""print('Cross validation: %.5f'%scores.mean())\n"", ""print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))""]"
"sns.lineplot(x = 'TOEFL Score', y = 'CGPA', data = df)",1,"[""sns.lineplot(x = 'TOEFL Score', y = 'CGPA', data = df)""]"
"ASSIGN = np.hstack((train_digital, train_category)) ASSIGN = np.hstack((test_digital, test_category)) train_features.shape, test_features.shape",0,"['train_features = np.hstack((train_digital, train_category))\n', 'test_features = np.hstack((test_digital, test_category))\n', 'train_features.shape, test_features.shape']"
"sns.catplot(x='Year', y='Assault on women', data=cbdr,height = 5, aspect = 4)",1,"[""sns.catplot(x='Year', y='Assault on women', data=cbdr,height = 5, aspect = 4)""]"
"ASSIGN = add_constant(data_new) pd.Series([variance_inflation_factor(ASSIGN.values, i) for i in range(ASSIGN.shape[1])], index=ASSIGN.columns)",0,"['X = add_constant(data_new)\n', 'pd.Series([variance_inflation_factor(X.values, i) \n', '           for i in range(X.shape[1])], index=X.columns)']"
"SLICE=df.Attrition.eq('Yes').mul(1) ASSIGN=[] ASSIGN=[] for key, value in dtypes.items(): if key!='Attrition': ASSIGN == ""int64"": ASSIGN.append(key) else: ASSIGN.append(key) ASSIGN = pd.get_dummies(ASSIGN, columns=cat)",0,"['# preprocessing : categorical encoding\n', ""df['Attrition']=df.Attrition.eq('Yes').mul(1) # change target from Yes/no to 1/0\n"", 'cont=[]\n', 'cat=[]\n', 'for key, value in dtypes.items():\n', ""    if key!='Attrition':\n"", '        if value == ""int64"":\n', '            cont.append(key)\n', '        else:\n', '            cat.append(key)\n', 'df = pd.get_dummies(df, columns=cat)']"
"ASSIGN = train[(train['hour'] < 19) & (train['hour'] > 7)] ASSIGN = plt.figure(figsize=(25, 60)) ASSIGN = [np.random.choice(day.loc[day['category_id'] == i, 'file_name'], 4) for i in day.category_id.unique()] ASSIGN = [i for j in ASSIGN for i in j] ASSIGN = [[i] * 4 for i in train.category_id.unique()] ASSIGN = [i for j in ASSIGN for i in j] for idx, img in enumerate(ASSIGN): ASSIGN = fig.add_subplot(14, 4, idx + 1, xticks=[], yticks=[]) ASSIGN = Image.open(""..path"" + img) plt.imshow(ASSIGN) ASSIGN.set_title(f'Label: {ASSIGN[idx]}')",1,"[""day = train[(train['hour'] < 19) & (train['hour'] > 7)]\n"", '# sample night images\n', 'fig = plt.figure(figsize=(25, 60))\n', ""imgs = [np.random.choice(day.loc[day['category_id'] == i, 'file_name'], 4) for i in day.category_id.unique()]\n"", 'imgs = [i for j in imgs for i in j]\n', 'labels = [[i] * 4 for i in train.category_id.unique()]\n', 'labels = [i for j in labels for i in j]\n', 'for idx, img in enumerate(imgs):\n', '    ax = fig.add_subplot(14, 4, idx + 1, xticks=[], yticks=[])\n', '    im = Image.open(""../input/train_images/"" + img)\n', '    plt.imshow(im)\n', ""    ax.set_title(f'Label: {labels[idx]}')""]"
"SETUP ASSIGN = Sequential() ASSIGN.add(Conv2D(64, kernel_size=3, activation=""relu"", input_shape=(28,28,1))) ASSIGN.add(Conv2D(32, kernel_size=3, activation=""relu"")) ASSIGN.add(Flatten()) ASSIGN.add(Dense(10, activation='softmax'))",0,"['##BUILDING THE MODEL\n', 'from keras.models import Sequential\n', 'from keras.layers import Dense, Conv2D, Flatten#create model\n', '\n', 'model = Sequential()#add model layers\n', '\n', 'model.add(Conv2D(64, kernel_size=3, activation=""relu"", input_shape=(28,28,1)))\n', 'model.add(Conv2D(32, kernel_size=3, activation=""relu""))\n', 'model.add(Flatten())\n', ""model.add(Dense(10, activation='softmax'))""]"
"ASSIGN = ASSIGN.permute(1, 0)",0,"['soundFormatted = soundFormatted.permute(1, 0)']"
"ASSIGN = pd.DataFrame([['West Bank and Gaza'],[2697687],[485],[5559],[0],[19],['57%']]) ASSIGN = ASSIGN.T ASSIGN.columns = population.columns ASSIGN = ASSIGN.append(westbank)",0,"[""westbank = pd.DataFrame([['West Bank and Gaza'],[2697687],[485],[5559],[0],[19],['57%']])\n"", 'westbank = westbank.T\n', 'westbank.columns = population.columns\n', '\n', 'population = population.append(westbank)']"
SETUP,0,['!tree {path}']
"CHECKPOINT ASSIGN = Sequential() ASSIGN.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3))) ASSIGN.add(MaxPooling2D((2, 2))) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) ASSIGN.add(MaxPooling2D((2, 2))) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) ASSIGN.add(MaxPooling2D((2, 2))) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) ASSIGN.add(MaxPooling2D((2, 2))) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Flatten()) ASSIGN.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) ASSIGN.add(Dropout(0.5)) ASSIGN.add(Dense(1, activation='sigmoid')) print(ASSIGN.summary()) input_and_run(ASSIGN,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.002,num_epochs=200)",0,"['##BUILDING THE MODEL 1\n', '\n', 'model2 = Sequential()#add model layers\n', '\n', ""model2.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)))\n"", 'model2.add(MaxPooling2D((2, 2)))\n', 'model2.add(Dropout(0.2))\n', ""model2.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n"", 'model2.add(MaxPooling2D((2, 2)))\n', 'model2.add(Dropout(0.2))\n', ""model2.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n"", 'model2.add(MaxPooling2D((2, 2)))\n', 'model2.add(Dropout(0.2))\n', ""model2.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n"", 'model2.add(MaxPooling2D((2, 2)))\n', 'model2.add(Dropout(0.2))\n', 'model2.add(Flatten())\n', ""model2.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n"", 'model2.add(Dropout(0.5))\n', ""model2.add(Dense(1, activation='sigmoid'))\n"", '\n', 'print(model2.summary())\n', 'input_and_run(model2,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.002,num_epochs=200)']"
"CHECKPOINT def input_and_run(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10): ASSIGN = keras.optimizers.Adam(learning_rate=alpha) ASSIGN=SGD(lr=alpha, momentum=0.9) model.compile(loss='binary_crossentropy', optimizer=ASSIGN,metrics=['accuracy']) model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=num_epochs) ASSIGN = model.evaluate(X_train,y_train) print(+str(ASSIGN[1]*100)) ASSIGN = model.evaluate(X_val,y_val) print(+str(ASSIGN[1]*100)) ASSIGN = model.evaluate(X_test,y_test) print(+str(ASSIGN[1]*100))",0,"['\n', '\n', 'def input_and_run(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10):\n', '    \n', '    #compile model using accuracy to measure model performance\n', '    opt = keras.optimizers.Adam(learning_rate=alpha)\n', '    opt2=SGD(lr=alpha, momentum=0.9)\n', ""    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n"", '    \n', '    #train the model\n', '    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=num_epochs)\n', '    \n', '    #Getting results\n', '    result = model.evaluate(X_train,y_train)\n', '    #print(result)\n', '    print(""Training accuracy = ""+str(result[1]*100))\n', '    result = model.evaluate(X_val,y_val)\n', '    #print(result)\n', '    print(""Validation accuracy = ""+str(result[1]*100))\n', '    result = model.evaluate(X_test,y_test)\n', '    #print(result)\n', '    print(""Test accuracy = ""+str(result[1]*100))\n', '\n']"
"vot_hard1.fit(x_train, x_test) vot_hard1.score(y_train,y_test)",0,"['vot_hard1.fit(x_train, x_test) \n', 'vot_hard1.score(y_train,y_test)']"
CHECKPOINT ASSIGN = pd.read_csv('..path') ASSIGN['Pred'] = test_preds submission_df,0,"[""submission_df = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WSampleSubmissionStage1_2020.csv')\n"", ""submission_df['Pred'] = test_preds\n"", 'submission_df']"
soundData.numel(),0,['soundData.numel()']
"model.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])",0,"['# Compile Model\n', ""model.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics = ['accuracy'])""]"
"plt.figure(figsize=(18,70)) sns.barplot(x=test_transaction.isnull().sum().sort_values(ascending=False),y=test_transaction.isnull().sum().sort_values(ascending=False).index) plt.title(""counts of missing value for test_transaction"",size=20)",1,"['plt.figure(figsize=(18,70))\n', 'sns.barplot(x=test_transaction.isnull().sum().sort_values(ascending=False),y=test_transaction.isnull().sum().sort_values(ascending=False).index)\n', 'plt.title(""counts of missing value for test_transaction"",size=20)']"
"ASSIGN = plt.subplots(1, 2, figsize=(17, 7), dpi=200) ax[0].scatter(x=train_embeddings[:, 0], y=train_embeddings[:, 1], c=train_Y, s=3, cmap='RdYlBu_r') ax[0].set_xlabel('ivis 1') ax[0].set_ylabel('ivis 2') ax[0].set_title('Training Set') ax[1].scatter(x=test_embeddings[:, 0], y=test_embeddings[:, 1], c=test_Y, s=3, cmap='RdYlBu_r') ax[1].set_xlabel('ivis 1') ax[1].set_ylabel('ivis 2') ax[1].set_title('Testing Set')",1,"['fig, ax = plt.subplots(1, 2, figsize=(17, 7), dpi=200)\n', ""ax[0].scatter(x=train_embeddings[:, 0], y=train_embeddings[:, 1], c=train_Y, s=3, cmap='RdYlBu_r')\n"", ""ax[0].set_xlabel('ivis 1')\n"", ""ax[0].set_ylabel('ivis 2')\n"", ""ax[0].set_title('Training Set')\n"", '\n', ""ax[1].scatter(x=test_embeddings[:, 0], y=test_embeddings[:, 1], c=test_Y, s=3, cmap='RdYlBu_r')\n"", ""ax[1].set_xlabel('ivis 1')\n"", ""ax[1].set_ylabel('ivis 2')\n"", ""ax[1].set_title('Testing Set')""]"
"ASSIGN = kickstarters_2017.goal ASSIGN = minmax_scaling(usd_goal, columns = [0]) ASSIGN=plt.subplots(1,2) sns.distplot(kickstarters_2017.goal, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Scaled data"")",1,"['# Your turn! \n', '\n', '# We just scaled the ""usd_goal_real"" column. What about the ""goal"" column?\n', '\n', 'usd_goal = kickstarters_2017.goal\n', '\n', '# scale the goals from 0 to 1\n', 'scaled_data = minmax_scaling(usd_goal, columns = [0])\n', '\n', '# plot the original & scaled data together to compare\n', 'fig, ax=plt.subplots(1,2)\n', 'sns.distplot(kickstarters_2017.goal, ax=ax[0])\n', 'ax[0].set_title(""Original Data"")\n', 'sns.distplot(scaled_data, ax=ax[1])\n', 'ax[1].set_title(""Scaled data"")']"
"train.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True) test.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)",0,"[""train.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)\n"", ""test.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)""]"
"SETUP CHECKPOINT plt.rcParams['figure.figsize']=(20,10) py.init_notebook_mode(connected=False) print(os.listdir('..path'))",0,"['import matplotlib.pyplot as plt\n', 'import numpy as np\n', 'import plotly.offline as py\n', 'import plotly.graph_objs as go\n', 'import seaborn as sns\n', '\n', 'from sklearn.metrics import roc_auc_score\n', ' \n', 'import cv2\n', 'import torch.nn as nn\n', 'import torch.optim as optim\n', 'from torch.utils.data import Dataset, DataLoader, sampler, TensorDataset, random_split\n', 'from torch.optim import lr_scheduler\n', 'from torchvision import datasets, transforms, models\n', 'from torch import utils\n', '\n', 'import  time, copy, glob, torchvision, torch, os, json, re\n', 'from collections import Counter, OrderedDict\n', '\n', 'from PIL import Image\n', 'from sklearn.metrics import classification_report\n', '\n', '\n', ""plt.rcParams['figure.figsize']=(20,10)\n"", 'py.init_notebook_mode(connected=False)\n', '%matplotlib inline\n', ""print(os.listdir('../input'))""]"
ASSIGN=df_map_final.head(15),0,"['#lets take one data frame for bottom 15 cities with minimum retaurants counts \n', 'df_plot_bottom=df_map_final.head(15)']"
"RF.fit(x_train,x_test) ASSIGN = RF.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('RandomForest(No HT).csv',index = False)",0,"['RF.fit(x_train,x_test)\n', 'model7pred = RF.predict(y_train)\n', ""submission7 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission7['PassengerId'] = result['PassengerId']\n"", ""submission7['Survived'] = model7pred\n"", ""submission7.to_csv('RandomForest(No HT).csv',index = False)""]"
def torch_seed(seed): os.environ['PYTHONHASHSEED'] = str(seed) random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True,0,"['def torch_seed(seed):\n', ""    os.environ['PYTHONHASHSEED'] = str(seed)\n"", '    random.seed(seed)\n', '    np.random.seed(seed)\n', '    torch.manual_seed(seed)\n', '    torch.cuda.manual_seed(seed)\n', '    torch.backends.cudnn.deterministic = True']"
CHECKPOINT city_vs_count,0,['city_vs_count']
"def get_mse(Y, Yhat): ASSIGN = Y - Yhat return ASSIGN.dot(ASSIGN) path(ASSIGN) def plot_train_vs_test_curves(X, Y, sample=20, max_deg=20): ASSIGN = len(X) ASSIGN = np.random.choice(N, sample) ASSIGN = X[train_idx] ASSIGN = Y[train_idx] ASSIGN = [idx for idx in range(N) if idx not in train_idx] ASSIGN = X[test_idx] ASSIGN = Y[test_idx] ASSIGN = [] ASSIGN = [] for deg in range(max_deg+1): ASSIGN = make_poly(Xtrain, deg) ASSIGN = fit(Xtrain_poly, Ytrain) ASSIGN = Xtrain_poly.dot(w) ASSIGN = get_mse(Ytrain, Yhat_train) ASSIGN = make_poly(Xtest, deg) ASSIGN = Xtest_poly.dot(w) ASSIGN = get_mse(Ytest, Yhat_test) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) plt.plot(ASSIGN, label=""train mse"") plt.plot(ASSIGN, label=""test mse"") plt.legend() plt.show() plt.plot(ASSIGN, label=""train mse"") plt.legend() plt.show()",1,"['def get_mse(Y, Yhat):\n', '    d = Y - Yhat\n', '    return d.dot(d) / len(d)\n', '\n', 'def plot_train_vs_test_curves(X, Y, sample=20, max_deg=20):\n', '    N = len(X)\n', '    train_idx = np.random.choice(N, sample)\n', '    Xtrain = X[train_idx]\n', '    Ytrain = Y[train_idx]\n', '    \n', '    test_idx = [idx for idx in range(N) if idx not in train_idx]\n', '    Xtest = X[test_idx]\n', '    Ytest = Y[test_idx]\n', '    \n', '    mse_trains = []\n', '    mse_tests = []\n', '    for deg in range(max_deg+1):\n', '        Xtrain_poly = make_poly(Xtrain, deg)\n', '        w = fit(Xtrain_poly, Ytrain)\n', '        Yhat_train = Xtrain_poly.dot(w)\n', '        mse_train = get_mse(Ytrain, Yhat_train)\n', '\n', '        Xtest_poly = make_poly(Xtest, deg)\n', '        Yhat_test = Xtest_poly.dot(w)\n', '        mse_test = get_mse(Ytest, Yhat_test)\n', '\n', '        mse_trains.append(mse_train)\n', '        mse_tests.append(mse_test)\n', '\n', '    plt.plot(mse_trains, label=""train mse"")\n', '    plt.plot(mse_tests, label=""test mse"")\n', '    plt.legend()\n', '    plt.show()\n', '\n', '    plt.plot(mse_trains, label=""train mse"")\n', '    plt.legend()\n', '    plt.show()\n', '        ']"
"def headline_by_year(year): ASSIGN=[] ASSIGN=irishtimes[irishtimes.year==str(year)] ASSIGN=None for i in range(len(ASSIGN)): ASSIGN=re.sub('[^a-zA-Z]',' ',irishtimes_headline_text['headline_text'][irishtimes_headline_text.index[i]]) ASSIGN=ASSIGN.lower() ASSIGN=ASSIGN.split() ASSIGN=PorterStemmer() ASSIGN=[ps.stem(word) for word in ASSIGN if not word in set(stopwords.words('english'))] ASSIGN.extend(ASSIGN) ASSIGN = WordCloud(background_color=""black"",random_state=40,max_words=200,max_font_size=40).generate(str(headline_text_new)) plt.figure(figsize=(20,15)) plt.imshow(ASSIGN, interpolation='bilinear') plt.title(""Wordcloud of ASSIGN in ""+str(year),size=20) plt.axis(""off"") plt.show()",1,"['def headline_by_year(year):\n', '    headline_text_new=[]#Initialize empty array to append clean text\n', '    irishtimes_headline_text=irishtimes[irishtimes.year==str(year)]\n', '    headline=None\n', '    for i in range(len(irishtimes_headline_text)):\n', ""        headline=re.sub('[^a-zA-Z]',' ',irishtimes_headline_text['headline_text'][irishtimes_headline_text.index[i]]) \n"", '        headline=headline.lower() #convert to lower case\n', '        headline=headline.split() #split to array(default delimiter is "" "")\n', '        ps=PorterStemmer() #creating porterStemmer object to take main stem of each word\n', ""        headline=[ps.stem(word) for word in headline if not word in set(stopwords.words('english'))] #loop for stemming each word  in string array at ith row\n"", '        headline_text_new.extend(headline)\n', '    wordcloud = WordCloud(background_color=""black"",random_state=40,max_words=200,max_font_size=40).generate(str(headline_text_new))\n', '    plt.figure(figsize=(20,15))\n', ""    plt.imshow(wordcloud, interpolation='bilinear')\n"", '    plt.title(""Wordcloud of headline in ""+str(year),size=20)\n', '    plt.axis(""off"")\n', '    plt.show()']"
"ASSIGN = poly.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('PolynomialSVC(No HT).csv',index = False)",0,"['model3pred = poly.predict(y_train)\n', ""submission3 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission3['PassengerId'] = result['PassengerId']\n"", ""submission3['Survived'] = model3pred\n"", ""submission3.to_csv('PolynomialSVC(No HT).csv',index = False)""]"
"ASSIGN=plt.subplots(2,2,figsize=(16,16)) Top10_category=irishtimes[irishtimes['headline_category'].isin(list(irishtimes.headline_category.value_counts()[:10].index[:10]))] sns.barplot(y=Top10_category.headline_category.value_counts().index,x=Top10_category.headline_category.value_counts(),ax=ax[0,0]) ax[0,0].set_title(""Top 10 category by counts"",size=20) ax[0,0].set_xlabel('counts',size=18) ax[0,0].set_ylabel('') Top10_category.groupby(['year','headline_category'])['headline_category'].agg('count').unstack('headline_category').plot(ax=ax[0,1]) ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1)) ax[0,1].set_title(""Top 10 category counts by year"",size=20) ax[0,1].set_ylabel('counts',size=18) ax[0,1].set_xlabel('year',size=18) Top10_category.groupby(['month','headline_category'])['headline_category'].agg('count').unstack('headline_category').plot(ax=ax[1,0]) ax[1,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(-0.25,1)) ax[1,0].set_title(""Top 10 category counts by month"",size=20) ax[1,0].set_ylabel('counts',size=18) ax[1,0].set_xlabel('month',size=18) Top10_category.groupby(['day','headline_category'])['headline_category'].agg('count').unstack('headline_category').plot(ax=ax[1,1]) ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1)) ax[1,1].set_title(""Top 10 category counts by day"",size=20) ax[1,1].set_ylabel('counts',size=18) ax[1,1].set_xlabel('day',size=18)",1,"['fig,ax=plt.subplots(2,2,figsize=(16,16))\n', ""Top10_category=irishtimes[irishtimes['headline_category'].isin(list(irishtimes.headline_category.value_counts()[:10].index[:10]))]\n"", 'sns.barplot(y=Top10_category.headline_category.value_counts().index,x=Top10_category.headline_category.value_counts(),ax=ax[0,0])\n', 'ax[0,0].set_title(""Top 10 category by counts"",size=20)\n', ""ax[0,0].set_xlabel('counts',size=18)\n"", ""ax[0,0].set_ylabel('')\n"", '\n', ""Top10_category.groupby(['year','headline_category'])['headline_category'].agg('count').unstack('headline_category').plot(ax=ax[0,1])\n"", 'ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n', 'ax[0,1].set_title(""Top 10 category counts by year"",size=20)\n', ""ax[0,1].set_ylabel('counts',size=18)\n"", ""ax[0,1].set_xlabel('year',size=18)\n"", '\n', ""Top10_category.groupby(['month','headline_category'])['headline_category'].agg('count').unstack('headline_category').plot(ax=ax[1,0])\n"", 'ax[1,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(-0.25,1))\n', 'ax[1,0].set_title(""Top 10 category counts by month"",size=20)\n', ""ax[1,0].set_ylabel('counts',size=18)\n"", ""ax[1,0].set_xlabel('month',size=18)\n"", '\n', ""Top10_category.groupby(['day','headline_category'])['headline_category'].agg('count').unstack('headline_category').plot(ax=ax[1,1])\n"", 'ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n', 'ax[1,1].set_title(""Top 10 category counts by day"",size=20)\n', ""ax[1,1].set_ylabel('counts',size=18)\n"", ""ax[1,1].set_xlabel('day',size=18)""]"
"os.listdir(""..path"")",0,"['os.listdir(""../input/pga-tour-20102018-data/"")']"
"sns.pairplot(data,vars = ['tenure','MonthlyCharges','TotalCharges'], hue=""Churn"")",1,"['sns.pairplot(data,vars = [\'tenure\',\'MonthlyCharges\',\'TotalCharges\'], hue=""Churn"")']"
"SETUP sns.heatmap(train.isnull(), cbar=False)",1,"['import seaborn as sns\n', '\n', 'sns.heatmap(train.isnull(), cbar=False)']"
logisticRegr.predict(X_train_pca[0:10]),0,['logisticRegr.predict(X_train_pca[0:10])']
"sns.catplot(x='Seller_Type', y ='Selling_Price', kind = 'swarm',data= data,hue = 'Fuel_Type')",1,"[""sns.catplot(x='Seller_Type', y ='Selling_Price', kind = 'swarm',data= data,hue = 'Fuel_Type')""]"
ASSIGN = pd.read_csv('..path'),0,"[""test_df =  pd.read_csv('../input/quora-insincere-questions-classification/test.csv')""]"
"ASSIGN=['O+','A+','B+','AB+','O-','A-','B-','AB-'] ASSIGN=[12,11,2,15,22,14,34,21] plt.bar(ASSIGN,ASSIGN,color=['red','red','red','red','black','red','red','red']) plt.title('BLOOD GROUP') plt.xlabel('X-axis') plt.ylabel(""Y-axis"")",1,"[""bg=['O+','A+','B+','AB+','O-','A-','B-','AB-']\n"", 'no=[12,11,2,15,22,14,34,21]\n', ""plt.bar(bg,no,color=['red','red','red','red','black','red','red','red'])\n"", ""plt.title('BLOOD GROUP')\n"", ""plt.xlabel('X-axis')\n"", 'plt.ylabel(""Y-axis"")']"
"SETUP CHECKPOINT estimator.append(('GBC',GradientBoostingClassifier(random_state = 5))) ASSIGN = cross_val_score(GBC,x_train,x_test,ASSIGN=10) ASSIGN = cv.mean() accuracy.append(ASSIGN) print(ASSIGN) print(ASSIGN.mean())",0,"['GBC = GradientBoostingClassifier(random_state = 5)\n', ""estimator.append(('GBC',GradientBoostingClassifier(random_state = 5)))\n"", 'cv = cross_val_score(GBC,x_train,x_test,cv=10)\n', 'accuracy8 = cv.mean()\n', 'accuracy.append(accuracy8)\n', 'print(cv)\n', 'print(cv.mean())']"
df3.head(5),0,['df3.head(5)']
"ASSIGN=ASSIGN.dropna(subset=['Translated_Review']) ASSIGN=ASSIGN.reindex(range(len(ASSIGN)), method='ffill') ASSIGN=[] for i in range(len(ASSIGN)): ASSIGN=re.sub('[^a-zA-Z]',' ',data_review['Translated_Review'][i]) ASSIGN=ASSIGN.lower() ASSIGN=ASSIGN.split() ASSIGN=PorterStemmer() ASSIGN=[ps.stem(word) for word in ASSIGN if not word in set(stopwords.words('english'))] ASSIGN.extend(ASSIGN)",0,"[""data_review=data_review.dropna(subset=['Translated_Review'])\n"", ""data_review=data_review.reindex(range(len(data_review)), method='ffill')\n"", 'headline_text_new=[]#Initialize empty array to append clean text\n', 'for i in range(len(data_review)):\n', ""    headline=re.sub('[^a-zA-Z]',' ',data_review['Translated_Review'][i]) \n"", '    headline=headline.lower() #convert to lower case\n', '    headline=headline.split() #split to array(default delimiter is "" "")\n', '    ps=PorterStemmer() #creating porterStemmer object to take main stem of each word\n', ""    headline=[ps.stem(word) for word in headline if not word in set(stopwords.words('english'))] #loop for stemming each word  in string array at ith row\n"", '    headline_text_new.extend(headline)']"
"df['char_count'] = df['review'].apply(len) df['word_count'] = df['review'].apply(lambda x: len(x.split())) df['word_density'] = df['char_count'] path(df['word_count']+1) df['punctuation_count'] = df['review'].apply(lambda x: len("""".join(_ for _ in x if _ in string.punctuation)))",0,"[""df['char_count'] = df['review'].apply(len)\n"", ""df['word_count'] = df['review'].apply(lambda x: len(x.split()))\n"", ""df['word_density'] = df['char_count'] / (df['word_count']+1)\n"", 'df[\'punctuation_count\'] = df[\'review\'].apply(lambda x: len("""".join(_ for _ in x if _ in string.punctuation))) ']"
"CHECKPOINT ASSIGN = model.predict(test_x) ASSIGN = decode_predictions(preds, top=3)[0] print('Predicted:', ASSIGN)",0,"['# model prediction\n', 'preds = model.predict(test_x)\n', '# decode prediction\n', 'dec_preds =  decode_predictions(preds, top=3)[0]\n', ""print('Predicted:', dec_preds)""]"
"sns.catplot(x = 'Title',y='Survived',hue = 'Sex', kind = 'bar', data = df3, col = 'Sex', palette = 'GnBu_d',aspect =2)",1,"[""sns.catplot(x = 'Title',y='Survived',hue = 'Sex', kind = 'bar', data = df3, col = 'Sex', palette = 'GnBu_d',aspect =2)""]"
"learn.fit_one_cycle(2, max_lr=slice(3e-5,3e-4))",0,"['learn.fit_one_cycle(2, max_lr=slice(3e-5,3e-4))']"
sample_submission.to_csv('simple_fastai_v3.csv'),0,"[""sample_submission.to_csv('simple_fastai_v3.csv')""]"
"ASSIGN=train['hour']<7 ASSIGN=train['hour']>19 ASSIGN.astype(int) ASSIGN.astype(int) ASSIGN=np.add(x,x1) ASSIGN.astype(int) SLICE=ASSIGN.astype(int)",0,"[""x=train['hour']<7\n"", ""x1=train['hour']>19\n"", 'x.astype(int)\n', 'x1.astype(int)\n', 'x2=np.add(x,x1)\n', 'x2.astype(int)\n', ""train['night']=x2.astype(int)""]"
CHECKPOINT ASSIGN = data['Chance of Admit '] Y,0,"[""Y = data['Chance of Admit ']\n"", 'Y']"
SETUP ASSIGN = LinearRegression(),0,"['from sklearn.linear_model import LinearRegression\n', 'model = LinearRegression()']"
SETUP ASSIGN = '.path',0,"['import tensorflow as tf\n', 'from tensorflow.keras.models import Sequential, save_model, load_model\n', '\n', ""filepath = './'""]"
SETUP def torch_gc(): gc.collect() torch.cuda.empty_cache(),0,"['def torch_gc():\n', '    import gc\n', '    gc.collect()\n', '    torch.cuda.empty_cache()']"
CHECKPOINT ASSIGN = pd.read_csv('path') print(.format(ASSIGN.shape)) ASSIGN = pd.read_csv('path') print(.format(ASSIGN.shape)),0,"[""train_df = pd.read_csv('/kaggle/input/learn-together/train.csv')\n"", 'print(""Size of Train dataframe is: {}"".format(train_df.shape))\n', ""test_df =  pd.read_csv('/kaggle/input/learn-together/test.csv')\n"", 'print(""Size of Test dataframe is: {}"".format(test_df.shape))']"
covid.tail(),0,['covid.tail()']
"SETUP CHECKPOINT sns.set_style('whitegrid') print(check_output([, ]).decode()) ASSIGN = pd.read_csv('..path') ASSIGN = data[(data.Sex == 'Female')] ASSIGN = women['Category'].value_counts() ASSIGN = by_categorypath ASSIGN = data['Category'].unique() plt.text(0.1, 1, 'Women Laureates Category', ASSIGN='center', ASSIGN='center', ASSIGN=12, color='k') plt.pie(ASSIGN.values, labels = ASSIGN) plt.axis('equal') plt.show() plt.clf()",1,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', ""sns.set_style('whitegrid')\n"", '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n', '\n', 'from subprocess import check_output\n', 'print(check_output([""ls"", ""../input""]).decode(""utf8""))\n', '\n', ""data = pd.read_csv('../input/archive.csv')\n"", ""women = data[(data.Sex == 'Female')]\n"", ""by_category = women['Category'].value_counts()\n"", 'percent = by_category/50\n', ""categories = data['Category'].unique() \n"", '\n', '#title\n', ""plt.text(0.1, 1, 'Women Laureates Category',\n"", ""        horizontalalignment='center',\n"", ""        verticalalignment='center',\n"", ""        fontsize=12, color='k')\n"", '\n', '# Plot\n', 'plt.pie(percent.values, labels = categories)\n', ""plt.axis('equal')\n"", 'plt.show()\n', 'plt.clf()']"
"model.fit(X_train,y_train)",0,"['model.fit(X_train,y_train)']"
"CHECKPOINT zomato.rename({'approx_cost(for two people)': 'approx_cost_for_2_people', 'listed_in(type)':'listed_in_type', 'listed_in(city)':'listed_in_city' }, axis=1, inplace=True) zomato.columns",0,"[""zomato.rename({'approx_cost(for two people)': 'approx_cost_for_2_people',\n"", ""               'listed_in(type)':'listed_in_type',\n"", ""               'listed_in(city)':'listed_in_city'\n"", '              }, axis=1, inplace=True)\n', 'zomato.columns']"
ASSIGN = pd.read_csv('..path'),0,"[""df = pd.read_csv('../input/labels.csv')""]"
SETUP,0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import os\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import keras\n', 'import cv2\n', 'from keras.datasets import fashion_mnist#download mnist data and split into train and test sets\n', 'from sklearn.model_selection import train_test_split\n', 'from sklearn.utils import shuffle\n', 'import matplotlib.pyplot as plt\n', 'from keras.utils import to_categorical\n', 'from keras.models import Sequential\n', 'from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout#create model\n', 'from keras.optimizers import SGD\n', 'from keras.preprocessing.image import ImageDataGenerator\n', '\n', 'from keras.preprocessing.image import load_img\n', 'from keras.preprocessing.image import img_to_array\n', 'from keras.applications.vgg16 import VGG16\n', '\n']"
CHECKPOINT print(os.listdir('..path')),0,"[""print(os.listdir('../input'))""]"
ASSIGN=np.vstack(li),0,"['#numpys vstack method to append all the datafames to stack the sequence of input vertically to make a single array\n', 'df_np=np.vstack(li)']"
"ASSIGN = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse', ASSIGN=None, max_features='auto', max_leaf_nodes=None, ASSIGN=None, min_impurity_decrease=0.0, ASSIGN=None, min_samples_leaf=50, ASSIGN=2, min_weight_fraction_leaf=0.0, ASSIGN=500, n_jobs=-1, oob_score=False, ASSIGN=50, verbose=1, warm_start=False) ASSIGN.fit(X_train, y_train)",0,"[""clf = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n"", ""                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n"", '                      max_samples=None, min_impurity_decrease=0.0,\n', '                      min_impurity_split=None, min_samples_leaf=50,\n', '                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n', '                      n_estimators=500, n_jobs=-1, oob_score=False,\n', '                      random_state=50, verbose=1, warm_start=False)\n', '\n', 'clf.fit(X_train, y_train)\n']"
"ASSIGN=data[data.IS_TRAFFIC==1] plt.figure(figsize=(20,20)) for i in range(6): ASSIGN=data_IS_TRAFFIC[data_IS_TRAFFIC.year==str(2014+i)] plt.subplot(3,2,i+1) plt.scatter('GEO_LON', 'GEO_LAT', data=ASSIGN, c=colors[ASSIGN['year'].iloc[0]]) plt.title(""The distribution of ASSIGN-accident in ""+str(2014+i),size=20) plt.xlabel('Longitude') plt.ylabel('LATITUDE')",1,"['data_IS_TRAFFIC=data[data.IS_TRAFFIC==1]\n', 'plt.figure(figsize=(20,20))\n', 'for i in range(6):\n', '    traffic=data_IS_TRAFFIC[data_IS_TRAFFIC.year==str(2014+i)]\n', '    plt.subplot(3,2,i+1)\n', ""    plt.scatter('GEO_LON', 'GEO_LAT', data=traffic, c=colors[traffic['year'].iloc[0]])\n"", '    plt.title(""The distribution of traffic-accident in ""+str(2014+i),size=20)\n', ""    plt.xlabel('Longitude')\n"", ""    plt.ylabel('LATITUDE')""]"
"submission_df.to_csv('FastAI_v6_corrected.csv', index = False)",0,"['\n', ""submission_df.to_csv('FastAI_v6_corrected.csv', index = False) #""]"
del scale_train['Id'],0,"[""del scale_train['Id']""]"
"CHECKPOINT ASSIGN = RandomForestRegressor(n_estimators = 220) ASSIGN.fit(x_train, Y_train) ASSIGN = model9.score(x_test,Y_test) print(ASSIGN*100,'%')",0,"['model9 = RandomForestRegressor(n_estimators = 220)\n', 'model9.fit(x_train, Y_train)\n', '\n', 'accuracy9 = model9.score(x_test,Y_test)\n', ""print(accuracy9*100,'%')""]"
"sns.set(rc={'figure.figsize':(15,5)}) sns.heatmap(df.isnull(),yticklabels=False)",1,"[""sns.set(rc={'figure.figsize':(15,5)})\n"", 'sns.heatmap(df.isnull(),yticklabels=False)']"
"plt.figure(figsize=(15,15)) sns.heatmap(data_mat.corr(),annot = True,fmt = "".2f"",cbar = True) plt.xticks(rotation=90) plt.yticks(rotation = 0)",1,"['plt.figure(figsize=(15,15))\n', 'sns.heatmap(data_mat.corr(),annot = True,fmt = "".2f"",cbar = True)\n', 'plt.xticks(rotation=90)\n', 'plt.yticks(rotation = 0)']"
"pd.set_option('mode.chained_assignment', None)",0,"[""pd.set_option('mode.chained_assignment', None)""]"
"ASSIGN = 0.0002 ASSIGN = nn.NLLLoss() ASSIGN = None def model_train_loop(cnn_config): dag_model.config = cnn_config dag_model.initialize_param_grads() for epoch in range(10): for i, data in enumerate(trainloader, 0): ASSIGN = data ASSIGN = images.to(device),labels.to(device) ASSIGN = dag_model(images) ASSIGN==0 and epoch==0: ASSIGN = optim.Adam(dag_model.myparameters, lr=lr, betas=(0.5, 0.999)) dag_model.zero_grad() ASSIGN = criterion(output, labels) ASSIGN.backward() ASSIGN.step() return Variable(((torch.sum(torch.argmax(ASSIGN,1) == labels)).float()path[0]), requires_grad=True)",0,"['lr = 0.0002\n', '# Initialize BCELoss function\n', 'criterion = nn.NLLLoss()\n', 'optimizer = None\n', '\n', '\n', 'def model_train_loop(cnn_config):\n', '    dag_model.config = cnn_config\n', '    dag_model.initialize_param_grads()\n', '    for epoch in range(10):\n', '        # For each batch in the dataloader\n', '        for i, data in enumerate(trainloader, 0):\n', '            images, labels = data\n', '            images, labels = images.to(device),labels.to(device)\n', '            output = dag_model(images)\n', '            if i==0 and epoch==0:\n', '                optimizer = optim.Adam(dag_model.myparameters, lr=lr, betas=(0.5, 0.999))\n', '            dag_model.zero_grad()\n', '            loss = criterion(output, labels)\n', '            loss.backward()\n', '            optimizer.step()\n', '#         print(loss)\n', '#             if i%100==0:\n', '#                 print(((torch.sum(torch.argmax(output,1) == labels)).float()/labels.shape[0]).item())\n', '#     #             print(loss.item())\n', '        \n', '    return Variable(((torch.sum(torch.argmax(output,1) == labels)).float()/labels.shape[0]), requires_grad=True)']"
len(test_transaction_new),0,['len(test_transaction_new)']
"ASSIGN = ['char_count', 'word_count', 'word_density', 'punctuation_count'] ASSIGN = plt.subplots(nrows=2, ncols=2, figsize=(10, 6), sharey=True) for i, feature in enumerate(ASSIGN): df.hist(column=feature, ax=axes.flatten()[i])",1,"[""#df.hist(column=['char_count', 'word_count'])\n"", ""features = ['char_count', 'word_count', 'word_density', 'punctuation_count']\n"", 'fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 6), sharey=True)\n', '\n', 'for i, feature in enumerate(features):\n', '    df.hist(column=feature, ax=axes.flatten()[i])\n']"
zomato_en['rate'] = zomato_en['rate'].fillna(zomato_en['rate'].mean()) zomato_en['approx_cost_for_2_people'] = zomato_en['approx_cost_for_2_people'].fillna(zomato_en['approx_cost_for_2_people'].mean()),0,"[""zomato_en['rate'] = zomato_en['rate'].fillna(zomato_en['rate'].mean())\n"", ""zomato_en['approx_cost_for_2_people'] = zomato_en['approx_cost_for_2_people'].fillna(zomato_en['approx_cost_for_2_people'].mean())""]"
"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])",0,"['#compile model using accuracy to measure model performance\n', ""model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])""]"
HTML('<iframe src=COVID.html width=1000 height=450><path>'),1,"[""HTML('<iframe src=COVID.html width=1000 height=450></iframe>')""]"
fastai.__version__,0,"['# check version\n', 'fastai.__version__']"
df2.tail(),0,"['## I will use train set only\n', 'df2.tail()']"
"ASSIGN = list(pd.DataFrame(X_train).columns) ASSIGN = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames}) ASSIGN.head()",0,"['colnames = list(pd.DataFrame(X_train).columns)\n', ""pcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\n"", 'pcs_df.head()']"
ASSIGN = models.densenet161(pretrained=True) for param in ASSIGN.parameters(): param.requires_grad = False,0,"['model = models.densenet161(pretrained=True)\n', '# Turn off training for their parameters\n', 'for param in model.parameters():\n', '    param.requires_grad = False']"
"tf.keras.models.save_model( model, filepath, ASSIGN=True, ASSIGN=True, ASSIGN=None, ASSIGN=None, ASSIGN=None )",0,"['tf.keras.models.save_model(\n', '    model,\n', '    filepath,\n', '    overwrite=True,\n', '    include_optimizer=True,\n', '    save_format=None,\n', '    signatures=None,\n', '    options=None\n', ')']"
ASSIGN = result['Survived'],0,"[""y_test = result['Survived']""]"
learn.lr_find() learn.recorder.plot(suggestion=True),1,"['learn.lr_find()\n', 'learn.recorder.plot(suggestion=True)']"
"ASSIGN = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', max_features=5000) ASSIGN.fit(df['review']) ASSIGN = tfidf_vect.transform(X_train) ASSIGN = tfidf_vect.transform(X_val) ASSIGN = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', ngram_range=(2,3), max_features=5000) ASSIGN.fit(df['review']) ASSIGN = tfidf_vect_ngram.transform(X_train) ASSIGN = tfidf_vect_ngram.transform(X_val) ASSIGN = TfidfVectorizer(analyzer='char', token_pattern=r'\w{1,}', ngram_range=(2,3), max_features=5000) ASSIGN.fit(df['review']) ASSIGN = tfidf_vect_ngram_chars.transform(X_train) ASSIGN = tfidf_vect_ngram_chars.transform(X_val) ASSIGN = tfidf_vect.transform(test_df['review']) ASSIGN = tfidf_vect_ngram.transform(test_df['review']) ASSIGN = tfidf_vect_ngram_chars.transform(test_df['review'])",0,"[""# Converting X_train and X_val to tfidf vectors (since out models can't take text data is input)\n"", ""tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n"", ""tfidf_vect.fit(df['review'])\n"", 'xtrain_tfidf =  tfidf_vect.transform(X_train)\n', 'xvalid_tfidf =  tfidf_vect.transform(X_val)\n', '\n', '# ngram level tf-idf \n', ""tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n"", ""tfidf_vect_ngram.fit(df['review'])\n"", 'xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n', 'xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_val)\n', '\n', '# characters level tf-idf\n', ""tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n"", ""tfidf_vect_ngram_chars.fit(df['review'])\n"", 'xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n', 'xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_val) \n', '\n', ""# Also creating for the X_test which is essentially test_df['review'] column\n"", ""xtest_tfidf =  tfidf_vect.transform(test_df['review'])\n"", ""xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_df['review'])\n"", ""xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_df['review']) ""]"
"CHECKPOINT for x,y in city_vs_count.items(): if(y==count_max): print(x)",0,"['#lets find for city count is max\n', '\n', 'for x,y in city_vs_count.items():\n', '    if(y==count_max):\n', '        print(x)\n', '    ']"
"CHECKPOINT ASSIGN = data_features[:len(y_train)] ASSIGN.loc[:,'SalePrice'] = y_train ASSIGN = train1.corr()['SalePrice'].sort_values(ascending = False) ASSIGN = corr1[:15] corr15",0,"['train1 = data_features[:len(y_train)]\n', ""train1.loc[:,'SalePrice'] = y_train\n"", ""corr1 = train1.corr()['SalePrice'].sort_values(ascending = False)\n"", 'corr15 = corr1[:15]\n', 'corr15']"
ASSIGN = ASSIGN[ASSIGN['date_account_created'] > '2013-02-01'],0,"[""df_all = df_all[df_all['date_account_created'] > '2013-02-01']""]"
"ASSIGN=lines[lines.city_id==114] ASSIGN=track_lines[track_lines.city_id==114] ASSIGN=tracks[tracks.city_id==114].drop(columns=['buildstart','opening','closure','city_id']) ASSIGN.columns=['section_id','geometry','length'] ASSIGN=pd.merge(ASSIGN,tokyo_tracks) ASSIGN=ASSIGN.drop(columns=['id','created_at','updated_at','city_id']) ASSIGN.columns=['section_id','id','geometry','length'] ASSIGN=pd.merge(tokyo_track_lines,ASSIGN) ASSIGN=stations[stations['city_id']==114] ASSIGN.head()",0,"['tokyo_lines=lines[lines.city_id==114]\n', 'tokyo_track_lines=track_lines[track_lines.city_id==114]\n', ""tokyo_tracks=tracks[tracks.city_id==114].drop(columns=['buildstart','opening','closure','city_id'])\n"", ""tokyo_tracks.columns=['section_id','geometry','length']\n"", 'tokyo_track_lines=pd.merge(tokyo_track_lines,tokyo_tracks)\n', ""tokyo_track_lines=tokyo_track_lines.drop(columns=['id','created_at','updated_at','city_id'])\n"", ""tokyo_track_lines.columns=['section_id','id','geometry','length']\n"", 'tokyo_lines=pd.merge(tokyo_track_lines,tokyo_lines)\n', ""tokyo_stations=stations[stations['city_id']==114]\n"", 'tokyo_stations.head()']"
SETUP,0,"['import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import os\n', 'from sklearn import preprocessing\n', 'import lightgbm as lgb\n', 'from sklearn.model_selection import KFold, StratifiedKFold\n', 'from sklearn.model_selection import train_test_split\n', 'from bayes_opt import BayesianOptimization\n', 'import seaborn as sns\n', 'from sklearn import metrics\n', 'from sklearn.model_selection import StratifiedKFold\n', 'from sklearn.metrics import mean_squared_error\n', 'from sklearn.metrics import roc_auc_score\n', 'from sklearn.metrics import accuracy_score\n', 'from sklearn.metrics import recall_score\n', 'from sklearn.metrics import f1_score\n', 'from sklearn.metrics import auc\n', 'from sklearn.metrics import precision_score\n', 'from sklearn.metrics import roc_curve\n', 'from scipy.interpolate import interp1d\n', 'import matplotlib.pyplot as plt\n', 'from sklearn.metrics import confusion_matrix']"
"xgb.fit(x_train,x_test) gbc.score(y_train,y_test)",0,"['xgb.fit(x_train,x_test)\n', 'gbc.score(y_train,y_test)']"
"final.drop(['Cabin'],axis = 1,inplace = True)",0,"[""final.drop(['Cabin'],axis = 1,inplace = True)""]"
"SETUP ASSIGN = {'C':[.0001 ,.001 ,0.1, 1, 10, 100, 1000], 'epsilon':[0.001, 0.01, 0.1, 0.5, 1, 2, 4] } ENR.fit(x_train, Y_train) ENR.best_params_",0,"['SVR = SVR()\n', '\n', ""parameters = {'C':[.0001 ,.001 ,0.1, 1, 10, 100, 1000],\n"", ""              'epsilon':[0.001, 0.01, 0.1, 0.5, 1, 2, 4]\n"", '             }\n', '\n', ""ENR = GridSearchCV(SVR, parameters, scoring='neg_mean_squared_error', cv = 10)\n"", '\n', 'ENR.fit(x_train, Y_train)\n', 'ENR.best_params_']"
"ASSIGN = pd.DataFrame(columns = ['Country','Province','Date','confirmed_pred','fata_pred']) for country in test_df.Country.unique(): for province in test_df.query(f""Country == '{country}'"")['Province'].unique(): ASSIGN = get_pred_for_province(country,province) ASSIGN = pd.concat((ASSIGN,province_pred),0)",0,"[""pred_table = pd.DataFrame(columns = ['Country','Province','Date','confirmed_pred','fata_pred'])\n"", 'for country in test_df.Country.unique():\n', '    for province in test_df.query(f""Country == \'{country}\'"")[\'Province\'].unique():\n', '        province_pred = get_pred_for_province(country,province)\n', '        pred_table = pd.concat((pred_table,province_pred),0)']"
"sns.set(style=""darkgrid"") ASSIGN = sns.regplot(x=""calories"", y=""dessert"", data=recipes, logistic=True) ASSIGN.figure.set_size_inches(8, 8)",1,"['sns.set(style=""darkgrid"")\n', 'g = sns.regplot(x=""calories"", y=""dessert"", data=recipes, logistic=True)\n', 'g.figure.set_size_inches(8, 8)']"
"CHECKPOINT ASSIGN = [""UNKNOWN MEDIUM BIRD"",""UNKNOWN SMALL BIRD"",""MOURNING DOVE"", ""GULL"",""UNKNOWN BIRD"",""KILLDEER"", ""AMERICAN KESTREL"",""BARN SWALLOW""] ASSIGN = species[species.isin(ASSIGN)] print(ASSIGN.value_counts())",0,"['top_species = [""UNKNOWN MEDIUM BIRD"",""UNKNOWN SMALL BIRD"",""MOURNING DOVE"", ""GULL"",""UNKNOWN BIRD"",""KILLDEER"", ""AMERICAN KESTREL"",""BARN SWALLOW""]\n', 'top_species = species[species.isin(top_species)]\n', 'print(top_species.value_counts())']"
cd ..path,0,['cd ../input/european-soccer-csv-files/']
SETUP,0,"['from statsmodels.tsa.ar_model import AR,ARResults']"
"ASSIGN = cnn_learner(data, models.resnet34, metrics=error_rate) ASSIGN.load('stage-2') ASSIGN = ClassificationInterpretation.from_learner(learn) ASSIGN.plot_top_losses(9, figsize=(12,8)) ASSIGN.plot_top_losses(9, figsize=(12,8),heatmap=False) ASSIGN.plot_confusion_matrix()",1,"['learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n', ""learn.load('stage-2')\n"", 'interp = ClassificationInterpretation.from_learner(learn)\n', 'interp.plot_top_losses(9, figsize=(12,8))\n', 'interp.plot_top_losses(9, figsize=(12,8),heatmap=False)\n', 'interp.plot_confusion_matrix()']"
"ASSIGN=data[data['YEAR']==2018][0:2000] ASSIGN=""Crime2018"" ASSIGN=folium.Map([Lat,Lon],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(boston_map) for lat,lon,label in zip(ASSIGN.Lat,ASSIGN.Long,ASSIGN.OFFENSE_CODE_GROUP): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN)",1,"[""data1=data[data['YEAR']==2018][0:2000]\n"", 'filename=""Crime2018""\n', 'boston_map=folium.Map([Lat,Lon],zoom_start=12)\n', 'incidents2=plugins.MarkerCluster().add_to(boston_map)\n', 'for lat,lon,label in zip(data1.Lat,data1.Long,data1.OFFENSE_CODE_GROUP):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(incidents2)\n', 'boston_map.add_child(incidents2)\n']"
"plt.figure(figsize=(15,6)) sns.distplot(points_df['z'], bins=500); plt.xlabel('z') plt.show()",1,"['plt.figure(figsize=(15,6))\n', ""sns.distplot(points_df['z'], bins=500);\n"", ""plt.xlabel('z')\n"", 'plt.show()']"
"ASSIGN = learn.get_preds(ds_type=DatasetType.Test) ASSIGN = ASSIGN[:, 1]",0,"['# predict test classes...\n', 'probas_test, _ = learn.get_preds(ds_type=DatasetType.Test) # run inference on test\n', 'probas_test = probas_test[:, 1] # only get probability tensor']"
"ASSIGN = ['bone_length','rotting_flesh','hair_length','has_soul'] ASSIGN = ['color','type']",0,"[""numerical = ['bone_length','rotting_flesh','hair_length','has_soul']\n"", ""categorical = ['color','type']""]"
"ASSIGN = pd.read_csv('..path') ASSIGN['isFraud'] = result_dict_lgb['prediction'] ASSIGN.to_csv('submission_IEEE_.csv',index=False)",0,"[""sample_submission = pd.read_csv('../input/sample_submission.csv')\n"", ""sample_submission['isFraud'] = result_dict_lgb['prediction'] \n"", ""sample_submission.to_csv('submission_IEEE_.csv',index=False)""]"
"ASSIGN=plt.subplots(1,2,figsize=(15,8)) sns.barplot(x=data_risk1['Facility Type'].value_counts()[:10],y=data_risk1['Facility Type'].value_counts()[:10].index,ax=ax[0]) ax[0].set_title(""Top 10 Facility Type by the counts of risk 1 "",size=20) ax[0].set_xlabel('counts',size=18) ASSIGN=data_risk1.groupby(['Facility Type'])['Inspection ID'].agg('ASSIGN').sort_values(ascending=False) ASSIGN=list(data_risk1.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10]) ASSIGN=list(count[:10]) ASSIGN.append(ASSIGN.agg(sum)-ASSIGN[:10].agg('sum')) ASSIGN.append('Other') ASSIGN=pd.DataFrame({""group"":groups,""counts"":counts}) ASSIGN=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum') ASSIGN.plot(kind='pie', y='ASSIGN', labels=ASSIGN,colors=ASSIGN,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1]) ax[1].set_ylabel('') ax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))",1,"['fig,ax=plt.subplots(1,2,figsize=(15,8))\n', ""sns.barplot(x=data_risk1['Facility Type'].value_counts()[:10],y=data_risk1['Facility Type'].value_counts()[:10].index,ax=ax[0])\n"", 'ax[0].set_title(""Top 10 Facility Type by the counts of risk 1 "",size=20)\n', ""ax[0].set_xlabel('counts',size=18)\n"", '\n', '\n', ""count=data_risk1.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False)\n"", ""groups=list(data_risk1.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10])\n"", 'counts=list(count[:10])\n', ""counts.append(count.agg(sum)-count[:10].agg('sum'))\n"", ""groups.append('Other')\n"", 'type_dict=pd.DataFrame({""group"":groups,""counts"":counts})\n', ""clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n"", ""type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n"", ""ax[1].set_ylabel('')\n"", 'ax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))']"
df.head(),0,['df.head()']
"CHECKPOINT ASSIGN = first_digit_df.copy() ASSIGN = second_digit_df.copy() ASSIGN = third_digit_df.copy() ASSIGN = [] for x in [ASSIGN, ASSIGN, ASSIGN]: ASSIGN.append(x.index.where(x['Difference (%)'] > 5).dropna()) ASSIGN = { 'First Digit Significant Values' : ASSIGN[0].astype(int).values, 'Second Digit Significant Values' : ASSIGN[1].astype(int).values, 'Third Digit Significant Values' : temp[2].astype(int).values } significant_numbers",0,"[""#We create new variables called 'significant numbers' which we will alter in order to deliver us just the significant digits\n"", '\n', 'first_digit_significant_numbers = first_digit_df.copy()\n', 'second_digit_significant_numbers = second_digit_df.copy()\n', 'third_digit_significant_numbers = third_digit_df.copy()\n', '\n', ""#This for loop adds the significant digit to a list called 'temp' when the difference of the number is greater than 5%\n"", 'temp = []\n', 'for x in [first_digit_significant_numbers, second_digit_significant_numbers, third_digit_significant_numbers]:\n', ""    temp.append(x.index.where(x['Difference (%)'] > 5).dropna())\n"", '\n', '#Now we convert the list into a cleaner version which will be easier to work with and put it in a dictionary\n', 'significant_numbers = {\n', ""    'First Digit Significant Values' : temp[0].astype(int).values,\n"", ""    'Second Digit Significant Values' : temp[1].astype(int).values,\n"", ""    'Third Digit Significant Values' : temp[2].astype(int).values\n"", '}\n', '#here is an example of what those numbers are\n', 'significant_numbers']"
"plt.figure(figsize=(50,60)) ASSIGN=Basemap(width=120000,height=900000,projection=""lcc"",resolution=""l"",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77) ASSIGN.drawcountries() ASSIGN.drawmapboundary(color=' ASSIGN.drawcoastlines() ASSIGN=np.array(df_plot_top[""lng""]) ASSIGN=np.array(df_plot_top[""ASSIGN""]) ASSIGN=np.array(df_plot_top[""COUNT""]) ASSIGN=np.array(df_plot_top[""CITY""]) ASSIGN=map(lg,lat) ASSIGN=df_plot_top[""COUNT""].apply(lambda x: int(x)path) plt.scatter(ASSIGN,s=ASSIGN,marker=""o"",c=ASSIGN) plt.title(""TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO"",fontsize=30,color='RED')",1,"['#lets plot this inside the map corresponding to the cities exact co-ordinates which we received from google api ,here marker color will be different as per marker size\n', '#plt.subplots(figsize=(20,50))\n', 'plt.figure(figsize=(50,60))\n', 'map=Basemap(width=120000,height=900000,projection=""lcc"",resolution=""l"",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n', 'map.drawcountries()\n', ""map.drawmapboundary(color='#f2f2f2')\n"", '\n', 'map.drawcoastlines()\n', '\n', '\n', '\n', 'lg=np.array(df_plot_top[""lng""])\n', 'lat=np.array(df_plot_top[""lat""])\n', 'pt=np.array(df_plot_top[""COUNT""])\n', 'city_name=np.array(df_plot_top[""CITY""])\n', '\n', 'x,y=map(lg,lat)\n', '\n', '#using lambda function to create different sizes of marker as per thecount \n', '\n', 'p_s=df_plot_top[""COUNT""].apply(lambda x: int(x)/2)\n', '\n', '#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is different.\n', 'plt.scatter(x,y,s=p_s,marker=""o"",c=p_s)\n', 'plt.title(""TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO"",fontsize=30,color=\'RED\')']"
ASSIGN=data[data.neighbourhood_group=='Brooklyn'] ASSIGN.head(),0,"[""data_Brooklyn=data[data.neighbourhood_group=='Brooklyn']\n"", 'data_Brooklyn.head()']"
"SETUP ASSIGN = torch.ASSIGN(""cuda:0"" if torch.cuda.is_available() else ""cpu"")",0,"[""ROOT_DIR = '../input/virtual-hack/car_data/car_data'\n"", ""TRAIN_DIR = '../input/virtual-hack/car_data/car_data/train'\n"", ""TEST_DIR = '../input/virtual-hack/car_data/car_data/test'\n"", 'device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n', 'NUM_CLASSES = 196']"
test_data.head(),0,['test_data.head()']
"CHECKPOINT ASSIGN=1 ASSIGN = ImageData() ASSIGN = DataLoader(dataset, batch_size=batch_size, shuffle=True) ASSIGN = next(iter(dataloader)) print(ASSIGN[1].shape) print(ASSIGN[0].shape) ASSIGN = a[0][0] ASSIGN = a[1][0] ASSIGN = plt.subplots(1,2) axarr[0].imshow(ASSIGN.permute(1,2,0)) axarr[1].imshow(ASSIGN.permute(1,2,0)) f.set_figheight(24) f.set_figwidth(24)",1,"['batch_size=1\n', 'dataset = ImageData()\n', 'dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n', '\n', 'a = next(iter(dataloader))\n', '\n', 'print(a[1].shape)\n', 'print(a[0].shape)\n', 'img1 = a[0][0]\n', 'img2 = a[1][0]\n', 'f, axarr = plt.subplots(1,2)\n', 'axarr[0].imshow(img1.permute(1,2,0))\n', 'axarr[1].imshow(img2.permute(1,2,0))\n', 'f.set_figheight(24)\n', 'f.set_figwidth(24)']"
artime.head(),0,['artime.head()']
df.head(),0,['df.head()']
ASSIGN = [] for prediction_string in prediction_strings: for car in prediction_string: ASSIGN.append(car) ASSIGN = np.array(ASSIGN),0,"['cars = []\n', 'for prediction_string in prediction_strings:\n', '    for car in prediction_string:\n', '        cars.append(car)\n', 'cars = np.array(cars)']"
"plt.bar(index.value_counts().index,index.value_counts()) plt.xlabel(""Crime type"") plt.ylabel(""Count"") plt.title(""Counting for Crime type"") plt.show()",1,"['plt.bar(index.value_counts().index,index.value_counts())\n', 'plt.xlabel(""Crime type"")\n', 'plt.ylabel(""Count"")\n', 'plt.title(""Counting for Crime type"")\n', 'plt.show()\n']"
"def rank_NEIGHBORHOOD(NEIGHBORHOOD_ID): ASSIGN=['2014','2015','2016','2017','2018','2019'] B={} for i in range(len(ASSIGN)): A=data[data.ASSIGN==ASSIGN[i]] ASSIGN=A.groupby(['NEIGHBORHOOD_ID'])['OFFENSE_ID'].agg('count') ASSIGN=A.groupby(['NEIGHBORHOOD_ID'])['OFFENSE_ID'].agg('count').ASSIGN(method='min',ascending=False) ASSIGN=pd.DataFrame({'rank':rank,'value':value}) B['ASSIGN '+ASSIGN[i]]=str(ASSIGN[ASSIGN.index==NEIGHBORHOOD_ID].iloc[0,0])+""path""+str(max(ASSIGN)) B['ASSIGN '+ASSIGN[i]]=str(ASSIGN[ASSIGN.index==NEIGHBORHOOD_ID].iloc[0,1]) return B",0,"['def rank_NEIGHBORHOOD(NEIGHBORHOOD_ID):\n', ""    year=['2014','2015','2016','2017','2018','2019']\n"", '    B={}\n', '    for i in range(len(year)):\n', '        A=data[data.year==year[i]]\n', ""        value=A.groupby(['NEIGHBORHOOD_ID'])['OFFENSE_ID'].agg('count')\n"", ""        rank=A.groupby(['NEIGHBORHOOD_ID'])['OFFENSE_ID'].agg('count').rank(method='min',ascending=False)\n"", ""        new=pd.DataFrame({'rank':rank,'value':value})\n"", '        B[\'rank \'+year[i]]=str(new[new.index==NEIGHBORHOOD_ID].iloc[0,0])+""/""+str(max(rank))\n', ""        B['value '+year[i]]=str(new[new.index==NEIGHBORHOOD_ID].iloc[0,1])\n"", '\n', '    return B']"
"plt.figure(figsize=(12,5)) plt.title(""Distribution of image categories(Target Variable)"") ASSIGN = sns.distplot(train[""category_id""])",1,"['plt.figure(figsize=(12,5))\n', 'plt.title(""Distribution of image categories(Target Variable)"")\n', 'ax = sns.distplot(train[""category_id""])']"
"sns.catplot(x='Transmission', y ='Selling_Price', kind = 'swarm',data= data)",1,"[""sns.catplot(x='Transmission', y ='Selling_Price', kind = 'swarm',data= data)""]"
"SETUP CHECKPOINT ASSIGN=[72,71,56,45,67,89,54,58,67,77,77,78,77,73,73,172,72,71,56,45,67, 89,54,58,67,172,77,78,77,73,73,172,12,54,64,75,75,77,88,66,70,12,54,64,75,75,77,88,66,70] def plot_his(ASSIGN): ASSIGN=min(heights)-min(heights)%10 ASSIGN=max(heights)+10 ASSIGN=list(range(start,end,5)) plt.hist(ASSIGN,ASSIGN,histtype='bar',rwidth=0.5,color=' plt.xlabel('ASSIGN in inches') plt.ylabel('No. of Students') plt.title(""Heights chart"") plt.show() print() plot_his(ASSIGN) ASSIGN=list(filter(lambda x: not x==172 and not x==12, ASSIGN)) print() plot_his(ASSIGN)",1,"['#2. Store height of 50 students in inches. Now while the data was beign recorded manually there has been some typing mistake and therefore height of 2 students have been recorded as 172 inch and 2 students have been recorded as 12 inch. Graphically plot and show how you can seggregate correct data from abnormal data.\n', 'from matplotlib import pyplot as plt\n', 'heights=[72,71,56,45,67,89,54,58,67,77,77,78,77,73,73,172,72,71,56,45,67,\n', '         89,54,58,67,172,77,78,77,73,73,172,12,54,64,75,75,77,88,66,70,12,54,64,75,75,77,88,66,70]\n', 'def plot_his(heights):\n', '    start=min(heights)-min(heights)%10\n', '    end=max(heights)+10\n', '    bins=list(range(start,end,5))\n', ""    plt.hist(heights,bins,histtype='bar',rwidth=0.5,color='#FF2400')\n"", ""    plt.xlabel('heights in inches')\n"", ""    plt.ylabel('No. of Students')\n"", '    plt.title(""Heights chart"")\n', '    plt.show()\n', 'print(""Abnormal Data"")\n', 'plot_his(heights)\n', 'heights=list(filter(lambda x: not x==172 and not x==12, heights))\n', 'print(""Correct Data"")\n', 'plot_his(heights)']"
learn.recorder.plot_losses(),1,['learn.recorder.plot_losses()']
"ASSIGN = 30 ASSIGN = model.fit(x_train, y_train, batch_size = 128, epochs = num_epochs, verbose = 1, validation_data=(x_test, y_test));",0,"['# Train Model\n', 'num_epochs = 30\n', 'history = model.fit(x_train, y_train, batch_size = 128, epochs = num_epochs, verbose = 1, validation_data=(x_test, y_test));']"
ASSIGN = ASSIGN,0,"[""relevant['Neutrophils'] = df['Neutrophils']""]"
SETUP ASSIGN = pd.read_csv(f'{DATA_PATH}path') ASSIGN = pd.read_csv(f'{DATA_PATH}path'),0,"[""DATA_PATH = '/kaggle/input/rs6-attrition-predict/'\n"", ""train = pd.read_csv(f'{DATA_PATH}/train.csv')\n"", ""test = pd.read_csv(f'{DATA_PATH}/test.csv')""]"
headline_by_year(1996),0,['headline_by_year(1996)']
"ASSIGN = pd.DataFrame(sorted(list(set(test.Date))),columns=['Date']) ASSIGN = ASSIGN[7:] ASSIGN.reset_index(inplace = True) del ASSIGN['index'] ASSIGN = (datetime.strptime('2020-05-14','%Y-%m-%d')-datetime.strptime('2020-04-08','%Y-%m-%d')).days ASSIGN = ['temp', 'min', 'max', 'stp', 'slp', 'dewp', 'rh', 'ah','wdsp', 'prcp', 'fog'] ASSIGN = pd.DataFrame(columns=['Date','temp', 'min', 'max', 'stp', 'slp', 'dewp', 'rh', 'ah','wdsp', 'prcp', 'fog']) for prov in list(set(train.Province)): ASSIGN = train[train.Province == prov] ASSIGN = date_pred_df.copy() for feature in ASSIGN: ASSIGN = df[feature] ASSIGN = pmdarima.auto_arima(ts) ASSIGN = model.predict(n_periods = nperiods) ASSIGN[feature] = ASSIGN ASSIGN['Province'] = prov ASSIGN = pd.concat([ASSIGN,province_pred],axis = 0)",0,"['#Using arima to predict the weather information for future\n', ""date_pred_df = pd.DataFrame(sorted(list(set(test.Date))),columns=['Date'])\n"", 'date_pred_df = date_pred_df[7:]\n', 'date_pred_df.reset_index(inplace = True)\n', ""del date_pred_df['index']\n"", '\n', ""nperiods = (datetime.strptime('2020-05-14','%Y-%m-%d')-datetime.strptime('2020-04-08','%Y-%m-%d')).days\n"", '\n', ""weather_feature = ['temp', 'min', 'max', 'stp', 'slp', 'dewp', 'rh', 'ah','wdsp', 'prcp', 'fog']\n"", ""weather_pred = pd.DataFrame(columns=['Date','temp', 'min', 'max', 'stp', 'slp', 'dewp', 'rh', 'ah','wdsp', 'prcp', 'fog'])\n"", 'for prov in list(set(train.Province)):\n', '    df = train[train.Province == prov]\n', '    province_pred = date_pred_df.copy()\n', '    for feature in weather_feature:\n', '        ts = df[feature]\n', '        model = pmdarima.auto_arima(ts)\n', '        pred = model.predict(n_periods = nperiods)\n', '        province_pred[feature] = pred\n', ""    province_pred['Province'] = prov\n"", '    weather_pred = pd.concat([weather_pred,province_pred],axis = 0)']"
total.tail(20),0,['total.tail(20)']
"CHECKPOINT replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=""kuram agency"") ASSIGN = suicide_attacks['City'].unique() ASSIGN.sort() cities",0,"[""# Your turn! It looks like 'kuram agency' and 'kurram agency' should\n"", '# be the same city. Correct the dataframe so that they are.\n', '\n', '\n', 'replace_matches_in_column(df=suicide_attacks, column=\'City\', string_to_match=""kuram agency"")\n', '\n', ""# get all the unique values in the 'City' column\n"", ""cities = suicide_attacks['City'].unique()\n"", '\n', '# sort them alphabetically and then take a closer look\n', 'cities.sort()\n', 'cities']"
"CHECKPOINT ASSIGN = time.time() ASSIGN = model.fit(train_x,train_y,validation_data=(x_val, y_val),batch_size=batch_size,epoch=EPOCHS,verbose=0) ASSIGN = time.time() print(round((ASSIGN-ASSIGN),2), )",0,"['# fit model\n', 'start = time.time()\n', 'history = model.fit(train_x,train_y,validation_data=(x_val, y_val),batch_size=batch_size,epoch=EPOCHS,verbose=0)\n', 'end = time.time()\n', 'print(round((end-start),2), ""seconds"")']"
"ASSIGN = ASSIGN.drop(['Patient addmited to intensive care unit (1=yes, 0=no)'], axis=1, inplace=True) ASSIGN = relevant",0,"[""covid = covid.drop(['Patient addmited to intensive care unit (1=yes, 0=no)'], axis=1, inplace=True)\n"", 'covid = relevant']"
SETUP,0,"['from scipy.special import boxcox1p\n', 'from scipy.stats import boxcox_normmax\n', 'from sklearn.base import BaseEstimator, TransformerMixin']"
"ASSIGN = {k:v for k,v in zip(data.path.values,data.target_label.values)}",0,"['target_label_map = {k:v for k,v in zip(data.path.values,data.target_label.values)}']"
ASSIGN = '..path' ASSIGN = '..path' ASSIGN = '..path',0,"[""x_train_path = '../input/red-sample/train_blur/'\n"", ""y_train_path = '../input/red-sample/train_sharp/'\n"", ""x_test_path = '../input/red-sample/test_blur/'""]"
ASSIGN = LogisticRegression(solver = 'lbfgs'),0,"[""logisticRegr = LogisticRegression(solver = 'lbfgs')""]"
ASSIGN=ASSIGN.fillna(ASSIGN.median()) ASSIGN=ASSIGN.fillna(train_transaction_new.median()),0,"['#train_transaction_new=train_transaction_new.fillna(-999)\n', '#test_transaction_new=test_transaction_new.fillna(-999)\n', 'train_transaction_new=train_transaction_new.fillna(train_transaction_new.median())\n', 'test_transaction_new=test_transaction_new.fillna(train_transaction_new.median())']"
dtree.feature_importances_,0,"['#不同資料與結果的關聯性\n', 'dtree.feature_importances_\n']"
"ASSIGN = Lasso() ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]} ASSIGN = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv = 100) ASSIGN.fit(X_train, y_train) ASSIGN.best_params_",0,"['lasso = Lasso()\n', '\n', ""parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n"", '\n', ""lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv = 100)\n"", '\n', 'lasso_regressor.fit(X_train, y_train)\n', '\n', 'lasso_regressor.best_params_']"
CHECKPOINT data.dtypes,0,['data.dtypes']
"SETUP ASSIGN = SVC() ASSIGN.fit(X_train, Y_train)",0,"['from sklearn.svm import SVC\n', 'svc_model = SVC()\n', 'svc_model.fit(X_train, Y_train)']"
CHECKPOINT sns.distplot(train['SalePrice']) print('Skewness: %f' % train['SalePrice'].skew()) print('Kurtosis: %f' % train['SalePrice'].kurt()),1,"[""sns.distplot(train['SalePrice'])\n"", ""print('Skewness: %f' % train['SalePrice'].skew())\n"", ""print('Kurtosis: %f' % train['SalePrice'].kurt())""]"
SETUP,0,"['import sklearn as sklearn\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', 'import sys\n', 'import re']"
"plt.figure(figsize=(8,8)) ASSIGN = train_identity_new.ASSIGN() sns.heatmap(ASSIGN, xticklabels=ASSIGN.columns,yticklabels=ASSIGN.columns,annot=True) plt.title(""correlation plot for train_identity_new"",size=28)",1,"['plt.figure(figsize=(8,8))\n', 'corr = train_identity_new.corr()\n', 'sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns,annot=True)\n', 'plt.title(""correlation plot for train_identity_new"",size=28)']"
CHECKPOINT predictions,0,['predictions']
csvData.head(),0,['csvData.head()']
"ASSIGN=plt.subplots(2,2,figsize=(20,16)) ASSIGN=data.Results.value_counts().index ASSIGN=data.Results.value_counts() sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,0]) ax[0,0].set_title(""The counts of Results of inspection "",size=20) ax[0,0].set_ylabel('counts',size=18) ax[0,0].set_xlabel('') data.groupby(['Results','year'])['Inspection ID'].agg('count').unstack('Results').plot(kind='bar',ax=ax[0,1]) ax[0,1].tick_params(axis='ASSIGN',labelrotation=360) ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,0.75)) ax[0,1].set_title(""The counts of results of inspection by year "",size=20) ax[0,1].set_ylabel('counts',size=18) sns.scatterplot(ASSIGN='Longitude',ASSIGN='Latitude',hue='Risk' ,hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'],data=data[data.Results=='Pass'], ax=ax[1,0]) ax[1,0].set_title(""The distribution of result is pass"",size=20) ax[1,0].set_xlabel('Longitude') ax[1,0].set_ylabel('LATITUDE') sns.scatterplot(ASSIGN='Longitude',ASSIGN='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=data[data.Results=='Fail'], ax=ax[1,1]) ax[1,1].set_title(""The distribution of result is fail"",size=20) ax[1,1].set_xlabel('Longitude') ax[1,1].set_ylabel('LATITUDE')",1,"['fig,ax=plt.subplots(2,2,figsize=(20,16))\n', 'x=data.Results.value_counts().index\n', 'y=data.Results.value_counts()\n', 'sns.barplot(x=x,y=y,ax=ax[0,0])\n', 'ax[0,0].set_title(""The counts of Results of inspection "",size=20)\n', ""ax[0,0].set_ylabel('counts',size=18)\n"", ""ax[0,0].set_xlabel('')\n"", '\n', ""data.groupby(['Results','year'])['Inspection ID'].agg('count').unstack('Results').plot(kind='bar',ax=ax[0,1])\n"", ""ax[0,1].tick_params(axis='x',labelrotation=360)\n"", 'ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,0.75))\n', 'ax[0,1].set_title(""The counts of results of inspection by year "",size=20)\n', ""ax[0,1].set_ylabel('counts',size=18)\n"", '\n', ""sns.scatterplot(x='Longitude',y='Latitude',hue='Risk' ,hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'],data=data[data.Results=='Pass'], ax=ax[1,0])\n"", 'ax[1,0].set_title(""The distribution of result is pass"",size=20)\n', ""ax[1,0].set_xlabel('Longitude')\n"", ""ax[1,0].set_ylabel('LATITUDE')\n"", '\n', ""sns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=data[data.Results=='Fail'], ax=ax[1,1])\n"", 'ax[1,1].set_title(""The distribution of result is fail"",size=20)\n', ""ax[1,1].set_xlabel('Longitude')\n"", ""ax[1,1].set_ylabel('LATITUDE')""]"
pca.n_components_,0,['pca.n_components_']
CHECKPOINT final['Fare'].dtypes,0,"[""final['Fare'].dtypes""]"
SETUP warnings.filterwarnings('ignore'),0,"['import os\n', '\n', '\n', 'import copy\n', 'import warnings\n', ""warnings.filterwarnings('ignore')\n"", '\n', 'import cv2\n', 'import keras\n', 'from keras import backend as K\n', 'from keras.models import Model, Sequential\n', 'from keras.layers import Dense, Dropout, BatchNormalization, Flatten, Input\n', 'from keras.layers import Conv2D, Activation, GlobalAveragePooling2D\n', 'from keras.preprocessing.image import ImageDataGenerator\n', 'from keras.preprocessing.image import load_img, img_to_array\n', 'from keras.applications.resnet50 import preprocess_input, ResNet50\n', 'import matplotlib\n', 'import matplotlib.pylab as plt\n', 'import numpy as np\n', 'import seaborn as sns\n', 'import shap\n', 'from sklearn.utils import shuffle\n', 'from sklearn.metrics import confusion_matrix\n', 'from sklearn.model_selection import train_test_split']"
"CHECKPOINT X_test,X_valid,y_test,y_valid = train_test_split(X_test_sub, y_test_sub, test_size=0.5, random_state=0 , shuffle =False) print(X_test.shape) print(X_valid.shape)",0,"['X_test,X_valid,y_test,y_valid = train_test_split(X_test_sub, y_test_sub, test_size=0.5, random_state=0 , shuffle =False)\n', 'print(X_test.shape)\n', 'print(X_valid.shape)']"
CHECKPOINT test_df['Season'] = test_df['ID'].map(lambda x: int(x[:4])) test_df['WTeamID'] = test_df['ID'].map(lambda x: int(x[5:9])) test_df['LTeamID'] = test_df['ID'].map(lambda x: int(x[10:14])) test_df,0,"[""test_df['Season'] = test_df['ID'].map(lambda x: int(x[:4]))\n"", ""test_df['WTeamID'] = test_df['ID'].map(lambda x: int(x[5:9]))\n"", ""test_df['LTeamID'] = test_df['ID'].map(lambda x: int(x[10:14]))\n"", 'test_df']"
"match_stats_df.drop(['goal', 'shoton', 'shotoff', 'foulcommit', 'card', 'cross', 'corner', 'possession'], axis=1, inplace= True)",0,"[""match_stats_df.drop(['goal', 'shoton', 'shotoff', 'foulcommit', 'card', 'cross', 'corner', 'possession'], axis=1, inplace= True)""]"
"sns.catplot(x='Year', y='Protection of Civil Rights (PCR) Act', data=cbdr,height = 5, aspect = 4)",1,"[""sns.catplot(x='Year', y='Protection of Civil Rights (PCR) Act', data=cbdr,height = 5, aspect = 4)""]"
check_NaN_Values_in_df(df_all),0,['check_NaN_Values_in_df(df_all)']
"X_TRAIN_FILE=""path"" X_TEST_FILE=""path"" Y_TRAIN_FILE=""path"" Y_TEST_FILE=""path""",0,"['X_TRAIN_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_X_train.npz""\n', 'X_TEST_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_X_test.npy""\n', 'Y_TRAIN_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_Y_train.npz""\n', 'Y_TEST_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_Y_test.npy""']"
val_counts_image_id['bbox_count'] = val_counts_image_id['image_id'],0,"[""val_counts_image_id['bbox_count'] = val_counts_image_id['image_id'] ""]"
"CHECKPOINT ASSIGN = model.evaluate(X_train,y_train) print(+str(ASSIGN[1]*100)) ASSIGN = model.evaluate(X_val,y_val) print(+str(ASSIGN[1]*100))",0,"['result = model.evaluate(X_train,y_train)\n', '#print(result)\n', 'print(""Training accuracy = ""+str(result[1]*100))\n', 'result = model.evaluate(X_val,y_val)\n', '#print(result)\n', 'print(""Validation accuracy = ""+str(result[1]*100))\n', '#result = model.evaluate(X_test,y_test)\n', '#print(result)\n', '#print(""Test accuracy = ""+str(result[1]*100))']"
"pipeline_optimizer.fit(X_train, y_train)",0,"['pipeline_optimizer.fit(X_train, y_train)']"
"if not os.path.exists(""output""): os.makedirs(""output"") df_all.to_csv(""outputpath"", sep=',', index=False)",0,"['if not os.path.exists(""output""):\n', '    os.makedirs(""output"")\n', '    \n', 'df_all.to_csv(""output/cleaned.csv"", sep=\',\', index=False)']"
"CHECKPOINT plt.plot(history.history['acc']) plt.plot(history.history['val_acc']) plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['train', 'valid'], loc='lower right') plt.show() print('-'*50) print('Training accuracy: ' + str(max(history.history['acc']))) print('Validation accuracy: ' + str(max(history.history['val_acc']))) print('-'*50) plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'valid'], loc='upper right') plt.show() print('-'*50) print('Training loss: ' + str(min(history.history['loss']))) print('Validation loss: ' + str(min(history.history['val_loss']))) print('-'*50)",1,"[""plt.plot(history.history['acc'])\n"", ""plt.plot(history.history['val_acc'])\n"", ""#plt.title('model accuracy')\n"", ""plt.ylabel('accuracy')\n"", ""plt.xlabel('epoch')\n"", ""plt.legend(['train', 'valid'], loc='lower right')\n"", 'plt.show()\n', '\n', ""print('-'*50)\n"", ""print('Training accuracy: ' + str(max(history.history['acc'])))\n"", ""print('Validation accuracy: ' + str(max(history.history['val_acc'])))\n"", ""print('-'*50)\n"", '\n', ""plt.plot(history.history['loss'])\n"", ""plt.plot(history.history['val_loss'])\n"", ""#plt.title('model loss')\n"", ""plt.ylabel('loss')\n"", ""plt.xlabel('epoch')\n"", ""plt.legend(['train', 'valid'], loc='upper right')\n"", 'plt.show()\n', '\n', ""print('-'*50)\n"", ""print('Training loss: ' + str(min(history.history['loss'])))\n"", ""print('Validation loss: ' + str(min(history.history['val_loss'])))\n"", ""print('-'*50)""]"
artime['Total'],0,"[""artime['Total']""]"
"ASSIGN = rec_img.detach().permute(1, 2, 0) plt.imshow(ASSIGN)",1,"['rimage = rec_img.detach().permute(1, 2, 0)\n', 'plt.imshow(rimage)']"
"sound, sound [0] , sound [1]",0,"[' sound, sound [0] , sound [1] # an array and freq']"
"ASSIGN = pd.read_csv('..path', index_col='Page') ASSIGN = full.iloc[:, -args.test_len:].values",0,"[""full = pd.read_csv('../input/web-traffic-time-series-forecasting/train_2.csv', index_col='Page')\n"", 'y_true = full.iloc[:, -args.test_len:].values']"
"ASSIGN = VotingClassifier(estimators = estimator, voting ='hard') ASSIGN.fit(x_train, x_test) ASSIGN = vot_hard.predict(y_train) ASSIGN.score(y_train,y_test)",0,"[""vot_hard = VotingClassifier(estimators = estimator, voting ='hard') \n"", 'vot_hard.fit(x_train, x_test) \n', 'y_pred = vot_hard.predict(y_train)\n', 'vot_hard.score(y_train,y_test)']"
"SETUP ASSIGN = pd.read_csv(SUB_PATH) ASSIGN = model_finetuned.predict(test_generator, verbose=1) ASSIGN.loc[:, 'healthy':] = ASSIGN ASSIGN.to_csv('submission_RESNET.csv', index=False) ASSIGN.head()",0,"['SUB_PATH = ""../input/plant-pathology-2020-fgvc7/sample_submission.csv""\n', '\n', 'sub = pd.read_csv(SUB_PATH)\n', 'probs_RESNET = model_finetuned.predict(test_generator, verbose=1)\n', ""sub.loc[:, 'healthy':] = probs_RESNET\n"", ""sub.to_csv('submission_RESNET.csv', index=False)\n"", 'sub.head()']"
ASSIGN = m.plot(forecastD),1,['death_forecast = m.plot(forecastD)']
"CHECKPOINT def train_model(dataloaders, model, criterion, optimizer, scheduler, num_epochs=5): ASSIGN = time.time() ASSIGN = copy.deepcopy(model.state_dict()) ASSIGN = 0.0 ASSIGN = {'train': [], 'valid':[]} ASSIGN = {'train': [], 'valid': []} for epoch in range(num_epochs): print('Epoch {}path{}'.format(epoch + 1, num_epochs)) print('-' * 10) for phase in ['train', 'valid']: ASSIGN == 'train': scheduler.step() model.train() else: model.eval() ASSIGN = 0.0 ASSIGN = 0 for inputs, labels in dataloaders[phase]: ASSIGN = ASSIGN.to(device) ASSIGN = ASSIGN.to(device) optimizer.zero_grad() with torch.set_grad_enabled(phase == 'train'): ASSIGN = model(inputs) ASSIGN = torch.max(outputs, 1) ASSIGN = criterion(outputs, labels) ASSIGN == 'train': ASSIGN.backward() optimizer.step() ASSIGN += ASSIGN.item() * ASSIGN.size(0) ASSIGN += torch.sum(preds == ASSIGN.data) ASSIGN = running_loss path[phase] ASSIGN = running_corrects.double() path[phase] ASSIGN[phase].append(ASSIGN) ASSIGN[phase].append(ASSIGN) print('{} Loss: {:.4f} Acc: {:.4f}'.format( phase, ASSIGN, ASSIGN)) ASSIGN == 'valid' and epoch_acc > best_acc: ASSIGN = epoch_acc ASSIGN = copy.deepcopy(model.state_dict()) print() ASSIGN = time.time() - since print('Training complete in {:.0f}m {:.0f}s'.format( ASSIGN path, ASSIGN % 60)) print('Best val Acc: {:4f}'.format(ASSIGN)) model.load_state_dict(ASSIGN) return model",0,"['def train_model(dataloaders, model, criterion, optimizer, scheduler, num_epochs=5):\n', '    since = time.time()\n', '\n', '    best_model_wts = copy.deepcopy(model.state_dict())\n', '    best_acc = 0.0\n', ""    losses = {'train': [], 'valid':[]}\n"", ""    acc = {'train': [], 'valid': []}\n"", '  \n', '    for epoch in range(num_epochs):\n', ""        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n"", ""        print('-' * 10)\n"", '\n', '    # Each epoch has a training and validation phase\n', ""        for phase in ['train', 'valid']:\n"", ""            if phase == 'train':\n"", '                scheduler.step()\n', '                model.train()  # Set model to training mode\n', '            else:\n', '                model.eval()   # Set model to evaluate mode\n', '\n', '            running_loss = 0.0\n', '            running_corrects = 0\n', '\n', '\n', '            for inputs, labels in dataloaders[phase]:\n', '                inputs = inputs.to(device)\n', '                labels = labels.to(device)\n', '\n', '                # zero the parameter gradients\n', '                optimizer.zero_grad()\n', '\n', '            # forward\n', '            # track history if only in train\n', ""                with torch.set_grad_enabled(phase == 'train'):\n"", '                    outputs = model(inputs)\n', '                    _, preds = torch.max(outputs, 1)\n', '                    loss = criterion(outputs, labels)\n', '\n', '                    # backward + optimize only if in training phase\n', ""                    if phase == 'train':\n"", '                        loss.backward()\n', '                        optimizer.step()\n', '\n', '                    running_loss += loss.item() * inputs.size(0)\n', '                    running_corrects += torch.sum(preds == labels.data)\n', '\n', '            epoch_loss = running_loss / dataset_sizes[phase]\n', '            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n', '            losses[phase].append(epoch_loss)\n', '            acc[phase].append(epoch_acc)\n', '\n', ""            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n"", '                      phase, epoch_loss, epoch_acc))\n', '\n', '            # deep copy the model\n', ""            if phase == 'valid' and epoch_acc > best_acc:\n"", '                best_acc = epoch_acc\n', '                best_model_wts = copy.deepcopy(model.state_dict())\n', '\n', '        print()\n', '\n', '    time_elapsed = time.time() - since\n', ""    print('Training complete in {:.0f}m {:.0f}s'.format(\n"", '      time_elapsed // 60, time_elapsed % 60))\n', ""    print('Best val Acc: {:4f}'.format(best_acc))\n"", '\n', '    # load best model weights\n', '    model.load_state_dict(best_model_wts)\n', '    return model']"
"ASSIGN = torch.mean(sound[0], dim=0).unsqueeze(1) soundData, soundData.shape",0,"['soundData = torch.mean(sound[0], dim=0).unsqueeze(1)\n', '# was as below\n', '# soundData = torch.mean(sound[0], dim=0).unsqueeze(0)# add a dim at idx 0 <-unsqueeze? \n', '# dim 0 is where the second channel comes in\n', 'soundData, soundData.shape']"
"sns.relplot( y = 'Selling_Price', x = 'Kms_Driven',  data = data)",1,"[""sns.relplot( y = 'Selling_Price', x = 'Kms_Driven',  data = data)""]"
data_features.groupby('Neighborhood')['LotFrontage'].mean(),0,"[""data_features.groupby('Neighborhood')['LotFrontage'].mean()""]"
"ASSIGN = pd.DataFrame(country_info.iloc[:,[0,1,4,5,6,8,9]]) ASSIGN.columns = ['Country','Population','Density','Land_Area','Migrants','MedAge','UrbanPopRate']",0,"['population = pd.DataFrame(country_info.iloc[:,[0,1,4,5,6,8,9]])\n', ""population.columns = ['Country','Population','Density','Land_Area','Migrants','MedAge','UrbanPopRate']""]"
CHECKPOINT FullRMSE,0,['FullRMSE']
"dt.fit(x_train,x_test) dt.score(y_train,y_test)",0,"['dt.fit(x_train,x_test)\n', 'dt.score(y_train,y_test)']"
CHECKPOINT x_pca.shape,0,['x_pca.shape']
"CHECKPOINT ASSIGN = keras.optimizers.Adam(learning_rate=0.002) loaded_model.compile(loss='binary_crossentropy', optimizer=ASSIGN,metrics=['accuracy']) ASSIGN = loaded_model.evaluate(X_train,Y_train) print(+str(ASSIGN[1]*100)) ASSIGN = loaded_model.evaluate(X_val,Y_val) print(+str(ASSIGN[1]*100)) ASSIGN = loaded_model.evaluate(X_test,Y_test) print(+str(ASSIGN[1]*100))",0,"['#Getting results\n', 'opt = keras.optimizers.Adam(learning_rate=0.002)\n', ""loaded_model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n"", 'result = loaded_model.evaluate(X_train,Y_train)\n', '#print(result)\n', 'print(""Training accuracy = ""+str(result[1]*100))\n', 'result = loaded_model.evaluate(X_val,Y_val)\n', '#print(result)\n', 'print(""Validation accuracy = ""+str(result[1]*100))\n', 'result = loaded_model.evaluate(X_test,Y_test)\n', '#print(result)\n', 'print(""Test accuracy = ""+str(result[1]*100))\n']"
"CHECKPOINT ASSIGN = linear_model.LogisticRegression() ASSIGN.fit(xtrain_count, y_train) ASSIGN=model1.score(xvalid_count, y_val) print('Accuracy Count LR:', ASSIGN) ASSIGN=model1.predict(xtest_count) ASSIGN = linear_model.LogisticRegression() ASSIGN.fit(xtrain_tfidf, y_train) ASSIGN=model2.score(xvalid_tfidf, y_val) print('Accuracy TFIDF LR:', ASSIGN) ASSIGN=model2.predict(xtest_tfidf) ASSIGN = linear_model.LogisticRegression() ASSIGN.fit(xtrain_tfidf_ngram, y_train) ASSIGN = model3.score(xvalid_tfidf_ngram, y_val) print('Accuracy TFIDF NGRAM LR:', ASSIGN) ASSIGN = model3.predict(xtest_tfidf_ngram)",0,"['model1 = linear_model.LogisticRegression()\n', 'model1.fit(xtrain_count, y_train)\n', 'accuracy=model1.score(xvalid_count, y_val)\n', ""print('Accuracy Count LR:', accuracy)\n"", 'test_pred1=model1.predict(xtest_count)\n', '\n', 'model2 = linear_model.LogisticRegression()\n', 'model2.fit(xtrain_tfidf, y_train)\n', 'accuracy=model2.score(xvalid_tfidf, y_val)\n', ""print('Accuracy TFIDF LR:', accuracy)\n"", 'test_pred2=model2.predict(xtest_tfidf)\n', '\n', 'model3 = linear_model.LogisticRegression()\n', 'model3.fit(xtrain_tfidf_ngram, y_train)\n', 'accuracy = model3.score(xvalid_tfidf_ngram, y_val)\n', ""print('Accuracy TFIDF NGRAM LR:', accuracy)\n"", 'test_pred3 = model3.predict(xtest_tfidf_ngram)']"
"sns.catplot(x='Year', y='Robbery', data=total ,height = 5, aspect = 4,kind = 'bar')",1,"[""sns.catplot(x='Year', y='Robbery', data=total ,height = 5, aspect = 4,kind = 'bar')""]"
"test['Total'].plot(legend=True) predictions1.plot(legend=True,figsize=(12,6));",1,"[""test['Total'].plot(legend=True)\n"", 'predictions1.plot(legend=True,figsize=(12,6));']"
"def intersection(lst1, lst2): ASSIGN = [value for value in lst1 if value in lst2] return lst3",0,"['def intersection(lst1, lst2): \n', '    lst3 = [value for value in lst1 if value in lst2] \n', '    return lst3 ']"
"sns.catplot(x='Year', y ='Selling_Price', kind = 'swarm',data= data)",1,"[""sns.catplot(x='Year', y ='Selling_Price', kind = 'swarm',data= data)""]"
"ASSIGN=data[data['YEAR']==2016][0:2000] ASSIGN=""Crime2016"" ASSIGN=folium.Map([Lat,Lon],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(boston_map) for lat,lon,label in zip(ASSIGN.Lat,ASSIGN.Long,ASSIGN.OFFENSE_CODE_GROUP): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN)",1,"[""data1=data[data['YEAR']==2016][0:2000]\n"", 'filename=""Crime2016""\n', 'boston_map=folium.Map([Lat,Lon],zoom_start=12)\n', 'incidents2=plugins.MarkerCluster().add_to(boston_map)\n', 'for lat,lon,label in zip(data1.Lat,data1.Long,data1.OFFENSE_CODE_GROUP):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(incidents2)\n', 'boston_map.add_child(incidents2)\n']"
y_train.head(),0,['y_train.head()']
"SETUP """"""Q2. Store data of marks acquired by a certain student and show them in form of a piechart and slice out the subject having least marks"""""" ASSIGN=['English','Bengali','Hindi','Maths','History','Geography'] ASSIGN = [87,89,93,92,98,95] plt.pie(ASSIGN,labels=ASSIGN,startangle=90,shadow=True, ASSIGN=(0.2,0,0,0,0,0),autopct='%1.2f%%') plt.show()",1,"['""""""Q2. Store data of marks acquired by a certain student and show \n', 'them in form of a piechart and slice out the subject having least \n', 'marks""""""\n', '\n', 'import matplotlib.pyplot as plt\n', ""subjects=['English','Bengali','Hindi','Maths','History','Geography']\n"", 'marks = [87,89,93,92,98,95]\n', 'plt.pie(marks,labels=subjects,startangle=90,shadow=True,\n', ""        explode=(0.2,0,0,0,0,0),autopct='%1.2f%%')\n"", 'plt.show()']"
len(dirs),0,['len(dirs)']
ASSIGN=data[data.IS_TRAFFIC==1] ASSIGN.head(),0,"['data_IS_TRAFFIC=data[data.IS_TRAFFIC==1]\n', 'data_IS_TRAFFIC.head()']"
"ASSIGN = pd.merge(ASSIGN,API_beds,left_on='Country',right_on='Country',how='left') ASSIGN = pd.merge(ASSIGN,API_beds,left_on='Country',right_on='Country',how='left')",0,"['#merge\n', ""train = pd.merge(train,API_beds,left_on='Country',right_on='Country',how='left')\n"", ""test = pd.merge(test,API_beds,left_on='Country',right_on='Country',how='left')""]"
"data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtCond'].isnull())][ ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]",0,"[""data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtCond'].isnull())][\n"", ""    ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]\n""]"
"test_df[cont].hist(bins=20, figsize=(15,15), color = 'darkorange') plt.suptitle(""Histogram for each test numeric input variable"") plt.show()",1,"[""test_df[cont].hist(bins=20, figsize=(15,15), color = 'darkorange')\n"", 'plt.suptitle(""Histogram for each test numeric input variable"")\n', 'plt.show()']"
CHECKPOINT cbsr.shape,0,['cbsr.shape']
"CHECKPOINT ASSIGN = GaussianNB() ASSIGN.fit(train_x, train_y) ASSIGN = gaussian.predict(test_x) ASSIGN = gaussian.score(train_x, train_y) print('training accuracy: %.5f' % ASSIGN) ASSIGN =gaussian.predict(test_x) X=accuracy_score(test_y, ASSIGN) print('test accuracy: %.5f' % X) ASSIGN = metrics.roc_curve(test_y, predict_y, pos_label=1) print('auc: %.5f' % metrics.auc(fpr, tpr)) ASSIGN = cross_val_score(gaussian,train_x,train_y,cv=5,scoring='accuracy') print('average of Cross validation: %.5f'%ASSIGN.mean()) print('standard deviation of Cross validation: %.5f'%ASSIGN.std(ddof=1))",0,"['# Gaussian Naive Bayes\n', '\n', 'gaussian = GaussianNB()\n', 'gaussian.fit(train_x, train_y)\n', 'predict_y = gaussian.predict(test_x)\n', 'acc_gaussian = gaussian.score(train_x, train_y)\n', ""print('training accuracy: %.5f' % acc_gaussian)\n"", '\n', 'predict_y =gaussian.predict(test_x)\n', 'X=accuracy_score(test_y, predict_y)\n', ""print('test accuracy: %.5f' % X)\n"", '\n', '#auc\n', 'fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n', ""print('auc: %.5f' % metrics.auc(fpr, tpr))\n"", '\n', '#Cross validation\n', ""scores = cross_val_score(gaussian,train_x,train_y,cv=5,scoring='accuracy')\n"", ""print('average of Cross validation: %.5f'%scores.mean())\n"", ""print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))""]"
"SETUP ASSIGN = [tokenizer.tokenize(sent) for sent in sentences] ASSIGN = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], ASSIGN=MAX_LEN, dtype=""long"", truncating=""post"", padding=""post"") ASSIGN = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts] ASSIGN = pad_sequences(ASSIGN, maxlen=MAX_LEN, dtype=""long"", truncating=""post"", padding=""post"") ASSIGN = [] for seq in ASSIGN: ASSIGN = [float(i>0) for i in seq] ASSIGN.append(ASSIGN) ASSIGN = torch.tensor(input_ids) ASSIGN = torch.tensor(attention_masks) ASSIGN = torch.tensor(labels) ASSIGN = 32 ASSIGN = TensorDataset(prediction_inputs, prediction_masks, prediction_labels) ASSIGN = SequentialSampler(prediction_data) ASSIGN = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)",0,"['tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n', 'MAX_LEN = 128\n', '# Pad our input tokens\n', 'input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n', '                          maxlen=MAX_LEN, dtype=""long"", truncating=""post"", padding=""post"")\n', '# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n', 'input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n', 'input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=""long"", truncating=""post"", padding=""post"")\n', '# Create attention masks\n', 'attention_masks = []\n', '\n', '# Create a mask of 1s for each token followed by 0s for padding\n', 'for seq in input_ids:\n', '  seq_mask = [float(i>0) for i in seq]\n', '  attention_masks.append(seq_mask) \n', '\n', 'prediction_inputs = torch.tensor(input_ids)\n', 'prediction_masks = torch.tensor(attention_masks)\n', 'prediction_labels = torch.tensor(labels)\n', '  \n', 'batch_size = 32  \n', '\n', '\n', 'prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n', 'prediction_sampler = SequentialSampler(prediction_data)\n', 'prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)']"
SETUP,0,"['from mpl_toolkits.mplot3d import Axes3D\n', 'from sklearn.preprocessing import StandardScaler\n', 'import matplotlib.pyplot as plt # plotting\n', 'import numpy as np # linear algebra\n', 'import os # accessing directory structure\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n']"
"SETUP ASSIGN=train[['MSSubClass','MSZoning','Neighborhood','OverallQual','OverallCond','SalePrice']] ASSIGN = LabelEncoder() ASSIGN['MSZoning_new'] = ASSIGN.fit_transform(ASSIGN['MSZoning']) ASSIGN['Neighborhood_new'] = ASSIGN.fit_transform(ASSIGN['Neighborhood']) ASSIGN.head()",0,"[""data_tree=train[['MSSubClass','MSZoning','Neighborhood','OverallQual','OverallCond','SalePrice']]\n"", 'from sklearn.preprocessing import LabelEncoder\n', 'labelencoder = LabelEncoder()\n', ""data_tree['MSZoning_new'] = labelencoder.fit_transform(data_tree['MSZoning'])\n"", ""data_tree['Neighborhood_new'] = labelencoder.fit_transform(data_tree['Neighborhood'])\n"", 'data_tree.head()']"
"ASSIGN = df[df.isin(result)] ASSIGN = ASSIGN.dropna(axis='columns', how='all') ASSIGN = ASSIGN.dropna(axis='index', how='all')",0,"[""#delete all values that arent on our list 'result'\n"", 'all_to_investigate = df[df.isin(result)]\n', '\n', ""#remove every column that is entirely 'NaN' values\n"", ""all_to_investigate = all_to_investigate.dropna(axis='columns', how='all')\n"", '\n', ""#remove every row that is entirely 'NaN' values\n"", ""all_to_investigate = all_to_investigate.dropna(axis='index', how='all')""]"
"ASSIGN= forecast1[['ds', 'yhat']] ASSIGN.columns = [['ForecastId', 'ConfirmedCases']]",0,"['## These are the predictions for Confirmed Covid-19 cases until 2020 April 22\n', ""forecastC= forecast1[['ds', 'yhat']]\n"", ""forecastC.columns = [['ForecastId', 'ConfirmedCases']]\n""]"
"ASSIGN = df['CGPA'] sns.distplot(ASSIGN , kde= True,rug = False, bins = 30)",1,"[""x = df['CGPA']\n"", 'sns.distplot(x , kde= True,rug = False, bins = 30)']"
"ASSIGN=plt.subplots(2,2,figsize=(25,16)) sns.barplot(x=data.Category.value_counts(),y=data.Category.value_counts().index,ax=ax[0,0]) ax[0,0].set_title(""Counts of Category"",size=20) ax[0,0].set_xlabel("""") data.Reviews=data.Reviews.astype('int') sns.barplot(x=data.groupby(['Category'])['Reviews'].agg('sum').sort_values(ascending=False),y=data.groupby(['Category'])['Reviews'].agg('sum').sort_values(ascending=False).index,ax=ax[0,1]) ax[0,1].set_title(""Number of reviews by Category"",size=20) ax[0,1].set_ylabel("""") data['new_install']=data.Installs.apply(lambda x:x.split('+')[0].strip(',').replace(',','')) data.new_install=data.new_install.astype('int') sns.barplot(x=data.groupby(['Category'])['new_install'].agg('sum').sort_values(ascending=False),y=data.groupby(['Category'])['new_install'].agg('sum').sort_values(ascending=False).index,ax=ax[1,0]) ax[1,0].set_title(""Number of installs by Category"",size=20) ax[1,0].set_ylabel("""") ax[1,0].set_xlabel("""") sns.boxplot(y=""Category"",x=""Rating"",data=data,ax=ax[1,1]) ax[1,1].set_ylabel("""") ax[1,1].set_title(""Distribution of rating by Category"",size=20)",1,"['fig,ax=plt.subplots(2,2,figsize=(25,16))\n', 'sns.barplot(x=data.Category.value_counts(),y=data.Category.value_counts().index,ax=ax[0,0])\n', 'ax[0,0].set_title(""Counts of Category"",size=20)\n', 'ax[0,0].set_xlabel("""")\n', '\n', ""data.Reviews=data.Reviews.astype('int')\n"", ""sns.barplot(x=data.groupby(['Category'])['Reviews'].agg('sum').sort_values(ascending=False),y=data.groupby(['Category'])['Reviews'].agg('sum').sort_values(ascending=False).index,ax=ax[0,1])\n"", 'ax[0,1].set_title(""Number of reviews by Category"",size=20)\n', 'ax[0,1].set_ylabel("""")\n', '\n', ""data['new_install']=data.Installs.apply(lambda x:x.split('+')[0].strip(',').replace(',',''))\n"", ""data.new_install=data.new_install.astype('int')\n"", '\n', ""sns.barplot(x=data.groupby(['Category'])['new_install'].agg('sum').sort_values(ascending=False),y=data.groupby(['Category'])['new_install'].agg('sum').sort_values(ascending=False).index,ax=ax[1,0])\n"", 'ax[1,0].set_title(""Number of installs by Category"",size=20)\n', 'ax[1,0].set_ylabel("""")\n', 'ax[1,0].set_xlabel("""")\n', '\n', 'sns.boxplot(y=""Category"",x=""Rating"",data=data,ax=ax[1,1])\n', 'ax[1,1].set_ylabel("""")\n', 'ax[1,1].set_title(""Distribution of rating by Category"",size=20)']"
"plt.hist(pos_probs, bins=100) plt.show()",1,"['# create a histogram of the predicted probabilities\n', 'plt.hist(pos_probs, bins=100)\n', 'plt.show()']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# Any results you write to the current directory are saved as output.']"
"ASSIGN = pd.DataFrame(['MS Zaandam',1432,189618,0.007552,1432,19,'100%']) ASSIGN = ASSIGN.T ASSIGN.columns = population.columns ASSIGN = ASSIGN.append(Shangdan)",0,"[""Shangdan = pd.DataFrame(['MS Zaandam',1432,189618,0.007552,1432,19,'100%'])\n"", 'Shangdan = Shangdan.T\n', 'Shangdan.columns = population.columns\n', '\n', 'population = population.append(Shangdan)']"
ASSIGN=pd.DataFrame(forecast),0,['forecast1=pd.DataFrame(forecast)\n']
data.loc[data.SepalWidthCm.isnull()],0,['data.loc[data.SepalWidthCm.isnull()]']
"CHECKPOINT ASSIGN = linear_model.Ridge(alpha=10) ASSIGN.fit(x_train,Y_train) ASSIGN = model13.score(x_test,Y_test) print(ASSIGN*100,'%')",0,"['model13 = linear_model.Ridge(alpha=10)\n', 'model13.fit(x_train,Y_train)\n', '\n', 'accuracy13 = model13.score(x_test,Y_test)\n', ""print(accuracy13*100,'%')""]"
"gs_1.fit(X_train, y_train) gs_1.best_params_",0,"['gs_1.fit(X_train, y_train)\n', 'gs_1.best_params_']"
ASSIGN = ASSIGN.reset_index() ASSIGN = ASSIGN.reset_index(),0,"['#drop sequence...\n', 'train_df = train_df.reset_index()\n', 'test_df = test_df.reset_index()']"
CHECKPOINT ASSIGN = data_features.isnull().sum().sort_values(ascending = False) ASSIGN = ASSIGN[ASSIGN > 0] missing_data,0,"['#Now the check the missing values\n', 'missing_data = data_features.isnull().sum().sort_values(ascending = False)\n', 'missing_data = missing_data[missing_data > 0]\n', 'missing_data']"
"plt.figure(figsize=(8,8)) ASSIGN = data.ASSIGN() sns.heatmap(ASSIGN, xticklabels=ASSIGN.columns,yticklabels=ASSIGN.columns,annot=True) plt.title(""correlation plot"",size=28)",1,"['plt.figure(figsize=(8,8))\n', 'corr = data.corr()\n', 'sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns,annot=True)\n', 'plt.title(""correlation plot"",size=28)']"
"model_train_loop([2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 4, 1, 4, 4, 5, 4])",0,"['model_train_loop([2, 1, 1, 2, 1, 1, 2, 1, 1, 2, 4, 1, 4, 4, 5, 4])']"
"plotScatterMatrix(df1, 20, 10)",1,"['plotScatterMatrix(df1, 20, 10)']"
SETUP,0,"['import numpy as np\n', 'import pandas as pd\n', 'import matplotlib.pyplot as plt \n', 'import seaborn as sns\n', 'import os\n', 'import geopandas as gpd\n', 'import folium\n', 'from folium import plugins\n', 'import datetime \n', 'import re']"
"class DenseLSTMForecast(nn.Module): def __init__(self, hidden_size): super(DenseLSTMForecast, self).__init__() self.lstm1 = nn.LSTMCell(1, hidden_size) self.lstm2 = nn.LSTMCell(hidden_size+1, hidden_size) self.lstm3 = nn.LSTMCell(2*hidden_size+1, hidden_size) self.linear = nn.Linear(3*hidden_size+1, 1) self.hidden_size = hidden_size def forward(self, x, future=0): ASSIGN = [] ASSIGN = torch.cuda if args.cuda else torch ASSIGN = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_()) ASSIGN = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_()) ASSIGN = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_()) ASSIGN = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_()) ASSIGN = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_()) ASSIGN = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_()) for x_t in x.chunk(x.size(1), dim=1): ASSIGN = ASSIGN.squeeze(dim=1) ASSIGN, ASSIGN = self.lstm1(ASSIGN, (ASSIGN, ASSIGN)) ASSIGN = torch.cat([x_t, h1_t], dim=1) ASSIGN, ASSIGN = self.lstm2(ASSIGN, (ASSIGN, ASSIGN)) ASSIGN = torch.cat([x_t, h1_t, h2_t], dim=1) ASSIGN, ASSIGN = self.lstm3(ASSIGN, (ASSIGN, ASSIGN)) ASSIGN = torch.cat([x_t, h1_t, h2_t, h3_t], dim=1) ASSIGN = self.linear(h3d_t) ASSIGN.append(ASSIGN) for i in range(future): ASSIGN, ASSIGN = self.lstm1(ASSIGN, (ASSIGN, ASSIGN)) ASSIGN = torch.cat([o_t, h1_t], dim=1) ASSIGN, ASSIGN = self.lstm2(ASSIGN, (ASSIGN, ASSIGN)) ASSIGN = torch.cat([o_t, h1_t, h2_t], dim=1) ASSIGN, ASSIGN = self.lstm3(ASSIGN, (ASSIGN, ASSIGN)) ASSIGN = torch.cat([o_t, h1_t, h2_t, h3_t], dim=1) ASSIGN = self.linear(h3d_t) ASSIGN.append(ASSIGN) return torch.stack(ASSIGN, dim=1)",0,"['class DenseLSTMForecast(nn.Module):\n', '    def __init__(self, hidden_size):\n', '        super(DenseLSTMForecast, self).__init__()\n', '        self.lstm1 = nn.LSTMCell(1, hidden_size)\n', '        self.lstm2 = nn.LSTMCell(hidden_size+1, hidden_size)\n', '        self.lstm3 = nn.LSTMCell(2*hidden_size+1, hidden_size)\n', '        self.linear = nn.Linear(3*hidden_size+1, 1)\n', '        self.hidden_size = hidden_size\n', '\n', '    def forward(self, x, future=0):\n', '        o = []\n', '        tt = torch.cuda if args.cuda else torch\n', '        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n', '        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n', '        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n', '        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n', '        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n', '        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n', '        \n', '        for x_t in x.chunk(x.size(1), dim=1):\n', '            x_t = x_t.squeeze(dim=1)\n', '            h1_t, c1_t = self.lstm1(x_t, (h1_t, c1_t))\n', '            h1d_t = torch.cat([x_t, h1_t], dim=1)\n', '            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n', '            h2d_t = torch.cat([x_t, h1_t, h2_t], dim=1)\n', '            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n', '            h3d_t = torch.cat([x_t, h1_t, h2_t, h3_t], dim=1)\n', '            o_t = self.linear(h3d_t)\n', '            o.append(o_t)\n', '\n', '            \n', '        for i in range(future):\n', '            h1_t, c1_t = self.lstm1(o_t, (h1_t, c1_t))\n', '            h1d_t = torch.cat([o_t, h1_t], dim=1)\n', '            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n', '            h2d_t = torch.cat([o_t, h1_t, h2_t], dim=1)\n', '            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n', '            h3d_t = torch.cat([o_t, h1_t, h2_t, h3_t], dim=1)\n', '            o_t = self.linear(h3d_t)\n', '            o.append(o_t)\n', '\n', '        return torch.stack(o, dim=1)']"
"gbc.fit(x_train,x_test) gbc.score(y_train,y_test)",0,"['gbc.fit(x_train,x_test)\n', 'gbc.score(y_train,y_test)']"
CHECKPOINT image_names_list,0,['image_names_list # only images with one or 2  or 3 wheat heads higher numbers dont converge well... for now...']
"CHECKPOINT ASSIGN=ASSIGN[:2000] ASSIGN = {'2014' : 'red', '2015' : 'blue','2016' :'green','2017':'brown','2018':'plum','2019':'purple'} ASSIGN=data_IS_TRAFFIC.GEO_LON.mean() ASSIGN=data_IS_TRAFFIC.GEO_LAT.mean() ASSIGN=folium.Map([Lat,Long],zoom_start=12) for i in range(len(ASSIGN.groupby(['GEO_LAT','GEO_LON'])['INCIDENT_ID'].agg('count').index)): ASSIGN=data_IS_TRAFFIC.groupby(['GEO_LAT','GEO_LON'])['INCIDENT_ID'].agg('count').index[i] folium.Circle(location=[ASSIGN], ASSIGN=data_IS_TRAFFIC.iloc[i]['OFFENSE_TYPE_ID'], ASSIGN=int(data_IS_TRAFFIC.groupby(['GEO_LAT','GEO_LON'])['INCIDENT_ID'].agg('count')[i])*70, ASSIGN=True, ASSIGN=colors[data_IS_TRAFFIC['year'].iloc[i]], ASSIGN=0.7,).add_to(data_IS_TRAFFIC_map) data_IS_TRAFFIC_map",1,"['data_IS_TRAFFIC=data_IS_TRAFFIC[:2000]\n', ""colors = {'2014' : 'red', '2015' : 'blue','2016' :'green','2017':'brown','2018':'plum','2019':'purple'}\n"", 'Long=data_IS_TRAFFIC.GEO_LON.mean()\n', 'Lat=data_IS_TRAFFIC.GEO_LAT.mean()\n', 'data_IS_TRAFFIC_map=folium.Map([Lat,Long],zoom_start=12)\n', ""for i in range(len(data_IS_TRAFFIC.groupby(['GEO_LAT','GEO_LON'])['INCIDENT_ID'].agg('count').index)):\n"", ""    lat,lon=data_IS_TRAFFIC.groupby(['GEO_LAT','GEO_LON'])['INCIDENT_ID'].agg('count').index[i]\n"", '    folium.Circle(location=[lat,lon],\n', ""    popup=data_IS_TRAFFIC.iloc[i]['OFFENSE_TYPE_ID'],\n"", ""    radius=int(data_IS_TRAFFIC.groupby(['GEO_LAT','GEO_LON'])['INCIDENT_ID'].agg('count')[i])*70,\n"", '    fill=True,\n', ""    fill_color=colors[data_IS_TRAFFIC['year'].iloc[i]],\n"", '    fill_opacity=0.7,).add_to(data_IS_TRAFFIC_map)\n', '\n', 'data_IS_TRAFFIC_map']"
"CHECKPOINT ASSIGN = logisticRegr.ASSIGN(X_train_pca, y_train) print(ASSIGN)",0,"['score = logisticRegr.score(X_train_pca, y_train)\n', 'print(score)']"
train.describe(),0,['train.describe()']
del data['customerID'],0,"['# Removing the customer id\n', ""del data['customerID'] #customerID is a uninque id so it dosn't give any information""]"
SETUP,0,"['import pandas as pd\n', 'import numpy as np\n', 'from fbprophet import Prophet']"
"ASSIGN = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[""temporal_inputs""].values]),(training_item_count-2,2,sequence_length)),(0,2,1) )).astype(np.float32) ASSIGN = np.asarray([np.asarray(x) for x in training_df[""stable_inputs""]]).astype(np.float32) ASSIGN = np.asarray([np.asarray(x) for x in training_df[""expected_cases""]]).astype(np.float32) ASSIGN = np.asarray([np.asarray(x) for x in training_df[""expected_fatalities""]]).astype(np.float32) ASSIGN = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in validation_df[""temporal_inputs""]]),(validation_item_count-2,2,sequence_length)),(0,2,1)) ).astype(np.float32) ASSIGN = np.asarray([np.asarray(x) for x in validation_df[""stable_inputs""]]).astype(np.float32) ASSIGN = np.asarray([np.asarray(x) for x in validation_df[""expected_cases""]]).astype(np.float32) ASSIGN = np.asarray([np.asarray(x) for x in validation_df[""expected_fatalities""]]).astype(np.float32) ASSIGN = torch.from_numpy(ASSIGN) ASSIGN = torch.from_numpy(ASSIGN) ASSIGN = torch.from_numpy(ASSIGN) ASSIGN = torch.from_numpy(ASSIGN) ASSIGN = torch.from_numpy(ASSIGN) ASSIGN = torch.from_numpy(ASSIGN) ASSIGN = torch.from_numpy(ASSIGN) ASSIGN = torch.from_numpy(ASSIGN) ASSIGN = torch.cat((Y_cases_train.reshape(14770,1),Y_fatalities_train.reshape(14770,1)),1) ASSIGN = torch.cat((Y_cases_test.reshape(1640,1),Y_fatalities_test.reshape(1640,1)),1)",0,"['X_temporal_train = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in training_df[""temporal_inputs""].values]),(training_item_count-2,2,sequence_length)),(0,2,1) )).astype(np.float32)\n', 'X_stable_train = np.asarray([np.asarray(x) for x in training_df[""stable_inputs""]]).astype(np.float32)\n', 'Y_cases_train = np.asarray([np.asarray(x) for x in training_df[""expected_cases""]]).astype(np.float32)\n', 'Y_fatalities_train = np.asarray([np.asarray(x) for x in training_df[""expected_fatalities""]]).astype(np.float32)\n', '\n', 'X_temporal_test = np.asarray(np.transpose(np.reshape(np.asarray([np.asarray(x) for x in validation_df[""temporal_inputs""]]),(validation_item_count-2,2,sequence_length)),(0,2,1)) ).astype(np.float32)\n', 'X_stable_test = np.asarray([np.asarray(x) for x in validation_df[""stable_inputs""]]).astype(np.float32)\n', 'Y_cases_test = np.asarray([np.asarray(x) for x in validation_df[""expected_cases""]]).astype(np.float32)\n', 'Y_fatalities_test = np.asarray([np.asarray(x) for x in validation_df[""expected_fatalities""]]).astype(np.float32)\n', '\n', '# Transform to tensor type\n', 'X_temporal_train = torch.from_numpy(X_temporal_train)\n', 'X_stable_train = torch.from_numpy(X_stable_train)\n', 'Y_cases_train = torch.from_numpy(Y_cases_train)\n', 'Y_fatalities_train = torch.from_numpy(Y_fatalities_train)\n', '\n', 'X_temporal_test = torch.from_numpy(X_temporal_test)\n', 'X_stable_test = torch.from_numpy(X_stable_test)\n', 'Y_cases_test = torch.from_numpy(Y_cases_test)\n', 'Y_fatalities_test = torch.from_numpy(Y_fatalities_test)\n', '\n', '# Merge two objective values\n', 'Y_train = torch.cat((Y_cases_train.reshape(14770,1),Y_fatalities_train.reshape(14770,1)),1)\n', 'Y_test = torch.cat((Y_cases_test.reshape(1640,1),Y_fatalities_test.reshape(1640,1)),1)']"
"ASSIGN = ['Affected and Uncured', 'Deaths', 'Recovered'] ASSIGN = [total_still_affected, total_deaths, total_recovered] ASSIGN = ['Yellow', 'Red', 'Green'] ASSIGN = (0, 0.2, 0) ASSIGN = ['Count'] ASSIGN = ['Affected and Uncured', 'Deaths', 'Recovered'] ASSIGN = [[total_still_affected],[total_deaths], [total_recovered]] ASSIGN = plt.subplots(1,2, figsize = (9,9)) axs[0].axis('tight') axs[0].axis('off') ASSIGN = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center') ASSIGN.set_fontsize(14) ASSIGN.scale(1.5, 1.5) axs[1].pie(ASSIGN, labels = ASSIGN, ASSIGN = ASSIGN, colors=ASSIGN, shadow=True, autopct='%1.1f%%') plt.title('Distribution at world level') plt.show()",1,"[""groups = ['Affected and Uncured', 'Deaths', 'Recovered']\n"", 'sizes = [total_still_affected, total_deaths, total_recovered]\n', ""colours = ['Yellow', 'Red', 'Green']\n"", 'explode = (0, 0.2, 0)\n', ""col_labels = ['Count']\n"", ""row_labels = ['Affected and Uncured', 'Deaths', 'Recovered']\n"", 'table_values = [[total_still_affected],[total_deaths], [total_recovered]]\n', '\n', '\n', 'fig, axs = plt.subplots(1,2, figsize = (9,9))\n', ""axs[0].axis('tight')\n"", ""axs[0].axis('off')\n"", ""the_table = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')\n"", 'the_table.set_fontsize(14)\n', 'the_table.scale(1.5, 1.5)\n', ""axs[1].pie(sizes, labels = groups, explode = explode, colors=colours, shadow=True, autopct='%1.1f%%')\n"", ""plt.title('Distribution at world level')\n"", 'plt.show()']"
relevant[col[6:21]] = relevant[col[6:21]].apply(normaliza),0,['relevant[col[6:21]] = relevant[col[6:21]].apply(normaliza)']
data_review.isnull().sum().sort_values(ascending=False),0,['data_review.isnull().sum().sort_values(ascending=False)']
"ASSIGN = pd.melt(ASSIGN, id_vars='Page', var_name='Date', value_name=""Visits"") ASSIGN = pd.read_csv('..path') ASSIGN['Date'] = ASSIGN['Page'].apply(lambda a: a[-10:]) ASSIGN['Page'] = ASSIGN['Page'].apply(lambda a: a[:-11]) ASSIGN = ASSIGN.merge(test, how=""left"") ASSIGN[['Id', 'Visits']].to_csv( '..path{}path'.format(args.seed), index=False)",0,"['test = pd.melt(test, id_vars=\'Page\', var_name=\'Date\', value_name=""Visits"")\n', '\n', ""key_df = pd.read_csv('../input/web-traffic-time-series-forecasting/key_2.csv')\n"", ""key_df['Date'] = key_df['Page'].apply(lambda a: a[-10:])\n"", ""key_df['Page'] = key_df['Page'].apply(lambda a: a[:-11])\n"", 'key_df = key_df.merge(test, how=""left"")\n', '\n', ""key_df[['Id', 'Visits']].to_csv(\n"", ""    '../working/{}/submission.csv'.format(args.seed), index=False)""]"
data_features['MasVnrArea'].groupby(data_features['MasVnrType']).describe(),0,"[""data_features['MasVnrArea'].groupby(data_features['MasVnrType']).describe()""]"
"CHECKPOINT print() ASSIGN = pd.read_csv('..path') ASSIGN.iloc[:, 1:] = ASSIGN.iloc[:, 1:].fillna(method='ffill', axis=1).fillna( ASSIGN='bfill', axis=1) ASSIGN = pd.date_range(args.forecast_start, args.forecast_end) for datetime in ASSIGN: ASSIGN[datetime.date().isoformat()] = 0 print() ASSIGN = pd.melt(full[list( ASSIGN.columns[args.offset+1:args.offset+args.val_len+1])+['Page']], ASSIGN='Page', var_name='Date', value_name=""Visits"") ASSIGN = ASSIGN.astype('datetime64[ns]') ASSIGN = ASSIGN.dt.dayofweek >= 5 print() ASSIGN = full.iloc[:, :args.offset+1] print() for i in args.windows: print(i, end=' ') ASSIGN = 'MW'+str(i) ASSIGN = pd.melt(train[list(train.columns[-i:])+['Page']], ASSIGN='Page', var_name='Date', value_name=val) ASSIGN = ASSIGN.astype('datetime64[ns]') ASSIGN= ASSIGN.dt.dayofweek >= 5 ASSIGN = tmp.groupby(['Page', 'Weekend']).median().reset_index() ASSIGN = ASSIGN.merge(tmp1, how='left') print() print() ASSIGN = test[[""MW7"", ""MW7"", ""MW14"", ""MW21"", ""MW35"", ""MW56"", ""MW91"", ""MW147"", ""MW238"", ""MW385"", ""MW623""]].median(axis=1)",0,"['print(""Getting data..."")\n', ""full = pd.read_csv('../input/web-traffic-time-series-forecasting/train_2.csv')\n"", ""full.iloc[:, 1:] = full.iloc[:, 1:].fillna(method='ffill', axis=1).fillna(\n"", ""        method='bfill', axis=1)\n"", 'datetime_list = pd.date_range(args.forecast_start, args.forecast_end)\n', 'for datetime in datetime_list:\n', '    full[datetime.date().isoformat()] = 0\n', '\n', 'print(""Constructing test set..."")\n', 'test = pd.melt(full[list(\n', ""    full.columns[args.offset+1:args.offset+args.val_len+1])+['Page']],\n"", '    id_vars=\'Page\', var_name=\'Date\', value_name=""Visits"")\n', ""test['Date'] = test['Date'].astype('datetime64[ns]')\n"", ""test['Weekend'] = test['Date'].dt.dayofweek >= 5\n"", '\n', 'print(""Constructing train set..."")\n', 'train = full.iloc[:, :args.offset+1]\n', '\n', 'print(""Getting medians..."")\n', 'for i in args.windows:\n', ""    print(i, end=' ')\n"", ""    val = 'MW'+str(i)\n"", ""    tmp = pd.melt(train[list(train.columns[-i:])+['Page']],\n"", ""                  id_vars='Page', var_name='Date', value_name=val)\n"", ""    tmp['Date'] = tmp['Date'].astype('datetime64[ns]')\n"", ""    tmp['Weekend']= tmp['Date'].dt.dayofweek >= 5           \n"", ""    tmp1 = tmp.groupby(['Page', 'Weekend']).median().reset_index()\n"", ""    test = test.merge(tmp1, how='left')\n"", 'print(""\\n"")\n', '\n', 'print(""Getting median of medians..."")\n', 'test[\'Predict\'] = test[[""MW7"", ""MW7"", ""MW14"", ""MW21"", ""MW35"", ""MW56"", ""MW91"",\n', '    ""MW147"", ""MW238"", ""MW385"", ""MW623""]].median(axis=1)']"
"CHECKPOINT ASSIGN = pd.read_csv(""..path"") print() print(ASSIGN.count().sum()) print() print(ASSIGN.isnull().sum())",0,"['#Q3.get the number of observations, missing values and nan values.\n', 'test_data = pd.read_csv(""../input/titanicdataset-traincsv/train.csv"")\n', 'print(""No.of Observations are:"")\n', 'print(test_data.count().sum())\n', 'print(""No. of Nan is:"")\n', 'print(test_data.isnull().sum())']"
"model.fit(X,train[""label""]) ASSIGN = model.predict(test) ASSIGN = pd.DataFrame({""ImageId"": list(range(1,len(predictions)+1)), ""Label"": ASSIGN}) ASSIGN.to_csv(""DR.csv"", index=False, header=True)",0,"['model.fit(X,train[""label""])\n', 'predictions = model.predict(test)\n', '\n', 'submissions = pd.DataFrame({""ImageId"": list(range(1,len(predictions)+1)),\n', '                         ""Label"": predictions})\n', 'submissions.to_csv(""DR.csv"", index=False, header=True)']"
"os.listdir(""..path"")",0,"['os.listdir(""../input/chicago-food-inspections"")']"
"CHECKPOINT ASSIGN = linear_model.Lasso(alpha=.001) ASSIGN.fit(x_t,Y_t) ASSIGN = model17.score(x_es,Y_es) print(ASSIGN*100,'%')",0,"['model17 = linear_model.Lasso(alpha=.001)\n', 'model17.fit(x_t,Y_t)\n', '\n', 'accuracy17 = model17.score(x_es,Y_es)\n', ""print(accuracy17*100,'%')""]"
"verify_images('..path', delete=True, max_size=500) verify_images('..path', delete=True, max_size=500)",0,"[""verify_images('../working/wolf', delete=True, max_size=500)\n"", ""verify_images('../working/dog', delete=True, max_size=500)""]"
"rf.fit(x_train,x_test) rf.score(y_train,y_test)",0,"['rf.fit(x_train,x_test)\n', 'rf.score(y_train,y_test)']"
"CHECKPOINT ASSIGN = pd.read_csv(""..path"") ASSIGN.head() ASSIGN.dropna(axis=1, how='all') print(ASSIGN.head()) print(ASSIGN.shape) ASSIGN[:50].mean() ASSIGN[ASSIGN['Sex']==1].mean() ASSIGN['Fare'].max()",0,"['#2\n', 'df = pd.read_csv(""../input/titanic/train_and_test2.csv"")\n', 'df.head()\n', '\n', ""df.dropna(axis=1, how='all')\n"", 'print(df.head())\n', 'print(df.shape)\n', '\n', 'df[:50].mean()\n', '\n', ""df[df['Sex']==1].mean()\n"", '\n', ""df['Fare'].max()""]"
cbdr.isnull().sum(),0,['cbdr.isnull().sum()']
CHECKPOINT country_df,0,['country_df']
"CHECKPOINT ASSIGN = model.evaluate(x_test, y_test, verbose = 0) print('Test loss: ', ASSIGN[0]) print('Test accuracy: ', ASSIGN[1])",0,"['# Evaluate Model\n', 'score = model.evaluate(x_test, y_test, verbose = 0)\n', ""print('Test loss: ', score[0])\n"", ""print('Test accuracy: ', score[1])""]"
"CHECKPOINT print(metrics.accuracy_score(y_train, predictions))",0,"['print(metrics.accuracy_score(y_train, predictions))']"
pca.fit(X_train),0,"['#Doing the PCA on the train data\n', 'pca.fit(X_train)']"
"SETUP CHECKPOINT ASSIGN = '..path' ASSIGN = pd.read_csv(iowa_file_path) ASSIGN = home_data.SalePrice ASSIGN = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd'] ASSIGN = home_data[features] train_X, val_X, train_y, val_y = train_test_split(ASSIGN, ASSIGN, random_state=1) ASSIGN = DecisionTreeRegressor(random_state=1) ASSIGN.fit(train_X, train_y) ASSIGN = iowa_model.predict(val_X) ASSIGN = mean_absolute_error(val_predictions, val_y) print(.format(ASSIGN)) ASSIGN = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1) ASSIGN.fit(train_X, train_y) ASSIGN = iowa_model.predict(val_X) ASSIGN = mean_absolute_error(val_predictions, val_y) print(.format(ASSIGN)) ASSIGN = RandomForestRegressor(random_state=1) ASSIGN.fit(train_X, train_y) ASSIGN = rf_model.predict(val_X) ASSIGN = mean_absolute_error(rf_val_predictions, val_y) print(.format(ASSIGN))",0,"['# Code you have previously used to load data\n', 'import pandas as pd\n', 'from sklearn.ensemble import RandomForestRegressor\n', 'from sklearn.metrics import mean_absolute_error\n', 'from sklearn.model_selection import train_test_split\n', 'from sklearn.tree import DecisionTreeRegressor\n', 'from learntools.core import *\n', 'import numpy as np\n', '\n', '\n', '\n', '# Path of the file to read. We changed the directory structure to simplify submitting to a competition\n', ""iowa_file_path = '../input/train.csv'\n"", '\n', 'home_data = pd.read_csv(iowa_file_path)\n', '# Create target object and call it y\n', 'y = home_data.SalePrice\n', '# Create X\n', ""features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n"", 'X = home_data[features]\n', '\n', '# Split into validation and training data\n', 'train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n', '\n', '# Specify Model\n', 'iowa_model = DecisionTreeRegressor(random_state=1)\n', '# Fit Model\n', 'iowa_model.fit(train_X, train_y)\n', '\n', '# Make validation predictions and calculate mean absolute error\n', 'val_predictions = iowa_model.predict(val_X)\n', 'val_mae = mean_absolute_error(val_predictions, val_y)\n', 'print(""Validation MAE when not specifying max_leaf_nodes: {:,.0f}"".format(val_mae))\n', '\n', '# Using best value for max_leaf_nodes\n', 'iowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\n', 'iowa_model.fit(train_X, train_y)\n', 'val_predictions = iowa_model.predict(val_X)\n', 'val_mae = mean_absolute_error(val_predictions, val_y)\n', 'print(""Validation MAE for best value of max_leaf_nodes: {:,.0f}"".format(val_mae))\n', '\n', '# Define the model. Set random_state to 1\n', 'rf_model = RandomForestRegressor(random_state=1)\n', 'rf_model.fit(train_X, train_y)\n', 'rf_val_predictions = rf_model.predict(val_X)\n', 'rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\n', '\n', 'print(""Validation MAE for Random Forest Model: {:,.0f}"".format(rf_val_mae))\n']"
"CHECKPOINT ASSIGN=folium.Map([39.7,-105],zoom_start=12) ASSIGN=pd.DataFrame({""Lat"":data['GEO_LAT'],""Long"":data['GEO_LON']}) ASSIGN=ASSIGN[:20000] ASSIGN.add_child(plugins.HeatMap(data=ASSIGN)) map_all",1,"['map_all=folium.Map([39.7,-105],zoom_start=12)\n', 'crime_new=pd.DataFrame({""Lat"":data[\'GEO_LAT\'],""Long"":data[\'GEO_LON\']})\n', 'crime_new=crime_new[:20000]\n', 'map_all.add_child(plugins.HeatMap(data=crime_new))\n', 'map_all']"
"ASSIGN=pd.read_csv(""..path"")",0,"['zomato_orgnl=pd.read_csv(""../input/zomato-bangalore-restaurants/zomato.csv"")']"
"ASSIGN = pd.concat([pred_table,original_train],axis = 0,sort = True) ASSIGN = pd.merge(original_test,final,on = ['Country','Province','Date'],how = 'left') ASSIGN = final_submit[['ForecastId','ConfirmedCases','Fatalities']]",0,"['final = pd.concat([pred_table,original_train],axis = 0,sort = True)\n', ""final_submit = pd.merge(original_test,final,on = ['Country','Province','Date'],how = 'left')\n"", ""submission = final_submit[['ForecastId','ConfirmedCases','Fatalities']]""]"
"ASSIGN = data_features.loc[data_features['Alley'].notnull(),'Alley'] data_features.loc[data_features['Alley'].notnull(),'Alley'].value_counts()",0,"[""a = data_features.loc[data_features['Alley'].notnull(),'Alley']\n"", ""data_features.loc[data_features['Alley'].notnull(),'Alley'].value_counts()""]"
"def createNetwork(seq_len): def addConv(network, features, kernel): ASSIGN = BatchNormalization()(ASSIGN) return Conv1D(features, kernel, padding='same', activation='relu')(ASSIGN) def addDense(ASSIGN, size): ASSIGN = BatchNormalization()(ASSIGN) ASSIGN = Dropout(0.2)(ASSIGN) return Dense(size, activation='relu')(ASSIGN) ASSIGN = Input(shape=(seq_len, 2)) ASSIGN = input for features in [16, 24, 32]: ASSIGN = addConv(ASSIGN, features, 5) ASSIGN = MaxPool1D(pool_size=5)(ASSIGN) for features in [64, 96, 128]: ASSIGN = addConv(ASSIGN, features, 5) ASSIGN = MaxPool1D(pool_size=5)(ASSIGN) for features in [256, 384, 512]: ASSIGN = addConv(ASSIGN, features, 5) ASSIGN = Flatten()(ASSIGN) for size in [128, 128]: ASSIGN = addDense(ASSIGN, size) ASSIGN = Dense(len(files), activation='softmax')(network) ASSIGN = Model(inputs = input, outputs = output) return model ASSIGN = createNetwork(sequence_length)",0,"['def createNetwork(seq_len):\n', '    \n', '    # Function to add a convolution layer with batch normalization\n', '    def addConv(network, features, kernel):\n', '        network = BatchNormalization()(network)\n', ""        return Conv1D(features, kernel, padding='same', activation='relu')(network)\n"", '    \n', '    # Function to add a dense layer with batch normalization and dropout\n', '    def addDense(network, size):\n', '        network = BatchNormalization()(network)\n', '        network = Dropout(0.2)(network)\n', ""        return Dense(size, activation='relu')(network)\n"", '    \n', '    \n', '    # Input layer\n', '    input = Input(shape=(seq_len, 2))\n', '    network = input\n', '    \n', '    # Add 1D Convolution\n', '    for features in [16, 24, 32]:\n', '        network = addConv(network, features, 5)\n', '    network = MaxPool1D(pool_size=5)(network)\n', '    \n', '    # Add 1D Convolution\n', '    for features in [64, 96, 128]:\n', '        network = addConv(network, features, 5)\n', '    network = MaxPool1D(pool_size=5)(network)\n', '\n', '    # Add 1D Convolution\n', '    for features in [256, 384, 512]:\n', '        network = addConv(network, features, 5)\n', '    #network = MaxPool1D(pool_size=5)(network)\n', '\n', '    # Flatten\n', '    network = Flatten()(network)\n', '    \n', '    # Dense layer for combination\n', '    for size in [128, 128]:\n', '        network = addDense(network, size)\n', '    \n', '    # Output layer\n', ""    output = Dense(len(files), activation='softmax')(network)\n"", '\n', '\n', '    # Create and compile model\n', '    model = Model(inputs = input, outputs = output)\n', ""#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n"", '\n', '#     # Display model\n', '#     model.summary()\n', '    return model\n', '\n', 'model = createNetwork(sequence_length)']"
"ASSIGN = Ridge() ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]} ASSIGN = GridSearchCV(ASSIGN, parameters, scoring='neg_mean_squared_error', cv = 100) ASSIGN.fit(x_train, Y_train) ASSIGN.best_params_",0,"['R = Ridge()\n', '\n', ""parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n"", '\n', ""R = GridSearchCV(R, parameters, scoring='neg_mean_squared_error', cv = 100)\n"", '\n', 'R.fit(x_train, Y_train)\n', 'R.best_params_']"
"ASSIGN=plt.figure(figsize=(20,40)) city_vs_count.plot(kind=""barh"",fontsize=30) plt.grid(b=True, which='both', color='Black',linestyle='-') plt.ylabel(""city names"",fontsize=50,color=""red"",fontweight='bold') plt.title(""CITY VS RESTAURANT COUNT GRAPH"",fontsize=50,color=""BLUE"",fontweight='bold') for v in range(len(city_vs_count)): plt.text(v+city_vs_count[v],v,city_vs_count[v],fontsize=20,color=""BLUE"",fontweight='bold')",1,"['#lets plot citywise restaurant count in barh form,and each bar should display the count of the corresponding restuants for that city\n', '\n', 'fig=plt.figure(figsize=(20,40))\n', 'city_vs_count.plot(kind=""barh"",fontsize=30)\n', ""plt.grid(b=True, which='both', color='Black',linestyle='-')\n"", 'plt.ylabel(""city names"",fontsize=50,color=""red"",fontweight=\'bold\')\n', 'plt.title(""CITY VS RESTAURANT COUNT GRAPH"",fontsize=50,color=""BLUE"",fontweight=\'bold\')\n', 'for v in range(len(city_vs_count)):\n', '    #plt.text(x axis location ,y axis location ,text value ,other parameters......)\n', '    plt.text(v+city_vs_count[v],v,city_vs_count[v],fontsize=20,color=""BLUE"",fontweight=\'bold\')']"
df.head(),0,['df.head()']
CHECKPOINT X_train,0,['X_train']
"ASSIGN=plt.subplots(1,2,figsize=(15,8)) ASSIGN = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",'cadetblue','hotpink','orange','darksalmon','brown') data_Brooklyn.neighbourhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=ASSIGN,ax=ax[0]) ax[0].set_title(""Top 10 neighbourhood by the number of rooms"",size=20) ax[0].set_xlabel('rooms',size=18) ASSIGN=data_Brooklyn['neighbourhood'].value_counts() ASSIGN=list(data_Brooklyn['neighbourhood'].value_counts().index)[:10] ASSIGN=list(count[:10]) ASSIGN.append(ASSIGN.agg(sum)-ASSIGN[:10].agg('sum')) ASSIGN.append('Other') ASSIGN=pd.DataFrame({""group"":groups,""counts"":counts}) ASSIGN=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum') ASSIGN = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1]) plt.legend(loc=0, bbox_to_anchor=(1.15,0.4)) plt.subplots_adjust(wspace =0.5, hspace =0) plt.ylabel('')",1,"['fig,ax=plt.subplots(1,2,figsize=(15,8))\n', 'clr = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",\'cadetblue\',\'hotpink\',\'orange\',\'darksalmon\',\'brown\')\n', ""data_Brooklyn.neighbourhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n"", 'ax[0].set_title(""Top 10 neighbourhood by the number of rooms"",size=20)\n', ""ax[0].set_xlabel('rooms',size=18)\n"", '\n', '\n', ""count=data_Brooklyn['neighbourhood'].value_counts()\n"", ""groups=list(data_Brooklyn['neighbourhood'].value_counts().index)[:10]\n"", 'counts=list(count[:10])\n', ""counts.append(count.agg(sum)-count[:10].agg('sum'))\n"", ""groups.append('Other')\n"", 'type_dict=pd.DataFrame({""group"":groups,""counts"":counts})\n', ""clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n"", ""qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n"", 'plt.legend(loc=0, bbox_to_anchor=(1.15,0.4)) \n', 'plt.subplots_adjust(wspace =0.5, hspace =0)\n', ""plt.ylabel('')""]"
SETUP np.random.seed(0) ASSIGN = pd.read_csv('..path') ASSIGN.sample(5),0,"['# Libraries\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', 'from sklearn.model_selection import train_test_split\n', 'from sklearn.metrics import confusion_matrix, classification_report\n', '\n', '# set seed for reproducibility\n', 'np.random.seed(0)\n', '\n', '#Data\n', ""tumor_data = pd.read_csv('../input/data.csv')\n"", 'tumor_data.sample(5)']"
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'fashion-mnist_train.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,"[""nRowsRead = 1000 # specify 'None' if want to read whole file\n"", '# fashion-mnist_train.csv has 60000 rows in reality, but we are only loading/previewing the first 1000 rows\n', ""df2 = pd.read_csv('../input/fashion-mnist_train.csv', delimiter=',', nrows = nRowsRead)\n"", ""df2.dataframeName = 'fashion-mnist_train.csv'\n"", 'nRow, nCol = df2.shape\n', ""print(f'There are {nRow} rows and {nCol} columns')""]"
data_import.head(),0,['data_import.head()']
SETUP,0,"['import numpy as np\n', 'import pandas as pd \n', 'import matplotlib.pyplot as plt\n', 'import os \n', 'import seaborn as sns\n', 'import geopandas as gpd\n', 'import folium\n', 'from folium import plugins\n', 'import datetime\n', 'import math']"
pca.components_,0,['pca.components_']
"ASSIGN = zomato_en.iloc[:,[2,3,5,6,7,8,9,11]] ASSIGN = zomato_en['rate']",0,"['x = zomato_en.iloc[:,[2,3,5,6,7,8,9,11]]\n', ""y = zomato_en['rate']""]"
CHECKPOINT df_final,0,"['#display the dataframe again\n', 'df_final']"
"ASSIGN = xgb.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('XGBoosting(HT).csv',index = False)",0,"['model18pred = xgb.predict(y_train)\n', ""submission18 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission18['PassengerId'] = result['PassengerId']\n"", ""submission18['Survived'] = model18pred\n"", ""submission18.to_csv('XGBoosting(HT).csv',index = False)""]"
train.head(),0,['train.head()']
"ASSIGN = plt.subplots(3,1, figsize=(20,20)) axes[0].imshow(np.squeeze(image.numpy()), cmap='gray') axes[0].set_title('image') axes[1].imshow(np.squeeze(lossy_image.numpy()), cmap='gray') axes[1].set_title('lossy image');",1,"['fig, axes = plt.subplots(3,1, figsize=(20,20))\n', '\n', ""axes[0].imshow(np.squeeze(image.numpy()), cmap='gray')\n"", ""axes[0].set_title('image')\n"", ""axes[1].imshow(np.squeeze(lossy_image.numpy()), cmap='gray')\n"", ""axes[1].set_title('lossy image');""]"
"ASSIGN=np.load(X_TRAIN_FILE) ASSIGN=a.f.arr_0 ASSIGN=np.load(Y_TRAIN_FILE) ASSIGN=a.f.arr_0 ASSIGN=np.load(X_TEST_FILE) ASSIGN=a ASSIGN=np.load(Y_TEST_FILE) ASSIGN=a ASSIGN, X_val, ASSIGN, Y_val = train_test_split(ASSIGN, ASSIGN, test_size=0.25, random_state=42)",0,"['a=np.load(X_TRAIN_FILE)\n', 'X_train=a.f.arr_0\n', 'a=np.load(Y_TRAIN_FILE)\n', 'Y_train=a.f.arr_0\n', 'a=np.load(X_TEST_FILE)\n', 'X_test=a\n', 'a=np.load(Y_TEST_FILE)\n', 'Y_test=a\n', '\n', 'X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)\n']"
"CHECKPOINT ASSIGN = resample(first_patient_pixels, first_patient, [1,1,1]) print(, first_patient_pixels.shape) print(, pix_resampled.shape)",0,"['pix_resampled, spacing = resample(first_patient_pixels, first_patient, [1,1,1])\n', 'print(""Shape before resampling\\t"", first_patient_pixels.shape)\n', 'print(""Shape after resampling\\t"", pix_resampled.shape)']"
"ASSIGN = pd.read_csv('..path') ASSIGN['ImageId'] = ASSIGN['Image_Label'].apply(lambda x: x.split('_')[0]) ASSIGN = pd.DataFrame(sub_df['ImageId'].unique(), columns=['ImageId'])",0,"[""sub_df = pd.read_csv('../input/understanding_cloud_organization/sample_submission.csv')\n"", ""sub_df['ImageId'] = sub_df['Image_Label'].apply(lambda x: x.split('_')[0])\n"", ""test_imgs = pd.DataFrame(sub_df['ImageId'].unique(), columns=['ImageId'])""]"
"final['Ticket_length'] = final['Ticket_length'].astype(str) final['Ticket_length'] = np.where(((final.Ticket_length == '4')),'Below 6',final.Ticket_length) final['Ticket_length'] = np.where(((final.Ticket_length == '5')),'At 6',final.Ticket_length) final['Ticket_length'] = np.where(((final.Ticket_length == '12')),'Above 6',final.Ticket_length)",0,"[""final['Ticket_length'] = final['Ticket_length'].astype(str)\n"", '\n', ""final['Ticket_length'] = np.where(((final.Ticket_length == '4')),'Below 6',final.Ticket_length)\n"", ""final['Ticket_length'] = np.where(((final.Ticket_length == '5')),'At 6',final.Ticket_length)\n"", ""final['Ticket_length'] = np.where(((final.Ticket_length == '12')),'Above 6',final.Ticket_length)\n""]"
"ASSIGN=LinearRegression() ASSIGN.fit(x_train,y_train)",0,"['reg=LinearRegression()\n', 'reg.fit(x_train,y_train)']"
SETUP,0,"['#Essential Imports\n', 'import pandas as pd\n', 'import numpy as np\n', 'import xmltodict\n', 'import collections']"
ASSIGN = ClassificationInterpretation.from_learner(learn) ASSIGN.plot_confusion_matrix(),1,"['interp = ClassificationInterpretation.from_learner(learn)\n', 'interp.plot_confusion_matrix()']"
"ASSIGN = [c for c in df_train.columns if c not in ['card_id', 'first_active_month']] ASSIGN = [c for c in features if 'feature_' in c]",0,"[""features = [c for c in df_train.columns if c not in ['card_id', 'first_active_month']]\n"", ""categorical_feats = [c for c in features if 'feature_' in c]""]"
SETUP,0,"['from mpl_toolkits.mplot3d import Axes3D\n', 'from sklearn.preprocessing import StandardScaler\n', 'import matplotlib.pyplot as plt # plotting\n', 'import numpy as np # linear algebra\n', 'import os # accessing directory structure\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n']"
ASSIGN=pd.DataFrame(df_np),0,"['#creating final dataframe from the numpy array\n', 'df_final=pd.DataFrame(df_np)']"
"sns.distplot(data[data['Retire']==1]['Age'], label = 'Retire == 1') sns.distplot(data[data['Retire']==0]['Age'], label = 'Retire == 0') plt.legend() plt.show()",1,"['#Distribution of age\n', ""sns.distplot(data[data['Retire']==1]['Age'], label = 'Retire == 1')\n"", ""sns.distplot(data[data['Retire']==0]['Age'], label = 'Retire == 0')\n"", 'plt.legend()\n', 'plt.show()']"
learn.lr_find() learn.recorder.plot(suggestion=True),1,"['learn.lr_find()\n', 'learn.recorder.plot(suggestion=True)']"
"CHECKPOINT ASSIGN = model2.predict(X_test) ASSIGN = mean_squared_error(y_test, y_pred2, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ASSIGN)))",0,"['y_pred2 = model2.predict(X_test)\n', '\n', 'val = mean_squared_error(y_test, y_pred2, squared=False)\n', 'val2 = str(round(val, 4))\n', '\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))\n""]"
match_df.info(),0,['match_df.info()']
"ASSIGN = [] ASSIGN = pd.read_csv('..path', index_col=['key_id'], ASSIGN=2048) for chunk in tqdm(ASSIGN, total=55): ASSIGN =[] for values in chunk.drawing.values: ASSIGN = json.loads(values) ASSIGN = [] for x_axis, y_axis in ASSIGN: ASSIGN.extend(list(zip(x_axis, y_axis))) ASSIGN = np.array(ASSIGN) ASSIGN = np.zeros((sequence_length, 2)) if sequence_length>ASSIGN.shape[0]: ASSIGN[:ASSIGN.shape[0],:] = ASSIGN else: ASSIGN = strokes[:sequence_length, :] ASSIGN.append(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.reshape(ASSIGN, (-1,sequence_length, 2)) ASSIGN = model.predict(X, verbose=0) ASSIGN = np.argsort(-testpreds)[:, 0:3] ASSIGN.append(ASSIGN) ASSIGN = np.concatenate(ttvlist)",0,"['#%% get test set\n', 'ttvlist = []\n', ""reader = pd.read_csv('../input/test_simplified.csv', index_col=['key_id'],\n"", '    chunksize=2048)\n', '\n', 'for chunk in tqdm(reader, total=55):\n', '    X =[]\n', '    for values in chunk.drawing.values:\n', '        image = json.loads(values)\n', '        strokes = []\n', '        for x_axis, y_axis in image:\n', '            strokes.extend(list(zip(x_axis, y_axis)))\n', '        strokes = np.array(strokes)\n', '        pad = np.zeros((sequence_length, 2))\n', '        if sequence_length>strokes.shape[0]:\n', '            pad[:strokes.shape[0],:] = strokes\n', '        else:\n', '            pad = strokes[:sequence_length, :]\n', '        X.append(pad)\n', '        \n', '    X = np.array(X)\n', '    X = np.reshape(X, (-1,sequence_length, 2))\n', '    testpreds = model.predict(X, verbose=0)\n', '    ttvs = np.argsort(-testpreds)[:, 0:3]\n', '    ttvlist.append(ttvs)\n', '#     imagebag = bag.from_sequence(chunk.drawing.values).map(draw_it)\n', '#     testarray = np.array(imagebag.compute())\n', '\n', '#     testarray = np.reshape(testarray, (testarray.shape[0], imheight, imwidth, 1))\n', '#     testpreds = model.predict(testarray, verbose=0)\n', '#     ttvs = np.argsort(-testpreds)[:, 0:3]  # top 3\n', '#     ttvlist.append(ttvs)\n', '    \n', 'ttvarray = np.concatenate(ttvlist)']"
"ASSIGN = load_scan(INPUT_FOLDER + patients[0]) ASSIGN = get_pixels_hu(first_patient) plt.hist(ASSIGN.flatten(), bins=180, color='c') plt.xlabel(""Hounsfield Units (HU)"") plt.ylabel(""Frequency"") plt.show() plt.imshow(ASSIGN[80], cmap=plt.cm.gray) plt.show()",1,"['first_patient = load_scan(INPUT_FOLDER + patients[0])\n', 'first_patient_pixels = get_pixels_hu(first_patient)\n', ""plt.hist(first_patient_pixels.flatten(), bins=180, color='c')\n"", 'plt.xlabel(""Hounsfield Units (HU)"")\n', 'plt.ylabel(""Frequency"")\n', 'plt.show()\n', '\n', '# Show some slice in the middle\n', 'plt.imshow(first_patient_pixels[80], cmap=plt.cm.gray)\n', 'plt.show()']"
"data.groupby(""target_label"").size()",0,"['data.groupby(""target_label"").size()']"
"CHECKPOINT def train(model, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): optimizer.zero_grad() ASSIGN = ASSIGN.to(device) ASSIGN = ASSIGN.to(device) ASSIGN = ASSIGN.requires_grad_() ASSIGN = model(data) ASSIGN = ASSIGN.permute(1, 0, 2) ASSIGN = F.nll_loss(output[0], target) ASSIGN.backward() optimizer.step() if batch_idx % log_interval == 0: print('Train Epoch: {} [{}path{} ({:.0f}%)]\tLoss: {:.6f}'.format( epoch, batch_idx * len(ASSIGN), len(train_loader.dataset), 100. * batch_idx path(train_loader), ASSIGN))",0,"['def train(model, epoch):\n', '    model.train()\n', '    for batch_idx, (data, target) in enumerate(train_loader):\n', '        optimizer.zero_grad()\n', '        data = data.to(device)\n', '        target = target.to(device)\n', '        data = data.requires_grad_() #set requires_grad to True for training\n', '        output = model(data)\n', '        output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x10 \n', '        loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex10 input\n', '        loss.backward()\n', '        optimizer.step()\n', '        if batch_idx % log_interval == 0: #print training stats\n', ""            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n"", '                epoch, batch_idx * len(data), len(train_loader.dataset),\n', '                100. * batch_idx / len(train_loader), loss))']"
train.head(),0,['train.head()']
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'books.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,"[""nRowsRead = 1000 # specify 'None' if want to read whole file\n"", '# books.csv has 13719 rows in reality, but we are only loading/previewing the first 1000 rows\n', ""df1 = pd.read_csv('../input/books.csv', delimiter=',', nrows = nRowsRead)\n"", ""df1.dataframeName = 'books.csv'\n"", 'nRow, nCol = df1.shape\n', ""print(f'There are {nRow} rows and {nCol} columns')""]"
SETUP CHECKPOINT warnings.filterwarnings('ignore') print(os.listdir()),0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'from datetime import datetime as dt\n', '\n', '# For Visualisation\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', '%matplotlib inline\n', '\n', '# To Scale our data\n', 'from sklearn.preprocessing import scale\n', '\n', '# Supress Warnings\n', 'import warnings\n', ""warnings.filterwarnings('ignore')\n"", '\n', 'from sklearn.datasets import fetch_mldata\n', 'from sklearn.decomposition import PCA\n', 'from sklearn.preprocessing import StandardScaler\n', 'from sklearn import metrics\n', 'from sklearn.model_selection import train_test_split\n', '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n', '\n', 'import os\n', 'print(os.listdir(""../input""))\n', '\n', '# Any results you write to the current directory are saved as output.']"
"def rank_export(country): ASSIGN=['2010','2011','2012','2013','2014','2015','2016','2017','2018'] B={} for i in range(len(ASSIGN)): A=data_export[data_export.ASSIGN==int(ASSIGN[i])] ASSIGN=A.groupby(['country'])['ASSIGN'].agg('sum') ASSIGN=A.groupby(['country'])['value'].agg('sum').ASSIGN(method='min',ascending=False) ASSIGN=pd.DataFrame({'rank':rank,'value':value}) B['ASSIGN '+ASSIGN[i]]=str(ASSIGN[ASSIGN.index==country].iloc[0,0])+""path""+str(max(ASSIGN)) B['ASSIGN '+ASSIGN[i]]=str(ASSIGN[ASSIGN.index==country].iloc[0,1]) return B",0,"['def rank_export(country):\n', ""    year=['2010','2011','2012','2013','2014','2015','2016','2017','2018']\n"", '    B={}\n', '    for i in range(len(year)):\n', '        A=data_export[data_export.year==int(year[i])]\n', ""        value=A.groupby(['country'])['value'].agg('sum')\n"", ""        rank=A.groupby(['country'])['value'].agg('sum').rank(method='min',ascending=False)\n"", ""        new=pd.DataFrame({'rank':rank,'value':value})\n"", '        B[\'rank \'+year[i]]=str(new[new.index==country].iloc[0,0])+""/""+str(max(rank))\n', ""        B['value '+year[i]]=str(new[new.index==country].iloc[0,1])\n"", '\n', '    return B']"
"SETUP CHECKPOINT ASSIGN = QuoraFeatureExtractor(num_words=95000, max_len=70) ASSIGN = qfe.fit_transform(input_df.question_text, fit_mask=input_df.target.notnull().values) print({ k: v.shape for (k, v) in ASSIGN.items() })",0,"['%%time\n', 'qfe = QuoraFeatureExtractor(num_words=95000, max_len=70)\n', 'input_X = qfe.fit_transform(input_df.question_text, fit_mask=input_df.target.notnull().values)\n', 'print({ k: v.shape for (k, v) in input_X.items() })']"
"ASSIGN = kickstarters_2017.usd_pledged_real > 0 ASSIGN = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges] ASSIGN = stats.boxcox(positive_pledges)[0] ASSIGN=plt.subplots(1,2) sns.distplot(ASSIGN, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Normalized data"")",1,"['# get the index of all positive pledges (Box-Cox only takes postive values)\n', 'index_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0\n', '\n', '# get only positive pledges (using their indexes)\n', 'positive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]\n', '\n', '# normalize the pledges (w/ Box-Cox)\n', 'normalized_pledges = stats.boxcox(positive_pledges)[0]\n', '\n', '# plot both together to compare\n', 'fig, ax=plt.subplots(1,2)\n', 'sns.distplot(positive_pledges, ax=ax[0])\n', 'ax[0].set_title(""Original Data"")\n', 'sns.distplot(normalized_pledges, ax=ax[1])\n', 'ax[1].set_title(""Normalized data"")']"
"CHECKPOINT ASSIGN = tourney_result.drop(['Season', 'WTeamID', 'LTeamID'], axis=1) ASSIGN.rename(columns={'WSeed':'Seed1', 'LSeed':'Seed2', 'WScoreT':'ScoreT1', 'LScoreT':'ScoreT2'}, inplace=True) tourney_win_result",0,"[""tourney_win_result = tourney_result.drop(['Season', 'WTeamID', 'LTeamID'], axis=1)\n"", ""tourney_win_result.rename(columns={'WSeed':'Seed1', 'LSeed':'Seed2', 'WScoreT':'ScoreT1', 'LScoreT':'ScoreT2'}, inplace=True)\n"", 'tourney_win_result']"
ASSIGN = ASSIGN.loc[ASSIGN.rate !='NEW'] ASSIGN = ASSIGN.loc[ASSIGN.rate !='-'].reset_index(drop=True),0,"[""zomato = zomato.loc[zomato.rate !='NEW']\n"", ""zomato = zomato.loc[zomato.rate !='-'].reset_index(drop=True)""]"
"ASSIGN = pd.read_csv(""..path"")",0,"['sub = pd.read_csv(""../input/mh-forest/Forest_Cover_participants_Data/sample_submission.csv"")']"
tb.info(),0,['tb.info()']
player_attr_df.describe(),0,['player_attr_df.describe()']
SETUP,0,['import pandas as pd\n']
"df.reset_index(drop=False, inplace=True)",0,"['df.reset_index(drop=False, inplace=True)']"
"data.groupby(['YEAR','MONTH'])['OFFENSE_CODE_GROUP'].agg('count').unstack('YEAR') ASSIGN=plt.subplots(figsize=(15,6)) data.groupby(['MONTH','YEAR'])['OFFENSE_CODE_GROUP'].agg('count').unstack().plot(ax=ax) plt.title(""Counting the number for Crime (month)"") plt.grid(True) data.groupby(['YEAR','MONTH'])['OFFENSE_CODE_GROUP'].agg('count').unstack('YEAR') ASSIGN=plt.subplots(figsize=(18,12)) data.groupby(['MONTH','YEAR'])['OFFENSE_CODE_GROUP'].agg('count').unstack().plot(kind='bar',ax=ax) pl.xticks(rotation=360) plt.title(""Counting the number for Crime (month)"") plt.grid(True)",1,"['\n', ""data.groupby(['YEAR','MONTH'])['OFFENSE_CODE_GROUP'].agg('count').unstack('YEAR')\n"", 'fig,ax=plt.subplots(figsize=(15,6))\n', ""data.groupby(['MONTH','YEAR'])['OFFENSE_CODE_GROUP'].agg('count').unstack().plot(ax=ax)\n"", 'plt.title(""Counting the number for Crime (month)"")\n', 'plt.grid(True)\n', '\n', ""data.groupby(['YEAR','MONTH'])['OFFENSE_CODE_GROUP'].agg('count').unstack('YEAR')\n"", 'fig,ax=plt.subplots(figsize=(18,12))\n', ""data.groupby(['MONTH','YEAR'])['OFFENSE_CODE_GROUP'].agg('count').unstack().plot(kind='bar',ax=ax)\n"", 'pl.xticks(rotation=360)\n', 'plt.title(""Counting the number for Crime (month)"")\n', 'plt.grid(True)']"
"ASSIGN = data['Birth Country'].unique() ASSIGN = {} for category in categories: for country in ASSIGN: ASSIGN = data.Category[(data['Category'] == category) & (data['Birth Country'] == country)].count() ASSIGN.setdefault(category, []).append(ASSIGN) for country in ASSIGN: ASSIGN.setdefault('country', []).append(country) ASSIGN = pd.DataFrame(dict) for lab, row in ASSIGN.iterrows() : ASSIGN.loc[lab, 'total'] = row['Economics'] + row['Chemistry'] + row['Literature'] + row['Medicine'] + row['Peace'] + row['Physics'] ASSIGN = ASSIGN.sort_values('total')",0,"[""countries = data['Birth Country'].unique()\n"", '\n', 'dict = {}\n', '\n', 'for category in categories:\n', '    for country in countries:\n', ""        x = data.Category[(data['Category'] == category) & (data['Birth Country'] == country)].count()\n"", '        #print(category, country, x)\n', '        dict.setdefault(category, []).append(x)\n', '        \n', 'for country in countries:\n', ""    dict.setdefault('country', []).append(country)\n"", '\n', 'df = pd.DataFrame(dict)\n', '\n', 'for lab, row in df.iterrows() :\n', ""    df.loc[lab, 'total'] = row['Economics'] + row['Chemistry'] + row['Literature'] + row['Medicine'] + row['Peace'] + row['Physics']\n"", '   \n', ""df = df.sort_values('total')\n"", '        ']"
"model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)",0,"['#train the model\n', 'model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)']"
"train_X, test_X, train_Y, test_Y = train_test_split(X, Y, stratify=Y, test_size=0.8, random_state=1234)",0,"['train_X, test_X, train_Y, test_Y = train_test_split(X, Y, stratify=Y, test_size=0.8, random_state=1234)']"
"ASSIGN = pd.get_dummies(ASSIGN, columns = [""Title""])",0,"['final = pd.get_dummies(final, columns = [""Title""])']"
"train_df.width.nunique(),train_df.height.nunique()",0,"['train_df.width.nunique(),train_df.height.nunique()']"
"CHECKPOINT ASSIGN=-73.92 ASSIGN=40.86 ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(manha_map) for lat,lon,label in zip(data_manha_65.latitude,data_manha_65.longitude,data_manha_65.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN) manha_map",1,"['Long=-73.92\n', 'Lat=40.86\n', 'manha_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'manha_rooms_map=plugins.MarkerCluster().add_to(manha_map)\n', 'for lat,lon,label in zip(data_manha_65.latitude,data_manha_65.longitude,data_manha_65.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(manha_rooms_map)\n', 'manha_map.add_child(manha_rooms_map)\n', '\n', 'manha_map']"
CHECKPOINT SLICE=2020 print(thisdict),0,"['#Replacing the value of year to 2020\n', 'thisdict[""year""]=2020\n', 'print(thisdict)']"
CHECKPOINT df_np.shape,0,"['#no of rows is represents the total no restaurants ,now of coloumns(12) is columns for the dataframe\n', 'df_np.shape']"
CHECKPOINT df_sec,0,"['#display the second dataframe contents\n', 'df_sec']"
"SETUP ASSIGN = SVR() ASSIGN = {'C':[.0001 ,.001 ,0.1, 1, 10, 100, 1000], 'epsilon':[0.001, 0.01, 0.1, 0.5, 1, 2, 4] } ASSIGN = GridSearchCV(Svr, parameters, scoring='neg_mean_squared_error', cv = 10) ASSIGN.fit(x_t, Y_t) ASSIGN.best_params_",0,"['from sklearn.svm import SVR\n', '\n', 'Svr = SVR()\n', '\n', ""parameters = {'C':[.0001 ,.001 ,0.1, 1, 10, 100, 1000],\n"", ""              'epsilon':[0.001, 0.01, 0.1, 0.5, 1, 2, 4]\n"", '             }\n', '\n', ""Enr = GridSearchCV(Svr, parameters, scoring='neg_mean_squared_error', cv = 10)\n"", '\n', 'Enr.fit(x_t, Y_t)\n', 'Enr.best_params_']"
df2.head(5),0,['df2.head(5)']
"CHECKPOINT tourney_win_result['result'] = 1 tourney_lose_result['result'] = 0 ASSIGN = pd.concat((tourney_win_result, tourney_lose_result)).reset_index(drop=True) tourney_result",0,"[""tourney_win_result['result'] = 1\n"", ""tourney_lose_result['result'] = 0\n"", 'tourney_result = pd.concat((tourney_win_result, tourney_lose_result)).reset_index(drop=True)\n', 'tourney_result']"
"CHECKPOINT ASSIGN = pd.get_dummies(data_features) ASSIGN = final_features.iloc[:len(y_train),:] ASSIGN = final_features.iloc[len(y_train):,:] print('The shape of train set is{},y set is{},and the shape of test set is{}'.format(ASSIGN.shape,y_train.shape,ASSIGN.shape))",0,"['final_features = pd.get_dummies(data_features)\n', 'X_train = final_features.iloc[:len(y_train),:]\n', 'X_test = final_features.iloc[len(y_train):,:]\n', ""print('The shape of train set is{},y set is{},and the shape of test set is{}'.format(X_train.shape,y_train.shape,X_test.shape))""]"
"Top10_crime_type=data[data['OFFENSE_CATEGORY_ID'].isin(list(data.OFFENSE_CATEGORY_ID.value_counts()[:10].index[:10]))] ASSIGN=plt.subplots(2,2,figsize=(20,20)) ASSIGN=Top10_crime_type.OFFENSE_CATEGORY_ID.value_counts().index ASSIGN=Top10_crime_type.OFFENSE_CATEGORY_ID.value_counts() sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,0]) ax[0,0].set_title(""Top 10 crime type by counts"",size=20) ax[0,0].set_xlabel('counts',size=18) ax[0,0].set_ylabel('') Top10_crime_type.groupby(['year','OFFENSE_CATEGORY_ID'])['INCIDENT_ID'].agg('count').unstack('OFFENSE_CATEGORY_ID').plot(ax=ax[0,1]) ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1)) ax[0,1].set_title(""Top 10 crime type counts by year"",size=20) ax[0,1].set_ylabel('counts',size=18) ax[0,1].set_xlabel('year',size=18) Top10_crime_type.groupby(['month','OFFENSE_CATEGORY_ID'])['INCIDENT_ID'].agg('count').unstack('OFFENSE_CATEGORY_ID').plot(ax=ax[1,0]) ax[1,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(-0.25,1)) ax[1,0].set_title(""Top 10 crime type counts by month"",size=20) ax[1,0].set_ylabel('counts',size=18) ax[1,0].set_xlabel('month',size=18) sns.scatterplot(ASSIGN=""GEO_LON"", ASSIGN=""GEO_LAT"", hue=""OFFENSE_CATEGORY_ID"",data=Top10_crime_type,ax=ax[1,1]) ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1)) ax[1,1].set_title(""The distribution of Top 10 crime type"",size=20) ax[1,1].set(xlabel='Longitude', ylabel='LATITUDE')",1,"[""Top10_crime_type=data[data['OFFENSE_CATEGORY_ID'].isin(list(data.OFFENSE_CATEGORY_ID.value_counts()[:10].index[:10]))]\n"", 'fig,ax=plt.subplots(2,2,figsize=(20,20))\n', 'y=Top10_crime_type.OFFENSE_CATEGORY_ID.value_counts().index\n', 'x=Top10_crime_type.OFFENSE_CATEGORY_ID.value_counts()\n', 'sns.barplot(x=x,y=y,ax=ax[0,0])\n', 'ax[0,0].set_title(""Top 10 crime type by counts"",size=20)\n', ""ax[0,0].set_xlabel('counts',size=18)\n"", ""ax[0,0].set_ylabel('')\n"", '\n', '\n', ""Top10_crime_type.groupby(['year','OFFENSE_CATEGORY_ID'])['INCIDENT_ID'].agg('count').unstack('OFFENSE_CATEGORY_ID').plot(ax=ax[0,1])\n"", 'ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n', 'ax[0,1].set_title(""Top 10 crime type counts by year"",size=20)\n', ""ax[0,1].set_ylabel('counts',size=18)\n"", ""ax[0,1].set_xlabel('year',size=18)\n"", '\n', ""Top10_crime_type.groupby(['month','OFFENSE_CATEGORY_ID'])['INCIDENT_ID'].agg('count').unstack('OFFENSE_CATEGORY_ID').plot(ax=ax[1,0])\n"", 'ax[1,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(-0.25,1))\n', 'ax[1,0].set_title(""Top 10 crime type counts by month"",size=20)\n', ""ax[1,0].set_ylabel('counts',size=18)\n"", ""ax[1,0].set_xlabel('month',size=18)\n"", '\n', 'sns.scatterplot(x=""GEO_LON"", y=""GEO_LAT"", hue=""OFFENSE_CATEGORY_ID"",data=Top10_crime_type,ax=ax[1,1])\n', 'ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n', 'ax[1,1].set_title(""The distribution of Top 10 crime type"",size=20)\n', ""ax[1,1].set(xlabel='Longitude', ylabel='LATITUDE')\n""]"
CHECKPOINT df[dep_var].value_counts()[1]path,0,"['# check % positive values in train set\n', 'df[dep_var].value_counts()[1]/199523']"
ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path'),0,"[""df_train = pd.read_csv('../input/elo-merchant-category-recommendation/train.csv')\n"", ""df_test = pd.read_csv('../input/elo-merchant-category-recommendation/test.csv')\n"", ""df_hist_trans = pd.read_csv('../input/elo-merchant-category-recommendation/historical_transactions.csv')\n"", ""df_new_merchant_trans = pd.read_csv('../input/elo-merchant-category-recommendation/new_merchant_transactions.csv')\n"", ""df_merchants = pd.read_csv('../input/elo-merchant-category-recommendation/merchants.csv')""]"
CHECKPOINT cbdr.shape,0,['cbdr.shape']
"learn.unfreeze() learn.fit_one_cycle(2, max_lr=slice(1e-6,3e-4)) learn.save('stage-2')",0,"['learn.unfreeze()\n', 'learn.fit_one_cycle(2, max_lr=slice(1e-6,3e-4))\n', ""learn.save('stage-2')""]"
"CHECKPOINT ASSIGN = myDataFrame(first_Digit, final_df['First Digit Test'], final_df['First Digit Expected']) ASSIGN = myDataFrame(second_Digit, final_df['Second Digit Test'], final_df['Second Digit Expected']) ASSIGN = myDataFrame(third_Digit, final_df['Third Digit Test'], final_df['Third Digit Expected']) for x in [ASSIGN, ASSIGN, ASSIGN]: for y in ['Digit Test (%)', 'Expected Values (%)', 'Difference (%)']: ASSIGN = round(ASSIGN.apply(lambda i: i*100), 2) first_digit_df",0,"['#This runs the function we created before on each of my different tests\n', ""first_digit_df = myDataFrame(first_Digit, final_df['First Digit Test'], final_df['First Digit Expected'])\n"", ""second_digit_df = myDataFrame(second_Digit, final_df['Second Digit Test'], final_df['Second Digit Expected'])\n"", ""third_digit_df = myDataFrame(third_Digit, final_df['Third Digit Test'], final_df['Third Digit Expected'])\n"", '\n', '#now I convert the numbers into percentage values to make my data tables more readable\n', 'for x in [first_digit_df, second_digit_df, third_digit_df]:\n', ""    for y in ['Digit Test (%)', 'Expected Values (%)', 'Difference (%)']:\n"", '        x[y] = round(x[y].apply(lambda i: i*100), 2)\n', '\n', '#Below is an example of the completed data table\n', 'first_digit_df']"
len(li),0,['len(li)']
"CHECKPOINT ASSIGN = LogisticRegression() ASSIGN.fit(X_train, y_train) ASSIGN = logreg_new.predict(X_test) print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(ASSIGN.score(X_test, y_test))) print(confusion_matrix(y_test, ASSIGN)) print(classification_report(y_test, ASSIGN))",0,"['# New model\n', 'logreg_new = LogisticRegression()\n', 'logreg_new.fit(X_train, y_train)\n', '\n', '#  Model metrics\n', 'y_pred = logreg_new.predict(X_test)\n', ""print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg_new.score(X_test, y_test)))\n"", 'print(confusion_matrix(y_test, y_pred))\n', 'print(classification_report(y_test, y_pred))']"
"ASSIGN = ['wolf','dog'] np.random.seed(42) ASSIGN = ImageDataBunch.from_folder('..path', train=""."", valid_pct=0.2, ASSIGN=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)",0,"[""classes = ['wolf','dog']\n"", 'np.random.seed(42)\n', 'data = ImageDataBunch.from_folder(\'../working\', train=""."", valid_pct=0.2,\n', '        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)']"
SETUP,0,"['import numpy as np\n', 'import random as rn\n', 'import matplotlib.pyplot as plt']"
"CHECKPOINT ASSIGN = model6.predict(X_test) ASSIGN = mean_squared_error(y_test, y_pred6, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ASSIGN)))",0,"['y_pred6 = model6.predict(X_test)\n', '\n', 'val = mean_squared_error(y_test, y_pred6, squared=False)\n', 'val6 = str(round(val, 4))\n', '\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred6)))\n""]"
"ASSIGN=df_final.merge(df_sec,on=""CITY"",how=""left"")",0,"['#merge this data frame to the existing df_final data frame using merge and join features from pandas,and creating a new data frame\n', 'df_final2=df_final.merge(df_sec,on=""CITY"",how=""left"")']"
"ASSIGN = nn.Sequential(nn.Linear(input_dim, 2048), nn.Dropout(0.5), nn.ReLU(), nn.Linear(2048, 256), nn.Dropout(0.5), nn.ReLU(), nn.Linear(256, output_dim), nn.Sigmoid()).double()",0,"['model = nn.Sequential(nn.Linear(input_dim, 2048),\n', '                      nn.Dropout(0.5),\n', '                      nn.ReLU(),\n', '                      nn.Linear(2048, 256),\n', '                      nn.Dropout(0.5),\n', '                      nn.ReLU(),\n', '                      nn.Linear(256, output_dim),\n', '                      nn.Sigmoid()).double()']"
df_all.sample(n=5),0,['df_all.sample(n=5)']
ASSIGN=reshape_X(normalize_X(ASSIGN)) ASSIGN=reshape_X(normalize_X(ASSIGN)) ASSIGN=reshape_X(normalize_X(ASSIGN)) ASSIGN=to_categorical(ASSIGN) ASSIGN=to_categorical(ASSIGN) ASSIGN=to_categorical(ASSIGN),0,"['#Pre processing the data\n', 'X_train=reshape_X(normalize_X(X_train))\n', 'X_val=reshape_X(normalize_X(X_val))\n', 'X_test=reshape_X(normalize_X(X_test))\n', 'y_train=to_categorical(y_train)\n', 'y_val=to_categorical(y_val)\n', 'y_test=to_categorical(y_test)\n']"
data_features['MasVnrArea'].groupby(data_features['MasVnrType']).median(),0,"[""data_features['MasVnrArea'].groupby(data_features['MasVnrType']).median()""]"
"ASSIGN = data_mat.drop('romantic', axis = 1) ASSIGN = data_mat['romantic']",0,"[""data_matf = data_mat.drop('romantic', axis = 1)\n"", ""data_matl = data_mat['romantic']""]"
test.head(),0,['test.head()']
"ASSIGN = np.array(train.iloc[:,train.columns != 'Survived']) ASSIGN = np.array(train.Survived).reshape(-1,1)",0,"[""X = np.array(train.iloc[:,train.columns != 'Survived'])\n"", 'y = np.array(train.Survived).reshape(-1,1)']"
"CHECKPOINT ASSIGN=confusion_matrix(y_test,predictions) print(ASSIGN)",0,"['reg_cm=confusion_matrix(y_test,predictions)\n', 'print(reg_cm)']"
"CHECKPOINT ASSIGN=data_manha.loc[(data_manha['price'] >=65) & (data_manha['price'] <80)] ASSIGN['label']=ASSIGN.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1) ASSIGN=-73.92 ASSIGN=40.86 ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(manha_65_80_map) for lat,lon,label in zip(ASSIGN.latitude,ASSIGN.longitude,ASSIGN.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN) manha_65_80_map",1,"[""data_manha_65_80=data_manha.loc[(data_manha['price'] >=65) & (data_manha['price'] <80)]\n"", ""data_manha_65_80['label']=data_manha_65_80.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n"", 'Long=-73.92\n', 'Lat=40.86\n', 'manha_65_80_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'manha_65_80_rooms_map=plugins.MarkerCluster().add_to(manha_65_80_map)\n', 'for lat,lon,label in zip(data_manha_65_80.latitude,data_manha_65_80.longitude,data_manha_65_80.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(manha_65_80_rooms_map)\n', 'manha_65_80_map.add_child(manha_65_80_rooms_map)\n', '\n', 'manha_65_80_map']"
"plt.figure(figsize = (7,6)) ASSIGN = plt.bar(top10_recover.keys(), top10_recover.values()) plt.xlabel('Country') plt.ylabel('Count') plt.title('Highest Recovered Cases in 10 countries') plt.xticks(list(top10_recover.keys()), rotation = 90) for bar in ASSIGN: ASSIGN = bar.get_height() plt.text(bar.get_x(), ASSIGN + .005, ASSIGN, rotation = 5) plt.show()",1,"['plt.figure(figsize = (7,6))\n', 'bars = plt.bar(top10_recover.keys(), top10_recover.values())\n', ""plt.xlabel('Country')\n"", ""plt.ylabel('Count')\n"", ""plt.title('Highest Recovered Cases in 10 countries')\n"", 'plt.xticks(list(top10_recover.keys()), rotation = 90)\n', 'for bar in bars:\n', '    yval = bar.get_height()\n', '    plt.text(bar.get_x(), yval + .005, yval, rotation = 5)\n', 'plt.show()']"
"df_all['first_affiliate_tracked'].fillna(-1, inplace=True)",0,"[""df_all['first_affiliate_tracked'].fillna(-1, inplace=True)""]"
"learn.unfreeze() learn.fit_one_cycle(10,max_lr=5e-5)",0,"['learn.unfreeze()\n', '#learn.fit_one_cycle(10,max_lr=1e-6)\n', 'learn.fit_one_cycle(10,max_lr=5e-5)']"
"sns.heatmap(test.isnull(), cbar=False)",1,"['sns.heatmap(test.isnull(), cbar=False)']"
SETUP,0,"['import numpy as np\n', 'import pandas as pd \n', 'import matplotlib.pyplot as plt\n', 'import os\n', 'import seaborn as sns\n', 'import math\n', 'import datetime']"
"ASSIGN = n_estimators[cv_results.index(max(cv_results))] print (""ASSIGN: "", ASSIGN) print (""best accuracy: "", max(cv_results))",0,"['best_n_estimators = n_estimators[cv_results.index(max(cv_results))]\n', 'print (""best_n_estimators: "", best_n_estimators)\n', 'print (""best accuracy: "", max(cv_results))']"
del X['ConfirmedCases'] del X['Fatalities'] del X['Id_x'] del X['Date'] del X['Id_y'] del X['Province'] del X['Country'],0,"[""del X['ConfirmedCases']\n"", ""del X['Fatalities']\n"", ""del X['Id_x']\n"", ""del X['Date']\n"", ""del X['Id_y']\n"", ""del X['Province']\n"", ""del X['Country']""]"
CHECKPOINT print() print(iris_data.shape) print() print(type(iris_data)) print() print(iris_data.head(3)),0,"['#Q4.load the iris data from a given csv file into a dataframe and \n', '#print the shape of the data, type of the data and first 3 rows.\n', 'print(""Shape of the data:"")\n', 'print(iris_data.shape)\n', 'print(""\\nData Type:"")\n', 'print(type(iris_data))\n', 'print(""\\nFirst 3 rows:"")\n', 'print(iris_data.head(3))']"
"ASSIGN=data[data['Countrypath']=='Mainland China'] ASSIGN= ASSIGN.groupby(['date','Provincepath'])['Confirmed', 'Deaths', 'Recovered'].max() ASSIGN = ASSIGN.reset_index() ASSIGN.head()",0,"[""china=data[data['Country/Region']=='Mainland China']\n"", ""china= china.groupby(['date','Province/State'])['Confirmed', 'Deaths', 'Recovered'].max()\n"", 'china = china.reset_index()\n', 'china.head()']"
SETUP,0,"['# 查看cleaned.csv 里面保存的是被清理之后的正确标签\n', '!cat {path}/cleaned.csv -n']"
final.Ticket.unique(),0,['final.Ticket.unique()']
ASSIGN = X.min() ASSIGN = (X - X_min).max() ASSIGN = (X - X_min)path,0,"['#Normalize X\n', 'X_min = X.min()\n', 'X_range = (X - X_min).max()\n', 'X_scaled = (X - X_min)/X_range']"
draw_df.drawing.values[0],0,['draw_df.drawing.values[0]']
"ASSIGN = RandomForestClassifier(n_estimators=100) ASSIGN.fit(X_train, Y_train) ASSIGN = random_forest.predict(X_test) ASSIGN.score(X_train, Y_train)",0,"['# Random Forests\n', '\n', 'random_forest = RandomForestClassifier(n_estimators=100)\n', '\n', 'random_forest.fit(X_train, Y_train)\n', '\n', 'Y_pred = random_forest.predict(X_test)\n', '\n', 'random_forest.score(X_train, Y_train)']"
"SETUP CHECKPOINT estimator.append(('KNN',KNeighborsClassifier(n_neighbors = 11))) ASSIGN = cross_val_score(KNN,x_train,x_test,ASSIGN=10) ASSIGN = cv.mean() accuracy.append(ASSIGN) print(ASSIGN) print(ASSIGN.mean())",0,"['KNN = KNeighborsClassifier(n_neighbors = 11)\n', ""estimator.append(('KNN',KNeighborsClassifier(n_neighbors = 11)))\n"", 'cv = cross_val_score(KNN,x_train,x_test,cv=10)\n', 'accuracy10 = cv.mean()\n', 'accuracy.append(accuracy10)\n', 'print(cv)\n', 'print(cv.mean())']"
"ASSIGN = range(0,40000,100) ASSIGN = [] for x in ASSIGN: ASSIGN.append(0.6432042 + 0.00002635951*x - 9.49824e-10*x**2 + 9.629459e-15*x**3) plt.plot(ASSIGN,ASSIGN);",1,"['# get tp driver\n', 'tp = range(0,40000,100) \n', 'y = []\n', 'for x in tp:\n', '    y.append(0.6432042 + 0.00002635951*x - 9.49824e-10*x**2 + 9.629459e-15*x**3) # from my own analysis...\n', '    \n', 'plt.plot(tp,y);']"
SETUP CHECKPOINT if KAGGLE_RUN: print('Kaggle run') pd.options.display.float_format = '{:.6f}'.format,0,"[""KAGGLE_RUN = (not os.path.exists('/opt/conda/home/.history'))\n"", ""if KAGGLE_RUN: print('Kaggle run')\n"", '\n', ""pd.options.display.float_format = '{:.6f}'.format""]"
"ASSIGN=['CSK','KKR','DC','MI'] ASSIGN=[149,218,188,143] plt.bar(ASSIGN,ASSIGN,color=['gold','purple','gold','gold']) plt.title('IPL TEAM SCORE GRAPH') plt.xlabel('TEAMS') plt.ylabel('SCORE')",1,"[""team=['CSK','KKR','DC','MI']\n"", 'score=[149,218,188,143]\n', ""plt.bar(team,score,color=['gold','purple','gold','gold'])\n"", ""plt.title('IPL TEAM SCORE GRAPH')\n"", ""plt.xlabel('TEAMS')\n"", ""plt.ylabel('SCORE')""]"
SETUP CHECKPOINT ASSIGN = np.array(img) path print(ASSIGN.shape),0,"['import numpy as np\n', 'test_x = np.array(img) / 255.0\n', 'print(test_x.shape)']"
"SETUP ASSIGN = random.randint(2, 20) ASSIGN = random_category() ASSIGN = sample_images_of_category(n, category) ASSIGN = n path+ min(1, n % 8) ASSIGN = min(n, 8) ASSIGN = rows * 2 ASSIGN = cols * 2 ASSIGN = plt.subplots(rows, cols, figsize=(width, height)) draw_images_on_subplots(ASSIGN, ASSIGN)",1,"['import random\n', 'from matplotlib import pyplot as plt\n', 'from learntools.python.quickdraw import random_category, sample_images_of_category, draw_images_on_subplots\n', '\n', '## Step 1: Sample some sketches\n', '# How many sketches to view - a random number from 2 to 20\n', 'n = random.randint(2, 20)\n', '# Choose a random quickdraw category. (Check out https://quickdraw.withgoogle.com/data for an overview of categories)\n', 'category = random_category()\n', 'imgs = sample_images_of_category(n, category)\n', '\n', '## Step 2: Choose the grid properties\n', '######## Your changes should go here ###############\n', 'rows = n // 8 + min(1, n % 8)\n', 'cols = min(n, 8)\n', 'height = rows * 2\n', 'width = cols * 2\n', '\n', '## Step 3: Create the grid\n', 'grid = plt.subplots(rows, cols, figsize=(width, height))\n', '\n', '## Step 4: Draw the sketches in the grid\n', 'draw_images_on_subplots(imgs, grid)']"
df.notnull().sum(),0,['df.notnull().sum()']
ASSIGN = 13 ASSIGN = 0.9 ASSIGN = int(len(trend_df)*training_percentage) ASSIGN = len(trend_df)-int(len(trend_df)*training_percentage) ASSIGN = trend_df[:training_item_count-2] ASSIGN = trend_df[training_item_count+2:],0,"['sequence_length = 13\n', 'training_percentage = 0.9\n', ""# The purpose of '-2'and'+2' is to make the number of samples in the training test set divisible by batchsize\n"", 'training_item_count = int(len(trend_df)*training_percentage)\n', 'validation_item_count = len(trend_df)-int(len(trend_df)*training_percentage)\n', 'training_df = trend_df[:training_item_count-2]\n', 'validation_df = trend_df[training_item_count+2:]']"
"'''train_12=pd.DataFrame(np.array(train_12)) ASSIGN=pd.DataFrame(np.array(ASSIGN)) ASSIGN=pd.DataFrame(np.array(ASSIGN)) ASSIGN=pd.DataFrame(np.array(ASSIGN)) ASSIGN=pd.DataFrame(np.array(ASSIGN)) ASSIGN=pd.DataFrame(np.array(ASSIGN)) ASSIGN=pd.DataFrame(np.array(ASSIGN)) ASSIGN=pd.concat([train_12,train_1,train_2,train_3,train_4,train_5,train_6],axis=0) ASSIGN.columns=columns_list del train_1 del train_2 del train_12 del train_3 del train_4 del train_5 del ASSIGN'''",0,"[""'''train_12=pd.DataFrame(np.array(train_12))\n"", 'train_1=pd.DataFrame(np.array(train_1))\n', 'train_2=pd.DataFrame(np.array(train_2))\n', 'train_3=pd.DataFrame(np.array(train_3))\n', 'train_4=pd.DataFrame(np.array(train_4))\n', 'train_5=pd.DataFrame(np.array(train_5))\n', 'train_6=pd.DataFrame(np.array(train_6))\n', 'train=pd.concat([train_12,train_1,train_2,train_3,train_4,train_5,train_6],axis=0)\n', 'train.columns=columns_list\n', 'del train_1\n', 'del train_2\n', 'del train_12\n', 'del train_3\n', 'del train_4\n', 'del train_5\n', ""del train_6'''""]"
data['target'].unique(),0,"[""data['target'].unique()""]"
ASSIGN = pd.read_csv('Country.csv') ASSIGN = pd.read_csv('League.csv') ASSIGN = pd.read_csv('Match.csv') ASSIGN = pd.read_csv('Player.csv') ASSIGN = pd.read_csv('Player_Attributes.csv') ASSIGN = pd.read_csv('Team.csv') ASSIGN = pd.read_csv('Team_Attributes.csv'),0,"['#Data comes in a sqlite format, and can be exported in csv format using DB browser (https://sqlitebrowser.org/)\n', '\n', '#reading data csv files\n', ""country_df = pd.read_csv('Country.csv')\n"", ""league_df = pd.read_csv('League.csv')\n"", ""match_df = pd.read_csv('Match.csv')\n"", ""player_df = pd.read_csv('Player.csv')\n"", ""player_attr_df = pd.read_csv('Player_Attributes.csv')\n"", ""team_df = pd.read_csv('Team.csv')\n"", ""team_attr_df = pd.read_csv('Team_Attributes.csv')""]"
league_df.info(),0,['league_df.info()']
CHECKPOINT ASSIGN=model.predict_classes(X_test) print(ASSIGN[0:10]),0,"['prediction=model.predict_classes(X_test)\n', 'print(prediction[0:10])']"
"ASSIGN=train['isFraud'] ASSIGN=ASSIGN.drop('isFraud',axis=1) ASSIGN=ASSIGN.drop('month',axis=1) ASSIGN=ASSIGN.drop('month',axis=1) ASSIGN = test.copy() del ASSIGN; gc.collect()",0,"[""y_train=train['isFraud']\n"", ""train=train.drop('isFraud',axis=1)\n"", ""train=train.drop('month',axis=1)\n"", ""test=test.drop('month',axis=1)\n"", 'X_test = test.copy()\n', 'del test; gc.collect()']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
"ASSIGN = test[['user_id']].copy() ASSIGN = predictions ASSIGN = ASSIGN.apply(lambda x: x if x >=0 else 0.0005) ASSIGN.to_csv('submission.csv', index=False)",0,"[""sub = test[['user_id']].copy()\n"", ""sub['Attrition'] = predictions\n"", ""sub['Attrition'] = sub['Attrition'].apply(lambda x: x if x >=0 else 0.0005)\n"", ""sub.to_csv('submission.csv', index=False)""]"
"X_train, X_test, y_train, y_test = train_test_split(df[cont], df['Attrition'], test_size=0.2, random_state=42) ASSIGN = pd.concat([X_train, y_train], 1) ASSIGN = pd.concat([X_test, y_test], 1)",0,"[""X_train, X_test, y_train, y_test = train_test_split(df[cont], df['Attrition'], test_size=0.2, random_state=42)\n"", 'train = pd.concat([X_train, y_train], 1)\n', 'test = pd.concat([X_test, y_test], 1)']"
"data.loc[:3,'SepalLengthCm']",0,"[""data.loc[:3,'SepalLengthCm']""]"
SETUP,0,"['import torch\n', 'import torch.nn as nn\n', 'import torch.nn.functional as F\n', 'import torch.optim as optim\n', 'from torch.utils.data import Dataset, DataLoader\n', 'from torchvision import transforms\n', 'from torchvision.utils import make_grid\n', 'import torchvision.utils as vutils\n', 'import matplotlib.animation as animation\n', 'from IPython.display import HTML\n', '\n', 'import matplotlib.pyplot as plt\n', 'from PIL import Image\n', 'import numpy as np\n', 'import pandas as pd\n', 'import copy\n', 'import time\n', 'import cv2 as cv\n', 'from tqdm import tqdm_notebook as tqdm\n', 'import matplotlib.image as mpimg\n', '\n', 'import torchvision.transforms.functional as TF']"
"ASSIGN=pd.DataFrame({'Id':filename_test,'Predicted':prediction}) ASSIGN.to_csv('submission.csv',index=False)",0,"[""submit=pd.DataFrame({'Id':filename_test,'Predicted':prediction})\n"", ""submit.to_csv('submission.csv',index=False)""]"
"SETUP X_train,X_test,y_train,y_test=train_test_split(train_transaction_new,train_transaction_new_label,test_size=0.2) del train_transaction_new ASSIGN = LogisticRegression(C=0.09,solver='lbfgs') ASSIGN.fit(X_train, y_train) ASSIGN = lr.predict_proba(X_test)[:, 1] ASSIGN=pd.DataFrame({'pred':proba_test,'real':y_test}) ASSIGN['pred_0_1']=ASSIGN.pred.apply(lambda x:1 if x>=0.5 else 0)",0,"['from sklearn.linear_model import LogisticRegression  \n', 'from sklearn.preprocessing import StandardScaler  \n', 'from sklearn.model_selection import train_test_split\n', 'X_train,X_test,y_train,y_test=train_test_split(train_transaction_new,train_transaction_new_label,test_size=0.2)\n', 'del train_transaction_new\n', ""lr = LogisticRegression(C=0.09,solver='lbfgs')  \n"", 'lr.fit(X_train, y_train)  \n', 'proba_test = lr.predict_proba(X_test)[:, 1]\n', ""LR_result=pd.DataFrame({'pred':proba_test,'real':y_test})\n"", ""LR_result['pred_0_1']=LR_result.pred.apply(lambda x:1 if x>=0.5 else 0)""]"
train.head(),0,['train.head()']
final.Cabin.value_counts(),0,['final.Cabin.value_counts()']
"SVM_all.fit(x_train,x_test) SVM_all.score(y_train,y_test)",0,"['SVM_all.fit(x_train,x_test)\n', 'SVM_all.score(y_train,y_test)']"
SETUP,0,"['import pandas as pd\n', 'import os\n', 'from shutil import copyfile\n', 'import matplotlib.pyplot as plt\n', 'from matplotlib.image import imread\n', 'import time\n', '\n', 'from tensorflow.keras.utils import to_categorical\n', 'from tensorflow.keras.models import Sequential\n', 'from tensorflow.keras.layers import Dense,Dropout,Activation\n', '\n', 'from sklearn.preprocessing import LabelEncoder\n', 'from sklearn.preprocessing import StandardScaler\n', 'from sklearn.model_selection import StratifiedShuffleSplit']"
ASSIGN = pca_last.transform(X_train) ASSIGN = pca_last.transform(X_test),0,"['X_train_pca = pca_last.transform(X_train)\n', 'X_test_pca = pca_last.transform(X_test)']"
"ASSIGN = train.copy() ASSIGN = test.copy() ASSIGN = set(X.Province) ASSIGN = set(X.Country) ASSIGN = pd.concat([ASSIGN,pd.get_dummies(ASSIGN.Country)],axis=1) ASSIGN = pd.concat([ASSIGN,pd.get_dummies(ASSIGN.Country)],axis=1) ASSIGN = pd.concat([ASSIGN,pd.get_dummies(ASSIGN.Province)[Province_set - Country_set]],axis=1) ASSIGN = pd.concat([ASSIGN,pd.get_dummies(ASSIGN.Province)[Province_set - Country_set]],axis=1) ASSIGN = X.ConfirmedCases ASSIGN = X.Fatalities",0,"['X = train.copy()\n', 'X_test = test.copy()\n', '\n', 'Province_set = set(X.Province)\n', 'Country_set = set(X.Country)\n', '\n', 'X = pd.concat([X,pd.get_dummies(X.Country)],axis=1)\n', 'X_test = pd.concat([X_test,pd.get_dummies(X_test.Country)],axis=1)\n', 'X = pd.concat([X,pd.get_dummies(X.Province)[Province_set - Country_set]],axis=1)\n', 'X_test = pd.concat([X_test,pd.get_dummies(X_test.Province)[Province_set - Country_set]],axis=1)\n', '\n', 'y_confirm = X.ConfirmedCases\n', 'y_fata = X.Fatalities\n']"
player_attr_df.info(),0,['player_attr_df.info()']
cbsr.isnull().sum(),0,['cbsr.isnull().sum()']
"ASSIGN = xgb.XGBRegressor() ASSIGN.fit(X,y_confirm)",0,"['reg_confirm = xgb.XGBRegressor()\n', 'reg_confirm.fit(X,y_confirm)']"
"ASSIGN = image_ids[np.random.choice(image_ids.shape[0], NUM_IMG_SAMPLES, replace=False)] show_samples(ASSIGN)",0,"['# Randomly select samples\n', 'samples = image_ids[np.random.choice(image_ids.shape[0], NUM_IMG_SAMPLES, replace=False)]\n', '\n', '# Show images and corresponding masks of too-far-away (not of interest) cars\n', 'show_samples(samples)']"
CHECKPOINT print(os.listdir('..path')),0,"[""print(os.listdir('../working'))""]"
"CHECKPOINT ASSIGN = data.path ASSIGN = data.target_label X_train, X_test_sub ,y_train,y_test_sub= train_test_split(ASSIGN,ASSIGN, test_size=0.3, random_state=0,shuffle = True) print(X_train.shape) print(X_test_sub.shape)",0,"['X = data.path\n', 'y = data.target_label\n', 'X_train, X_test_sub ,y_train,y_test_sub= train_test_split(X,y, test_size=0.3, random_state=0,shuffle = True)\n', 'print(X_train.shape)\n', 'print(X_test_sub.shape)']"
total['Total Atrocities'] = total['Murder'] +total['Assault on women']+total['Kidnapping and Abduction']+total['Dacoity']+total['Robbery']+total['Arson']+total['Hurt']+total['Prevention of atrocities (POA) Act']+total['Protection of Civil Rights (PCR) Act']+total['Other Crimes Against SCs'] total.head(15),0,"[""total['Total Atrocities'] = total['Murder'] +total['Assault on women']+total['Kidnapping and Abduction']+total['Dacoity']+total['Robbery']+total['Arson']+total['Hurt']+total['Prevention of atrocities (POA) Act']+total['Protection of Civil Rights (PCR) Act']+total['Other Crimes Against SCs']\n"", 'total.head(15)']"
"plt.figure(figsize=(8,6)) plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='rainbow') plt.xlabel('First principal component') plt.ylabel('Second Principal Component')",1,"['#Using scatter plot show where the Principal components lie on the graph. \n', 'plt.figure(figsize=(8,6))\n', ""plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='rainbow')\n"", ""plt.xlabel('First principal component')\n"", ""plt.ylabel('Second Principal Component')""]"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") print() print(ASSIGN.keys()) print() print(ASSIGN.shape)",0,"['#1 Write a python program using Scikit_learn to print the keys, number of rows-columns , feature names and the description of the iris data.\n', 'import pandas as pd\n', 'iris_data = pd.read_csv(""../input/iris-dataset/iris.data.csv"")\n', 'print(""\\nKeys of Iris dataset:"")\n', 'print(iris_data.keys())\n', 'print(""\\nNumber of rows and columns of Iris dataset:"")\n', 'print(iris_data.shape)']"
data_tree_for_test[data_tree_for_test.MSZoning.isnull()==True],0,['data_tree_for_test[data_tree_for_test.MSZoning.isnull()==True]']
"ASSIGN=[] ASSIGN=[] ASSIGN=[] for i in range(len(tokyo_lines)): ASSIGN=tokyo_lines.iloc[i].geometry.split('(')[1].split(')')[0].split(',') for j in range(len(ASSIGN)): ASSIGN.append(ASSIGN[j].split(' ')[0]) ASSIGN.append(ASSIGN[j].split(' ')[1]) ASSIGN.append(tokyo_lines.url_name[i]) ASSIGN=pd.DataFrame({'x':x,'y':y,'z':z}) SLICE=SLICE.astype(float) SLICE=SLICE.astype(float) plt.figure(figsize=(25, 25)) plt.subplot(2, 2, 1) ASSIGN=sns.scatterplot(x=""x"", y=""y"", hue=""z"",data=fix) plt.legend(loc=0, bbox_to_anchor=(1.05,0.6)) plt.title(""lines for Tokyo"",size=20) ASSIGN.get_legend().remove() ASSIGN.set(xlabel='Longitude', ylabel='LATITUDE') plt.subplot(2,2,2) (tokyo_lines.groupby(['url_name'])['length'].sum()path).sort_values(ascending= False)[:10].sort_values().plot.barh() plt.ylabel(' ') plt.xlabel('length(km)') plt.title(""Top 10 track by length"",size=20) plt.subplot(2,2,3) tokyo_stations.groupby(['opening'])['id'].agg('count').plot() plt.xlabel(' ') plt.ylabel('stations') plt.title(""Number of opening stations by year"",size=20) plt.subplot(2,2,4) tokyo_lines.name.value_counts()[:10].sort_values().plot.barh() plt.xlabel('counts') plt.title(""Top 10 line by number"",size=20)",1,"['x=[]\n', 'y=[]\n', 'z=[]\n', 'for i in range(len(tokyo_lines)):\n', ""    sp=tokyo_lines.iloc[i].geometry.split('(')[1].split(')')[0].split(',')\n"", '    for j in range(len(sp)):\n', ""        x.append(sp[j].split(' ')[0])\n"", ""        y.append(sp[j].split(' ')[1])\n"", '        z.append(tokyo_lines.url_name[i])\n', ""fix=pd.DataFrame({'x':x,'y':y,'z':z})\n"", ""fix['x']=fix['x'].astype(float)\n"", ""fix['y']=fix['y'].astype(float)\n"", 'plt.figure(figsize=(25, 25))\n', 'plt.subplot(2, 2, 1) \n', 'ax=sns.scatterplot(x=""x"", y=""y"", hue=""z"",data=fix)\n', 'plt.legend(loc=0, bbox_to_anchor=(1.05,0.6))\n', 'plt.title(""lines for Tokyo"",size=20)\n', 'ax.get_legend().remove()\n', ""ax.set(xlabel='Longitude', ylabel='LATITUDE')\n"", 'plt.subplot(2,2,2)\n', ""(tokyo_lines.groupby(['url_name'])['length'].sum()/1000).sort_values(ascending= False)[:10].sort_values().plot.barh()\n"", ""plt.ylabel(' ')\n"", ""plt.xlabel('length(km)')\n"", 'plt.title(""Top 10 track by length"",size=20)\n', 'plt.subplot(2,2,3)\n', ""tokyo_stations.groupby(['opening'])['id'].agg('count').plot()\n"", ""plt.xlabel(' ')\n"", ""plt.ylabel('stations')\n"", 'plt.title(""Number of opening stations by year"",size=20)\n', 'plt.subplot(2,2,4)\n', 'tokyo_lines.name.value_counts()[:10].sort_values().plot.barh()\n', ""plt.xlabel('counts')\n"", 'plt.title(""Top 10 line by number"",size=20)']"
"CHECKPOINT ASSIGN = LogisticRegression() ASSIGN = ASSIGN.fit(train_x, train_y) ASSIGN = logreg.predict(test_x) ASSIGN = logreg.score(train_x, train_y) print('training accuracy: %.5f' % ASSIGN) ASSIGN =logreg.predict(test_x) X=accuracy_score(test_y, ASSIGN) print('test accuracy: %.5f' % X) ASSIGN = metrics.roc_curve(test_y, predict_y, pos_label=1) print('auc: %.5f' % metrics.auc(fpr, tpr)) ASSIGN = cross_val_score(logreg,train_x,train_y,cv=5,scoring='accuracy') print('average of Cross validation: %.5f'%ASSIGN.mean()) print('standard deviation of Cross validation: %.5f'%ASSIGN.std(ddof=1))",0,"['# logistic regression\n', '\n', 'logreg = LogisticRegression()\n', 'logreg = logreg.fit(train_x, train_y)\n', 'predict_y = logreg.predict(test_x)\n', 'acc_log = logreg.score(train_x, train_y)\n', ""print('training accuracy: %.5f' % acc_log)\n"", '\n', 'predict_y =logreg.predict(test_x)\n', 'X=accuracy_score(test_y, predict_y)\n', ""print('test accuracy: %.5f' % X)\n"", '\n', '#auc\n', 'fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n', ""print('auc: %.5f' % metrics.auc(fpr, tpr))\n"", '\n', '#Cross validation\n', ""scores = cross_val_score(logreg,train_x,train_y,cv=5,scoring='accuracy')\n"", '# 計算Cross validation的平均值與標準差\n', ""print('average of Cross validation: %.5f'%scores.mean())\n"", ""print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))""]"
"SETUP CHECKPOINT print(check_output([, ]).decode())                #list the input folder files",0,"['from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n', 'from keras.models import Sequential                                   # model\n', 'from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D   # layers\n', 'from keras.optimizers import RMSprop                                  # optimizer\n', 'from keras.preprocessing.image import ImageDataGenerator              \n', 'from keras.callbacks import ReduceLROnPlateau                         # callback function\n', '\n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline\n', '\n', 'from subprocess import check_output\n', 'print(check_output([""ls"", ""../input""]).decode(""utf8""))                #list the input folder files']"
"ASSIGN=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[2000:4000] ASSIGN=34.78 ASSIGN=32.05 ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN['label']=ASSIGN.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1) ASSIGN=plugins.MarkerCluster().add_to(data_total_cars_2_map) for lat,lon,label in zip(ASSIGN.latitude,ASSIGN.longitude,ASSIGN.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN)",1,"[""data_total_cars_2=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[2000:4000]\n"", 'Long=34.78\n', 'Lat=32.05\n', 'data_total_cars_2_map=folium.Map([Lat,Long],zoom_start=12)\n', ""data_total_cars_2['label']=data_total_cars_2.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n"", '\n', 'data_total_cars_2_cars_map=plugins.MarkerCluster().add_to(data_total_cars_2_map)\n', 'for lat,lon,label in zip(data_total_cars_2.latitude,data_total_cars_2.longitude,data_total_cars_2.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_2_cars_map)\n', 'data_total_cars_2_map.add_child(data_total_cars_2_cars_map)\n', '\n']"
"len(train), len(test)",0,"['len(train), len(test)']"
"ASSIGN=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[4000:6000] ASSIGN=34.78 ASSIGN=32.05 ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN['label']=ASSIGN.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1) ASSIGN=plugins.MarkerCluster().add_to(data_total_cars_3_map) for lat,lon,label in zip(ASSIGN.latitude,ASSIGN.longitude,ASSIGN.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN)",1,"[""data_total_cars_3=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[4000:6000]\n"", 'Long=34.78\n', 'Lat=32.05\n', 'data_total_cars_3_map=folium.Map([Lat,Long],zoom_start=12)\n', ""data_total_cars_3['label']=data_total_cars_3.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n"", '\n', 'data_total_cars_3_cars_map=plugins.MarkerCluster().add_to(data_total_cars_3_map)\n', 'for lat,lon,label in zip(data_total_cars_3.latitude,data_total_cars_3.longitude,data_total_cars_3.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_3_cars_map)\n', 'data_total_cars_3_map.add_child(data_total_cars_3_cars_map)\n', '\n']"
"plot_importance(importance_df, figsize=(12, 20))",0,"['plot_importance(importance_df, figsize=(12, 20))']"
SETUP,0,"['from torch.distributions import Categorical\n', 'from torch.autograd import Variable']"
"for i in range(len(test)): if test.Date[i]<datetime.strptime('2020-04-09','%Y-%m-%d'): test.drop(i,inplace=True)",0,"['for i in range(len(test)):\n', ""    if test.Date[i]<datetime.strptime('2020-04-09','%Y-%m-%d'):\n"", '        test.drop(i,inplace=True)']"
"SETUP ASSIGN = plot_confusion_matrix(rfc, X, y, ASSIGN=class_names, ASSIGN=plt.cm.Blues, ASSIGN=None) plt.show()",1,"['import matplotlib.pyplot as plt\n', '\n', 'disp = plot_confusion_matrix(rfc, X, y,\n', '                         display_labels=class_names,\n', '                         cmap=plt.cm.Blues,\n', '                         normalize=None)\n', 'plt.show()']"
"ASSIGN = kickstarters_2017.usd_goal_real ASSIGN = minmax_scaling(usd_goal, columns = [0]) ASSIGN=plt.subplots(1,2) sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Scaled data"")",1,"['# select the usd_goal_real column\n', 'usd_goal = kickstarters_2017.usd_goal_real\n', '\n', '# scale the goals from 0 to 1\n', 'scaled_data = minmax_scaling(usd_goal, columns = [0])\n', '\n', '# plot the original & scaled data together to compare\n', 'fig, ax=plt.subplots(1,2)\n', 'sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0])\n', 'ax[0].set_title(""Original Data"")\n', 'sns.distplot(scaled_data, ax=ax[1])\n', 'ax[1].set_title(""Scaled data"")']"
"data.show_batch(rows=3, figsize=(12,8))",0,"['data.show_batch(rows=3, figsize=(12,8))']"
"SETUP CHECKPOINT print(check_output([, ]).decode())",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns \n', '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n', '\n', 'from subprocess import check_output\n', 'print(check_output([""ls"", ""../input""]).decode(""utf8""))\n', '\n', '# Any results you write to the current directory are saved as output.']"
"df_train['outliers'] = 0 df_train.loc[df_train['target'] < -30, 'outliers'] = 1 df_train['outliers'].value_counts()",0,"[""df_train['outliers'] = 0\n"", ""df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n"", ""df_train['outliers'].value_counts()""]"
learn.save('stage-2'),0,"[""learn.save('stage-2')""]"
SETUP,0,"['%matplotlib inline\n', 'from copy import deepcopy\n', 'from collections import OrderedDict\n', 'import gc\n', 'import matplotlib.pyplot as plt\n', 'from tqdm import tqdm_notebook\n', 'import numpy as np\n', 'import pandas as pd\n', 'import torch\n', 'import torch.nn as nn\n', 'import torch.nn.functional as F\n', 'from torch.optim import SGD,Adam,lr_scheduler\n', 'from torch.utils.data import random_split\n', 'import torchvision\n', 'from torchvision import transforms,models\n']"
"final['Embarked'].fillna(""S"",inplace = True)",0,"['final[\'Embarked\'].fillna(""S"",inplace = True)']"
SETUP,0,"['import numpy as np\n', 'import pandas as pd\n', 'import os\n', 'import matplotlib.pyplot as plt\n', 'from glob import glob as gb']"
"ASSIGN = cnn_learner(db, models.resnet34, metrics=error_rate) ASSIGN.load('stage-2'); ASSIGN = DatasetFormatter().from_similars(learn_cln) len(idxs)",0,"['learn_cln = cnn_learner(db, models.resnet34, metrics=error_rate)\n', ""learn_cln.load('stage-2');\n"", 'ds, idxs = DatasetFormatter().from_similars(learn_cln)\n', 'len(idxs)']"
"CHECKPOINT ASSIGN = ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2'] ASSIGN = ['GarageType','GarageCond','GarageFinish','GarageQual'] ASSIGN = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu'] ASSIGN = data_features.isnull().sum().sort_values(ascending = False) ASSIGN = ASSIGN[ASSIGN > 0] missing_data",0,"[""bsmt_var = ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']\n"", ""garage_var = ['GarageType','GarageCond','GarageFinish','GarageQual']\n"", ""NONE_var = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu']\n"", 'missing_data = data_features.isnull().sum().sort_values(ascending = False)\n', 'missing_data = missing_data[missing_data > 0]\n', 'missing_data']"
dataset.info(),0,['dataset.info()']
"plt.figure(figsize=(25,10)) sns.heatmap(df.corr(), annot=True, linewidth=0.5, cmap='coolwarm')",1,"['plt.figure(figsize=(25,10))\n', ""sns.heatmap(df.corr(), annot=True, linewidth=0.5, cmap='coolwarm')""]"
SETUP,0,['!pip install ../input/efficientnet-keras-source-code/repository/qubvel-efficientnet-c993591']
"SETUP CHECKPOINT ASSIGN=np.array([9,5,4,3,2,6]) ASSIGN=np.array([5,8,6,9,2,1]) print() print(ASSIGN.base is ASSIGN) print() print(ASSIGN.base is ASSIGN) ASSIGN=a%3==0 ASSIGN=b%3==0 print() print(ASSIGN[ASSIGN]) print(ASSIGN[ASSIGN]) ASSIGN[::-1].sort() print() print(ASSIGN) print() print(np.sum(ASSIGN))",0,"['#1\n', 'import numpy as np\n', 'a=np.array([9,5,4,3,2,6])\n', 'b=np.array([5,8,6,9,2,1])\n', 'print(""CHECK IF B HAS SAME VIEWS TO MEMORY IN A"")\n', 'print(b.base is a)\n', 'print(""CHECK IF A HAS SAME VIEWS TO MEMORY IN B"")\n', 'print(a.base is b)\n', 'div_by_3=a%3==0\n', 'div1_by_3=b%3==0\n', 'print(""Divisible By 3"")\n', 'print(a[div_by_3])\n', 'print(b[div1_by_3])\n', 'b[::-1].sort()\n', 'print(""SECOND ARRAY SORTED"")\n', 'print(b)\n', 'print(""SUM OF ELEMENTS OF FIRST ARRAY"")\n', 'print(np.sum(a))']"
def init_weights(m): if type(m) == nn.Linear: torch.nn.init.xavier_uniform_(m.weight) m.bias.data.fill_(0.01) try: torch.nn.init.xavier_uniform(m.weight) except: _ ASSIGN = netG.apply(init_weights),0,"['def init_weights(m):\n', '    if type(m) == nn.Linear:\n', '        torch.nn.init.xavier_uniform_(m.weight)\n', '        m.bias.data.fill_(0.01)\n', '    try:\n', '        torch.nn.init.xavier_uniform(m.weight)\n', '    except:\n', '        _\n', '#         print(m)\n', '_ = netG.apply(init_weights)']"
"CHECKPOINT def input_and_run2(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10): ASSIGN = ImageDataGenerator(width_shift_range=0.2,height_shift_range=0.2,horizontal_flip=True) ASSIGN.fit(X_train) ASSIGN = keras.optimizers.Adam(learning_rate=alpha) ASSIGN=SGD(lr=alpha, momentum=0.9) model.compile(loss='binary_crossentropy', optimizer=ASSIGN,metrics=['accuracy']) model.fit(ASSIGN.flow(X_train, y_train),validation_data=(X_val, y_val), epochs=num_epochs) ASSIGN = model.evaluate(X_train,y_train) print(+str(ASSIGN[1]*100)) ASSIGN = model.evaluate(X_val,y_val) print(+str(ASSIGN[1]*100)) ASSIGN = model.evaluate(X_test,y_test) print(+str(ASSIGN[1]*100))",0,"['\n', '\n', 'def input_and_run2(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10):\n', '    \n', '    datagen = ImageDataGenerator(width_shift_range=0.2,height_shift_range=0.2,horizontal_flip=True)\n', '    datagen.fit(X_train)\n', '    \n', '    #compile model using accuracy to measure model performance\n', '    opt = keras.optimizers.Adam(learning_rate=alpha)\n', '    opt2=SGD(lr=alpha, momentum=0.9)\n', ""    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n"", '    \n', '    #train the model\n', '    model.fit(datagen.flow(X_train, y_train),validation_data=(X_val, y_val), epochs=num_epochs)\n', '    \n', '    #Getting results\n', '    result = model.evaluate(X_train,y_train)\n', '    #print(result)\n', '    print(""Training accuracy = ""+str(result[1]*100))\n', '    result = model.evaluate(X_val,y_val)\n', '    #print(result)\n', '    print(""Validation accuracy = ""+str(result[1]*100))\n', '    result = model.evaluate(X_test,y_test)\n', '    #print(result)\n', '    print(""Test accuracy = ""+str(result[1]*100))\n', '\n', '\n']"
"SETUP ASSIGN = ResNet50(include_top=False, weights='imagenet', input_shape=(384,384,3)) ASSIGN = model_finetuned.output ASSIGN = GlobalAveragePooling2D()(ASSIGN) ASSIGN = Dense(128, activation=""relu"")(ASSIGN) ASSIGN = Dense(64, activation=""relu"")(ASSIGN) ASSIGN = Dense(4, activation=""softmax"")(x) ASSIGN = Model(inputs=ASSIGN.input, outputs=predictions) ASSIGN.compile(optimizer='adam', ASSIGN = 'categorical_crossentropy', ASSIGN=['accuracy']) ASSIGN.summary()",0,"['from keras.applications.resnet50 import ResNet50\n', 'from keras.models import Model\n', 'import keras\n', 'from keras import optimizers\n', ""model_finetuned = ResNet50(include_top=False, weights='imagenet', input_shape=(384,384,3))\n"", 'x = model_finetuned.output\n', 'x = GlobalAveragePooling2D()(x)\n', 'x = Dense(128, activation=""relu"")(x)\n', 'x = Dense(64, activation=""relu"")(x)\n', 'predictions = Dense(4, activation=""softmax"")(x)\n', 'model_finetuned = Model(inputs=model_finetuned.input, outputs=predictions)\n', ""model_finetuned.compile(optimizer='adam',\n"", ""                  loss = 'categorical_crossentropy',\n"", ""                  metrics=['accuracy'])\n"", 'model_finetuned.summary()']"
"CHECKPOINT ASSIGN = model19.predict(x_es) ASSIGN = mean_squared_error(Y_es, y_pred19, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, ASSIGN)))",0,"['y_pred19 = model19.predict(x_es)\n', '\n', 'val = mean_squared_error(Y_es, y_pred19, squared=False)\n', 'val19 = str(round(val, 4))\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred19)))\n""]"
SETUP ASSIGN = PCA(n_components = 2) ASSIGN = pca.fit_transform(ASSIGN) ASSIGN = pca.transform(ASSIGN),0,"['from sklearn.decomposition import PCA\n', 'pca = PCA(n_components = 2)\n', '\n', 'X_train = pca.fit_transform(X_train)\n', 'X_test = pca.transform(X_test)']"
"submission.to_csv(""submission.csv"",index=False)",0,"['submission.to_csv(""submission.csv"",index=False)']"
"SETUP np.random.seed(921) X_train,X_test,y_train,y_test=train_test_split(img,label,test_size=0.2) del img ASSIGN=ASSIGN.astype(int) ASSIGN=ASSIGN.astype(int) ASSIGN=np.array(ASSIGN).reshape(-1,1) ASSIGN=np.array(ASSIGN).reshape(-1,1) ASSIGN=ASSIGN.reshape(-1,32,32,3)path ASSIGN=ASSIGN.reshape(-1,32,32,3)path ASSIGN=np_utils.to_categorical(ASSIGN,num_classes=max(label)+1) ASSIGN=np_utils.to_categorical(ASSIGN,num_classes=max(label)+1)",0,"['np.random.seed(921)\n', 'from keras.utils import np_utils\n', 'from keras.models import Sequential\n', 'from keras.layers import Convolution2D,Dense,MaxPool2D,Activation,Dropout,Flatten\n', 'from keras.optimizers import Adam\n', 'from sklearn.model_selection import train_test_split\n', 'from keras.layers.normalization import BatchNormalization\n', 'X_train,X_test,y_train,y_test=train_test_split(img,label,test_size=0.2)\n', 'del img\n', 'y_train=y_train.astype(int)\n', 'y_test=y_test.astype(int)\n', 'y_train=np.array(y_train).reshape(-1,1)\n', 'y_test=np.array(y_test).reshape(-1,1)\n', 'X_train=X_train.reshape(-1,32,32,3)/255 #Normalize\n', 'X_test=X_test.reshape(-1,32,32,3)/255\n', 'y_train=np_utils.to_categorical(y_train,num_classes=max(label)+1)\n', 'y_test=np_utils.to_categorical(y_test,num_classes=max(label)+1)']"
ASSIGN = scaler.transform(df),0,['scaled_data = scaler.transform(df)']
predict_out.head(),0,['predict_out.head()']
"population.Country[population.Country == 'United States'] = 'US' population.Country[population.Country == 'Taiwan'] = 'Taiwan*' population.Country[population.Country == 'South Korea'] = 'Korea, South' population.Country[population.Country == 'Côte d\'Ivoire'] = 'Cote d\'Ivoire' population.Country[population.Country == 'Czech Republic (Czechia)'] = 'Czechia' population.Country[population.Country == 'Myanmar'] = 'Burma' population.Country[population.Country == 'St. Vincent & Grenadines'] = 'Saint Vincent and the Grenadines' population.Country[population.Country == 'Saint Kitts & Nevis']  = 'Saint Kitts and Nevis' population.Country[population.Country == 'Sao Tome & Principe']  = 'Sao Tome and Principe'",0,"[""population.Country[population.Country == 'United States'] = 'US'\n"", ""population.Country[population.Country == 'Taiwan'] = 'Taiwan*'\n"", ""population.Country[population.Country == 'South Korea'] = 'Korea, South'\n"", ""population.Country[population.Country == 'Côte d\\'Ivoire'] = 'Cote d\\'Ivoire'\n"", ""population.Country[population.Country == 'Czech Republic (Czechia)'] = 'Czechia'\n"", ""population.Country[population.Country == 'Myanmar'] = 'Burma'\n"", ""population.Country[population.Country == 'St. Vincent & Grenadines'] = 'Saint Vincent and the Grenadines'\n"", ""population.Country[population.Country == 'Saint Kitts & Nevis']  = 'Saint Kitts and Nevis'\n"", ""population.Country[population.Country == 'Sao Tome & Principe']  = 'Sao Tome and Principe'""]"
ASSIGN = pd.DataFrame() ASSIGN['Id'] = test_df['Id'] ASSIGN['sentiment'] = [int(i) for i in final_pred],0,"['sub_df = pd.DataFrame()\n', ""sub_df['Id'] = test_df['Id']\n"", ""sub_df['sentiment'] = [int(i) for i in final_pred]""]"
"CHECKPOINT ASSIGN = df.drop(['Chance of Admit '], axis = 1) X",0,"[""X = df.drop(['Chance of Admit '], axis = 1)\n"", 'X']"
"sns.catplot(x = 'Parch',y='Survived',hue = 'Pclass',kind = 'violin', data = df3, palette = 'cubehelix', col = 'Pclass')",1,"[""sns.catplot(x = 'Parch',y='Survived',hue = 'Pclass',kind = 'violin', data = df3, palette = 'cubehelix', col = 'Pclass')""]"
"plt.figure(figsize=(35,55)) for i in range(len(stations.country.unique())): plt.subplot(9,3,i+1) stations[stations.country==stations.country.unique()[i]].name.value_counts().sort_values().plot.barh() plt.xlabel('stations') plt.title(""The number of stations in ""+stations.country.unique()[i],size=20)",1,"['plt.figure(figsize=(35,55))\n', 'for i in range(len(stations.country.unique())):\n', '    plt.subplot(9,3,i+1)\n', '    stations[stations.country==stations.country.unique()[i]].name.value_counts().sort_values().plot.barh()\n', ""    plt.xlabel('stations')\n"", '    plt.title(""The number of stations in ""+stations.country.unique()[i],size=20)\n']"
"def get_data_loaders(train_data_path, val_data_path): ASSIGN = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(256), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) ]) ASSIGN = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform) ASSIGN = data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4) ASSIGN = torchvision.datasets.ImageFolder(root=val_data_path, transform=transform) ASSIGN = data.DataLoader(val_data, batch_size=32, shuffle=True, num_workers=4) ASSIGN = train_data.classes ASSIGN = val_data.classes return train_loader, val_loader, train_class_names, val_class_names",0,"['def get_data_loaders(train_data_path, val_data_path):\n', '    transform = transforms.Compose([\n', '        transforms.Resize(256),\n', '        transforms.CenterCrop(256),\n', '        transforms.ToTensor(),\n', '        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n', '        ])\n', '\n', '    train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n', '    train_loader = data.DataLoader(train_data, batch_size=32, shuffle=True,  num_workers=4)\n', '    val_data = torchvision.datasets.ImageFolder(root=val_data_path, transform=transform)\n', '    val_loader  = data.DataLoader(val_data, batch_size=32, shuffle=True, num_workers=4) \n', '    \n', '    train_class_names = train_data.classes\n', '    val_class_names = val_data.classes\n', '    \n', '    return train_loader, val_loader, train_class_names, val_class_names']"
ASSIGN = nn.NLLLoss() ASSIGN = optim.Adam(model.classifier.parameters()),0,"['criterion = nn.NLLLoss()\n', '# Set the optimizer function using torch.optim as optim library\n', 'optimizer = optim.Adam(model.classifier.parameters())']"
CHECKPOINT X_test,0,['X_test']
CHECKPOINT cat_to_names,0,['cat_to_names']
"CHECKPOINT ASSIGN=data_Queens.loc[(data_Queens['price'] <100)][2000:2800] ASSIGN['label']=ASSIGN.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1) ASSIGN=-73.80 ASSIGN=40.70 ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(data_Queens_100_2_map) for lat,lon,label in zip(ASSIGN.latitude,ASSIGN.longitude,ASSIGN.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN) data_Queens_100_2_map",1,"[""data_Queens_100_2=data_Queens.loc[(data_Queens['price'] <100)][2000:2800]\n"", ""data_Queens_100_2['label']=data_Queens_100_2.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n"", 'Long=-73.80\n', 'Lat=40.70\n', 'data_Queens_100_2_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'data_Queens_100_2_rooms_map=plugins.MarkerCluster().add_to(data_Queens_100_2_map)\n', 'for lat,lon,label in zip(data_Queens_100_2.latitude,data_Queens_100_2.longitude,data_Queens_100_2.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_Queens_100_2_rooms_map)\n', 'data_Queens_100_2_map.add_child(data_Queens_100_2_rooms_map)\n', '\n', 'data_Queens_100_2_map']"
"learn.fit_one_cycle(2,max_lr=1e-6)",0,"['learn.fit_one_cycle(2,max_lr=1e-6)']"
SETUP ASSIGN = train_x.shape[1] ASSIGN = 128,0,"['input_dim = train_x.shape[1]\n', 'EPOCHS = 100\n', 'batch_size = 128']"
data_2019['year']=data_2019.Date.apply(lambda x:x.split('-')[0]) data_2019['month']=data_2019.Date.apply(lambda x:x.split('-')[1]) data_2019['day']=data_2019.Date.apply(lambda x:x.split('-')[2]) data_2019.head(),0,"[""data_2019['year']=data_2019.Date.apply(lambda x:x.split('-')[0])\n"", ""data_2019['month']=data_2019.Date.apply(lambda x:x.split('-')[1])\n"", ""data_2019['day']=data_2019.Date.apply(lambda x:x.split('-')[2])\n"", 'data_2019.head()']"
"''' ASSIGN = ['poly'] ASSIGN = [1,2,3] ASSIGN = ['balanced','dict'] ASSIGN = [.1,1,10,] ASSIGN = ['scale','auto'] ASSIGN = svm.SVC() ASSIGN = {'kernel':['poly'],'class_weight':['balanced','dict'] ,'C': [.1,1,10],'degree':[1,2,3],'gamma':['scale','auto']} ASSIGN = GridSearchCV(s, parameters, scoring='accuracy' ,cv =10) ASSIGN.fit(x_train, x_test) ASSIGN.best_params_ '''",0,"[""'''\n"", ""kernel = ['poly']\n"", 'degree = [1,2,3]\n', ""class_weight = ['balanced','dict']\n"", 'C = [.1,1,10,]\n', ""gamma = ['scale','auto']\n"", '\n', 's = svm.SVC()\n', '\n', ""parameters = {'kernel':['poly'],'class_weight':['balanced','dict'] ,'C': [.1,1,10],'degree':[1,2,3],'gamma':['scale','auto']}\n"", '\n', ""svcc = GridSearchCV(s, parameters, scoring='accuracy' ,cv =10)\n"", 'svcc.fit(x_train, x_test)\n', 'svcc.best_params_\n', ""'''""]"
"ASSIGN=test['hour']<7 ASSIGN=test['hour']>19 ASSIGN.astype(int) ASSIGN.astype(int) ASSIGN=np.add(x,x1) ASSIGN.astype(int) SLICE=ASSIGN.astype(int)",0,"[""x=test['hour']<7\n"", ""x1=test['hour']>19\n"", 'x.astype(int)\n', 'x1.astype(int)\n', 'x2=np.add(x,x1)\n', 'x2.astype(int)\n', ""test['night']=x2.astype(int)""]"
df.head(),0,['df.head()']
ASSIGN = pd.read_csv('..path') ASSIGN = overall_data[overall_data.intent == 'Suicide'] ASSIGN = overall_data[overall_data.intent == 'Homicide'],0,"[""overall_data = pd.read_csv('../input/guns.csv')\n"", ""suicide_data = overall_data[overall_data.intent == 'Suicide']\n"", ""homicide_data = overall_data[overall_data.intent == 'Homicide']""]"
"mnb.fit(x_train,x_test) mnb.score(y_train,y_test)",0,"['mnb.fit(x_train,x_test)\n', 'mnb.score(y_train,y_test)']"
ASSIGN = 'grizzly' ASSIGN = 'grizzly_bear.txt',0,"[""folder = 'grizzly'\n"", ""file = 'grizzly_bear.txt'""]"
"CHECKPOINT ASSIGN = ['black', 'grizzly', 'teddy'] for c in ASSIGN: print(c) verify_images(pathpath, delete=True, max_size=500)",0,"['# 创建类别\n', ""classes = ['black', 'grizzly', 'teddy']\n"", '# 删除不能被打开的错误图像\n', 'for c in classes:\n', '    print(c)\n', '    verify_images(path/c, delete=True, max_size=500)']"
"ASSIGN = range(1,20) ASSIGN = {} ASSIGN = [] for k in ASSIGN: ASSIGN = KNeighborsClassifier(n_neighbors = k) ASSIGN.fit(x_train,x_test) ASSIGN = knn.predict(y_train) ASSIGN = metrics.accuracy_score(ASSIGN,y_pred) ASSIGN.append(metrics.accuracy_score(result['Survived'],ASSIGN)) plt.plot(ASSIGN,ASSIGN) plt.xlabel(""Value of K"") plt.ylabel(""Accuracy"")",1,"['Krange = range(1,20)\n', 'scores = {}\n', 'scores_list = []\n', 'for k in Krange:\n', '    knn = KNeighborsClassifier(n_neighbors = k)\n', '    knn.fit(x_train,x_test)\n', '    y_pred = knn.predict(y_train)\n', ""    scores[k] = metrics.accuracy_score(result['Survived'],y_pred)\n"", ""    scores_list.append(metrics.accuracy_score(result['Survived'],y_pred))\n"", '    \n', 'plt.plot(Krange,scores_list)\n', 'plt.xlabel(""Value of K"")\n', 'plt.ylabel(""Accuracy"")']"
train[:10],0,['train[:10]']
"ASSIGN = ASSIGN.iloc[:, :53] ASSIGN = ASSIGN.iloc[:, :52]",0,"['# KISS\n', 'train_df = train_df.iloc[:, :53]\n', 'test_df = test_df.iloc[:, :52]']"
"plt.figure(figsize=(15,15)) for i in range(9): plt.subplot(3,3,i+1) plt.imshow(img[i])",1,"['plt.figure(figsize=(15,15))\n', 'for i in range(9):\n', '    plt.subplot(3,3,i+1)\n', '    plt.imshow(img[i])']"
"CHECKPOINT print('Train set shape:', train.shape) print('Test set shape:', test.shape) print('NaN in Train:',train.isnull().values.any()) print('NaN in Test:',test.isnull().values.any()) print('Train set overview:') display(train.head())",0,"[""print('Train set shape:', train.shape)\n"", ""print('Test set shape:', test.shape)\n"", ""print('NaN in Train:',train.isnull().values.any())\n"", ""print('NaN in Test:',test.isnull().values.any())\n"", ""print('Train set overview:')\n"", 'display(train.head())']"
SETUP CHECKPOINT print(),0,"[""# You don't need to worry for now about what this code does or how it works. If you're ever curious about the \n"", ""# code behind these exercises, it's available under an open source license here: https://github.com/Kaggle/learntools/\n"", ""# (But if you can understand that code, you'll probably find these lessons boring :)\n"", 'from learntools.core import binder; binder.bind(globals())\n', 'from learntools.python.ex1 import *\n', 'print(""Setup complete! You\'re ready to start question 0."")']"
"ASSIGN=pd.Index(data['YEAR']) ASSIGN =figure().gca() ASSIGN.xaxis.set_major_locator(MaxNLocator(integer=True)) ASSIGN.bar(ASSIGN.value_counts().index,ASSIGN.value_counts()) plt.xlabel(""Year"") plt.ylabel(""Count"") plt.title(""Counting the number for Crime (Year)"") plt.show(ASSIGN)",1,"[""Crime_year=pd.Index(data['YEAR'])\n"", 'ax =figure().gca()\n', 'ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n', 'ax.bar(Crime_year.value_counts().index,Crime_year.value_counts())\n', 'plt.xlabel(""Year"")\n', 'plt.ylabel(""Count"")\n', 'plt.title(""Counting the number for Crime (Year)"")\n', 'plt.show(ax)\n']"
"CHECKPOINT ASSIGN = ['revenue less expenses', 'Total expenses'] ASSIGN = all_to_investigate.drop(to_drop) final_list",0,"[""to_drop = ['revenue less expenses', 'Total expenses']\n"", 'final_list = all_to_investigate.drop(to_drop)\n', 'final_list']"
"CHECKPOINT ASSIGN = pd.read_csv(""..path"", sep = ',',encoding = ""ISO-8859-1"", header= 0) ASSIGN = pd.read_csv(""..path"", sep = ',',encoding = ""ISO-8859-1"", header= 0) print(.format(ASSIGN.shape)) print(.format(ASSIGN.shape))",0,"['mnist = pd.read_csv(""../input/train.csv"",  sep = \',\',encoding = ""ISO-8859-1"", header= 0)\n', 'holdout = pd.read_csv(""../input/test.csv"",  sep = \',\',encoding = ""ISO-8859-1"", header= 0)\n', '\n', 'print(""Dimensions of train: {}"".format(mnist.shape))\n', 'print(""Dimensions of test: {}"".format(holdout.shape))']"
SETUP,0,"['#The models trained\n', 'from sklearn.linear_model import LogisticRegression\n', 'from sklearn.svm import LinearSVC\n', 'from sklearn import svm\n', 'from sklearn.naive_bayes import GaussianNB\n', 'from sklearn.naive_bayes import MultinomialNB\n', 'from sklearn.tree import DecisionTreeClassifier\n', 'from sklearn.ensemble import RandomForestClassifier\n', 'from sklearn.ensemble import GradientBoostingClassifier\n', 'from sklearn.neighbors import KNeighborsClassifier\n', 'from xgboost import XGBClassifier\n', 'from sklearn.naive_bayes import BernoulliNB\n', '\n', '#For Scaling and Hyperparameter Tuning\n', 'from sklearn.model_selection import train_test_split\n', 'from sklearn.model_selection import GridSearchCV\n', 'from sklearn.model_selection import RandomizedSearchCV\n', 'from sklearn.preprocessing import MinMaxScaler\n', 'from sklearn.preprocessing import StandardScaler\n', 'from sklearn.model_selection import cross_val_score\n', 'from sklearn.metrics import accuracy_score,confusion_matrix\n', 'from sklearn import metrics\n', '\n', '#Voting Classifier\n', 'from sklearn.ensemble import VotingClassifier ']"
"def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test): ASSIGN=normalize_X(ASSIGN) ASSIGN=normalize_X(ASSIGN) ASSIGN=normalize_X(ASSIGN) ASSIGN = shuffle(ASSIGN, random_state=42) ASSIGN = shuffle(ASSIGN, random_state=42) ASSIGN = shuffle(ASSIGN, random_state=42) return X_train,X_val,X_test,y_train,y_val,y_test",0,"['def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test):\n', '    X_train=normalize_X(X_train)\n', '    X_val=normalize_X(X_val)\n', '    X_test=normalize_X(X_test)\n', '    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n', '    X_val, y_val = shuffle(X_val, y_val, random_state=42)\n', '    X_test, y_test = shuffle(X_test, y_test, random_state=42)\n', '    return X_train,X_val,X_test,y_train,y_val,y_test']"
CHECKPOINT ASSIGN = suicide_attacks['City'].unique() ASSIGN.sort() cities,0,"[""# get all the unique values in the 'City' column\n"", ""cities = suicide_attacks['City'].unique()\n"", '\n', '# sort them alphabetically and then take a closer look\n', 'cities.sort()\n', 'cities']"
"SETUP ASSIGN = pd.read_csv(f""{DATA_PATH}path"") ASSIGN = pd.read_csv(f""{DATA_PATH}path"") ASSIGN = pd.read_csv(f""{DATA_PATH}path"")",0,"['%%time\n', 'DATA_PATH = ""../input/rsna-str-pulmonary-embolism-detection""\n', '\n', 'train = pd.read_csv(f""{DATA_PATH}/train.csv"")\n', 'test = pd.read_csv(f""{DATA_PATH}/test.csv"")\n', 'sample_submission = pd.read_csv(f""{DATA_PATH}/sample_submission.csv"")']"
"ASSIGN=data.isnull().sum().sort_values(ascending=False)[:6].index ASSIGN=data.isnull().sum().sort_values(ascending=False)[:6] plt.figure(figsize=(8,8)) sns.barplot(ASSIGN,ASSIGN) plt.title(""counts of missing value"",size=20)",1,"['y=data.isnull().sum().sort_values(ascending=False)[:6].index\n', 'x=data.isnull().sum().sort_values(ascending=False)[:6]\n', 'plt.figure(figsize=(8,8))\n', 'sns.barplot(x,y)\n', 'plt.title(""counts of missing value"",size=20)']"
"CHECKPOINT CHECKPOINT ASSIGN = train.drop('SalePrice',axis=1) ASSIGN = pd.concat((train_X, test)).reset_index(drop=True) print(ASSIGN.shape) data_features.columns",0,"[""train_X = train.drop('SalePrice',axis=1)\n"", 'data_features = pd.concat((train_X, test)).reset_index(drop=True)\n', 'print(data_features.shape)\n', 'data_features.columns\n', '#We concatenate the train set and the test set since we need to handle the data both on the train set and the test set.']"
ASSIGN = 'cuda' if torch.cuda.is_available() else 'cpu',0,"[""device = 'cuda' if torch.cuda.is_available() else 'cpu'""]"
os.listdir('..path'),0,"[""os.listdir('../input/novel-corona-virus-2019-dataset')""]"
"SETUP CHECKPOINT def get_language(page): ASSIGN = re.search('[a-z][a-z].wikipedia.org', page) if ASSIGN: return ASSIGN[0][0:2] return 'na' ASSIGN = train.Page.map(get_language) print(Counter(train.lang))",0,"['def get_language(page):\n', ""    res = re.search('[a-z][a-z].wikipedia.org', page)\n"", '    if res:\n', '        return res[0][0:2]\n', ""    return 'na'\n"", '\n', ""train['lang'] = train.Page.map(get_language)\n"", '\n', 'from collections import Counter\n', '\n', 'print(Counter(train.lang))']"
final['Title'].value_counts(),0,"[""final['Title'].value_counts()""]"
"final.head() final.drop(['PassengerId'],axis = 1,inplace = True) final.drop(['SibSp','Parch','Family'],axis = 1,inplace = True)",0,"['final.head()\n', ""final.drop(['PassengerId'],axis = 1,inplace = True)\n"", ""final.drop(['SibSp','Parch','Family'],axis = 1,inplace = True)""]"
"train_df[cont].hist(bins=20, figsize=(15,15), color = 'orange') plt.suptitle(""Histogram for each train numeric input variable"") plt.show()",1,"[""train_df[cont].hist(bins=20, figsize=(15,15), color = 'orange')\n"", 'plt.suptitle(""Histogram for each train numeric input variable"")\n', 'plt.show()']"
"def matriz_deconfusao(cm, target_names, ASSIGN='Matriz de Confusão', ASSIGN=None, ASSIGN=True): ASSIGN = np.trace(cm) path(np.sum(cm)) ASSIGN = 1 - accuracy if ASSIGN is None: ASSIGN = plt.get_cmap('Blues') plt.figure(figsize=(8, 6)) plt.imshow(cm, interpolation='nearest', ASSIGN=ASSIGN) plt.ASSIGN(ASSIGN) plt.colorbar() if target_names is not None: ASSIGN = np.arange(len(target_names)) plt.xticks(ASSIGN, target_names, rotation=45) plt.yticks(ASSIGN, target_names) if ASSIGN: ASSIGN = ASSIGN.astype('float') path(axis=1)[:, np.newaxis] ASSIGN = cm.max() path() path for i, j in itertools.product(range(ASSIGN.shape[0]), range(ASSIGN.shape[1])): if ASSIGN: plt.text(j, i, ""{:0.4f}"".format(ASSIGN[i, j]), ASSIGN=""center"", ASSIGN=""white"" if cm[i, j] > thresh else ""black"") else: plt.text(j, i, ""{:,}"".format(ASSIGN[i, j]), ASSIGN=""center"", ASSIGN=""white"" if cm[i, j] > thresh else ""black"") plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label\naccuracy={:0.4f}; ASSIGN={:0.4f}'.format(ASSIGN, ASSIGN)) plt.show()",1,"['def matriz_deconfusao(cm, target_names,\n', ""                          title='Matriz de Confusão',\n"", '                          cmap=None,\n', '                          normalize=True):\n', '    \n', '    accuracy = np.trace(cm) / float(np.sum(cm))\n', '    misclass = 1 - accuracy\n', '\n', '    if cmap is None:\n', ""        cmap = plt.get_cmap('Blues')\n"", '\n', '    plt.figure(figsize=(8, 6))\n', ""    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n"", '    plt.title(title)\n', '    plt.colorbar()\n', '\n', '    if target_names is not None:\n', '        tick_marks = np.arange(len(target_names))\n', '        plt.xticks(tick_marks, target_names, rotation=45)\n', '        plt.yticks(tick_marks, target_names)\n', '\n', '    if normalize:\n', ""        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n"", '\n', '\n', '    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n', '    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n', '        if normalize:\n', '            plt.text(j, i, ""{:0.4f}"".format(cm[i, j]),\n', '                     horizontalalignment=""center"",\n', '                     color=""white"" if cm[i, j] > thresh else ""black"")\n', '        else:\n', '            plt.text(j, i, ""{:,}"".format(cm[i, j]),\n', '                     horizontalalignment=""center"",\n', '                     color=""white"" if cm[i, j] > thresh else ""black"")\n', '\n', '\n', '    plt.tight_layout()\n', ""    plt.ylabel('True label')\n"", ""    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n"", '    plt.show()\n']"
ASSIGN = pd.read_csv('..path') ASSIGN.head(),0,"[""df = pd.read_csv('../input/train.csv')\n"", 'df.head()']"
CHECKPOINT ASSIGN = data_num.skew().sort_values(ascending = False) skew_features,0,"['skew_features = data_num.skew().sort_values(ascending = False)\n', 'skew_features']"
"CHECKPOINT def plotCorrelationMatrix(df, graphWidth): ASSIGN = df.dataframeName ASSIGN = ASSIGN.dropna('columns') ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]] if ASSIGN.shape[1] < 2: print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2') return ASSIGN = df.ASSIGN() plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k') ASSIGN = plt.matshow(corr, fignum = 1) plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90) plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns) plt.gca().xaxis.tick_bottom() plt.colorbar(ASSIGN) plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15) plt.show()",1,"['# Correlation matrix\n', 'def plotCorrelationMatrix(df, graphWidth):\n', '    filename = df.dataframeName\n', ""    df = df.dropna('columns') # drop columns with NaN\n"", '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n', '    if df.shape[1] < 2:\n', ""        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n"", '        return\n', '    corr = df.corr()\n', ""    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n"", '    corrMat = plt.matshow(corr, fignum = 1)\n', '    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n', '    plt.yticks(range(len(corr.columns)), corr.columns)\n', '    plt.gca().xaxis.tick_bottom()\n', '    plt.colorbar(corrMat)\n', ""    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n"", '    plt.show()\n']"
SETUP,0,"['from sklearn.linear_model import LinearRegression\n', 'from sklearn.tree import DecisionTreeRegressor\n', 'from sklearn.ensemble import RandomForestRegressor\n', 'from sklearn.model_selection import train_test_split\n', 'from sklearn.model_selection import GridSearchCV\n', 'from sklearn import metrics\n', 'from sklearn import linear_model\n', 'from sklearn.linear_model import Lasso\n', 'from sklearn.linear_model import ElasticNet\n', 'from sklearn.linear_model import Ridge\n', 'from sklearn.svm import SVR\n', 'from sklearn.metrics import mean_squared_error\n', '\n', '\n', 'from sklearn_pandas import DataFrameMapper\n', 'from numpy import asarray\n', 'from sklearn.preprocessing import MinMaxScaler\n', 'from sklearn.preprocessing import StandardScaler']"
"SETUP CHECKPOINT ASSIGN = cross_val_score(dtree,train_x,train_y,cv=5,scoring='accuracy') print('average of Cross validation: %.5f'%ASSIGN.mean()) print('standard deviation of Cross validation: %.5f'%ASSIGN.std(ddof=1))",0,"['# 交叉驗證 cross validation \n', 'from sklearn.model_selection import cross_val_score\n', 'from sklearn.model_selection import train_test_split\n', ""scores = cross_val_score(dtree,train_x,train_y,cv=5,scoring='accuracy')\n"", '\n', '# 計算平均值與標準差\n', ""print('average of Cross validation: %.5f'%scores.mean())\n"", ""print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))""]"
ASSIGN = LR.predict(X_test),0,['y_pred = LR.predict(X_test)']
"ASSIGN = pd.read_csv('..path', sep = ',')",0,"[""df2 = pd.read_csv('../input/covid19-global-forecasting-week-1/train.csv', sep = ',')""]"
"data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageQual'].notnull()), ['GarageQual']]",0,"[""data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageQual'].notnull()), ['GarageQual']]\n"", '#it seems like they have the same missing values.\n', '#the same for the two below\n', '#so fill them all with NONE']"
"ASSIGN = Controller().to(device) ASSIGN = optim.Adam(controller_model.parameters(), lr=0.1, betas=(0.5, 0.999)) ASSIGN()",0,"['controller_model = Controller().to(device)\n', 'controller_optimizer = optim.Adam(controller_model.parameters(), lr=0.1, betas=(0.5, 0.999))\n', 'controller_model()']"
"ASSIGN = pd.date_range('2020path', periods=50, freq='D')",0,"[""days = pd.date_range('2020/1/22', periods=50, freq='D')""]"
df.isnull().sum(),0,['df.isnull().sum()']
"SETUP ASSIGN = sm.GLM(y_train,(sm.add_constant(X_train_pca)), family = sm.families.Binomial()) ASSIGN.fit().summary()",0,"['import statsmodels.api as sm\n', '# Logistic regression model\n', 'logpca = sm.GLM(y_train,(sm.add_constant(X_train_pca)), family = sm.families.Binomial())\n', 'logpca.fit().summary()']"
"confirmed.columns = ['ds','y'] ASSIGN = pd.to_datetime(ASSIGN)",0,"[""confirmed.columns = ['ds','y']\n"", ""confirmed['ds'] = pd.to_datetime(confirmed['ds'])""]"
"ASSIGN = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270] ASSIGN = RandomForestRegressor() ASSIGN = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]} ASSIGN = GridSearchCV(Rf, parameters,scoring='neg_mean_squared_error', cv=10) ASSIGN.fit(x_t, Y_t) ASSIGN.best_params_",0,"['n_estimators = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]\n', '\n', 'Rf = RandomForestRegressor()\n', '\n', ""parameters = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}\n"", '\n', ""Rfr = GridSearchCV(Rf, parameters,scoring='neg_mean_squared_error', cv=10)\n"", '\n', 'Rfr.fit(x_t, Y_t)\n', '\n', 'Rfr.best_params_']"
"final['Cabin_final'].fillna('Unknown',inplace = True)",0,"[""final['Cabin_final'].fillna('Unknown',inplace = True)""]"
"type(skipped), type(skipped.numpy()), skipped.numpy()",0,"['type(skipped), type(skipped.numpy()), skipped.numpy() ']"
predictions.mean(),0,['predictions.mean()']
train_data['color'].unique(),0,"[""train_data['color'].unique()""]"
"ASSIGN={} for col in ['avg_sales_lag3','avg_purchases_lag3','active_months_lag3','avg_sales_lag6','avg_purchases_lag6','active_months_lag6','avg_sales_lag12','avg_purchases_lag12','active_months_lag12','numerical_1','numerical_2']: SLICE= ['mean'] ASSIGN= get_new_columns('merchants',aggs) ASSIGN = df_merchants.groupby('merchant_category_id').agg(aggs) ASSIGN.columns = ASSIGN ASSIGN.reset_index(drop=False,inplace=True) ASSIGN=ASSIGN.merge(df_merchants_group,on='merchant_category_id',how='left') ASSIGN=ASSIGN.merge(df_merchants_group.reset_index(),on='merchant_category_id',how='left') ASSIGN.head()",0,"['aggs={}\n', ""for col in ['avg_sales_lag3','avg_purchases_lag3','active_months_lag3','avg_sales_lag6','avg_purchases_lag6','active_months_lag6','avg_sales_lag12','avg_purchases_lag12','active_months_lag12','numerical_1','numerical_2']:\n"", ""    aggs[col]= ['mean']\n"", '    \n', ""new_columns= get_new_columns('merchants',aggs)\n"", ""df_merchants_group = df_merchants.groupby('merchant_category_id').agg(aggs)\n"", 'df_merchants_group.columns = new_columns\n', 'df_merchants_group.reset_index(drop=False,inplace=True)\n', ""df_train=df_train.merge(df_merchants_group,on='merchant_category_id',how='left')\n"", ""df_test=df_test.merge(df_merchants_group.reset_index(),on='merchant_category_id',how='left')\n"", 'df_train.head()']"
"CHECKPOINT print(pipeline_optimizer.score(X_test, y_test))",0,"['print(pipeline_optimizer.score(X_test, y_test))']"
"SETUP ASSIGN = RandomOverSampler(random_state=0) ASSIGN = ros.fit_sample(train, y_train) del train; gc.collect()''' ASSIGN = SMOTE(frac=0.1, random_state=1) ASSIGN = smote.fit_sample(train, y_train)''' '''from imblearn.under_sampling import RandomUnderSampler ASSIGN=RandomUnderSampler(return_indices=True) ASSIGN = ran.fit_sample(train,y_train) del train; gc.collect()'''",0,"['from imblearn.over_sampling import RandomOverSampler\n', 'ros = RandomOverSampler(random_state=0)\n', 'train_data, y_train = ros.fit_sample(train, y_train)\n', ""del train; gc.collect()'''\n"", 'from imblearn.over_sampling import SMOTE\n', 'smote = SMOTE(frac=0.1, random_state=1)\n', ""train_data, y_train = smote.fit_sample(train, y_train)'''\n"", '\n', ""'''from imblearn.under_sampling import RandomUnderSampler\n"", 'ran=RandomUnderSampler(return_indices=True) ##intialize to return indices of dropped rows\n', 'train_data,y_train,dropped = ran.fit_sample(train,y_train)\n', ""del train; gc.collect()'''""]"
WH4(X_test_full) WH4(X) ASSIGN = GaussianMixture(n_components = 15) ASSIGN.fit(X) X['g_mixture'] = ASSIGN.predict(X) X_test_full['g_mixture'] = ASSIGN.predict(X_test_full),0,"['WH4(X_test_full)\n', 'WH4(X)\n', '\n', '\n', 'gm = GaussianMixture(n_components  = 15)\n', 'gm.fit(X)\n', ""X['g_mixture'] = gm.predict(X)\n"", ""X_test_full['g_mixture'] = gm.predict(X_test_full)""]"
ASSIGN=df_map_final.tail(20),0,"['#lets take one data frame for top 20 cities with most retaurants counts \n', 'df_plot_top=df_map_final.tail(20)']"
test.head(),0,['test.head()']
"ASSIGN=test[['Id','MSSubClass','MSZoning','Neighborhood','OverallQual','OverallCond']] ASSIGN.isnull().sum()",0,"[""data_tree_for_test=test[['Id','MSSubClass','MSZoning','Neighborhood','OverallQual','OverallCond']]\n"", 'data_tree_for_test.isnull().sum()']"
"def create_model(): ASSIGN = models.densenet121(pretrained=True) for param in ASSIGN.parameters(): param.requires_grad = False ASSIGN = model.classifier.in_features ASSIGN.classifier = nn.Sequential( nn.Linear(ASSIGN, NUM_CLASSES), nn.LogSoftmax(dim=1)) ASSIGN.to(device) return model",0,"['def create_model():\n', '    model = models.densenet121(pretrained=True)\n', '\n', '    for param in model.parameters():\n', '        param.requires_grad = False\n', '\n', '    n_inputs = model.classifier.in_features\n', '    model.classifier = nn.Sequential(\n', '                    nn.Linear(n_inputs, NUM_CLASSES),\n', '                    nn.LogSoftmax(dim=1))\n', '    # Move model to the device specified above\n', '\n', '    model.to(device)\n', '    return model']"
CHECKPOINT data.describe,0,['data.describe']
"CHECKPOINT ASSIGN = RandomForestClassifier(random_state = 5) estimator.append(('ASSIGN',RandomForestClassifier(random_state = 5))) ASSIGN = cross_val_score(RF,x_train,x_test,ASSIGN=10) ASSIGN = cv.mean() accuracy.append(ASSIGN) print(ASSIGN) print(ASSIGN.mean())",0,"['RF = RandomForestClassifier(random_state = 5)\n', ""estimator.append(('RF',RandomForestClassifier(random_state = 5)))\n"", 'cv = cross_val_score(RF,x_train,x_test,cv=10)\n', 'accuracy7 = cv.mean()\n', 'accuracy.append(accuracy7)\n', 'print(cv)\n', 'print(cv.mean())']"
"data_features.loc[(data_features['PoolQC'].notnull()),['PoolArea','PoolQC']]",0,"[""data_features.loc[(data_features['PoolQC'].notnull()),['PoolArea','PoolQC']]""]"
"sns.jointplot(x = 'GRE Score', y = 'University Rating', data=df)",1,"[""sns.jointplot(x = 'GRE Score', y = 'University Rating', data=df)""]"
CHECKPOINT print() all(recipes['rating'] == recipes['rating'].astype(int)),0,"['print(""Is this variable only integers?"")\n', '\n', ""all(recipes['rating'] == recipes['rating'].astype(int))""]"
"train_digital.shape, test_digital.shape, train_category.shape, test_category.shape",0,"['train_digital.shape, test_digital.shape, train_category.shape, test_category.shape']"
SETUP sns.set_style('whitegrid'),0,"['# Imports\n', '\n', '# pandas\n', 'import pandas as pd\n', 'from pandas import Series,DataFrame\n', '\n', '# numpy, matplotlib, seaborn\n', 'import numpy as np\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', ""sns.set_style('whitegrid')\n"", '%matplotlib inline\n', '\n', '# machine learning\n', 'from sklearn.linear_model import LogisticRegression\n', 'from sklearn.svm import SVC, LinearSVC\n', 'from sklearn.ensemble import RandomForestClassifier\n', 'from sklearn.neighbors import KNeighborsClassifier\n', 'from sklearn.naive_bayes import GaussianNB']"
cbsr['Total Atrocities'] = cbsr['Murder'] +cbsr['Assault on women']+cbsr['Kidnapping and Abduction']+cbsr['Dacoity']+cbsr['Robbery']+cbsr['Arson']+cbsr['Hurt']+cbsr['Prevention of atrocities (POA) Act']+cbsr['Protection of Civil Rights (PCR) Act']+cbsr['Other Crimes Against SCs'] cbsr.head(),0,"[""cbsr['Total Atrocities'] = cbsr['Murder'] +cbsr['Assault on women']+cbsr['Kidnapping and Abduction']+cbsr['Dacoity']+cbsr['Robbery']+cbsr['Arson']+cbsr['Hurt']+cbsr['Prevention of atrocities (POA) Act']+cbsr['Protection of Civil Rights (PCR) Act']+cbsr['Other Crimes Against SCs']\n"", 'cbsr.head()\n']"
"ASSIGN = train[(train['hour'] > 19) | (train['hour'] < 7)] ASSIGN = len(train) - len(night) ASSIGN = 'Day', 'Night' ASSIGN = [len(night), day] ASSIGN = ['lightcoral', 'lightskyblue'] ASSIGN = (0.1, 0) plt.pie(ASSIGN, ASSIGN=ASSIGN, ASSIGN=ASSIGN, ASSIGN=ASSIGN, ASSIGN='%1.1f%%', shadow=True, startangle=140) plt.axis('equal') plt.show()",1,"[""night = train[(train['hour'] > 19) | (train['hour'] < 7)]\n"", 'day = len(train) - len(night)\n', ""labels = 'Day', 'Night'\n"", 'sizes = [len(night), day]\n', ""colors = ['lightcoral', 'lightskyblue']\n"", 'explode = (0.1, 0) \n', 'plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n', ""autopct='%1.1f%%', shadow=True, startangle=140)\n"", ""plt.axis('equal')\n"", 'plt.show()']"
os.listdir(test_image_dir),0,['os.listdir(test_image_dir)']
"SETUP ASSIGN = ['English', 'Maths', 'Science ', 'History', 'Geography'] ASSIGN = [86, 83, 86, 90, 88] ASSIGN = ['r', 'y', 'g', 'b','c'] plt.pie(ASSIGN, labels = ASSIGN, ASSIGN=ASSIGN, ASSIGN=90, shadow = True, explode = (0, 0.5, 0, 0, 0), ASSIGN = 1.2, autopct = '%1.1f%%') plt.show()",1,"['#3\n', 'import matplotlib.pyplot as plt\n', ""teams = ['English', 'Maths', 'Science ', 'History', 'Geography']\n"", 'slices = [86, 83, 86, 90, 88]\n', ""colors = ['r', 'y', 'g', 'b','c']\n"", 'plt.pie(slices, labels = teams, colors=colors,\n', ' startangle=90, shadow = True, explode = (0, 0.5, 0, 0, 0),\n', "" radius = 1.2, autopct = '%1.1f%%')\n"", 'plt.show()']"
"ASSIGN = os.path.join(DATASET_DIR, 'train_masks', '{}.{}'.format(image_ids[0], 'jpg')) ASSIGN = cv2.imread(mask_path, 0).astype(np.int) path for id in image_ids[1:]: ASSIGN = os.path.join(DATASET_DIR, 'train_masks', '{}.{}'.format(id, 'jpg')) try: ASSIGN = cv2.imread(mask_path, 0).astype(np.int) path ASSIGN = np.add(ASSIGN, mask) except: pass ASSIGN = plt.subplots(figsize=(18, 16)) ax.set_title('ASSIGN distribution') ASSIGN = ax.imshow(mask_accru) plt.show()",1,"[""mask_path = os.path.join(DATASET_DIR, 'train_masks', '{}.{}'.format(image_ids[0], 'jpg'))\n"", 'mask_accru = cv2.imread(mask_path, 0).astype(np.int) / 255\n', 'for id in image_ids[1:]:\n', ""    mask_path = os.path.join(DATASET_DIR, 'train_masks', '{}.{}'.format(id, 'jpg'))\n"", '    try:\n', '        mask = cv2.imread(mask_path, 0).astype(np.int) / 255\n', '        mask_accru = np.add(mask_accru, mask)\n', '    except:\n', '        pass\n', '\n', 'fig, ax = plt.subplots(figsize=(18, 16))\n', ""ax.set_title('mask distribution')\n"", 'im = ax.imshow(mask_accru)\n', 'plt.show()']"
"SETUP ASSIGN = Imputer(missing_values='NaN', strategy='most_frequent', axis=0) df_merchants['avg_sales_lag3']=ASSIGN.fit(pd.DataFrame(df_merchants['avg_sales_lag3'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['avg_sales_lag3'].values.reshape(-1,1))) df_merchants['avg_sales_lag6']=ASSIGN.fit(pd.DataFrame(df_merchants['avg_sales_lag6'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['avg_sales_lag6'].values.reshape(-1,1))) df_merchants['avg_sales_lag12']=ASSIGN.fit(pd.DataFrame(df_merchants['avg_sales_lag12'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['avg_sales_lag12'].values.reshape(-1,1))) df_merchants['category_2']=ASSIGN.fit(pd.DataFrame(df_merchants['category_2'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['category_2'].values.reshape(-1,1))) df_merchants.head()",0,"['from sklearn.preprocessing import Imputer\n', ""imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n"", ""df_merchants['avg_sales_lag3']=imp.fit(pd.DataFrame(df_merchants['avg_sales_lag3'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['avg_sales_lag3'].values.reshape(-1,1)))\n"", ""df_merchants['avg_sales_lag6']=imp.fit(pd.DataFrame(df_merchants['avg_sales_lag6'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['avg_sales_lag6'].values.reshape(-1,1)))\n"", ""df_merchants['avg_sales_lag12']=imp.fit(pd.DataFrame(df_merchants['avg_sales_lag12'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['avg_sales_lag12'].values.reshape(-1,1)))\n"", ""df_merchants['category_2']=imp.fit(pd.DataFrame(df_merchants['category_2'].values.reshape(-1,1))).transform(pd.DataFrame(df_merchants['category_2'].values.reshape(-1,1)))\n"", 'df_merchants.head()']"
np.sum(image.numpy() - lossy_image.numpy()),0,['np.sum(image.numpy() - lossy_image.numpy())']
ASSIGN = ivis.transform(train_X) ASSIGN = ivis.transform(test_X),0,"['train_embeddings = ivis.transform(train_X)\n', 'test_embeddings = ivis.transform(test_X)']"
CHECKPOINT print(train_data.shape) train_data.describe() train_data.head() print(df.shape) df.describe() df.head() print(test_data.shape) test_data.describe() test_data.head(),0,"['print(train_data.shape)\n', 'train_data.describe()\n', 'train_data.head()\n', '\n', 'print(df.shape)\n', 'df.describe()\n', 'df.head()\n', '\n', 'print(test_data.shape)\n', 'test_data.describe()\n', 'test_data.head()']"
data.describe(),0,['data.describe()']
"ASSIGN = ['Exterior1st','Exterior2nd','SaleType','Electrical','KitchenQual'] for col in ASSIGN: data_features[col].fillna(data_features[col].mode()[0], inplace = True)",0,"[""common_for_NA = ['Exterior1st','Exterior2nd','SaleType','Electrical','KitchenQual']\n"", 'for col in common_for_NA:\n', '    data_features[col].fillna(data_features[col].mode()[0], inplace = True)']"
interp.plot_confusion_matrix(),1,['interp.plot_confusion_matrix()']
"ASSIGN = pd.read_csv(""..path"")",0,"['test = pd.read_csv(""../input/test.csv"")']"
"SETUP CHECKPOINT print(check_output([, ]).decode())",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n', '\n', 'from subprocess import check_output\n', 'print(check_output([""ls"", ""../input""]).decode(""utf8""))\n', '\n', '# Any results you write to the current directory are saved as output.']"
"CHECKPOINT print(ssim(reimg, reimg_28)) print(ssim(reimg, reimg_16)) print(ssim(reimg, reimg_8))",0,"['# import torch\n', '# from torch.autograd import Variable\n', '\n', '# img1 = Variable(torch.rand(1, 1, 256, 256))\n', '# img2 = Variable(torch.rand(1, 1, 256, 256))\n', '\n', '# if torch.cuda.is_available():\n', '#     img1 = img1.cuda()\n', '#     img2 = img2.cuda()\n', '\n', '\n', '# reimg_28 = reimg_28.view(1,reimg_28.shape[0],reimg_28.shape[1],reimg_28.shape[2])\n', '# reimg = reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2])\n', '# reimg = reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2])\n', 'print(ssim(reimg, reimg_28))\n', 'print(ssim(reimg, reimg_16))\n', 'print(ssim(reimg, reimg_8))']"
"CHECKPOINT ASSIGN=tree.DecisionTreeClassifier(max_depth=7) ASSIGN=ASSIGN.fit(train_x,train_y) ASSIGN = cross_val_score(dtree,train_x,train_y,cv=5,scoring='accuracy') print(ASSIGN) print('average of Cross validation: %.5f'%ASSIGN.mean()) print('standard deviation of Cross validation: %.5f'%ASSIGN.std(ddof=1))",0,"['dtree=tree.DecisionTreeClassifier(max_depth=7)\n', 'dtree=dtree.fit(train_x,train_y)\n', ""scores = cross_val_score(dtree,train_x,train_y,cv=5,scoring='accuracy')\n"", 'print(scores)\n', ""print('average of Cross validation: %.5f'%scores.mean())\n"", ""print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))""]"
pca_last.n_components_,0,['pca_last.n_components_']
ASSIGN = pd.DataFrame(),0,['my_submission = pd.DataFrame()']
SETUP,0,"['import numpy as np\n', 'import pandas as pd\n', 'import cv2\n', 'import json\n', 'import os\n', 'import matplotlib\n', 'import matplotlib.pyplot as plt\n', 'from matplotlib.patches import Polygon\n', 'from matplotlib.collections import PatchCollection\n', 'from mpl_toolkits.mplot3d import Axes3D\n', 'import seaborn as sns']"
"ASSIGN = {k:v for k,v in zip(data.path.values,data.target_label.values)}",0,"['target_label_map = {k:v for k,v in zip(data.path.values,data.target_label.values)}']"
X_test.head(),0,['X_test.head()']
"ASSIGN = min(30, X.columns.size) ASSIGN = AdaBoostClassifier(n_estimators=300, ASSIGN=DecisionTreeClassifier( ASSIGN=2, ASSIGN=ASSIGN), ASSIGN=ASSIGN) ASSIGN = ExtraTreesClassifier(n_estimators=500, ASSIGN=2, ASSIGN=2, ASSIGN=50, ASSIGN=ASSIGN, ASSIGN=ASSIGN, ASSIGN=-1) ASSIGN = LGBMClassifier(n_estimators=300, ASSIGN=128, ASSIGN=-1, ASSIGN=ASSIGN, ASSIGN=-1) ASSIGN = RandomForestClassifier(n_estimators=300, ASSIGN=ASSIGN, ASSIGN=-1) ASSIGN = [('AdaBoostClassifier', ab_clf), ('ExtraTreesClassifier', ASSIGN), ('LGBMClassifier', ASSIGN), ('RandomForestClassifier', ASSIGN)]",0,"['max_features = min(30, X.columns.size)\n', '\n', 'ab_clf = AdaBoostClassifier(n_estimators=300,\n', '                            base_estimator=DecisionTreeClassifier(\n', '                                min_samples_leaf=2,\n', '                                random_state=random_state),\n', '                            random_state=random_state)\n', '\n', 'et_clf = ExtraTreesClassifier(n_estimators=500,\n', '                              min_samples_leaf=2,\n', '                              min_samples_split=2,\n', '                              max_depth=50,\n', '                              max_features=max_features,\n', '                              random_state=random_state,\n', '                              n_jobs=-1)\n', '\n', 'lg_clf = LGBMClassifier(n_estimators=300,\n', '                        num_leaves=128,\n', '                        verbose=-1,\n', '                        random_state=random_state,\n', '                        n_jobs=-1)\n', '\n', 'rf_clf = RandomForestClassifier(n_estimators=300,\n', '                                random_state=random_state,\n', '                                n_jobs=-1)\n', '\n', ""ensemble = [('AdaBoostClassifier', ab_clf),\n"", ""            ('ExtraTreesClassifier', et_clf),\n"", ""            ('LGBMClassifier', lg_clf),\n"", ""            ('RandomForestClassifier', rf_clf)]""]"
CHECKPOINT ASSIGN = dtree.predict(test_x) predict_y,0,"['#把訓練好的模型套用到測試數據\n', 'predict_y = dtree.predict(test_x)\n', 'predict_y']"
df_all.sample(n=5),0,['df_all.sample(n=5)']
data.info(),0,['data.info()']
"ASSIGN=pd.DataFrame(df_np,columns=[""CITY"",""lat"",""lng""])",0,"['#creating a second dataframe with city name, latitude and longitude\n', 'df_sec=pd.DataFrame(df_np,columns=[""CITY"",""lat"",""lng""])']"
"def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow): ASSIGN = df.ASSIGN() ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]] nRow, nCol = ASSIGN.shape ASSIGN = list(df) ASSIGN = (nCol + nGraphPerRow - 1) path plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k') for i in range(min(nCol, nGraphShown)): plt.subplot(ASSIGN, nGraphPerRow, i + 1) ASSIGN = df.iloc[:, i] if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)): ASSIGN = columnDf.value_counts() ASSIGN.plot.bar() else: ASSIGN.hist() plt.ylabel('counts') plt.xticks(rotation = 90) plt.title(f'{ASSIGN[i]} (column {i})') plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0) plt.show()",1,"['# Distribution graphs (histogram/bar graph) of column data\n', 'def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n', '    nunique = df.nunique()\n', '    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n', '    nRow, nCol = df.shape\n', '    columnNames = list(df)\n', '    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n', ""    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n"", '    for i in range(min(nCol, nGraphShown)):\n', '        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n', '        columnDf = df.iloc[:, i]\n', '        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n', '            valueCounts = columnDf.value_counts()\n', '            valueCounts.plot.bar()\n', '        else:\n', '            columnDf.hist()\n', ""        plt.ylabel('counts')\n"", '        plt.xticks(rotation = 90)\n', ""        plt.title(f'{columnNames[i]} (column {i})')\n"", '    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n', '    plt.show()\n']"
"plt.pie(mrks,labels=slc,explode=(0,0,0,0,0,0.1))",1,"['plt.pie(mrks,labels=slc,explode=(0,0,0,0,0,0.1))']"
"submission.to_csv('path',index = False)",0,"[""submission.to_csv('/kaggle/working/submission.csv',index = False)""]"
ASSIGN=data[data.OFFENSE_CATEGORY_ID=='larceny'] ASSIGN.head(),0,"[""data_larceny=data[data.OFFENSE_CATEGORY_ID=='larceny']\n"", 'data_larceny.head()']"
tumor_data.describe(),0,['tumor_data.describe()']
"for col in bsmt_var, garage_var,NONE_var: data_features[col] = data_features[col].fillna('NONE') data_features['GarageYrBlt'] = data_features['GarageYrBlt'].fillna(0)",0,"['for col in bsmt_var, garage_var,NONE_var:\n', ""    data_features[col] = data_features[col].fillna('NONE')\n"", ""data_features['GarageYrBlt'] = data_features['GarageYrBlt'].fillna(0)""]"
"CHECKPOINT ASSIGN = x_trainpath ASSIGN = to_categorical(ASSIGN, 10) print('x train datasets shape: ',ASSIGN.shape) print('y label datasets shape: ',ASSIGN.shape) ASSIGN.head()",0,"['x_train = x_train/255.0\n', 'y_label = to_categorical(y_label, 10)\n', ""print('x train datasets shape: ',x_train.shape)\n"", ""print('y label datasets shape: ',y_label.shape)\n"", 'x_train.head()']"
ASSIGN = cleaning_train.isnull().sum().sort_values(ascending = False).head(12).index.values.tolist() cleaning_train[ASSIGN] = cleaning_train[ASSIGN].fillna(cleaning_train[ASSIGN].mode().iloc[0]),0,"['# clean with mode on categorical\n', 'list_miss = cleaning_train.isnull().sum().sort_values(ascending = False).head(12).index.values.tolist()\n', 'cleaning_train[list_miss] = cleaning_train[list_miss].fillna(cleaning_train[list_miss].mode().iloc[0])']"
"ASSIGN = preprocess_input(copy.deepcopy(x_test)) ASSIGN = model.predict(x) ASSIGN = np.argmax(ASSIGN, axis=1) ASSIGN = np.argmax(y_test, axis=1) ASSIGN = confusion_matrix(y_trues, y_preds) ASSIGN = plt.subplots(figsize=(7, 6)) sns.heatmap(ASSIGN, annot=True, fmt='d', cmap='Blues', cbar_kws={'shrink': .3}, linewidths=.1, ax=ax) ax.set( ASSIGN=list(label_to_class.keys()), ASSIGN=list(label_to_class.keys()), ASSIGN='confusion matrix', ASSIGN='True label', ASSIGN='Predicted label' ) ASSIGN = dict(rotation=45, ha='center', rotation_mode='anchor') plt.setp(ax.get_yticklabels(), **ASSIGN) plt.setp(ax.get_xticklabels(), **ASSIGN) plt.show()",1,"['## plot confusion matrix\n', '\n', 'x = preprocess_input(copy.deepcopy(x_test))\n', 'y_preds = model.predict(x)\n', 'y_preds = np.argmax(y_preds, axis=1)\n', 'y_trues = np.argmax(y_test, axis=1)\n', 'cm = confusion_matrix(y_trues, y_preds)\n', '\n', 'fig, ax = plt.subplots(figsize=(7, 6))\n', '\n', ""sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'shrink': .3}, linewidths=.1, ax=ax)\n"", '\n', 'ax.set(\n', '    xticklabels=list(label_to_class.keys()),\n', '    yticklabels=list(label_to_class.keys()),\n', ""    title='confusion matrix',\n"", ""    ylabel='True label',\n"", ""    xlabel='Predicted label'\n"", ')\n', ""params = dict(rotation=45, ha='center', rotation_mode='anchor')\n"", 'plt.setp(ax.get_yticklabels(), **params)\n', 'plt.setp(ax.get_xticklabels(), **params)\n', 'plt.show()']"
"sns.set(style=""darkgrid"") ASSIGN = sns.regplot(x=""calories"", y=""dessert"", data=recipes, fit_reg=False) ASSIGN.figure.set_size_inches(8, 8)",1,"['sns.set(style=""darkgrid"")\n', 'g = sns.regplot(x=""calories"", y=""dessert"", data=recipes, fit_reg=False)\n', 'g.figure.set_size_inches(8, 8)']"
submission_df['Pred'].hist(),1,"[""submission_df['Pred'].hist()""]"
"X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)",0,"['X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)']"
"CHECKPOINT ASSIGN = [""age"",""class of worker"",""detailed industry recode"",""detailed occupation recode"",""education"", ""wage per hour"",""enroll in edu inst last wk"",""marital status"",""major industry code"", ""major occupation code"",""race"",""hispanic origin"",""sex"",""member of a labor union"", ""reason for unemployment"",""full or part time employment stat"",""capital gains"",""capital losses"", ""dividends from stocks"",""tax filer stat"",""region of previous residence"",""state of previous residence"", ""detailed household and family stat"",""detailed household summary in household"",""instance weight"", ""migration code-change in msa"",""migration code-change in reg"",""migration code-move within reg"", ""live in this house 1 year ago"",""migration prev res in sunbelt"",""num persons worked for employer"", ""family members under 18"",""country of birth father"",""country of birth mother"",""country of birth self"", ""citizenship"",""own business or self employed"",""fill inc questionnaire for veteran\'s admin"", ""veterans benefits"",""weeks worked in year"",""year"",""income class""] ASSIGN = pd.read_csv(""..path"", names=col, header=None) print(.format(ASSIGN.shape)) print('NaN in Train:',ASSIGN.isnull().values.any()) ASSIGN = pd.read_csv(""..path"", names=col[0:-1], header=None) print(.format(ASSIGN.shape)) print('NaN in Test:',ASSIGN.isnull().values.any()) ASSIGN = pd.read_csv(""..path"") ASSIGN = sub['index']",0,"['# load all datasets\n', 'col = [""age"",""class of worker"",""detailed industry recode"",""detailed occupation recode"",""education"",\n', '       ""wage per hour"",""enroll in edu inst last wk"",""marital status"",""major industry code"",\n', '       ""major occupation code"",""race"",""hispanic origin"",""sex"",""member of a labor union"",\n', '       ""reason for unemployment"",""full or part time employment stat"",""capital gains"",""capital losses"",\n', '       ""dividends from stocks"",""tax filer stat"",""region of previous residence"",""state of previous residence"",\n', '       ""detailed household and family stat"",""detailed household summary in household"",""instance weight"",\n', '       ""migration code-change in msa"",""migration code-change in reg"",""migration code-move within reg"",\n', '       ""live in this house 1 year ago"",""migration prev res in sunbelt"",""num persons worked for employer"",\n', '       ""family members under 18"",""country of birth father"",""country of birth mother"",""country of birth self"",\n', '       ""citizenship"",""own business or self employed"",""fill inc questionnaire for veteran\\\'s admin"",\n', '       ""veterans benefits"",""weeks worked in year"",""year"",""income class""]\n', '\n', 'df = pd.read_csv(""../input/ml-challenge-week6/census-income.data"", names=col, header=None)\n', 'print(""Shape of Train dataframe is: {}"".format(df.shape))\n', ""print('NaN in Train:',df.isnull().values.any())\n"", 'test = pd.read_csv(""../input/ml-challenge-week6/census-income.test"", names=col[0:-1], header=None)\n', 'print(""Shape of Test dataframe is: {}"".format(test.shape))\n', ""print('NaN in Test:',test.isnull().values.any())\n"", 'sub = pd.read_csv(""../input/ml-challenge-week6/sampleSubmission.csv"")\n', ""zub = sub['index']""]"
CHECKPOINT df.shape,0,['df.shape']
SETUP,0,"['#Import packages\n', 'import pandas as pd\n', 'import numpy as np\n', 'import seaborn as sns\n', 'import matplotlib.pyplot as plt']"
"CHECKPOINT ASSIGN = glob('..path*.csv') ASSIGN = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word'] ASSIGN = [] for f in ASSIGN[0:6]: ASSIGN = pd.read_csv(f, nrows=10) ASSIGN = ASSIGN[ASSIGN.recognized==True].head(2) ASSIGN.append(ASSIGN) ASSIGN = pd.DataFrame(np.concatenate(drawlist), columns=cnames) draw_df",0,"[""fnames = glob('../input/train_simplified/*.csv') #<class 'list'>\n"", ""cnames = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n"", 'drawlist = []\n', 'for f in fnames[0:6]: # num of word : 5\n', '    first = pd.read_csv(f, nrows=10) # make sure we get a recognized drawing\n', '    first = first[first.recognized==True].head(2) # top head 2 get \n', '    drawlist.append(first)\n', ""draw_df = pd.DataFrame(np.concatenate(drawlist), columns=cnames) # <class 'pandas.core.frame.DataFrame'>\n"", 'draw_df']"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") print() print(ASSIGN.shape) print() print(type(ASSIGN)) print() print(ASSIGN.head(3))",0,"['#Write a Python program to load the iris data from a given csv file into a dataframe and print the shape of the data, type of the data and first 3 rows.\n', 'import pandas as pd\n', 'data = pd.read_csv(""../input/iriscsv/Iris.csv"")\n', 'print(""Shape of the data:"")\n', 'print(data.shape)\n', 'print(""\\nData Type:"")\n', 'print(type(data))\n', 'print(""\\nFirst 3 rows:"")\n', 'print(data.head(3))']"
"data.classes, data.c, len(data.train_ds), len(data.valid_ds)",0,"['data.classes, data.c, len(data.train_ds), len(data.valid_ds)']"
"sub.to_csv(""Tree_version_8a.csv"", index=False)",0,"['sub.to_csv(""Tree_version_8a.csv"", index=False)']"
"sns.catplot(x='Year', y='Hurt', data=total ,height = 5, aspect = 4,kind = 'bar')",1,"[""sns.catplot(x='Year', y='Hurt', data=total ,height = 5, aspect = 4,kind = 'bar')""]"
"CHECKPOINT '''from sklearn.tree import DecisionTreeRegressor ASSIGN=[] for j in range(1000): ASSIGN=train_test_split(x,y,test_size=.1,random_state=j) ASSIGN=DecisionTreeRegressor().fit(x_train,y_train) ASSIGN.append(ASSIGN.score(x_test,y_test)) J= ASSIGN.index(np.max(ASSIGN)) J",0,"[""'''from sklearn.tree import DecisionTreeRegressor\n"", 'ts_score=[]\n', 'for j in range(1000):\n', '    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=j)\n', '    dc=DecisionTreeRegressor().fit(x_train,y_train)\n', '    ts_score.append(dc.score(x_test,y_test))\n', 'J= ts_score.index(np.max(ts_score))\n', '\n', 'J\n', ""#J=105'''""]"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") X= ASSIGN.drop(""species"", axis=1) ASSIGN= train[""species""] X_train, X_test,y_train, y_test= train_test_split(X,ASSIGN, test_size=0.3) ASSIGN= LogisticRegression() ASSIGN.fit(X_train, y_train) ASSIGN = logmodel.predict(X_test) print(, f1_score(y_test, ASSIGN, average='macro')) print(, f1_score(y_test, ASSIGN, average='micro')) print(, f1_score(y_test, ASSIGN, average='weighted')) print() confusion_matrix(y_test, ASSIGN)",0,"['#4\n', 'import pandas as pd\n', 'import numpy as np\n', 'from sklearn.model_selection import train_test_split\n', 'from sklearn.metrics import confusion_matrix\n', 'from sklearn.model_selection import cross_val_predict\n', 'from sklearn.linear_model import LogisticRegression\n', 'from sklearn.metrics import f1_score\n', '\n', '\n', 'train = pd.read_csv(""../input/iris-flower-dataset/IRIS.csv"")\n', 'X= train.drop(""species"", axis=1)\n', 'y= train[""species""]\n', 'X_train, X_test,y_train, y_test= train_test_split(X,y, test_size=0.3)\n', '\n', 'logmodel= LogisticRegression()\n', 'logmodel.fit(X_train, y_train)\n', '\n', 'predictions = logmodel.predict(X_test)\n', '\n', 'print(""F1 Score(macro):"", f1_score(y_test, predictions, average=\'macro\'))\n', 'print(""F1 Score(micro):"", f1_score(y_test, predictions, average=\'micro\'))\n', 'print(""F1 Score(weighted):"", f1_score(y_test, predictions, average=\'weighted\'))\n', 'print(""\\confusion Matrix(below):\\n"")\n', 'confusion_matrix(y_test, predictions)\n']"
CHECKPOINT final_df,0,['final_df']
"def TOTAvgRank_DD_DAP_TE_TB(player,month): ASSIGN=['Driving Distance - (TOTAL DRIVES)','Driving Accuracy Percentage - (%)','Total Eagles - (TOTAL)','Total Birdies - (TOTAL)'] B={} for i in range(len(ASSIGN)): A=data_2019[data_2019.Variable==ASSIGN[i]] A=A[A.month==month] A.dropna(subset=['Value']) A.Value=A.Value.astype(float) ASSIGN=A.groupby(['Player Name'])['Value'].agg('mean') ASSIGN=A.groupby(['Player Name'])['Value'].agg('mean').ASSIGN(method='min',ascending=False) ASSIGN=pd.DataFrame({'rank':rank,'value':value}) B['type'+str(i)]=str(ASSIGN[i]) B['ASSIGN'+str(i)]=str(ASSIGN[ASSIGN.index==player].iloc[0,0])+""path""+str(max(ASSIGN)) B['ASSIGN'+str(i)]=str(ASSIGN[ASSIGN.index==player].iloc[0,1]) return B",0,"['def TOTAvgRank_DD_DAP_TE_TB(player,month):\n', ""    Ranktype=['Driving Distance - (TOTAL DRIVES)','Driving Accuracy Percentage - (%)','Total Eagles - (TOTAL)','Total Birdies - (TOTAL)']\n"", '    B={}\n', '    for i in range(len(Ranktype)):\n', '        A=data_2019[data_2019.Variable==Ranktype[i]]\n', '        A=A[A.month==month]\n', ""        A.dropna(subset=['Value'])\n"", '        A.Value=A.Value.astype(float)\n', ""        value=A.groupby(['Player Name'])['Value'].agg('mean')\n"", ""        rank=A.groupby(['Player Name'])['Value'].agg('mean').rank(method='min',ascending=False)\n"", ""        new=pd.DataFrame({'rank':rank,'value':value})\n"", ""        B['type'+str(i)]=str(Ranktype[i])\n"", '        B[\'rank\'+str(i)]=str(new[new.index==player].iloc[0,0])+""/""+str(max(rank))\n', ""        B['value'+str(i)]=str(new[new.index==player].iloc[0,1])\n"", '\n', '    return B\n']"
ASSIGN = pd.read_csv('..path') ASSIGN.head(),0,"[""data = pd.read_csv('../input/bank-customer-retirement/Bank_Customer_retirement.csv')\n"", 'data.head()']"
CHECKPOINT tempData.shape,0,['tempData.shape']
"ASSIGN= cbdr.groupby(['STATEpath','DISTRICT','Year'])['Robbery'].sum().reset_index().sort_values(by='Robbery',ascending=False) ASSIGN.head(10).style.background_gradient(cmap='Oranges')",0,"[""s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Robbery'].sum().reset_index().sort_values(by='Robbery',ascending=False)\n"", ""s.head(10).style.background_gradient(cmap='Oranges')""]"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") ASSIGN.head() ASSIGN.dropna(axis=1, how='all') print(ASSIGN.head()) print(ASSIGN.shape) print(ASSIGN[:50].mean()) print(ASSIGN[ASSIGN['Sex']==1].mean()) print(ASSIGN['Fare'].max())",0,"['#Q2 Load the titanic dataset, remove missing values from all attributes, find mean value of first 50 samples, find the mean of the number of male passengers( Sex=1) on the ship, find the highest fare paid by any passenger.\n', 'import pandas as pd\n', 'df = pd.read_csv(""../input/titanic/train_and_test2.csv"")\n', 'df.head()\n', '\n', ""df.dropna(axis=1, how='all')\n"", 'print(df.head())\n', 'print(df.shape)\n', '\n', 'print(df[:50].mean())\n', '\n', ""print(df[df['Sex']==1].mean())\n"", '\n', ""print(df['Fare'].max())""]"
"sns.countplot(x = 'Pclass', data = df1)",1,"[""sns.countplot(x = 'Pclass', data = df1)""]"
my_submission.head(),0,['my_submission.head()']
final['Cabin_final'].value_counts(),0,"[""final['Cabin_final'].value_counts()""]"
"ggplot(recipes, aes(x='calories', y='dessert')) + geom_point() + \ stat_smooth(method=""lm"", color='blue')",1,"[""ggplot(recipes, aes(x='calories', y='dessert')) + geom_point() + \\\n"", 'stat_smooth(method=""lm"", color=\'blue\')']"
"SETUP plt.figure(figsize=(24,10)) plt.subplot(1,2,1) ASSIGN=[] ASSIGN=[] ASSIGN=[] for i in range(9): r1,r2=CHINA['rank '+str(i+2010)].split('path') R=float(r1)path(r2) R=1-R ASSIGN.append(1.5+R*math.sin(0+i*2*math.pipath)) ASSIGN.append(1.5+R*math.cos(0+i*2*math.pipath)) ASSIGN.append('rank '+str(i+2010)+' '+CHINA['rank '+str(i+2010)]) ASSIGN.append(ASSIGN[0]) ASSIGN.append(ASSIGN[0]) plt.plot(ASSIGN,ASSIGN, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2) for i, txt in enumerate(ASSIGN): plt.annotate(txt, (ASSIGN[i], ASSIGN[i])) plt.fill(ASSIGN, ASSIGN,""coral"") plt.xlim(0.45,2.7) plt.ylim(0.45,2.7) plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2) plt.title(""CHINA's export rank by year"",size=18) plt.subplot(1,2,2) ASSIGN=[] ASSIGN=[] ASSIGN=[] for i in range(9): r1,r2=CHINA['rank '+str(i+2010)].split('path') R=float(r1)path(r2) R=1-R ASSIGN.append(1.5+R*math.sin(0+i*2*math.pipath)) ASSIGN.append(1.5+R*math.cos(0+i*2*math.pipath)) ASSIGN.append('rank '+str(i+2010)+' '+CHINA['rank '+str(i+2010)]) ASSIGN.append(ASSIGN[0]) ASSIGN.append(ASSIGN[0]) plt.plot(ASSIGN,ASSIGN, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2) for i, txt in enumerate(ASSIGN): plt.annotate(txt, (ASSIGN[i], ASSIGN[i])) plt.xlim(0.45,2.7) plt.ylim(0.45,2.7) plt.fill(ASSIGN, ASSIGN,""plum"") plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2) plt.title(""CHINA's import rank by year"",size=18)",1,"['plt.figure(figsize=(24,10))\n', 'plt.subplot(1,2,1)\n', ""CHINA=rank_export('CHINA P RP')\n"", 'y=[]\n', 'x=[]\n', 'n=[]\n', 'for i in range(9):\n', ""    r1,r2=CHINA['rank '+str(i+2010)].split('/')\n"", '    R=float(r1)/float(r2)\n', '    R=1-R\n', '    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n', '    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n', ""    n.append('rank '+str(i+2010)+' '+CHINA['rank '+str(i+2010)])\n"", '    \n', 'x.append(x[0])\n', 'y.append(y[0])\n', ""plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n"", 'for i, txt in enumerate(n):\n', '    plt.annotate(txt, (x[i], y[i]))\n', '    plt.fill(x, y,""coral"")\n', '    plt.xlim(0.45,2.7)\n', '    plt.ylim(0.45,2.7)\n', ""    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n"", '    plt.title(""CHINA\'s export rank by year"",size=18)\n', '    \n', 'plt.subplot(1,2,2)\n', ""CHINA=rank_import('CHINA P RP')\n"", 'y=[]\n', 'x=[]\n', 'n=[]\n', 'for i in range(9):\n', ""    r1,r2=CHINA['rank '+str(i+2010)].split('/')\n"", '    R=float(r1)/float(r2)\n', '    R=1-R\n', '    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n', '    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n', ""    n.append('rank '+str(i+2010)+' '+CHINA['rank '+str(i+2010)])\n"", '    \n', 'x.append(x[0])\n', 'y.append(y[0])\n', ""plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n"", 'for i, txt in enumerate(n):\n', '    plt.annotate(txt, (x[i], y[i]))\n', '    plt.xlim(0.45,2.7)\n', '    plt.ylim(0.45,2.7)\n', '    plt.fill(x, y,""plum"")\n', ""    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n"", '    plt.title(""CHINA\'s import rank by year"",size=18)   ']"
"CHECKPOINT ASSIGN = pd.merge(ASSIGN, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left') ASSIGN.rename(columns={'Seed':'Seed1'}, inplace=True) ASSIGN = ASSIGN.drop('TeamID', axis=1) ASSIGN = pd.merge(ASSIGN, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left') ASSIGN.rename(columns={'Seed':'Seed2'}, inplace=True) ASSIGN = ASSIGN.drop('TeamID', axis=1) ASSIGN = pd.merge(ASSIGN, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left') ASSIGN.rename(columns={'Score':'ScoreT1'}, inplace=True) ASSIGN = ASSIGN.drop('TeamID', axis=1) ASSIGN = pd.merge(ASSIGN, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left') ASSIGN.rename(columns={'Score':'ScoreT2'}, inplace=True) ASSIGN = ASSIGN.drop('TeamID', axis=1) test_df",0,"[""test_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n"", ""test_df.rename(columns={'Seed':'Seed1'}, inplace=True)\n"", ""test_df = test_df.drop('TeamID', axis=1)\n"", ""test_df = pd.merge(test_df, tourney_seed, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n"", ""test_df.rename(columns={'Seed':'Seed2'}, inplace=True)\n"", ""test_df = test_df.drop('TeamID', axis=1)\n"", ""test_df = pd.merge(test_df, season_score, left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'], how='left')\n"", ""test_df.rename(columns={'Score':'ScoreT1'}, inplace=True)\n"", ""test_df = test_df.drop('TeamID', axis=1)\n"", ""test_df = pd.merge(test_df, season_score, left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], how='left')\n"", ""test_df.rename(columns={'Score':'ScoreT2'}, inplace=True)\n"", ""test_df = test_df.drop('TeamID', axis=1)\n"", 'test_df']"
"ASSIGN=df_final[""CITY""].value_counts().sort_values(ascending=True)",0,"['#lets check city wise restaurant counts and save it in ascending order\n', '\n', 'city_vs_count=df_final[""CITY""].value_counts().sort_values(ascending=True)']"
"CHECKPOINT ASSIGN = XGBClassifier(colsample_bytree = .6,eta = 0.5,gamma = 1,max_depth = 5,min_child_weight = 6,subsample = 1) Estimator.append(('ASSIGN',XGBClassifier(colsample_bytree = .6,eta = 0.5,gamma = 1,max_depth = 5,min_child_weight = 6,subsample = 1))) ASSIGN = cross_val_score(xgb,x_train,x_test,ASSIGN=10) Accuracy8 = ASSIGN.mean() Accuracy.append(Accuracy8) print(ASSIGN) print(ASSIGN.mean())",0,"['xgb = XGBClassifier(colsample_bytree = .6,eta = 0.5,gamma = 1,max_depth = 5,min_child_weight = 6,subsample = 1)\n', ""Estimator.append(('xgb',XGBClassifier(colsample_bytree = .6,eta = 0.5,gamma = 1,max_depth = 5,min_child_weight = 6,subsample = 1)))\n"", 'cv = cross_val_score(xgb,x_train,x_test,cv=10)\n', 'Accuracy8 = cv.mean()\n', 'Accuracy.append(Accuracy8)\n', 'print(cv)\n', 'print(cv.mean())']"
relevant.info(),0,['relevant.info()']
SETUP,0,"[""# We import required lib's\n"", 'from fastai.vision import *\n', 'import os']"
"sns.catplot(x='Year', y='Murder', data=total ,height = 5, aspect = 4,kind = 'bar')",1,"[""sns.catplot(x='Year', y='Murder', data=total ,height = 5, aspect = 4,kind = 'bar')""]"
ASSIGN=ASSIGN[ASSIGN.GEO_LAT>39],0,['data=data[data.GEO_LAT>39]']
ASSIGN=5 ASSIGN=X_train[image_no] plt.imshow(ASSIGN),1,"['##Seeing example of random images\n', 'image_no=5\n', 'img=X_train[image_no]\n', 'plt.imshow(img)']"
"SETUP ASSIGN = plt.figure(figsize = (10,10)) plt.scatter(pcs_df.PC1, pcs_df.PC2) plt.xlabel('Principal Component 1') plt.ylabel('Principal Component 2') for i, txt in enumerate(pcs_df.Feature): plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i])) plt.tight_layout() plt.show()",1,"['%matplotlib inline\n', 'fig = plt.figure(figsize = (10,10))\n', 'plt.scatter(pcs_df.PC1, pcs_df.PC2)\n', ""plt.xlabel('Principal Component 1')\n"", ""plt.ylabel('Principal Component 2')\n"", 'for i, txt in enumerate(pcs_df.Feature):\n', '    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\n', 'plt.tight_layout()\n', 'plt.show()']"
SETUP CHECKPOINT print(tf.__version__),0,"['import tensorflow as tf\n', 'print(tf.__version__)']"
del X_test['ForecastId'] del X_test['Date'] del X_test['Province'] del X_test['Country'],0,"[""del X_test['ForecastId']\n"", ""del X_test['Date']\n"", ""del X_test['Province']\n"", ""del X_test['Country']""]"
"ASSIGN = model.fit( x_train, y_label, ASSIGN=1000, ASSIGN=20, ASSIGN=0.2, ASSIGN=2, ASSIGN=[lr_reduction] )",0,"['history = model.fit(\n', '    x_train,\n', '    y_label,\n', '    batch_size=1000,\n', '    epochs=20,\n', '    validation_split=0.2,\n', '    verbose=2,\n', '    callbacks=[lr_reduction]\n', ')']"
"data_features[['MSSubClass', 'MSZoning']]",0,"[""data_features[['MSSubClass', 'MSZoning']]""]"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
CHECKPOINT print(data.isnull().sum()),0,['print(data.isnull().sum())']
pip install xmltodict,0,['pip install xmltodict']
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") ASSIGN = train.drop(""species"",axis=1) ASSIGN = train[""species""] X_train, X_test, y_train, y_test = train_test_split(ASSIGN, ASSIGN, test_size = 0.3, random_state = 101) ASSIGN = LogisticRegression() ASSIGN.fit(X_train,y_train) ASSIGN = logmodel.predict(X_test) print(,f1_score(y_test, ASSIGN,average='macro')) print(,f1_score(y_test, ASSIGN,average='micro')) print(,f1_score(y_test, ASSIGN,average='weighted')) print() confusion_matrix(y_test, ASSIGN)",0,"['#Q4 Load the iris dataset, print the confusion matrix and f1_score as computed on the features.\n', 'import pandas as pd\n', 'import numpy as np\n', 'from sklearn.model_selection import train_test_split\n', 'from sklearn.metrics import confusion_matrix\n', 'from sklearn.model_selection import cross_val_predict\n', 'from sklearn.linear_model import LogisticRegression\n', 'from sklearn.metrics import f1_score\n', '\n', 'train = pd.read_csv(""../input/iris-flower-dataset/IRIS.csv"")\n', '\n', '\n', 'X = train.drop(""species"",axis=1)\n', 'y = train[""species""]\n', '\n', 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n', '\n', 'logmodel = LogisticRegression()\n', 'logmodel.fit(X_train,y_train)\n', '\n', 'predictions = logmodel.predict(X_test)\n', '\n', 'print(""F1 Score(macro):"",f1_score(y_test, predictions,average=\'macro\'))\n', 'print(""F1 Score(micro):"",f1_score(y_test, predictions,average=\'micro\'))\n', 'print(""F1 Score(weighted):"",f1_score(y_test, predictions,average=\'weighted\'))\n', 'print(""\\nConfusion Matrix(below):\\n"")\n', 'confusion_matrix(y_test, predictions)']"
"ASSIGN = Sequential() ASSIGN.add(Dense(1024,input_dim=input_dim)) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Activation('sigmoid')) ASSIGN.add(Dense(512)) ASSIGN.add(Dropout(0.3)) ASSIGN.add(Activation('sigmoid')) ASSIGN.add(Dense(99)) ASSIGN.add(Activation('softmax'))",0,"['model = Sequential()\n', 'model.add(Dense(1024,input_dim=input_dim))\n', 'model.add(Dropout(0.2))\n', ""model.add(Activation('sigmoid'))\n"", 'model.add(Dense(512))\n', 'model.add(Dropout(0.3))\n', ""model.add(Activation('sigmoid'))\n"", 'model.add(Dense(99))\n', ""model.add(Activation('softmax'))""]"
"CHECKPOINT ASSIGN = model14.predict(x_es) ASSIGN = mean_squared_error(Y_es, y_pred14, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, ASSIGN)))",0,"['y_pred14 = model14.predict(x_es)\n', '\n', 'val = mean_squared_error(Y_es, y_pred14, squared=False)\n', 'val14 = str(round(val, 4))\n', '\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred14)))\n""]"
match_stats_df.date = pd.to_datetime(match_stats_df.date),0,"['#change date from string to datetime\n', 'match_stats_df.date = pd.to_datetime(match_stats_df.date)']"
"ASSIGN=pd.read_csv('..path') SLICE=ASSIGN.ObservationDate.apply(lambda x:datetime.datetime.strptime(str(x),'%mpath%dpath%Y').strftime('%Y-%m-%d')) ASSIGN.head()",0,"[""data=pd.read_csv('../input/novel-corona-virus-2019-dataset/covid_19_data.csv')\n"", ""data['date']=data.ObservationDate.apply(lambda x:datetime.datetime.strptime(str(x),'%m/%d/%Y').strftime('%Y-%m-%d'))\n"", 'data.head()']"
"X_TRAIN_FILE=""path"" X_TEST_FILE=""path"" Y_TRAIN_FILE=""path"" Y_TEST_FILE=""path""",0,"['X_TRAIN_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_X_train.npz""\n', 'X_TEST_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_X_test.npy""\n', 'Y_TRAIN_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_Y_train.npz""\n', 'Y_TEST_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_Y_test.npy""']"
"sns.relplot(x ='Total Atrocities', y ='Year', col = 'STATEpath', data = cbsr, height=3 ,col_wrap = 9)",1,"[""sns.relplot(x ='Total Atrocities', y ='Year', col = 'STATE/UT', data = cbsr, height=3 ,col_wrap = 9)""]"
"tumor_data['target'] = tumor_data['diagnosis'].replace({'B': 1, 'M': 0}) tumor_data[['id', 'diagnosis', 'target']].sample(5)",0,"[""tumor_data['target'] = tumor_data['diagnosis'].replace({'B': 1, 'M': 0})\n"", ""# Let's show if the convertion was rightly done\n"", ""tumor_data[['id', 'diagnosis', 'target']].sample(5)""]"
"ASSIGN=pd.read_csv(""..path"") ASSIGN.head() ASSIGN=pd.read_csv(""..path"") ASSIGN.head()",0,"['latnigrin=pd.read_csv(""../input/ireland-historical-news/w3-latnigrin-text.csv"")\n', 'latnigrin.head()\n', 'irishtimes=pd.read_csv(""../input/ireland-historical-news/irishtimes-date-text.csv"")\n', 'irishtimes.head()']"
"stack.score(X, train_y)",0,"['stack.score(X, train_y)']"
ASSIGN = clf.predict(test_embeddings) ASSIGN = clf.predict_proba(test_embeddings),0,"['labels = clf.predict(test_embeddings)\n', 'proba = clf.predict_proba(test_embeddings)']"
"ASSIGN = plt.subplots(1,2,figsize=(20,5)) sns.distplot(data.groupby(""target_label"").size(), ax=ax[0], color=""Orange"", kde=False) ax[0].set_xlabel(""Number of images"") ax[0].set_ylabel(""Frequency""); sns.countplot(data.target, palette=""Set2"", ax=ax[1]); ax[1].set_xlabel(""Names of Class"") ax[1].set_title(""Data Distribution"");",1,"['# cancer_perc = data.groupby(""patient_id"").target.value_counts()/ data.groupby(""patient_id"").target.size()\n', '# cancer_perc = cancer_perc.unstack()\n', '\n', 'fig, ax = plt.subplots(1,2,figsize=(20,5))\n', 'sns.distplot(data.groupby(""target_label"").size(), ax=ax[0], color=""Orange"", kde=False)\n', 'ax[0].set_xlabel(""Number of images"")\n', 'ax[0].set_ylabel(""Frequency"");\n', '\n', 'sns.countplot(data.target, palette=""Set2"", ax=ax[1]);\n', 'ax[1].set_xlabel(""Names of Class"")\n', 'ax[1].set_title(""Data Distribution"");']"
"SETUP CHECKPOINT print(classification_report(y_test, predict_test))",0,"['from sklearn.metrics import classification_report\n', 'print(classification_report(y_test, predict_test))']"
len(data),0,['len(data)']
"ASSIGN = np.float32(train.loc[:, 'healthy':'scab'].values) ASSIGN = train_test_split(train, test_size = 0.15)",0,"[""train_labels = np.float32(train.loc[:, 'healthy':'scab'].values)\n"", '\n', 'train, val = train_test_split(train, test_size = 0.15)\n']"
"SETUP ''' ASSIGN = ['l1','l2'] ASSIGN = ['hinge','squared_hinge'] ASSIGN = ['dict','balanced','None'] ASSIGN = [.1,1,10,50,100,150] ASSIGN = {'penalty':['l1','l2'],'loss':['hinge','squared_hinge'],'class_weight':['dict','balanced','None'] ,'C': [.1,1,10,50,100,150]} ASSIGN = GridSearchCV(SVM, parameters, scoring='accuracy' ,cv =10) ASSIGN.fit(x_train, x_test) ASSIGN.best_params_ '''",0,"[""'''\n"", ""penalty = ['l1','l2']\n"", ""loss = ['hinge','squared_hinge']\n"", ""class_weight = ['dict','balanced','None']\n"", 'C = [.1,1,10,50,100,150]\n', '\n', 'SVM = LinearSVC()\n', '\n', ""parameters = {'penalty':['l1','l2'],'loss':['hinge','squared_hinge'],'class_weight':['dict','balanced','None'] ,'C': [.1,1,10,50,100,150]}\n"", '\n', ""SVM_classifier = GridSearchCV(SVM, parameters, scoring='accuracy' ,cv =10)\n"", 'SVM_classifier.fit(x_train, x_test)\n', 'SVM_classifier.best_params_\n', ""'''""]"
ASSIGN=data[data.neighbourhood_group=='Manhattan'] ASSIGN.head(),0,"[""data_manha=data[data.neighbourhood_group=='Manhattan']\n"", 'data_manha.head()']"
"SETUP ASSIGN = [0.1, 1, 10, 100, 1000] ASSIGN = [1, 0.1, 0.01, 0.001] ASSIGN = GridSearchCV(SVC(), {'C': C_values, 'gamma': gamma_values, 'kernel': ['rbf']}, refit = True, verbose = 4) ASSIGN.fit(X_train_scaled, Y_train) ASSIGN.best_params_",0,"['# We can automate the refinement of C and gamma using the GridSearchCV library\n', 'from sklearn.model_selection import GridSearchCV\n', 'C_values =  [0.1, 1, 10, 100, 1000]\n', 'gamma_values = [1, 0.1, 0.01, 0.001]\n', ""grid = GridSearchCV(SVC(), {'C': C_values, 'gamma': gamma_values, 'kernel': ['rbf']}, refit = True, verbose = 4)\n"", '# Find best pair of C and gamma values\n', 'grid.fit(X_train_scaled, Y_train)\n', 'grid.best_params_']"
"CHECKPOINT for name in models.keys(): ASSIGN = models[name] ASSIGN.fit(X_train, y_train) ASSIGN = cv_rmse(model,X_train, y_train) print(ASSIGN.mean)",0,"['for name in models.keys():\n', '    model = models[name]\n', '    model.fit(X_train, y_train)\n', '    scores = cv_rmse(model,X_train, y_train)\n', '    print(scores.mean)']"
"ASSIGN=[] ASSIGN=train.id[:10000] ASSIGN=train.category_id[:10000] for file in ASSIGN: ASSIGN=cv2.imread(""..path""+file+'.jpg') ASSIGN=cv2.resize(image,(32,32)) ASSIGN.append(ASSIGN) ASSIGN=np.array(ASSIGN)",0,"['img=[]\n', 'filename=train.id[:10000]\n', 'label=train.category_id[:10000]\n', 'for file in filename:\n', '    image=cv2.imread(""../input/iwildcam-2019-fgvc6/train_images/""+file+\'.jpg\')\n', '    res=cv2.resize(image,(32,32))\n', '    img.append(res)\n', 'img=np.array(img)']"
"ASSIGN = df['GRE Score'] sns.distplot(ASSIGN , kde= True,rug = False, bins = 30)",1,"[""x = df['GRE Score']\n"", 'sns.distplot(x , kde= True,rug = False, bins = 30)']"
"ASSIGN = [] for x in df: ASSIGN.append(df[x].values) ASSIGN = [] for i in range(0,3): ASSIGN = [x for x in all[i] if str(x) != 'nan'] ASSIGN.append(ASSIGN) ASSIGN = [] for z in range(len(ASSIGN)): for y in ASSIGN[z]: ASSIGN.append(y)",0,"['## Remove all ""nan"" values for blank cells\n', 'all = []\n', 'for x in df:\n', '    all.append(df[x].values)\n', '\n', 'cData = []\n', 'for i in range(0,3):\n', ""    cleanedList = [x for x in all[i] if str(x) != 'nan']\n"", '    cData.append(cleanedList)\n', 'finalData = []\n', 'for z in range(len(cData)):\n', '    for y in cData[z]:\n', '        finalData.append(y)\n', '#finalData is the cleaned data without nan values, we still have to clear the trailing decimal points and zeroes\n']"
"ASSIGN = {} for id, label in zip(pred_img_ids, pred_labels_expanded): ASSIGN[id] = label",0,"['id_label_dict = {}\n', 'for id, label in zip(pred_img_ids, pred_labels_expanded):\n', '    id_label_dict[id] = label']"
"ASSIGN = (data_features['BsmtFinType1'].notnull() & data_features['BsmtCond'].isnull()) data_features.loc[ASSIGN,'BsmtCond'] = 'TA'",0,"[""condition3 = (data_features['BsmtFinType1'].notnull() & data_features['BsmtCond'].isnull())\n"", ""data_features.loc[condition3,'BsmtCond'] = 'TA'""]"
ASSIGN = ClassificationInterpretation.from_learner(learn) ASSIGN.plot_confusion_matrix(),1,"['interp = ClassificationInterpretation.from_learner(learn)\n', 'interp.plot_confusion_matrix()']"
"ASSIGN = 5 ASSIGN = ImageData(is_train=False) ASSIGN = num_images_to_show ASSIGN = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True) ASSIGN = next(iter(valid_dataloader)).to(device) ASSIGN = next(iter(valid_dataloader)).to(device)",0,"['num_images_to_show = 5\n', '\n', '\n', 'valid_dataset = ImageData(is_train=False)\n', 'batch_size = num_images_to_show\n', 'valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n', 'valid_batch = next(iter(valid_dataloader)).to(device)\n', 'valid_batch_1 = next(iter(valid_dataloader)).to(device)\n']"
SETUP,0,"['import numpy as np\n', 'import pandas as pd \n', 'import re #library to clean data\n', 'import nltk #Natural Language tool kit\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns \n', 'import os \n', 'import datetime\n', 'from nltk.corpus import stopwords #to remove stopword\n', 'from nltk.stem.porter import PorterStemmer \n', 'from PIL import Image\n', 'from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n']"
"ASSIGN = len(train) ASSIGN = len(train)+len(test)-1 ASSIGN = f'AR(12) Predictions' ASSIGN = ARfit.predict(start=start,end=end).rename(rename)",0,"['start = len(train)\n', 'end = len(train)+len(test)-1\n', ""rename = f'AR(12) Predictions'\n"", '\n', 'predictions11 = ARfit.predict(start=start,end=end).rename(rename)']"
"CHECKPOINT ASSIGN = KNeighborsClassifier(n_neighbors = 10) ASSIGN.fit(train_x, train_y) ASSIGN = knn.predict(test_x) ASSIGN = knn.score(train_x, train_y) print('training accuracy: %.5f' % ASSIGN) ASSIGN =knn.predict(test_x) X=accuracy_score(test_y, ASSIGN) print('test accuracy: %.5f' % X) ASSIGN = metrics.roc_curve(test_y, predict_y, pos_label=1) print('auc: %.5f' % metrics.auc(fpr, tpr)) ASSIGN = cross_val_score(knn,train_x,train_y,cv=5,scoring='accuracy') print('average of Cross validation: %.5f'%ASSIGN.mean()) print('standard deviation of Cross validation: %.5f'%ASSIGN.std(ddof=1))",0,"['# knn\n', '\n', 'knn = KNeighborsClassifier(n_neighbors = 10)\n', 'knn.fit(train_x, train_y)\n', 'predict_y = knn.predict(test_x)\n', 'acc_knn = knn.score(train_x, train_y)\n', ""print('training accuracy: %.5f' % acc_knn)\n"", '\n', 'predict_y =knn.predict(test_x)\n', 'X=accuracy_score(test_y, predict_y)\n', ""print('test accuracy: %.5f' % X)\n"", '\n', '#auc\n', 'fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n', ""print('auc: %.5f' % metrics.auc(fpr, tpr))\n"", '\n', '#Cross validation\n', ""scores = cross_val_score(knn,train_x,train_y,cv=5,scoring='accuracy')\n"", ""print('average of Cross validation: %.5f'%scores.mean())\n"", ""print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))""]"
"SETUP CHECKPOINT warnings.filterwarnings(""ignore"") ASSIGN = pd.read_csv('..path') print(ASSIGN.head())",0,"['import numpy as np\n', 'import pandas as pd\n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline\n', 'import seaborn as sns\n', 'import warnings\n', 'import seaborn as sns\n', 'warnings.filterwarnings(""ignore"")\n', '\n', ""vg_sales = pd.read_csv('../input/vgsales.csv')\n"", 'print(vg_sales.head())']"
"SETUP def build_model(): """"""build model function"""""" ASSIGN = Input(shape=(W, H, 3)) ASSIGN = MobileNetV2( ASSIGN=False, ASSIGN='imagenet', ASSIGN=ASSIGN, ) ASSIGN = Sequential() ASSIGN.add(GlobalAveragePooling2D()) ASSIGN.add(Dense(n_classes, activation='softmax')) ASSIGN = Model(input=densenet_121.input, output=top_model(densenet_121.output)) for layer in ASSIGN.layers[:-11]: layer.trainable = False or isinstance(layer, BatchNormalization) ASSIGN.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) return model",0,"['import keras\n', 'import tensorflow as tf\n', 'from keras.applications.mobilenet_v2 import MobileNetV2\n', '\n', 'def build_model():\n', '    """"""build model function""""""\n', '    \n', '    # Resnet\n', '    input_tensor = Input(shape=(W, H, 3)) # To change input shape\n', '    densenet_121 = MobileNetV2(\n', '        include_top=False,                # To change output shape\n', ""        weights='imagenet',               # Use pre-trained model\n"", '        input_tensor=input_tensor,        # Change input shape for this task\n', '    )\n', '    \n', '    # fc layer\n', '    top_model = Sequential()\n', '    top_model.add(GlobalAveragePooling2D())               # Add GAP for cam\n', ""    top_model.add(Dense(n_classes, activation='softmax')) # Change output shape for this task\n"", '    \n', '    # model\n', '    model = Model(input=densenet_121.input, output=top_model(densenet_121.output))\n', '    \n', '    # frozen weights\n', '    for layer in model.layers[:-11]:\n', '        layer.trainable = False or isinstance(layer, BatchNormalization) # If Batch Normalization layer, it should be trainable\n', '        \n', '    # compile\n', ""    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"", '    \n', '    return model']"
SETUP,0,"['import numpy as np\n', 'import pandas as pd\n', 'import os\n', 'import matplotlib.pyplot as plt\n', 'from glob import glob as gb']"
SETUP CHECKPOINT ASSIGN = np.array(img) path print(ASSIGN.shape),0,"['import numpy as np\n', 'test_x = np.array(img) / 255.0\n', 'print(test_x.shape)']"
"match_lineup_df.drop('index', axis=1, inplace=True)",0,"[""match_lineup_df.drop('index', axis=1, inplace=True)""]"
"CHECKPOINT print( % (tumor_data.shape[0], tumor_data.shape[1]))",0,"['print(""The dataset has %d rows and %d columns"" % (tumor_data.shape[0], tumor_data.shape[1]))']"
ASSIGN = data.corr(),0,['correlation = data.corr()']
"ASSIGN='netgain' ASSIGN = ['realtionship_status', 'industry', 'genre', 'targeted_sex', 'airtime', 'airlocation', 'expensive', 'money_back_guarantee'] ASSIGN = ['average_runtime(minutes_per_week)','ratings'] ASSIGN= [Categorify,Normalize] ASSIGN = TabularList.from_df(test,cat_names=cat, cont_names=cont , procs=procs) ASSIGN = (TabularList.from_df(train, cat_names=cat, cont_names=cont , procs=procs) .split_subsets(train_size=0.8, valid_size=0.2, seed=34) .label_from_df(cols=ASSIGN) .add_test(ASSIGN) .databunch())",0,"[""dep_var='netgain'\n"", ""cat = ['realtionship_status', 'industry', 'genre', 'targeted_sex', 'airtime', 'airlocation',\n"", ""       'expensive', 'money_back_guarantee']\n"", ""cont = ['average_runtime(minutes_per_week)','ratings']\n"", 'procs= [Categorify,Normalize]\n', '\n', 'inception = TabularList.from_df(test,cat_names=cat, cont_names=cont , procs=procs)\n', 'data = (TabularList.from_df(train, cat_names=cat, cont_names=cont , procs=procs)\n', '                .split_subsets(train_size=0.8, valid_size=0.2, seed=34)\n', '                .label_from_df(cols=dep_var)\n', '                .add_test(inception)\n', '                .databunch())']"
model.save('mnist_cnn.h5'),0,"['# Save Model\n', ""model.save('mnist_cnn.h5')""]"
"ASSIGN = m.predict(future) ASSIGN[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()",0,"['forecastD = m.predict(future)\n', ""forecastD[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()""]"
"SETUP ASSIGN=['O+','A+','B+','AB+','O-','A-','B-','AB-'] ASSIGN=[9,12,2,10,7,1,4,5] plt.plot(ASSIGN,ASSIGN,'r--',ASSIGN,ASSIGN,'g^') plt.title('Blood group distribution of 50 patients') plt.ylabel('No of Patients') plt.xlabel('Blood group') plt.show()",1,"['from matplotlib import pyplot as plt\n', ""x=['O+','A+','B+','AB+','O-','A-','B-','AB-']\n"", 'y=[9,12,2,10,7,1,4,5]\n', ""plt.plot(x,y,'r--',x,y,'g^')\n"", ""plt.title('Blood group distribution of 50 patients')\n"", ""plt.ylabel('No of Patients')\n"", ""plt.xlabel('Blood group')\n"", 'plt.show()']"
"CHECKPOINT ASSIGN=Sequential() ASSIGN.add(Convolution2D(filters=32,kernel_size=(3,3),input_shape=(32,32,3),padding='same')) ASSIGN.add(BatchNormalization()) ASSIGN.add(Activation('relu')) ASSIGN.add(Dropout(rate=0.35)) ASSIGN.add(MaxPool2D(pool_size=(2,2),padding='same')) ASSIGN.add(Convolution2D(filters=64,kernel_size=(3,3),padding='same')) ASSIGN.add(BatchNormalization()) ASSIGN.add(Activation('relu')) ASSIGN.add(Dropout(rate=0.45)) ASSIGN.add(MaxPool2D(pool_size=(2,2),padding='same')) ASSIGN.add(Flatten()) ASSIGN.add(Dense(1024,activation='relu')) ASSIGN.add(BatchNormalization()) ASSIGN.add(Activation('relu')) ASSIGN.add(Dropout(rate=0.75)) ASSIGN.add(Dense(500,activation='softmax')) ASSIGN.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) ASSIGN=model.fit(X_train,y_train,validation_split=0.2,epochs=100,batch_size=128,verbose=1) ASSIGN=model.evaluate(X_test,y_test,verbose=1) print(,accuracy[1])#accuracy for test set def show_train_history(ASSIGN,train,validation): plt.plot(ASSIGN.history[train]) plt.plot(ASSIGN.history[validation]) plt.title('Train History') plt.ylabel('train') plt.xlabel('Epoch') plt.legend(['train','validation'],loc='upper left') plt.show() show_train_history(ASSIGN,'acc','val_acc')",1,"['model=Sequential()\n', ""model.add(Convolution2D(filters=32,kernel_size=(3,3),input_shape=(32,32,3),padding='same'))\n"", 'model.add(BatchNormalization())\n', ""model.add(Activation('relu'))\n"", 'model.add(Dropout(rate=0.35))\n', '\n', ""model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n"", '\n', ""model.add(Convolution2D(filters=64,kernel_size=(3,3),padding='same'))\n"", 'model.add(BatchNormalization())\n', ""model.add(Activation('relu'))\n"", 'model.add(Dropout(rate=0.45))\n', '\n', ""model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n"", '\n', 'model.add(Flatten())\n', '\n', ""model.add(Dense(1024,activation='relu'))\n"", 'model.add(BatchNormalization())\n', ""model.add(Activation('relu'))\n"", 'model.add(Dropout(rate=0.75))\n', '\n', ""model.add(Dense(500,activation='softmax'))\n"", '\n', ""model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n"", '\n', 'train_history=model.fit(X_train,y_train,validation_split=0.2,epochs=100,batch_size=128,verbose=1)\n', 'accuracy=model.evaluate(X_test,y_test,verbose=1)\n', 'print(""test accuracy:"",accuracy[1])#accuracy for test set\n', '\n', '\n', '\n', 'def show_train_history(train_history,train,validation):\n', '\tplt.plot(train_history.history[train])\n', '\tplt.plot(train_history.history[validation])\n', ""\tplt.title('Train History')\n"", ""\tplt.ylabel('train')\n"", ""\tplt.xlabel('Epoch')\n"", ""\tplt.legend(['train','validation'],loc='upper left')\n"", '\tplt.show()\n', '\n', ""show_train_history(train_history,'acc','val_acc') #acc:accuracy for training set. val_acc:accuracy for validation.""]"
CHECKPOINT def column_nan(missing): ASSIGN = [] for i in missing: if i > 0: ASSIGN.append(i) print(len(ASSIGN)) ASSIGN = trainset.isnull().sum().sort_values(ascending = False) ASSIGN = testset.isnull().sum().sort_values(ascending = False),0,"['#build function to count caolumn that has missing value\n', 'def column_nan(missing):\n', '    s = []\n', '    for i in missing:\n', '        if i > 0:\n', '            s.append(i)\n', '    print(len(s))\n', 'train = trainset.isnull().sum().sort_values(ascending = False)\n', 'test = testset.isnull().sum().sort_values(ascending = False)']"
SETUP,0,"['import os\n', 'import argparse\n', 'import pickle\n', 'import time\n', '\n', ""import numpy as np; np.seterr(invalid='ignore')\n"", 'import pandas as pd\n', 'from sklearn.preprocessing import StandardScaler\n', '\n', 'import torch\n', 'import torch.nn as nn\n', 'from torch.autograd import Variable\n', 'import torch.optim as optim\n', 'from torch.optim.lr_scheduler import MultiStepLR\n', 'from torch.utils.data import TensorDataset, DataLoader']"
ASSIGN=pd.read_csv('..path') ASSIGN.head(),0,"[""data=pd.read_csv('../input/new-york-city-airbnb-open-data/AB_NYC_2019.csv')\n"", 'data.head()']"
"ASSIGN=df_map.merge(df_sec,on=""CITY"",how=""left"")",0,"['#merging this data frame with df_sec data frame(which we created using city names,longitude and latitude)\n', 'df_map_final=df_map.merge(df_sec,on=""CITY"",how=""left"")']"
"ASSIGN=pd.DataFrame(test_transaction,columns=train_transaction.isnull().sum().sort_values()[:250].index) del test_transaction del train_transaction ID=ASSIGN.TransactionID ASSIGN=ASSIGN.drop(columns=['TransactionID','isFraud']) ASSIGN.head()",0,"['test_transaction_new=pd.DataFrame(test_transaction,columns=train_transaction.isnull().sum().sort_values()[:250].index)\n', 'del test_transaction\n', 'del train_transaction\n', 'ID=test_transaction_new.TransactionID\n', ""test_transaction_new=test_transaction_new.drop(columns=['TransactionID','isFraud'])\n"", 'test_transaction_new.head()']"
"def get_pred_for_province(country,province): ASSIGN = datetime.strptime('2020-04-13','%Y-%m-%d') ASSIGN = datetime.strptime('2020-04-26','%Y-%m-%d') ASSIGN = [] ASSIGN = get_initial_input(country,province,str(start_date)[0:10],str(end_date)[0:10]) for i in range(0,19): ASSIGN = str(start_date+timedelta(days = i))[0:10] ASSIGN = str(end_date+timedelta(days = i))[0:10] ASSIGN = get_pred(country,province,trend_input,stable_input) ASSIGN.append([ASSIGN,original_output[0],original_output[1]]) ASSIGN = ASSIGN[1:] ASSIGN = torch.as_tensor(output) ASSIGN = torch.as_tensor(output_tensor.reshape(1,1,2)) ASSIGN = torch.cat((ASSIGN,new),0) ASSIGN = pd.DataFrame(pred,columns=['Date','confirmed_pred','fata_pred']) ASSIGN['Province'] = province ASSIGN['Country'] = country ASSIGN = ASSIGN[['Country','Province','Date','confirmed_pred','fata_pred']] return pred_for_province",0,"['def get_pred_for_province(country,province):\n', ""    start_date = datetime.strptime('2020-04-13','%Y-%m-%d')\n"", ""    end_date = datetime.strptime('2020-04-26','%Y-%m-%d')\n"", '    pred = []\n', '    trend_input,stable_input = get_initial_input(country,province,str(start_date)[0:10],str(end_date)[0:10])\n', '    for i in range(0,19):\n', '        start = str(start_date+timedelta(days = i))[0:10]\n', '        end = str(end_date+timedelta(days = i))[0:10]\n', '        output,original_output = get_pred(country,province,trend_input,stable_input)\n', '        pred.append([end,original_output[0],original_output[1]])\n', '        trend_input = trend_input[1:]\n', '        output_tensor = torch.as_tensor(output)\n', '        new = torch.as_tensor(output_tensor.reshape(1,1,2))\n', '        trend_input = torch.cat((trend_input,new),0)\n', ""    pred_for_province = pd.DataFrame(pred,columns=['Date','confirmed_pred','fata_pred'])\n"", ""    pred_for_province['Province'] = province\n"", ""    pred_for_province['Country'] = country\n"", ""    pred_for_province = pred_for_province[['Country','Province','Date','confirmed_pred','fata_pred']]\n"", '    return pred_for_province']"
train.head(),0,['train.head()']
"CHECKPOINT ASSIGN = (1,1,2,3,5,8,21) print(ASSIGN)",0,"['tupple1 = (1,1,2,3,5,8,21)\n', 'print(tupple1)']"
CHECKPOINT ASSIGN = data_features.select_dtypes(include = ['object']).columns ASSIGN = data_features.select_dtypes(exclude = ['object']).columns print(ASSIGN) print('Categorial features :' + str(len(ASSIGN)) + '\n') print(ASSIGN) print('Numerical features :' + str(len(ASSIGN))),0,"[""cat_features = data_features.select_dtypes(include = ['object']).columns\n"", ""num_features = data_features.select_dtypes(exclude = ['object']).columns\n"", 'print(cat_features)\n', ""print('Categorial features :' + str(len(cat_features)) + '\\n')\n"", '\n', 'print(num_features)\n', ""print('Numerical features :' + str(len(num_features)))""]"
SETUP,0,['from sklearn.preprocessing import StandardScaler']
"CHECKPOINT ASSIGN = LogisticRegression() ASSIGN.fit(train_x, train_y) ASSIGN = cross_val_score(dtree,train_x,train_y,cv=5,scoring='accuracy') print('Cross validation: %.5f'%ASSIGN.mean()) print('standard deviation of Cross validation: %.5f'%ASSIGN.std(ddof=1))",0,"['#Logistic Regression\n', 'logreg = LogisticRegression()\n', 'logreg.fit(train_x, train_y)\n', '\n', '#Cross validation\n', ""scores = cross_val_score(dtree,train_x,train_y,cv=5,scoring='accuracy')\n"", ""print('Cross validation: %.5f'%scores.mean())\n"", ""print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))""]"
"def reload_data(): ASSIGN = pd.read_csv(""train.csv"") ASSIGN = pd.read_csv(""test.csv"") ASSIGN = [train_data, test_data] ASSIGN = pd.concat(ASSIGN, axis=0, sort=False) return train_data, test_data, df def load_pred(): ASSIGN = pd.read_csv(""predictions.csv"", index_col='id') return pred ASSIGN = reload_data() ASSIGN = reload_data()",0,"['# method to reload data\n', 'def reload_data():\n', '    # Load test & train datasets\n', '    train_data = pd.read_csv(""train.csv"")\n', '    test_data = pd.read_csv(""test.csv"")\n', '    df = [train_data, test_data]\n', '    df = pd.concat(df, axis=0, sort=False)\n', '    \n', '    return train_data, test_data, df\n', '\n', '# load predictions\n', 'def load_pred():\n', '    # id should be index\n', '    pred = pd.read_csv(""predictions.csv"", index_col=\'id\')\n', '    return pred\n', '\n', 'train_data, test_data, df = reload_data()\n', '# for backup reason\n', 'train_data_copy, test_data_copy, df = reload_data()']"
"ASSIGN = plt.subplots(figsize=(6,6)) ASSIGN = sns.cubehelix_palette(as_cmap = True, dark=0,light = 1,reverse=True) sns.kdeplot(x,y,ASSIGN=ASSIGN, n_levels = 60, shade= True)",1,"['f, ax = plt.subplots(figsize=(6,6))\n', 'cmap = sns.cubehelix_palette(as_cmap = True, dark=0,light = 1,reverse=True)\n', 'sns.kdeplot(x,y,cmap=cmap, n_levels = 60, shade= True)']"
"ASSIGN = train.isna().sum(axis=0) path[0] ASSIGN = test.isna().sum(axis=0) path[0] ASSIGN = (training_missing path).sort_values(ascending=False) ASSIGN = ASSIGN[ASSIGN<1e6] ASSIGN=change[change>4].reset_index() ASSIGN.columns=['train_more_id','rate'] ASSIGN=change[change<0.4].reset_index() ASSIGN.columns=['test_more_id','rate'] ASSIGN=train_more['ASSIGN'].values train[ASSIGN].head()",0,"['training_missing = train.isna().sum(axis=0) / train.shape[0] \n', 'test_missing = test.isna().sum(axis=0) / test.shape[0] \n', 'change = (training_missing / test_missing).sort_values(ascending=False)\n', 'change = change[change<1e6] # remove the divide by zero errors\n', 'train_more=change[change>4].reset_index()\n', ""train_more.columns=['train_more_id','rate']\n"", 'test_more=change[change<0.4].reset_index()\n', ""test_more.columns=['test_more_id','rate']\n"", ""train_more_id=train_more['train_more_id'].values\n"", 'train[train_more_id].head()']"
"''' ASSIGN = [250,500,750,1000] ASSIGN = [.01,.1,1,5] ASSIGN = [.01,.1,1,5] ASSIGN = [2,3,4,5] ASSIGN = [5,10,15,20,25] ASSIGN = ['deviance','exponential'] ASSIGN = ['auto','sqrt','log2'] ASSIGN = GradientBoostingClassifier() ASSIGN = {'n_estimators': [250,500,750,1000],'loss': ['deviance','exponential'],'max_features':['auto','sqrt','log2'],'learning_rate':[.01,.1,1,5],'subsample':[.01,.1,1,5], 'ASSIGN':[2,3,4,5],'ASSIGN':[5,10,15,20,25]} ASSIGN = RandomizedSearchCV(GB, parameters, scoring='accuracy' ,cv =50) ASSIGN.fit(x_train, x_test) ASSIGN.best_params_ '''",0,"[""'''\n"", 'n_estimators = [250,500,750,1000]\n', 'learning_rate = [.01,.1,1,5]\n', 'subsample = [.01,.1,1,5]\n', 'min_samples_split = [2,3,4,5]\n', 'max_depth = [5,10,15,20,25]\n', ""loss = ['deviance','exponential']\n"", ""max_features = ['auto','sqrt','log2']\n"", '\n', 'GB = GradientBoostingClassifier()\n', '\n', ""parameters = {'n_estimators': [250,500,750,1000],'loss': ['deviance','exponential'],'max_features':['auto','sqrt','log2'],'learning_rate':[.01,.1,1,5],'subsample':[.01,.1,1,5],\n"", ""             'min_samples_split':[2,3,4,5],'max_depth':[5,10,15,20,25]}\n"", '\n', ""GBClassifier = RandomizedSearchCV(GB, parameters, scoring='accuracy' ,cv =50)\n"", 'GBClassifier.fit(x_train, x_test)\n', 'GBClassifier.best_params_\n', ""'''""]"
"SETUP CHECKPOINT ASSIGN = KFold(n_splits=5, shuffle=True, random_state=15) ASSIGN = np.zeros(len(df_train)) ASSIGN = np.zeros(len(df_test)) ASSIGN = pd.DataFrame() ASSIGN = time.time() for fold_, (trn_idx, val_idx) in enumerate(ASSIGN.split(df_train.values, target.values)): print(.format(fold_)) ASSIGN = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats) ASSIGN = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats) ASSIGN = 10000 ASSIGN = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200) ASSIGN[val_idx] = ASSIGN.predict(df_train.iloc[val_idx][features], num_iteration=ASSIGN.best_iteration) ASSIGN = pd.DataFrame() ASSIGN[""feature""] = features ASSIGN[""importance""] = ASSIGN.feature_importance() ASSIGN[""fold""] = fold_ + 1 ASSIGN = pd.concat([ASSIGN, fold_importance_df], axis=0) ASSIGN += ASSIGN.predict(df_test[features], num_iteration=ASSIGN.best_iteration) path print(.format(log_loss(target, ASSIGN)))",0,"['from sklearn.model_selection import StratifiedKFold, KFold\n', 'from sklearn.metrics import mean_squared_error\n', 'from sklearn.metrics import log_loss\n', '%%time\n', 'folds = KFold(n_splits=5, shuffle=True, random_state=15)\n', 'oof = np.zeros(len(df_train))\n', 'predictions = np.zeros(len(df_test))\n', 'feature_importance_df = pd.DataFrame()\n', '\n', 'start = time.time()\n', '\n', '\n', 'for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values, target.values)):\n', '    print(""fold n°{}"".format(fold_))\n', '    trn_data = lgb.Dataset(df_train.iloc[trn_idx][features], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n', '    val_data = lgb.Dataset(df_train.iloc[val_idx][features], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n', '\n', '    num_round = 10000\n', '    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 200)\n', '    oof[val_idx] = clf.predict(df_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n', '    \n', '    fold_importance_df = pd.DataFrame()\n', '    fold_importance_df[""feature""] = features\n', '    fold_importance_df[""importance""] = clf.feature_importance()\n', '    fold_importance_df[""fold""] = fold_ + 1\n', '    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n', '    \n', '    predictions += clf.predict(df_test[features], num_iteration=clf.best_iteration) / folds.n_splits\n', '\n', 'print(""CV score: {:<8.5f}"".format(log_loss(target, oof)))']"
"CHECKPOINT ASSIGN = linear_model.Lasso(alpha=.01) ASSIGN.fit(x_train,Y_train) ASSIGN = model10.score(x_test,Y_test) print(ASSIGN*100,'%')",0,"['model10 = linear_model.Lasso(alpha=.01)\n', 'model10.fit(x_train,Y_train)\n', '\n', 'accuracy10 = model10.score(x_test,Y_test)\n', ""print(accuracy10*100,'%')""]"
"CHECKPOINT print(len(X_temporal_train),len(X_temporal_test))",0,"['print(len(X_temporal_train),len(X_temporal_test))']"
"ASSIGN = final['Fare'] sns.distplot(ASSIGN, hist=True, rug=True)",1,"[""x = final['Fare']\n"", 'sns.distplot(x, hist=True, rug=True)']"
sub.describe(),0,['sub.describe()']
"sns.countplot(x = 'Sex', data = df1)",1,"[""sns.countplot(x = 'Sex', data = df1)""]"
"ASSIGN = xgb.XGBRegressor() ASSIGN.fit(X,y_fata)",0,"['reg_fata = xgb.XGBRegressor()\n', 'reg_fata.fit(X,y_fata)']"
zomato['rate'].unique(),0,"[""zomato['rate'].unique()""]"
"plt.figure(figsize=(15,6)) sns.distplot(points_df['x'], bins=500); plt.xlabel('x') plt.show()",1,"['plt.figure(figsize=(15,6))\n', ""sns.distplot(points_df['x'], bins=500);\n"", ""plt.xlabel('x')\n"", 'plt.show()']"
"SETUP CHECKPOINT ASSIGN = int(input(""Enter the size of the matrix here:-"")) ASSIGN = np.eye(sz) print(, ASSIGN)",0,"['import numpy as np\n', 'from scipy import sparse\n', 'sz = int(input(""Enter the size of the matrix here:-""))\n', 'arr = np.eye(sz)\n', 'print(""NumPy array:"", arr)']"
"def get_conf_scaler(country,province): ASSIGN = train_df2.query(f""Country == '{country}' and Province =='{province}'"") ASSIGN = train_df2_province['ConfirmedCases'] ASSIGN = train_df2_province['Fatalities'] ASSIGN = preprocessing.MinMaxScaler() ASSIGN = preprocessing.MinMaxScaler() ASSIGN.fit(np.array(ASSIGN).reshape(-1,1)) ASSIGN.fit(np.array(ASSIGN).reshape(-1,1)) return province_conf_scaler,province_fata_scaler",0,"['def get_conf_scaler(country,province):\n', '    train_df2_province = train_df2.query(f""Country == \'{country}\' and Province ==\'{province}\'"")\n', ""    train_df2_province_conf = train_df2_province['ConfirmedCases']\n"", ""    train_df2_province_fata = train_df2_province['Fatalities']\n"", '    province_conf_scaler = preprocessing.MinMaxScaler()\n', '    province_fata_scaler = preprocessing.MinMaxScaler()\n', '    province_conf_scaler.fit(np.array(train_df2_province_conf).reshape(-1,1))\n', '    province_fata_scaler.fit(np.array(train_df2_province_fata).reshape(-1,1))\n', '    return province_conf_scaler,province_fata_scaler']"
"ASSIGN = ASSIGN[ASSIGN.columns.drop(['address','activities','nursery','freetime'])]",0,"[""data_mat = data_mat[data_mat.columns.drop(['address','activities','nursery','freetime'])]""]"
"ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"")",0,"['df1 = pd.read_csv(""../input/titanic/train.csv"")\n', 'tf1 = pd.read_csv(""../input/titanic/test.csv"")\n', 'result = pd.read_csv(""../input/titanic/gender_submission.csv"")']"
"ASSIGN=34.78 ASSIGN=32.05 ASSIGN=folium.Map([Lat,Long],zoom_start=12) data_total_cars_7['label']=data_total_cars_7.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1) ASSIGN=plugins.MarkerCluster().add_to(data_total_cars_7_map) for lat,lon,label in zip(data_total_cars_7.latitude,data_total_cars_7.longitude,data_total_cars_7.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN)",1,"['Long=34.78\n', 'Lat=32.05\n', 'data_total_cars_7_map=folium.Map([Lat,Long],zoom_start=12)\n', ""data_total_cars_7['label']=data_total_cars_7.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n"", '\n', 'data_total_cars_7_cars_map=plugins.MarkerCluster().add_to(data_total_cars_7_map)\n', 'for lat,lon,label in zip(data_total_cars_7.latitude,data_total_cars_7.longitude,data_total_cars_7.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_7_cars_map)\n', 'data_total_cars_7_map.add_child(data_total_cars_7_cars_map)\n', '\n']"
ASSIGN = ASSIGN.map(lambda n: np.log(n) if n > 0 else 0),0,"['final[""Fare""] = final[""Fare""].map(lambda n: np.log(n) if n > 0 else 0)\n']"
x_train.head(),0,['x_train.head()']
ASSIGN = ASSIGN.astype(str) final['Ticket_length'] = final.Ticket.apply(len) final['Ticket_length'].astype(int) final['Ticket_length'].unique(),0,"[""final['Ticket'] = final['Ticket'].astype(str)\n"", ""final['Ticket_length'] = final.Ticket.apply(len)\n"", ""final['Ticket_length'].astype(int)\n"", ""final['Ticket_length'].unique()""]"
df2.head(5),0,['df2.head(5)']
ASSIGN = nn.NLLLoss() ASSIGN = optim.Adam(model.parameters()),0,"['criterion = nn.NLLLoss()\n', '# Set the optimizer function using torch.optim as optim library\n', 'optimizer = optim.Adam(model.parameters())']"
CHECKPOINT data.target_label,0,['data.target_label']
CHECKPOINT titanic_df.info() print() test_df.info(),0,"['titanic_df.info()\n', 'print(""----------------------------"")\n', 'test_df.info()']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'from keras.datasets import mnist#download mnist data and split into train and test sets\n', 'from sklearn.model_selection import train_test_split\n', 'import matplotlib.pyplot as plt\n', 'from keras.utils import to_categorical\n', '\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
"ASSIGN = history.history ASSIGN = history_dict['loss'] ASSIGN = history_dict['val_loss'] ASSIGN = history_dict['acc'] ASSIGN = history_dict['val_acc'] ASSIGN = range(1,len(loss_values)+1) plt.clf() plt.subplot(311) plt.plot(ASSIGN, ASSIGN,'bo-') plt.plot(ASSIGN, ASSIGN,'rs-') plt.xlabel('Iterations') plt.ylabel('Loss & Accuracy ') plt.title(""For Train Data"") plt.subplot(313) plt.plot(ASSIGN, ASSIGN,'bo-') plt.plot(ASSIGN, ASSIGN,'rs-') plt.xlabel('Iterations') plt.ylabel('Loss & Accuracy') plt.title(""For validation Data"") plt.show()",1,"['history_dict = history.history\n', ""loss_values = history_dict['loss']\n"", ""val_loss_values = history_dict['val_loss']\n"", ""acc_values = history_dict['acc']\n"", ""val_acc_values = history_dict['val_acc']\n"", 'epochs = range(1,len(loss_values)+1)\n', 'plt.clf()\n', 'plt.subplot(311)\n', ""plt.plot(epochs, loss_values,'bo-')\n"", ""plt.plot(epochs, acc_values,'rs-')\n"", ""plt.xlabel('Iterations')\n"", ""plt.ylabel('Loss & Accuracy ')\n"", 'plt.title(""For Train Data"")\n', '\n', 'plt.subplot(313)\n', ""plt.plot(epochs, val_loss_values,'bo-')\n"", ""plt.plot(epochs, val_acc_values,'rs-')\n"", ""plt.xlabel('Iterations')\n"", ""plt.ylabel('Loss & Accuracy')\n"", 'plt.title(""For validation Data"")\n', '\n', 'plt.show()']"
"ASSIGN = datetime.strptime('2020-04-09','%Y-%m-%d') ASSIGN = datetime.strptime('2020-04-10','%Y-%m-%d') ASSIGN = datetime.strptime('2020-04-11','%Y-%m-%d') ASSIGN = datetime.strptime('2020-04-12','%Y-%m-%d')",0,"[""day1 = datetime.strptime('2020-04-09','%Y-%m-%d')\n"", ""day2 = datetime.strptime('2020-04-10','%Y-%m-%d')\n"", ""day3 = datetime.strptime('2020-04-11','%Y-%m-%d')\n"", ""day4 = datetime.strptime('2020-04-12','%Y-%m-%d')""]"
"data.classes, data.c, len(data.train_ds), len(data.valid_ds)",0,"['data.classes, data.c, len(data.train_ds), len(data.valid_ds)']"
SETUP,0,"[""DATASET_DIR = '/kaggle/input/pku-autonomous-driving/'\n"", ""JSON_DIR = os.path.join(DATASET_DIR, 'car_models_json')\n"", 'NUM_IMG_SAMPLES = 10 # The number of image samples used for visualization']"
SETUP CHECKPOINT py.init_notebook_mode(connected=True) print(os.listdir()),0,"['\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'from scipy import stats\n', 'from sklearn.preprocessing import LabelEncoder\n', 'from sklearn.model_selection import train_test_split\n', 'from sklearn.linear_model import LogisticRegression\n', 'from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve\n', 'from sklearn.metrics import  recall_score, classification_report\n', 'from statsmodels.stats.outliers_influence import variance_inflation_factor\n', 'from statsmodels.tools.tools import add_constant\n', 'from sklearn.tree import DecisionTreeClassifier\n', 'from sklearn import metrics\n', '\n', '# Plotting the graphs\n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline\n', 'import seaborn as sns\n', 'import plotly.graph_objs as go\n', 'import plotly.offline as py\n', 'py.init_notebook_mode(connected=True)\n', '\n', 'import os\n', 'print(os.listdir(""../input""))\n', '\n', '# Any results you write to the current directory are saved as output.']"
"CHECKPOINT ASSIGN = linear_model.Ridge(alpha=0.01) ASSIGN.fit(x_t,Y_t) ASSIGN = model16.score(x_es,Y_es) print(ASSIGN*100,'%')",0,"['model16 = linear_model.Ridge(alpha=0.01)\n', 'model16.fit(x_t,Y_t)\n', '\n', 'accuracy16 = model16.score(x_es,Y_es)\n', ""print(accuracy16*100,'%')""]"
"ASSIGN= death_predict[['ds', 'yhat']] ASSIGN.columns = [['ForecastId', 'Fatalities']]",0,"['## These are the predictions for Deaths by Covid-19 cases until 2020 April 24\n', '\n', ""forecastDeath= death_predict[['ds', 'yhat']]\n"", ""forecastDeath.columns = [['ForecastId', 'Fatalities']]\n""]"
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'links_small.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,"[""nRowsRead = 1000 # specify 'None' if want to read whole file\n"", '# links_small.csv has 9125 rows in reality, but we are only loading/previewing the first 1000 rows\n', ""df3 = pd.read_csv('../input/links_small.csv', delimiter=',', nrows = nRowsRead)\n"", ""df3.dataframeName = 'links_small.csv'\n"", 'nRow, nCol = df3.shape\n', ""print(f'There are {nRow} rows and {nCol} columns')""]"
"CHECKPOINT ASSIGN = go.Figure(data=[ go.Line(name='train_acc', x=history_1.epoch, y=history_1.history['accuracy']), go.Line(name='Val_acc', x=history_1.epoch, y=history_1.history['val_accuracy'])]) ASSIGN.update_layout( ASSIGN=""Accuracy"", ASSIGN=""epoch"", ASSIGN=""accuracy"", ASSIGN=dict( ASSIGN=""Courier New, monospace"", ASSIGN=18, ASSIGN=""#7f7f7f"" )) fig",1,"['fig = go.Figure(data=[\n', ""    go.Line(name='train_acc', x=history_1.epoch, y=history_1.history['accuracy']),\n"", ""    go.Line(name='Val_acc', x=history_1.epoch, y=history_1.history['val_accuracy'])])\n"", '\n', 'fig.update_layout(\n', '    title=""Accuracy"",\n', '    xaxis_title=""epoch"",\n', '    yaxis_title=""accuracy"",\n', '    font=dict(\n', '        family=""Courier New, monospace"",\n', '        size=18,\n', '        color=""#7f7f7f""\n', '    ))\n', 'fig']"
"SETUP CHECKPOINT print(os.listdir('..path')) py.init_notebook_mode(connected=False) sns.set(rc={'figure.figsize':(20,10)})",0,"['# Other Imports\n', 'import json\n', 'import matplotlib.pyplot as plt\n', 'import numpy as np\n', 'import os\n', 'import pandas as pd\n', 'import plotly.graph_objs as go\n', 'import plotly.offline as py\n', 'import seaborn as sns\n', '\n', '# Pytorch Imports\n', 'import torch\n', 'import torch.nn as nn\n', 'import torch.nn.functional as F\n', 'import torch.optim as optim\n', 'import torch.utils.data as data\n', 'import torchvision\n', 'from torchvision import transforms\n', 'import torchvision.models as models\n', '\n', ""print(os.listdir('../input'))\n"", '%matplotlib inline\n', 'py.init_notebook_mode(connected=False)\n', ""sns.set(rc={'figure.figsize':(20,10)})\n"", '\n', '%env JOBLIB_TEMP_FOLDER=/tmp']"
"ASSIGN = train['Id'] ASSIGN = test['Id'] train.drop('Id',inplace = True, axis = 1) test.drop('Id',inplace = True, axis = 1)",0,"[""train_id = train['Id']\n"", ""test_id = test['Id']\n"", ""train.drop('Id',inplace = True, axis = 1)\n"", ""test.drop('Id',inplace = True, axis = 1)""]"
ASSIGN = population[population.Country == 'ASSIGN'] ASSIGN = 'Congo (Brazzaville)' ASSIGN = Congo.copy() ASSIGN = 'Congo (Kinshasa)' ASSIGN = Congo.copy() ASSIGN = ASSIGN.append(new1) ASSIGN = ASSIGN.append(new2),0,"[""Congo = population[population.Country == 'Congo']\n"", ""Congo['Country'] = 'Congo (Brazzaville)'\n"", 'new1 = Congo.copy()\n', ""Congo['Country'] = 'Congo (Kinshasa)'\n"", 'new2 = Congo.copy()\n', '\n', 'population = population.append(new1)\n', 'population = population.append(new2)']"
cleaning_train.isnull().sum().sort_values(),0,['cleaning_train.isnull().sum().sort_values()']
"sns.catplot(x='Year', y='Protection of Civil Rights (PCR) Act', data=total ,height = 5, aspect = 4,kind = 'bar')",1,"[""sns.catplot(x='Year', y='Protection of Civil Rights (PCR) Act', data=total ,height = 5, aspect = 4,kind = 'bar')""]"
"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)",0,"['X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 1)']"
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'keywords.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,"[""nRowsRead = 1000 # specify 'None' if want to read whole file\n"", '# keywords.csv has 46419 rows in reality, but we are only loading/previewing the first 1000 rows\n', ""df2 = pd.read_csv('../input/keywords.csv', delimiter=',', nrows = nRowsRead)\n"", ""df2.dataframeName = 'keywords.csv'\n"", 'nRow, nCol = df2.shape\n', ""print(f'There are {nRow} rows and {nCol} columns')""]"
"sns.pairplot( tumor_data, ASSIGN = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean'], ASSIGN = 'target' )",1,"['sns.pairplot(\n', '    tumor_data,\n', ""    vars = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean'],\n"", ""    hue = 'target'\n"", ')']"
"ASSIGN = pd.DataFrame(columns={""infection_trend"",""fatality_trend"",""quarantine_trend"",""school_trend"",""total_population"",""expected_cases"",""expected_fatalities""}) ASSIGN = scale_train ASSIGN = 14 ASSIGN = [] with tqdm(total=len(list(ASSIGN.Country.unique()))) as pbar: for country in ASSIGN.Country.unique(): for province in ASSIGN.query(f""Country=='{country}'"").Province.unique(): ASSIGN = train_df.query(f""Country=='{country}' and Province=='{province}'"") for i in range(0,len(ASSIGN)): if i+ASSIGN<=len(ASSIGN): ASSIGN = [float(x) for x in province_df[i:i+days_in_sequence-1].ConfirmedCases.values] ASSIGN = [float(x) for x in province_df[i:i+days_in_sequence-1].Fatalities.values] ASSIGN = float(province_df.iloc[i].Days_After_1stJan) ASSIGN = float(province_df.iloc[i].Dayofweek) ASSIGN = float(province_df.iloc[i].Month) ASSIGN= float(province_df.iloc[i].Day) ASSIGN = float(province_df.iloc[i].Population) ASSIGN = float(province_df.iloc[i].Density) ASSIGN = float(province_df.iloc[i].Land_Area) ASSIGN = float(province_df.iloc[i].Migrants) ASSIGN = float(province_df.iloc[i].MedAge) ASSIGN = float(province_df.iloc[i].UrbanPopRate) ASSIGN = float(province_df.iloc[i].API_beds) ASSIGN = float(province_df.iloc[i+days_in_sequence-1].ConfirmedCases) ASSIGN = float(province_df.iloc[i+days_in_sequence-1].Fatalities) ASSIGN.append({""ASSIGN"":ASSIGN, ""ASSIGN"":ASSIGN, ""stable_inputs"":[ASSIGN,ASSIGN,ASSIGN,ASSIGN,ASSIGN,ASSIGN,ASSIGN], ""ASSIGN"":ASSIGN, ""ASSIGN"":ASSIGN}) pbar.update(1) ASSIGN = pd.DataFrame(trend_list)",0,"['trend_df = pd.DataFrame(columns={""infection_trend"",""fatality_trend"",""quarantine_trend"",""school_trend"",""total_population"",""expected_cases"",""expected_fatalities""})\n', '\n', 'train_df = scale_train\n', 'days_in_sequence = 14\n', '\n', 'trend_list = []\n', '\n', 'with tqdm(total=len(list(train_df.Country.unique()))) as pbar:\n', '    for country in train_df.Country.unique():\n', '        for province in train_df.query(f""Country==\'{country}\'"").Province.unique():\n', '            province_df = train_df.query(f""Country==\'{country}\' and Province==\'{province}\'"")\n', '            \n', '\n', '            for i in range(0,len(province_df)):\n', '                if i+days_in_sequence<=len(province_df):\n', '                    #prepare all the trend inputs\n', '                    infection_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].ConfirmedCases.values]\n', '                    fatality_trend = [float(x) for x in province_df[i:i+days_in_sequence-1].Fatalities.values]\n', '\n', '                    #preparing all the stable inputs\n', '                    days_after_1stJan = float(province_df.iloc[i].Days_After_1stJan)\n', '                    dayofweek = float(province_df.iloc[i].Dayofweek)\n', '                    month = float(province_df.iloc[i].Month)\n', '                    day= float(province_df.iloc[i].Day)\n', '                    population = float(province_df.iloc[i].Population)\n', '                    density = float(province_df.iloc[i].Density)\n', '                    land_area = float(province_df.iloc[i].Land_Area)\n', '                    migrants = float(province_df.iloc[i].Migrants)\n', '                    medage = float(province_df.iloc[i].MedAge)\n', '                    urbanpoprate = float(province_df.iloc[i].UrbanPopRate)\n', '                    beds = float(province_df.iloc[i].API_beds)\n', '\n', '                    #True cases in i+days_in_sequence-1 day\n', '                    expected_cases = float(province_df.iloc[i+days_in_sequence-1].ConfirmedCases)\n', '                    expected_fatalities = float(province_df.iloc[i+days_in_sequence-1].Fatalities)\n', '\n', '                    trend_list.append({""infection_trend"":infection_trend,\n', '                                     ""fatality_trend"":fatality_trend,\n', '                                     ""stable_inputs"":[population,density,land_area,migrants,medage,urbanpoprate,beds],\n', '                                     ""expected_cases"":expected_cases,\n', '                                     ""expected_fatalities"":expected_fatalities})\n', '        pbar.update(1)\n', 'trend_df = pd.DataFrame(trend_list)']"
"CHECKPOINT ASSIGN = model1.predict(X_test) ASSIGN = mean_squared_error(y_test, y_pred1, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ASSIGN)))",0,"['y_pred1 = model1.predict(X_test)\n', '\n', 'val = mean_squared_error(y_test, y_pred1, squared=False)\n', 'val1 = str(round(val, 4))\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))\n""]"
"DD=data_2019[data_2019.Variable=='Driving Distance - (TOTAL DISTANCE)'].iloc[:,[0,4]] DD['Value_new']=DD['Value'].apply(lambda x:''.join(x.split(','))) DD.Value_new=DD.Value_new.astype(int) DD=DD.drop(columns='Value') plt.figure(figsize=(22,15)) plt.subplot(2,2,1) DD.groupby('Player Name')['Value_new'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='darkorange') plt.title(""Top 10 player by Total Driving Distance"",size=20) plt.xlabel('meters') plt.ylabel('') plt.subplot(2,2,2) DD=data_2019[data_2019.Variable=='Driving Distance - (AVG.)'].iloc[:,[0,4]] DD.Value=DD.Value.astype(float) DD.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='navy') plt.title(""Top 10 player by Average Driving Distance"",size=20) plt.xlabel('meters') plt.ylabel('') plt.subplot(2,2,3) DD=data_2019[data_2019.Variable=='Driving Distance - (ROUNDS)'].iloc[:,[0,4]] DD.Value=DD.Value.astype(float) DD.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='purple') plt.title(""Top 10 player by Rounds Driving Distance"",size=20) plt.xlabel('meters') plt.ylabel('') plt.subplot(2,2,4) DD=data_2019[data_2019.Variable=='Driving Distance - (TOTAL DRIVES)'].iloc[:,[0,4]] DD.Value=DD.Value.astype(float) DD.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh() plt.title(""Top 10 player by TOTAL DRIVES Driving Distance"",size=20) plt.xlabel('meters') plt.ylabel('')",1,"[""DD=data_2019[data_2019.Variable=='Driving Distance - (TOTAL DISTANCE)'].iloc[:,[0,4]]\n"", ""DD['Value_new']=DD['Value'].apply(lambda x:''.join(x.split(',')))\n"", 'DD.Value_new=DD.Value_new.astype(int)\n', ""DD=DD.drop(columns='Value')\n"", 'plt.figure(figsize=(22,15))\n', 'plt.subplot(2,2,1)\n', ""DD.groupby('Player Name')['Value_new'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='darkorange')\n"", 'plt.title(""Top 10 player by Total Driving Distance"",size=20)\n', ""plt.xlabel('meters')\n"", ""plt.ylabel('')\n"", 'plt.subplot(2,2,2)\n', ""DD=data_2019[data_2019.Variable=='Driving Distance - (AVG.)'].iloc[:,[0,4]]\n"", 'DD.Value=DD.Value.astype(float)\n', ""DD.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='navy')\n"", 'plt.title(""Top 10 player by Average Driving Distance"",size=20)\n', ""plt.xlabel('meters')\n"", ""plt.ylabel('')\n"", 'plt.subplot(2,2,3)\n', ""DD=data_2019[data_2019.Variable=='Driving Distance - (ROUNDS)'].iloc[:,[0,4]]\n"", 'DD.Value=DD.Value.astype(float)\n', ""DD.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='purple')\n"", 'plt.title(""Top 10 player by Rounds Driving Distance"",size=20)\n', ""plt.xlabel('meters')\n"", ""plt.ylabel('')\n"", 'plt.subplot(2,2,4)\n', ""DD=data_2019[data_2019.Variable=='Driving Distance - (TOTAL DRIVES)'].iloc[:,[0,4]]\n"", 'DD.Value=DD.Value.astype(float)\n', ""DD.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh()\n"", 'plt.title(""Top 10 player by TOTAL DRIVES Driving Distance"",size=20)\n', ""plt.xlabel('meters')\n"", ""plt.ylabel('')\n""]"
"ASSIGN = test_df.text.values ASSIGN = [""[CLS] "" + sentence + "" [SEP]"" for sentence in ASSIGN] ASSIGN = np.random.rand(len(sentences))",0,"['sentences = test_df.text.values\n', 'sentences = [""[CLS] "" + sentence + "" [SEP]"" for sentence in sentences]\n', 'labels = np.random.rand(len(sentences))']"
SETUP,0,['from sklearn.model_selection import train_test_split']
"ASSIGN = pd.DataFrame({'first': ttvarray[:,0], 'second': ttvarray[:,1], 'third': ttvarray[:,2]}) ASSIGN = ASSIGN.replace(numstonames) ASSIGN['words'] = ASSIGN['first'] + "" "" + ASSIGN['second'] + "" "" + ASSIGN['third'] ASSIGN = pd.read_csv('..path', index_col=['key_id']) ASSIGN = preds_df.words.values ASSIGN.to_csv('subcnn_small.csv') ASSIGN.head()",0,"[""preds_df = pd.DataFrame({'first': ttvarray[:,0], 'second': ttvarray[:,1], 'third': ttvarray[:,2]})\n"", 'preds_df = preds_df.replace(numstonames)\n', 'preds_df[\'words\'] = preds_df[\'first\'] + "" "" + preds_df[\'second\'] + "" "" + preds_df[\'third\']\n', '\n', ""sub = pd.read_csv('../input/sample_submission.csv', index_col=['key_id'])\n"", ""sub['word'] = preds_df.words.values\n"", ""sub.to_csv('subcnn_small.csv')\n"", 'sub.head()']"
CHECKPOINT ASSIGN = ASSIGN.map(strtonum) print(bikes.columns),0,"['# Extraction of digital\n', ""bikes['Precipitation'] = bikes['Precipitation'].map(strtonum)\n"", ""# print(bikes['Precipitation'])\n"", 'print(bikes.columns)\n', '\n']"
match_stats_df[match_stats_df.goals_info.isnull() == False],0,['match_stats_df[match_stats_df.goals_info.isnull() == False]']
"plt.figure(figsize=(12,12)) ASSIGN=data.groupby('date')['total_cars'].agg('sum').index ASSIGN=data.groupby('date')['total_cars'].agg('sum') sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN) plt.title(""total_car in the data by date"",size=24) plt.xlabel('cars')",1,"['plt.figure(figsize=(12,12))\n', ""y=data.groupby('date')['total_cars'].agg('sum').index\n"", ""x=data.groupby('date')['total_cars'].agg('sum')\n"", 'sns.barplot(x=x,y=y)\n', 'plt.title(""total_car in the data by date"",size=24)\n', ""plt.xlabel('cars')""]"
"sns.FacetGrid(train, hue=""height"", size=10).map(plt.scatter, ""category_id"", ""location"").add_legend()",1,"['sns.FacetGrid(train, hue=""height"", size=10).map(plt.scatter, ""category_id"", ""location"").add_legend()']"
"slope_df.plot(kind = 'bar', figsize = (10,6), title = 'Never Smoked: % Changes from 1994 to 2010') slope_df1.plot(kind = 'bar', figsize = (10,6), title = 'Smoke everyday: % Changes from 1994 to 2010') plt.show()",1,"[""slope_df.plot(kind = 'bar', figsize = (10,6), title = 'Never Smoked: % Changes from 1994 to 2010')\n"", ""slope_df1.plot(kind = 'bar', figsize = (10,6), title = 'Smoke everyday: % Changes from 1994 to 2010')\n"", 'plt.show()']"
"def create_model(): ASSIGN = models.densenet161(pretrained=True) for param in ASSIGN.parameters(): param.requires_grad = False ASSIGN = model.classifier.in_features ASSIGN.classifier = nn.Sequential(nn.Linear(ASSIGN, 2048), nn.ReLU(), nn.Linear(2048, 512), nn.ReLU(), nn.Linear(512, 102), nn.LogSoftmax(dim=1)) ASSIGN.to(device) return model",0,"['def create_model():\n', '    model = models.densenet161(pretrained=True)\n', '\n', '    for param in model.parameters():\n', '        param.requires_grad = False\n', '\n', '    num_filters = model.classifier.in_features\n', '    model.classifier = nn.Sequential(nn.Linear(num_filters, 2048),\n', '                               nn.ReLU(),\n', '                               nn.Linear(2048, 512),\n', '                               nn.ReLU(),\n', '                               nn.Linear(512, 102),\n', '                               nn.LogSoftmax(dim=1))\n', '\n', '    # Move model to the device specified above\n', '    model.to(device)\n', '    return model']"
"CHECKPOINT print(sum(diff_df.ConfirmedCases < 0),sum(diff_df.Fatalities<0))",0,"['print(sum(diff_df.ConfirmedCases < 0),sum(diff_df.Fatalities<0))']"
"test['Total'].plot(legend=True) predictions1.plot(legend=True) predictions2.plot(legend=True,figsize=(12,6));",1,"[""test['Total'].plot(legend=True)\n"", 'predictions1.plot(legend=True)\n', 'predictions2.plot(legend=True,figsize=(12,6));']"
CHECKPOINT print() all(recipes['rating'].apply(np.isreal)),0,"[""# We'll use the numpy isreal() function\n"", '# See https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.isreal.html\n', 'print(""Is this variable numeric?"")\n', ""all(recipes['rating'].apply(np.isreal)) # Check that every row is True.""]"
ASSIGN = Prophet(interval_width=0.97) ASSIGN.fit(confirmed) ASSIGN = m.make_future_dataframe(periods=29) ASSIGN = future.copy() ASSIGN.tail(),0,"['m = Prophet(interval_width=0.97)\n', 'm.fit(confirmed)\n', 'future = m.make_future_dataframe(periods=29)\n', 'future_confirmed = future.copy() \n', 'future.tail()']"
"SETUP X_train, X_val, y_train, y_val = train_test_split(df['review'], df['sentiment'], test_size=0.3, ASSIGN=17)",0,"['from sklearn.model_selection import train_test_split, StratifiedKFold\n', 'from sklearn.neighbors import KNeighborsClassifier\n', 'from sklearn.tree import DecisionTreeClassifier\n', '\n', ""X_train, X_val, y_train, y_val = train_test_split(df['review'], df['sentiment'], test_size=0.3,\n"", 'random_state=17)']"
"ggplot(recpies, aes(x = calories, y = dessert)) + geom_point() + geom_smooth(method = ""glm"", # plot a regression... method.args = list(family = ""binomial"")) # ...from the binomial family",1,"['# plot & add a regression line\n', 'ggplot(recpies, aes(x = calories, y = dessert)) + # draw a \n', '    geom_point() + # add points\n', '    geom_smooth(method = ""glm"", # plot a regression...\n', '    method.args = list(family = ""binomial"")) # ...from the binomial family']"
"total.drop(total[total['STATEpath'] == 'TOTAL (UTs)'].index , inplace = True) total.drop(total[total['STATEpath'] == 'TOTAL (STATES)'].index , inplace = True)",0,"[""total.drop(total[total['STATE/UT'] == 'TOTAL (UTs)'].index , inplace = True) \n"", ""total.drop(total[total['STATE/UT'] == 'TOTAL (STATES)'].index , inplace = True) ""]"
SETUP,0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""# for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '#     for filename in filenames:\n', '#         print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
"sns.countplot(top_known_species) plt.title(""Top Known Species That Impact with Aircraft"") plt.xticks(rotation='vertical')",1,"['sns.countplot(top_known_species)\n', 'plt.title(""Top Known Species That Impact with Aircraft"")\n', ""plt.xticks(rotation='vertical')""]"
CHECKPOINT ASSIGN = tumor_data['Unnamed: 32'].isnull().sum() ASSIGN = tumor_data['Unnamed: 32'].shape[0] ASSIGN == number_of_rows: print('The whole \'Unnamed: 32\' column has empty values.') else: print('There are non-empty values in the \'Unnamed: 32\' column.'),0,"[""missing_values = tumor_data['Unnamed: 32'].isnull().sum()\n"", ""number_of_rows = tumor_data['Unnamed: 32'].shape[0]\n"", 'if missing_values == number_of_rows:\n', ""    print('The whole \\'Unnamed: 32\\' column has empty values.')\n"", 'else:\n', ""    print('There are non-empty values in the \\'Unnamed: 32\\' column.')""]"
"final['Age'].fillna(final['Age'].median(),inplace = True)",0,"[""final['Age'].fillna(final['Age'].median(),inplace = True)\n""]"
"ASSIGN = pd.DataFrame({'id':test_data['id'], 'type':pred_final}) ASSIGN.head()",0,"[""submission = pd.DataFrame({'id':test_data['id'], 'type':pred_final})\n"", 'submission.head()']"
"CHECKPOINT ASSIGN = pd_data['RainTomorrow'].head(55000) ASSIGN= pd_data['RainTomorrow'].tail(1420) ASSIGN = pd_data.head(55000).drop(['RainTomorrow'], axis=1) ASSIGN= pd_data.tail(1420).drop(['RainTomorrow'], axis=1) print(ASSIGN.head()) print(ASSIGN.head())",0,"['#Task: Split the data into train and test\n', ""train_y = pd_data['RainTomorrow'].head(55000)\n"", ""test_y= pd_data['RainTomorrow'].tail(1420)\n"", ""train_x = pd_data.head(55000).drop(['RainTomorrow'], axis=1)\n"", ""test_x= pd_data.tail(1420).drop(['RainTomorrow'], axis=1)\n"", 'print(train_y.head())\n', 'print(train_x.head())']"
"def keras_rnn_init(m): for name, param in m.named_parameters(): if 'weight_ih' in name: torch.nn.init.xavier_uniform_(param) if 'weight_hh' in name: torch.nn.init.orthogonal_(param) if 'bias_' in name: torch.nn.init.constant_(param, 0)",0,"['def keras_rnn_init(m):\n', '    for name, param in m.named_parameters():\n', ""        if 'weight_ih' in name: torch.nn.init.xavier_uniform_(param)\n"", ""        if 'weight_hh' in name: torch.nn.init.orthogonal_(param)\n"", ""        if 'bias_' in name: torch.nn.init.constant_(param, 0)""]"
"def pie_plot(Column): ASSIGN = pd.crosstab(data[Column],data['Churn']) ASSIGN = go.Pie(labels = ct1.index, ASSIGN = ct1.iloc[:,0], ASSIGN=0.3, ASSIGN=dict(x=[0,.45])) ASSIGN = go.Pie(labels = ct1.index, ASSIGN = ct1.iloc[:,1], ASSIGN=dict(x=[.55,1]), ASSIGN=0.3) ASSIGN = go.Layout(dict(title = Column + "" distribution in customer attrition "", ASSIGN = ""rgb(243,243,243)"", ASSIGN = ""rgb(243,243,243)"", ASSIGN = [dict(text = ""churn customers"", ASSIGN = dict(size = 13), ASSIGN = False, ASSIGN = .15, y = 1), dict(text = ""Non churn customers"", ASSIGN = dict(size = 13), ASSIGN = False, ASSIGN = .88,y = 1) ] ) ) ASSIGN = go.Figure(data=[trace1,trace2],layout=layout) py.iplot(ASSIGN)",1,"['def pie_plot(Column):    \n', ""    ct1 = pd.crosstab(data[Column],data['Churn'])\n"", '    trace1 = go.Pie(labels = ct1.index,\n', '                    values = ct1.iloc[:,0],\n', '                    hole=0.3,\n', '                    domain=dict(x=[0,.45]))\n', '    trace2 = go.Pie(labels = ct1.index,\n', '                    values = ct1.iloc[:,1],\n', '                    domain=dict(x=[.55,1]),\n', '                    hole=0.3)\n', '\n', '    layout = go.Layout(dict(title = Column + "" distribution in customer attrition "",\n', '                                plot_bgcolor  = ""rgb(243,243,243)"",\n', '                                paper_bgcolor = ""rgb(243,243,243)"",\n', '                                annotations = [dict(text = ""churn customers"",\n', '                                                    font = dict(size = 13),\n', '                                                    showarrow = False,\n', '                                                    x = .15, y = 1),\n', '                                               dict(text = ""Non churn customers"",\n', '                                                    font = dict(size = 13),\n', '                                                    showarrow = False,\n', '                                                    x = .88,y = 1)\n', '\n', '                                              ]\n', '                               )\n', '                          )\n', '\n', '    fig = go.Figure(data=[trace1,trace2],layout=layout)\n', '    py.iplot(fig)']"
"ASSIGN = KNeighborsClassifier(n_neighbors = 3) ASSIGN.fit(X_train, Y_train) ASSIGN = knn.predict(X_test) ASSIGN.score(X_train, Y_train)",0,"['knn = KNeighborsClassifier(n_neighbors = 3)\n', '\n', 'knn.fit(X_train, Y_train)\n', '\n', 'Y_pred = knn.predict(X_test)\n', '\n', 'knn.score(X_train, Y_train)']"
"CHECKPOINT ASSIGN=os.listdir(""..path"") dirs",0,"['#list all the directories\n', 'dirs=os.listdir(""../input/zomato_data/"")\n', 'dirs']"
covid.head(),0,['covid.head()']
pd.options.display.float_format = '{:.2f}'.format pca.explained_variance_ratio_,0,"[""pd.options.display.float_format = '{:.2f}'.format\n"", 'pca.explained_variance_ratio_']"
"sns.jointplot(x = 'Present_Price', y ='Selling_Price', data=data, color = 'Green')",1,"[""sns.jointplot(x = 'Present_Price', y ='Selling_Price', data=data, color = 'Green')""]"
"ASSIGN = pd.DataFrame({ ""PassengerId"": test_df[""PassengerId""], ""Survived"": Y_pred }) ASSIGN.to_csv('titanic.csv', index=False)",0,"['submission = pd.DataFrame({\n', '        ""PassengerId"": test_df[""PassengerId""],\n', '        ""Survived"": Y_pred\n', '    })\n', ""submission.to_csv('titanic.csv', index=False)""]"
train.info(),0,['train.info()']
CHECKPOINT df_map,0,"['#displaying the data frame\n', 'df_map']"
CHECKPOINT X.shape,0,['X.shape']
"CHECKPOINT ASSIGN = linear_model.Lasso(alpha=.001) ASSIGN.fit(x_t,Y_t) ASSIGN = model15.score(x_es,Y_es) print(ASSIGN*100,'%')",0,"['model15 = linear_model.Lasso(alpha=.001)\n', 'model15.fit(x_t,Y_t)\n', '\n', 'accuracy15 = model15.score(x_es,Y_es)\n', ""print(accuracy15*100,'%')""]"
"CHECKPOINT ASSIGN = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0009) ASSIGN = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False) ASSIGN = Input(shape=(224,224, 3)) ASSIGN = ResNet50(input_tensor= input_tensor, weights='imagenet', include_top=False) ASSIGN = base_model.output print(ASSIGN.shape) ASSIGN = squeeze_excitation_layer(base_output, 2048, ratio=4, concate=False) ASSIGN = BatchNormalization()(ASSIGN) ASSIGN = concatenate([base_output, ASSIGN], axis=3) ASSIGN = GlobalAveragePooling2D()(x) ASSIGN = Flatten()(ASSIGN) ASSIGN = concatenate([gap,ASSIGN]) ASSIGN = Dense(512, activation='relu')(ASSIGN) ASSIGN = BatchNormalization()(ASSIGN) ASSIGN = Dense(512, activation='relu')(ASSIGN) ASSIGN = BatchNormalization()(ASSIGN) ASSIGN = Dense(5, activation='softmax')(x) ASSIGN = Model(inputs=input_tensor, outputs=predict) for layer in (ASSIGN.layers): layer.trainable = False ASSIGN.compile(optimizer=ASSIGN, ASSIGN='categorical_crossentropy', ASSIGN=[keras.ASSIGN.categorical_accuracy])",0,"['\n', '\n', 'adam = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0009)\n', 'sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n', '\n', 'input_tensor = Input(shape=(224,224, 3))\n', '#backbone\n', ""base_model = ResNet50(input_tensor= input_tensor, weights='imagenet', include_top=False)\n"", 'base_output = base_model.output\n', 'print(base_output.shape)\n', '# channel-attention\n', 'x = squeeze_excitation_layer(base_output, 2048, ratio=4, concate=False)\n', 'x = BatchNormalization()(x)\n', '\n', '# #concat\n', 'x = concatenate([base_output, x], axis=3)\n', '# spp\n', '\n', 'gap = GlobalAveragePooling2D()(x)\n', 'x = Flatten()(x)\n', 'x = concatenate([gap,x])\n', ""x = Dense(512, activation='relu')(x)\n"", 'x = BatchNormalization()(x)\n', ""x = Dense(512, activation='relu')(x)\n"", 'x = BatchNormalization()(x)\n', ""predict = Dense(5, activation='softmax')(x)\n"", 'model = Model(inputs=input_tensor, outputs=predict)\n', '\n', 'for layer in (base_model.layers):\n', '    layer.trainable = False\n', '\n', 'model.compile(optimizer=adam,\n', ""                      loss='categorical_crossentropy',\n"", '                      metrics=[keras.metrics.categorical_accuracy])    \n', '\n', '# for l in model.layers:\n', '#   print(l.name)']"
"ASSIGN = tscovid.iloc[0:405, 4:54]",0,"['artime = tscovid.iloc[0:405, 4:54]']"
"ASSIGN = pd.DataFrame([['Kosovo'],[2000700],[168],[10887],[0],[19],['57%']]) ASSIGN = ASSIGN.T ASSIGN.columns = population.columns ASSIGN = ASSIGN.append(kosovo)",0,"[""kosovo = pd.DataFrame([['Kosovo'],[2000700],[168],[10887],[0],[19],['57%']])\n"", 'kosovo = kosovo.T\n', 'kosovo.columns = population.columns\n', '\n', '\n', 'population = population.append(kosovo)']"
"X_train, X_test, y_train, y_test = train_test_split(covid.drop('SARS-Cov-2 exam result',axis=1), covid['SARS-Cov-2 exam result'], test_size=0.25)",0,"[""X_train, X_test, y_train, y_test = train_test_split(covid.drop('SARS-Cov-2 exam result',axis=1), covid['SARS-Cov-2 exam result'], test_size=0.25)""]"
"ASSIGN = stats.boxcox(original_data) ASSIGN=plt.subplots(1,2) sns.distplot(original_data, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN[0], ax=ax[1]) ax[1].set_title(""Normalized data"")",1,"['# normalize the exponential data with boxcox\n', 'normalized_data = stats.boxcox(original_data)\n', '\n', '# plot both together to compare\n', 'fig, ax=plt.subplots(1,2)\n', 'sns.distplot(original_data, ax=ax[0])\n', 'ax[0].set_title(""Original Data"")\n', 'sns.distplot(normalized_data[0], ax=ax[1])\n', 'ax[1].set_title(""Normalized data"")']"
SETUP,0,['from fastai.vision import *']
SETUP,0,"['import requests\n', 'from bs4 import BeautifulSoup\n', 'import lxml\n', 'import os\n', 'import urllib\n', 'import sys\n', 'import pandas as pd\n', 'import numpy as np\n', 'from PIL import Image\n', 'import cv2\n', 'import csv\n', 'import multiprocessing\n', 'import matplotlib.pyplot as plt']"
"ASSIGN=plt.subplots(figsize=(25,15)) sns.boxplot(x=""OverallCond"", y=""SalePrice"", data=train,ax=ax) ax.set_title(""Boxplot of Price for OverallCond"",size=20)",1,"['fig,ax=plt.subplots(figsize=(25,15))\n', 'sns.boxplot(x=""OverallCond"", y=""SalePrice"", data=train,ax=ax)\n', 'ax.set_title(""Boxplot of Price for OverallCond"",size=20)']"
ASSIGN = pd.DataFrame() ASSIGN['Id'] = test_df['Id'] ASSIGN['sentiment'] = [int(p) for p in predictions],0,"['sub_df = pd.DataFrame()\n', ""sub_df['Id'] = test_df['Id']\n"", ""sub_df['sentiment'] = [int(p) for p in predictions]""]"
"ASSIGN = [Half1RMSE,Half2RMSE,Half3RMSE] FullRMSE = pd.concat(ASSIGN, axis = 1)",0,"['frames = [Half1RMSE,Half2RMSE,Half3RMSE] \n', 'FullRMSE = pd.concat(frames, axis = 1)']"
"data.show_batch(rows=3, figsize=(15,11))",1,"['data.show_batch(rows=3, figsize=(15,11))']"
"ASSIGN=plt.subplots(2,2,figsize=(20,20)) ASSIGN=data.year.value_counts() ASSIGN=data.year.value_counts().index sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,0]) ax[0,0].set_title(""The number of crimes by year"",size=20) ax[0,0].set_ylabel('counts',size=18) ax[0,0].set_xlabel('') ASSIGN=data.month.value_counts() ASSIGN=data.month.value_counts().index sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,1]) ax[0,1].set_title(""The number of crimes by month"",size=20) ax[0,1].set_ylabel('counts',size=18) ax[0,1].set_xlabel('') ASSIGN=data.hour.value_counts() ASSIGN=data.hour.value_counts().index sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[1,0]) ax[1,0].set_title(""The number of crimes by hour"",size=20) ax[1,0].set_ylabel('counts',size=18) ax[1,0].set_xlabel('') ASSIGN=data.NEIGHBORHOOD_ID.value_counts()[:10].index ASSIGN=data.NEIGHBORHOOD_ID.value_counts()[:10] sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[1,1]) ax[1,1].set_title(""The 10 NEIGHBORHOOD_ID by the number of crimes"",size=20) ax[1,1].set_xlabel('counts',size=18) ax[1,1].set_ylabel('')",1,"['fig,ax=plt.subplots(2,2,figsize=(20,20))\n', 'y=data.year.value_counts()\n', 'x=data.year.value_counts().index\n', 'sns.barplot(x=x,y=y,ax=ax[0,0])\n', 'ax[0,0].set_title(""The number of crimes by year"",size=20)\n', ""ax[0,0].set_ylabel('counts',size=18)\n"", ""ax[0,0].set_xlabel('')\n"", '\n', '\n', 'y=data.month.value_counts()\n', 'x=data.month.value_counts().index\n', 'sns.barplot(x=x,y=y,ax=ax[0,1])\n', 'ax[0,1].set_title(""The number of crimes by month"",size=20)\n', ""ax[0,1].set_ylabel('counts',size=18)\n"", ""ax[0,1].set_xlabel('')\n"", '\n', 'y=data.hour.value_counts()\n', 'x=data.hour.value_counts().index\n', 'sns.barplot(x=x,y=y,ax=ax[1,0])\n', 'ax[1,0].set_title(""The number of crimes by hour"",size=20)\n', ""ax[1,0].set_ylabel('counts',size=18)\n"", ""ax[1,0].set_xlabel('')\n"", '\n', '\n', '\n', '\n', 'y=data.NEIGHBORHOOD_ID.value_counts()[:10].index\n', 'x=data.NEIGHBORHOOD_ID.value_counts()[:10]\n', 'sns.barplot(x=x,y=y,ax=ax[1,1])\n', 'ax[1,1].set_title(""The 10 NEIGHBORHOOD_ID by the number of crimes"",size=20)\n', ""ax[1,1].set_xlabel('counts',size=18)\n"", ""ax[1,1].set_ylabel('')""]"
"SETUP CHECKPOINT ASSIGN=[72,71,56,45,67,89,54,58,67,77,77,78,77,73,73,172,72,71,56, 45,67,89,54,58,67,172,77,78,77,73,73,172,12,54,64,75,75,77, 88,66,70,12,54,64,75,75,77,88,66,70] def plot_his(ASSIGN): ASSIGN=min(heights)-min(heights)%10 ASSIGN=max(heights)+10 ASSIGN=list(range(start,end,5)) plt.hist(ASSIGN,ASSIGN,histtype='bar',rwidth=0.5,color='c') plt.xlabel('height of students (inches)') plt.ylabel('No.of Students') plt.show() print('Total Data') plot_his(ASSIGN) ASSIGN=list(filter(lambda x: not x==172 and not x==12, ASSIGN)) print('Normal Data') plot_his(ASSIGN)",1,"['#Q2.Store height of 50 srudents in inches.Now while the data was being recorded manually there has been some typing mistakes therefore height of two students has been recorded as 172 inches and 2 students recorded as 12 inches. Graphically plot and show how you can seggregate the normal data from the abnormal data.\n', 'import matplotlib.pyplot as plt\n', 'heights=[72,71,56,45,67,89,54,58,67,77,77,78,77,73,73,172,72,71,56,\n', '         45,67,89,54,58,67,172,77,78,77,73,73,172,12,54,64,75,75,77,\n', '         88,66,70,12,54,64,75,75,77,88,66,70]\n', 'def plot_his(heights):\n', '    start=min(heights)-min(heights)%10\n', '    end=max(heights)+10\n', '    bins=list(range(start,end,5))\n', ""    plt.hist(heights,bins,histtype='bar',rwidth=0.5,color='c')\n"", ""    plt.xlabel('height of students (inches)')\n"", ""    plt.ylabel('No.of Students')\n"", '    plt.show()\n', ""print('Total Data')\n"", 'plot_his(heights)\n', 'heights=list(filter(lambda x: not x==172 and not x==12, heights))\n', ""print('Normal Data')\n"", 'plot_his(heights)']"
learn.recorder.plot(),1,['learn.recorder.plot()']
SETUP,0,"['import numpy as np\n', 'import matplotlib.pyplot as plt\n', 'import pandas as pd\n', 'from tqdm import tqdm']"
ASSIGN = pd.get_dummies(ASSIGN).values.tolist(),0,"[""df['breed'] = pd.get_dummies(df['breed']).values.tolist()""]"
"ASSIGN = ClassificationInterpretation.from_learner(learn) ASSIGN.plot_top_losses(9, figsize=(12,8)) ASSIGN.plot_top_losses(9, figsize=(12,8), heatmap=False) ASSIGN.plot_confusion_matrix()",1,"['interp = ClassificationInterpretation.from_learner(learn)\n', 'interp.plot_top_losses(9, figsize=(12,8))\n', 'interp.plot_top_losses(9, figsize=(12,8), heatmap=False)\n', 'interp.plot_confusion_matrix()']"
CHECKPOINT print(test),0,['print(test)']
"ASSIGN = cnn_learner(data, models.resnet34, metrics=error_rate) ASSIGN.fit_one_cycle(4) ASSIGN.save('stage-1')",0,"['learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n', 'learn.fit_one_cycle(4)\n', ""learn.save('stage-1')""]"
CHECKPOINT ASSIGN=ASSIGN[ASSIGN>4000] print(ASSIGN),0,"['species_count=species_count[species_count>4000]\n', 'print(species_count)']"
"def lineplot(df, title = 'Sales by Year', ylabel ='Sales' , legendsize = 10, legendloc = 'upper left'): ASSIGN = df.index.values ASSIGN = df.NA_Sales ASSIGN = df.EU_Sales ASSIGN = df.JP_Sales ASSIGN = df.Other_Sales ASSIGN = df.Global_Sales if df is count_sales_group: ASSIGN = [na, eu, jp, other] ASSIGN = ['NA', 'EU', 'JP', 'OTHER'] else: ASSIGN = [na, eu, jp, other, global_] ASSIGN = ['NA', 'EU', 'JP', 'OTHER', 'WORLD WIDE'] for i, region in enumerate(ASSIGN): plt.plot(ASSIGN, region, label = ASSIGN[i]) plt.ylabel(ylabel) plt.xlabel('Year') plt.title(title) plt.legend(loc=legendloc, prop = {'size':legendsize}) plt.show() plt.clf() for i, region in enumerate(ASSIGN): plt.plot(ASSIGN, region, label = ASSIGN[i]) plt.yscale('log') plt.ylabel(ylabel) plt.xlabel('Year') plt.title(title + '(Log)') plt.legend(loc=legendloc, prop = {'size':legendsize}) plt.show() plt.clf()",1,"[""def lineplot(df, title = 'Sales by Year', ylabel ='Sales' , legendsize = 10, legendloc = 'upper left'):\n"", '\n', '    year = df.index.values\n', '    na = df.NA_Sales\n', '    eu = df.EU_Sales\n', '    jp = df.JP_Sales\n', '    other = df.Other_Sales\n', '    global_ = df.Global_Sales\n', '    \n', '    if df is count_sales_group:\n', '        region_list = [na, eu, jp, other]\n', ""        columns = ['NA', 'EU', 'JP', 'OTHER']\n"", '    else:\n', '        region_list = [na, eu, jp, other, global_]\n', ""        columns = ['NA', 'EU', 'JP', 'OTHER', 'WORLD WIDE']\n"", '\n', '    for i, region in enumerate(region_list):\n', '        plt.plot(year, region, label = columns[i])\n', '\n', '    plt.ylabel(ylabel)\n', ""    plt.xlabel('Year')\n"", '    plt.title(title)\n', ""    plt.legend(loc=legendloc, prop = {'size':legendsize})\n"", '    plt.show()\n', '    plt.clf()\n', '\n', '    for i, region in enumerate(region_list):\n', '        plt.plot(year, region, label = columns[i])\n', '\n', ""    plt.yscale('log')\n"", '    plt.ylabel(ylabel)\n', ""    plt.xlabel('Year')\n"", ""    plt.title(title + '(Log)')\n"", ""    plt.legend(loc=legendloc, prop = {'size':legendsize})\n"", '    plt.show()\n', '    plt.clf()']"
"ASSIGN = recover_dict['Mainland China'] ASSIGN = recover_dict['Italy'] ASSIGN = 0 for key in recover_dict: if key != 'Mainland China' and key != 'Italy': ASSIGN+=recover_dict[key] ASSIGN = ['China', 'Italy', 'Others'] ASSIGN = [china_r_number, italy_r_number, others_r] ASSIGN = ['Red', 'Green', 'Grey'] ASSIGN = (0.1, 0, 0) ASSIGN = ['Count'] ASSIGN = ['China', 'Italy', 'Others'] ASSIGN = [[china_r_number], [italy_r_number], [others_r]] ASSIGN = plt.subplots(1,2, figsize = (8,8)) axs[0].axis('tight') axs[0].axis('off') ASSIGN = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center') ASSIGN.set_fontsize(14) ASSIGN.scale(1.5, 1.5) axs[1].pie(ASSIGN, labels = ASSIGN, ASSIGN = ASSIGN, colors=ASSIGN, shadow=True, autopct='%1.1f%%') plt.title('Global Proportions of 2 severely striken countries') plt.show()",1,"[""china_r_number = recover_dict['Mainland China']\n"", ""italy_r_number = recover_dict['Italy']\n"", 'others_r = 0\n', 'for key in recover_dict:\n', ""    if key != 'Mainland China' and key != 'Italy':\n"", '        others_r+=recover_dict[key]\n', '        \n', ""groups = ['China', 'Italy', 'Others']\n"", 'sizes = [china_r_number, italy_r_number, others_r]\n', ""colours = ['Red', 'Green', 'Grey']\n"", 'explode = (0.1, 0, 0)\n', ""col_labels = ['Count']\n"", ""row_labels = ['China', 'Italy', 'Others']\n"", 'table_values = [[china_r_number], [italy_r_number], [others_r]]\n', '\n', '\n', 'fig, axs = plt.subplots(1,2, figsize = (8,8))\n', ""axs[0].axis('tight')\n"", ""axs[0].axis('off')\n"", ""the_table = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')\n"", 'the_table.set_fontsize(14)\n', 'the_table.scale(1.5, 1.5)\n', ""axs[1].pie(sizes, labels = groups, explode = explode, colors=colours, shadow=True, autopct='%1.1f%%')\n"", ""plt.title('Global Proportions of 2 severely striken countries')\n"", 'plt.show()']"
"print('Highest Calorie meal: {} \n\ Highest Fat meal: {} \n\ Highest Cholesterol meal: {} \n\ Highest Sodium meal: {} \n\ Highest Sugar meal: {} \n\ Highest Carb meal: {}'.format(menu.Item[menu['Calories'].idxmax()],\ menu.Item[menu['Total Fat'].idxmax()],\ menu.Item[menu['Cholesterol'].idxmax()],\ menu.Item[menu['Sodium'].idxmax()],\ menu.Item[menu['Sugars'].idxmax()],\ menu.Item[menu['Carbohydrates'].idxmax()]))",0,"[""print('Highest Calorie meal: {} \\n\\\n"", '       Highest Fat meal: {} \\n\\\n', '       Highest Cholesterol meal: {} \\n\\\n', '       Highest Sodium meal: {} \\n\\\n', '       Highest Sugar meal: {} \\n\\\n', ""       Highest Carb meal: {}'.format(menu.Item[menu['Calories'].idxmax()],\\\n"", ""                                     menu.Item[menu['Total Fat'].idxmax()],\\\n"", ""                                     menu.Item[menu['Cholesterol'].idxmax()],\\\n"", ""                                     menu.Item[menu['Sodium'].idxmax()],\\\n"", ""                                     menu.Item[menu['Sugars'].idxmax()],\\\n"", ""                                     menu.Item[menu['Carbohydrates'].idxmax()]))""]"
"CHECKPOINT ASSIGN = LinearRegression() ASSIGN.fit(x_train, Y_train) ASSIGN = model7.score(x_test,Y_test) print(ASSIGN*100,'%')",0,"['model7 = LinearRegression()\n', 'model7.fit(x_train, Y_train)\n', '\n', 'accuracy7 = model7.score(x_test,Y_test)\n', ""print(accuracy7*100,'%')""]"
final['Cabin_final'] = df['Cabin'].str[0],0,"[""final['Cabin_final'] = df['Cabin'].str[0]""]"
"ASSIGN=ASSIGN.dropna(subset=['Violations','Facility Type','Latitude','Longitude','AKA Name']) ASSIGN.isnull().sum()",0,"[""data=data.dropna(subset=['Violations','Facility Type','Latitude','Longitude','AKA Name'])\n"", 'data.isnull().sum()']"
"CHECKPOINT ASSIGN = train.drop([""label""], axis=1) ASSIGN = ASSIGN.values.astype('int32') print(ASSIGN) ASSIGN = 2 ASSIGN[ASSIGN <= ASSIGN] = 0 ASSIGN[ASSIGN > ASSIGN] = 1 print(ASSIGN)",0,"['#reset\n', 'X = train.drop([""label""], axis=1)\n', ""X = X.values.astype('int32')\n"", '\n', '# clean data\n', 'print(X)\n', 'cut = 2#50\n', 'X[X <= cut] = 0\n', 'X[X > cut] = 1\n', 'print(X)']"
"covid.reset_index(drop=True, inplace=True)",0,"['covid.reset_index(drop=True, inplace=True)']"
"SETUP os.environ[""CUDA_VISIBLE_DEVICES""] = ""2"" ASSIGN = '..path' ASSIGN = os.path.join(train_dir, 'train_images') ASSIGN = pd.read_csv(os.path.join(train_dir, 'ASSIGN.csv')) ASSIGN['ClassId_EncodedPixels'] = ASSIGN.apply(lambda row: (row['ClassId'], row['EncodedPixels']), axis = 1) ASSIGN = train.groupby('ImageId')['ClassId_EncodedPixels'].apply(list) ASSIGN=256 ASSIGN=256 ASSIGN=3 ASSIGN=10 ASSIGN=1 ASSIGN=ASSIGN.dropna(subset=['EncodedPixels'])",0,"['import os\n', 'os.environ[""CUDA_VISIBLE_DEVICES""] = ""2""\n', 'import sys\n', 'import pandas as pd\n', 'import numpy as np\n', 'import cv2\n', 'import matplotlib.pyplot as plt\n', 'import tensorflow as tf\n', 'import seaborn as sns\n', 'from PIL import Image\n', 'from sklearn.model_selection import train_test_split\n', ""train_dir = '../input/severstal-steel-defect-detection/' \n"", ""train_image_dir = os.path.join(train_dir, 'train_images') \n"", ""train = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n"", ""train['ClassId_EncodedPixels'] = train.apply(lambda row: (row['ClassId'], row['EncodedPixels']), axis = 1)\n"", ""grouped_EncodedPixels = train.groupby('ImageId')['ClassId_EncodedPixels'].apply(list)\n"", 'img_h=256\n', 'img_w=256\n', 'k_size=3\n', 'batch_size=10\n', 'epochs=1\n', ""train=train.dropna(subset=['EncodedPixels'])\n""]"
"ASSIGN = plt.subplots(num_images_to_show,4) axarr[0,0].title.set_text('Original \n Image') axarr[0,1].title.set_text('Reconstructed Image with \n 43% Compression') axarr[0,2].title.set_text('Reconstructed Image with \n 68% Compression') axarr[0,3].title.set_text('Reconstructed Image with \n 84% Compression') for i in range(4): axarr[0,i].title.set_fontsize(15) for i in range(num_images_to_show): axarr[i,0].imshow((valid_batch[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5) axarr[i,1].imshow((reconstructed_img_28[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5) axarr[i,2].imshow((reconstructed_img_16[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5) axarr[i,3].imshow((reconstructed_img_8[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5) f.set_figheight(20) f.set_figwidth(20) plt.show()",1,"['\n', 'f, axarr = plt.subplots(num_images_to_show,4)\n', '\n', ""axarr[0,0].title.set_text('Original \\n Image')\n"", ""axarr[0,1].title.set_text('Reconstructed Image with \\n 43% Compression')\n"", ""axarr[0,2].title.set_text('Reconstructed Image with \\n 68% Compression')\n"", ""axarr[0,3].title.set_text('Reconstructed Image with \\n 84% Compression')\n"", '\n', 'for i in range(4):\n', '    axarr[0,i].title.set_fontsize(15)\n', '\n', 'for i in range(num_images_to_show):\n', '    axarr[i,0].imshow((valid_batch[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5)\n', '    axarr[i,1].imshow((reconstructed_img_28[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5)\n', '    axarr[i,2].imshow((reconstructed_img_16[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5)\n', '    axarr[i,3].imshow((reconstructed_img_8[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5)\n', '    f.set_figheight(20)\n', '    f.set_figwidth(20)\n', 'plt.show()']"
ASSIGN = [] for i in range(len(train)): if (train.Date[i] == day1) or (train.Date[i] == day2) or (train.Date[i] == day3) or (train.Date[i] == day4): ASSIGN.append(i) ASSIGN = (ASSIGN.drop(index = index_delete)).reset_index(drop = True),0,"['index_delete = []\n', 'for i in range(len(train)):\n', '    if (train.Date[i] == day1) or (train.Date[i] == day2) or (train.Date[i] == day3) or (train.Date[i] == day4):\n', '        index_delete.append(i)\n', '        \n', 'train = (train.drop(index = index_delete)).reset_index(drop = True)']"
"sns.heatmap(train.isnull(), cbar=False)",1,"['sns.heatmap(train.isnull(), cbar=False)']"
"SETUP X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)",0,"['#Split data into training and testing sets\n', 'from sklearn.model_selection import train_test_split\n', 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)']"
"CHECKPOINT ASSIGN = np.unique(cars[..., 0].astype(np.uint8), return_counts=True) ASSIGN = zip(unique, counts) for i, model_type in enumerate(ASSIGN): print('{}.\t Model type: {:<22} | {} cars'.format(i, models_map[model_type[0]], model_type[1]))",0,"['unique, counts = np.unique(cars[..., 0].astype(np.uint8), return_counts=True)\n', 'all_model_types = zip(unique, counts)\n', '\n', 'for i, model_type in enumerate(all_model_types):\n', ""    print('{}.\\t Model type: {:<22} | {} cars'.format(i, models_map[model_type[0]], model_type[1]))""]"
"ASSIGN = [20000 ,20000, 22500, 10000]",0,"['minsizes = [20000 ,20000, 22500, 10000]\n']"
CHECKPOINT final,0,['final\n']
"SETUP Image(filename = 'inf_348a992bb.jpg', width = 512, height = 512)",0,"['#Import library\n', 'from IPython.display import Image# Load image from local storage\n', ""Image(filename = 'inf_348a992bb.jpg', width = 512, height = 512)""]"
"ASSIGN=plt.subplots(3,1,figsize=(15,36)) sns.boxplot(x=""neighbourhood_group"", y=""price"", data=data[data.room_type=='Shared room'],ax=ax[0]) ax[0].set_title(""Boxplot of Price for 'Shared room' in each neighbourhood_group"",size=20) sns.boxplot(x=""neighbourhood_group"", y=""price"", data=data[data.room_type=='Private room'],ax=ax[1]) ax[1].set_title(""Boxplot of Price for 'Private room' in each neighbourhood_group"",size=20) sns.boxplot(x=""neighbourhood_group"", y=""price"", data=data[data.room_type=='Entire homepath'],ax=ax[2]) ax[2].set_title(""Boxplot of Price for 'Entire homepath' in each neighbourhood_group"",size=20)",1,"['fig,ax=plt.subplots(3,1,figsize=(15,36))\n', 'sns.boxplot(x=""neighbourhood_group"", y=""price"", data=data[data.room_type==\'Shared room\'],ax=ax[0])\n', 'ax[0].set_title(""Boxplot of Price for \'Shared room\' in each neighbourhood_group"",size=20)\n', '\n', 'sns.boxplot(x=""neighbourhood_group"", y=""price"", data=data[data.room_type==\'Private room\'],ax=ax[1])\n', 'ax[1].set_title(""Boxplot of Price for \'Private room\' in each neighbourhood_group"",size=20)\n', '\n', 'sns.boxplot(x=""neighbourhood_group"", y=""price"", data=data[data.room_type==\'Entire home/apt\'],ax=ax[2])\n', 'ax[2].set_title(""Boxplot of Price for \'Entire home/apt\' in each neighbourhood_group"",size=20)']"
ASSIGN = 2,0,['scale_ratio = 2']
def nan2mean(df): for x in list(df.columns.values): if x in numerical: ASSIGN = ASSIGN.fillna(0) return df ASSIGN=nan2mean(ASSIGN) ASSIGN=nan2mean(ASSIGN),0,"['def nan2mean(df):\n', '    for x in list(df.columns.values):\n', '        if x in numerical:\n', '            #print(""___________________""+x)\n', '            #print(df[x].isna().sum())\n', '            df[x] = df[x].fillna(0)\n', '           #print(""Mean-""+str(df[x].mean()))\n', '    return df\n', 'train_df=nan2mean(train_df)\n', 'test_df=nan2mean(test_df)']"
learn.recorder.plot_losses(),1,['learn.recorder.plot_losses()']
"df_final.drop(columns=[""PAGE NO""],axis=1,inplace=True)",0,"['#header column ""PAGE NO"" is not required ,i used it while scraping the data from zomato to do some sort of validation,lets remove the column\n', 'df_final.drop(columns=[""PAGE NO""],axis=1,inplace=True)']"
ASSIGN = pca.transform(scaled_data),0,['x_pca = pca.transform(scaled_data)']
"ASSIGN=plt.subplots(1,2,figsize=(15,8)) ASSIGN = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",'cadetblue','hotpink','orange','darksalmon','brown') data_Queens.neighbourhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=ASSIGN,ax=ax[0]) ax[0].set_title(""Top 10 neighbourhood by the number of rooms"",size=20) ax[0].set_xlabel('rooms',size=18) ASSIGN=data_Queens['neighbourhood'].value_counts() ASSIGN=list(data_Queens['neighbourhood'].value_counts().index)[:10] ASSIGN=list(count[:10]) ASSIGN.append(ASSIGN.agg(sum)-ASSIGN[:10].agg('sum')) ASSIGN.append('Other') ASSIGN=pd.DataFrame({""group"":groups,""counts"":counts}) ASSIGN=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum') ASSIGN = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1]) plt.legend(loc=0, bbox_to_anchor=(1.15,0.4)) plt.subplots_adjust(wspace =0.5, hspace =0) plt.ylabel('')",1,"['fig,ax=plt.subplots(1,2,figsize=(15,8))\n', 'clr = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",\'cadetblue\',\'hotpink\',\'orange\',\'darksalmon\',\'brown\')\n', ""data_Queens.neighbourhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n"", 'ax[0].set_title(""Top 10 neighbourhood by the number of rooms"",size=20)\n', ""ax[0].set_xlabel('rooms',size=18)\n"", '\n', '\n', ""count=data_Queens['neighbourhood'].value_counts()\n"", ""groups=list(data_Queens['neighbourhood'].value_counts().index)[:10]\n"", 'counts=list(count[:10])\n', ""counts.append(count.agg(sum)-count[:10].agg('sum'))\n"", ""groups.append('Other')\n"", 'type_dict=pd.DataFrame({""group"":groups,""counts"":counts})\n', ""clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n"", ""qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n"", 'plt.legend(loc=0, bbox_to_anchor=(1.15,0.4)) \n', 'plt.subplots_adjust(wspace =0.5, hspace =0)\n', ""plt.ylabel('')""]"
SETUP SETUP ASSIGN = os.listdir(INPUT_FOLDER) ASSIGN.sort(),0,"['%matplotlib inline\n', '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import dicom\n', 'import os\n', 'import scipy.ndimage\n', 'import matplotlib.pyplot as plt\n', '\n', 'from skimage import measure, morphology\n', 'from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n', '\n', '# Some constants \n', ""INPUT_FOLDER = '../input/sample_images/'\n"", 'patients = os.listdir(INPUT_FOLDER)\n', 'patients.sort()']"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") print() print(ASSIGN.keys()) print() print(ASSIGN.shape) print() print(type(ASSIGN))",0,"['#Q1.printing the keys, number of rows-columns, feature names \n', '#and the description of the Iris data.\n', 'import pandas as pd\n', 'iris_data = pd.read_csv(""../input/iris-dataset/iris.data.csv"")\n', 'print(""\\nKeys of Iris dataset:"")\n', 'print(iris_data.keys())\n', 'print(""\\nNumber of rows and columns of Iris dataset:"")\n', 'print(iris_data.shape)\n', 'print(""Data type:"")\n', 'print(type(iris_data))']"
"SETUP def auroc_score(input, target): ASSIGN = input.cpu().numpy()[:,1], target.cpu().numpy() return roc_auc_score(target, input) class AUROC(Callback): ASSIGN = -20 def __init__(self, learn, **kwargs): self.learn = learn def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['AUROC']) def on_epoch_begin(self, **kwargs): self.output, self.target = [], [] def on_batch_end(self, last_target, last_output, train, **kwargs): if not train: self.output.append(last_output) self.target.append(last_target) def on_epoch_end(self, last_metrics, **kwargs): if len(self.output) > 0: ASSIGN = torch.cat(self.ASSIGN) ASSIGN = torch.cat(self.ASSIGN) ASSIGN = F.softmax(output, dim=1) ASSIGN = auroc_score(preds, target) return add_metrics(last_metrics, [ASSIGN])",0,"['from fastai.tabular import *\n', 'from sklearn.metrics import roc_auc_score\n', '\n', 'def auroc_score(input, target):\n', '    input, target = input.cpu().numpy()[:,1], target.cpu().numpy()\n', '    return roc_auc_score(target, input)\n', '\n', 'class AUROC(Callback):\n', '    _order = -20 #Needs to run before the recorder\n', '\n', '    def __init__(self, learn, **kwargs): self.learn = learn\n', ""    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['AUROC'])\n"", '    def on_epoch_begin(self, **kwargs): self.output, self.target = [], []\n', '    \n', '    def on_batch_end(self, last_target, last_output, train, **kwargs):\n', '        if not train:\n', '            self.output.append(last_output)\n', '            self.target.append(last_target)\n', '                \n', '    def on_epoch_end(self, last_metrics, **kwargs):\n', '        if len(self.output) > 0:\n', '            output = torch.cat(self.output)\n', '            target = torch.cat(self.target)\n', '            preds = F.softmax(output, dim=1)\n', '            metric = auroc_score(preds, target)\n', '            return add_metrics(last_metrics, [metric])']"
"model.load_weights(""model.h5"")",0,"['model.load_weights(""model.h5"")']"
"ASSIGN = Ridge() ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]} ASSIGN = GridSearchCV(ASSIGN, parameters, scoring='neg_mean_squared_error', cv = 100) ASSIGN.fit(x_t, Y_t) ASSIGN.best_params_",0,"['r = Ridge()\n', '\n', ""parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n"", '\n', ""r = GridSearchCV(r, parameters, scoring='neg_mean_squared_error', cv = 100)\n"", '\n', 'r.fit(x_t, Y_t)\n', 'r.best_params_']"
SETUP,0,"['from sklearn.model_selection import train_test_split\n', 'from sklearn.linear_model import LogisticRegression']"
ASSIGN = df.sum(axis=0),0,['total = df.sum(axis=0)']
"CHECKPOINT def plotCorrelationMatrix(df, graphWidth): ASSIGN = df.dataframeName ASSIGN = ASSIGN.dropna('columns') ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]] if ASSIGN.shape[1] < 2: print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2') return ASSIGN = df.ASSIGN() plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k') ASSIGN = plt.matshow(corr, fignum = 1) plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90) plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns) plt.gca().xaxis.tick_bottom() plt.colorbar(ASSIGN) plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15) plt.show()",1,"['# Correlation matrix\n', 'def plotCorrelationMatrix(df, graphWidth):\n', '    filename = df.dataframeName\n', ""    df = df.dropna('columns') # drop columns with NaN\n"", '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n', '    if df.shape[1] < 2:\n', ""        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n"", '        return\n', '    corr = df.corr()\n', ""    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n"", '    corrMat = plt.matshow(corr, fignum = 1)\n', '    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n', '    plt.yticks(range(len(corr.columns)), corr.columns)\n', '    plt.gca().xaxis.tick_bottom()\n', '    plt.colorbar(corrMat)\n', ""    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n"", '    plt.show()\n']"
SETUP def tf_seed_everything(seed): os.environ['PYTHONHASHSEED'] = str(seed) random.seed(seed) np.random.seed(seed) tf.set_random_seed(seed),0,"['def tf_seed_everything(seed):\n', '    import tensorflow as tf\n', ""    os.environ['PYTHONHASHSEED'] = str(seed)\n"", '    random.seed(seed)\n', '    np.random.seed(seed)\n', '    tf.set_random_seed(seed)']"
"titanic_df['Family'] =  titanic_df[""Parch""] + titanic_df[""SibSp""] titanic_df['Family'].loc[titanic_df['Family'] > 0] = 1 titanic_df['Family'].loc[titanic_df['Family'] == 0] = 0 test_df['Family'] =  test_df[""Parch""] + test_df[""SibSp""] test_df['Family'].loc[test_df['Family'] > 0] = 1 test_df['Family'].loc[test_df['Family'] == 0] = 0 ASSIGN = ASSIGN.drop(['SibSp','Parch'], axis=1) ASSIGN  = ASSIGN.drop(['SibSp','Parch'], axis=1) fig, (axis1,axis2) = plt.subplots(1,2,sharex=True,figsize=(10,5)) sns.countplot(x='Family', data=ASSIGN, order=[1,0], ax=axis1) ASSIGN = titanic_df[[""Family"", ""Survived""]].groupby(['Family'],as_index=False).mean() sns.barplot(x='Family', y='Survived', data=ASSIGN, order=[1,0], ax=axis2) axis1.set_xticklabels([""With Family"",""Alone""], rotation=0)",1,"['# Family\n', '\n', '# Instead of having two columns Parch & SibSp, \n', '# we can have only one column represent if the passenger had any family member aboard or not,\n', '# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.\n', 'titanic_df[\'Family\'] =  titanic_df[""Parch""] + titanic_df[""SibSp""]\n', ""titanic_df['Family'].loc[titanic_df['Family'] > 0] = 1\n"", ""titanic_df['Family'].loc[titanic_df['Family'] == 0] = 0\n"", '\n', 'test_df[\'Family\'] =  test_df[""Parch""] + test_df[""SibSp""]\n', ""test_df['Family'].loc[test_df['Family'] > 0] = 1\n"", ""test_df['Family'].loc[test_df['Family'] == 0] = 0\n"", '\n', '# drop Parch & SibSp\n', ""titanic_df = titanic_df.drop(['SibSp','Parch'], axis=1)\n"", ""test_df    = test_df.drop(['SibSp','Parch'], axis=1)\n"", '\n', '# plot\n', 'fig, (axis1,axis2) = plt.subplots(1,2,sharex=True,figsize=(10,5))\n', '\n', ""# sns.factorplot('Family',data=titanic_df,kind='count',ax=axis1)\n"", ""sns.countplot(x='Family', data=titanic_df, order=[1,0], ax=axis1)\n"", '\n', ""# average of survived for those who had/didn't have any family member\n"", 'family_perc = titanic_df[[""Family"", ""Survived""]].groupby([\'Family\'],as_index=False).mean()\n', ""sns.barplot(x='Family', y='Survived', data=family_perc, order=[1,0], ax=axis2)\n"", '\n', 'axis1.set_xticklabels([""With Family"",""Alone""], rotation=0)']"
logisticRegr.predict(X_train_pca),0,['logisticRegr.predict(X_train_pca)']
len(train_transaction_new),0,['len(train_transaction_new)']
"ASSIGN=len(train) ASSIGN=len(train)+len(test)-1 ASSIGN = AR2fit.predict(start=start, end=end).rename('AR(2) Predictions')",0,"['start=len(train)\n', 'end=len(train)+len(test)-1\n', ""predictions2 = AR2fit.predict(start=start, end=end).rename('AR(2) Predictions')""]"
"ASSIGN = gbc.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('GradientBoosting(HT).csv',index = False)",0,"['model17pred = gbc.predict(y_train)\n', ""submission17 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission17['PassengerId'] = result['PassengerId']\n"", ""submission17['Survived'] = model17pred\n"", ""submission17.to_csv('GradientBoosting(HT).csv',index = False)""]"
"list(os.listdir(""..path""))",0,"['list(os.listdir(""../input/rsna-str-pulmonary-embolism-detection""))']"
"ASSIGN = pd.read_csv(os.path.join(DATASET_DIR, 'train.csv'))",0,"[""df = pd.read_csv(os.path.join(DATASET_DIR, 'train.csv'))""]"
"submission.to_csv('submission.csv', index=False)",0,"[""submission.to_csv('submission.csv', index=False)\n""]"
"sns.jointplot(x = 'GRE Score', y = 'CGPA', data=df)",1,"[""sns.jointplot(x = 'GRE Score', y = 'CGPA', data=df)""]"
"SETUP X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)",0,"['from sklearn.model_selection import train_test_split\n', 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)']"
val_counts_image_id.head(),0,['val_counts_image_id.head() # ']
"knn.fit(x_train1,x_test) knn.score(y_train1,y_test)",0,"['knn.fit(x_train1,x_test)\n', 'knn.score(y_train1,y_test)']"
ASSIGN=pd.read_csv('..path') ASSIGN=data[data.isnull().any(axis=1)].head() ASSIGN.iloc[0:3],0,"[""data=pd.read_csv('../input/flight-route-database/routes.csv')\n"", 'nan=data[data.isnull().any(axis=1)].head()\n', 'nan.iloc[0:3]']"
SETUP,0,"['import numpy as np\n', 'import pandas as pd\n', 'import os']"
val_counts_image_id['image_id'][cond_1 | cond_2],0,"[""val_counts_image_id['image_id'][cond_1 | cond_2]""]"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") print(ASSIGN.info())",0,"['# Write a Python program to get the number of observations, missing values and nan values.\n', 'import pandas as pd\n', 'iris = pd.read_csv(""../input/iriscsv/Iris.csv"")\n', 'print(iris.info())']"
ASSIGN = train_data.pop('id') ASSIGN = test_data.pop('id') ASSIGN = train_data.pop('species') ASSIGN = LabelEncoder().fit(ASSIGN).transform(ASSIGN) ASSIGN = to_categorical(ASSIGN) ASSIGN = StandardScaler().fit(train_data).transform(train_data) ASSIGN = StandardScaler().fit(test_data).transform(test_data),0,"[""# remove column 'id' from train data und save in variable train_id\n"", ""train_id = train_data.pop('id')\n"", ""test_id = test_data.pop('id')\n"", '\n', ""# remove column 'species' from train data und save in variable train_y, then transform into categorical\n"", ""train_y = train_data.pop('species')\n"", 'train_y = LabelEncoder().fit(train_y).transform(train_y)\n', 'train_y = to_categorical(train_y)\n', '\n', '#scale training data\n', 'train_x = StandardScaler().fit(train_data).transform(train_data)\n', 'test_x = StandardScaler().fit(test_data).transform(test_data)']"
"ASSIGN = output1.groupby(['Models'])['Accuracy'].mean().reset_index().sort_values(by='Accuracy',ascending=False) ASSIGN.head(10).style.background_gradient(cmap='Reds')",0,"[""o = output1.groupby(['Models'])['Accuracy'].mean().reset_index().sort_values(by='Accuracy',ascending=False)\n"", ""o.head(10).style.background_gradient(cmap='Reds')\n""]"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
"CHECKPOINT for param in model.bert.parameters(): param.requires_grad = False for name, param in model.named_parameters(): if param.requires_grad: print(name)",0,"['for param in model.bert.parameters():\n', '  param.requires_grad = False\n', '\n', 'for name, param in model.named_parameters():                \n', '    if param.requires_grad:\n', '        print(name)']"
test[test.TransactionDT>2.5e7][train_more_id]=imp.fit_transform(test[test.TransactionDT>2.5e7][train_more_id]) train[train_more_id]=imp.fit_transform(train[train_more_id]),0,"['test[test.TransactionDT>2.5e7][train_more_id]=imp.fit_transform(test[test.TransactionDT>2.5e7][train_more_id])\n', 'train[train_more_id]=imp.fit_transform(train[train_more_id])']"
CHECKPOINT print() is.numeric(recpies$rating),0,"['# are the ratings all numeric?\n', 'print(""Is this variable numeric?"")\n', 'is.numeric(recpies$rating)']"
"CHECKPOINT ASSIGN = model16.predict(x_es) ASSIGN = mean_squared_error(Y_es, y_pred16, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, ASSIGN)))",0,"['y_pred16 = model16.predict(x_es)\n', '\n', 'val = mean_squared_error(Y_es, y_pred16, squared=False)\n', 'val16 = str(round(val, 4))\n', '\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred16)))\n""]"
"sns.catplot(x='Year', y='Total Atrocities', data=cbdr,height = 5, aspect = 4)",1,"[""sns.catplot(x='Year', y='Total Atrocities', data=cbdr,height = 5, aspect = 4)""]"
CHECKPOINT train.columns,0,['train.columns']
"SETUP ASSIGN = ElasticNet() ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]} ENR.fit(x_train, Y_train) ENR.best_params_",0,"['EN = ElasticNet()\n', '\n', ""parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n"", '\n', ""ENR = GridSearchCV(Elasticnet, parameters, scoring='neg_mean_squared_error', cv = 100)\n"", '\n', 'ENR.fit(x_train, Y_train)\n', 'ENR.best_params_']"
SETUP ASSIGN = StandardScaler() ASSIGN.fit(CC),0,"['from sklearn.preprocessing import StandardScaler\n', 'scaler = StandardScaler()\n', 'scaler.fit(CC)\n']"
final['Title'].value_counts(),0,"[""final['Title'].value_counts()""]"
"SETUP ASSIGN = None, None ASSIGN = ['EmployeeNumber', 'Over18', 'StandardHours'] for col in [var for var in category_cols if var not in ASSIGN]: ASSIGN = LabelEncoder(), OneHotEncoder() lbe.fit(pd.concat([train[col], test[col]]).values.reshape(-1, 1)) ASSIGN = lbe.transform(ASSIGN) ASSIGN = lbe.transform(ASSIGN) ohe.fit(pd.concat([train[col], test[col]]).values.reshape(-1, 1)) ASSIGN = ohe.transform(train[col].values.reshape(-1, 1)).todense() ASSIGN = ohe.transform(test[col].values.reshape(-1, 1)).todense() if train_category is None: ASSIGN = oht_train ASSIGN = oht_test else: ASSIGN = np.hstack((ASSIGN, oht_train)) ASSIGN = np.hstack((ASSIGN, oht_test))",0,"['from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n', '\n', 'train_category, test_category = None, None\n', ""drop_cols = ['EmployeeNumber', 'Over18', 'StandardHours']\n"", 'for col in [var for var in category_cols if var not in drop_cols]:\n', '    lbe, ohe = LabelEncoder(), OneHotEncoder()\n', '    \n', '    lbe.fit(pd.concat([train[col], test[col]]).values.reshape(-1, 1))\n', '    train[col] = lbe.transform(train[col])\n', '    test[col] = lbe.transform(test[col])\n', '    \n', '    ohe.fit(pd.concat([train[col], test[col]]).values.reshape(-1, 1))\n', '    oht_train = ohe.transform(train[col].values.reshape(-1, 1)).todense()\n', '    oht_test = ohe.transform(test[col].values.reshape(-1, 1)).todense()\n', '    \n', '    if train_category is None:\n', '        train_category = oht_train\n', '        test_category = oht_test\n', '    else:\n', '        train_category = np.hstack((train_category, oht_train))\n', '        test_category = np.hstack((test_category, oht_test))']"
"ASSIGN=pd.read_csv(""..path"") ASSIGN.head()",0,"['data=pd.read_csv(""../input/chicago-food-inspections/food-inspections.csv"")\n', 'data.head()']"
"SETUP CHECKPOINT ASSIGN = fold.ASSIGN ASSIGN = fold.holdout_idx ASSIGN = input_df.loc[fold.holdout_idx, 'target'] ASSIGN = [] ASSIGN = [] for li, learn in enumerate([learn1, learn2, learn3, learn4]): for ep in [5]: if ep >= len(learn.epoch_params): continue learn.restore_epoch(ep) ASSIGN.append(learn.predict(idx=ASSIGN)) ASSIGN = f1_curve(hold_y, ensemble_hold[-1]).max() print('learn%d ep%d %.6f' % (li+1, ep, ASSIGN)) ASSIGN.append(learn.predict(idx=ASSIGN)) ASSIGN = pd.Series(np.mean(ensemble_test, axis=0), index=fold.data.loc[test_idx, 'qid']) ASSIGN = f1_curve(hold_y, np.mean(ensemble_hold, axis=0)) ASSIGN = score.idxmax() print('F1score=',ASSIGN.max()) ASSIGN = pd.read_csv('..path') ASSIGN['prediction'] = (ASSIGN.loc[ASSIGN.qid] >= ASSIGN).astype(int).values ASSIGN.to_csv('submission.csv', index=False)",0,"['%%time\n', 'test_idx = fold.test_idx\n', 'hold_idx = fold.holdout_idx\n', ""hold_y = input_df.loc[fold.holdout_idx, 'target']\n"", 'ensemble_hold = []\n', 'ensemble_test = []\n', 'for li, learn in enumerate([learn1, learn2, learn3, learn4]):\n', '    for ep in [5]:\n', '        if ep >= len(learn.epoch_params): continue\n', '        learn.restore_epoch(ep)\n', '        ensemble_hold.append(learn.predict(idx=hold_idx))\n', '        f1 = f1_curve(hold_y, ensemble_hold[-1]).max()\n', ""        print('learn%d ep%d %.6f' % (li+1, ep, f1))\n"", '        ensemble_test.append(learn.predict(idx=test_idx))\n', '\n', ""ensemble_test_s = pd.Series(np.mean(ensemble_test, axis=0), index=fold.data.loc[test_idx, 'qid'])\n"", '\n', '\n', '\n', '\n', '\n', 'score = f1_curve(hold_y, np.mean(ensemble_hold, axis=0))\n', 'thresh = score.idxmax()\n', ""print('F1score=',score.max())\n"", '\n', '\n', '\n', ""submission_df = pd.read_csv('../input/sample_submission.csv')\n"", ""submission_df['prediction'] = (ensemble_test_s.loc[submission_df.qid] >= thresh).astype(int).values\n"", ""submission_df.to_csv('submission.csv', index=False)""]"
"ASSIGN = tumor_data.drop(['target'], axis = 1) ASSIGN = tumor_data['target']",0,"['# Independent variables\n', ""X = tumor_data.drop(['target'], axis = 1)\n"", '# Dependent variable\n', ""Y = tumor_data['target']""]"
train.API_beds[train.API_beds.isnull()] = np.nanmedian(train.API_beds) test.API_beds[test.API_beds.isnull()] = np.nanmedian(test.API_beds),0,"['#fill NAN\n', 'train.API_beds[train.API_beds.isnull()] = np.nanmedian(train.API_beds)\n', 'test.API_beds[test.API_beds.isnull()] = np.nanmedian(test.API_beds)']"
"ASSIGN = pd.DataFrame(df_outlier_prob.sort_values(by='target',ascending = False).head(25000)['card_id'])",0,"[""outlier_id = pd.DataFrame(df_outlier_prob.sort_values(by='target',ascending = False).head(25000)['card_id'])""]"
"ASSIGN = train_df[target[0]].unique() ASSIGN = [] for i in ASSIGN: ASSIGN = len(train_df[train_df[target[0]]==i]) ASSIGN.append(ASSIGN) ASSIGN=categories ASSIGN=val ASSIGN=['green','red','orange','blue','darkorange','grey','pink'] plt.axis('equal') plt.title('target classes distribution') plt.pie(ASSIGN, explode=(0,0,0,0,0,0,0), ASSIGN=ASSIGN,ASSIGN=ASSIGN,autopct='%1.2f%%', shadow=True, startangle=90) plt.show()",1,"['categories = train_df[target[0]].unique()\n', 'val = []\n', 'for i in categories:\n', '    temp = len(train_df[train_df[target[0]]==i])\n', '    val.append(temp)\n', 'labels=categories\n', 'sizes=val\n', ""colors=['green','red','orange','blue','darkorange','grey','pink']\n"", ""plt.axis('equal')\n"", ""plt.title('target classes distribution')\n"", ""plt.pie(sizes, explode=(0,0,0,0,0,0,0), labels=labels,colors=colors,autopct='%1.2f%%', shadow=True, startangle=90)\n"", 'plt.show()                        ']"
ASSIGN = Prophet(interval_width=0.97) ASSIGN.fit(death) ASSIGN = m.make_future_dataframe(periods=29) ASSIGN = future.copy() ASSIGN.tail(),0,"['m = Prophet(interval_width=0.97)\n', 'm.fit(death)\n', 'future = m.make_future_dataframe(periods=29)\n', 'future_deaths = future.copy() \n', 'future.tail()']"
"ASSIGN = int(ASSIGN.view(1,ASSIGN.shape[0],ASSIGN.shape[1],ASSIGN.shape[2]) * 256) ASSIGN = int(ASSIGN.view(1,ASSIGN.shape[0],ASSIGN.shape[1],ASSIGN.shape[2]) *256) ASSIGN = int(ASSIGN.view(1,ASSIGN.shape[0],ASSIGN.shape[1],ASSIGN.shape[2]) *256) ASSIGN = int(ASSIGN.view(1,ASSIGN.shape[0],ASSIGN.shape[1],ASSIGN.shape[2]) *256)",0,"['reimg = int(reimg.view(1,reimg.shape[0],reimg.shape[1],reimg.shape[2]) * 256)\n', 'reimg_28 = int(reimg_28.view(1,reimg_28.shape[0],reimg_28.shape[1],reimg_28.shape[2]) *256)\n', 'reimg_16 = int(reimg_16.view(1,reimg_16.shape[0],reimg_16.shape[1],reimg_16.shape[2]) *256)\n', 'reimg_8 = int(reimg_8.view(1,reimg_8.shape[0],reimg_8.shape[1],reimg_8.shape[2]) *256)']"
"ASSIGN = final['Name'].str.split('.', n=1, expand = True) ASSIGN = ASSIGN ASSIGN = ASSIGN ASSIGN = final['First'].str.split(',', n=1, expand = True) final['Last Name'] = ASSIGN[0] ASSIGN = new1[1] ASSIGN = final['Title'].str.split('', n=1, expand = True)",0,"[""new = final['Name'].str.split('.', n=1, expand = True)\n"", ""final['First'] = new[0]\n"", ""final['Last'] = new[1]\n"", ""new1 = final['First'].str.split(',', n=1, expand = True)\n"", ""final['Last Name'] = new1[0]\n"", ""final['Title'] = new1[1]\n"", ""new2 = final['Title'].str.split('', n=1, expand = True)""]"
CHECKPOINT print() print(+str(sum(Y_train==1))++str(sum(Y_train==0))+) print(+str(sum(Y_test==1))++str(sum(Y_test==0))+),0,"['print(""Distribution of cats and dogs in the different sets"")\n', 'print(""TRAIN  :  ""+str(sum(Y_train==1))+"" cats vs ""+str(sum(Y_train==0))+"" dogs"")\n', 'print(""TEST  :  ""+str(sum(Y_test==1))+"" cats vs ""+str(sum(Y_test==0))+"" dogs"")']"
CHECKPOINT print(os.listdir('..path')),0,"[""print(os.listdir('../input'))""]"
ASSIGN = learn.get_preds(DatasetType.Test),0,['test_pred = learn.get_preds(DatasetType.Test)']
"model.eval() ASSIGN = [], [] for batch in prediction_dataloader: ASSIGN = tuple(t.to(device) for t in ASSIGN) ASSIGN = batch with torch.no_grad(): ASSIGN = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) ASSIGN = ASSIGN.detach().cpu().numpy() ASSIGN = b_labels.to('cpu').numpy() predictions.append(ASSIGN) true_labels.append(ASSIGN)",0,"['model.eval()\n', '\n', '# Tracking variables \n', 'predictions , true_labels = [], []\n', '\n', '# Predict \n', 'for batch in prediction_dataloader:\n', '  # Add batch to GPU\n', '  batch = tuple(t.to(device) for t in batch)\n', '  # Unpack the inputs from our dataloader\n', '  b_input_ids, b_input_mask, b_labels = batch\n', '  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n', '  with torch.no_grad():\n', '    # Forward pass, calculate logit predictions\n', '    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n', '\n', '  # Move logits and labels to CPU\n', '  logits = logits.detach().cpu().numpy()\n', ""  label_ids = b_labels.to('cpu').numpy()\n"", '  \n', '  # Store predictions and true labels\n', '  predictions.append(logits)\n', '  true_labels.append(label_ids)']"
SETUP CHECKPOINT warnings.filterwarnings('ignore') sns.set() print(os.listdir('..path')),0,"['from sklearn import model_selection, preprocessing, linear_model, metrics\n', 'from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n', 'from sklearn import decomposition, ensemble\n', 'from sklearn.model_selection import GridSearchCV, cross_val_score\n', 'from sklearn.metrics import accuracy_score\n', '\n', 'import numpy as np\n', 'import pandas as pd\n', '\n', 'import warnings\n', ""warnings.filterwarnings('ignore')\n"", '\n', '# Matplotlib forms basis for visualization in Python\n', 'import matplotlib.pyplot as plt\n', 'import matplotlib.ticker as ticker\n', '\n', '# We will use the Seaborn library\n', 'import seaborn as sns\n', 'sns.set()\n', '\n', 'import os, string\n', '# Graphics in SVG format are more sharp and legible\n', ""%config InlineBackend.figure_format = 'svg'\n"", ""print(os.listdir('../input/'))""]"
SETUP,0,"['import numpy as np\n', 'import pandas as pd \n', 'import matplotlib.pyplot as plt\n', 'import os \n', 'import seaborn as sns\n', 'import pylab as pl\n', 'import geopandas as gpd\n', 'import folium\n', 'from folium import plugins\n', '\n']"
"ASSIGN = m.predict(future) ASSIGN[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()",0,"['forecast = m.predict(future)\n', ""forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()""]"
"ASSIGN= plt.plot(n_estimators, cv_results, 'b', label=""cross validated accuracy"") plt.ylabel('accuracy') plt.xlabel('n_estimators') plt.legend() plt.show()",1,"['line1= plt.plot(n_estimators, cv_results, \'b\', label=""cross validated accuracy"")\n', ""plt.ylabel('accuracy')\n"", ""plt.xlabel('n_estimators')\n"", 'plt.legend()\n', 'plt.show()']"
ASSIGN = ASSIGN.join(pd.get_dummies(ASSIGN['income class'])),0,"['# transform target in category type\n', ""df = df.join(pd.get_dummies(df['income class']))""]"
"def get_cat_to_name_data(file_path): """""" Imports the cat_to_name.json file and returns a Pandas DataFrame """""" with open(file_path, 'r') as f: ASSIGN = json.load(f) return cat_to_names",0,"['def get_cat_to_name_data(file_path):\n', '    """""" Imports the cat_to_name.json file and returns a Pandas DataFrame """"""\n', ""    with open(file_path, 'r') as f:\n"", '        cat_to_names = json.load(f)\n', '    return cat_to_names']"
CHECKPOINT CHECKPOINT ASSIGN = ASSIGN[abs(ASSIGN) > 0.5] print('The mean skewness of the variables is{}'.format(np.mean(data_num.skew()))) print('There are {} features have to boxcox1p transform'.format(len(ASSIGN))) skew_features,0,"['skew_features = skew_features[abs(skew_features) > 0.5]\n', ""print('The mean skewness of the variables is{}'.format(np.mean(data_num.skew())))\n"", ""print('There are {} features have to boxcox1p transform'.format(len(skew_features)))\n"", 'skew_features']"
train_df['bbox_adj'] = train_df['bboxs'].map(xyxy_to_yxyx),0,"[""train_df['bbox_adj'] = train_df['bboxs'].map(xyxy_to_yxyx)""]"
"SETUP ASSIGN = torch.ASSIGN(""cuda"" if torch.cuda.is_available() else ""cpu"")",0,"[""CAT_TO_NAME_PATH = '../input/hackathon-blossom-flower-classification/cat_to_name.json'\n"", 'TRAIN_DATA_PATH = ""../input/hackathon-blossom-flower-classification/flower_data/flower_data/train""\n', 'VAL_DATA_PATH = ""../input/hackathon-blossom-flower-classification/flower_data/flower_data/valid""\n', ""TEST_DATA_PATH = '../input/hackathon-blossom-flower-classification/test set/'\n"", ""CHECKPOINT_PATH = '../input/model-checkpoints/'\n"", '\n', 'device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")']"
"ASSIGN = df1['Age'] sns.distplot(ASSIGN, hist=True, rug=True)",1,"[""x = df1['Age']\n"", 'sns.distplot(x, hist=True, rug=True)']"
CHECKPOINT print(os.listdir()),0,"['# nothing in our folder yet\n', 'print(os.listdir(""../working""))']"
"CHECKPOINT ASSIGN=data[:2000] ASSIGN=data_2000.GEO_LON.mean() ASSIGN=data_2000.GEO_LAT.mean() ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(data_map) for lat,lon,label in zip(ASSIGN.GEO_LAT,ASSIGN.GEO_LON,ASSIGN.OFFENSE_CATEGORY_ID): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN) data_map",1,"['data_2000=data[:2000]\n', 'Long=data_2000.GEO_LON.mean()\n', 'Lat=data_2000.GEO_LAT.mean()\n', 'data_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'data_crime_map=plugins.MarkerCluster().add_to(data_map)\n', 'for lat,lon,label in zip(data_2000.GEO_LAT,data_2000.GEO_LON,data_2000.OFFENSE_CATEGORY_ID):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_crime_map)\n', 'data_map.add_child(data_crime_map)\n', '\n', 'data_map']"
"CHECKPOINT def test(model, epoch): model.eval() ASSIGN = 0 for data, target in test_loader: ASSIGN = ASSIGN.to(device) ASSIGN = ASSIGN.to(device) ASSIGN = model(data) ASSIGN = ASSIGN.permute(1, 0, 2) ASSIGN = output.max(2)[1] ASSIGN += ASSIGN.eq(ASSIGN).cpu().sum().item() print('\nTest set: Accuracy: {}path{} ({:.0f}%)\n'.format( ASSIGN, len(test_loader.dataset), 100. * ASSIGN path(test_loader.dataset)))",0,"['def test(model, epoch):\n', '    model.eval()\n', '    correct = 0\n', '    for data, target in test_loader:\n', '        data = data.to(device)\n', '        target = target.to(device)\n', '        output = model(data)\n', '        output = output.permute(1, 0, 2)\n', '        pred = output.max(2)[1] # get the index of the max log-probability\n', '        correct += pred.eq(target).cpu().sum().item()\n', ""    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n"", '        correct, len(test_loader.dataset),\n', '        100. * correct / len(test_loader.dataset)))']"
"CHECKPOINT ASSIGN = ASSIGN path ASSIGN = ASSIGN path print('ASSIGN shape:', ASSIGN.shape) print(ASSIGN.shape[0], 'train samples') print(ASSIGN.shape[0], 'test samples')",0,"['# data convert to floating point\n', ""#x_train = x_train.astype('float32') / 255\n"", 'x_train = x_train / 255.0\n', ""#x_test = x_test.astype('float32') / 255\n"", 'x_test = x_test / 255.0\n', ""print('x_train shape:', x_train.shape)\n"", ""print(x_train.shape[0], 'train samples')\n"", ""print(x_test.shape[0], 'test samples')""]"
"ASSIGN = pd.read_csv(""..path"", index_col = 'Unnamed: 0')",0,"['df = pd.read_csv(""../input/raw-benford-numbers-edited/Raw Benford Numbers.csv"", index_col = \'Unnamed: 0\')']"
player_df.birthday = pd.to_datetime(player_df.birthday),0,['player_df.birthday = pd.to_datetime(player_df.birthday)']
"ASSIGN = rec_img.detach().permute(1, 2, 0) plt.imshow(ASSIGN)",1,"['rimage = rec_img.detach().permute(1, 2, 0)\n', 'plt.imshow(rimage)']"
"CHECKPOINT ASSIGN = 'cuda' ASSIGN = Generator(True,block_config =block_config,num_encoded_channels = num_encoded_channels).to(device) ASSIGN = torch.randn(IMG_WIDTH*IMG_HEIGHT*3 * 1).view((-1,3,IMG_HEIGHT,IMG_WIDTH)).to(device) ASSIGN = netG(inp) print(ASSIGN.shape) print((ASSIGN.shape[1]*ASSIGN.shape[2]*ASSIGN.shape[3])path(IMG_WIDTH*IMG_HEIGHT*3)) ASSIGN = output.shape del inp,netG,output torch.cuda.empty_cache()",0,"[""device = 'cuda'\n"", 'netG = Generator(True,block_config =block_config,num_encoded_channels = num_encoded_channels).to(device)\n', '# netG.apply(weights_init)\n', 'inp = torch.randn(IMG_WIDTH*IMG_HEIGHT*3 * 1).view((-1,3,IMG_HEIGHT,IMG_WIDTH)).to(device)\n', 'output = netG(inp)\n', 'print(output.shape)\n', 'print((output.shape[1]*output.shape[2]*output.shape[3])/(IMG_WIDTH*IMG_HEIGHT*3))\n', 'encoded_size = output.shape\n', 'del inp,netG,output\n', 'torch.cuda.empty_cache()']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
model.save('.path'),0,"[""#model.save_weights('./models/leaf_classification_weights_best.h5')\n"", ""model.save('./models/leaf_classification_model_best.h5')""]"
match_lineup_df.reset_index(inplace=True),0,['match_lineup_df.reset_index(inplace=True)']
"ASSIGN=[] ASSIGN=[] for key, value in dtypes.items(): if key!='Attrition': ASSIGN == ""int64"": ASSIGN.append(key) else: ASSIGN.append(key)",0,"['cont=[]\n', 'cat=[]\n', 'for key, value in dtypes.items():\n', ""    if key!='Attrition':\n"", '        if value == ""int64"":\n', '            cont.append(key)\n', '        else:\n', '            cat.append(key)']"
"ASSIGN = ASSIGN[ASSIGN.GrLivArea < 4500] ASSIGN.reset_index(drop=True, inplace=True) ASSIGN = [30, 88, 462, 631, 1322] ASSIGN = ASSIGN.drop(ASSIGN.index[outliers])",0,"['train = train[train.GrLivArea < 4500]\n', 'train.reset_index(drop=True, inplace=True)\n', 'outliers = [30, 88, 462, 631, 1322]\n', 'train = train.drop(train.index[outliers])']"
"ASSIGN = ASSIGN.iloc[:,:].values ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], 28, 28, 1)",0,"['test = test.iloc[:,:].values\n', 'test = test.reshape(test.shape[0], 28, 28, 1)']"
ASSIGN = pd.get_dummies(data.Fuel_Type) ASSIGN = pd.get_dummies(data.Seller_Type) ASSIGN = pd.get_dummies(data.Transmission),0,"['dummy1 = pd.get_dummies(data.Fuel_Type)\n', 'dummy2 = pd.get_dummies(data.Seller_Type)\n', 'dummy3 = pd.get_dummies(data.Transmission)']"
"def plotScatterMatrix(df, plotSize, textSize): ASSIGN = ASSIGN.select_dtypes(include =[np.number]) ASSIGN = ASSIGN.dropna('columns') ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]] ASSIGN = list(df) if len(ASSIGN) > 10: ASSIGN = ASSIGN[:10] ASSIGN = ASSIGN[columnNames] ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde') ASSIGN = df.corr().values for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)): ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize) plt.suptitle('Scatter and Density Plot') plt.show()",1,"['# Scatter and density plots\n', 'def plotScatterMatrix(df, plotSize, textSize):\n', '    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n', '    # Remove rows and columns that would lead to df being singular\n', ""    df = df.dropna('columns')\n"", '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n', '    columnNames = list(df)\n', '    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n', '        columnNames = columnNames[:10]\n', '    df = df[columnNames]\n', ""    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n"", '    corrs = df.corr().values\n', '    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n', ""        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n"", ""    plt.suptitle('Scatter and Density Plot')\n"", '    plt.show()\n']"
"ASSIGN = ASSIGN.str.split('\s+').ASSIGN ASSIGN = ASSIGN.str.split('\s+').str[-1] ASSIGN = pd.to_numeric(ASSIGN.str[:2], errors='coerce') sns.FacetGrid(train, hue=""category_id"", size=10).map(plt.scatter, ""hour"", ""location"").add_legend()",1,"[""train['date'] = train['date_captured'].str.split('\\s+').str[0]\n"", ""train['time'] = train['date_captured'].str.split('\\s+').str[-1]    \n"", ""train['hour'] = pd.to_numeric(train['time'].str[:2], errors='coerce')\n"", 'sns.FacetGrid(train, hue=""category_id"", size=10).map(plt.scatter, ""hour"", ""location"").add_legend()']"
"ASSIGN = ResUNet(img_h=img_h, img_w=img_w) ASSIGN = tf.keras.optimizers.Adam(lr = 0.05, epsilon = 0.1) ASSIGN.compile(optimizer=ASSIGN, loss=focal_tversky_loss, metrics=[tversky])",0,"['model = ResUNet(img_h=img_h, img_w=img_w)\n', 'adam = tf.keras.optimizers.Adam(lr = 0.05, epsilon = 0.1)\n', 'model.compile(optimizer=adam, loss=focal_tversky_loss, metrics=[tversky])']"
"sns.set(rc={'figure.figsize':(15,5)}) sns.countplot(x = 'SibSp', data = df1)",1,"[""sns.set(rc={'figure.figsize':(15,5)})\n"", ""sns.countplot(x = 'SibSp', data = df1)""]"
menu.head(),0,['menu.head()']
"model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])",0,"['# compile model\n', ""model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])""]"
SETUP,0,['PIXEL_MEAN = 0.25\n']
"ASSIGN = torch.nn.BCEWithLogitsLoss() ASSIGN = torch.optim.Adam(model.parameters(), lr=lr_rate)",0,"['# Criterion and Optimizer\n', 'criterion = torch.nn.BCEWithLogitsLoss()\n', 'optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)']"
CHECKPOINT df,0,['df']
covid_data['Last Update'].unique(),0,"['# to get the last updated date of the dataset\n', '\n', ""covid_data['Last Update'].unique()""]"
"ASSIGN = trainset.drop(['PoolQC','MiscFeature','Alley','Fence'], axis= 1) ASSIGN = testset.drop(['PoolQC','MiscFeature','Alley','Fence'], axis= 1)",0,"[""#clean columns 'PoolQC','MiscFeature','Alley','Fence' because columns has missing values more than 80%\n"", ""cleaning_train = trainset.drop(['PoolQC','MiscFeature','Alley','Fence'], axis= 1)\n"", ""cleaning_test = testset.drop(['PoolQC','MiscFeature','Alley','Fence'], axis= 1)""]"
ASSIGN = cleaning_test.isnull().sum().sort_values(ascending = False).head(18).index.values.tolist() cleaning_test[ASSIGN] = cleaning_test[ASSIGN].fillna(cleaning_test[ASSIGN].mode().iloc[0]),0,"['# cleaning testset in categorical\n', 'lst_categ = cleaning_test.isnull().sum().sort_values(ascending = False).head(18).index.values.tolist()\n', 'cleaning_test[lst_categ] = cleaning_test[lst_categ].fillna(cleaning_test[lst_categ].mode().iloc[0])']"
ASSIGN = ClassificationInterpretation.from_learner(learn) ASSIGN.plot_confusion_matrix(),1,"['interp = ClassificationInterpretation.from_learner(learn)\n', 'interp.plot_confusion_matrix()']"
"CHECKPOINT SETUP ASSIGN = pd.read_csv(""..path"") data",0,"['import pandas as pd\n', 'data = pd.read_csv(""../input/titanic/train_and_test2.csv"")\n', 'data\n']"
CHECKPOINT print() print(),0,"['print(""You\'ve successfully run some Python code"")\n', 'print(""Congratulations!"")']"
CHECKPOINT ASSIGN = pd.read_csv('..path') pd_data.shape,0,"['#read in\n', ""pd_data = pd.read_csv('../input/weatherAUS.csv')\n"", 'pd_data.shape']"
"ASSIGN = kickstarters_2017.pledged > 0 ASSIGN = kickstarters_2017.pledged.loc[index_of_pledged] ASSIGN = stats.boxcox(positive_pledges)[0] ASSIGN=plt.subplots(1,2) sns.distplot(ASSIGN, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Normalized data"")",1,"['# Your turn! \n', '# We looked as the usd_pledged_real column. What about the ""pledged"" column? Does it have the same info?\n', '\n', '# get the index of all positive pledges (Box-Cox only takes postive values)\n', 'index_of_pledged = kickstarters_2017.pledged > 0\n', '\n', '# get only positive pledges (using their indexes)\n', 'positive_pledges = kickstarters_2017.pledged.loc[index_of_pledged]\n', '\n', '# normalize the pledges (w/ Box-Cox)\n', 'normalized_pledges = stats.boxcox(positive_pledges)[0]\n', '\n', '# plot both together to compare\n', 'fig, ax=plt.subplots(1,2)\n', 'sns.distplot(positive_pledges, ax=ax[0])\n', 'ax[0].set_title(""Original Data"")\n', 'sns.distplot(normalized_pledges, ax=ax[1])\n', 'ax[1].set_title(""Normalized data"")']"
ASSIGN = df[cols[0:20]],0,['relevant = df[cols[0:20]]']
zomato_orgnl.info(),0,['zomato_orgnl.info()']
CHECKPOINT li2,0,['li2']
data.info(),0,['data.info()']
"x_train, x_test, Y_train, Y_test = train_test_split(x, Y, test_size = 0.20,shuffle = False)",0,"['x_train, x_test, Y_train, Y_test = train_test_split(x, Y, test_size = 0.20,shuffle = False)']"
"def get_max_pool_layer(ksize): return nn.MaxPool2d(ksize).apply(weights_init) def get_avg_pool_layer(ksize): return nn.AvgPool2d(ksize).apply(weights_init) def get_conv_layer(in_lyr,out_lyr,ksize): ASSIGN = nn.Sequential( nn.ReLU(inplace=True), nn.Conv2d(in_lyr, out_lyr, ksize), nn.BatchNorm2d(out_lyr), ) return ASSIGN.apply(weights_init) class depthwise_separable_conv(nn.Module): def __init__(self, nin, kernels_per_layer, nout): super(depthwise_separable_conv, self).__init__() self.depthwise = nn.Conv2d(nin, nin * kernels_per_layer, kernel_size=3, padding=1, groups=nin).apply(weights_init) self.pointwise = nn.Conv2d(nin * kernels_per_layer, nout, kernel_size=1).apply(weights_init) def forward(self, x): ASSIGN = self.depthwise(x) ASSIGN = self.pointwise(ASSIGN) return out def get_sep_layer(in_lyr,out_lyr,ksize): return depthwise_separable_conv(in_lyr,ksize,out_lyr)",0,"['def get_max_pool_layer(ksize):\n', '    return nn.MaxPool2d(ksize).apply(weights_init)\n', '\n', '\n', 'def get_avg_pool_layer(ksize):\n', '    return nn.AvgPool2d(ksize).apply(weights_init)\n', '\n', 'def get_conv_layer(in_lyr,out_lyr,ksize):\n', '    seq_layer = nn.Sequential(\n', '            nn.ReLU(inplace=True),\n', '            nn.Conv2d(in_lyr, out_lyr, ksize),\n', '            nn.BatchNorm2d(out_lyr),\n', '    )\n', '    return seq_layer.apply(weights_init)\n', '\n', 'class depthwise_separable_conv(nn.Module):\n', '    def __init__(self, nin, kernels_per_layer, nout):\n', '        super(depthwise_separable_conv, self).__init__()\n', '        self.depthwise = nn.Conv2d(nin, nin * kernels_per_layer, kernel_size=3, padding=1, groups=nin).apply(weights_init)\n', '        self.pointwise = nn.Conv2d(nin * kernels_per_layer, nout, kernel_size=1).apply(weights_init)\n', '\n', '    def forward(self, x):\n', '        out = self.depthwise(x)\n', '        out = self.pointwise(out)\n', '        return out\n', '\n', 'def get_sep_layer(in_lyr,out_lyr,ksize):\n', '    return depthwise_separable_conv(in_lyr,ksize,out_lyr)\n']"
model.summary(),0,['model.summary()']
train_df['nulls1'] = train_df.isna().sum(axis=1) test_df['nulls1'] = test_df.isna().sum(axis=1),0,"[""train_df['nulls1'] = train_df.isna().sum(axis=1)\n"", ""test_df['nulls1'] = test_df.isna().sum(axis=1)""]"
"ASSIGN = [r for r in range(sums['en'].shape[0])] ASSIGN = plt.figure(1, figsize=[10, 10]) plt.ylabel('View Per Page') plt.xlabel('Day') plt.title('Pages in Different Languages') ASSIGN={'en':'English','ja':'Japanese','de':'German', 'na':'Media','fr':'French','zh':'Chinese', 'ru':'Russian','es':'Spanish' } for key in sums: plt.plot(ASSIGN,sums[key],label = ASSIGN[key] ) plt.legend() plt.show()",1,"[""days = [r for r in range(sums['en'].shape[0])]\n"", '\n', 'fig = plt.figure(1, figsize=[10, 10])\n', ""plt.ylabel('View Per Page')\n"", ""plt.xlabel('Day')\n"", ""plt.title('Pages in Different Languages')\n"", ""labels={'en':'English','ja':'Japanese','de':'German',\n"", ""        'na':'Media','fr':'French','zh':'Chinese',\n"", ""        'ru':'Russian','es':'Spanish'\n"", '       }\n', '\n', 'for key in sums:\n', '    plt.plot(days,sums[key],label = labels[key] )\n', '    \n', 'plt.legend()\n', 'plt.show()']"
data_features['LotFrontage'],0,"[""data_features['LotFrontage']""]"
"ASSIGN = plt.subplots(figsize=(6, 6)) ASSIGN = sns.countplot(x=""netgain"", data=train, label=""Label count"") sns.despine(bottom=True)",1,"['f, ax = plt.subplots(figsize=(6, 6))\n', 'ax = sns.countplot(x=""netgain"", data=train, label=""Label count"")\n', 'sns.despine(bottom=True)']"
"SETUP CHECKPOINT ASSIGN=train_test_split(data_tree[['neighbourhood_group_new','neighbourhood_new','room_type_new','minimum_nights','number_of_reviews']],data_tree[['price']],test_size=0.1,random_state=0) ASSIGN=DecisionTreeRegressor(criterion='mse',max_depth=3,random_state=0) ASSIGN=ASSIGN.fit(x_train,y_train) ASSIGN=y_test['price'] ASSIGN=Reg_tree.ASSIGN(x_test) print(,np.mean(abs(np.multiply(np.array(y_test.T-ASSIGN),np.array(1path)))))",0,"['from sklearn.model_selection import train_test_split\n', 'from sklearn.tree import DecisionTreeRegressor\n', ""x_train,x_test,y_train,y_test=train_test_split(data_tree[['neighbourhood_group_new','neighbourhood_new','room_type_new','minimum_nights','number_of_reviews']],data_tree[['price']],test_size=0.1,random_state=0)\n"", ""Reg_tree=DecisionTreeRegressor(criterion='mse',max_depth=3,random_state=0)\n"", 'Reg_tree=Reg_tree.fit(x_train,y_train)\n', ""y=y_test['price']\n"", 'predict=Reg_tree.predict(x_test)\n', 'print(""median absolute deviation (MAD): "",np.mean(abs(np.multiply(np.array(y_test.T-predict),np.array(1/y_test)))))']"
"ASSIGN = confirm_dict['Mainland China'] ASSIGN = confirm_dict['Italy'] ASSIGN = 0 for key in confirm_dict: if key != 'Mainland China' and key != 'Italy': ASSIGN+=confirm_dict[key] ASSIGN = ['China', 'Italy', 'Others'] ASSIGN = [china_c_number, italy_c_number, others_c] ASSIGN = ['Red', 'Green', 'Grey'] ASSIGN = (0.1, 0, 0) ASSIGN = ['Count'] ASSIGN = ['China', 'Italy', 'Others'] ASSIGN = [[china_c_number], [italy_c_number], [others_c]] ASSIGN = plt.subplots(1,2, figsize = (8,8)) axs[0].axis('tight') axs[0].axis('off') ASSIGN = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center') ASSIGN.set_fontsize(14) ASSIGN.scale(1.5, 1.5) axs[1].pie(ASSIGN, labels = ASSIGN, ASSIGN = ASSIGN, colors=ASSIGN, shadow=True, autopct='%1.1f%%') plt.title('Global Proportions of 2 severely striken countries') plt.show()",1,"[""china_c_number = confirm_dict['Mainland China']\n"", ""italy_c_number = confirm_dict['Italy']\n"", 'others_c = 0\n', 'for key in confirm_dict:\n', ""    if key != 'Mainland China' and key != 'Italy':\n"", '        others_c+=confirm_dict[key]\n', '        \n', ""groups = ['China', 'Italy', 'Others']\n"", 'sizes = [china_c_number, italy_c_number, others_c]\n', ""colours = ['Red', 'Green', 'Grey']\n"", 'explode = (0.1, 0, 0)\n', ""col_labels = ['Count']\n"", ""row_labels = ['China', 'Italy', 'Others']\n"", 'table_values = [[china_c_number], [italy_c_number], [others_c]]\n', '\n', '\n', 'fig, axs = plt.subplots(1,2, figsize = (8,8))\n', ""axs[0].axis('tight')\n"", ""axs[0].axis('off')\n"", ""the_table = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')\n"", 'the_table.set_fontsize(14)\n', 'the_table.scale(1.5, 1.5)\n', ""axs[1].pie(sizes, labels = groups, explode = explode, colors=colours, shadow=True, autopct='%1.1f%%')\n"", ""plt.title('Global Proportions of 2 severely striken countries')\n"", 'plt.show()']"
ASSIGN=pd.read_csv('..path'),0,"[""data=pd.read_csv('../input/iris/Iris.csv')""]"
train.describe(),0,['train.describe()']
"plt.figure(figsize = (12,5)) ASSIGN = plt.subplots(1,2) sns.distplot(data['Salary'], ax = ax[0]) sns.boxplot(data = data, x = 'Clicked', y = 'Salary', ax = ax[1]) plt.show()",1,"['#Distribution of salary\n', 'plt.figure(figsize = (12,5))\n', 'fig, ax = plt.subplots(1,2)\n', ""sns.distplot(data['Salary'], ax = ax[0])\n"", ""sns.boxplot(data = data, x = 'Clicked', y = 'Salary', ax = ax[1])\n"", 'plt.show()']"
"train.replace(np.inf,0,inplace=True)",0,"[""#the outlier 'inf' will make the auto_arima come to an error,so it's replaced by 0\n"", 'train.replace(np.inf,0,inplace=True)']"
"NA_for_0 = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath', 'GarageArea', 'GarageCars','MasVnrArea'] for col in NA_for_0: data_features[col] = data_features[col].fillna(0)",0,"[""NA_for_0 = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath',\n"", ""            'GarageArea', 'GarageCars','MasVnrArea']\n"", 'for col in NA_for_0:\n', '    data_features[col] = data_features[col].fillna(0)']"
CHECKPOINT ASSIGN = suicide_attacks['City'].unique() ASSIGN.sort() cities,0,"[""# get all the unique values in the 'City' column\n"", ""cities = suicide_attacks['City'].unique()\n"", '\n', '# sort them alphabetically and then take a closer look\n', 'cities.sort()\n', 'cities']"
"CHECKPOINT def imshow(img): ASSIGN = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ], ASSIGN = [ 1path, 1path, 1path]), transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ], ASSIGN = [ 1., 1., 1. ]), ]) ASSIGN = inverse_transform(img) ASSIGN = img.numpy() plt.imshow(np.transpose(ASSIGN, (1, 2, 0))) plt.show() def visualize(): ASSIGN = iter(train_loader) ASSIGN = dataiter.next() ASSIGN = dataset.ASSIGN imshow(torchvision.utils.make_grid(images[:4])) for j in range(4): print(.format(labels[j].item(), ASSIGN[labels[j]])) visualize()",1,"['def imshow(img):\n', '    inverse_transform = transforms.Compose([\n', '      transforms.Normalize(mean = [ 0., 0., 0. ],\n', '                           std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n', '      transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n', '                           std = [ 1., 1., 1. ]),\n', '      ])\n', '\n', '    unnormalized_img = inverse_transform(img)\n', '    npimg = img.numpy()\n', '    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n', '    plt.show()\n', '  \n', '\n', 'def visualize(): \n', '    dataiter = iter(train_loader)\n', '    images, labels = dataiter.next()\n', '\n', '    classes = dataset.classes\n', '    imshow(torchvision.utils.make_grid(images[:4]))\n', '    for j in range(4):\n', '        print(""label: {},  name: {}"".format(labels[j].item(),\n', '                                        classes[labels[j]]))\n', 'visualize()']"
type(league_df.name.iloc[0]),0,['type(league_df.name.iloc[0])']
"CHECKPOINT for column in binary_features: if (column == 'address'): data_mat[column] = data_mat[column].apply(lambda x: 1 if (x == 'U') else 0) else: data_mat[column] = data_mat[column].apply(lambda x: 1 if (x =='yes') else 0) print(column,,data_mat[column].unique())",0,"['for column in binary_features:\n', ""    if (column == 'address'):\n"", ""        data_mat[column] = data_mat[column].apply(lambda x: 1 if (x == 'U') else 0)\n"", '    else:\n', ""        data_mat[column] = data_mat[column].apply(lambda x: 1 if (x =='yes') else 0)\n"", '    print(column,""-"",data_mat[column].unique())']"
"ASSIGN = sns.FacetGrid(titanic_df, hue=""Survived"",aspect=4) ASSIGN.map(sns.kdeplot,'Age',shade= True) ASSIGN.set(xlim=(0, titanic_df['Age'].max())) ASSIGN.add_legend() fig, axis1 = plt.subplots(1,1,figsize=(18,4)) ASSIGN = titanic_df[[""Age"", ""Survived""]].groupby(['Age'],as_index=False).mean() sns.barplot(x='Age', y='Survived', data=ASSIGN)",1,"['# .... continue with plot Age column\n', '\n', '# peaks for survived/not survived passengers by their age\n', 'facet = sns.FacetGrid(titanic_df, hue=""Survived"",aspect=4)\n', ""facet.map(sns.kdeplot,'Age',shade= True)\n"", ""facet.set(xlim=(0, titanic_df['Age'].max()))\n"", 'facet.add_legend()\n', '\n', '# average survived passengers by age\n', 'fig, axis1 = plt.subplots(1,1,figsize=(18,4))\n', 'average_age = titanic_df[[""Age"", ""Survived""]].groupby([\'Age\'],as_index=False).mean()\n', ""sns.barplot(x='Age', y='Survived', data=average_age)""]"
"SETUP with open(""tree1.dot"", 'w') as f: ASSIGN = export_graphviz(Reg_tree, ASSIGN=f, ASSIGN = 3, ASSIGN = True, ASSIGN = ['neighbourhood_group_new','neighbourhood_new','room_type_new','minimum_nights','number_of_reviews'], ASSIGN = True, ASSIGN= True ) check_call(['dot','-Tpng','tree1.dot','-o','tree1.png']) ASSIGN = Image.open(""tree1.png"") ASSIGN = ImageDraw.Draw(img) ASSIGN.save('sample-out.png') PImage(""sample-out.png"")",1,"['from subprocess import check_call\n', 'from PIL import Image, ImageDraw, ImageFont\n', 'from IPython.display import Image as PImage\n', 'from sklearn.tree import export_graphviz\n', 'with open(""tree1.dot"", \'w\') as f:\n', '     f = export_graphviz(Reg_tree,\n', '                              out_file=f,\n', '                              max_depth = 3,\n', '                              impurity = True,\n', ""                              feature_names = ['neighbourhood_group_new','neighbourhood_new','room_type_new','minimum_nights','number_of_reviews'],\n"", '                              rounded = True,\n', '                              filled= True )\n', ""check_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n"", 'img = Image.open(""tree1.png"")\n', 'draw = ImageDraw.Draw(img)\n', ""img.save('sample-out.png')\n"", 'PImage(""sample-out.png"")']"
ASSIGN=data[data.OFFENSE_CATEGORY_ID=='traffic-accident'] ASSIGN.head(),0,"[""data_traf=data[data.OFFENSE_CATEGORY_ID=='traffic-accident']\n"", 'data_traf.head()']"
SETUP,0,"['\n', 'import os\n', 'from glob import glob\n', 'import re\n', 'import ast\n', 'import numpy as np \n', 'import pandas as pd\n', 'from PIL import Image, ImageDraw \n', 'from tqdm import tqdm\n', 'from dask import bag\n', 'import json\n', '\n', 'import tensorflow as tf\n', 'from tensorflow import keras\n', 'from tensorflow.keras.models import Sequential\n', 'from keras.models import Model\n', 'from tensorflow.keras.layers import Dense, Dropout, Flatten\n', 'from tensorflow.keras.layers import Conv2D, MaxPooling2D\n', 'from tensorflow.keras.metrics import top_k_categorical_accuracy\n', 'from keras.layers import Input, Conv1D, Dense, Dropout, BatchNormalization, Flatten, MaxPool1D\n', 'from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping']"
"ASSIGN = np.random.exponential(size = 1000) ASSIGN = minmax_scaling(original_data, columns = [0]) ASSIGN=plt.subplots(1,2) sns.distplot(ASSIGN, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Scaled data"")",1,"['# generate 1000 data points randomly drawn from an exponential distribution\n', 'original_data = np.random.exponential(size = 1000)\n', '\n', '# mix-max scale the data between 0 and 1\n', 'scaled_data = minmax_scaling(original_data, columns = [0])\n', '\n', '# plot both together to compare\n', 'fig, ax=plt.subplots(1,2)\n', 'sns.distplot(original_data, ax=ax[0])\n', 'ax[0].set_title(""Original Data"")\n', 'sns.distplot(scaled_data, ax=ax[1])\n', 'ax[1].set_title(""Scaled data"")']"
trend_df.head(),0,['trend_df.head()']
data_export.isnull().sum(),0,['data_export.isnull().sum()']
"plotPerColumnDistribution(df1, 10, 5)",1,"['plotPerColumnDistribution(df1, 10, 5)']"
learn.lr_find() learn.recorder.plot(suggestion=True),1,"['#learn.freeze()\n', 'learn.lr_find()\n', 'learn.recorder.plot(suggestion=True)']"
"ASSIGN = ASSIGN.append(test, ignore_index = True)",0,"['# merge train & test dataset\n', 'df = df.append(test, ignore_index = True)']"
CHECKPOINT dc,0,"['#displaying the dictionary\n', 'dc']"
CHECKPOINT dataset.shape,0,['dataset.shape']
"ASSIGN = pd.read_csv('path') ASSIGN.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)",0,"[""temperature = pd.read_csv('/kaggle/input/weather-data-for-covid19-data-analysis/training_data_with_weather_info_week_4.csv')\n"", ""temperature.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)""]"
ASSIGN = 'black' ASSIGN = 'black_bear.txt',0,"[""folder = 'black'\n"", ""file = 'black_bear.txt'""]"
ASSIGN = final[:891] ASSIGN = StandardScaler() ASSIGN = feature_scaler.fit_transform(ASSIGN) ASSIGN = final[891:] ASSIGN = StandardScaler() ASSIGN = feature_scaler.fit_transform(ASSIGN),0,"['x_train1 = final[:891]\n', 'feature_scaler = StandardScaler()\n', 'x_train1 = feature_scaler.fit_transform(x_train1)\n', 'y_train1 = final[891:]\n', 'feature_scaler = StandardScaler()\n', 'y_train1 = feature_scaler.fit_transform(y_train1)']"
"CHECKPOINT ASSIGN=stations[stations['city_id']==91] ASSIGN=135.5 ASSIGN=34.53 ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(osaka_map) for lat,lon,label in zip(ASSIGN.ASSIGN,ASSIGN.ASSIGN,ASSIGN.stations_name): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN) osaka_map",1,"['\n', ""stations_osaka=stations[stations['city_id']==91]\n"", 'Long=135.5\n', 'Lat=34.53\n', 'osaka_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'osaka_stations_map=plugins.MarkerCluster().add_to(osaka_map)\n', 'for lat,lon,label in zip(stations_osaka.Lat,stations_osaka.Long,stations_osaka.stations_name):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(osaka_stations_map)\n', 'osaka_map.add_child(osaka_stations_map)\n', '\n', 'osaka_map']"
"CHECKPOINT ASSIGN = ""..path"" ASSIGN = 0 ASSIGN =[] for n in range(len(folder)): ASSIGN = os.path.join(base_path, folder[n]) print(ASSIGN) ASSIGN = os.listdir(image_path) print(len(ASSIGN)) ASSIGN.append(len(ASSIGN)) ASSIGN += len(ASSIGN) print(.format(ASSIGN)) print(ASSIGN)",0,"['base_path = ""../input/lung-colon-sobel/trainable_sobel""\n', 'total_images = 0\n', 'image_class =[]\n', 'for n in range(len(folder)):\n', '  image_path = os.path.join(base_path, folder[n]) \n', '  print(image_path)\n', '  # class_path = patient_path + ""/"" + str(c) + ""/""\n', '  subfiles = os.listdir(image_path)\n', '  print(len(subfiles))\n', '  image_class.append(len(subfiles))\n', '  total_images += len(subfiles)\n', 'print(""The number of total images are:{}"".format(total_images))  \n', 'print(image_class)']"
"data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageFinish'].notnull()), ['GarageFinish']]",0,"[""data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageFinish'].notnull()), ['GarageFinish']]""]"
"torch.save(netG, ""netG.model"")",0,"['torch.save(netG, ""netG.model"")']"
data['new_Price']=data.Price.apply(lambda x:  x.strip('$') if x!='0' else x.strip('')) data.new_Price=data.new_Price.astype(float),0,"[""data['new_Price']=data.Price.apply(lambda x:  x.strip('$') if x!='0' else x.strip(''))\n"", 'data.new_Price=data.new_Price.astype(float)']"
"CHECKPOINT print() for i in range(0,5): plt.imshow(X_train[i]) print(Y_train[i]) plt.show()",1,"['\n', 'print(""Images 1 to 5 :"")\n', 'for i in range(0,5):\n', '    plt.imshow(X_train[i])\n', '    print(Y_train[i])\n', '    plt.show()']"
"sns.scatterplot(data = X_scaled, x = 'Age', y = '401K Savings')",1,"[""sns.scatterplot(data = X_scaled, x = 'Age', y = '401K Savings')""]"
"ASSIGN = [len(str2coords(s)) for s in train['PredictionString']] plt.figure(figsize=(15,6)) sns.countplot(ASSIGN); plt.xlabel('Number of cars in image');",1,"[""lens = [len(str2coords(s)) for s in train['PredictionString']]\n"", '\n', 'plt.figure(figsize=(15,6))\n', 'sns.countplot(lens);\n', ""plt.xlabel('Number of cars in image');""]"
SETUP,0,"['import pandas as pd\n', 'import numpy as np\n', 'import matplotlib.pyplot as plt\n', 'import csv\n', 'import os']"
"ASSIGN = tabular_learner(data, layers=[200,100],metrics=accuracy, callback_fns=AUROC)",0,"['learn = tabular_learner(data, layers=[200,100],metrics=accuracy, callback_fns=AUROC)']"
"ASSIGN=len(train) ASSIGN=len(train)+len(test)-1 ASSIGN = AR1fit.predict(start=start, end=end).rename('AR(1) Predictions')",0,"['start=len(train)\n', 'end=len(train)+len(test)-1\n', ""predictions1 = AR1fit.predict(start=start, end=end).rename('AR(1) Predictions')""]"
"svc.fit(x_train,x_test) svc.score(y_train,y_test)",0,"['svc.fit(x_train,x_test)\n', 'svc.score(y_train,y_test)']"
"os.listdir(""..path"")",0,"['os.listdir(""../input/ieee-fraud-detection"")']"
"ASSIGN=pd.read_csv(""..path"") ASSIGN=pd.read_csv(""..path"") ASSIGN.head()",0,"['data_export=pd.read_csv(""../input/india-trade-data/2018-2010_export.csv"")\n', 'data_import=pd.read_csv(""../input/india-trade-data/2018-2010_import.csv"")\n', 'data_export.head()']"
"ASSIGN=34.78 ASSIGN=32.05 ASSIGN=folium.Map([Lat,Long],zoom_start=12) data_total_cars_1['label']=data_total_cars_1.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1) ASSIGN=plugins.MarkerCluster().add_to(data_total_cars_1_map) for lat,lon,label in zip(data_total_cars_1.latitude,data_total_cars_1.longitude,data_total_cars_1.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN)",1,"['Long=34.78\n', 'Lat=32.05\n', 'data_total_cars_1_map=folium.Map([Lat,Long],zoom_start=12)\n', ""data_total_cars_1['label']=data_total_cars_1.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n"", '\n', 'data_total_cars_1_cars_map=plugins.MarkerCluster().add_to(data_total_cars_1_map)\n', 'for lat,lon,label in zip(data_total_cars_1.latitude,data_total_cars_1.longitude,data_total_cars_1.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_1_cars_map)\n', 'data_total_cars_1_map.add_child(data_total_cars_1_cars_map)\n', '\n']"
"ASSIGN = lr.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('LogisticRegression(HT).csv',index = False)",0,"['model11pred = lr.predict(y_train)\n', ""submission11 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission11['PassengerId'] = result['PassengerId']\n"", ""submission11['Survived'] = model11pred\n"", ""submission11.to_csv('LogisticRegression(HT).csv',index = False)""]"
"LR.score(y_train,y_test)",0,"['LR.score(y_train,y_test)']"
"model.save_weights(""model.h5"")",0,"['model.save_weights(""model.h5"")']"
"CHECKPOINT print(classification_report(test_Y, labels)) print('Confusion Matrix') print(confusion_matrix(test_Y, labels)) print('Average Precision: '+str(average_precision_score(test_Y, proba[:, 1]))) print('ROC AUC: '+str(roc_auc_score(test_Y, labels)))",0,"['print(classification_report(test_Y, labels))\n', '\n', ""print('Confusion Matrix')\n"", 'print(confusion_matrix(test_Y, labels))\n', ""print('Average Precision: '+str(average_precision_score(test_Y, proba[:, 1])))\n"", ""print('ROC AUC: '+str(roc_auc_score(test_Y, labels)))""]"
"CHECKPOINT ASSIGN = LinearSVC(C = 0.1,penalty = 'l2', loss = 'hinge',class_weight = 'balanced') ASSIGN = cross_val_score(svc,x_train,x_test,ASSIGN=10) Accuracy2 = ASSIGN.mean() Accuracy.append(Accuracy2) print(ASSIGN) print(ASSIGN.mean())",0,"[""svc = LinearSVC(C = 0.1,penalty = 'l2', loss = 'hinge',class_weight = 'balanced')\n"", 'cv = cross_val_score(svc,x_train,x_test,cv=10)\n', 'Accuracy2 = cv.mean()\n', 'Accuracy.append(Accuracy2)\n', 'print(cv)\n', 'print(cv.mean())']"
"sns.heatmap(confusion_matrix(y_test, y_pred), annot = True, fmt = 'd') plt.xlabel('Actual Value') plt.ylabel('Predicted Value')",1,"[""sns.heatmap(confusion_matrix(y_test, y_pred), annot = True, fmt = 'd')\n"", ""plt.xlabel('Actual Value')\n"", ""plt.ylabel('Predicted Value')""]"
"ASSIGN=[] for dir1 in dirs: ASSIGN=os.listdir(r""..path""+dir1) for file in ASSIGN: ASSIGN=pd.read_csv(""..path""+dir1+""path""+file,quotechar='""',delimiter=""|"") ASSIGN.append(ASSIGN.values)",0,"['#storing all the files from every directory\n', 'li=[]\n', 'for dir1 in dirs:\n', '    files=os.listdir(r""../input/zomato_data/""+dir1)\n', '    #reading each file from list of files from previous step and creating pandas data fame    \n', '    for file in files:\n', '        \n', '        df_file=pd.read_csv(""../input/zomato_data/""+dir1+""/""+file,quotechar=\'""\',delimiter=""|"")\n', '#appending the dataframe into a list\n', '        li.append(df_file.values)']"
"model.compile(optimizer=adam, ASSIGN='categorical_crossentropy', ASSIGN=[keras.ASSIGN.categorical_accuracy]) model.summary()",0,"['model.compile(optimizer=adam,\n', '              \n', ""                  loss='categorical_crossentropy',\n"", '                  metrics=[keras.metrics.categorical_accuracy])\n', 'model.summary()']"
"ASSIGN=plt.subplots(figsize=(25,15)) sns.boxplot(x=""OverallQual"", y=""SalePrice"", data=train,ax=ax) ax.set_title(""Boxplot of Price for OverallQual"",size=20)",1,"['fig,ax=plt.subplots(figsize=(25,15))\n', 'sns.boxplot(x=""OverallQual"", y=""SalePrice"", data=train,ax=ax)\n', 'ax.set_title(""Boxplot of Price for OverallQual"",size=20)']"
"CHECKPOINT print(first_numbers, second_numbers, third_numbers)",0,"['print(first_numbers, second_numbers, third_numbers)']"
"ASSIGN = mnb.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('MultinomialNB(HT).csv',index = False)",0,"['model15pred = mnb.predict(y_train)\n', ""submission15 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission15['PassengerId'] = result['PassengerId']\n"", ""submission15['Survived'] = model15pred\n"", ""submission15.to_csv('MultinomialNB(HT).csv',index = False)""]"
CHECKPOINT reimg.shape,0,['reimg.shape']
"CHECKPOINT ASSIGN = data.path ASSIGN = data.target_label X_train, X_test_sub ,y_train,y_test_sub= train_test_split(ASSIGN,ASSIGN, test_size=0.3, random_state=0,shuffle = True) print(X_train.shape) print(X_test_sub.shape)",0,"['X = data.path\n', 'y = data.target_label\n', 'X_train, X_test_sub ,y_train,y_test_sub= train_test_split(X,y, test_size=0.3, random_state=0,shuffle = True)\n', 'print(X_train.shape)\n', 'print(X_test_sub.shape)']"
"CHECKPOINT ASSIGN = data_features.isnull().sum().sort_values(ascending=False) ASSIGN = ASSIGN[ASSIGN>0] ASSIGN = totalpath(data_features) percent ASSIGN = pd.concat((total,percent),axis=1,keys=['total','percent']) missing_data",0,"['#first get the total missing values\n', 'total = data_features.isnull().sum().sort_values(ascending=False)\n', 'total = total[total>0]\n', 'percent = total/len(data_features)\n', 'percent\n', ""missing_data = pd.concat((total,percent),axis=1,keys=['total','percent'])\n"", 'missing_data']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# Any results you write to the current directory are saved as output.']"
CHECKPOINT print(np.array(lst).shape) print(np.array(y).shape),0,"['\n', 'print(np.array(lst).shape)\n', 'print(np.array(y).shape)']"
final.isnull().sum(),0,['final.isnull().sum()']
"ggplot(bikes,aes(x='Low Temp (°F)', y='Brooklyn Bridge')) + geom_point(color='red') + \ stat_smooth(method=""loess"", color='violet', se=True)",1,"[""ggplot(bikes,aes(x='Low Temp (°F)', y='Brooklyn Bridge')) + geom_point(color='red') + \\\n"", 'stat_smooth(method=""loess"", color=\'violet\', se=True)']"
"SETUP SETUP """""" Created on Sat Sep  2 20:13:30 2017 """""" def hurst(data): ASSIGN = list() ASSIGN = len(data) ASSIGN = [2,4,8,16,32,64] ASSIGN = Npath(ranges) for i in range(len(ASSIGN)): for r in range(ASSIGN[i]): ASSIGN = data[int(L[i]*r):int(L[i]*(r+1))] ASSIGN = np.mean(Range) ASSIGN = np.subtract(Range,meanvalue) ASSIGN = np.sqrt((sum(Deviation*Deviation))path(L[i]-1)) ASSIGN = ASSIGN.cumsum() ASSIGN = max(Deviation) ASSIGN = min(Deviation) ASSIGN.append((ASSIGN-ASSIGN)path) ARS.append(np.mean(ASSIGN)) ASSIGN = np.log(ARS) ASSIGN = np.polyfit(GAP,a,1)[0]*2 return(ASSIGN) def rolling(close_data,ASSIGN): ASSIGN = close_data.rolling(window=N).apply(hurst) return(ASSIGN)",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', '# -*- coding: utf-8 -*-\n', '""""""\n', 'Created on Sat Sep  2 20:13:30 2017\n', '\n', '""""""\n', 'import numpy as np\n', '#计算hurst的值\n', 'def hurst(data):\n', '    RS = list()\n', '    ARS = []\n', '    N = len(data)\n', '    ranges = [2,4,8,16,32,64]\n', '    L = N/np.array(ranges)\n', '    for i in range(len(ranges)):\n', '        for r in range(ranges[i]):\n', '            Range = data[int(L[i]*r):int(L[i]*(r+1))]\n', '            meanvalue = np.mean(Range)\n', '            Deviation = np.subtract(Range,meanvalue)\n', '            sigma = np.sqrt((sum(Deviation*Deviation))/(L[i]-1))\n', '            Deviation = Deviation.cumsum()\n', '            maxi = max(Deviation)\n', '            mini = min(Deviation)\n', '        RS.append((maxi-mini)/sigma)\n', '        ARS.append(np.mean(RS))\n', '    GAP = np.log(L)\n', '    a = np.log(ARS)\n', '    hurst_exponent = np.polyfit(GAP,a,1)[0]*2\n', '    return(hurst_exponent)\n', '#滚动窗口,N为窗口大小,\n', 'def rolling(close_data,N):\n', '    hurst_value = close_data.rolling(window=N).apply(hurst)\n', '    return(hurst_value)']"
"sns.distplot(data[data['Retire']==1]['401K Savings'], label = 'Retire == 1') sns.distplot(data[data['Retire']==0]['401K Savings'], label = 'Retire == 0') plt.legend() plt.show()",1,"['#Distribution of 401k savings\n', ""sns.distplot(data[data['Retire']==1]['401K Savings'], label = 'Retire == 1')\n"", ""sns.distplot(data[data['Retire']==0]['401K Savings'], label = 'Retire == 0')\n"", 'plt.legend()\n', 'plt.show()']"
"ASSIGN = pd.DataFrame(mnist.isnull().sum().sort_values(ascending=False), columns=['Total']) ASSIGN = pd.DataFrame(round(100*(mnist.isnull().sum()path[0]),2).sort_values(ascending=False)\ ,columns=['Percentage']) pd.concat([ASSIGN, ASSIGN], axis = 1).head()",0,"['# Checking for total count and percentage of null values in all columns of the dataframe.\n', '\n', ""total = pd.DataFrame(mnist.isnull().sum().sort_values(ascending=False), columns=['Total'])\n"", 'percentage = pd.DataFrame(round(100*(mnist.isnull().sum()/mnist.shape[0]),2).sort_values(ascending=False)\\\n', ""                          ,columns=['Percentage'])\n"", 'pd.concat([total, percentage], axis = 1).head()']"
"trend_df[""temporal_inputs""] = [np.asarray([trends[""infection_trend""],trends[""fatality_trend""]]) for idx,trends in trend_df.iterrows()] ASSIGN = shuffle(ASSIGN)",0,"['trend_df[""temporal_inputs""] = [np.asarray([trends[""infection_trend""],trends[""fatality_trend""]]) for idx,trends in trend_df.iterrows()]\n', 'trend_df = shuffle(trend_df)']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
"ASSIGN=ASSIGN.dropna(subset=['wuhan(0)_not_wuhan(1)','latitude']) ASSIGN=ASSIGN.fillna(value = {'sex' : 'Unknown', 'age' : 'Unknown'}) ASSIGN=ASSIGN.drop(index=ASSIGN[ASSIGN.sex=='4000'].index) ASSIGN=ASSIGN.replace(to_replace = 'Female', value ='female') ASSIGN=ASSIGN.replace(to_replace = 'Male', value ='male') ASSIGN['label']=ASSIGN.apply(lambda x: ('age:'+str(x['age']),'sex:'+str(x['sex']),'geo_resolution:'+str(x['geo_resolution']),'Confirmed_date:'+str(x['date_confirmation'])),axis=1) plt.figure(figsize=(10,10)) sns.barplot(x=ASSIGN.isnull().sum().sort_values(ascending=False),y=ASSIGN.isnull().sum().sort_values(ascending=False).index) plt.title(""counts of missing value for COVID19_open_line_list"",size=20)",1,"[""data2=data2.dropna(subset=['wuhan(0)_not_wuhan(1)','latitude'])\n"", ""data2=data2.fillna(value = {'sex' : 'Unknown', 'age' : 'Unknown'})\n"", ""data2=data2.drop(index=data2[data2.sex=='4000'].index)\n"", ""data2=data2.replace(to_replace = 'Female', value ='female')\n"", ""data2=data2.replace(to_replace = 'Male', value ='male')\n"", ""data2['label']=data2.apply(lambda x: ('age:'+str(x['age']),'sex:'+str(x['sex']),'geo_resolution:'+str(x['geo_resolution']),'Confirmed_date:'+str(x['date_confirmation'])),axis=1)\n"", 'plt.figure(figsize=(10,10))\n', 'sns.barplot(x=data2.isnull().sum().sort_values(ascending=False),y=data2.isnull().sum().sort_values(ascending=False).index)\n', 'plt.title(""counts of missing value for COVID19_open_line_list"",size=20)']"
ASSIGN = create_model(),0,['model = create_model()']
"ASSIGN = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)",0,"[""tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)""]"
ASSIGN=np.vstack(li1),0,"['#numpys vstack method to append all the datafames to stack the sequence of input vertically to make a single array\n', 'df_np=np.vstack(li1)']"
league_df.head(5),0,['league_df.head(5)']
df_all.sample(n=5),0,['df_all.sample(n=5)']
"sns.catplot(x = 'SibSp',y='Survived',hue = 'Pclass',kind = 'violin', data = df3, palette = 'BuGn_r', col = 'Pclass')",1,"[""sns.catplot(x = 'SibSp',y='Survived',hue = 'Pclass',kind = 'violin', data = df3, palette = 'BuGn_r', col = 'Pclass')""]"
CHECKPOINT holdout.shape,0,['holdout.shape']
df1.head(5),0,['df1.head(5)']
ASSIGN = clf.predict(test_df),0,['test_preds = clf.predict(test_df)']
"ASSIGN = len(Y[Y==1]) path(Y) plt.plot([0, 1], [ASSIGN, ASSIGN], linestyle='--', label='No Skill') ASSIGN = precision_recall_curve(test_Y, pos_probs) plt.plot(recall, precision, marker='.', label='Logistic') plt.xlabel('Recall') plt.ylabel('Precision') plt.legend() plt.show()",1,"['# calculate the no skill line as the proportion of the positive class\n', 'no_skill = len(Y[Y==1]) / len(Y)\n', '# plot the no skill precision-recall curve\n', ""plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n"", '# calculate model precision-recall curve\n', 'precision, recall, _ = precision_recall_curve(test_Y, pos_probs)\n', '# plot the model precision-recall curve\n', ""plt.plot(recall, precision, marker='.', label='Logistic')\n"", '# axis labels\n', ""plt.xlabel('Recall')\n"", ""plt.ylabel('Precision')\n"", '# show the legend\n', 'plt.legend()\n', '# show the plot\n', 'plt.show()']"
"ASSIGN = lgb.train(params, lgb_train, ASSIGN=10000, ASSIGN=lgb_eval,early_stopping_rounds=500)",0,"['gbm = lgb.train(params,  lgb_train,  \n', '                num_boost_round=10000,  \n', '                valid_sets=lgb_eval,early_stopping_rounds=500)  ']"
"def plot_3d(image, threshold=-300): ASSIGN = image.transpose(2,1,0) ASSIGN = measure.marching_cubes(p, threshold) ASSIGN = plt.figure(figsize=(10, 10)) ASSIGN = fig.add_subplot(111, projection='3d') ASSIGN = Poly3DCollection(verts[faces], alpha=0.1) ASSIGN = [0.5, 0.5, 1] ASSIGN.set_facecolor(ASSIGN) ASSIGN.add_collection3d(ASSIGN) ASSIGN.set_xlim(0, ASSIGN.shape[0]) ASSIGN.set_ylim(0, ASSIGN.shape[1]) ASSIGN.set_zlim(0, ASSIGN.shape[2]) plt.show()",1,"['def plot_3d(image, threshold=-300):\n', '    \n', '    # Position the scan upright, \n', '    # so the head of the patient would be at the top facing the camera\n', '    p = image.transpose(2,1,0)\n', '    \n', '    verts, faces = measure.marching_cubes(p, threshold)\n', '\n', '    fig = plt.figure(figsize=(10, 10))\n', ""    ax = fig.add_subplot(111, projection='3d')\n"", '\n', '    # Fancy indexing: `verts[faces]` to generate a collection of triangles\n', '    mesh = Poly3DCollection(verts[faces], alpha=0.1)\n', '    face_color = [0.5, 0.5, 1]\n', '    mesh.set_facecolor(face_color)\n', '    ax.add_collection3d(mesh)\n', '\n', '    ax.set_xlim(0, p.shape[0])\n', '    ax.set_ylim(0, p.shape[1])\n', '    ax.set_zlim(0, p.shape[2])\n', '\n', '    plt.show()']"
"sns.catplot(x='Year', y='Prevention of atrocities (POA) Act', data=total ,height = 5, aspect = 4,kind = 'bar')",1,"[""sns.catplot(x='Year', y='Prevention of atrocities (POA) Act', data=total ,height = 5, aspect = 4,kind = 'bar')""]"
"ASSIGN = train[['Country','Province','Date','ConfirmedCases','Fatalities']] ASSIGN = pd.DataFrame(columns = ['Country','Province','Date','ConfirmedCases','Fatalities']) for country in ASSIGN.Country.unique(): for province in ASSIGN[ASSIGN.Country == country].Province.unique(): ASSIGN = train_exam.query(f""Country == '{country}' and Province == '{province}'"") ASSIGN = province_df.ConfirmedCases ASSIGN = province_df.Fatalities ASSIGN = conf.diff() ASSIGN = fata.diff() ASSIGN.ConfirmedCases = ASSIGN ASSIGN.Fatalities = ASSIGN ASSIGN = pd.concat([ASSIGN,province_df],0)",0,"[""train_exam = train[['Country','Province','Date','ConfirmedCases','Fatalities']]\n"", ""diff_df = pd.DataFrame(columns = ['Country','Province','Date','ConfirmedCases','Fatalities'])\n"", 'for country in train_exam.Country.unique():\n', '    for province in train_exam[train_exam.Country == country].Province.unique():\n', '        province_df = train_exam.query(f""Country == \'{country}\' and Province == \'{province}\'"")\n', '        conf = province_df.ConfirmedCases\n', '        fata = province_df.Fatalities\n', '        diff_conf = conf.diff()\n', '        diff_fata = fata.diff()\n', '        province_df.ConfirmedCases = diff_conf\n', '        province_df.Fatalities = diff_fata\n', '        diff_df = pd.concat([diff_df,province_df],0)']"
"ASSIGN = {} ASSIGN['en'] = train[train.lang=='en'].iloc[:,0:-1] ASSIGN['ja'] = train[train.lang=='ja'].iloc[:,0:-1] ASSIGN['de'] = train[train.lang=='de'].iloc[:,0:-1] ASSIGN['na'] = train[train.lang=='na'].iloc[:,0:-1] ASSIGN['fr'] = train[train.lang=='fr'].iloc[:,0:-1] ASSIGN['zh'] = train[train.lang=='zh'].iloc[:,0:-1] ASSIGN['ru'] = train[train.lang=='ru'].iloc[:,0:-1] ASSIGN['es'] = train[train.lang=='es'].iloc[:,0:-1] ASSIGN = {} for key in ASSIGN: ASSIGN = ASSIGN.iloc[:,1:].sum(axis=0) ASSIGN.ASSIGN",0,"['lang_sets = {}\n', ""lang_sets['en'] = train[train.lang=='en'].iloc[:,0:-1]\n"", ""lang_sets['ja'] = train[train.lang=='ja'].iloc[:,0:-1]\n"", ""lang_sets['de'] = train[train.lang=='de'].iloc[:,0:-1]\n"", ""lang_sets['na'] = train[train.lang=='na'].iloc[:,0:-1]\n"", ""lang_sets['fr'] = train[train.lang=='fr'].iloc[:,0:-1]\n"", ""lang_sets['zh'] = train[train.lang=='zh'].iloc[:,0:-1]\n"", ""lang_sets['ru'] = train[train.lang=='ru'].iloc[:,0:-1]\n"", ""lang_sets['es'] = train[train.lang=='es'].iloc[:,0:-1]\n"", '\n', 'sums = {}\n', 'for key in lang_sets:\n', '    sums[key] = lang_sets[key].iloc[:,1:].sum(axis=0) / lang_sets[key].shape[0]']"
"ASSIGN = segment_lung_mask(pix_resampled, False) ASSIGN = segment_lung_mask(pix_resampled, True)",0,"['segmented_lungs = segment_lung_mask(pix_resampled, False)\n', 'segmented_lungs_fill = segment_lung_mask(pix_resampled, True)']"
"CHECKPOINT tumor_data.drop(['Unnamed: 32'], axis= 1, inplace = True) tumor_data.columns",0,"[""tumor_data.drop(['Unnamed: 32'], axis= 1, inplace = True)\n"", 'tumor_data.columns']"
tumor_data.isna().sum(),0,['tumor_data.isna().sum()']
SETUP warnings.filterwarnings('ignore') np.random.seed(4590),0,"['import numpy as np\n', 'import pandas as pd\n', 'import datetime\n', 'import gc\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', 'import lightgbm as lgb\n', 'from sklearn.model_selection import StratifiedKFold\n', 'from sklearn.metrics import mean_squared_error\n', 'import time\n', 'import warnings\n', ""warnings.filterwarnings('ignore')\n"", 'np.random.seed(4590)']"
CHECKPOINT print(finalData),0,['print(finalData)']
"df_all['timestamp_first_active']=pd.to_datetime(df_all['timestamp_first_active'], format='%Y%m%d%H%M%S') df_all.sample(n=5)",0,"[""df_all['timestamp_first_active']=pd.to_datetime(df_all['timestamp_first_active'], format='%Y%m%d%H%M%S')\n"", 'df_all.sample(n=5)']"
data.iloc[:3],0,['data.iloc[:3]']
"ASSIGN = pd.read_csv('path', sep=',')",0,"[""tscovid = pd.read_csv('/kaggle/input/novel-corona-virus-2019-dataset/time_series_covid_19_deaths.csv', sep=',')""]"
ASSIGN = ASSIGN[ASSIGN['calories'] < 10000].dropna(),0,"[""recipes = recipes[recipes['calories'] < 10000].dropna()""]"
SETUP,0,"['import matplotlib.pyplot as plt\n', 'import re\n', 'import os\n', '%matplotlib inline\n', '\n', '\n', 'import argparse\n', 'import pickle\n', '\n', ""import numpy as np; np.seterr(invalid='ignore')\n"", 'import pandas as pd']"
"SETUP ASSIGN = OneHotEncoder(handle_unknown='ignore') ASSIGN = pd.DataFrame(enc.fit_transform(train[['Embarked']]).toarray()) ASSIGN = ASSIGN.join(enc_df) ASSIGN = pd.DataFrame(enc.fit_transform(test[['Embarked']]).toarray()) ASSIGN = ASSIGN.join(enc_df) ASSIGN = ASSIGN.drop('Embarked',axis = 1) ASSIGN = ASSIGN.drop('Embarked',axis = 1)",0,"['from sklearn.preprocessing import OneHotEncoder\n', '\n', ""enc = OneHotEncoder(handle_unknown='ignore')\n"", '\n', '\n', ""enc_df = pd.DataFrame(enc.fit_transform(train[['Embarked']]).toarray())\n"", 'train = train.join(enc_df)\n', '\n', ""enc_df = pd.DataFrame(enc.fit_transform(test[['Embarked']]).toarray())\n"", 'test = test.join(enc_df)\n', '\n', '\n', ""train = train.drop('Embarked',axis = 1)\n"", ""test = test.drop('Embarked',axis = 1)""]"
"ASSIGN=plt.figure(figsize=(20,40)) city_vs_count.plot(kind=""barh"",fontsize=30) plt.grid(b=True, which='both', color='Black',linestyle='-') plt.ylabel(""city names"",fontsize=50,color=""red"",fontweight='bold') plt.title(""CITY VS RESTAURANT COUNT GRAPH"",fontsize=50,color=""BLUE"",fontweight='bold')",1,"['#lets plot citywise restaurant count in barh form\n', '\n', 'fig=plt.figure(figsize=(20,40))\n', 'city_vs_count.plot(kind=""barh"",fontsize=30)\n', ""plt.grid(b=True, which='both', color='Black',linestyle='-')\n"", 'plt.ylabel(""city names"",fontsize=50,color=""red"",fontweight=\'bold\')\n', 'plt.title(""CITY VS RESTAURANT COUNT GRAPH"",fontsize=50,color=""BLUE"",fontweight=\'bold\')']"
"CHECKPOINT def input_and_run3(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10): ASSIGN = keras.optimizers.Adam(learning_rate=alpha) ASSIGN=SGD(lr=alpha, momentum=0.9) model.compile(loss='binary_crossentropy', optimizer=ASSIGN,metrics=['accuracy']) model.fit(X_train,y_train,validation_data=(X_val, y_val), epochs=num_epochs) ASSIGN = model.evaluate(X_train,y_train) print(+str(ASSIGN[1]*100)) ASSIGN = model.evaluate(X_val,y_val) print(+str(ASSIGN[1]*100)) ASSIGN = model.evaluate(X_test,y_test) print(+str(ASSIGN[1]*100))",0,"['\n', '\n', 'def input_and_run3(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10):\n', '    \n', '    #datagen = ImageDataGenerator(width_shift_range=0.2,height_shift_range=0.2,horizontal_flip=True)\n', '    #datagen.fit(X_train)\n', '    \n', '    #compile model using accuracy to measure model performance\n', '    opt = keras.optimizers.Adam(learning_rate=alpha)\n', '    opt2=SGD(lr=alpha, momentum=0.9)\n', ""    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n"", '    \n', '    #train the model\n', '    model.fit(X_train,y_train,validation_data=(X_val, y_val), epochs=num_epochs)\n', '    #model.fit(datagen.flow(X_train, y_train),validation_data=(X_val, y_val), epochs=num_epochs)\n', '    \n', '    #Getting results\n', '    result = model.evaluate(X_train,y_train)\n', '    #print(result)\n', '    print(""Training accuracy = ""+str(result[1]*100))\n', '    result = model.evaluate(X_val,y_val)\n', '    #print(result)\n', '    print(""Validation accuracy = ""+str(result[1]*100))\n', '    result = model.evaluate(X_test,y_test)\n', '    #print(result)\n', '    print(""Test accuracy = ""+str(result[1]*100))\n', '\n', '\n']"
"ASSIGN = ASSIGN.drop(['Customer ID'], axis = 1) ASSIGN.head()",0,"['#Drop customer ID\n', ""data = data.drop(['Customer ID'], axis = 1)\n"", 'data.head()']"
train['Lat'][train['Lat'].isnull()] = np.nanmedian(train['Lat']) train['Long'][train['Long'].isnull()] = np.nanmedian(train['Long']) train['temp'][train['temp'].isnull()] = np.nanmedian(train['temp']) train['min'][train['min'].isnull()] = np.nanmedian(train['min']) train['max'][train['max'].isnull()] = np.nanmedian(train['max']) train['slp'][train['slp'].isnull()] = np.nanmedian(train['slp']) train['dewp'][train['dewp'].isnull()] = np.nanmedian(train['dewp']) train['rh'][train['rh'].isnull()] = np.nanmedian(train['rh']) train['ah'][train['ah'].isnull()] = np.nanmedian(train['ah']) train['stp'][train['stp'].isnull()] = np.nanmedian(train['stp']) train['wdsp'][train['wdsp'].isnull()] = np.nanmedian(train['wdsp']) train['prcp'][train['prcp'].isnull()] = np.nanmedian(train['prcp']) train['fog'][train['fog'].isnull()] = np.nanmedian(train['fog']),0,"['#fill NAN\n', ""train['Lat'][train['Lat'].isnull()] = np.nanmedian(train['Lat'])\n"", ""train['Long'][train['Long'].isnull()] = np.nanmedian(train['Long'])\n"", ""train['temp'][train['temp'].isnull()] = np.nanmedian(train['temp'])\n"", ""train['min'][train['min'].isnull()] = np.nanmedian(train['min'])\n"", ""train['max'][train['max'].isnull()] = np.nanmedian(train['max'])\n"", ""train['slp'][train['slp'].isnull()] = np.nanmedian(train['slp'])\n"", ""train['dewp'][train['dewp'].isnull()] = np.nanmedian(train['dewp'])\n"", ""train['rh'][train['rh'].isnull()] = np.nanmedian(train['rh'])\n"", ""train['ah'][train['ah'].isnull()] = np.nanmedian(train['ah'])\n"", ""train['stp'][train['stp'].isnull()] = np.nanmedian(train['stp'])\n"", ""train['wdsp'][train['wdsp'].isnull()] = np.nanmedian(train['wdsp'])\n"", ""train['prcp'][train['prcp'].isnull()] = np.nanmedian(train['prcp'])\n"", ""train['fog'][train['fog'].isnull()] = np.nanmedian(train['fog'])""]"
tourney_win_result['Seed_diff'] = tourney_win_result['Seed1'] - tourney_win_result['Seed2'] tourney_win_result['ScoreT_diff'] = tourney_win_result['ScoreT1'] - tourney_win_result['ScoreT2'] tourney_lose_result['Seed_diff'] = tourney_lose_result['Seed1'] - tourney_lose_result['Seed2'] tourney_lose_result['ScoreT_diff'] = tourney_lose_result['ScoreT1'] - tourney_lose_result['ScoreT2'],0,"[""tourney_win_result['Seed_diff'] = tourney_win_result['Seed1'] - tourney_win_result['Seed2']\n"", ""tourney_win_result['ScoreT_diff'] = tourney_win_result['ScoreT1'] - tourney_win_result['ScoreT2']\n"", ""tourney_lose_result['Seed_diff'] = tourney_lose_result['Seed1'] - tourney_lose_result['Seed2']\n"", ""tourney_lose_result['ScoreT_diff'] = tourney_lose_result['ScoreT1'] - tourney_lose_result['ScoreT2']""]"
ASSIGN = TPOTClassifier(),0,['pipeline_optimizer = TPOTClassifier()']
SETUP,0,"['WEIGHTS_FILE=""/kaggle/input/cat-dog-numpy/model2.h5""']"
"'''from imblearn.under_sampling import RandomUnderSampler ASSIGN=RandomUnderSampler(return_indices=True) train_12,y_train12,dropped12 = ASSIGN.fit_sample(train_12,train_12['isFraud']) train_1,y_train1,dropped1 = ASSIGN.fit_sample(train_1,train_1['isFraud']) train_2,y_train2,dropped2 = ASSIGN.fit_sample(train_2,train_2['isFraud']) train_3,y_train3,dropped3 = ASSIGN.fit_sample(train_3,train_3['isFraud']) train_4,y_train4,dropped4 = ASSIGN.fit_sample(train_4,train_4['isFraud']) train_5,y_train5,dropped5 = ASSIGN.fit_sample(train_5,train_5['isFraud']) train_6,y_train6,dropped6 = ASSIGN.fit_sample(train_6,train_6['isFraud']) del train del y_train12 del y_train1 del y_train2 del y_train3 del y_train4 del y_train5 del y_train6'''",0,"[""'''from imblearn.under_sampling import RandomUnderSampler\n"", 'ran=RandomUnderSampler(return_indices=True) ##intialize to return indices of dropped rows\n', ""train_12,y_train12,dropped12 = ran.fit_sample(train_12,train_12['isFraud'])\n"", ""train_1,y_train1,dropped1 = ran.fit_sample(train_1,train_1['isFraud'])\n"", ""train_2,y_train2,dropped2 = ran.fit_sample(train_2,train_2['isFraud'])\n"", ""train_3,y_train3,dropped3 = ran.fit_sample(train_3,train_3['isFraud'])\n"", ""train_4,y_train4,dropped4 = ran.fit_sample(train_4,train_4['isFraud'])\n"", ""train_5,y_train5,dropped5 = ran.fit_sample(train_5,train_5['isFraud'])\n"", ""train_6,y_train6,dropped6 = ran.fit_sample(train_6,train_6['isFraud'])\n"", 'del train\n', 'del y_train12\n', 'del y_train1\n', 'del y_train2\n', 'del y_train3\n', 'del y_train4\n', 'del y_train5\n', ""del y_train6'''""]"
"lineplot(average_sales_group, title = 'Average Revenue per Game per Year', ylabel ='Sales (In Millions)', legendsize = 8, legendloc = 'upper right')",1,"[""lineplot(average_sales_group, title = 'Average Revenue per Game per Year', ylabel ='Sales (In Millions)', legendsize = 8, legendloc = 'upper right')""]"
key_df.head(),0,['key_df.head()']
learn.lr_find() learn.recorder.plot(suggestion=True),1,"['learn.lr_find()\n', 'learn.recorder.plot(suggestion=True)']"
"poly.fit(x_train,x_test) poly.score(y_train,y_test)",0,"['poly.fit(x_train,x_test)\n', 'poly.score(y_train,y_test)']"
"ASSIGN = tabular_learner(data, layers=[200,100],metrics=accuracy, callback_fns=AUROC)",0,"['learn = tabular_learner(data, layers=[200,100],metrics=accuracy, callback_fns=AUROC)\n', '#learn = tabular_learner(data, layers=[1000,500,100],emb_drop=0.04,ps=(0.001, 0.01, 0.1),metrics=accuracy, callback_fns=AUROC,wd=1e-2)#.to_fp16()']"
"Gaussian Naive Bayes ASSIGN = GaussianNB() ASSIGN.fit(X_train, Y_train) ASSIGN = gaussian.predict(X_test) ASSIGN.score(X_train, Y_train)",0,"['Gaussian Naive Bayes\n', '\n', 'gaussian = GaussianNB()\n', '\n', 'gaussian.fit(X_train, Y_train)\n', '\n', 'Y_pred = gaussian.predict(X_test)\n', '\n', 'gaussian.score(X_train, Y_train)']"
"ASSIGN = [10,24,32,64,128] ASSIGN = 10 ASSIGN = 6 def get_new_layer(actvn,x): ASSIGN = 1 ASSIGN = ASSIGN -1 ASSIGN[ASSIGN] = x.shape[1] ASSIGN == 0: return get_conv_layer(x.shape[1],ASSIGN[ASSIGN],3).to(device) ASSIGN == 1: return get_conv_layer(x.shape[1],ASSIGN[ASSIGN],5).to(device) ASSIGN == 2: return get_sep_layer(x.shape[1],ASSIGN[ASSIGN],3).to(device) ASSIGN == 3: return get_sep_layer(x.shape[1],ASSIGN[ASSIGN],5).to(device) ASSIGN == 4: return get_max_pool_layer(3).to(device) ASSIGN == 5: return get_avg_pool_layer(3).to(device) return get_avg_pool_layer(3).to(device) def get_out_layer(size): ASSIGN = nn.Sequential( nn.Linear(in_features=size[1]*size[2]*size[3], out_features=10), nn.LogSoftmax(), ) return ASSIGN.apply(weights_init).to(device)",0,"['# 1. Conv 3\n', '# 2. Conv 5\n', '# 3. Sep 3\n', '# 4. Sep 5\n', '# 5. MaxPool\n', '# 6. AvgPool\n', '\n', 'out_features_shape = [10,24,32,64,128]\n', 'output_size = 10\n', 'num_actvn_fns = 6\n', '\n', 'def get_new_layer(actvn,x):\n', '    ofsindex = 1\n', '    actvn = actvn -1\n', '#     print(""Actvn -- ""+str(actvn))\n', '    \n', '    out_features_shape[ofsindex] = x.shape[1]\n', '    \n', '    if actvn == 0:\n', '        return get_conv_layer(x.shape[1],out_features_shape[ofsindex],3).to(device)\n', '    if actvn == 1:\n', '        return get_conv_layer(x.shape[1],out_features_shape[ofsindex],5).to(device)\n', '    if actvn == 2:\n', '        return get_sep_layer(x.shape[1],out_features_shape[ofsindex],3).to(device)\n', '    if actvn == 3:\n', '        return get_sep_layer(x.shape[1],out_features_shape[ofsindex],5).to(device)\n', '    if actvn == 4:\n', '        return get_max_pool_layer(3).to(device)\n', '    if actvn == 5:\n', '        return get_avg_pool_layer(3).to(device)\n', '    return get_avg_pool_layer(3).to(device)\n', '\n', 'def get_out_layer(size):\n', '    nlayer = nn.Sequential(\n', '            nn.Linear(in_features=size[1]*size[2]*size[3], out_features=10),\n', '            nn.LogSoftmax(),\n', '        )\n', '    return nlayer.apply(weights_init).to(device)']"
"SETUP for column in numerical: ASSIGN = StandardScaler() if train_df[column].max() > 100 and train_df[column].min() >= 0: train_df[column] = np.log1p(train_df[column]) test_df[column] = np.log1p(test_df[column]) ASSIGN.fit(np.concatenate([train_df[column].values.reshape(-1,1), test_df[column].values.reshape(-1,1)])) train_df[column] = ASSIGN.transform(train_df[column].values.reshape(-1,1)) test_df[column] = ASSIGN.transform(test_df[column].values.reshape(-1,1))",0,"['from sklearn.preprocessing import StandardScaler\n', '\n', 'for column in numerical:\n', '    scaler = StandardScaler()\n', '    if train_df[column].max() > 100 and train_df[column].min() >= 0:\n', '        train_df[column] = np.log1p(train_df[column])\n', '        test_df[column] = np.log1p(test_df[column])\n', '    scaler.fit(np.concatenate([train_df[column].values.reshape(-1,1), test_df[column].values.reshape(-1,1)]))\n', '    train_df[column] = scaler.transform(train_df[column].values.reshape(-1,1))\n', '    test_df[column] = scaler.transform(test_df[column].values.reshape(-1,1))']"
len(os.listdir('..path')),0,"[""len(os.listdir('../input/iwildcam-2019-fgvc6/train_images'))""]"
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'fashion-mnist_test.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,"[""nRowsRead = 1000 # specify 'None' if want to read whole file\n"", '# fashion-mnist_test.csv has 10000 rows in reality, but we are only loading/previewing the first 1000 rows\n', ""df1 = pd.read_csv('../input/fashion-mnist_test.csv', delimiter=',', nrows = nRowsRead)\n"", ""df1.dataframeName = 'fashion-mnist_test.csv'\n"", 'nRow, nCol = df1.shape\n', ""print(f'There are {nRow} rows and {nCol} columns')""]"
ASSIGN = ASSIGN + ASSIGN + 1,0,"['final[""Family""] = final[""SibSp""] + final[""Parch""] + 1']"
"pp.fit(df.question_text.to_numpy(),y)",0,"['pp.fit(df.question_text.to_numpy(),y)']"
"CHECKPOINT model5.save(""model5.h5"") print()",0,"['model5.save(""model5.h5"")\n', 'print(""Saved model to disk"")']"
CHECKPOINT print(torch.__version__),0,['print(torch.__version__)']
"GBC.fit(x_train,x_test) GBC.score(y_train,y_test)",0,"['GBC.fit(x_train,x_test)\n', 'GBC.score(y_train,y_test)']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['%matplotlib inline\n', 'import pandas as pd\n', 'from fastai.tabular import *\n', 'import fastai \n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))']"
df.tail(),0,"['#an example of some of our data\n', 'df.tail()']"
"SETUP SETUP ASSIGN = datetime.datetime.strptime(START_DATE, '%Y-%m-%d') ASSIGN = ASSIGN.apply(lambda x: (startdate + datetime.timedelta(seconds = x))) ASSIGN = ASSIGN.apply(lambda x: (startdate + datetime.timedelta(seconds = x))) ASSIGN = ASSIGN.dt.hour ASSIGN = ASSIGN.dt.hour ASSIGN = ASSIGN.dt.month ASSIGN = ASSIGN.dt.month",0,"['import datetime\n', ""START_DATE = '2017-12-01'\n"", ""startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n"", ""train['TransactionDT'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n"", ""test['TransactionDT'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n"", '\n', ""train['hour'] = train['TransactionDT'].dt.hour\n"", ""test['hour'] = test['TransactionDT'].dt.hour\n"", '\n', ""train['month'] = train['TransactionDT'].dt.month\n"", ""test['month'] = test['TransactionDT'].dt.month\n"", ""#print(train['TransactionDT'])""]"
CHECKPOINT ASSIGN = data_features.isnull().sum().sort_values(ascending = False) ASSIGN = ASSIGN[ASSIGN > 0] missing_data,0,"['missing_data = data_features.isnull().sum().sort_values(ascending = False)\n', 'missing_data = missing_data[missing_data > 0]\n', 'missing_data']"
"CHECKPOINT ASSIGN = 0.1 ASSIGN = int(valfrac * train_grand.shape[0]) print(ASSIGN) np.random.shuffle(train_grand) y_train, X_train = train_grand[ASSIGN: , 0], train_grand[ASSIGN: , 1:] y_val, X_val = train_grand[0:ASSIGN, 0], train_grand[0:ASSIGN, 1:] del train_grand ASSIGN = keras.utils.to_categorical(ASSIGN, num_classes) ASSIGN = ASSIGN.reshape(-1, sequence_length,2) ASSIGN = keras.utils.to_categorical(ASSIGN, num_classes) ASSIGN = ASSIGN.reshape(-1, sequence_length,2) print(ASSIGN.shape, ""\n"", ASSIGN.shape, ""\n"", ASSIGN.shape, ""\n"", ASSIGN.shape)",0,"['valfrac = 0.1 \n', 'cutpt = int(valfrac * train_grand.shape[0])\n', 'print(cutpt)\n', '\n', 'np.random.shuffle(train_grand)\n', 'y_train, X_train = train_grand[cutpt: , 0], train_grand[cutpt: , 1:]\n', 'y_val, X_val = train_grand[0:cutpt, 0], train_grand[0:cutpt, 1:]\n', '\n', 'del train_grand\n', '\n', 'y_train = keras.utils.to_categorical(y_train, num_classes)\n', 'X_train = X_train.reshape(-1, sequence_length,2)\n', '\n', 'y_val = keras.utils.to_categorical(y_val, num_classes)\n', 'X_val = X_val.reshape(-1, sequence_length,2)\n', '\n', 'print(y_train.shape, ""\\n"",\n', '      X_train.shape, ""\\n"",\n', '      y_val.shape, ""\\n"",\n', '      X_val.shape)\n']"
"sns.boxplot(data_features['MasVnrType'],data_features['MasVnrArea'])",1,"[""sns.boxplot(data_features['MasVnrType'],data_features['MasVnrArea'])\n"", ""#Maybe fill it with 'Stone' is more reasonable.\n"", '#Something strange here. None means no masonry but there still several values. May theuy are outliars that I should remove?']"
"ASSIGN = '..path' ASSIGN = pd.read_csv(test_data_path) ASSIGN = test_data[features] ASSIGN = np.nan_to_num(ASSIGN) ASSIGN = rf_model_on_full_data.predict(test_X) ASSIGN = pd.DataFrame({'Id': test_data.Id, 'SalePrice': ASSIGN}) ASSIGN.to_csv('submission.csv', index=False)",0,"['# path to file you will use for predictions\n', ""test_data_path = '../input/test.csv'\n"", '\n', '# read test data file using pandas\n', 'test_data = pd.read_csv(test_data_path)\n', '\n', '# create test_X which comes from test_data but includes only the columns you used for prediction.\n', '# The list of columns is stored in a variable called features\n', 'test_X = test_data[features]\n', 'test_X = np.nan_to_num(test_X)\n', '\n', '# make predictions which we will submit. \n', 'test_preds = rf_model_on_full_data.predict(test_X)\n', '\n', '# The lines below shows how to save predictions in format used for competition scoring\n', '# Just uncomment them.\n', '\n', ""output = pd.DataFrame({'Id': test_data.Id,\n"", ""                       'SalePrice': test_preds})\n"", ""output.to_csv('submission.csv', index=False)""]"
CHECKPOINT data.path.values,0,['data.path.values']
"CHECKPOINT print('Expectative for coronavirus deaths till April 01 is', fcast)",0,"[""print('Expectative for coronavirus deaths till April 01 is', fcast)""]"
"CHECKPOINT ASSIGN=data_risk3[:2000] ASSIGN['AKA Name']=ASSIGN['AKA Name'].apply(lambda x:x.strip('`').strip()) ASSIGN=data_risk3_2000.Longitude.mean() ASSIGN=data_risk3_2000.Latitude.mean() ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(risk3_map) for lat,lon,label in zip(ASSIGN.Latitude,ASSIGN.Longitude,ASSIGN['AKA Name']): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN) risk3_map",1,"['data_risk3_2000=data_risk3[:2000]\n', ""data_risk3_2000['AKA Name']=data_risk3_2000['AKA Name'].apply(lambda x:x.strip('`').strip())\n"", 'Long=data_risk3_2000.Longitude.mean()\n', 'Lat=data_risk3_2000.Latitude.mean()\n', 'risk3_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'risk3_distribution_map=plugins.MarkerCluster().add_to(risk3_map)\n', ""for lat,lon,label in zip(data_risk3_2000.Latitude,data_risk3_2000.Longitude,data_risk3_2000['AKA Name']):\n"", '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(risk3_distribution_map)\n', 'risk3_map.add_child(risk3_distribution_map)\n', '\n', 'risk3_map']"
train_identity.head(),0,['train_identity.head()']
"ASSIGN=np.load(X_TRAIN_FILE) ASSIGN=a.f.arr_0 ASSIGN=np.load(Y_TRAIN_FILE) ASSIGN=a.f.arr_0 ASSIGN=np.load(X_TEST_FILE) ASSIGN=a ASSIGN=np.load(Y_TEST_FILE) ASSIGN=a ASSIGN, X_val, ASSIGN, Y_val = train_test_split(ASSIGN, ASSIGN, test_size=0.25, random_state=42)",0,"['a=np.load(X_TRAIN_FILE)\n', 'X_train=a.f.arr_0\n', 'a=np.load(Y_TRAIN_FILE)\n', 'Y_train=a.f.arr_0\n', 'a=np.load(X_TEST_FILE)\n', 'X_test=a\n', 'a=np.load(Y_TEST_FILE)\n', 'Y_test=a\n', '\n', 'X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)\n']"
SETUP sys.path.append(os.path.abspath('.path')),0,"['import pandas as pd\n', 'import matplotlib.pyplot as plt\n', 'import sys\n', 'import os\n', '\n', ""sys.path.append(os.path.abspath('./ivis-explain'))\n"", 'from sklearn.preprocessing import StandardScaler, MinMaxScaler\n', 'from sklearn.model_selection import train_test_split\n', 'from sklearn.metrics import confusion_matrix, average_precision_score, roc_auc_score, classification_report, roc_curve, precision_recall_curve\n', 'from sklearn.linear_model import LogisticRegression\n', '\n', 'from ivis import Ivis\n', 'from ivis_explanations import LinearExplainer\n', '\n']"
match_bets_df.reset_index(inplace= True),0,['match_bets_df.reset_index(inplace= True)']
CHECKPOINT data.columns,0,['data.columns']
CHECKPOINT ASSIGN = LabelEncoder() data['target_label'] = ASSIGN.fit_transform(data['target']) ASSIGN = ASSIGN.sample(frac=1).reset_index(drop=True) data,0,"['# creating instance of labelencoder\n', 'labelencoder = LabelEncoder()\n', '# Assigning numerical values and storing in another column\n', ""data['target_label'] = labelencoder.fit_transform(data['target'])\n"", 'data = data.sample(frac=1).reset_index(drop=True)\n', 'data']"
"sns.countplot(x = 'Survived', data = df1)",1,"[""sns.countplot(x = 'Survived', data = df1)""]"
ASSIGN = secondDigit(finalData) ASSIGN = thirdDigit(finalData) ASSIGN = firstDigit(finalData),0,"['second_Digit = secondDigit(finalData)\n', 'third_Digit = thirdDigit(finalData)\n', 'first_Digit = firstDigit(finalData)']"
"SETUP CHECKPOINT ASSIGN = np.array([[2,3,5], [1,6,11]], np.int32) ASSIGN = (b[b%2!=0]) print(ASSIGN)",0,"['# 2. Extract the odd numbers from a 2D array using numpy package.\n', 'import numpy as np\n', 'b = np.array([[2,3,5], [1,6,11]], np.int32)\n', 'c = (b[b%2!=0])\n', 'print(c)']"
"sns.catplot(x ='Survived', y ='SibSp', data = df1)",1,"[""sns.catplot(x ='Survived', y ='SibSp', data = df1)""]"
CHECKPOINT for col in category_cols: ASSIGN = train[col].nunique() ASSIGN = test[col].nunique() ASSIGN = len(train.loc[train[col].isna()]) path(train) ASSIGN = len(test.loc[test[col].isna()]) path(test) print(f'Col name:{col:30}\tunique cate num in train:{ASSIGN:5}\tunique cate num in train:{ASSIGN:5}\tnull sample in train:{ASSIGN:.2f}\tnull sample in test:{ASSIGN:.2f}'),0,"['for col in category_cols:\n', '    nunique_tr = train[col].nunique()\n', '    nunique_te = test[col].nunique()\n', '    na_tr = len(train.loc[train[col].isna()]) / len(train)\n', '    na_te = len(test.loc[test[col].isna()]) / len(test)\n', ""    print(f'Col name:{col:30}\\tunique cate num in train:{nunique_tr:5}\\tunique cate num in train:{nunique_te:5}\\tnull sample in train:{na_tr:.2f}\\tnull sample in test:{na_te:.2f}')\n"", '    ']"
"learn.fit_one_cycle(3,max_lr=1e-8)",0,"['# 2nd learning phase using suggested learning rate during 3 cycles\n', 'learn.fit_one_cycle(3,max_lr=1e-8)']"
"ASSIGN = ASSIGN.drop(['Patient addmited to semi-intensive unit (1=yes, 0=no)'], axis=1, inplace=True) ASSIGN = relevant",0,"[""covid = covid.drop(['Patient addmited to semi-intensive unit (1=yes, 0=no)'], axis=1, inplace=True)\n"", 'covid = relevant']"
data_2019[data_2019.Statistic=='Official World Golf Ranking'].Variable.unique(),0,"[""data_2019[data_2019.Statistic=='Official World Golf Ranking'].Variable.unique()""]"
"CHECKPOINT ASSIGN = go.Figure(data=[ go.Line(name='Confirmed', x=formated_gdf[formated_gdf['Countrypath']=='Taiwan']['date'], y=formated_gdf[formated_gdf['Countrypath']=='Taiwan']['Confirmed']), go.Line(name='Deaths', x=formated_gdf[formated_gdf['Countrypath']=='Taiwan']['date'], y=formated_gdf[formated_gdf['Countrypath']=='Taiwan']['Deaths']), go.Line(name='Recovered', x=formated_gdf[formated_gdf['Countrypath']=='Taiwan']['date'], y=formated_gdf[formated_gdf['Countrypath']=='Taiwan']['Recovered']), ]) ASSIGN.update_layout( ASSIGN=""Number of Confirmed,Recovered,death in Taiwan for each day"", ASSIGN=""date"", ASSIGN=""People"", ASSIGN=dict( ASSIGN=""Courier New, monospace"", ASSIGN=18, ASSIGN=""#7f7f7f"" )) fig",1,"['fig = go.Figure(data=[\n', ""    go.Line(name='Confirmed', x=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['Confirmed']),\n"", ""    go.Line(name='Deaths', x=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['Deaths']),\n"", ""    go.Line(name='Recovered', x=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Taiwan']['Recovered']),\n"", '])\n', '\n', 'fig.update_layout(\n', '    title=""Number of Confirmed,Recovered,death in Taiwan for each day"",\n', '    xaxis_title=""date"",\n', '    yaxis_title=""People"",\n', '    font=dict(\n', '        family=""Courier New, monospace"",\n', '        size=18,\n', '        color=""#7f7f7f""\n', '    ))\n', 'fig']"
"CHECKPOINT ASSIGN = metrics.roc_curve(test_y, predict_y, pos_label=1) print('max_depth=7 auc: %.5f' % metrics.auc(fpr, tpr))",0,"['fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n', ""print('max_depth=7 auc: %.5f' % metrics.auc(fpr, tpr))""]"
"def plotScatterMatrix(df, plotSize, textSize): ASSIGN = ASSIGN.select_dtypes(include =[np.number]) ASSIGN = ASSIGN.dropna('columns') ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]] ASSIGN = list(df) if len(ASSIGN) > 10: ASSIGN = ASSIGN[:10] ASSIGN = ASSIGN[columnNames] ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde') ASSIGN = df.corr().values for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)): ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize) plt.suptitle('Scatter and Density Plot') plt.show()",1,"['# Scatter and density plots\n', 'def plotScatterMatrix(df, plotSize, textSize):\n', '    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n', '    # Remove rows and columns that would lead to df being singular\n', ""    df = df.dropna('columns')\n"", '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n', '    columnNames = list(df)\n', '    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n', '        columnNames = columnNames[:10]\n', '    df = df[columnNames]\n', ""    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n"", '    corrs = df.corr().values\n', '    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n', ""        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n"", ""    plt.suptitle('Scatter and Density Plot')\n"", '    plt.show()\n']"
"ASSIGN = data.drop(['Clicked'], axis = 1).values ASSIGN = data['Clicked'].values",0,"['#Split data into dependent and independent variables\n', ""X = data.drop(['Clicked'], axis = 1).values\n"", ""y = data['Clicked'].values""]"
"ASSIGN=plt.subplots(1,2,figsize=(15,8)) sns.barplot(x=data_risk3['Facility Type'].value_counts()[:10],y=data_risk3['Facility Type'].value_counts()[:10].index,ax=ax[0]) ax[0].set_title(""Top 10 Facility Type by the counts of risk 3 "",size=20) ax[0].set_xlabel('counts',size=18) ASSIGN=data_risk3.groupby(['Facility Type'])['Inspection ID'].agg('ASSIGN').sort_values(ascending=False) ASSIGN=list(data_risk3.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10]) ASSIGN=list(count[:10]) ASSIGN.append(ASSIGN.agg(sum)-ASSIGN[:10].agg('sum')) ASSIGN.append('Other') ASSIGN=pd.DataFrame({""group"":groups,""counts"":counts}) ASSIGN=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum') ASSIGN.plot(kind='pie', y='ASSIGN', labels=ASSIGN,colors=ASSIGN,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1]) ax[1].set_ylabel('') ax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))",1,"['fig,ax=plt.subplots(1,2,figsize=(15,8))\n', ""sns.barplot(x=data_risk3['Facility Type'].value_counts()[:10],y=data_risk3['Facility Type'].value_counts()[:10].index,ax=ax[0])\n"", 'ax[0].set_title(""Top 10 Facility Type by the counts of risk 3 "",size=20)\n', ""ax[0].set_xlabel('counts',size=18)\n"", '\n', '\n', ""count=data_risk3.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False)\n"", ""groups=list(data_risk3.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10])\n"", 'counts=list(count[:10])\n', ""counts.append(count.agg(sum)-count[:10].agg('sum'))\n"", ""groups.append('Other')\n"", 'type_dict=pd.DataFrame({""group"":groups,""counts"":counts})\n', ""clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n"", ""type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n"", ""ax[1].set_ylabel('')\n"", 'ax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))']"
"sns.relplot(x ='SOP', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')",1,"[""sns.relplot(x ='SOP', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')""]"
"CHECKPOINT ASSIGN = RandomForestClassifier(oob_score = True,n_estimators =650 ,min_samples_split = 4,max_features = 'log2',max_depth =6,criterion = 'gini',class_weight = 'balanced_subsample',bootstrap = True) Estimator.append(('ASSIGN',RandomForestClassifier(oob_score = True,n_estimators =650 ,min_samples_split = 4,max_features = 'log2',max_depth =6,criterion = 'gini',class_weight = 'balanced_subsample',bootstrap = True))) ASSIGN = cross_val_score(rf,x_train,x_test,ASSIGN=10) Accuracy6 = ASSIGN.mean() Accuracy.append(Accuracy6) print(ASSIGN) print(ASSIGN.mean())",0,"[""rf = RandomForestClassifier(oob_score = True,n_estimators =650 ,min_samples_split = 4,max_features = 'log2',max_depth =6,criterion = 'gini',class_weight = 'balanced_subsample',bootstrap = True)\n"", ""Estimator.append(('rf',RandomForestClassifier(oob_score = True,n_estimators =650 ,min_samples_split = 4,max_features = 'log2',max_depth =6,criterion = 'gini',class_weight = 'balanced_subsample',bootstrap = True)))\n"", 'cv = cross_val_score(rf,x_train,x_test,cv=10)\n', 'Accuracy6 = cv.mean()\n', 'Accuracy.append(Accuracy6)\n', 'print(cv)\n', 'print(cv.mean())']"
"X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)",0,"['X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)']"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") print() print(ASSIGN.keys()) print() print(ASSIGN.shape)",0,"['#1\n', 'import pandas as pd \n', 'iris_data = pd.read_csv(""../input/iris-dataset/iris.data.csv"")\n', 'print(""\\nKeys of Iris dataset:"")\n', 'print(iris_data.keys())\n', 'print(""\\nNumber of rows and columns of Iris dataset:"")\n', 'print(iris_data.shape)']"
ASSIGN = pd.DataFrame(ASSIGN),0,['artime = pd.DataFrame(artime)']
"CHECKPOINT ASSIGN = go.Figure(data=[ go.Line(name='Confirmed', x=Italy['date'], y=Italy['Confirmed']), go.Line(name='Deaths', x=Italy['date'], y=Italy['Deaths']), go.Line(name='Recovered', x=Italy['date'], y=Italy['Recovered']), ]) ASSIGN.update_layout( ASSIGN=""Number of Confirmed,Recovered,death in Italy for each day"", ASSIGN=""date"", ASSIGN=""People"", ASSIGN=dict( ASSIGN=""Courier New, monospace"", ASSIGN=18, ASSIGN=""#7f7f7f"" )) fig",1,"['fig = go.Figure(data=[\n', ""    go.Line(name='Confirmed', x=Italy['date'], y=Italy['Confirmed']),\n"", ""    go.Line(name='Deaths', x=Italy['date'], y=Italy['Deaths']),\n"", ""    go.Line(name='Recovered', x=Italy['date'], y=Italy['Recovered']),\n"", '])\n', '\n', 'fig.update_layout(\n', '    title=""Number of Confirmed,Recovered,death in Italy for each day"",\n', '    xaxis_title=""date"",\n', '    yaxis_title=""People"",\n', '    font=dict(\n', '        family=""Courier New, monospace"",\n', '        size=18,\n', '        color=""#7f7f7f""\n', '    ))\n', 'fig']"
"df_final[""CITY""].unique()",0,"['#lets count how many unique cities are there \n', '\n', 'df_final[""CITY""].unique()']"
"SETUP CHECKPOINT ASSIGN = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', 'Matthew', 'Laura'], 'score': [12.5, 9, 16.5, 6.0, 9, 20, np.nan, 5.5], 'attempts': [1, 3, 2, 3, 2, 3, 1, 1], 'qualify': ['yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no']} ASSIGN = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'] ASSIGN = pd.DataFrame(exam_data , index=labels) print(ASSIGN)",0,"['import pandas as pd\n', 'import numpy as np\n', ""exam_data  = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', 'Matthew', 'Laura'],\n"", ""        'score': [12.5, 9, 16.5, 6.0, 9, 20, np.nan, 5.5],\n"", ""        'attempts': [1, 3, 2, 3, 2, 3, 1, 1],\n"", ""        'qualify': ['yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no']}\n"", ""labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n"", '\n', 'df = pd.DataFrame(exam_data , index=labels)\n', 'print(df)']"
learn.lr_find() learn.recorder.plot(suggestion=True),1,"['learn.lr_find()\n', 'learn.recorder.plot(suggestion=True)']"
CHECKPOINT data,0,['data#total no. of observations']
"CHECKPOINT with pd.option_context('display.max_rows', None, 'display.max_columns', None): print(pred_df_with_cat_number)",0,"[""with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n"", '    print(pred_df_with_cat_number)']"
"league_df.drop('country_id', axis= 1, inplace= True)",0,"['#country_id column drop\n', ""league_df.drop('country_id', axis= 1, inplace= True)""]"
ASSIGN = pca.transform(X_train) ASSIGN = pca.transform(X_test),0,"['X_train_pca = pca.transform(X_train)\n', 'X_test_pca = pca.transform(X_test)']"
"ASSIGN = range(0,11,2) ASSIGN = range(1,12, 2) ASSIGN = draw_df[draw_df.index.isin(evens)] ASSIGN = draw_df[draw_df.index.isin(odds)] ASSIGN = [ast.literal_eval(pts) for pts in df1.drawing.values] ASSIGN = [ast.literal_eval(pts) for pts in df2.drawing.values] ASSIGN = df2.word.tolist() for i, example in enumerate(ASSIGN): plt.figure(figsize=(6,3)) for x,y in example: plt.subplot(1,2,1) plt.plot(x, y, marker='.') plt.axis('off') for x,y, in ASSIGN[i]: plt.subplot(1,2,2) plt.plot(x, y, marker='.') plt.axis('off') ASSIGN = labels[i] plt.title(ASSIGN, fontsize=10) plt.show()",1,"['evens = range(0,11,2)\n', 'odds = range(1,12, 2)\n', '# We have drawing images, 2 per label, consecutively\n', 'df1 = draw_df[draw_df.index.isin(evens)]\n', 'df2 = draw_df[draw_df.index.isin(odds)]\n', '\n', 'example1s = [ast.literal_eval(pts) for pts in df1.drawing.values]\n', 'example2s = [ast.literal_eval(pts) for pts in df2.drawing.values]\n', 'labels = df2.word.tolist()\n', '\n', 'for i, example in enumerate(example1s):\n', '    plt.figure(figsize=(6,3))\n', '    \n', '    for x,y in example:\n', '        plt.subplot(1,2,1)\n', ""        plt.plot(x, y, marker='.')\n"", ""        plt.axis('off')\n"", '\n', '    for x,y, in example2s[i]:\n', '        plt.subplot(1,2,2)\n', ""        plt.plot(x, y, marker='.')\n"", ""        plt.axis('off')\n"", '        label = labels[i]\n', '        plt.title(label, fontsize=10)\n', '\n', '    plt.show()  ']"
"CHECKPOINT ASSIGN = metrics.roc_curve(test_y, predict_y, pos_label=1) print('max_depth=3 auc: %.5f' % metrics.auc(fpr, tpr))",0,"['#auc\n', 'fpr, tpr, thresholds = metrics.roc_curve(test_y, predict_y, pos_label=1)\n', ""print('max_depth=3 auc: %.5f' % metrics.auc(fpr, tpr))""]"
"formated_gdf['size'] = formated_gdf['Deaths'].pow(0.3) ASSIGN = px.scatter_geo(formated_gdf, locations=""Countrypath"", locationmode='country names', ASSIGN=""Deaths"", size='size', hover_name=""Countrypath"", ASSIGN= [0, max(formated_gdf['Deaths'])+2], ASSIGN=""natural earth"", animation_frame=""date"", ASSIGN='Deaths for each day') ASSIGN.show()",1,"[""formated_gdf['size'] = formated_gdf['Deaths'].pow(0.3)\n"", '\n', 'fig = px.scatter_geo(formated_gdf, locations=""Country/Region"", locationmode=\'country names\', \n', '                     color=""Deaths"", size=\'size\', hover_name=""Country/Region"", \n', ""                     range_color= [0, max(formated_gdf['Deaths'])+2], \n"", '                     projection=""natural earth"", animation_frame=""date"", \n', ""                     title='Deaths for each day')\n"", 'fig.show()']"
"ASSIGN=plt.subplots(1,2,figsize=(15,8)) ASSIGN = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",'cadetblue','hotpink','orange','darksalmon','brown') train.MSSubClass.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=ASSIGN,ax=ax[0]) ax[0].set_title(""Top 10 MSSubClass by counts"",size=20) ax[0].set_xlabel('counts',size=18) ASSIGN=train.MSSubClass.value_counts() ASSIGN=list(train.MSSubClass.value_counts().index)[:10] ASSIGN=list(count[:10]) ASSIGN.append(ASSIGN.agg(sum)-ASSIGN[:10].agg('sum')) ASSIGN.append('Other') ASSIGN=pd.DataFrame({""group"":groups,""counts"":counts}) ASSIGN=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum') ASSIGN = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1]) plt.legend(loc=0, bbox_to_anchor=(1.15,0.8)) plt.subplots_adjust(wspace =0.5, hspace =0) plt.ylabel('')",1,"['fig,ax=plt.subplots(1,2,figsize=(15,8))\n', 'clr = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",\'cadetblue\',\'hotpink\',\'orange\',\'darksalmon\',\'brown\')\n', ""train.MSSubClass.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n"", 'ax[0].set_title(""Top 10 MSSubClass by counts"",size=20)\n', ""ax[0].set_xlabel('counts',size=18)\n"", '\n', '\n', 'count=train.MSSubClass.value_counts()\n', 'groups=list(train.MSSubClass.value_counts().index)[:10]\n', 'counts=list(count[:10])\n', ""counts.append(count.agg(sum)-count[:10].agg('sum'))\n"", ""groups.append('Other')\n"", 'type_dict=pd.DataFrame({""group"":groups,""counts"":counts})\n', ""clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n"", ""qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n"", 'plt.legend(loc=0, bbox_to_anchor=(1.15,0.8)) \n', 'plt.subplots_adjust(wspace =0.5, hspace =0)\n', ""plt.ylabel('')""]"
SETUP,0,"['from keras.layers import Dense\n', 'from keras.models import Sequential']"
"''' ASSIGN = ['gini','entropy'] ASSIGN = ['best','random'] ASSIGN = [5,10,15,20,25] ASSIGN = [2,3,4,5] ASSIGN = ['dict','balanced','None'] ASSIGN = [5,6] ASSIGN = DecisionTreeClassifier() ASSIGN = {'criterion': ['gini','entropy'],'splitter': ['best','random'], 'max_depth':[5,10,15,20,25],'min_samples_split':[2,3,4,5],'class_weight':['dict','balanced','None'],'random_state':[5,6]} ASSIGN = GridSearchCV(Tree, parameters, scoring='accuracy' ,cv = 10) ASSIGN.fit(x_train, x_test) ASSIGN.best_params_ '''",0,"[""'''\n"", ""criterion = ['gini','entropy']\n"", ""splitter = ['best','random']\n"", 'max_depth = [5,10,15,20,25]\n', 'min_samples_split = [2,3,4,5]\n', ""class_weight = ['dict','balanced','None']\n"", 'random_state = [5,6]\n', '\n', '\n', 'Tree = DecisionTreeClassifier()\n', '\n', ""parameters = {'criterion': ['gini','entropy'],'splitter': ['best','random'], 'max_depth':[5,10,15,20,25],'min_samples_split':[2,3,4,5],'class_weight':['dict','balanced','None'],'random_state':[5,6]}\n"", '\n', ""tree_classifier = GridSearchCV(Tree, parameters, scoring='accuracy' ,cv = 10)\n"", 'tree_classifier.fit(x_train, x_test)\n', 'tree_classifier.best_params_\n', ""'''""]"
"len(df_final[""CITY""].unique())",0,"['len(df_final[""CITY""].unique())']"
"SETUP for card_id in most_likely_liers['card_id']: model_without_outliers.loc[model_without_outliers['card_id']==card_id,'target']\ = most_likely_liers.loc[most_likely_liers['card_id']==card_id,'target'].values",0,"['%%time\n', ""for card_id in most_likely_liers['card_id']:\n"", ""    model_without_outliers.loc[model_without_outliers['card_id']==card_id,'target']\\\n"", ""    = most_likely_liers.loc[most_likely_liers['card_id']==card_id,'target'].values""]"
"ASSIGN=plt.subplots(figsize=(25,15)) sns.boxplot(x=""Neighborhood"", y=""SalePrice"", data=train,ax=ax) ax.set_title(""Boxplot of Price for Neighborhood"",size=20)",1,"['fig,ax=plt.subplots(figsize=(25,15))\n', 'sns.boxplot(x=""Neighborhood"", y=""SalePrice"", data=train,ax=ax)\n', 'ax.set_title(""Boxplot of Price for Neighborhood"",size=20)']"
"ASSIGN = plt.subplots(1,3, figsize = (20,6)) sns.distplot(miss['LotFrontage'], color = 'b',ax = axes[0]) sns.distplot(miss['GarageYrBlt'], color = 'r', ax = axes[1]) sns.distplot(miss['MasVnrArea'], color = 'y',ax = axes[2])",1,"['# see distribution\n', 'fig,axes = plt.subplots(1,3, figsize = (20,6))\n', ""sns.distplot(miss['LotFrontage'], color = 'b',ax = axes[0])\n"", ""sns.distplot(miss['GarageYrBlt'], color = 'r', ax = axes[1])\n"", ""sns.distplot(miss['MasVnrArea'], color = 'y',ax = axes[2])""]"
"os.listdir(""..path"")",0,"['os.listdir(""../input/india-trade-data"")']"
"def check_NaN_Values_in_df(df): for col in df: ASSIGN = df[col].isnull().sum() if ASSIGN != 0: print (col + "" => "" + str(ASSIGN) + "" NaN Values"")",0,"['def check_NaN_Values_in_df(df):\n', '    for col in df:\n', '        nan_count = df[col].isnull().sum()\n', '        \n', '        if nan_count != 0:\n', '            print (col + "" => "" + str(nan_count) + "" NaN Values"")\n', '        ']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
"ASSIGN = tscovid.iloc[0:405, 4:54]",0,"['df = tscovid.iloc[0:405, 4:54]']"
"ASSIGN = Generator(False,block_config =block_config,num_encoded_channels = num_encoded_channels).to(device)",0,"['# netD = Discriminator().to(device).apply(weights_init)\n', 'netG = Generator(False,block_config =block_config,num_encoded_channels = num_encoded_channels).to(device)\n']"
"SETUP CHECKPOINT ASSIGN = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', 'Matthew', 'Laura'], 'score': [12.5, 9, 16.5, 6.0, 9, 20, np.nan, 5.5], 'attempts': [1, 3, 2, 3, 2, 3, 1, 1], 'qualify': ['yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no']} ASSIGN = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'] ASSIGN = pd.DataFrame(exam_data , index=labels) print() print(ASSIGN.iloc[[1, 3, 5, 6], [1, 3]])",0,"['import pandas as pd\n', 'import numpy as np\n', ""exam_data  = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', 'Matthew', 'Laura'],\n"", ""        'score': [12.5, 9, 16.5, 6.0, 9, 20, np.nan, 5.5],\n"", ""        'attempts': [1, 3, 2, 3, 2, 3, 1, 1],\n"", ""        'qualify': ['yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no']}\n"", ""labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n"", 'df = pd.DataFrame(exam_data , index=labels)\n', 'print(""Select specific columns and rows:"")\n', 'print(df.iloc[[1, 3, 5, 6], [1, 3]])']"
"sns.catplot(x='Year', y='Hurt', data=cbdr,height = 5, aspect = 4)",1,"[""sns.catplot(x='Year', y='Hurt', data=cbdr,height = 5, aspect = 4)""]"
SETUP,0,"['import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', 'from sklearn import metrics']"
"ASSIGN=pd.concat([ASSIGN,pd.get_dummies(ASSIGN['color'])],axis=1) ASSIGN.drop('color',axis=1,inplace=True) ASSIGN.head()",0,"[""test_data=pd.concat([test_data,pd.get_dummies(test_data['color'])],axis=1)\n"", ""test_data.drop('color',axis=1,inplace=True)\n"", 'test_data.head()']"
"CHECKPOINT def fast_auc(y_true, y_prob): """""" fast roc_auc computation: https:path """""" ASSIGN = np.asarray(ASSIGN) ASSIGN = ASSIGN[np.argsort(y_prob)] ASSIGN = 0 ASSIGN = 0 ASSIGN = len(y_true) for i in range(ASSIGN): ASSIGN = y_true[i] ASSIGN += (1 - ASSIGN) ASSIGN += ASSIGN * ASSIGN ASSIGN= (nfalse * (n - nfalse)) return auc def eval_auc(ASSIGN, y_pred): """""" Fast ASSIGN eval function for lgb. """""" return 'auc', fast_auc(y_true, y_pred), True def group_mean_log_mae(ASSIGN, y_pred, types, floor=1e-9): """""" Fast metric computation for this competition: https:path Code is from this kernel: https:path """""" ASSIGN = (y_true-y_pred).abs().groupby(types).mean() return np.log(ASSIGN.map(lambda x: max(x, floor))).mean() def train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='ASSIGN', columns=None, plot_feature_importance=False, model=None, ASSIGN=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1): """""" A function to train a variety of classification models. Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances. :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing) :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing) :params: y - target :params: folds - folds to split data :params: model_type - type of model to use :params: eval_metric - metric to use :params: columns - columns to use. If None - use all columns :params: plot_feature_importance - whether to plot feature importance of LGB :params: model - sklearn model, works only for ""sklearn"" model type """""" ASSIGN = X.ASSIGN if ASSIGN is None else ASSIGN ASSIGN = folds.ASSIGN if splits is None else n_folds ASSIGN = ASSIGN[columns] ASSIGN = {'auc': {'lgb_metric_name': eval_auc, 'catboost_metric_name': 'AUC', 'sklearn_scoring_function': metrics.roc_auc_score}, } ASSIGN = {} ASSIGN == 'usual': ASSIGN = np.zeros((len(X), 1)) ASSIGN = np.zeros((len(X_test), 1)) ASSIGN == 'rank': ASSIGN = np.zeros((len(X), 1)) ASSIGN = np.zeros((len(X_test), 1)) ASSIGN = [] ASSIGN = pd.DataFrame() for fold_n, (train_index, valid_index) in enumerate(folds.split(X)): print(f'Fold {fold_n + 1} started at {time.ctime()}') if type(X) == np.ndarray: X_train, X_valid = X[ASSIGN][train_index], X[ASSIGN][valid_index] ASSIGN = y[train_index], y[valid_index] else: X_train, X_valid = X[ASSIGN].iloc[train_index], X[ASSIGN].iloc[valid_index] ASSIGN = y.iloc[train_index], y.iloc[valid_index] ASSIGN == 'lgb': ASSIGN = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs) ASSIGN.fit(X_train, y_train, ASSIGN=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'], ASSIGN=ASSIGN, early_stopping_rounds=early_stopping_rounds) ASSIGN = model.predict_proba(X_valid)[:, 1] ASSIGN = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1] ASSIGN == 'xgb': ASSIGN = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns) ASSIGN = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns) ASSIGN = [(train_data, 'train'), (valid_data, 'valid_data')] ASSIGN = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params) ASSIGN = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit) ASSIGN = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit) ASSIGN == 'sklearn': ASSIGN = ASSIGN ASSIGN.fit(X_train, y_train) ASSIGN = model.predict(X_valid).reshape(-1,) ASSIGN = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid) print(f'Fold {fold_n}. {eval_metric}: {ASSIGN:.4f}.') print('') ASSIGN = model.predict_proba(X_test) ASSIGN == 'cat': ASSIGN = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params, ASSIGN=metrics_dict[eval_metric]['catboost_metric_name']) ASSIGN.fit(X_train, y_train, ASSIGN=(X_valid, y_valid), cat_features=[], use_best_model=True, ASSIGN=False) ASSIGN = model.predict(X_valid) ASSIGN = model.predict(X_test) ASSIGN == 'usual': ASSIGN[valid_index] = ASSIGN.reshape(-1, 1) ASSIGN.append(ASSIGN[eval_metric]['sklearn_scoring_function'](y_valid, ASSIGN)) ASSIGN += ASSIGN.reshape(-1, 1) ASSIGN == 'rank': ASSIGN[valid_index] = ASSIGN.reshape(-1, 1) ASSIGN.append(ASSIGN[eval_metric]['sklearn_scoring_function'](y_valid, ASSIGN)) ASSIGN += pd.Series(ASSIGN).rank().values.reshape(-1, 1) ASSIGN == 'lgb' and plot_feature_importance: ASSIGN = pd.DataFrame() ASSIGN[""feature""] = ASSIGN ASSIGN[""importance""] = ASSIGN.feature_importances_ ASSIGN[""fold""] = fold_n + 1 ASSIGN = pd.concat([ASSIGN, fold_importance], axis=0) ASSIGN= n_splits print('CV mean ASSIGN: {0:.4f}, std: {1:.4f}.'.format(np.mean(ASSIGN), np.std(ASSIGN))) ASSIGN['ASSIGN'] = ASSIGN ASSIGN['ASSIGN'] = ASSIGN ASSIGN['ASSIGN'] = ASSIGN ASSIGN == 'lgb': if plot_feature_importance: ASSIGN[""importance""] path= ASSIGN ASSIGN = feature_importance[[""feature"", ""importance""]].groupby(""feature"").mean().sort_values( ASSIGN=""importance"", ascending=False)[:50].index ASSIGN = feature_importance.loc[feature_importance.feature.isin(cols)] plt.figure(figsize=(16, 12)); sns.barplot(x=""importance"", y=""feature"", data=ASSIGN.sort_values(ASSIGN=""importance"", ascending=False)); plt.title('LGB Features (avg over folds)'); ASSIGN['ASSIGN'] = ASSIGN ASSIGN['top_columns'] = ASSIGN return result_dict",0,"['def fast_auc(y_true, y_prob):\n', '    """"""\n', '    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n', '    """"""\n', '    y_true = np.asarray(y_true)\n', '    y_true = y_true[np.argsort(y_prob)]\n', '    nfalse = 0\n', '    auc = 0\n', '    n = len(y_true)\n', '    for i in range(n):\n', '        y_i = y_true[i]\n', '        nfalse += (1 - y_i)\n', '        auc += y_i * nfalse\n', '    auc /= (nfalse * (n - nfalse))\n', '    return auc\n', '\n', '\n', 'def eval_auc(y_true, y_pred):\n', '    """"""\n', '    Fast auc eval function for lgb.\n', '    """"""\n', ""    return 'auc', fast_auc(y_true, y_pred), True\n"", '\n', '\n', 'def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n', '    """"""\n', '    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n', '    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n', '    """"""\n', '    maes = (y_true-y_pred).abs().groupby(types).mean()\n', '    return np.log(maes.map(lambda x: max(x, floor))).mean()\n', ""def train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n"", ""                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n"", '    """"""\n', '    A function to train a variety of classification models.\n', '    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n', '    \n', '    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n', '    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n', '    :params: y - target\n', '    :params: folds - folds to split data\n', '    :params: model_type - type of model to use\n', '    :params: eval_metric - metric to use\n', '    :params: columns - columns to use. If None - use all columns\n', '    :params: plot_feature_importance - whether to plot feature importance of LGB\n', '    :params: model - sklearn model, works only for ""sklearn"" model type\n', '    \n', '    """"""\n', '    columns = X.columns if columns is None else columns\n', '    n_splits = folds.n_splits if splits is None else n_folds\n', '    X_test = X_test[columns]\n', '    \n', '    # to set up scoring parameters\n', ""    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n"", ""                        'catboost_metric_name': 'AUC',\n"", ""                        'sklearn_scoring_function': metrics.roc_auc_score},\n"", '                    }\n', '    \n', '    result_dict = {}\n', ""    if averaging == 'usual':\n"", '        # out-of-fold predictions on train data\n', '        oof = np.zeros((len(X), 1))\n', '\n', '        # averaged predictions on train data\n', '        prediction = np.zeros((len(X_test), 1))\n', '        \n', ""    elif averaging == 'rank':\n"", '        # out-of-fold predictions on train data\n', '        oof = np.zeros((len(X), 1))\n', '\n', '        # averaged predictions on train data\n', '        prediction = np.zeros((len(X_test), 1))\n', '\n', '    \n', '    # list of scores on folds\n', '    scores = []\n', '    feature_importance = pd.DataFrame()\n', '    \n', '    # split and train on folds\n', '    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n', ""        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n"", '        if type(X) == np.ndarray:\n', '            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n', '            y_train, y_valid = y[train_index], y[valid_index]\n', '        else:\n', '            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n', '            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n', '            \n', ""        if model_type == 'lgb':\n"", '            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n', '            model.fit(X_train, y_train, \n', ""                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n"", '                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n', '            \n', '            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n', '            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n', '            \n', ""        if model_type == 'xgb':\n"", '            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n', '            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n', '\n', ""            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n"", '            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n', '            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n', '            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n', '        \n', ""        if model_type == 'sklearn':\n"", '            model = model\n', '            model.fit(X_train, y_train)\n', '            \n', '            y_pred_valid = model.predict(X_valid).reshape(-1,)\n', ""            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n"", ""            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n"", ""            print('')\n"", '            \n', '            y_pred = model.predict_proba(X_test)\n', '        \n', ""        if model_type == 'cat':\n"", ""            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n"", ""                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n"", '            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n', '\n', '            y_pred_valid = model.predict(X_valid)\n', '            y_pred = model.predict(X_test)\n', '        \n', ""        if averaging == 'usual':\n"", '            \n', '            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n', ""            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n"", '            \n', '            prediction += y_pred.reshape(-1, 1)\n', '\n', ""        elif averaging == 'rank':\n"", '                                  \n', '            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n', ""            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n"", '                                  \n', '            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n', '        \n', ""        if model_type == 'lgb' and plot_feature_importance:\n"", '            # feature importance\n', '            fold_importance = pd.DataFrame()\n', '            fold_importance[""feature""] = columns\n', '            fold_importance[""importance""] = model.feature_importances_\n', '            fold_importance[""fold""] = fold_n + 1\n', '            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n', '\n', '    prediction /= n_splits\n', '    \n', ""    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n"", '    \n', ""    result_dict['oof'] = oof\n"", ""    result_dict['prediction'] = prediction\n"", ""    result_dict['scores'] = scores\n"", '    \n', ""    if model_type == 'lgb':\n"", '        if plot_feature_importance:\n', '            feature_importance[""importance""] /= n_splits\n', '            cols = feature_importance[[""feature"", ""importance""]].groupby(""feature"").mean().sort_values(\n', '                by=""importance"", ascending=False)[:50].index\n', '\n', '            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n', '\n', '            plt.figure(figsize=(16, 12));\n', '            sns.barplot(x=""importance"", y=""feature"", data=best_features.sort_values(by=""importance"", ascending=False));\n', ""            plt.title('LGB Features (avg over folds)');\n"", '            \n', ""            result_dict['feature_importance'] = feature_importance\n"", ""            result_dict['top_columns'] = cols\n"", '        \n', '    return result_dict']"
"plt.bar(strike_x,strike_y,color='orange') plt.title(""Parts Striked in the Aircraft"") plt.xticks(rotation='vertical')",1,"[""plt.bar(strike_x,strike_y,color='orange')\n"", 'plt.title(""Parts Striked in the Aircraft"")\n', ""plt.xticks(rotation='vertical')""]"
"pd_data['Location'] = pd_data['Location'].map( {'Darwin':0,'Perth':1,'Brisbane':2,'MelbourneAirport':3, 'PerthAirport':4,'SydneyAirport':5,'Watsonia':6,'Mildura':7, 'MountGambier':8,'NorfolkIsland':9,'Cairns':10,'Townsville':11, 'WaggaWagga':12,'AliceSprings':13,'Nuriootpa':14,'Hobart':15, 'Moree':16,'Melbourne':17,'Portland':18,'Woomera':19, 'Sydney':20,'Sale':21,'CoffsHarbour':22,'Williamtown':23, 'Canberra':24,'Cobar':25} ).astype(int) ASSIGN=pd_data['RainTomorrow'] ASSIGN=pd_data.drop(['RainTomorrow'], axis=1)",0,"[""pd_data['Location'] = pd_data['Location'].map( {'Darwin':0,'Perth':1,'Brisbane':2,'MelbourneAirport':3,\n"", ""                                                'PerthAirport':4,'SydneyAirport':5,'Watsonia':6,'Mildura':7,\n"", ""                                                'MountGambier':8,'NorfolkIsland':9,'Cairns':10,'Townsville':11,\n"", ""                                                'WaggaWagga':12,'AliceSprings':13,'Nuriootpa':14,'Hobart':15,\n"", ""                                                'Moree':16,'Melbourne':17,'Portland':18,'Woomera':19,\n"", ""                                                'Sydney':20,'Sale':21,'CoffsHarbour':22,'Williamtown':23,\n"", ""                                                'Canberra':24,'Cobar':25} ).astype(int)\n"", ""train_y=pd_data['RainTomorrow']\n"", ""train_x=pd_data.drop(['RainTomorrow'], axis=1)""]"
"ASSIGN = SVM_all.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('PolynomialSVM(HT).csv',index = False)",0,"['model13pred = SVM_all.predict(y_train)\n', ""submission13 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission13['PassengerId'] = result['PassengerId']\n"", ""submission13['Survived'] = model13pred\n"", ""submission13.to_csv('PolynomialSVM(HT).csv',index = False)""]"
"KNN.fit(x_train,x_test) KNN.score(y_train,y_test)",0,"['KNN.fit(x_train,x_test)\n', 'KNN.score(y_train,y_test)']"
SETUP,0,"['import numpy as np\n', 'import pandas as pd\n', 'import seaborn as sb\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', 'from sklearn.linear_model import LogisticRegression\n', 'from sklearn.linear_model import LinearRegression\n', 'from sklearn.model_selection import train_test_split\n', 'from sklearn.metrics import classification_report\n', 'from sklearn.metrics import confusion_matrix\n', 'from sklearn.metrics import r2_score']"
"plt.figure(figsize = [12,5]) sns.distplot(data[data['Clicked']==0]['Time Spent on Site'], label = 'Clicked==0') sns.distplot(data[data['Clicked']==1]['Time Spent on Site'], label = 'Clicked==1') plt.legend() plt.show()",1,"['plt.figure(figsize = [12,5])\n', ""sns.distplot(data[data['Clicked']==0]['Time Spent on Site'], label = 'Clicked==0')\n"", ""sns.distplot(data[data['Clicked']==1]['Time Spent on Site'], label = 'Clicked==1')\n"", 'plt.legend()\n', 'plt.show()']"
"ASSIGN=data_manha[data_manha.price<65] ASSIGN['label']=ASSIGN.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1) ASSIGN.head()",0,"['data_manha_65=data_manha[data_manha.price<65]\n', ""data_manha_65['label']=data_manha_65.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n"", 'data_manha_65.head()']"
holdout.head(),0,['holdout.head()']
"CHECKPOINT SETUP ASSIGN=np.array([1,2,0,True,False],dtype=np.bool) arr1",0,"['#Creation of Boolean Array\n', 'import numpy as np\n', 'arr1=np.array([1,2,0,True,False],dtype=np.bool)\n', 'arr1']"
pd.DataFrame(X_test).head(),0,['pd.DataFrame(X_test).head()']
ASSIGN=data[data.Risk=='Risk 3 (Low)'] ASSIGN.head(),0,"[""data_risk3=data[data.Risk=='Risk 3 (Low)']\n"", 'data_risk3.head()']"
"def get_data(): ASSIGN = os.path.join(args.intermediate_path, 'raw_data.pkl') ASSIGN = os.path.join(args.intermediate_path, 'scaled_data.pkl') ASSIGN = os.path.join(args.intermediate_path, 'scaler.pkl') if not args.read_from_file: ASSIGN = pd.read_csv(os.path.join(args.data_path, args.train_file), ASSIGN='Page') ASSIGN = data_df.values.copy() ASSIGN = ASSIGN.fillna(method='ffill', axis=1).fillna( ASSIGN='bfill', axis=1) ASSIGN = np.nan_to_num(data_df.values.astype('float32')) ASSIGN = np.log1p(ASSIGN) ASSIGN = StandardScaler() ASSIGN.fit(np.swapaxes(ASSIGN, 0, 1)) ASSIGN = scaler.transform(np.swapaxes(data, 0, 1)) ASSIGN = np.swapaxes(ASSIGN, 0, 1) with open(ASSIGN, 'wb') as f: pickle.dump(ASSIGN, f) with open(ASSIGN, 'wb') as f: pickle.dump(ASSIGN, f) with open(ASSIGN, 'wb') as f: pickle.dump(ASSIGN, f) else: with open(ASSIGN, 'rb') as f: ASSIGN = pickle.load(f) with open(ASSIGN, 'rb') as f: ASSIGN = pickle.load(f) with open(ASSIGN, 'rb') as f: ASSIGN = pickle.load(f) return raw_data, scaled_data, scaler",0,"['def get_data():\n', ""    raw_data_file = os.path.join(args.intermediate_path, 'raw_data.pkl')\n"", '    scaled_data_file = os.path.join(args.intermediate_path,\n', ""                                    'scaled_data.pkl')\n"", ""    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n"", '    \n', '    if not args.read_from_file:\n', '        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n', ""                              index_col='Page')\n"", '        raw_data = data_df.values.copy()\n', ""        data_df = data_df.fillna(method='ffill', axis=1).fillna(\n"", ""            method='bfill', axis=1)\n"", ""        data = np.nan_to_num(data_df.values.astype('float32'))\n"", '        data = np.log1p(data)\n', '        scaler = StandardScaler()\n', '        scaler.fit(np.swapaxes(data, 0, 1))\n', '        scaled_data = scaler.transform(np.swapaxes(data, 0, 1))\n', '        scaled_data = np.swapaxes(scaled_data, 0, 1)\n', '        \n', ""        with open(raw_data_file, 'wb') as f:\n"", '            pickle.dump(raw_data, f)\n', ""        with open(scaled_data_file, 'wb') as f:\n"", '            pickle.dump(scaled_data, f)\n', ""        with open(scaler_file, 'wb') as f:\n"", '            pickle.dump(scaler, f)\n', '    else:\n', ""        with open(raw_data_file, 'rb') as f:\n"", '            raw_data = pickle.load(f)\n', ""        with open(scaled_data_file, 'rb') as f:\n"", '            scaled_data = pickle.load(f)\n', ""        with open(scaler_file, 'rb') as f:\n"", '            scaler = pickle.load(f)\n', '    return raw_data, scaled_data, scaler']"
CHECKPOINT df.dtypes,0,['df.dtypes']
ASSIGN = [] ASSIGN = [] ASSIGN = [],0,"['train_x_img_paths = []\n', 'train_y_img_paths = []\n', 'test_img_paths = []']"
"CHECKPOINT print(metrics.accuracy_score(y_test, predict_test))",0,"['print(metrics.accuracy_score(y_test, predict_test))']"
data.describe(),0,['data.describe()']
CHECKPOINT print(x_train.shape),0,['print(x_train.shape)']
my_submission['target'] = final_preds,0,"[""my_submission['target'] = final_preds""]"
"os.listdir(""..path"")",0,"['os.listdir(""../input/autotel-shared-car-locations"")']"
"SETUP ASSIGN=folium.map.FeatureGroup() ASSIGN=42.3 ASSIGN=-71.1 ASSIGN=data[data['YEAR']==2015][0:2000] ASSIGN=""Crime2015"" ASSIGN=folium.Map([Lat,Lon],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(boston_map) for lat,lon,label in zip(ASSIGN.ASSIGN,ASSIGN.Long,ASSIGN.OFFENSE_CODE_GROUP): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN)",1,"['import geopandas as gpd\n', 'import folium\n', '\n', '\n', 'incidents=folium.map.FeatureGroup()\n', '\n', '#for lat,lon, in zip(data.Lat,data.Long):\n', ""#\tincidents.add_child(folium.CircleMarker([lat,lon],radius=7,color='yellow',fill=True,fill_color='red',fill_opacity=0.4))\n"", '\n', 'Lat=42.3\n', 'Lon=-71.1\n', '#boston_map=folium.Map([Lat,Lon],zoom_start=12)\n', '#boston_map.add_child(incidents)\n', '#boston_map.save(""mymap.html"")\n', '\n', 'from folium import plugins\n', '\n', ""data1=data[data['YEAR']==2015][0:2000]\n"", 'filename=""Crime2015""\n', 'boston_map=folium.Map([Lat,Lon],zoom_start=12)\n', 'incidents2=plugins.MarkerCluster().add_to(boston_map)\n', 'for lat,lon,label in zip(data1.Lat,data1.Long,data1.OFFENSE_CODE_GROUP):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(incidents2)\n', 'boston_map.add_child(incidents2)\n']"
CHECKPOINT df_final,0,"['#displaying the dataframe\n', 'df_final']"
"CHECKPOINT ASSIGN = LogisticRegression() ASSIGN.fit(X_train, y_train) ASSIGN = logreg.predict(X_test) print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(ASSIGN.score(X_test, y_test))) print(confusion_matrix(y_test, ASSIGN)) print(classification_report(y_test, ASSIGN))",0,"['# Logistic regression Model\n', '\n', 'logreg = LogisticRegression()\n', 'logreg.fit(X_train, y_train)\n', '\n', '#  Model metrics\n', 'y_pred = logreg.predict(X_test)\n', ""print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n"", 'print(confusion_matrix(y_test, y_pred))\n', 'print(classification_report(y_test, y_pred))']"
"ASSIGN=plt.subplots(2,2,figsize=(25,25)) sns.barplot(x=irishtimes.year.value_counts().index,y=irishtimes.year.value_counts(),ax=ax[0,0]) ax[0,0].set_title(""Bar chart for year"",size=30) ax[0,0].set_xlabel('year',size=20) ax[0,0].set_ylabel('counts',size=20) sns.barplot(x=irishtimes.month.value_counts().index,y=irishtimes.month.value_counts(),ax=ax[0,1]) ax[0,1].set_title(""Bar chart for month"",size=30) ax[0,1].set_xlabel('month',size=20) ax[0,1].set_ylabel('counts',size=20) sns.barplot(x=irishtimes.day.value_counts().index,y=irishtimes.day.value_counts(),ax=ax[1,0]) ax[1,0].set_title(""Bar chart for day"",size=30) ax[1,0].set_xlabel('day',size=20) ax[1,0].set_ylabel('counts',size=20) irishtimes.groupby(['date'])['headline_category'].agg('count').plot(ax=ax[1,1]) ax[1,1].set_title(""Number of news for date"",size=30) ax[1,1].set_xlabel('date',size=20) ax[1,1].set_ylabel('counts',size=20)",1,"['fig,ax=plt.subplots(2,2,figsize=(25,25))\n', 'sns.barplot(x=irishtimes.year.value_counts().index,y=irishtimes.year.value_counts(),ax=ax[0,0])\n', 'ax[0,0].set_title(""Bar chart for year"",size=30)\n', ""ax[0,0].set_xlabel('year',size=20)\n"", ""ax[0,0].set_ylabel('counts',size=20)\n"", '\n', 'sns.barplot(x=irishtimes.month.value_counts().index,y=irishtimes.month.value_counts(),ax=ax[0,1])\n', 'ax[0,1].set_title(""Bar chart for month"",size=30)\n', ""ax[0,1].set_xlabel('month',size=20)\n"", ""ax[0,1].set_ylabel('counts',size=20)\n"", '\n', 'sns.barplot(x=irishtimes.day.value_counts().index,y=irishtimes.day.value_counts(),ax=ax[1,0])\n', 'ax[1,0].set_title(""Bar chart for day"",size=30)\n', ""ax[1,0].set_xlabel('day',size=20)\n"", ""ax[1,0].set_ylabel('counts',size=20)\n"", '\n', ""irishtimes.groupby(['date'])['headline_category'].agg('count').plot(ax=ax[1,1])\n"", 'ax[1,1].set_title(""Number of news for date"",size=30)\n', ""ax[1,1].set_xlabel('date',size=20)\n"", ""ax[1,1].set_ylabel('counts',size=20)""]"
"plt.figure(figsize=(14,9)) sns.heatmap(df.isnull(), cbar=False, yticklabels=False)",1,"['# o número de células com valores NaN é muito grande. Exposição gráfica para melhor visualização\n', 'plt.figure(figsize=(14,9))\n', 'sns.heatmap(df.isnull(), cbar=False, yticklabels=False)']"
"image.numpy().shape, lossy_image.numpy().shape",0,"['image.numpy().shape, lossy_image.numpy().shape']"
"ASSIGN=pd.concat([ASSIGN,pd.get_dummies(ASSIGN['color'])],axis=1) ASSIGN.drop('color',axis=1,inplace=True) ASSIGN.head()",0,"['#one-hot-encoding categorrical attribute:color\n', ""train_data=pd.concat([train_data,pd.get_dummies(train_data['color'])],axis=1)\n"", ""train_data.drop('color',axis=1,inplace=True)\n"", 'train_data.head()']"
"CHECKPOINT ASSIGN = go.Figure(data=[ go.Line(name='Confirmed', x=formated_gdf[formated_gdf['Countrypath']=='US']['date'], y=formated_gdf[formated_gdf['Countrypath']=='US']['Confirmed']), go.Line(name='Deaths', x=formated_gdf[formated_gdf['Countrypath']=='US']['date'], y=formated_gdf[formated_gdf['Countrypath']=='US']['Deaths']), go.Line(name='Recovered', x=formated_gdf[formated_gdf['Countrypath']=='US']['date'], y=formated_gdf[formated_gdf['Countrypath']=='US']['Recovered']), ]) ASSIGN.update_layout( ASSIGN=""Number of Confirmed,Recovered,death in US for each day"", ASSIGN=""date"", ASSIGN=""People"", ASSIGN=dict( ASSIGN=""Courier New, monospace"", ASSIGN=18, ASSIGN=""#7f7f7f"" )) fig",1,"['fig = go.Figure(data=[\n', ""    go.Line(name='Confirmed', x=formated_gdf[formated_gdf['Country/Region']=='US']['date'], y=formated_gdf[formated_gdf['Country/Region']=='US']['Confirmed']),\n"", ""    go.Line(name='Deaths', x=formated_gdf[formated_gdf['Country/Region']=='US']['date'], y=formated_gdf[formated_gdf['Country/Region']=='US']['Deaths']),\n"", ""    go.Line(name='Recovered', x=formated_gdf[formated_gdf['Country/Region']=='US']['date'], y=formated_gdf[formated_gdf['Country/Region']=='US']['Recovered']),\n"", '])\n', '\n', 'fig.update_layout(\n', '    title=""Number of Confirmed,Recovered,death in US for each day"",\n', '    xaxis_title=""date"",\n', '    yaxis_title=""People"",\n', '    font=dict(\n', '        family=""Courier New, monospace"",\n', '        size=18,\n', '        color=""#7f7f7f""\n', '    ))\n', 'fig']"
SETUP ASSIGN = PCA(n_components=2) ASSIGN.fit(scaled_data),0,"['from sklearn.decomposition import PCA\n', 'pca = PCA(n_components=2)\n', 'pca.fit(scaled_data)']"
"plt.figure(figsize=(15,8)) sns.heatmap(corr, annot=True) plt.savefig(""image0.png"")",1,"['plt.figure(figsize=(15,8))\n', 'sns.heatmap(corr, annot=True)\n', 'plt.savefig(""image0.png"")']"
data['Country'].value_counts(),0,"['#Count by country\n', ""data['Country'].value_counts()""]"
menu.info(),0,['menu.info()']
"SETUP ASSIGN = ImageDataGenerator( horizontal_flip=True, ASSIGN=True, ASSIGN=10, ASSIGN=0.1, ASSIGN=0.1, ASSIGN=.1, ASSIGN='nearest', ASSIGN=0.1, ASSIGN=1path, ASSIGN=[0.5, 1.5])",0,"['from keras.preprocessing.image import ImageDataGenerator\n', 'train_datagen = ImageDataGenerator( horizontal_flip=True,\n', '    vertical_flip=True,\n', '    rotation_range=10,\n', '    width_shift_range=0.1,\n', '    height_shift_range=0.1,\n', '    zoom_range=.1,\n', ""    fill_mode='nearest',\n"", '    shear_range=0.1,\n', '    rescale=1/255,\n', '    brightness_range=[0.5, 1.5])\n']"
CHECKPOINT print(X_train.shape) print(y_train.shape) print(X_val.shape) print(y_val.shape) print(X_test.shape) print(y_test.shape),0,"['###Seeing dimensions of the different sets\n', 'print(X_train.shape)\n', 'print(y_train.shape)\n', 'print(X_val.shape)\n', 'print(y_val.shape)\n', 'print(X_test.shape)\n', 'print(y_test.shape)']"
"ASSIGN = WordCloud(background_color=""black"",max_words=200,max_font_size=40,random_state=10).generate(str(headline_text_new)) plt.figure(figsize=(20,15)) plt.imshow(ASSIGN, interpolation='bilinear') plt.axis(""off"") plt.show()",1,"['\n', 'wordcloud = WordCloud(background_color=""black"",max_words=200,max_font_size=40,random_state=10).generate(str(headline_text_new))\n', '\n', 'plt.figure(figsize=(20,15))\n', ""plt.imshow(wordcloud, interpolation='bilinear')\n"", 'plt.axis(""off"")\n', 'plt.show()']"
ASSIGN = pd.read_csv('..path'),0,"[""test_df = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WSampleSubmissionStage1_2020.csv')""]"
"ASSIGN=data_Brooklyn.loc[(data_Brooklyn['price'] >=10) & (data_Brooklyn['price'] <65)][:2000] ASSIGN['label']=ASSIGN.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1) ASSIGN.head()",0,"[""data_Brooklyn_10_65=data_Brooklyn.loc[(data_Brooklyn['price'] >=10) & (data_Brooklyn['price'] <65)][:2000]\n"", ""data_Brooklyn_10_65['label']=data_Brooklyn_10_65.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n"", 'data_Brooklyn_10_65.head()']"
df.isnull().sum(),0,['df.isnull().sum()']
del df_hist_trans;gc.collect() df_train.head(5),0,"['del df_hist_trans;gc.collect()\n', '#del df_new_merchant_trans;gc.collect()\n', 'df_train.head(5)']"
len(train_transaction),0,['len(train_transaction)']
final_df.std(),0,['final_df.std()']
"SETUP CHECKPOINT warnings.filterwarnings(""ignore"") for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename)) os.environ['OMP_NUM_THREADS'] = '8'",0,"['%matplotlib inline\n', 'import pandas as pd\n', 'import networkx as nx \n', 'import matplotlib.pyplot as plt\n', 'from tqdm import tqdm\n', 'from IPython.display import Image\n', 'import warnings\n', 'warnings.filterwarnings(""ignore"") \n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', ""os.environ['OMP_NUM_THREADS'] = '8' ""]"
"CHECKPOINT ASSIGN = Sequential() ASSIGN.add(VGG16(include_top=False, input_shape=(100, 100, 3))) for layer in ASSIGN.layers: layer.trainable = False ASSIGN.add(Flatten()) ASSIGN.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) ASSIGN.add(Dropout(0.5)) ASSIGN.add(Dense(1, activation='sigmoid')) print(ASSIGN.summary()) input_and_run3(ASSIGN,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.001,num_epochs=200)",0,"['model5 = Sequential()\n', 'model5.add(VGG16(include_top=False, input_shape=(100, 100, 3)))\n', '# mark loaded layers as not trainable\n', 'for layer in model5.layers:\n', '    layer.trainable = False\n', '# add new classifier layers\n', '#flat1 = Flatten()(model.layers[-1].output)\n', ""#class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n"", ""#output = Dense(1, activation='sigmoid')(class1)\n"", '\n', 'model5.add(Flatten())\n', ""model5.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n"", 'model5.add(Dropout(0.5))\n', ""model5.add(Dense(1, activation='sigmoid'))\n"", 'print(model5.summary())\n', 'input_and_run3(model5,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.001,num_epochs=200)\n', '\n', '\n', '# define new model\n', '#model = Model(inputs=model.inputs, outputs=output)\n', '# compile model\n', '#opt = SGD(lr=0.001, momentum=0.9)\n', ""#model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n"", '#return model']"
SETUP np.random.seed(0),0,"[""# modules we'll use\n"", 'import pandas as pd\n', 'import numpy as np\n', '\n', '# helpful modules\n', 'import fuzzywuzzy\n', 'from fuzzywuzzy import process\n', 'import chardet\n', '\n', '# set seed for reproducibility\n', 'np.random.seed(0)']"
learn.lr_find() learn.recorder.plot(suggestion=True),1,"['learn.lr_find()\n', 'learn.recorder.plot(suggestion=True)']"
SETUP,0,"['from PIL import Image\n', 'import matplotlib.pyplot as plt']"
"CHECKPOINT model6.save(""model6.h5"") print()",0,"['model6.save(""model6.h5"")\n', 'print(""Saved model to disk"")']"
"for df in [df_hist_trans,df_new_merchant_trans]: df['category_2'].fillna(1.0,inplace=True) df['category_3'].fillna('A',inplace=True) df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)",0,"['for df in [df_hist_trans,df_new_merchant_trans]:\n', ""    df['category_2'].fillna(1.0,inplace=True)\n"", ""    df['category_3'].fillna('A',inplace=True)\n"", ""    df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)""]"
"CHECKPOINT ASSIGN = pd.read_csv(""..path"") ASSIGN= pd.read_csv(""..path"") print('ASSIGN datasets shape: ',ASSIGN.shape) print('ASSIGN datasets shape: ',ASSIGN.shape) ASSIGN.head()",0,"['# create the training & test sets, skipping the header row with [1:]\n', 'train = pd.read_csv(""../input/train.csv"")\n', 'test= pd.read_csv(""../input/test.csv"")\n', ""print('train datasets shape: ',train.shape)\n"", ""print('test datasets shape: ',test.shape)\n"", 'train.head()']"
train.head(10),0,['train.head(10)']
"ASSIGN=plt.subplots(1,2,figsize=(15,8)) ASSIGN = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",'cadetblue','hotpink','orange','darksalmon','brown') train.groupby(['OverallCond'])['Id'].agg('count').plot(kind='bar',color=ASSIGN,ax=ax[0]) ax[0].set_title(""Top 10 OverallCond by counts"",size=20) ax[0].set_xlabel('counts',size=18) ASSIGN=train.groupby(['OverallCond'])['Id'].agg('ASSIGN') ASSIGN=list(train.groupby(['OverallCond'])['Id'].agg('count').index) ASSIGN=list(count) ASSIGN=pd.DataFrame({""group"":groups,""counts"":counts}) ASSIGN=(""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",'cadetblue','hotpink','orange','darksalmon','brown') ASSIGN = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1]) plt.legend(loc=0, bbox_to_anchor=(1.20,0.8)) plt.subplots_adjust(wspace =0.5, hspace =0) plt.ylabel('')",1,"['fig,ax=plt.subplots(1,2,figsize=(15,8))\n', 'clr = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",\'cadetblue\',\'hotpink\',\'orange\',\'darksalmon\',\'brown\')\n', ""train.groupby(['OverallCond'])['Id'].agg('count').plot(kind='bar',color=clr,ax=ax[0])\n"", 'ax[0].set_title(""Top 10 OverallCond by counts"",size=20)\n', ""ax[0].set_xlabel('counts',size=18)\n"", '\n', '\n', ""count=train.groupby(['OverallCond'])['Id'].agg('count')\n"", ""groups=list(train.groupby(['OverallCond'])['Id'].agg('count').index)\n"", 'counts=list(count)\n', 'type_dict=pd.DataFrame({""group"":groups,""counts"":counts})\n', 'clr1=(""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",\'cadetblue\',\'hotpink\',\'orange\',\'darksalmon\',\'brown\')\n', ""qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n"", 'plt.legend(loc=0, bbox_to_anchor=(1.20,0.8)) \n', 'plt.subplots_adjust(wspace =0.5, hspace =0)\n', ""plt.ylabel('')""]"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
"ASSIGN = ASSIGN.drop('Name',axis = 1) ASSIGN = ASSIGN.drop('Sex',axis = 1) ASSIGN = ASSIGN.drop('Ticket',axis = 1) ASSIGN = ASSIGN.drop('Cabin',axis = 1) ASSIGN = ASSIGN.drop('Name',axis = 1) ASSIGN = ASSIGN.drop('Sex',axis = 1) ASSIGN = ASSIGN.drop('Ticket',axis = 1) ASSIGN = ASSIGN.drop('Cabin',axis = 1)",0,"[""train = train.drop('Name',axis = 1)\n"", ""train = train.drop('Sex',axis = 1)\n"", ""train = train.drop('Ticket',axis = 1)\n"", ""train = train.drop('Cabin',axis = 1)\n"", '\n', '\n', ""test = test.drop('Name',axis = 1)\n"", ""test = test.drop('Sex',axis = 1)\n"", ""test = test.drop('Ticket',axis = 1)\n"", ""test = test.drop('Cabin',axis = 1)""]"
ASSIGN = val_counts_image_id['bbox_count']==1 ASSIGN = val_counts_image_id['bbox_count']==2 ASSIGN = val_counts_image_id['bbox_count']==3,0,"[""cond_1 = val_counts_image_id['bbox_count']==1\n"", ""cond_2 = val_counts_image_id['bbox_count']==2\n"", ""cond_3 = val_counts_image_id['bbox_count']==3""]"
"SETUP CHECKPOINT ASSIGN=KFold(n_splits=10, shuffle=True, random_state=False) ASSIGN = DecisionTreeClassifier() ASSIGN = [] for train_id, test_id in ASSIGN.split(data_matf,data_matl): X_train, X_test = data_matf.values[train_id], data_matf.values[test_id] ASSIGN = data_matl.values[train_id], data_matl.values[test_id] ASSIGN.fit(X_train,y_train) ASSIGN = dtree.predict(X_test) ASSIGN = accuracy_score(y_test, predictions) ASSIGN.append(ASSIGN) plt.plot(range(10),ASSIGN) plt.show() ASSIGN = np.mean(outcomesDt) print(,ASSIGN)",1,"['from sklearn.model_selection import KFold\n', 'from sklearn.metrics import accuracy_score\n', 'from sklearn.tree import DecisionTreeClassifier\n', '\n', 'kf=KFold(n_splits=10, shuffle=True, random_state=False)\n', 'dtree = DecisionTreeClassifier()\n', '\n', 'outcomesDt = []\n', 'for train_id, test_id in kf.split(data_matf,data_matl):\n', '    X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]\n', '    y_train, y_test = data_matl.values[train_id], data_matl.values[test_id]\n', '    dtree.fit(X_train,y_train)\n', '    predictions = dtree.predict(X_test)\n', '    accuracy = accuracy_score(y_test, predictions)\n', '    outcomesDt.append(accuracy)\n', 'plt.plot(range(10),outcomesDt)\n', 'plt.show()\n', 'average_error_Dt = np.mean(outcomesDt)\n', 'print(""the average error is equal to "",average_error_Dt)']"
data_import.isnull().sum(),0,['data_import.isnull().sum()']
CHECKPOINT print(X_train.shape) print(Y_train.shape) print(X_val.shape) print(Y_val.shape) print(X_test.shape) print(Y_test.shape),0,"['print(X_train.shape)\n', 'print(Y_train.shape)\n', 'print(X_val.shape)\n', 'print(Y_val.shape)\n', 'print(X_test.shape)\n', 'print(Y_test.shape)']"
"CHECKPOINT def plot_img(img): plt.imshow(img, cmap='gray') plt.axis(""off"") plt.show() def plot_img_by_id(id, species = ''): ASSIGN = '.path' + str(id) + '.jpg' ASSIGN = imread(src) plt.imshow(ASSIGN, cmap='gray') plt.suptitle('Predicted species: ' + species) plt.axis(""off"") plt.show() def plot_img_by_species(species): ASSIGN = '.path' + str(species) + 'path' plt.figure(figsize=(28,28)) ASSIGN = len(os.listdir(ldir)), 1 ASSIGN = 1 print(species) for d in os.listdir(ASSIGN): ASSIGN = ldir + d ASSIGN = imread(src) plt.subplot(y, x, ASSIGN) plt.imshow(ASSIGN, cmap='gray') plt.axis(""off"") ASSIGN += 1 plt.show()",1,"['def plot_img(img):\n', ""    plt.imshow(img, cmap='gray')\n"", '    plt.axis(""off"")\n', '    plt.show()\n', '    \n', ""def plot_img_by_id(id, species = ''):\n"", ""    src = './LeafClassification/' + str(id) + '.jpg'\n"", '    img = imread(src)\n', ""    plt.imshow(img, cmap='gray')\n"", ""    plt.suptitle('Predicted species: ' + species)\n"", '    plt.axis(""off"")\n', '    plt.show()\n', '    \n', 'def plot_img_by_species(species):\n', ""    ldir = './training_data/' + str(species) + '/'\n"", '    plt.figure(figsize=(28,28))\n', ""    #plt.suptitle('Predicted species: ' + species)\n"", '    x, y = len(os.listdir(ldir)), 1\n', '     \n', '    i = 1\n', '    print(species)\n', '    for d in os.listdir(ldir):\n', '        src = ldir + d\n', '        img = imread(src)\n', '        \n', '        plt.subplot(y, x, i)\n', ""        plt.imshow(img, cmap='gray')\n"", '        plt.axis(""off"")\n', '        i += 1\n', '            \n', '    plt.show()']"
ASSIGN=pd.read_csv('path'),0,"[""test_data=pd.read_csv('/kaggle/input/ghouls-goblins-and-ghosts-boo/test.csv.zip')""]"
"test['income class'] = ' - 50000.' test[""hispanic origin""] = test[""hispanic origin""].fillna('NA') test[""state of previous residence""] = test[""state of previous residence""].fillna('?') test['migration code-change in msa'] = test['migration code-change in msa'].fillna('?') test['migration code-change in reg'] = test['migration code-change in reg'].fillna('?') test['migration code-move within reg'] = test['migration code-move within reg'].fillna('?') test['migration prev res in sunbelt'] = test['migration prev res in sunbelt'].fillna('?') test['country of birth father'] = test['country of birth father'].fillna('?') test['country of birth mother'] = test['country of birth mother'].fillna('?') test['country of birth self'] = test['country of birth self'].fillna('?')",0,"['# align test target\n', ""test['income class'] = ' - 50000.'\n"", '# NaN corrections in test dataset\n', 'test[""hispanic origin""] = test[""hispanic origin""].fillna(\'NA\')\n', 'test[""state of previous residence""] = test[""state of previous residence""].fillna(\'?\')\n', ""test['migration code-change in msa'] = test['migration code-change in msa'].fillna('?')\n"", ""test['migration code-change in reg'] = test['migration code-change in reg'].fillna('?')\n"", ""test['migration code-move within reg'] = test['migration code-move within reg'].fillna('?')\n"", ""test['migration prev res in sunbelt'] = test['migration prev res in sunbelt'].fillna('?')\n"", ""test['country of birth father'] = test['country of birth father'].fillna('?')\n"", ""test['country of birth mother'] = test['country of birth mother'].fillna('?')\n"", ""test['country of birth self'] = test['country of birth self'].fillna('?')""]"
"data_features['MSSubClass'].groupby((data_features['GarageCond'],data_features['GarageQual'])).count()",0,"[""data_features['MSSubClass'].groupby((data_features['GarageCond'],data_features['GarageQual'])).count()\n"", '#These two variables have positive correlation.So we can use the mode to fill the missing in GarageCond and GarageQual. ']"
"ASSIGN = deaths_dict['Mainland China'] ASSIGN = deaths_dict['Italy'] ASSIGN = 0 for key in deaths_dict: if key != 'Mainland China' and key != 'Italy': ASSIGN+=deaths_dict[key] ASSIGN = ['China', 'Italy', 'Others'] ASSIGN = [china_d_number, italy_d_number, others_d] ASSIGN = ['Red', 'Green', 'Grey'] ASSIGN = (0.1, 0, 0) ASSIGN = ['Count'] ASSIGN = ['China', 'Italy', 'Others'] ASSIGN = [[china_d_number], [italy_d_number], [others_d]] ASSIGN = plt.subplots(1,2, figsize = (8,8)) axs[0].axis('tight') axs[0].axis('off') ASSIGN = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center') ASSIGN.set_fontsize(14) ASSIGN.scale(1.5, 1.5) axs[1].pie(ASSIGN, labels = ASSIGN, ASSIGN = ASSIGN, colors=ASSIGN, shadow=True, autopct='%1.1f%%') plt.title('Global Proportions of 2 severely striken countries') plt.show()",1,"[""china_d_number = deaths_dict['Mainland China']\n"", ""italy_d_number = deaths_dict['Italy']\n"", 'others_d = 0\n', 'for key in deaths_dict:\n', ""    if key != 'Mainland China' and key != 'Italy':\n"", '        others_d+=deaths_dict[key]\n', '        \n', ""groups = ['China', 'Italy', 'Others']\n"", 'sizes = [china_d_number, italy_d_number, others_d]\n', ""colours = ['Red', 'Green', 'Grey']\n"", 'explode = (0.1, 0, 0)\n', ""col_labels = ['Count']\n"", ""row_labels = ['China', 'Italy', 'Others']\n"", 'table_values = [[china_d_number], [italy_d_number], [others_d]]\n', '\n', '\n', 'fig, axs = plt.subplots(1,2, figsize = (8,8))\n', ""axs[0].axis('tight')\n"", ""axs[0].axis('off')\n"", ""the_table = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')\n"", 'the_table.set_fontsize(14)\n', 'the_table.scale(1.5, 1.5)\n', ""axs[1].pie(sizes, labels = groups, explode = explode, colors=colours, shadow=True, autopct='%1.1f%%')\n"", ""plt.title('Global Proportions of 2 severely striken countries')\n"", 'plt.show()']"
"CHECKPOINT print('> Cross-validating classifiers') for label, clf in ensemble: ASSIGN = cross_val_score(clf, X, train_y, ASSIGN=5, ASSIGN='accuracy', ASSIGN=0, ASSIGN=-1) print(' -- {: <24} : {:.3f} : {}'.format(label, np.mean(ASSIGN), np.around(ASSIGN, 3))) print('> Fitting stack') ASSIGN = StackingCVClassifier(classifiers=[ab_clf, et_clf, lg_clf, rf_clf], ASSIGN=rf_clf, ASSIGN=5, ASSIGN=True, ASSIGN=True, ASSIGN=True, ASSIGN=True, ASSIGN=1, ASSIGN=ASSIGN, ASSIGN=-1) ASSIGN = ASSIGN.fit(X, train_y)",0,"[""print('> Cross-validating classifiers')\n"", 'for label, clf in ensemble:\n', '    score = cross_val_score(clf, X, train_y,\n', '                            cv=5,\n', ""                            scoring='accuracy',\n"", '                            verbose=0,\n', '                            n_jobs=-1)\n', '\n', ""    print('  -- {: <24} : {:.3f} : {}'.format(label, np.mean(score), np.around(score, 3)))\n"", '\n', '\n', ""print('> Fitting stack')\n"", '\n', 'stack = StackingCVClassifier(classifiers=[ab_clf, et_clf, lg_clf, rf_clf],\n', '                             meta_classifier=rf_clf,\n', '                             cv=5,\n', '                             stratify=True,\n', '                             shuffle=True,\n', '                             use_probas=True,\n', '                             use_features_in_secondary=True,\n', '                             verbose=1,\n', '                             random_state=random_state,\n', '                             n_jobs=-1)\n', '\n', 'stack = stack.fit(X, train_y)\n']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import seaborn as sns\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
"CHECKPOINT ASSIGN=-73.94 ASSIGN=40.72 Brooklyn_10_65_map=folium.Map([ASSIGN,ASSIGN],zoom_start=12) Brooklyn_10_65_rooms_map=plugins.MarkerCluster().add_to(Brooklyn_10_65_map) for lat,lon,label in zip(data_Brooklyn_10_65.latitude,data_Brooklyn_10_65.longitude,data_Brooklyn_10_65.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(Brooklyn_10_65_rooms_map) Brooklyn_10_65_map.add_child(Brooklyn_10_65_rooms_map) Brooklyn_10_65_map",1,"['Long=-73.94\n', 'Lat=40.72\n', 'Brooklyn_10_65_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'Brooklyn_10_65_rooms_map=plugins.MarkerCluster().add_to(Brooklyn_10_65_map)\n', 'for lat,lon,label in zip(data_Brooklyn_10_65.latitude,data_Brooklyn_10_65.longitude,data_Brooklyn_10_65.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(Brooklyn_10_65_rooms_map)\n', 'Brooklyn_10_65_map.add_child(Brooklyn_10_65_rooms_map)\n', '\n', 'Brooklyn_10_65_map']"
CHECKPOINT data.classes,0,['data.classes']
ASSIGN=[np.argmax(i) for i in pred],0,['pred_final=[np.argmax(i) for i in pred]']
ASSIGN = artime.iloc[:42] ASSIGN = artime.iloc[42:],0,"['train = artime.iloc[:42]\n', 'test = artime.iloc[42:]']"
"SETUP ASSIGN = LogisticRegression(random_state = 0) ASSIGN.fit(X_train, y_train)",0,"['#Import and fit logistic regression mode\n', 'from sklearn.linear_model import LogisticRegression\n', 'LR = LogisticRegression(random_state = 0)\n', 'LR.fit(X_train, y_train)']"
SETUP,0,"['import matplotlib\n', 'import matplotlib.pyplot as plt\n', '\n', 'import os\n', 'import random\n', 'import io\n', 'import imageio\n', 'import glob\n', 'import scipy.misc\n', 'import numpy as np\n', 'import pandas as pd\n', '\n', 'from six import BytesIO\n', 'from PIL import Image, ImageDraw, ImageFont\n', 'from IPython.display import display, Javascript\n', 'from IPython.display import Image as IPyImage\n', '\n', 'import tensorflow as tf\n', '\n', 'from object_detection.utils import label_map_util\n', 'from object_detection.utils import config_util\n', 'from object_detection.utils import visualization_utils as viz_utils\n', '# from object_detection.utils import colab_utils\n', '# this was for the annotator can be done away with in this kaggle env\n', 'from object_detection.builders import model_builder\n', '\n', '%matplotlib inline']"
"CHECKPOINT ASSIGN = np.diag(cm) path(cm, axis = 1) recall",0,"['recall = np.diag(cm) / np.sum(cm, axis = 1)\n', 'recall']"
SETUP ASSIGN = StandardScaler() ASSIGN = sc.fit_transform(ASSIGN),0,"['#Scale X\n', 'from sklearn.preprocessing import StandardScaler\n', 'sc = StandardScaler()\n', 'X = sc.fit_transform(X)']"
ASSIGN = keras.utils.to_categorical(ASSIGN) ASSIGN = keras.utils.to_categorical(ASSIGN) ASSIGN = y_train.shape[1],0,"['y_train = keras.utils.to_categorical(y_train)\n', 'y_test = keras.utils.to_categorical(y_test)\n', 'num_classes = y_train.shape[1]']"
"plt.figure(figsize=(16,9)) plt.plot(data_features['LotArea'], data_features['LotFrontage'])",1,"['plt.figure(figsize=(16,9))\n', ""plt.plot(data_features['LotArea'], data_features['LotFrontage'])""]"
"ASSIGN = model.evaluate(X_train, y_ohe, batch_size=128)",0,"['loss_and_metrics = model.evaluate(X_train, y_ohe, batch_size=128)']"
SETUP,0,"['import os\n', 'import re\n', 'from glob import glob\n', 'from tqdm import tqdm\n', 'import numpy as np\n', 'import pandas as pd\n', 'import ast\n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline']"
irishtimes.isnull().sum(),0,['irishtimes.isnull().sum()']
SETUP,0,['import pmdarima']
"ASSIGN=plt.subplots(1,2,figsize=(25,16)) sns.barplot(x=data['Content Rating'].value_counts(),y=data['Content Rating'].value_counts().index,ax=ax[0]) ax[0].set_title(""Counts of Content Rating"",size=20) ax[0].set_xlabel("""") sns.barplot(x=data.groupby(['Content Rating'])['new_Price'].agg('sum'),y=data.groupby(['Content Rating'])['new_Price'].agg('sum').index,ax=ax[1]) ax[1].set_title(""Total Price by Content Rating"",size=20) ax[1].set_ylabel("""") ax[1].set_xlabel(""Total Price"")",1,"['fig,ax=plt.subplots(1,2,figsize=(25,16))\n', ""sns.barplot(x=data['Content Rating'].value_counts(),y=data['Content Rating'].value_counts().index,ax=ax[0])\n"", 'ax[0].set_title(""Counts of Content Rating"",size=20)\n', 'ax[0].set_xlabel("""")\n', '\n', ""sns.barplot(x=data.groupby(['Content Rating'])['new_Price'].agg('sum'),y=data.groupby(['Content Rating'])['new_Price'].agg('sum').index,ax=ax[1])\n"", 'ax[1].set_title(""Total Price by Content Rating"",size=20)\n', 'ax[1].set_ylabel("""")\n', 'ax[1].set_xlabel(""Total Price"")\n']"
"CHECKPOINT ASSIGN = KNeighborsClassifier(n_neighbors = 10) ASSIGN.fit(train_x, train_y) ASSIGN = cross_val_score(knn,train_x,train_y,cv=5,scoring='accuracy') print('Cross validation: %.5f'%ASSIGN.mean()) print('standard deviation of Cross validation: %.5f'%ASSIGN.std(ddof=1))",0,"['#knn\n', 'knn = KNeighborsClassifier(n_neighbors = 10)\n', 'knn.fit(train_x, train_y)\n', '\n', '#Cross validation\n', ""scores = cross_val_score(knn,train_x,train_y,cv=5,scoring='accuracy')\n"", ""print('Cross validation: %.5f'%scores.mean())\n"", ""print('standard deviation of Cross validation: %.5f'%scores.std(ddof=1))""]"
CHECKPOINT ht,0,['ht']
"CHECKPOINT ASSIGN=stations[stations['city_id']==206] ASSIGN=-73.97 ASSIGN=40.78 New_York_map=folium.Map([ASSIGN,ASSIGN],zoom_start=12) New_York_stations_map=plugins.MarkerCluster().add_to(New_York_map) for lat,lon,label in zip(ASSIGN.ASSIGN,ASSIGN.ASSIGN,ASSIGN.stations_name): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(New_York_stations_map) New_York_map.add_child(New_York_stations_map) New_York_map",1,"[""stations_New_York=stations[stations['city_id']==206]\n"", 'Long=-73.97\n', 'Lat=40.78\n', 'New_York_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'New_York_stations_map=plugins.MarkerCluster().add_to(New_York_map)\n', 'for lat,lon,label in zip(stations_New_York.Lat,stations_New_York.Long,stations_New_York.stations_name):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(New_York_stations_map)\n', 'New_York_map.add_child(New_York_stations_map)\n', '\n', 'New_York_map']"
total1['Total Atrocities'] = total1['Murder'] +total1['Assault on women']+total1['Kidnapping and Abduction']+total1['Dacoity']+total1['Robbery']+total1['Arson']+total1['Hurt']+total1['Prevention of atrocities (POA) Act']+total1['Protection of Civil Rights (PCR) Act']+total1['Other Crimes Against SCs'] total1.head(),0,"[""total1['Total Atrocities'] = total1['Murder'] +total1['Assault on women']+total1['Kidnapping and Abduction']+total1['Dacoity']+total1['Robbery']+total1['Arson']+total1['Hurt']+total1['Prevention of atrocities (POA) Act']+total1['Protection of Civil Rights (PCR) Act']+total1['Other Crimes Against SCs']\n"", 'total1.head()']"
"CHECKPOINT ASSIGN=zomato_orgnl.drop(['url','dish_liked','phone'],axis=1) zomato.columns",0,"[""zomato=zomato_orgnl.drop(['url','dish_liked','phone'],axis=1)\n"", 'zomato.columns']"
covid['SARS-Cov-2 exam result'] = pd.get_dummies(covid['SARS-Cov-2 exam result']),0,"['# convertendo celulas categóricas em numericas\n', ""covid['SARS-Cov-2 exam result'] = pd.get_dummies(covid['SARS-Cov-2 exam result'])""]"
"def f1_curve(target, preds, t_min=0.01, t_max=0.99, steps=99): ASSIGN = {} for t in np.linspace(t_min, t_max, steps): with warnings.catch_warnings(): warnings.simplefilter(""ignore"") ASSIGN = sklearn.metrics.f1_score(target, preds >= t) return pd.Series(ASSIGN).sort_index()",0,"['def f1_curve(target, preds, t_min=0.01, t_max=0.99, steps=99):\n', '    curve = {}\n', '    for t in np.linspace(t_min, t_max, steps):\n', '        with warnings.catch_warnings():\n', '            warnings.simplefilter(""ignore"")\n', '            curve[t] = sklearn.metrics.f1_score(target, preds >= t)\n', '    return pd.Series(curve).sort_index()']"
CHECKPOINT device,0,['device']
ASSIGN=np.vstack(li),0,"['#numpys vstack method to append all the datafames to stack the sequence of input vertically to make a single array\n', 'df_np=np.vstack(li)']"
type(X),0,['type(X)']
"test_df[""Fare""].fillna(test_df[""Fare""].median(), inplace=True) titanic_df['Fare'] = titanic_df['Fare'].astype(int) test_df['Fare']    = test_df['Fare'].astype(int) ASSIGN = titanic_df[""Fare""][titanic_df[""Survived""] == 0] ASSIGN   = titanic_df[""Fare""][titanic_df[""Survived""] == 1] ASSIGN = DataFrame([fare_not_survived.mean(), fare_survived.mean()]) ASSIGN   = DataFrame([fare_not_survived.std(), fare_survived.std()]) titanic_df['Fare'].plot(kind='hist', figsize=(15,3),bins=100, xlim=(0,50)) ASSIGN.index.names = ASSIGN.index.names = [""Survived""] ASSIGN.plot(yerr=ASSIGN,kind='bar',legend=False)",1,"['# Fare\n', '\n', '# only for test_df, since there is a missing ""Fare"" values\n', 'test_df[""Fare""].fillna(test_df[""Fare""].median(), inplace=True)\n', '\n', '# convert from float to int\n', ""titanic_df['Fare'] = titanic_df['Fare'].astype(int)\n"", ""test_df['Fare']    = test_df['Fare'].astype(int)\n"", '\n', ""# get fare for survived & didn't survive passengers \n"", 'fare_not_survived = titanic_df[""Fare""][titanic_df[""Survived""] == 0]\n', 'fare_survived     = titanic_df[""Fare""][titanic_df[""Survived""] == 1]\n', '\n', '# get average and std for fare of survived/not survived passengers\n', 'avgerage_fare = DataFrame([fare_not_survived.mean(), fare_survived.mean()])\n', 'std_fare      = DataFrame([fare_not_survived.std(), fare_survived.std()])\n', '\n', '# plot\n', ""titanic_df['Fare'].plot(kind='hist', figsize=(15,3),bins=100, xlim=(0,50))\n"", '\n', 'avgerage_fare.index.names = std_fare.index.names = [""Survived""]\n', ""avgerage_fare.plot(yerr=std_fare,kind='bar',legend=False)""]"
"ASSIGN=data[data['Countrypath']=='ASSIGN'] ASSIGN= ASSIGN.groupby(['date'])['Confirmed', 'Deaths', 'Recovered'].max() ASSIGN = ASSIGN.reset_index() ASSIGN.head()",0,"[""Italy=data[data['Country/Region']=='Italy']\n"", ""Italy= Italy.groupby(['date'])['Confirmed', 'Deaths', 'Recovered'].max()\n"", 'Italy = Italy.reset_index()\n', 'Italy.head()']"
match_stats_df['poss_info'] = match_stats_df.possession.apply(lambda val: parse_poss(val)),0,"[""match_stats_df['poss_info'] = match_stats_df.possession.apply(lambda val: parse_poss(val))""]"
ASSIGN = ASSIGN.map(lambda s: 1 if s == 1 else 0) ASSIGN = ASSIGN.map(lambda s: 1 if s == 2 else 0) ASSIGN = ASSIGN.map(lambda s: 1 if 3 <= s <= 4 else 0) ASSIGN = ASSIGN.map(lambda s: 1 if s >= 5 else 0),0,"[""final['Single'] = final['Family'].map(lambda s: 1 if s == 1 else 0)\n"", ""final['SmallF'] = final['Family'].map(lambda s: 1 if  s == 2  else 0)\n"", ""final['MedF'] = final['Family'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n"", ""final['LargeF'] = final['Family'].map(lambda s: 1 if s >= 5 else 0)""]"
CHECKPOINT print(X_train.shape) print(Y_train.shape) print(X_val.shape) print(Y_val.shape) print(X_test.shape) print(Y_test.shape),0,"['print(X_train.shape)\n', 'print(Y_train.shape)\n', 'print(X_val.shape)\n', 'print(Y_val.shape)\n', 'print(X_test.shape)\n', 'print(Y_test.shape)']"
"ASSIGN = StandardScaler().fit(train_X[['Amount']]) train_X.loc[:, ['Amount']] = ASSIGN.transform(train_X[['Amount']]) test_X.loc[:, ['Amount']] = ASSIGN.transform(test_X[['Amount']]) ASSIGN = MinMaxScaler().fit(train_X) ASSIGN = minmax_scaler.transform(ASSIGN) ASSIGN = minmax_scaler.transform(ASSIGN)",0,"[""# standard_scaler = StandardScaler().fit(train_X[['Time', 'Amount']])\n"", ""# train_X.loc[:, ['Time', 'Amount']] = standard_scaler.transform(train_X[['Time', 'Amount']])\n"", ""# test_X.loc[:, ['Time', 'Amount']] = standard_scaler.transform(test_X[['Time', 'Amount']])\n"", '\n', ""standard_scaler = StandardScaler().fit(train_X[['Amount']])\n"", ""train_X.loc[:, ['Amount']] = standard_scaler.transform(train_X[['Amount']])\n"", ""test_X.loc[:, ['Amount']] = standard_scaler.transform(test_X[['Amount']])\n"", 'minmax_scaler = MinMaxScaler().fit(train_X)\n', 'train_X = minmax_scaler.transform(train_X)\n', 'test_X = minmax_scaler.transform(test_X)']"
CHECKPOINT ASSIGN = clf.predict(test_features) predictions.shape,0,"['predictions = clf.predict(test_features)\n', 'predictions.shape']"
"(X_train, y_train), (X_test, y_test) = mnist.load_data() X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=10000, random_state=42)",0,"['\n', '(X_train, y_train), (X_test, y_test) = mnist.load_data()\n', 'X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=10000, random_state=42)']"
"ASSIGN = plt.subplots(ncols=2) ASSIGN = train[""V80""].fillna(-999) ASSIGN = test[test[""TransactionDT""]>2.5e7][""V80""].fillna(-999) # values following the shift axs[0].hist(ASSIGN, alpha=0.5, normed=True, bins=25) axs[1].hist(ASSIGN, alpha=0.5, normed=True, bins=25) fig.set_size_inches(7,3) plt.tight_layout()",1,"['fig, axs = plt.subplots(ncols=2)\n', '\n', 'train_vals = train[""V80""].fillna(-999)\n', 'test_vals = test[test[""TransactionDT""]>2.5e7][""V80""].fillna(-999) # values following the shift\n', '\n', '\n', 'axs[0].hist(train_vals, alpha=0.5, normed=True, bins=25)\n', '    \n', 'axs[1].hist(test_vals, alpha=0.5, normed=True, bins=25)\n', '\n', '\n', 'fig.set_size_inches(7,3)\n', 'plt.tight_layout()']"
"CHECKPOINT ASSIGN = [""MOURNING DOVE"", ""GULL"",""KILLDEER"", ""AMERICAN KESTREL"",""BARN SWALLOW""] ASSIGN = species[species.isin(ASSIGN)] print(ASSIGN.value_counts())",0,"['top_known_species = [""MOURNING DOVE"", ""GULL"",""KILLDEER"", ""AMERICAN KESTREL"",""BARN SWALLOW""]\n', 'top_known_species = species[species.isin(top_known_species)]\n', 'print(top_known_species.value_counts())']"
"ASSIGN=data_Queens.loc[(data_Queens['price'] <100)][:2000] ASSIGN['label']=ASSIGN.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1) ASSIGN.head()",0,"[""data_Queens_100_1=data_Queens.loc[(data_Queens['price'] <100)][:2000]\n"", ""data_Queens_100_1['label']=data_Queens_100_1.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n"", 'data_Queens_100_1.head()']"
player_attr_df.isnull().sum(),0,['player_attr_df.isnull().sum()']
tb.head(),0,['tb.head()']
"ASSIGN = plt.subplots(1,2,figsize=(20,5)) sns.distplot(data.groupby(""target_label"").size(), ax=ax[0], color=""Orange"", kde=False) ax[0].set_xlabel(""Number of images"") ax[0].set_ylabel(""Frequency""); sns.countplot(data.target, palette=""Set2"", ax=ax[1]); ax[1].set_xlabel(""Names of Class"") ax[1].set_title(""Data Distribution"");",1,"['# cancer_perc = data.groupby(""patient_id"").target.value_counts()/ data.groupby(""patient_id"").target.size()\n', '# cancer_perc = cancer_perc.unstack()\n', '\n', 'fig, ax = plt.subplots(1,2,figsize=(20,5))\n', 'sns.distplot(data.groupby(""target_label"").size(), ax=ax[0], color=""Orange"", kde=False)\n', 'ax[0].set_xlabel(""Number of images"")\n', 'ax[0].set_ylabel(""Frequency"");\n', '\n', 'sns.countplot(data.target, palette=""Set2"", ax=ax[1]);\n', 'ax[1].set_xlabel(""Names of Class"")\n', 'ax[1].set_title(""Data Distribution"");']"
"plt.figure(figsize=(50,50)) ASSIGN=Basemap(width=120000,height=900000,projection=""tmerc"",resolution=""l"",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=20,lon_0=88) ASSIGN.drawcountries() ASSIGN.drawmapboundary(color=' ASSIGN.drawcoastlines() ASSIGN=np.array(df_plot_bottom[""lng""]) ASSIGN=np.array(df_plot_bottom[""ASSIGN""]) ASSIGN=np.array(df_plot_bottom[""COUNT""]) ASSIGN=np.array(df_plot_bottom[""CITY""]) ASSIGN=map(lg,lat) ASSIGN=df_plot_bottom[""COUNT""].apply(lambda x: int(x)*50) plt.scatter(ASSIGN,s=ASSIGN,marker=""o"",c=ASSIGN) for a,b ,c,d in zip(ASSIGN,ASSIGN,ASSIGN): plt.text(a-3000,b,c,fontsize=30,color=""r"") plt.text(a+60000,b+30000,d,fontsize=30) plt.title(""BOTTOM 15 INDIAN CITIES MINIMUM RESTAURANT COUNTS PLOT AS PER ZOMATO"",fontsize=30,color='RED')",1,"['#lets plot with the city names and restaurants count inside the map corresponding to the cities exact co-ordinates which we received from google api ,here marker color will be different as per marker size\n', '#plt.subplots(figsize=(20,50))\n', 'plt.figure(figsize=(50,50))\n', 'map=Basemap(width=120000,height=900000,projection=""tmerc"",resolution=""l"",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=20,lon_0=88)\n', 'map.drawcountries()\n', ""map.drawmapboundary(color='#f2f2f2')\n"", '\n', 'map.drawcoastlines()\n', '\n', '\n', '\n', 'lg=np.array(df_plot_bottom[""lng""])\n', 'lat=np.array(df_plot_bottom[""lat""])\n', 'pt=np.array(df_plot_bottom[""COUNT""])\n', 'city_name=np.array(df_plot_bottom[""CITY""])\n', '\n', 'x,y=map(lg,lat)\n', '\n', '#using lambda function to create different sizes of marker as per thecount \n', '\n', 'p_s=df_plot_bottom[""COUNT""].apply(lambda x: int(x)*50)\n', '\n', '#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is different.\n', 'plt.scatter(x,y,s=p_s,marker=""o"",c=p_s)\n', '\n', 'for a,b ,c,d in zip(x,y,city_name,pt):\n', '    #plt.text takes x position , y position ,text(city name) ,font size and color as arguments\n', '    plt.text(a-3000,b,c,fontsize=30,color=""r"")\n', '    #plt.text takes x position , y position ,text(restaurant counts) ,font size and color as arguments, like above . but only i have changed the x and y position to make it more clean and easier to read\n', '    plt.text(a+60000,b+30000,d,fontsize=30)\n', '   \n', '    \n', '    \n', 'plt.title(""BOTTOM 15 INDIAN CITIES MINIMUM RESTAURANT COUNTS PLOT AS PER ZOMATO"",fontsize=30,color=\'RED\')']"
"ASSIGN=plt.figure(figsize=(20,10)) ASSIGN=2 ASSIGN=5 for i in range(1,11): ASSIGN.add_subplot(ASSIGN,ASSIGN,i) ASSIGN=train['ImageId'][i] ASSIGN=cv2.imread(""..path""+Graph) ASSIGN= cv2.cvtColor(ASSIGN,cv2.COLOR_BGR2RGB) ASSIGN = rle2mask(train['EncodedPixels'].iloc[i], img_new) ASSIGN[ASSIGN==1,0] = 255 plt.imshow(ASSIGN) plt.show()",1,"['fig=plt.figure(figsize=(20,10))\n', 'col=2\n', 'row=5\n', 'for i in range(1,11):\n', '\tfig.add_subplot(row,col,i)\n', ""\tGraph=train['ImageId'][i]\n"", '\timg_new=cv2.imread(""../input/severstal-steel-defect-detection/train_images/""+Graph)\n', '\timg_new= cv2.cvtColor(img_new,cv2.COLOR_BGR2RGB)\n', ""\tmask = rle2mask(train['EncodedPixels'].iloc[i], img_new)\n"", '\timg_new[mask==1,0] = 255\n', '\tplt.imshow(img_new)\n', 'plt.show()']"
SETUP ASSIGN = 7 ASSIGN = TimeSeriesSplit(n_splits=n_fold) ASSIGN = KFold(n_splits=5),0,"['from sklearn.model_selection import TimeSeriesSplit\n', 'n_fold = 7\n', 'folds = TimeSeriesSplit(n_splits=n_fold)\n', 'folds = KFold(n_splits=5)']"
"SETUP CHECKPOINT ASSIGN = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', 'Matthew', 'Laura'], 'score': [12.5, 9, 16.5, 6.0, 9, 20, np.nan, 5.5], 'attempts': [1, 3, 2, 3, 2, 3, 1, 1], 'qualify': ['yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no']} ASSIGN = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'] ASSIGN = pd.DataFrame(exam_data , index=labels) print() print(ASSIGN[ASSIGN['score'].isnull()])",0,"['import pandas as pd\n', 'import numpy as np\n', ""exam_data  = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily', 'Michael', 'Matthew', 'Laura'],\n"", ""        'score': [12.5, 9, 16.5, 6.0, 9, 20, np.nan, 5.5],\n"", ""        'attempts': [1, 3, 2, 3, 2, 3, 1, 1],\n"", ""        'qualify': ['yes', 'no', 'yes', 'no', 'no', 'yes', 'yes', 'no']}\n"", ""labels = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n"", 'df = pd.DataFrame(exam_data , index=labels)\n', 'print(""Rows where score is missing:"")\n', ""print(df[df['score'].isnull()])""]"
"CHECKPOINT ASSIGN = np.r_[1, 6, 11:55] match_df.drop(match_df.columns[ASSIGN],axis=1, inplace=True) match_df.columns",0,"['#confirmation step\n', 'cols = np.r_[1, 6, 11:55]\n', 'match_df.drop(match_df.columns[cols],axis=1, inplace=True)\n', 'match_df.columns']"
"ASSIGN = ' 50000+.' ASSIGN = [FillMissing, Categorify, Normalize]",0,"['# parameters for NN model\n', ""dep_var =  ' 50000+.'\n"", 'procs = [FillMissing, Categorify, Normalize]']"
SETUP,0,"['from keras.utils import np_utils\n', 'from keras.models import Sequential\n', 'from keras.layers import Convolution2D,Dense,MaxPool2D,Activation,Dropout,Flatten\n', 'from keras.layers import GlobalAveragePooling2D\n', 'from keras.optimizers import Adam\n', 'from sklearn.model_selection import train_test_split\n', 'from keras.layers.normalization import BatchNormalization\n', 'import os \n', 'import pandas as pd\n', 'import plotly.graph_objs as go\n', 'import matplotlib.ticker as ticker\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', 'import plotly.express as px\n', 'import cv2\n', 'import numpy as np\n', 'from sklearn.model_selection import train_test_split']"
(5 - 3) path,0,['(5 - 3) // 2']
SETUP ASSIGN = InceptionV3(weights='imagenet'),0,"['# Import Model\n', '#from tensorflow.keras.applications import VGG16\n', '#from tensorflow.keras.applications import ResNet101V2\n', 'from tensorflow.keras.applications import InceptionV3\n', '\n', '#from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n', '#from tensorflow.keras.applications.resnet import preprocess_input, decode_predictions\n', 'from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n', '\n', '# Load Model\n', ""#model = VGG16(weights='imagenet')\n"", ""#model = ResNet101V2(weights='imagenet')\n"", ""model = InceptionV3(weights='imagenet')""]"
"SETUP for i in tqdm(range(0,len(sub))): sub.iloc[i,0] = predictions[i][0] sub.iloc[i,1] = predictions[i][1] sub.iloc[i,2] = predictions[i][2] sub.iloc[i,3] = predictions[i][3] sub.iloc[i,4] = predictions[i][4] sub.iloc[i,5] = predictions[i][5] sub.iloc[i,6] = predictions[i][6]",0,"['from tqdm import tqdm\n', '\n', 'for i in tqdm(range(0,len(sub))):\n', '    sub.iloc[i,0] = predictions[i][0]\n', '    sub.iloc[i,1] = predictions[i][1]\n', '    sub.iloc[i,2] = predictions[i][2]\n', '    sub.iloc[i,3] = predictions[i][3]\n', '    sub.iloc[i,4] = predictions[i][4]\n', '    sub.iloc[i,5] = predictions[i][5]\n', '    sub.iloc[i,6] = predictions[i][6]\n', '    ']"
"plt.figure(figsize=(50,60)) ASSIGN=Basemap(width=120000,height=900000,projection=""lcc"",resolution=""l"",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77) ASSIGN.drawcountries() ASSIGN.drawmapboundary(color=' ASSIGN.drawcoastlines() ASSIGN=np.array(df_plot_top[""lng""]) ASSIGN=np.array(df_plot_top[""ASSIGN""]) ASSIGN=np.array(df_plot_top[""COUNT""]) ASSIGN=np.array(df_plot_top[""CITY""]) ASSIGN=map(lg,lat) ASSIGN=df_plot_top[""COUNT""].apply(lambda x: int(x)path) plt.scatter(ASSIGN,s=ASSIGN,marker=""o"",c='BLUE') plt.title(""TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO"",fontsize=30,color='RED')",1,"['#lets plot this inside the map corresponding to the cities exact co-ordinates which we received from google api \n', '#plt.subplots(figsize=(20,50))\n', 'plt.figure(figsize=(50,60))\n', 'map=Basemap(width=120000,height=900000,projection=""lcc"",resolution=""l"",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n', 'map.drawcountries()\n', ""map.drawmapboundary(color='#f2f2f2')\n"", '\n', 'map.drawcoastlines()\n', '\n', '\n', '\n', 'lg=np.array(df_plot_top[""lng""])\n', 'lat=np.array(df_plot_top[""lat""])\n', 'pt=np.array(df_plot_top[""COUNT""])\n', 'city_name=np.array(df_plot_top[""CITY""])\n', '\n', 'x,y=map(lg,lat)\n', '\n', '#using lambda function to create different sizes of marker as per thecount \n', '\n', 'p_s=df_plot_top[""COUNT""].apply(lambda x: int(x)/2)\n', '\n', '#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is always blue.\n', 'plt.scatter(x,y,s=p_s,marker=""o"",c=\'BLUE\')\n', 'plt.title(""TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO"",fontsize=30,color=\'RED\')']"
"ASSIGN = ASSIGN[ASSIGN.GrLivArea < 4500] ASSIGN.reset_index(drop = True, inplace = True) ASSIGN = [30, 88, 462, 631, 1322] ASSIGN.drop(ASSIGN.index[ASSIGN], inplace = True)",0,"['train = train[train.GrLivArea < 4500]\n', 'train.reset_index(drop = True, inplace = True)\n', 'outliars = [30, 88, 462, 631, 1322]\n', 'train.drop(train.index[outliars], inplace = True)\n', '#If you want to know why remove these values u can read kernels about EDA']"
"sns.heatmap(tf.isnull(),yticklabels=False)",1,"['sns.heatmap(tf.isnull(),yticklabels=False)']"
"submission_df.to_csv('submission.csv', index=False)",0,"[""submission_df.to_csv('submission.csv', index=False)""]"
"sns.relplot(x ='Research', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')",1,"[""sns.relplot(x ='Research', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')""]"
"model_without_outliers.to_csv(""combining_submission.csv"", index=False)",0,"['model_without_outliers.to_csv(""combining_submission.csv"", index=False)']"
"SETUP warnings.filterwarnings(""ignore"") os.environ['OMP_NUM_THREADS'] = '8'",0,"['import pandas as pd\n', 'import numpy as np\n', 'from sklearn.ensemble import RandomForestRegressor\n', 'import matplotlib.pyplot as plt\n', 'from sklearn.utils import shuffle\n', 'from sklearn.model_selection import GridSearchCV\n', 'import warnings; \n', 'warnings.filterwarnings(""ignore"") \n', 'import os \n', ""os.environ['OMP_NUM_THREADS'] = '8' # speed up using 8 cpu""]"
"ASSIGN = {'XGBoost':XGBRegressor(n_estimators=1000, ASSIGN=6, ASSIGN ='reg:squarederror')}",0,"[""models = {'XGBoost':XGBRegressor(n_estimators=1000,\n"", '                                max_depth=6,\n', ""                                objective ='reg:squarederror')}""]"
"SETUP ASSIGN = pd.read_csv(""..path"") np.random.seed(0)",0,"[""# modules we'll use\n"", 'import pandas as pd\n', 'import numpy as np\n', '\n', '# for Box-Cox Transformation\n', 'from scipy import stats\n', '\n', '# for min_max scaling\n', 'from mlxtend.preprocessing import minmax_scaling\n', '\n', '# plotting modules\n', 'import seaborn as sns\n', 'import matplotlib.pyplot as plt\n', '\n', '# read in all our data\n', 'kickstarters_2017 = pd.read_csv(""../input/kickstarter-projects/ks-projects-201801.csv"")\n', '\n', '# set seed for reproducibility\n', 'np.random.seed(0)']"
SLICE=data['Inspection Date'].apply(lambda x:x.split('-')[0]) SLICE=data['Inspection Date'].apply(lambda x:x.split('-')[1]) SLICE=data['Inspection Date'].apply(lambda x:x.split('-')[2].split('T')[0]) data.head(),0,"[""data['year']=data['Inspection Date'].apply(lambda x:x.split('-')[0])\n"", ""data['month']=data['Inspection Date'].apply(lambda x:x.split('-')[1])\n"", ""data['day']=data['Inspection Date'].apply(lambda x:x.split('-')[2].split('T')[0])\n"", 'data.head()']"
CHECKPOINT print(model.summary()),0,['print(model.summary())']
"plt.figure(figsize = (30, 20)) sns.heatmap(tumor_data.corr(), cmap = ""RdBu_r"")",1,"['# First, we make sure that the graphic is crearly visible\n', 'plt.figure(figsize = (30, 20))\n', '# And now, draw the heatmap\n', 'sns.heatmap(tumor_data.corr(), cmap = ""RdBu_r"")']"
CHECKPOINT print(X_train.shape) print(Y_train.shape) print(X_val.shape) print(Y_val.shape) print(X_test.shape) print(Y_test.shape),0,"['print(X_train.shape)\n', 'print(Y_train.shape)\n', 'print(X_val.shape)\n', 'print(Y_val.shape)\n', 'print(X_test.shape)\n', 'print(Y_test.shape)']"
SETUP SETUP SETUP SETUP,0,"['# move images to working folder\n', '!mkdir ../working/dog\n', '!mkdir ../working/wolf\n', '!cp ../input/dog-v1/* ../working/dog\n', '!cp ../input/wolf-v1/* ../working/wolf']"
data_Queens.neighbourhood.unique(),0,['data_Queens.neighbourhood.unique()']
ASSIGN = relevant.columns,0,['col = relevant.columns']
"''' ASSIGN = [0.01,0.1, 1, 10, 100] ASSIGN = [True,False] ASSIGN = MultinomialNB() ASSIGN = {'alpha': [0.01,0.1, 1, 10, 100],'fit_prior' : [True,False]} ASSIGN = GridSearchCV(mnb, parameters, scoring='accuracy',cv =10) ASSIGN.fit(x_train, x_test) ASSIGN.best_params_ '''",0,"[""'''\n"", 'alpha = [0.01,0.1, 1, 10, 100]\n', 'fit_prior = [True,False]\n', '\n', 'mnb = MultinomialNB()\n', '\n', ""parameters = {'alpha': [0.01,0.1, 1, 10, 100],'fit_prior' : [True,False]}\n"", '\n', ""mn = GridSearchCV(mnb, parameters, scoring='accuracy',cv =10)\n"", 'mn.fit(x_train, x_test)\n', 'mn.best_params_\n', ""'''""]"
"ASSIGN = pd.concat([cbs,cbsr]).drop_duplicates(keep = False)",0,"['total = pd.concat([cbs,cbsr]).drop_duplicates(keep = False)']"
final.head(),0,['final.head()']
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# Any results you write to the current directory are saved as output.']"
temperature.Date = temperature.Date.apply(lambda x:get_dt(x)),0,"['#transform date to datetime type\n', 'temperature.Date = temperature.Date.apply(lambda x:get_dt(x))']"
sns.set() sns.countplot(train_data['type']),1,"['sns.set()\n', ""sns.countplot(train_data['type'])""]"
"ASSIGN = plt.subplots() sns.boxplot(df['word_count'], order=range(0,max(df['word_count'])), ax=ax) ax.xaxis.set_major_locator(ticker.MultipleLocator(200)) ax.xaxis.set_major_formatter(ticker.ScalarFormatter()) fig.set_size_inches(8, 4) plt.show()",1,"['fig, ax = plt.subplots()\n', ""sns.boxplot(df['word_count'], order=range(0,max(df['word_count'])), ax=ax)\n"", '\n', 'ax.xaxis.set_major_locator(ticker.MultipleLocator(200))\n', 'ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n', 'fig.set_size_inches(8, 4)\n', 'plt.show()']"
"ASSIGN = cnn_learner(data, models.resnet34, metrics=error_rate) ASSIGN.fit_one_cycle(4) ASSIGN = ClassificationInterpretation.from_learner(learn) ASSIGN.plot_top_losses(9, figsize=(12,8)) ASSIGN.plot_top_losses(9, figsize=(12,8),heatmap=False) ASSIGN.plot_confusion_matrix()",1,"['learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n', 'learn.fit_one_cycle(4)\n', 'interp = ClassificationInterpretation.from_learner(learn)\n', 'interp.plot_top_losses(9, figsize=(12,8))\n', 'interp.plot_top_losses(9, figsize=(12,8),heatmap=False)\n', 'interp.plot_confusion_matrix()']"
logisticRegr.predict(X_train_pca[0:10]),0,['logisticRegr.predict(X_train_pca[0:10])']
ASSIGN = relevant,0,['covid = relevant']
"match_stats_df[""goals_info""] = match_stats_df.apply(lambda row: parse_goal(row.goal, row.home_team_api_id, row.away_team_api_id), axis= 1)",0,"['match_stats_df[""goals_info""] = match_stats_df.apply(lambda row: parse_goal(row.goal, row.home_team_api_id, row.away_team_api_id), axis= 1)']"
df['Survived'].value_counts(),0,"[""df['Survived'].value_counts()""]"
ASSIGN = list(match_bets_df.columns) ASSIGN.remove('id'),0,"['cols = list(match_bets_df.columns)\n', ""cols.remove('id')""]"
"SETUP CHECKPOINT ASSIGN=RandomForestClassifier(n_estimators=10) ASSIGN=[] for train_id, test_id in kf.split(data_matf,data_matl): X_train, X_test = data_matf.values[train_id], data_matf.values[test_id] ASSIGN = data_matl.values[train_id], data_matl.values[test_id] ASSIGN.fit(X_train,y_train) ASSIGN = Rf.predict(X_test) ASSIGN = accuracy_score(y_test, predictions) ASSIGN.append(ASSIGN) plt.plot(range(10),ASSIGN) plt.show() print(,np.mean(ASSIGN))",1,"['from sklearn.ensemble import RandomForestClassifier\n', 'Rf=RandomForestClassifier(n_estimators=10)\n', 'outcomesRf=[]\n', 'for train_id, test_id in kf.split(data_matf,data_matl):\n', '    X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]\n', '    y_train, y_test = data_matl.values[train_id], data_matl.values[test_id]\n', '    Rf.fit(X_train,y_train)\n', '    predictions = Rf.predict(X_test)\n', '    accuracy = accuracy_score(y_test, predictions)\n', '    outcomesRf.append(accuracy)\n', 'plt.plot(range(10),outcomesRf)\n', 'plt.show()\n', 'print(""the average error is equal to "",np.mean(outcomesRf))']"
"SETUP warnings.filterwarnings(""ignore"")",0,"['import warnings\n', 'warnings.filterwarnings(""ignore"")']"
SETUP,0,"['import numpy as np\n', 'import pandas as pd\n', 'import os \n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', 'import plotly.express as px\n', 'import folium\n', 'import datetime\n', 'import plotly.graph_objs as go\n', 'import matplotlib.ticker as ticker\n', 'import matplotlib.animation as animation\n', 'from IPython.display import HTML']"
"ASSIGN = {} for state in states: ASSIGN = stats.linregress(tb.Year[tb.State == state], tb['Smoke everyday'][tb.State == state]) ASSIGN[state] = slope ASSIGN = pd.DataFrame([slope_dict1]).transpose() ASSIGN.columns = ['slope'] ASSIGN.sort(columns = 'slope',ascending = False, inplace = True)",0,"['slope_dict1 = {}\n', '\n', 'for state in states:\n', ""    slope, intercept, r_value, p_value, std_err = stats.linregress(tb.Year[tb.State == state], tb['Smoke everyday'][tb.State == state])\n"", '    slope_dict1[state] = slope\n', '    \n', 'slope_df1 = pd.DataFrame([slope_dict1]).transpose()\n', ""slope_df1.columns = ['slope']\n"", ""slope_df1.sort(columns = 'slope',ascending = False, inplace = True)""]"
"data_tree_for_test.MSZoning[[455,756,790,1444]]='RL'",0,"[""data_tree_for_test.MSZoning[[455,756,790,1444]]='RL'""]"
"CHECKPOINT thisdict.pop(""model"") print(thisdict)",0,"['#Removing an item\n', 'thisdict.pop(""model"")\n', 'print(thisdict)']"
final.Cabin.isnull().sum(),0,['final.Cabin.isnull().sum()']
test.head(),0,['test.head()']
"plotPerColumnDistribution(df2, 10, 5)",1,"['plotPerColumnDistribution(df2, 10, 5)']"
"SETUP """"""Q1.Store blood groups of 50 different atients and show the no of patients having O- blood grouQ1. Store blood groups of 50 different atients and show the no of patients having O- blood group"""""" ASSIGN=['O+', 'A+', 'B+', 'AB+', 'O-', 'A-', 'B-', 'AB-'] ASSIGN=[5, 10, 12, 5, 3, 4, 5, 6] ASSIGN = ['b','b','b','b','g','b','b','b'] plt.bar(ASSIGN,ASSIGN,color=ASSIGN) plt.legend() plt.xlabel('Blood Groups') plt.ylabel('No. of Patients') plt.title('Blood Group Data Set') plt.show()",1,"['""""""Q1.Store blood groups of 50 different atients and show the no of \n', 'patients having O- blood grouQ1. Store blood groups of 50 different \n', 'atients and show the no of patients having O- blood group""""""\n', '\n', 'from matplotlib import pyplot as plt\n', ""blood_grp=['O+', 'A+', 'B+', 'AB+', 'O-', 'A-', 'B-', 'AB-']\n"", 'patients=[5, 10, 12, 5, 3, 4, 5, 6]\n', ""colors = ['b','b','b','b','g','b','b','b']\n"", 'plt.bar(blood_grp,patients,color=colors)\n', 'plt.legend()\n', ""plt.xlabel('Blood Groups')\n"", ""plt.ylabel('No. of Patients')\n"", ""plt.title('Blood Group Data Set')\n"", 'plt.show()']"
"sns.catplot(x='Year', y='Dacoity', data=total ,height = 5, aspect = 4,kind = 'bar')",1,"[""sns.catplot(x='Year', y='Dacoity', data=total ,height = 5, aspect = 4,kind = 'bar')""]"
"def mask2rle(img): ASSIGN= img.T.flatten() ASSIGN= np.concatenate([[0],ASSIGN,[0]]) ASSIGN = np.where(img_flt[1:] != img_flt[:-1])[0] ASSIGN[1::2] -= ASSIGN[::2] return ' '.join(str(x) for x in ASSIGN)",0,"['def mask2rle(img):\n', '\timg_flt= img.T.flatten()\n', '\timg_flt= np.concatenate([[0],img_flt,[0]]) \n', '\truns = np.where(img_flt[1:] != img_flt[:-1])[0]  \n', '\truns[1::2] -= runs[::2]\n', ""\treturn ' '.join(str(x) for x in runs)""]"
SETUP CHECKPOINT print(os.listdir('..path')),0,"['import torch\n', 'from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n', 'from keras.preprocessing.sequence import pad_sequences\n', 'from sklearn.model_selection import train_test_split\n', 'from pytorch_pretrained_bert import BertTokenizer, BertConfig\n', 'from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n', 'from tqdm import tqdm, trange\n', 'import pandas as pd\n', 'import io\n', 'import numpy as np\n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline\n', '\n', 'import torch\n', 'from pytorch_pretrained_bert.tokenization import BertTokenizer\n', 'import os\n', ""print(os.listdir('../input/'))""]"
"ASSIGN = svc.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('SVCLinear(HT).csv',index = False)",0,"['model12pred = svc.predict(y_train)\n', ""submission12 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission12['PassengerId'] = result['PassengerId']\n"", ""submission12['Survived'] = model12pred\n"", ""submission12.to_csv('SVCLinear(HT).csv',index = False)""]"
plt.boxplot(ht) plt.title('Height') plt.xlabel('X-axis') plt.ylabel('Y-axis') plt.show(),1,"['plt.boxplot(ht)\n', ""plt.title('Height')\n"", ""plt.xlabel('X-axis')\n"", ""plt.ylabel('Y-axis')\n"", 'plt.show()']"
"CHECKPOINT ASSIGN = season_result[['Season', 'WTeamID', 'WScore']] ASSIGN = season_result[['Season', 'LTeamID', 'LScore']] ASSIGN.rename(columns={'WTeamID':'TeamID', 'WScore':'Score'}, inplace=True) ASSIGN.rename(columns={'LTeamID':'TeamID', 'LScore':'Score'}, inplace=True) ASSIGN = pd.concat((season_win_result, season_lose_result)).reset_index(drop=True) season_result",0,"[""season_win_result = season_result[['Season', 'WTeamID', 'WScore']]\n"", ""season_lose_result = season_result[['Season', 'LTeamID', 'LScore']]\n"", ""season_win_result.rename(columns={'WTeamID':'TeamID', 'WScore':'Score'}, inplace=True)\n"", ""season_lose_result.rename(columns={'LTeamID':'TeamID', 'LScore':'Score'}, inplace=True)\n"", 'season_result = pd.concat((season_win_result, season_lose_result)).reset_index(drop=True)\n', 'season_result']"
for i in range(len(temperature)): if temperature.Province[i] is np.NaN: temperature.Province[i] = temperature.Country[i],0,"['# fill NAN of Province with Country name\n', 'for i in range(len(temperature)):\n', '    if temperature.Province[i] is np.NaN:\n', '        temperature.Province[i] = temperature.Country[i]']"
"def _plot(model, cam_func, img, cls_true): """"""plot original image, heatmap from cam and superimpose image"""""" ASSIGN = np.expand_dims(img, axis=0) ASSIGN = preprocess_input(copy.deepcopy(ASSIGN)) ASSIGN = np.uint8(ASSIGN) ASSIGN = cam_func(model=model, x=x, layer_name=model.layers[-2].name) ASSIGN = superimpose(img, cam) ASSIGN = plt.subplots(ncols=2, figsize=(8, 6)) axs[0].imshow(ASSIGN) axs[0].axis('off') axs[1].imshow(superimposed_img) axs[1].axis('off') plt.suptitle('True label: ' + class_to_label[cls_true] + ' path: ' + class_to_label[cls_pred]) plt.tight_layout() plt.show()",1,"['def _plot(model, cam_func, img, cls_true):\n', '    """"""plot original image, heatmap from cam and superimpose image""""""\n', '    \n', '    # for cam\n', '    x = np.expand_dims(img, axis=0)\n', '    x = preprocess_input(copy.deepcopy(x))\n', '\n', '    # for superimpose\n', '    img = np.uint8(img)\n', '\n', '    # cam / superimpose\n', '    cls_pred, cam = cam_func(model=model, x=x, layer_name=model.layers[-2].name)\n', '    img, heatmap, superimposed_img = superimpose(img, cam)\n', '\n', '    fig, axs = plt.subplots(ncols=2, figsize=(8, 6))\n', '\n', '    axs[0].imshow(img)\n', ""    #axs[0].set_title('True label: ' + class_to_label[cls_true] + ' / Predicted label : ' + class_to_label[cls_pred])\n"", ""    axs[0].axis('off')\n"", '\n', '    #axs[1].imshow(heatmap)\n', ""    #axs[1].set_title('heatmap')\n"", ""    #axs[1].axis('off')\n"", '\n', '    axs[1].imshow(superimposed_img)\n', '    #axs[1].set_title(class_to_label[cls_true])\n', ""    axs[1].axis('off')\n"", '\n', ""    plt.suptitle('True label: ' + class_to_label[cls_true] + ' / Predicted label : ' + class_to_label[cls_pred])\n"", '    plt.tight_layout()\n', '    plt.show()\n', '    #fig.savefig(""colon_aca_prewitt.jpeg"",bbox_inches=\'tight\', pad_inches=0)\n', '    \n', '    ']"
ASSIGN = pd.read_csv('..path'),0,"[""season_result = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament/WDataFiles_Stage1/WRegularSeasonCompactResults.csv')""]"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# Any results you write to the current directory are saved as output.']"
CHECKPOINT test,0,['test']
submission_df.describe(),0,"['# check results\n', 'submission_df.describe()']"
"CHECKPOINT ASSIGN = model5.predict(X_test) ASSIGN = mean_squared_error(y_test, y_pred5, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ASSIGN)))",0,"['y_pred5 = model5.predict(X_test)\n', '\n', 'val = mean_squared_error(y_test, y_pred5, squared=False)\n', 'val5 = str(round(val, 4))\n', '\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred5)))\n""]"
"sns.factorplot('Pclass','Survived',order=[1,2,3], data=titanic_df,size=5) ASSIGN = pd.get_dummies(titanic_df['Pclass']) ASSIGN.columns = ['Class_1','Class_2','Class_3'] ASSIGN.drop(['Class_3'], axis=1, inplace=True) ASSIGN = pd.get_dummies(test_df['Pclass']) ASSIGN.columns = ['Class_1','Class_2','Class_3'] ASSIGN.drop(['Class_3'], axis=1, inplace=True) titanic_df.drop(['Pclass'],axis=1,inplace=True) test_df.drop(['Pclass'],axis=1,inplace=True) ASSIGN = ASSIGN.join(pclass_dummies_titanic) ASSIGN  = ASSIGN.join(pclass_dummies_test)",1,"['# Pclass\n', '\n', ""# sns.factorplot('Pclass',data=titanic_df,kind='count',order=[1,2,3])\n"", ""sns.factorplot('Pclass','Survived',order=[1,2,3], data=titanic_df,size=5)\n"", '\n', '# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers\n', ""pclass_dummies_titanic  = pd.get_dummies(titanic_df['Pclass'])\n"", ""pclass_dummies_titanic.columns = ['Class_1','Class_2','Class_3']\n"", ""pclass_dummies_titanic.drop(['Class_3'], axis=1, inplace=True)\n"", '\n', ""pclass_dummies_test  = pd.get_dummies(test_df['Pclass'])\n"", ""pclass_dummies_test.columns = ['Class_1','Class_2','Class_3']\n"", ""pclass_dummies_test.drop(['Class_3'], axis=1, inplace=True)\n"", '\n', ""titanic_df.drop(['Pclass'],axis=1,inplace=True)\n"", ""test_df.drop(['Pclass'],axis=1,inplace=True)\n"", '\n', 'titanic_df = titanic_df.join(pclass_dummies_titanic)\n', 'test_df    = test_df.join(pclass_dummies_test)']"
"ASSIGN = pd.read_csv(""path"") ASSIGN.loc[ASSIGN[""Species""] == ""Iris-setosa"",""Species""] = 1 ASSIGN.loc[ASSIGN[""Species""] == ""Iris-versicolor"",""Species""] = 2 ASSIGN.loc[ASSIGN[""Species""] == ""Iris-virginica"",""Species""] = 3 ASSIGN = ASSIGN.to_numpy() np.random.shuffle(ASSIGN) ASSIGN = np.asarray(ASSIGN,dtype = np.float64) ASSIGN = dataset.shape[0] ASSIGN = dataset[:int(0.75*len_dataset),:] ASSIGN = train[:,1:-1] ASSIGN = train[:,-1] ASSIGN = pd.get_dummies(ASSIGN) ASSIGN = ASSIGN.to_numpy() ASSIGN=len(Y[0]) ASSIGN = X.shape ASSIGN = dataset[int(0.75*len_dataset):,:] ASSIGN = test[:,1:-1] ASSIGN = test[:,-1] ASSIGN = pd.get_dummies(ASSIGN) ASSIGN = ASSIGN.to_numpy() ASSIGN = Xt.shape",0,"['# Read dataset, divide it into train and test set\n', 'dataset = pd.read_csv(""/kaggle/input/iris/Iris.csv"")\n', 'dataset.loc[dataset[""Species""] == ""Iris-setosa"",""Species""] = 1\n', 'dataset.loc[dataset[""Species""] == ""Iris-versicolor"",""Species""] = 2\n', 'dataset.loc[dataset[""Species""] == ""Iris-virginica"",""Species""] = 3\n', 'dataset = dataset.to_numpy()\n', 'np.random.shuffle(dataset)\n', '\n', 'dataset = np.asarray(dataset,dtype = np.float64)\n', 'len_dataset = dataset.shape[0]\n', '\n', 'train = dataset[:int(0.75*len_dataset),:]\n', 'X = train[:,1:-1]\n', 'Y = train[:,-1]\n', 'Y = pd.get_dummies(Y)\n', 'Y = Y.to_numpy()\n', 'op_neurons=len(Y[0])\n', 'N,p = X.shape\n', '\n', 'test = dataset[int(0.75*len_dataset):,:]\n', 'Xt = test[:,1:-1]\n', 'Yt = test[:,-1]\n', 'Yt = pd.get_dummies(Yt)\n', 'Yt = Yt.to_numpy()\n', 'Nt,pt = Xt.shape']"
"def rotate(x, angle): ASSIGN = ASSIGN + angle ASSIGN = ASSIGN - (ASSIGN + np.pi) path(2 * np.pi) * 2 * np.pi return x plt.figure(figsize=(15,6)) sns.distplot(points_df['roll'].map(lambda ASSIGN: rotate(ASSIGN, np.pi)), bins=500); plt.xlabel('roll rotated by pi') plt.show()",1,"['def rotate(x, angle):\n', '    x = x + angle\n', '    x = x - (x + np.pi) // (2 * np.pi) * 2 * np.pi\n', '    return x\n', '\n', 'plt.figure(figsize=(15,6))\n', ""sns.distplot(points_df['roll'].map(lambda x: rotate(x, np.pi)), bins=500);\n"", ""plt.xlabel('roll rotated by pi')\n"", 'plt.show()']"
"CHECKPOINT ASSIGN = logisticRegr.ASSIGN(X_test_pca, y_test) print(ASSIGN)",0,"['score = logisticRegr.score(X_test_pca, y_test)\n', 'print(score)']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
"def make_poly(X, deg): ASSIGN = len(X) ASSIGN = [np.ones(n)] for d in range(deg): ASSIGN.append(X**(d+1)) return np.vstack(data).T def fit(X, Y): return np.linalg.solve(X.T.dot(X), X.T.dot(Y)) def fit_and_display(X, Y, sample, deg): ASSIGN = len(X) ASSIGN = np.random.choice(N, sample) ASSIGN = X[train_idx] ASSIGN = Y[train_idx] plt.scatter(ASSIGN, ASSIGN) plt.show() ASSIGN = make_poly(Xtrain, deg) ASSIGN = fit(Xtrain_poly, Ytrain) ASSIGN = make_poly(X, deg) ASSIGN = X_poly.dot(w) plt.plot(X, Y) plt.plot(X, ASSIGN) plt.scatter(ASSIGN, ASSIGN) plt.title(""deg = %d"" %deg) plt.show()",1,"['def make_poly(X, deg):\n', '    n = len(X)\n', '    data = [np.ones(n)]\n', '    for d in range(deg):\n', '        data.append(X**(d+1))\n', '    return np.vstack(data).T\n', '\n', 'def fit(X, Y):\n', '    return np.linalg.solve(X.T.dot(X), X.T.dot(Y))\n', '\n', 'def fit_and_display(X, Y, sample, deg):\n', '    N = len(X)\n', '    train_idx = np.random.choice(N, sample)\n', '    Xtrain = X[train_idx]\n', '    Ytrain = Y[train_idx]\n', '    \n', '    plt.scatter(Xtrain, Ytrain)\n', '    plt.show()\n', '    \n', '    #fit poly\n', '    Xtrain_poly = make_poly(Xtrain, deg)\n', '    w = fit(Xtrain_poly, Ytrain)\n', '    \n', '    # display the polynomial\n', '    X_poly = make_poly(X, deg)\n', '    Y_hat = X_poly.dot(w)\n', '    plt.plot(X, Y)\n', '    plt.plot(X, Y_hat)\n', '    plt.scatter(Xtrain, Ytrain)\n', '    plt.title(""deg = %d"" %deg)\n', '    plt.show()\n', '    \n', '#for deg in range(5,10):\n', ' #   fit_and_display(X, Y, 10, deg)']"
"KNN.fit(x_train,x_test) ASSIGN = KNN.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('KNN(No HT).csv',index = False)",0,"['KNN.fit(x_train,x_test)\n', 'model10pred = KNN.predict(y_train)\n', ""submission10 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission10['PassengerId'] = result['PassengerId']\n"", ""submission10['Survived'] = model10pred\n"", ""submission10.to_csv('KNN(No HT).csv',index = False)""]"
"ASSIGN=plt.subplots(2,2,figsize=(20,20)) ASSIGN=data_traf.year.value_counts() ASSIGN=data_traf.year.value_counts().index sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,0]) ax[0,0].set_title(""The number of traffic-accident by year"",size=20) ax[0,0].set_ylabel('counts',size=18) ax[0,0].set_xlabel('') ASSIGN=data_traf.month.value_counts() ASSIGN=data_traf.month.value_counts().index sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,1]) ax[0,1].set_title(""The number of traffic-accident by month"",size=20) ax[0,1].set_ylabel('counts',size=18) ax[0,1].set_xlabel('') ASSIGN=data_traf.hour.value_counts() ASSIGN=data_traf.hour.value_counts().index sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[1,0]) ax[1,0].set_title(""The number of traffic-accident by hour"",size=20) ax[1,0].set_ylabel('counts',size=18) ax[1,0].set_xlabel('') sns.scatterplot(ASSIGN=""GEO_LON"", ASSIGN=""GEO_LAT"", hue=""NEIGHBORHOOD_ID"",data=data_traf,ax=ax[1,1]) ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,2.5)) ax[1,1].set_title(""The distribution of traffic-accident"",size=20) ax[1,1].set(xlabel='Longitude', ylabel='LATITUDE')",1,"['fig,ax=plt.subplots(2,2,figsize=(20,20))\n', 'y=data_traf.year.value_counts()\n', 'x=data_traf.year.value_counts().index\n', 'sns.barplot(x=x,y=y,ax=ax[0,0])\n', 'ax[0,0].set_title(""The number of traffic-accident by year"",size=20)\n', ""ax[0,0].set_ylabel('counts',size=18)\n"", ""ax[0,0].set_xlabel('')\n"", '\n', '\n', 'y=data_traf.month.value_counts()\n', 'x=data_traf.month.value_counts().index\n', 'sns.barplot(x=x,y=y,ax=ax[0,1])\n', 'ax[0,1].set_title(""The number of traffic-accident by month"",size=20)\n', ""ax[0,1].set_ylabel('counts',size=18)\n"", ""ax[0,1].set_xlabel('')\n"", '\n', 'y=data_traf.hour.value_counts()\n', 'x=data_traf.hour.value_counts().index\n', 'sns.barplot(x=x,y=y,ax=ax[1,0])\n', 'ax[1,0].set_title(""The number of traffic-accident by hour"",size=20)\n', ""ax[1,0].set_ylabel('counts',size=18)\n"", ""ax[1,0].set_xlabel('')\n"", '\n', '\n', 'sns.scatterplot(x=""GEO_LON"", y=""GEO_LAT"", hue=""NEIGHBORHOOD_ID"",data=data_traf,ax=ax[1,1])\n', 'ax[1,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,2.5))\n', 'ax[1,1].set_title(""The distribution of traffic-accident"",size=20)\n', ""ax[1,1].set(xlabel='Longitude', ylabel='LATITUDE')""]"
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'credits.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,"[""nRowsRead = 1000 # specify 'None' if want to read whole file\n"", '# credits.csv has 45476 rows in reality, but we are only loading/previewing the first 1000 rows\n', ""df1 = pd.read_csv('../input/credits.csv', delimiter=',', nrows = nRowsRead)\n"", ""df1.dataframeName = 'credits.csv'\n"", 'nRow, nCol = df1.shape\n', ""print(f'There are {nRow} rows and {nCol} columns')""]"
"sns.countplot(x = 'Title', data = df3)",1,"[""sns.countplot(x = 'Title', data = df3)""]"
ASSIGN=pd.read_csv('..path') ASSIGN=pd.read_csv('..path') ASSIGN.head(),0,"[""train_id=pd.read_csv('../input/train_identity.csv')\n"", ""test_id=pd.read_csv('../input/test_identity.csv')\n"", 'train_id.head()\n', '#print(train_id.describe())\n', '#print(train_id.isnull().sum())']"
"SETUP CHECKPOINT ASSIGN = [2,3,5,7],[73,37,19,17],[101,111,153,59] ASSIGN = np.array(b) print(ASSIGN)",0,"['#2D array\n', 'import numpy as np\n', 'b = [2,3,5,7],[73,37,19,17],[101,111,153,59]\n', 'c = np.array(b)\n', 'print(c)\n']"
train_transaction_new.ProductCD[:5],0,['train_transaction_new.ProductCD[:5]']
"CHECKPOINT ASSIGN = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True) ASSIGN = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True) print(+str(ASSIGN.shape)) print(+str(ASSIGN.shape))",0,"[""train_df = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\n"", ""test_df = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n"", '\n', 'print(""Train shape : ""+str(train_df.shape))\n', 'print(""Test shape  : ""+str(test_df.shape))']"
"ASSIGN = kickstarters_2017.ASSIGN ASSIGN = minmax_scaling(goal, columns = [0]) ASSIGN=plt.subplots(1,2) sns.distplot(kickstarters_2017.ASSIGN, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Scaled data"")",1,"['# Your turn! \n', '\n', '# We just scaled the ""usd_goal_real"" column. What about the ""goal"" column?\n', '\n', '# select the usd_goal_real column\n', 'goal = kickstarters_2017.goal\n', '\n', '# scale the goals from 0 to 1\n', 'scaled_data = minmax_scaling(goal, columns = [0])\n', '\n', '# plot the original & scaled data together to compare\n', 'fig, ax=plt.subplots(1,2)\n', 'sns.distplot(kickstarters_2017.goal, ax=ax[0])\n', 'ax[0].set_title(""Original Data"")\n', 'sns.distplot(scaled_data, ax=ax[1])\n', 'ax[1].set_title(""Scaled data"")\n']"
df_all['age'] = df_all['age'].apply(lambda x: remove_age_outliers(x) if(not np.isnan(x)) else x),0,"[""df_all['age'] = df_all['age'].apply(lambda x: remove_age_outliers(x) if(not np.isnan(x)) else x)""]"
"ASSIGN = [""MSSubClass"",'MoSold','YrSold'] for var in ASSIGN: data_features[var] = data_features[var].apply(str)",0,"['#Some number features stand for categories.\n', 'str_var = [""MSSubClass"",\'MoSold\',\'YrSold\']\n', 'for var in str_var:\n', '    data_features[var] = data_features[var].apply(str)']"
SETUP ASSIGN = load_breast_cancer() ASSIGN.keys(),0,"['from sklearn.datasets import load_breast_cancer\n', 'cancer_data = load_breast_cancer()\n', 'cancer_data.keys()']"
"XGB.fit(x_train,x_test) XGB.score(y_train,y_test)",0,"['XGB.fit(x_train,x_test)\n', 'XGB.score(y_train,y_test)']"
"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,shuffle = False)",0,"['X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,shuffle = False)']"
"ASSIGN = model.get_layer(""dense_3"").ASSIGN ASSIGN = Model(model.input, output)",0,"['#getting the 2D output:\n', 'output = model.get_layer(""dense_3"").output\n', 'extr = Model(model.input, output)']"
os.listdir('..path'),0,"[""os.listdir('../input/ireland-historical-news')""]"
tumor_data['Unnamed: 32'].sample(8),0,"[""tumor_data['Unnamed: 32'].sample(8)""]"
"plotCorrelationMatrix(df3, 8)",1,"['plotCorrelationMatrix(df3, 8)']"
"data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageYrBlt'].notnull()), ['GarageYrBlt']]",0,"[""data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageYrBlt'].notnull()), ['GarageYrBlt']]""]"
CHECKPOINT ASSIGN = logisticRegr.predict(holdout_pca) predict_holdout,0,"['predict_holdout = logisticRegr.predict(holdout_pca)\n', 'predict_holdout']"
"ASSIGN = train.iloc[:,1:].values ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], 28, 28, 1) ASSIGN = X_trainpath ASSIGN = train[""label""].values",0,"['X_train = train.iloc[:,1:].values\n', 'X_train = X_train.reshape(X_train.shape[0], 28, 28, 1) #reshape to rectangular\n', 'X_train = X_train/255 #pixel values are 0 - 255 - this makes puts them in the range 0 - 1\n', '\n', 'y_train = train[""label""].values']"
SETUP ASSIGN = '.path',0,"['import tensorflow as tf\n', 'from tensorflow.keras.models import Sequential, save_model, load_model\n', '\n', ""filepath = './'""]"
"ASSIGN = create_cnn(data, models.resnet34, metrics=error_rate)",0,"['learn = create_cnn(data, models.resnet34, metrics=error_rate)']"
"ASSIGN=[] ASSIGN=train.image_id for file in ASSIGN: ASSIGN=cv2.imread(""..path""+file) ASSIGN=cv2.resize(image,(256,256)) ASSIGN.append(ASSIGN) ASSIGN=np.array(ASSIGN)",0,"['img=[]\n', 'filename=train.image_id\n', 'for file in filename:\n', '    image=cv2.imread(""../input/plant-pathology-2020-fgvc7/images/""+file)\n', '    res=cv2.resize(image,(256,256))\n', '    img.append(res)\n', 'img=np.array(img)\n']"
SETUP,0,"['%matplotlib inline\n', 'import numpy as np \n', 'import pandas as pd \n', 'from sklearn.model_selection import KFold\n', 'from sklearn.preprocessing import LabelEncoder\n', 'from lightgbm import LGBMClassifier, LGBMRegressor\n', 'from sklearn.model_selection import cross_validate\n', 'from tqdm import tqdm_notebook\n', 'import multiprocessing\n', 'import matplotlib.pyplot as plt # drawing graph\n', 'import warnings; warnings.filterwarnings(""ignore"") \n', ""import os; os.environ['OMP_NUM_THREADS'] = '4' # speed up using 4 cpu""]"
"ASSIGN=pd.read_csv(""..path"") ASSIGN.head()",0,"['data=pd.read_csv(""../input/google-play-store-apps/googleplaystore.csv"")\n', 'data.head()']"
"Support Vector Machines ASSIGN = SVC() ASSIGN.fit(X_train, Y_train) ASSIGN = svc.predict(X_test) ASSIGN.score(X_train, Y_train)",0,"['Support Vector Machines\n', '\n', 'svc = SVC()\n', '\n', 'svc.fit(X_train, Y_train)\n', '\n', 'Y_pred = svc.predict(X_test)\n', '\n', 'svc.score(X_train, Y_train)']"
final['Fare'].isnull().sum(),0,"[""final['Fare'].isnull().sum()""]"
"ASSIGN=ASSIGN.dropna(subset=['GEO_LAT','GEO_LON']) ASSIGN.isnull().sum()",0,"[""data=data.dropna(subset=['GEO_LAT','GEO_LON'])\n"", 'data.isnull().sum()']"
CHECKPOINT final.dtypes,0,['final.dtypes']
CHECKPOINT print() print(+str(sum(Y_train==1))++str(sum(Y_train==0))+) print(+str(sum(Y_val==1))++str(sum(Y_val==0))+) print(+str(sum(Y_test==1))++str(sum(Y_test==0))+),0,"['print(""Distribution of cats and dogs in the different sets"")\n', 'print(""TRAIN  :  ""+str(sum(Y_train==1))+"" cats vs ""+str(sum(Y_train==0))+"" dogs"")\n', 'print(""VAL  :  ""+str(sum(Y_val==1))+"" cats vs ""+str(sum(Y_val==0))+"" dogs"")\n', 'print(""TEST  :  ""+str(sum(Y_test==1))+"" cats vs ""+str(sum(Y_test==0))+"" dogs"")']"
"sns.catplot(x='Year', y='Murder', data=cbdr,height = 5, aspect = 4)",1,"[""sns.catplot(x='Year', y='Murder', data=cbdr,height = 5, aspect = 4)""]"
"def plot_figures( sizes, pie_title, start_angle, bar_title, bar_ylabel, labels, explode, ASSIGN=None, ): ASSIGN = plt.subplots(figsize=(14, 14)) ASSIGN = np.arange(len(labels)) ASSIGN = ax.bar(y_pos, sizes, align='center') ax.set_xticks(ASSIGN, labels) ax.set_ylabel(bar_ylabel) ax.set_title(bar_title) if ASSIGN is not None: for idx, item in enumerate(ASSIGN): item.set_color(ASSIGN[idx]) def autolabel(rects): """""" Attach a text label above each bar displaying its height """""" for rect in rects: ASSIGN = rect.get_height() ax.text( rect.get_x() + rect.get_width()path, ASSIGN, '%d' % int(ASSIGN), ASSIGN='center', va='bottom', fontweight='bold' ) autolabel(ASSIGN) ASSIGN = plt.subplots(figsize=(14, 14)) ASSIGN = ax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=False, startangle=start_angle, counterclock=False) ax.axis('equal') ax.set_title(pie_title) if ASSIGN is not None: for idx, item in enumerate(ASSIGN[0]): item.set_color(ASSIGN[idx]) plt.show()",1,"['def plot_figures(\n', '    sizes,\n', '    pie_title,\n', '    start_angle,\n', '    bar_title,\n', '    bar_ylabel,\n', '    labels,\n', '    explode,\n', '    colors=None,\n', '):\n', '    fig, ax = plt.subplots(figsize=(14, 14))\n', '\n', '    y_pos = np.arange(len(labels))\n', ""    barlist = ax.bar(y_pos, sizes, align='center')\n"", '    ax.set_xticks(y_pos, labels)\n', '    ax.set_ylabel(bar_ylabel)\n', '    ax.set_title(bar_title)\n', '    if colors is not None:\n', '        for idx, item in enumerate(barlist):\n', '            item.set_color(colors[idx])\n', '\n', '    def autolabel(rects):\n', '        """"""\n', '        Attach a text label above each bar displaying its height\n', '        """"""\n', '        for rect in rects:\n', '            height = rect.get_height()\n', '            ax.text(\n', '                rect.get_x() + rect.get_width()/2., height,\n', ""                '%d' % int(height),\n"", ""                ha='center', va='bottom', fontweight='bold'\n"", '            )\n', '\n', '    autolabel(barlist)\n', '    \n', '    fig, ax = plt.subplots(figsize=(14, 14))\n', '    \n', ""    pielist = ax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=False, startangle=start_angle, counterclock=False)\n"", ""    ax.axis('equal')\n"", '    ax.set_title(pie_title)\n', '    if colors is not None:\n', '        for idx, item in enumerate(pielist[0]):\n', '            item.set_color(colors[idx])\n', '\n', '    plt.show()']"
"CHECKPOINT ASSIGN = data[""Species Name""] ASSIGN=species.value_counts() print(ASSIGN)",0,"['species = data[""Species Name""]\n', 'species_count=species.value_counts()\n', 'print(species_count)']"
"SETUP ASSIGN = plt.figure(figsize = (8,8)) plt.scatter(X_train_pca[:,0], X_train_pca[:,1], c = y_train) plt.xlabel('Principal Component 1') plt.ylabel('Principal Component 2') plt.tight_layout() plt.show()",1,"['%matplotlib inline\n', 'fig = plt.figure(figsize = (8,8))\n', 'plt.scatter(X_train_pca[:,0], X_train_pca[:,1], c = y_train)\n', ""plt.xlabel('Principal Component 1')\n"", ""plt.ylabel('Principal Component 2')\n"", 'plt.tight_layout()\n', 'plt.show()']"
"ASSIGN = pd.DataFrame(cancer_data['data'],columns=cancer_data['feature_names']) ASSIGN.head()",0,"[""CC = pd.DataFrame(cancer_data['data'],columns=cancer_data['feature_names'])\n"", 'CC.head()']"
"match_bets_df.drop('index', inplace= True, axis= 1)",0,"[""match_bets_df.drop('index', inplace= True, axis= 1)""]"
"CHECKPOINT ASSIGN = linear_model.Lasso(alpha=.01) ASSIGN.fit(x_train,Y_train) ASSIGN = model11.score(x_test,Y_test) print(ASSIGN*100,'%')",0,"['model11 = linear_model.Lasso(alpha=.01)\n', 'model11.fit(x_train,Y_train)\n', '\n', 'accuracy11 = model11.score(x_test,Y_test)\n', ""print(accuracy11*100,'%')""]"
"SLICE=irishtimes.publish_date.apply(lambda x:datetime.datetime.strptime(str(x),'%Y%m%d').strftime('%Y-%m-%d')) SLICE=irishtimes.date.apply(lambda x:x.split('-')[0]) SLICE=irishtimes.date.apply(lambda x:x.split('-')[1]) SLICE=irishtimes.date.apply(lambda x:x.split('-')[2]) irishtimes.head()",0,"[""irishtimes['date']=irishtimes.publish_date.apply(lambda x:datetime.datetime.strptime(str(x),'%Y%m%d').strftime('%Y-%m-%d'))\n"", ""irishtimes['year']=irishtimes.date.apply(lambda x:x.split('-')[0])\n"", ""irishtimes['month']=irishtimes.date.apply(lambda x:x.split('-')[1])\n"", ""irishtimes['day']=irishtimes.date.apply(lambda x:x.split('-')[2])\n"", 'irishtimes.head()']"
"SETUP ASSIGN=data[['neighbourhood_group','neighbourhood','room_type','minimum_nights','number_of_reviews','price']] ASSIGN = LabelEncoder() ASSIGN['neighbourhood_group_new'] = ASSIGN.fit_transform(ASSIGN['neighbourhood_group']) ASSIGN['neighbourhood_new'] = ASSIGN.fit_transform(ASSIGN['neighbourhood']) ASSIGN['room_type_new'] = ASSIGN.fit_transform(ASSIGN['room_type']) ASSIGN.head()",0,"[""data_tree=data[['neighbourhood_group','neighbourhood','room_type','minimum_nights','number_of_reviews','price']]\n"", 'from sklearn.preprocessing import LabelEncoder\n', 'labelencoder = LabelEncoder()\n', ""data_tree['neighbourhood_group_new'] = labelencoder.fit_transform(data_tree['neighbourhood_group'])\n"", ""data_tree['neighbourhood_new'] = labelencoder.fit_transform(data_tree['neighbourhood'])\n"", ""data_tree['room_type_new'] = labelencoder.fit_transform(data_tree['room_type'])\n"", 'data_tree.head()']"
zomato_orgnl.isnull().sum(),0,['zomato_orgnl.isnull().sum()']
CHECKPOINT print(y_train.shape) print(y_test.shape) print(num_classes),0,"['print(y_train.shape)\n', 'print(y_test.shape)\n', 'print(num_classes)']"
"ASSIGN = kickstarters_2017.usd_goal_real ASSIGN = minmax_scaling(usd_goal, columns = [0]) ASSIGN=plt.subplots(1,2) sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Scaled data"")",1,"['# select the usd_goal_real column\n', 'usd_goal = kickstarters_2017.usd_goal_real\n', '\n', '# scale the goals from 0 to 1\n', 'scaled_data = minmax_scaling(usd_goal, columns = [0])\n', '\n', '# plot the original & scaled data together to compare\n', 'fig, ax=plt.subplots(1,2)\n', 'sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0])\n', 'ax[0].set_title(""Original Data"")\n', 'sns.distplot(scaled_data, ax=ax[1])\n', 'ax[1].set_title(""Scaled data"")']"
"original_train.Date = original_train.Date.apply(lambda x:get_dt(x)) original_train.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True) for i in range(len(original_train)): if original_train.Province[i] is np.NaN: original_train.Province[i] = original_train.Country[i] for i in range(len(original_train)): if original_train.Date[i]<datetime.strptime('2020-04-02','%Y-%m-%d') or original_train.Date[i]>datetime.strptime('2020-04-25','%Y-%m-%d'): original_train.drop(i,inplace=True) del original_train['Id'] original_train.Province.replace('Cote d\'Ivoire','Cote d Ivoire',inplace=True) original_train.Date = original_train.Date.apply(lambda x:get_str_date(x))",0,"['original_train.Date = original_train.Date.apply(lambda x:get_dt(x))\n', ""original_train.rename(columns = {'Province_State':'Province','Country_Region':'Country'},inplace = True)\n"", 'for i in range(len(original_train)):\n', '    if original_train.Province[i] is np.NaN:\n', '        original_train.Province[i] = original_train.Country[i]\n', '        \n', 'for i in range(len(original_train)):\n', ""    if original_train.Date[i]<datetime.strptime('2020-04-02','%Y-%m-%d') or original_train.Date[i]>datetime.strptime('2020-04-25','%Y-%m-%d'):\n"", '        original_train.drop(i,inplace=True)\n', '        \n', ""del original_train['Id']\n"", ""#del original_train['Country']\n"", '\n', ""original_train.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n"", ""#del pred_table['Country']\n"", 'original_train.Date = original_train.Date.apply(lambda x:get_str_date(x))']"
"CHECKPOINT ASSIGN = model10.predict(x_test) ASSIGN = mean_squared_error(Y_test, y_pred10, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, ASSIGN)))",0,"['y_pred10 = model10.predict(x_test)\n', '\n', 'val = mean_squared_error(Y_test, y_pred10, squared=False)\n', 'val10 = str(round(val, 4))\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred10)))\n""]"
"ASSIGN = ASSIGN.reshape(1,224,224,3)",0,"['test_x = test_x.reshape(1,224,224,3)']"
"SETUP CHECKPOINT ASSIGN = np.array([1,1,.5,0,0.66], dtype=bool) print(ASSIGN)",0,"['#boolean array\n', 'import numpy as np\n', 'd = np.array([1,1,.5,0,0.66], dtype=bool)\n', 'print(d)']"
"CHECKPOINT ASSIGN = 20 for epoch in range(1, 2): ASSIGN == 31: print() scheduler.step() train(model, epoch) test(model, epoch)",0,"['log_interval = 20\n', 'for epoch in range(1, 2): # use 41\n', '    if epoch == 31:\n', '        print(""First round of training complete. Setting learn rate to 0.001."")\n', '    scheduler.step()\n', '    train(model, epoch)\n', '    test(model, epoch)']"
"ASSIGN=pd.read_csv(""..path"") ASSIGN.head() ASSIGN=ASSIGN[ASSIGN['total_cars']>0]",0,"['data=pd.read_csv(""../input/autotel-shared-car-locations/sample_table.csv"")\n', 'data.head()\n', ""data=data[data['total_cars']>0]""]"
"CHECKPOINT ASSIGN=pd.read_csv(""..path"",encoding=""gbk"") ASSIGN=ASSIGN.loc[(ASSIGN['Lat']>35)&(ASSIGN['Long']< -60)] ASSIGN=ASSIGN.dropna(subset=[""STREET""]) ASSIGN=['OFFENSE_CODE_GROUP'] for j in ASSIGN: print(j,ASSIGN[j].unique())",0,"['data=pd.read_csv(""../input/crimes-in-boston/crime.csv"",encoding=""gbk"")\n', ""data=data.loc[(data['Lat']>35)&(data['Long']< -60)] #remove NA from 'Lat' and 'Long'\n"", 'data=data.dropna(subset=[""STREET""])\n', ""columns=['OFFENSE_CODE_GROUP']\n"", 'for j in columns:\n', '\tprint(j,data[j].unique())\n', '\n']"
"SETUP ASSIGN=[87,83,86,90,88] ASSIGN=['English','Maths','Science','History','Geography'] plt.pie(ASSIGN,labels=ASSIGN,startangle=90,shadow=True,explode=(0.08,0.5,0.08,0.08,0.08),autopct='%1.1f%%')",1,"['#Q3.A student has got the following marks ( English = 86, Maths = 83, Science = 86, History =90, Geography = 88). Wisely choose a graph to represent this data such that it justifies the purpose of data visualization. Highlight the subject in which the student has got least marks. \n', 'from matplotlib import pyplot as plt\n', 'slices=[87,83,86,90,88]\n', ""Subject=['English','Maths','Science','History','Geography']\n"", ""plt.pie(slices,labels=Subject,startangle=90,shadow=True,explode=(0.08,0.5,0.08,0.08,0.08),autopct='%1.1f%%')""]"
"GBC.fit(x_train,x_test) ASSIGN = GBC.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('GradientBoosting(No HT).csv',index = False)",0,"['GBC.fit(x_train,x_test)\n', 'model8pred = GBC.predict(y_train)\n', ""submission8 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission8['PassengerId'] = result['PassengerId']\n"", ""submission8['Survived'] = model8pred\n"", ""submission8.to_csv('GradientBoosting(No HT).csv',index = False)""]"
"CHECKPOINT ASSIGN=os.listdir(""..path"") dirs",0,"['#list all the directories\n', 'dirs=os.listdir(""../input/zomato_data/"")\n', 'dirs']"
"ASSIGN = pd.read_csv(""covid_19_data.csv"") ASSIGN.head()",0,"['covid_data = pd.read_csv(""covid_19_data.csv"")\n', 'covid_data.head()']"
ASSIGN = pd.DataFrame() ASSIGN['Id'] = test_df['Id'],0,"['my_submission = pd.DataFrame()\n', ""my_submission['Id'] = test_df['Id']""]"
"ASSIGN = [(x path(second_Digit)) for x in second_Digit] ASSIGN = plt.figure() ASSIGN = fig.add_axes([0,0,1,1]) ASSIGN = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] ASSIGN = np.arange(len(index)) plt.xticks(ASSIGN, ASSIGN) ASSIGN.plot(ASSIGN, Benford_percentiles['Second Digit Expected']) ASSIGN.plot(ASSIGN, ASSIGN) plt.title('Second Digit; Expected values are in blue, actual are in orange') plt.show()",1,"['second_Digit_percentile = [(x / sum(second_Digit)) for x in second_Digit]\n', 'fig = plt.figure()\n', 'ax = fig.add_axes([0,0,1,1])\n', ""index = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"", 'x = np.arange(len(index))\n', 'plt.xticks(x, index)\n', ""ax.plot(x, Benford_percentiles['Second Digit Expected'])\n"", 'ax.plot(x, second_Digit_percentile)\n', ""plt.title('Second Digit; Expected values are in blue, actual are in orange')\n"", 'plt.show()']"
"ASSIGN=plt.subplots(2,2,figsize=(20,20)) ASSIGN=train_identity_new.id_36.value_counts().index ASSIGN=train_identity_new.id_36.value_counts() sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,0],order=['T','F']) ax[0,0].set_title(""Bar chart for id_36"",size=20) ax[0,0].set_xlabel('counts',size=18) ax[0,0].set_ylabel('') ASSIGN=train_identity_new.id_35.value_counts().index ASSIGN=train_identity_new.id_35.value_counts() sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,1],order=['T','F']) ax[0,1].set_title(""Bar chart for id_35"",size=20) ax[0,1].set_xlabel('counts',size=18) ax[0,1].set_ylabel('') ASSIGN=train_identity_new.id_15.value_counts().index ASSIGN=train_identity_new.id_15.value_counts() sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[1,0]) ax[1,0].set_title(""Bar chart for id_15"",size=20) ax[1,0].set_xlabel('counts',size=18) ax[1,0].set_ylabel('') ASSIGN=train_identity_new.id_29.value_counts().index ASSIGN=train_identity_new.id_29.value_counts() sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[1,1]) ax[1,1].set_title(""Bar chart for id_29"",size=20) ax[1,1].set_xlabel('counts',size=18) ax[1,1].set_ylabel('')",1,"['fig,ax=plt.subplots(2,2,figsize=(20,20))\n', 'y=train_identity_new.id_36.value_counts().index\n', 'x=train_identity_new.id_36.value_counts()\n', ""sns.barplot(x=x,y=y,ax=ax[0,0],order=['T','F'])\n"", 'ax[0,0].set_title(""Bar chart for id_36"",size=20)\n', ""ax[0,0].set_xlabel('counts',size=18)\n"", ""ax[0,0].set_ylabel('')\n"", '\n', 'y=train_identity_new.id_35.value_counts().index\n', 'x=train_identity_new.id_35.value_counts()\n', ""sns.barplot(x=x,y=y,ax=ax[0,1],order=['T','F'])\n"", 'ax[0,1].set_title(""Bar chart for id_35"",size=20)\n', ""ax[0,1].set_xlabel('counts',size=18)\n"", ""ax[0,1].set_ylabel('')\n"", '\n', 'y=train_identity_new.id_15.value_counts().index\n', 'x=train_identity_new.id_15.value_counts()\n', 'sns.barplot(x=x,y=y,ax=ax[1,0])\n', 'ax[1,0].set_title(""Bar chart for id_15"",size=20)\n', ""ax[1,0].set_xlabel('counts',size=18)\n"", ""ax[1,0].set_ylabel('')\n"", '\n', 'y=train_identity_new.id_29.value_counts().index\n', 'x=train_identity_new.id_29.value_counts()\n', 'sns.barplot(x=x,y=y,ax=ax[1,1])\n', 'ax[1,1].set_title(""Bar chart for id_29"",size=20)\n', ""ax[1,1].set_xlabel('counts',size=18)\n"", ""ax[1,1].set_ylabel('')""]"
len(test_transaction),0,['len(test_transaction)']
"for i in range(200): _plot(model=model, cam_func=grad_cam, img=Images[i], cls_true=Classes[i])",1,"['for i in range(200):\n', '\n', '    _plot(model=model, cam_func=grad_cam, img=Images[i], cls_true=Classes[i])\n']"
"train_df2.Country.replace('Cote d\'Ivoire','Cote d Ivoire',inplace=True) train_df2.Province.replace('Cote d\'Ivoire','Cote d Ivoire',inplace=True) train_df2.replace(np.inf,0,inplace=True) train_df2.UrbanPopRate = train_df2.UrbanPopRate.apply(lambda x:get_percent(x)) test_df.Country.replace('Cote d\'Ivoire','Cote d Ivoire',inplace=True) test_df.Province.replace('Cote d\'Ivoire','Cote d Ivoire',inplace=True) test_df.replace(np.inf,0,inplace=True) test_df.UrbanPopRate = test_df.UrbanPopRate.apply(lambda x:get_percent(x))",0,"['# same preprocess as before\n', ""train_df2.Country.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n"", ""train_df2.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n"", 'train_df2.replace(np.inf,0,inplace=True)\n', 'train_df2.UrbanPopRate = train_df2.UrbanPopRate.apply(lambda x:get_percent(x))\n', '\n', ""test_df.Country.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n"", ""test_df.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n"", 'test_df.replace(np.inf,0,inplace=True)\n', 'test_df.UrbanPopRate = test_df.UrbanPopRate.apply(lambda x:get_percent(x))']"
"ASSIGN = tabular_learner(data, layers=[200,100],metrics=accuracy)",0,"['learn = tabular_learner(data, layers=[200,100],metrics=accuracy)']"
"SETUP CHECKPOINT tf.keras.backend.set_learning_phase(True) ASSIGN = 4 ASSIGN = 0.001 ASSIGN = 200 ASSIGN = tf.keras.optimizers.Adam(learning_rate=learning_rate, decay=learning_rate path) ASSIGN = get_model_train_step_function( detection_model, ASSIGN, to_fine_tune) print('Start fine-tuning!', flush=True) for idx in range(ASSIGN): ASSIGN = list(range(len(train_images_np))) random.shuffle(ASSIGN) ASSIGN = all_keys[:batch_size] ASSIGN = [gt_box_tensors[key] for key in example_keys] ASSIGN = [gt_classes_one_hot_tensors[key] for key in example_keys] ASSIGN = [train_image_tensors[key] for key in example_keys] ASSIGN = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list) if idx % 10 == 0: print('batch ' + str(idx) + ' of ' + str(ASSIGN) + ', loss=' + str(ASSIGN.numpy()), flush=True) print('Done fine-tuning!')",0,"['%%time\n', 'tf.keras.backend.set_learning_phase(True)\n', '\n', '# These parameters can be tuned; since our training set has 5 images\n', ""# it doesn't make sense to have a much larger batch size, though we could\n"", '# fit more examples in memory if we wanted to.\n', 'batch_size = 4\n', 'learning_rate = 0.001\n', 'num_batches = 200\n', '\n', '# optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n', 'optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,  decay=learning_rate / num_batches)\n', 'train_step_fn = get_model_train_step_function(\n', '    detection_model, optimizer, to_fine_tune)\n', '\n', '\n', ""print('Start fine-tuning!', flush=True)\n"", 'for idx in range(num_batches):\n', '  # Grab keys for a random subset of examples\n', '  all_keys = list(range(len(train_images_np)))\n', '  random.shuffle(all_keys)\n', '  example_keys = all_keys[:batch_size]\n', '\n', '  # Note that we do not do data augmentation in this demo.  If you want a\n', '  # a fun exercise, we recommend experimenting with random horizontal flipping\n', '  # and random cropping :)\n', '  gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n', '  gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n', '  image_tensors = [train_image_tensors[key] for key in example_keys]\n', '\n', '  # Training step (forward pass + backwards pass)\n', '  total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n', '\n', '  if idx % 10 == 0:\n', ""    print('batch ' + str(idx) + ' of ' + str(num_batches)\n"", ""    + ', loss=' +  str(total_loss.numpy()), flush=True)\n"", '\n', ""print('Done fine-tuning!')""]"
ASSIGN = pd.read_csv('..path'),0,"[""df  = pd.read_csv('../input/quora-insincere-questions-classification/train.csv')""]"
"ASSIGN = [(x path(third_Digit)) for x in third_Digit] ASSIGN = plt.figure() ASSIGN = fig.add_axes([0,0,1,1]) ASSIGN = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] ASSIGN = np.arange(len(index)) plt.xticks(ASSIGN, ASSIGN) ASSIGN.plot(ASSIGN, Benford_percentiles['Third Digit Expected']) ASSIGN.plot(ASSIGN, ASSIGN) plt.title('Third Digit; Expected values are in blue, actual are in orange') plt.show()",1,"['third_Digit_percentile = [(x / sum(third_Digit)) for x in third_Digit]\n', 'fig = plt.figure()\n', 'ax = fig.add_axes([0,0,1,1])\n', ""index = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"", 'x = np.arange(len(index))\n', 'plt.xticks(x, index)\n', ""ax.plot(x, Benford_percentiles['Third Digit Expected'])\n"", 'ax.plot(x, third_Digit_percentile)\n', ""plt.title('Third Digit; Expected values are in blue, actual are in orange')\n"", 'plt.show()']"
"X_TRAIN_FILE=""path"" X_TEST_FILE=""path"" Y_TRAIN_FILE=""path"" Y_TEST_FILE=""path""",0,"['X_TRAIN_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_X_train.npz""\n', 'X_TEST_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_X_test.npy""\n', 'Y_TRAIN_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_Y_train.npz""\n', 'Y_TEST_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_Y_test.npy""']"
test_transaction.head(),0,['test_transaction.head()']
"ASSIGN = data_features.loc[data_features['MiscFeature'].notnull(),'MiscFeature'] data_features.loc[data_features['MiscFeature'].notnull(),'MiscFeature'].value_counts()",0,"['#The code below just for a simple look\n', ""a = data_features.loc[data_features['MiscFeature'].notnull(),'MiscFeature']\n"", ""data_features.loc[data_features['MiscFeature'].notnull(),'MiscFeature'].value_counts()""]"
CHECKPOINT SETUP 3,0,"['3\n', 'import numpy as np']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
"ASSIGN = knn.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('KNN(StdScaler).csv',index = False)",0,"['model19pred = knn.predict(y_train)\n', ""submission19 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission19['PassengerId'] = result['PassengerId']\n"", ""submission19['Survived'] = model19pred\n"", ""submission19.to_csv('KNN(StdScaler).csv',index = False)""]"
data.isnull().sum(),0,['data.isnull().sum()']
"ASSIGN = VotingClassifier(estimators = Estimator, voting ='soft') ASSIGN.fit(x_train, x_test) ASSIGN = vot_soft1.predict(y_train) ASSIGN.score(y_train,y_test) ASSIGN = vot_soft1.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('SoftVoting(HT).csv',index = False)",0,"[""vot_soft1 = VotingClassifier(estimators = Estimator, voting ='soft') \n"", 'vot_soft1.fit(x_train, x_test) \n', 'y_pred = vot_soft1.predict(y_train)\n', 'vot_soft1.score(y_train,y_test)\n', '\n', 'modelpred3 = vot_soft1.predict(y_train)\n', ""sub3 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""sub3['PassengerId'] = result['PassengerId']\n"", ""sub3['Survived'] = modelpred3\n"", ""sub3.to_csv('SoftVoting(HT).csv',index = False)""]"
SLICE=data.timestamp.apply(lambda x:x.split(' ')[0]) pd.to_datetime(data['date']),0,"[""data['date']=data.timestamp.apply(lambda x:x.split(' ')[0])\n"", ""pd.to_datetime(data['date'])""]"
"ASSIGN=china[china['date']==max(china['date'])].sort_values('Confirmed',ascending=False)[:10] ASSIGN=ASSIGN.drop(columns=['date']) ASSIGN['Recovered rate']=ASSIGN['Recovered']path['Confirmed'] ASSIGN.style.background_gradient(cmap='Reds')",0,"[""china_table=china[china['date']==max(china['date'])].sort_values('Confirmed',ascending=False)[:10]\n"", ""china_table=china_table.drop(columns=['date'])\n"", ""china_table['Recovered rate']=china_table['Recovered']/china_table['Confirmed']\n"", ""china_table.style.background_gradient(cmap='Reds')""]"
SETUP,0,"['\n', 'import os\n', 'from glob import glob\n', 'import re\n', 'import ast\n', 'import numpy as np \n', 'import pandas as pd\n', 'from PIL import Image, ImageDraw \n', 'from tqdm import tqdm\n', 'from dask import bag\n', '\n', 'import tensorflow as tf\n', 'from tensorflow import keras\n', 'from tensorflow.keras.models import Sequential\n', 'from keras.utils import to_categorical\n', 'from tensorflow.keras.layers import Dense, Dropout, Flatten\n', 'from tensorflow.keras.layers import Conv2D, MaxPooling2D\n', 'from tensorflow.keras.metrics import top_k_categorical_accuracy\n', 'from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping']"
CHECKPOINT SETUP 2,0,"['2\n', 'import matplotlib.pyplot as plt']"
CHECKPOINT total,0,['total']
data['target'].unique(),0,"[""data['target'].unique()""]"
CHECKPOINT def get_seed(x): return int(x[1:3]) tourney_result['WSeed'] = tourney_result['WSeed'].map(lambda x: get_seed(x)) tourney_result['LSeed'] = tourney_result['LSeed'].map(lambda x: get_seed(x)) tourney_result,0,"['def get_seed(x):\n', '    return int(x[1:3])\n', '\n', ""tourney_result['WSeed'] = tourney_result['WSeed'].map(lambda x: get_seed(x))\n"", ""tourney_result['LSeed'] = tourney_result['LSeed'].map(lambda x: get_seed(x))\n"", 'tourney_result']"
"CHECKPOINT print('The data size is',len(data))",0,"[""print('The data size is',len(data))""]"
"ASSIGN = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20] ASSIGN = Ridge() ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]} ASSIGN = GridSearchCV(ridge, parameters,scoring='neg_mean_squared_error', cv=100) ASSIGN.fit(X_train, y_train) ASSIGN.best_params_",0,"['alpha = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n', '\n', 'ridge = Ridge()\n', '\n', ""parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n"", '\n', ""ridge_regressor = GridSearchCV(ridge, parameters,scoring='neg_mean_squared_error', cv=100)\n"", '\n', 'ridge_regressor.fit(X_train, y_train)\n', 'ridge_regressor.best_params_']"
"CHECKPOINT ASSIGN = DecisionTreeRegressor() ASSIGN.fit(X_train, y_train) ASSIGN = model2.score(X_test,y_test) print(ASSIGN*100,'%')",0,"['model2 = DecisionTreeRegressor()\n', 'model2.fit(X_train, y_train)\n', '\n', 'accuracy2 = model2.score(X_test,y_test)\n', ""print(accuracy2*100,'%')""]"
"SETUP ASSIGN=tree.DecisionTreeClassifier(max_depth=3) ASSIGN=ASSIGN.fit(train_x,train_y) ASSIGN = tree.export_graphviz(dtree, ASSIGN=True, ASSIGN=list(train_x), ASSIGN=['No rain','rain'], ASSIGN=True) ASSIGN = graphviz.Source(dot_data)",1,"['import graphviz \n', 'dtree=tree.DecisionTreeClassifier(max_depth=3)\n', 'dtree=dtree.fit(train_x,train_y)\n', 'dot_data = tree.export_graphviz(dtree, \n', '                filled=True, \n', '                feature_names=list(train_x),\n', ""                class_names=['No rain','rain'],\n"", '                special_characters=True)\n', 'graph = graphviz.Source(dot_data)  \n']"
CHECKPOINT tf.shape,0,['tf.shape']
"ASSIGN = keras.utils.to_categorical(ASSIGN, n_classes) ASSIGN = keras.utils.to_categorical(ASSIGN, n_classes) y_train.shape, y_test.shape",0,"['## to one-hot\n', '\n', 'y_train = keras.utils.to_categorical(y_train, n_classes)\n', 'y_test = keras.utils.to_categorical(y_test, n_classes)\n', '\n', 'y_train.shape, y_test.shape']"
"ASSIGN=plt.subplots(1,2,figsize=(18,8)) ASSIGN = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",'cadetblue','hotpink','orange','darksalmon','brown') data.groupby(['host_name'])['number_of_reviews'].agg('sum').sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=ASSIGN,ax=ax[0]) ax[0].set_title(""Top 10 host by the number of reviews"",size=20) ax[0].set_xlabel('reviews',size=18) ax[0].set_ylabel('') data.groupby(['host_name'])['price'].agg('mean').sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=ASSIGN,ax=ax[1]) ax[1].set_title(""Top 10 host by the average of price for rooms"",size=20) ax[1].set_xlabel('average of price',size=18) ax[1].set_ylabel('')",1,"['fig,ax=plt.subplots(1,2,figsize=(18,8))\n', 'clr = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",\'cadetblue\',\'hotpink\',\'orange\',\'darksalmon\',\'brown\')\n', ""data.groupby(['host_name'])['number_of_reviews'].agg('sum').sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n"", 'ax[0].set_title(""Top 10 host by the number of reviews"",size=20)\n', ""ax[0].set_xlabel('reviews',size=18)\n"", ""ax[0].set_ylabel('')\n"", '\n', ""data.groupby(['host_name'])['price'].agg('mean').sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[1])\n"", 'ax[1].set_title(""Top 10 host by the average of price for rooms"",size=20)\n', ""ax[1].set_xlabel('average of price',size=18)\n"", ""ax[1].set_ylabel('')""]"
CHECKPOINT df_train.columns,0,['df_train.columns']
SETUP,0,"['CAT_TRAIN_PATH=""/kaggle/input/cat-and-dog/training_set/training_set/cats/""\n', 'DOG_TRAIN_PATH=""/kaggle/input/cat-and-dog/training_set/training_set/dogs/""\n', 'CAT_TEST_PATH=""/kaggle/input/cat-and-dog/test_set/test_set/cats/""\n', 'DOG_TEST_PATH=""/kaggle/input/cat-and-dog/test_set/test_set/dogs/""']"
SETUP CHECKPOINT print(os.listdir('..path')),0,"['import os\n', ""print(os.listdir('../input/naruto'))""]"
"CHECKPOINT ASSIGN=np.empty_like(X, dtype=""float64"")   # ASSIGN X standardised ASSIGN=np.empty_like(Y, dtype=""float64"")   # ASSIGN Y standardised ASSIGN = (X - Xmin)path(Xmax-Xmin) ASSIGN = (Y - Ymin)path(Ymax-Ymin) for i in range(0, X.size): print(, i, .format(ASSIGN[i]),.format(ASSIGN[i])) plt.scatter(ASSIGN,ASSIGN, 10, color = 'blue') plt.show()",1,"['X_s=np.empty_like(X, dtype=""float64"")     # X_s  X standardised\n', 'Y_s=np.empty_like(Y, dtype=""float64"")     # Y_s  Y standardised\n', '\n', 'X_s = (X - Xmin)/(Xmax-Xmin)\n', 'Y_s = (Y - Ymin)/(Ymax-Ymin)\n', '\n', '\n', 'for i in range(0, X.size):\n', '    print(""i= "", i, ""\\t\\tx {:6.2f}"".format(X_s[i]),""\\ty {:6.2f}"".format(Y_s[i]))\n', '\n', '\n', '\n', ""plt.scatter(X_s,Y_s, 10, color = 'blue')\n"", 'plt.show()']"
"ASSIGN = covid_data[covid_data['Countrypath'] == 'Germany'] ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for dat in ASSIGN['ObservationDate'].unique(): ASSIGN = germany_data[germany_data['ObservationDate'] == dat] ASSIGN = sub['Confirmed'].sum() ASSIGN = sub['Deaths'].sum() ASSIGN = sub['Recovered'].sum() ASSIGN.append(dat) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN = pd.Series(ASSIGN) ASSIGN =pd.Series(ASSIGN) ASSIGN = pd.Series(ASSIGN) ASSIGN = pd.Series(ASSIGN) ASSIGN = [date.min(), date[len(date)path], date.max()] plt.figure(figsize=(8,8)) plt.plot(ASSIGN, ASSIGN, color = 'yellow') plt.plot(ASSIGN, ASSIGN, color = 'red') plt.plot(ASSIGN, ASSIGN, color = 'green') plt.xticks(ASSIGN, ASSIGN) plt.xlabel('Date') plt.ylabel('Cummulative Count cases') plt.title('Trend Curve of Confirmed Cases in Germany') plt.legend(['Confirmed', 'Death', 'Recovered']) plt.show()",1,"[""germany_data = covid_data[covid_data['Country/Region'] == 'Germany']\n"", 'date = []\n', 'c = []\n', 'd = []\n', 'r = []\n', ""for dat in germany_data['ObservationDate'].unique():\n"", ""    sub = germany_data[germany_data['ObservationDate'] == dat]\n"", ""    confirm = sub['Confirmed'].sum()\n"", ""    death = sub['Deaths'].sum()\n"", ""    recover = sub['Recovered'].sum()\n"", '    date.append(dat)\n', '    c.append(confirm)\n', '    d.append(death)\n', '    r.append(recover)\n', '\n', 'date = pd.Series(date)\n', 'c  =pd.Series(c)\n', 'd = pd.Series(d)\n', 'r = pd.Series(r)\n', '\n', 't = [date.min(), date[len(date)//2], date.max()]\n', 'plt.figure(figsize=(8,8))\n', ""plt.plot(date, c, color = 'yellow')\n"", ""plt.plot(date, d, color = 'red')\n"", ""plt.plot(date, r, color = 'green')\n"", 'plt.xticks(t, t)\n', ""plt.xlabel('Date')\n"", ""plt.ylabel('Cummulative Count cases')\n"", ""plt.title('Trend Curve of Confirmed Cases in Germany')\n"", ""plt.legend(['Confirmed', 'Death', 'Recovered'])\n"", 'plt.show()']"
df_all.age = df_all.age.astype(int) df_all.sample(n=5),0,"['df_all.age = df_all.age.astype(int)\n', 'df_all.sample(n=5)']"
"plt.figure(figsize=(15,15)) ASSIGN = data.ASSIGN() sns.heatmap(ASSIGN, xticklabels=ASSIGN.columns,yticklabels=ASSIGN.columns) plt.title(""correlation plot"",size=28)",1,"['plt.figure(figsize=(15,15))\n', 'corr = data.corr()\n', 'sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns)\n', 'plt.title(""correlation plot"",size=28)']"
"df_all.drop('timestamp_first_active', axis=1, inplace=True) df_all.drop('language', axis=1, inplace=True)",0,"[""df_all.drop('timestamp_first_active', axis=1, inplace=True)\n"", ""df_all.drop('language', axis=1, inplace=True)""]"
"def remove_age_outliers(x, min_value=15, max_value=90): if np.logical_or(x<=min_value, x>=max_value): return np.nan else: return x",0,"['def remove_age_outliers(x, min_value=15, max_value=90):\n', '    if np.logical_or(x<=min_value, x>=max_value):\n', '        return np.nan\n', '    else:\n', '        return x']"
CHECKPOINT SETUP 4,0,"['4\n', 'import pandas as pd\n', 'import numpy as np']"
"sns.catplot(x = 'Embarked',y='Survived',kind = 'point', data = df3, hue = 'Pclass', col = 'Pclass')",1,"[""sns.catplot(x = 'Embarked',y='Survived',kind = 'point', data = df3, hue = 'Pclass', col = 'Pclass')""]"
"CHECKPOINT print(classification_report(Y_test, Y_predicted))",0,"['print(classification_report(Y_test, Y_predicted))']"
"CHECKPOINT tumor_data.drop(['id'], axis = 1, inplace=True) tumor_data.columns",0,"[""tumor_data.drop(['id'], axis = 1, inplace=True)\n"", 'tumor_data.columns']"
"sns.catplot(x='Year', y='Arson', data=total ,height = 5, aspect = 4,kind = 'bar')",1,"[""sns.catplot(x='Year', y='Arson', data=total ,height = 5, aspect = 4,kind = 'bar')""]"
"ASSIGN = output2.groupby(['Models after Hyperparameter Tuning'])['Accuracy after HT'].mean().reset_index().sort_values(by='Accuracy after HT',ascending=False) ASSIGN.head(10).style.background_gradient(cmap='Reds')",0,"[""r = output2.groupby(['Models after Hyperparameter Tuning'])['Accuracy after HT'].mean().reset_index().sort_values(by='Accuracy after HT',ascending=False)\n"", ""r.head(10).style.background_gradient(cmap='Reds')\n""]"
len(dirs),0,['len(dirs)']
ASSIGN=data[data.Risk=='Risk 2 (Medium)'] ASSIGN.head(),0,"[""data_risk2=data[data.Risk=='Risk 2 (Medium)']\n"", 'data_risk2.head()']"
"pred_df_with_species_name.to_csv('my_predictions.csv', sep='\t', encoding='utf-8')",0,"[""pred_df_with_species_name.to_csv('my_predictions.csv', sep='\\t', encoding='utf-8')""]"
SETUP,0,"['import pandas as pd\n', 'import numpy as np\n', 'from collections import Counter\n', 'from datetime import datetime\n', 'from google.cloud import bigquery\n', 'import matplotlib.pyplot as plt\n', 'import os']"
CHECKPOINT graph,0,['graph']
CHECKPOINT tumor_data.dtypes,0,['tumor_data.dtypes']
CHECKPOINT print() column_nan(train) print() column_nan(test),0,"['print(""Sum of column has Nan in trainset:"")\n', 'column_nan(train)\n', 'print(""Sum of column has Nan in testset:"")\n', 'column_nan(test)']"
"ASSIGN='Attrition' ASSIGN=[ Normalize] ASSIGN = (TabularList.from_df(smote_df, cont_names=col , procs=procs,) .split_subsets(train_size=0.8, valid_size=0.2, seed=34) .label_from_df(cols=ASSIGN) .databunch())",0,"[""dep_var='Attrition'\n"", 'procs=[ Normalize]\n', 'data = (TabularList.from_df(smote_df, cont_names=col , procs=procs,)\n', '                .split_subsets(train_size=0.8, valid_size=0.2, seed=34)\n', '                .label_from_df(cols=dep_var)\n', '                .databunch())']"
china['Provincepath'].unique(),0,"[""china['Province/State'].unique()""]"
CHECKPOINT train_df.shape,0,['train_df.shape']
"ASSIGN = pd.read_csv(""..path(30-November-2017).csv"", ASSIGN='Windows-1252')",0,"['# read in our dat\n', 'suicide_attacks = pd.read_csv(""../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv"", \n', ""                              encoding='Windows-1252')""]"
learn.unfreeze(),0,['learn.unfreeze()']
"ASSIGN = cleaning_train.isnull().sum(axis=0).reset_index().sort_values(0,ascending=False).head(15) ASSIGN.columns = ['variable','missing'] ASSIGN = s['variable'].tolist() ASSIGN = cleaning_train[col_miss_train] ASSIGN.describe()",0,"['#cleaning trainig_set\n', 's = cleaning_train.isnull().sum(axis=0).reset_index().sort_values(0,ascending=False).head(15)\n', ""s.columns = ['variable','missing']\n"", ""col_miss_train = s['variable'].tolist()\n"", 'miss = cleaning_train[col_miss_train]\n', 'miss.describe()']"
"ASSIGN=data[data.carsList.apply(lambda x:re.match('37',x)!=None)] ASSIGN=data_37[data.date==data_37.date.unique()[0]].drop_duplicates(subset=['latitude']) for i in range(1,len(ASSIGN.date.unique())): ASSIGN=data_37[data.date==data_37.date.unique()[i]].drop_duplicates(subset=['latitude']) ASSIGN=pd.concat([ASSIGN,data_37_concate],axis=0) ASSIGN=34.78 ASSIGN=32.05 ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(data_37_map) for lat,lon,label in zip(ASSIGN.latitude,ASSIGN.longitude,ASSIGN.timestamp): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN)",1,"[""data_37=data[data.carsList.apply(lambda x:re.match('37',x)!=None)]\n"", ""data_37_new=data_37[data.date==data_37.date.unique()[0]].drop_duplicates(subset=['latitude'])\n"", 'for i in range(1,len(data_37.date.unique())):\n', ""    data_37_concate=data_37[data.date==data_37.date.unique()[i]].drop_duplicates(subset=['latitude'])\n"", '    data_37_new=pd.concat([data_37_new,data_37_concate],axis=0)\n', '\n', 'Long=34.78\n', 'Lat=32.05\n', 'data_37_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'data_37_cars_map=plugins.MarkerCluster().add_to(data_37_map)\n', 'for lat,lon,label in zip(data_37_new.latitude,data_37_new.longitude,data_37_new.timestamp):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_37_cars_map)\n', 'data_37_map.add_child(data_37_cars_map)\n', '\n']"
"CHECKPOINT ASSIGN=pd.read_csv(""..path"") ASSIGN=pd.read_csv(""..path"") train",0,"['train=pd.read_csv(""../input/house-prices-advanced-regression-techniques/train.csv"")\n', 'test=pd.read_csv(""../input/house-prices-advanced-regression-techniques/test.csv"")\n', 'train']"
data_Brooklyn.neighbourhood.unique(),0,['data_Brooklyn.neighbourhood.unique()']
SETUP warnings.filterwarnings('ignore'),0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import os\n', 'import warnings\n', ""warnings.filterwarnings('ignore')\n"", 'from sklearn.metrics import roc_auc_score\n', 'from sklearn.metrics import accuracy_score\n', 'from sklearn.feature_selection import VarianceThreshold\n', 'from sklearn.mixture import GaussianMixture\n', 'from sklearn.covariance import GraphicalLasso\n', 'from sklearn.preprocessing import StandardScaler']"
"ASSIGN = pd.read_csv(""..path"")",0,"['data = pd.read_csv(""../input/vehicle-dataset-from-cardekho/car data.csv"")']"
"sns.catplot(x ='Survived', y ='Parch', data = df1)",1,"[""sns.catplot(x ='Survived', y ='Parch', data = df1)""]"
ASSIGN = pd.read_csv('path'),0,"[""train = pd.read_csv('/kaggle/input/covid19-with-population/update_train_processed.csv')""]"
"CHECKPOINT ASSIGN = pd.read_excel('..path', sheet_name=0,dtype=dtypes) print(.format(ASSIGN.shape))",0,"['# source : https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/\n', ""df = pd.read_excel('../input/WA_Fn-UseC_-HR-Employee-Attrition.xlsx', sheet_name=0,dtype=dtypes)\n"", 'print(""Shape of dataframe is: {}"".format(df.shape))']"
ASSIGN=pd.read_csv('..path'),0,"[""data=pd.read_csv('../input/iris/Iris.csv')""]"
SETUP ASSIGN = LabelEncoder() data_tree_for_test['MSZoning_new'] = ASSIGN.fit_transform(data_tree_for_test['MSZoning']) data_tree_for_test['Neighborhood_new'] = ASSIGN.fit_transform(data_tree_for_test['Neighborhood']) data_tree_for_test.head(),0,"['from sklearn.preprocessing import LabelEncoder\n', 'labelencoder = LabelEncoder()\n', ""data_tree_for_test['MSZoning_new'] = labelencoder.fit_transform(data_tree_for_test['MSZoning'])\n"", ""data_tree_for_test['Neighborhood_new'] = labelencoder.fit_transform(data_tree_for_test['Neighborhood'])\n"", 'data_tree_for_test.head()']"
"data_features['MSSubClass'].groupby((data_features['BsmtCond'],data_features['BsmtQual'])).count()",0,"[""data_features['MSSubClass'].groupby((data_features['BsmtCond'],data_features['BsmtQual'])).count()""]"
"CHECKPOINT ASSIGN = RandomForestRegressor(n_estimators = 190) ASSIGN.fit(X_train, y_train) ASSIGN = model3.score(X_test,y_test) print(ASSIGN*100,'%')",0,"['model3 = RandomForestRegressor(n_estimators = 190)\n', 'model3.fit(X_train, y_train)\n', '\n', 'accuracy3 = model3.score(X_test,y_test)\n', ""print(accuracy3*100,'%')""]"
my_submission['Id'] = od.keys() my_submission['Predicted'] = od.values(),0,"[""my_submission['Id'] = od.keys()\n"", ""my_submission['Predicted'] = od.values()""]"
ASSIGN = LogisticRegression(solver = 'lbfgs'),0,"[""logisticRegr = LogisticRegression(solver = 'lbfgs')""]"
"CHECKPOINT ASSIGN = vg_sales[['Name', 'Year','Global_Sales']].sort(columns = 'Global_Sales', ascending = False) print(ASSIGN.head(20))",0,"[""Top_games = vg_sales[['Name', 'Year','Global_Sales']].sort(columns = 'Global_Sales', ascending = False)\n"", 'print(Top_games.head(20))']"
"CHECKPOINT ASSIGN = {'Yes': 1, 'No': 0} ASSIGN = train[target_col].map(target_col_dict).values train_labels.shape",0,"[""target_col_dict = {'Yes': 1, 'No': 0}\n"", 'train_labels = train[target_col].map(target_col_dict).values\n', 'train_labels.shape']"
"ASSIGN = Path('ASSIGN') ASSIGN.mkdir(parents=True, exist_ok=True)",0,"['# 设置所有数据存放的根目录\n', ""path = Path('/kaggle/working/data/bears')\n"", 'path.mkdir(parents=True, exist_ok=True)']"
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'fashion-mnist_test.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,"[""nRowsRead = 1000 # specify 'None' if want to read whole file\n"", '# fashion-mnist_test.csv has 10000 rows in reality, but we are only loading/previewing the first 1000 rows\n', ""df1 = pd.read_csv('../input/fashion-mnist_test.csv', delimiter=',', nrows = nRowsRead)\n"", ""df1.dataframeName = 'fashion-mnist_test.csv'\n"", 'nRow, nCol = df1.shape\n', ""print(f'There are {nRow} rows and {nCol} columns')""]"
SETUP,0,"['import tensorflow as tf\n', 'import tensorflow_io as tfio\n', 'from pathlib import Path\n', 'import matplotlib.pyplot as plt']"
"def myDataFrame(sample, digit_test, expected): ASSIGN = [] for x in range(len(digit_test)): ASSIGN.append(digit_test[x]-expected[x]) if len(sample) < 10: sample.insert(0, 0) ASSIGN = pd.DataFrame({ 'Sample Count': sample, 'Digit Test (%)': digit_test, 'Expected Values (%)': expected, 'Difference (%)': difference }) return output",0,"['def myDataFrame(sample, digit_test, expected):\n', '    difference = []\n', '    for x in range(len(digit_test)):\n', '        difference.append(digit_test[x]-expected[x])\n', '    if len(sample) < 10:\n', '        sample.insert(0, 0)\n', '    output = pd.DataFrame({\n', ""        'Sample Count': sample, \n"", ""        'Digit Test (%)': digit_test, \n"", ""        'Expected Values (%)': expected, \n"", ""        'Difference (%)': difference\n"", '                        })\n', '    return output']"
"ASSIGN = pd.read_csv(""..path"")",0,"['df = pd.read_csv(""../input/graduate-admissions/Admission_Predict_Ver1.1.csv"")']"
ASSIGN = pca_last.transform(holdout) pd.DataFrame(ASSIGN).head(),0,"['holdout_pca = pca_last.transform(holdout)\n', 'pd.DataFrame(holdout_pca).head()']"
ASSIGN=pd.read_csv('path') ASSIGN.head(),0,"[""train_data=pd.read_csv('/kaggle/input/ghouls-goblins-and-ghosts-boo/train.csv.zip')\n"", 'train_data.head()']"
"SETUP ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"")",0,"['import pandas as pd\n', 'import seaborn as sns\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', 'from sklearn import preprocessing\n', 'sample_submission = pd.read_csv(""../input/house-prices-advanced-regression-techniques/sample_submission.csv"")\n', 'test = pd.read_csv(""../input/house-prices-advanced-regression-techniques/test.csv"")\n', 'train = pd.read_csv(""../input/house-prices-advanced-regression-techniques/train.csv"")']"
"CHECKPOINT print (""Rows     : "" ,data.shape[0]) print (""Columns  : "" ,data.shape[1]) print (""\nFeatures : \n"" ,data.columns.tolist()) print(,data.info())",0,"['print (""Rows     : "" ,data.shape[0])\n', 'print (""Columns  : "" ,data.shape[1])\n', 'print (""\\nFeatures : \\n"" ,data.columns.tolist())\n', 'print(""\\nData Information : \\n"",data.info())']"
train.YearsAtCompany.unique(),0,['train.YearsAtCompany.unique()']
val_counts_image_id['image_id']  = val_counts_image_id.index,0,"[""val_counts_image_id['image_id']  = val_counts_image_id.index""]"
ASSIGN = logmodel.predict(X_test),0,['predictions = logmodel.predict(X_test)']
"pd_data['RainToday'].replace({'No':0,'Yes':1},inplace=True) pd_data['RainTomorrow'].replace({'No':0,'Yes':1},inplace=True) pd_data.head()",0,"['#change yes/no to 1/0\n', ""pd_data['RainToday'].replace({'No':0,'Yes':1},inplace=True)\n"", ""pd_data['RainTomorrow'].replace({'No':0,'Yes':1},inplace=True)\n"", 'pd_data.head()']"
ASSIGN = final[891:] ASSIGN = MinMaxScaler() ASSIGN = feature_scaler.fit_transform(ASSIGN),0,"['y_train = final[891:]\n', 'feature_scaler = MinMaxScaler()\n', 'y_train = feature_scaler.fit_transform(y_train)']"
train[:5],0,['train[:5]']
"def plotScatterMatrix(df, plotSize, textSize): ASSIGN = ASSIGN.select_dtypes(include =[np.number]) ASSIGN = ASSIGN.dropna('columns') ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]] ASSIGN = list(df) if len(ASSIGN) > 10: ASSIGN = ASSIGN[:10] ASSIGN = ASSIGN[columnNames] ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde') ASSIGN = df.corr().values for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)): ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize) plt.suptitle('Scatter and Density Plot') plt.show()",1,"['# Scatter and density plots\n', 'def plotScatterMatrix(df, plotSize, textSize):\n', '    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n', '    # Remove rows and columns that would lead to df being singular\n', ""    df = df.dropna('columns')\n"", '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n', '    columnNames = list(df)\n', '    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n', '        columnNames = columnNames[:10]\n', '    df = df[columnNames]\n', ""    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n"", '    corrs = df.corr().values\n', '    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n', ""        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n"", ""    plt.suptitle('Scatter and Density Plot')\n"", '    plt.show()\n']"
ASSIGN = pd.read_csv('..path') ASSIGN = tfidf_vect.transform(test_df['review']) ASSIGN = ASSIGN.toarray(),0,"[""test_df = pd.read_csv('../input/test.csv')\n"", ""xtest_tfidf =  tfidf_vect.transform(test_df['review'])\n"", 'xtest_tfidf = xtest_tfidf.toarray()']"
ASSIGN = torch.tensor(df['breed'].tolist()),0,"[""labels = torch.tensor(df['breed'].tolist())""]"
CHECKPOINT ASSIGN = pd.read_csv('..path') ASSIGN['ImageId'] = ASSIGN['Image_Label'].apply(lambda x: x.split('_')[0]) ASSIGN['ClassId'] = ASSIGN['Image_Label'].apply(lambda x: x.split('_')[1]) ASSIGN['hasMask'] = ~ ASSIGN['EncodedPixels'].isna() print(ASSIGN.shape) ASSIGN.head(),0,"[""train_df = pd.read_csv('../input/understanding_cloud_organization/train.csv')\n"", ""train_df['ImageId'] = train_df['Image_Label'].apply(lambda x: x.split('_')[0])\n"", ""train_df['ClassId'] = train_df['Image_Label'].apply(lambda x: x.split('_')[1])\n"", ""train_df['hasMask'] = ~ train_df['EncodedPixels'].isna()\n"", '\n', 'print(train_df.shape)\n', 'train_df.head()']"
learn.lr_find() learn.recorder.plot(suggestion=True),1,"['learn.lr_find()\n', 'learn.recorder.plot(suggestion=True)']"
ASSIGN = final[:891] ASSIGN = MinMaxScaler() ASSIGN = feature_scaler.fit_transform(ASSIGN),0,"['x_train = final[:891]\n', 'feature_scaler = MinMaxScaler()\n', 'x_train = feature_scaler.fit_transform(x_train)']"
"def predict(model, test_dataloader): model.eval() ASSIGN = [] for inputs, _ in test_dataloader: ASSIGN = ASSIGN.cuda() ASSIGN = model(inputs) ASSIGN = torch.round(output) ASSIGN.extend([p.item() for p in ASSIGN]) return predictions ASSIGN = predict(model, test_dataloader)",0,"['def predict(model, test_dataloader):\n', '    model.eval()\n', '    predictions = []\n', '    for inputs, _ in test_dataloader:\n', '        inputs = inputs.cuda()\n', '        output = model(inputs)\n', '        preds = torch.round(output)\n', '        predictions.extend([p.item() for p in preds])\n', '    return predictions\n', '\n', 'predictions = predict(model, test_dataloader)']"
ASSIGN = days,0,"[""artime['Day'] = days""]"
"plotPerColumnDistribution(df2, 10, 5)",1,"['plotPerColumnDistribution(df2, 10, 5)']"
"ASSIGN = data.drop(['Retire'], axis = 1) ASSIGN = data['Retire']",0,"['#Split into X and y\n', ""X = data.drop(['Retire'], axis = 1)\n"", ""y = data['Retire']""]"
"SETUP plt.figure(figsize=(24,10)) plt.subplot(1,2,1) ASSIGN=[] ASSIGN=[] ASSIGN=[] for i in range(9): r1,r2=USA['rank '+str(i+2010)].split('path') R=float(r1)path(r2) R=1-R ASSIGN.append(1.5+R*math.sin(0+i*2*math.pipath)) ASSIGN.append(1.5+R*math.cos(0+i*2*math.pipath)) ASSIGN.append('rank '+str(i+2010)+' '+USA['rank '+str(i+2010)]) ASSIGN.append(ASSIGN[0]) ASSIGN.append(ASSIGN[0]) plt.plot(ASSIGN,ASSIGN, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2) for i, txt in enumerate(ASSIGN): plt.annotate(txt, (ASSIGN[i], ASSIGN[i])) plt.fill(ASSIGN, ASSIGN,""coral"") plt.xlim(0.45,2.7) plt.ylim(0.45,2.7) plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2) plt.title(""USA's export rank by year"",size=18) plt.subplot(1,2,2) ASSIGN=[] ASSIGN=[] ASSIGN=[] for i in range(9): r1,r2=USA['rank '+str(i+2010)].split('path') R=float(r1)path(r2) R=1-R ASSIGN.append(1.5+R*math.sin(0+i*2*math.pipath)) ASSIGN.append(1.5+R*math.cos(0+i*2*math.pipath)) ASSIGN.append('rank '+str(i+2010)+' '+USA['rank '+str(i+2010)]) ASSIGN.append(ASSIGN[0]) ASSIGN.append(ASSIGN[0]) plt.plot(ASSIGN,ASSIGN, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2) for i, txt in enumerate(ASSIGN): plt.annotate(txt, (ASSIGN[i], ASSIGN[i])) plt.xlim(0.45,2.7) plt.ylim(0.45,2.7) plt.fill(ASSIGN, ASSIGN,""plum"") plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2) plt.title(""USA's import rank by year"",size=18)",1,"['plt.figure(figsize=(24,10))\n', 'plt.subplot(1,2,1)\n', ""USA=rank_export('U S A')\n"", 'y=[]\n', 'x=[]\n', 'n=[]\n', 'for i in range(9):\n', ""    r1,r2=USA['rank '+str(i+2010)].split('/')\n"", '    R=float(r1)/float(r2)\n', '    R=1-R\n', '    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n', '    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n', ""    n.append('rank '+str(i+2010)+' '+USA['rank '+str(i+2010)])\n"", '    \n', 'x.append(x[0])\n', 'y.append(y[0])\n', ""plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n"", 'for i, txt in enumerate(n):\n', '    plt.annotate(txt, (x[i], y[i]))\n', '    plt.fill(x, y,""coral"")\n', '    plt.xlim(0.45,2.7)\n', '    plt.ylim(0.45,2.7)\n', ""    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n"", '    plt.title(""USA\'s export rank by year"",size=18)\n', '    \n', 'plt.subplot(1,2,2)\n', ""USA=rank_import('U S A')\n"", 'y=[]\n', 'x=[]\n', 'n=[]\n', 'for i in range(9):\n', ""    r1,r2=USA['rank '+str(i+2010)].split('/')\n"", '    R=float(r1)/float(r2)\n', '    R=1-R\n', '    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n', '    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n', ""    n.append('rank '+str(i+2010)+' '+USA['rank '+str(i+2010)])\n"", '    \n', 'x.append(x[0])\n', 'y.append(y[0])\n', ""plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n"", 'for i, txt in enumerate(n):\n', '    plt.annotate(txt, (x[i], y[i]))\n', '    plt.xlim(0.45,2.7)\n', '    plt.ylim(0.45,2.7)\n', '    plt.fill(x, y,""plum"")\n', ""    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n"", '    plt.title(""USA\'s import rank by year"",size=18)    \n', '    ']"
"artime.set_index('Day', inplace=True)",0,"[""artime.set_index('Day', inplace=True)""]"
"ASSIGN = open('predictions_neuralnetwork_1.csv','w') ASSIGN.write(predict_out.to_csv())",0,"['# write file to csv\n', ""fp = open('predictions_neuralnetwork_1.csv','w')\n"", 'fp.write(predict_out.to_csv())']"
suicide_attacks['City'] = suicide_attacks['City'].str.lower() suicide_attacks['City'] = suicide_attacks['City'].str.strip(),0,"['# convert to lower case\n', ""suicide_attacks['City'] = suicide_attacks['City'].str.lower()\n"", '# remove trailing white spaces\n', ""suicide_attacks['City'] = suicide_attacks['City'].str.strip()""]"
"CHECKPOINT ASSIGN = fuzzywuzzy.process.extract(""d.i khan"", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio) matches",0,"['# get the top 10 closest matches to ""d.i khan""\n', 'matches = fuzzywuzzy.process.extract(""d.i khan"", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)\n', '\n', '# take a look at them\n', 'matches']"
ASSIGN=pd.read_csv('..path') ASSIGN=pd.read_csv('..path'),0,"[""train_data=pd.read_csv('../input/train_transaction.csv')\n"", ""test_data=pd.read_csv('../input/test_transaction.csv')\n"", '#print(train_data.describe())\n', '#print(train_data.isnull().sum())']"
"data_features['MSSubClass'].groupby((data_features['BsmtExposure'],data_features['BsmtCond'])).count()",0,"[""data_features['MSSubClass'].groupby((data_features['BsmtExposure'],data_features['BsmtCond'])).count()\n"", ""#The proportion of TA is larger than other values so use TA to fill 'BsmtCond'""]"
"ASSIGN = pd.DataFrame(['Diamond Princess',2666,191522,0.01392,2666,19,'100%']) ASSIGN = ASSIGN.T ASSIGN.columns = population.columns ASSIGN = ASSIGN.append(DP)",0,"[""DP = pd.DataFrame(['Diamond Princess',2666,191522,0.01392,2666,19,'100%'])\n"", 'DP = DP.T\n', 'DP.columns = population.columns\n', '\n', 'population = population.append(DP)']"
"CHECKPOINT ASSIGN = linear_model.Lasso(alpha=.001) ASSIGN.fit(X_train,y_train) ASSIGN = model4.score(X_test,y_test) print(ASSIGN*100,'%')",0,"['model4 = linear_model.Lasso(alpha=.001)\n', 'model4.fit(X_train,y_train)\n', '\n', 'accuracy4 = model4.score(X_test,y_test)\n', ""print(accuracy4*100,'%')""]"
"SETUP plt.figure(figsize=(24,10)) plt.subplot(1,2,1) ASSIGN=[] ASSIGN=[] ASSIGN=[] for i in range(9): r1,r2=ALBANIA['rank '+str(i+2010)].split('path') R=float(r1)path(r2) R=1-R ASSIGN.append(1.5+R*math.sin(0+i*2*math.pipath)) ASSIGN.append(1.5+R*math.cos(0+i*2*math.pipath)) ASSIGN.append('rank '+str(i+2010)+' '+ALBANIA['rank '+str(i+2010)]) ASSIGN.append(ASSIGN[0]) ASSIGN.append(ASSIGN[0]) plt.plot(ASSIGN,ASSIGN, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2) for i, txt in enumerate(ASSIGN): plt.annotate(txt, (ASSIGN[i], ASSIGN[i])) plt.fill(ASSIGN, ASSIGN,""coral"") plt.xlim(0.45,2.7) plt.ylim(0.45,2.7) plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2) plt.title(""ALBANIA's export rank by year"",size=18) plt.subplot(1,2,2) ASSIGN=[] ASSIGN=[] ASSIGN=[] for i in range(9): r1,r2=ALBANIA['rank '+str(i+2010)].split('path') R=float(r1)path(r2) R=1-R ASSIGN.append(1.5+R*math.sin(0+i*2*math.pipath)) ASSIGN.append(1.5+R*math.cos(0+i*2*math.pipath)) ASSIGN.append('rank '+str(i+2010)+' '+ALBANIA['rank '+str(i+2010)]) ASSIGN.append(ASSIGN[0]) ASSIGN.append(ASSIGN[0]) plt.plot(ASSIGN,ASSIGN, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2) for i, txt in enumerate(ASSIGN): plt.annotate(txt, (ASSIGN[i], ASSIGN[i])) plt.xlim(0.45,2.7) plt.ylim(0.45,2.7) plt.fill(ASSIGN, ASSIGN,""plum"") plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2) plt.title(""ALBANIA's import rank by year"",size=18)",1,"['plt.figure(figsize=(24,10))\n', 'plt.subplot(1,2,1)\n', ""ALBANIA=rank_export('ALBANIA')\n"", 'y=[]\n', 'x=[]\n', 'n=[]\n', 'for i in range(9):\n', ""    r1,r2=ALBANIA['rank '+str(i+2010)].split('/')\n"", '    R=float(r1)/float(r2)\n', '    R=1-R\n', '    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n', '    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n', ""    n.append('rank '+str(i+2010)+' '+ALBANIA['rank '+str(i+2010)])\n"", '    \n', 'x.append(x[0])\n', 'y.append(y[0])\n', ""plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n"", 'for i, txt in enumerate(n):\n', '    plt.annotate(txt, (x[i], y[i]))\n', '    plt.fill(x, y,""coral"")\n', '    plt.xlim(0.45,2.7)\n', '    plt.ylim(0.45,2.7)\n', ""    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n"", '    plt.title(""ALBANIA\'s export rank by year"",size=18)\n', '    \n', 'plt.subplot(1,2,2)\n', ""ALBANIA=rank_import('ALBANIA')\n"", 'y=[]\n', 'x=[]\n', 'n=[]\n', 'for i in range(9):\n', ""    r1,r2=ALBANIA['rank '+str(i+2010)].split('/')\n"", '    R=float(r1)/float(r2)\n', '    R=1-R\n', '    y.append(1.5+R*math.sin(0+i*2*math.pi/9))\n', '    x.append(1.5+R*math.cos(0+i*2*math.pi/9))\n', ""    n.append('rank '+str(i+2010)+' '+ALBANIA['rank '+str(i+2010)])\n"", '    \n', 'x.append(x[0])\n', 'y.append(y[0])\n', ""plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n"", 'for i, txt in enumerate(n):\n', '    plt.annotate(txt, (x[i], y[i]))\n', '    plt.xlim(0.45,2.7)\n', '    plt.ylim(0.45,2.7)\n', '    plt.fill(x, y,""plum"")\n', ""    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n"", '    plt.title(""ALBANIA\'s import rank by year"",size=18)    ']"
CHECKPOINT ASSIGN=model.predict_classes(img_test) print(ASSIGN[0:10]),0,"['prediction=model.predict_classes(img_test)\n', 'print(prediction[0:10])']"
"ASSIGN = ASSIGN.drop(['Patient addmited to regular ward (1=yes, 0=no)'], axis=1, inplace=True) ASSIGN = relevant",0,"[""covid = covid.drop(['Patient addmited to regular ward (1=yes, 0=no)'], axis=1, inplace=True)\n"", 'covid = relevant']"
ASSIGN = pd.read_csv('..path') ASSIGN.head(),0,"[""trainDF = pd.read_csv('../input/train.csv')\n"", 'trainDF.head()']"
CHECKPOINT ASSIGN=ASSIGN.dropna(how='any') print(ASSIGN.shape),0,"['#drop NAN\n', ""pd_data=pd_data.dropna(how='any')\n"", 'print(pd_data.shape)']"
"CHECKPOINT ASSIGN = DataFrame(titanic_df.columns.delete(0)) ASSIGN.columns = ['Features'] ASSIGN[""Coefficient Estimate""] = pd.Series(logreg.coef_[0]) coeff_df",0,"['# get Correlation Coefficient for each feature using Logistic Regression\n', 'coeff_df = DataFrame(titanic_df.columns.delete(0))\n', ""coeff_df.columns = ['Features']\n"", 'coeff_df[""Coefficient Estimate""] = pd.Series(logreg.coef_[0])\n', '\n', '# preview\n', 'coeff_df']"
df_all.sample(n=5),0,['df_all.sample(n=5)']
ASSIGN=data[data.total_cars>=3].drop_duplicates(subset=['latitude']) ASSIGN.head(),0,"[""data_total_cars_7=data[data.total_cars>=3].drop_duplicates(subset=['latitude'])\n"", 'data_total_cars_7.head()']"
ASSIGN = covid_data['Provincepath'].unique() ASSIGN = covid_data['Countrypath'].unique(),0,"[""states = covid_data['Province/State'].unique()\n"", ""countries = covid_data['Country/Region'].unique()""]"
"SETUP ASSIGN = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270] ASSIGN = RandomForestRegressor() ASSIGN = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]} RFR.fit(X_train, y_train) RFR.best_params_",0,"['n_estimators = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]\n', '\n', 'RF = RandomForestRegressor()\n', '\n', ""parameters = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}\n"", '\n', ""RFR = GridSearchCV(RF, parameters,scoring='neg_mean_squared_error', cv=5)\n"", '\n', 'RFR.fit(X_train, y_train)\n', '\n', 'RFR.best_params_\n']"
"ASSIGN= cbdr.groupby(['STATEpath','DISTRICT','Year'])['Murder'].sum().reset_index().sort_values(by='Murder',ascending=False) ASSIGN.head(10).style.background_gradient(cmap='Reds')",1,"[""s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Murder'].sum().reset_index().sort_values(by='Murder',ascending=False)\n"", ""s.head(10).style.background_gradient(cmap='Reds')""]"
ASSIGN = pd.DataFrame(train_df['image_id'].value_counts()),0,"[""val_counts_image_id = pd.DataFrame(train_df['image_id'].value_counts())""]"
"ASSIGN = sorted(confirm_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True) ASSIGN = sorted(deaths_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True) ASSIGN = sorted(recover_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)",0,"['confirm_dict_sorted = sorted(confirm_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\n', 'deaths_dict_sorted = sorted(deaths_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\n', 'recover_dict_sorted = sorted(recover_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)']"
"ASSIGN = ASSIGN.drop([""TransactionDT""], axis = 1) ASSIGN = ASSIGN.drop([""TransactionDT""], axis = 1)",0,"['train_df = train_df.drop([""TransactionDT""], axis = 1)\n', 'test_df = test_df.drop([""TransactionDT""], axis = 1)']"
"sns.catplot(x='Year', y='Dacoity', data=cbdr,height = 5, aspect = 4)",1,"[""sns.catplot(x='Year', y='Dacoity', data=cbdr,height = 5, aspect = 4)""]"
"CHECKPOINT ASSIGN = StratifiedShuffleSplit(n_splits=10, test_size=0.2,random_state=12345) ASSIGN = next(iter(sss.split(train_x, train_y))) ASSIGN = train_x[train_index], train_x[val_index] ASSIGN = train_y[train_index], train_y[val_index] print(,x_train.shape) print(,x_val.shape)",0,"['## retain class balances\n', 'sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2,random_state=12345)\n', 'train_index, val_index = next(iter(sss.split(train_x, train_y)))\n', 'x_train, x_val = train_x[train_index], train_x[val_index]\n', 'y_train, y_val = train_y[train_index], train_y[val_index]\n', 'print(""x_train dim: "",x_train.shape)\n', 'print(""x_val dim:   "",x_val.shape)']"
ASSIGN = lambda x: 1 path(1 + np.exp(-x)),0,['sigmoid = lambda x: 1 / (1 + np.exp(-x))']
"TE=data_2019[data_2019.Variable=='Total Eagles - (ROUNDS)'].iloc[:,[0,4]] plt.figure(figsize=(22,15)) plt.subplot(1,2,1) TE.Value=TE.Value.astype(int) TE.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='firebrick') plt.title(""Top 10 player by Total Eagles(Rounds)"",size=20) plt.xlabel('counts') plt.ylabel('') plt.subplot(1,2,2) TE=data_2019[data_2019.Variable=='Total Eagles - (TOTAL)'].iloc[:,[0,4]] TE=TE.dropna(subset=['Value']) TE.Value=TE.Value.astype(int) TE.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='cyan') plt.xticks(np.linspace(0, 20, 5)) plt.title(""Top 10 player by Total Eagles(Total)"",size=20) plt.xlabel('counts') plt.ylabel('')",1,"[""TE=data_2019[data_2019.Variable=='Total Eagles - (ROUNDS)'].iloc[:,[0,4]]\n"", 'plt.figure(figsize=(22,15))\n', '\n', 'plt.subplot(1,2,1)\n', 'TE.Value=TE.Value.astype(int)\n', ""TE.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='firebrick')\n"", 'plt.title(""Top 10 player by Total Eagles(Rounds)"",size=20)\n', ""plt.xlabel('counts')\n"", ""plt.ylabel('')\n"", '\n', 'plt.subplot(1,2,2)\n', ""TE=data_2019[data_2019.Variable=='Total Eagles - (TOTAL)'].iloc[:,[0,4]]\n"", ""TE=TE.dropna(subset=['Value'])\n"", 'TE.Value=TE.Value.astype(int)\n', ""TE.groupby('Player Name')['Value'].max().sort_values(ascending=False)[:10].sort_values().plot.barh(color='cyan')\n"", 'plt.xticks(np.linspace(0, 20, 5))\n', 'plt.title(""Top 10 player by Total Eagles(Total)"",size=20)\n', ""plt.xlabel('counts')\n"", ""plt.ylabel('')\n"", '\n']"
"CHECKPOINT ASSIGN = model13.predict(x_test) ASSIGN = mean_squared_error(Y_test, y_pred13, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, ASSIGN)))",0,"['y_pred13 = model13.predict(x_test)\n', '\n', 'val = mean_squared_error(Y_test, y_pred13, squared=False)\n', 'val13 = str(round(val, 4))\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred13)))\n""]"
"ASSIGN = model_finetuned.fit_generator(train_generator, ASSIGN=100, ASSIGN=25,validation_data=val_generator,validation_steps=100 ,verbose=1,callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.3,patience=3, min_lr=0.000001)],use_multiprocessing=False, ASSIGN=True)",0,"['history_1 = model_finetuned.fit_generator(train_generator,                                    \n', '                                  steps_per_epoch=100, \n', '                                  epochs=25,validation_data=val_generator,validation_steps=100\n', ""                                  ,verbose=1,callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.3,patience=3, min_lr=0.000001)],use_multiprocessing=False,\n"", '               shuffle=True)']"
"ASSIGN = vot_soft.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('SoftVoting(NO HT).csv',index = False)",0,"['modelpred1 = vot_soft.predict(y_train)\n', ""sub1 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""sub1['PassengerId'] = result['PassengerId']\n"", ""sub1['Survived'] = modelpred1\n"", ""sub1.to_csv('SoftVoting(NO HT).csv',index = False)""]"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import matplotlib.pyplot as plt # drawing graph\n', '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# Any results you write to the current directory are saved as output.']"
CHECKPOINT test,0,['test']
ASSIGN = pd.read_csv('path'),0,"[""API_beds = pd.read_csv('/kaggle/input/newest-bed-api-for-each-country/Newest_avg_bed_API.csv')""]"
"ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path') ASSIGN = X.columns ASSIGN = [] for i in range(0, len(ASSIGN)): ASSIGN = col[i] if ASSIGN[-8:] == '(meters)': ASSIGN = ASSIGN[:-8] if ASSIGN[-9:] == '(degrees)': ASSIGN = ASSIGN[:-9] ASSIGN.append(ASSIGN) ASSIGN.columns = ASSIGN ASSIGN = X_test_full.columns ASSIGN = [] for i in range(0, len(ASSIGN)): ASSIGN = col[i] if ASSIGN[-8:] == '(meters)': ASSIGN = ASSIGN[:-8] if ASSIGN[-9:] == '(degrees)': ASSIGN = ASSIGN[:-9] ASSIGN.append(ASSIGN) ASSIGN.columns = ASSIGN ASSIGN = X.Cover_Type ASSIGN.drop(['Cover_Type'], axis=1, inplace=True) ASSIGN = X ASSIGN = y",0,"['# Read the data\n', ""X = pd.read_csv('../input/mh-forest/Forest_Cover_participants_Data/train.csv')\n"", ""X_test_full = pd.read_csv('../input/mh-forest/Forest_Cover_participants_Data/test.csv')\n"", '\n', 'col = X.columns\n', 'newcol = []\n', 'for i in range(0, len(col)):\n', '    temp = col[i]\n', ""    if temp[-8:] == '(meters)':\n"", '        #print(temp[:-8])\n', '        temp = temp[:-8]\n', ""    if temp[-9:] == '(degrees)':\n"", '        #print(temp[:-9])\n', '        temp = temp[:-9]\n', '    newcol.append(temp)\n', 'X.columns = newcol\n', '\n', 'col = X_test_full.columns\n', 'newcol = []\n', 'for i in range(0, len(col)):\n', '    temp = col[i]\n', ""    if temp[-8:] == '(meters)':\n"", '        #print(temp[:-8])\n', '        temp = temp[:-8]\n', ""    if temp[-9:] == '(degrees)':\n"", '        #print(temp[:-9])\n', '        temp = temp[:-9]\n', '    newcol.append(temp)\n', 'X_test_full.columns = newcol\n', '\n', '\n', 'y = X.Cover_Type\n', ""X.drop(['Cover_Type'], axis=1, inplace=True)\n"", '\n', ""#X.drop(['Id'], axis=1, inplace=True)\n"", '\n', 'train_X = X\n', 'train_y = y']"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path""); ASSIGN = df[0:3]; print(ASSIGN)",0,"['#Q.3 Write a panda sprogram to get the first three rows of a given DataFrame.\n', 'import pandas as pd\n', 'df = pd.read_csv(""../input/prediction-of-asteroid-diameter/Asteroid.csv"");\n', 'row = df[0:3];\n', 'print(row)']"
"ASSIGN = pd.DataFrame() ASSIGN = full.index ASSIGN = pd.date_range(args.forecast_start, args.forecast_end) for datetime in ASSIGN: ASSIGN[datetime.date().isoformat()] = 0 ASSIGN.iloc[:, 1:] = np.around(prediction[:, 2:])",0,"['test = pd.DataFrame()\n', 'test[""Page""] = full.index\n', 'datetime_list = pd.date_range(args.forecast_start, args.forecast_end)\n', 'for datetime in datetime_list:\n', '    test[datetime.date().isoformat()] = 0\n', 'test.iloc[:, 1:] = np.around(prediction[:, 2:])']"
"vot_soft1.fit(x_train, x_test) vot_soft1.score(y_train,y_test)",0,"['vot_soft1.fit(x_train, x_test) \n', 'vot_soft1.score(y_train,y_test)']"
headline_by_year(2018),0,['headline_by_year(2018)']
"sns.catplot(x='Year', y='Assault on women', data=total ,height = 5, aspect = 4,kind = 'bar')",1,"[""sns.catplot(x='Year', y='Assault on women', data=total ,height = 5, aspect = 4,kind = 'bar')""]"
"CHECKPOINT print(, len(data)) print(, len(data[data['Clicked']==1])) print(, len(data[data['Clicked']==0]))",0,"['print(""The total number of users in this dataset is: "", len(data))\n', 'print(""The number of users who clicked through the ads is: "", len(data[data[\'Clicked\']==1]))\n', 'print(""The number of users who did not click through the ads is: "", len(data[data[\'Clicked\']==0]))']"
"ASSIGN = ElasticNet() ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]} ASSIGN = GridSearchCV(en, parameters, scoring='neg_mean_squared_error', cv = 100) ASSIGN.fit(x_t, Y_t) ASSIGN.best_params_",0,"['en = ElasticNet()\n', '\n', ""parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n"", '\n', ""enr = GridSearchCV(en, parameters, scoring='neg_mean_squared_error', cv = 100)\n"", '\n', 'enr.fit(x_t, Y_t)\n', 'enr.best_params_']"
"sns.heatmap(confusion_matrix(y_test, y_pred_gs1), annot = True) plt.xlabel('Actual Value') plt.ylabel('Predicted Value')",1,"['sns.heatmap(confusion_matrix(y_test, y_pred_gs1), annot = True)\n', ""plt.xlabel('Actual Value')\n"", ""plt.ylabel('Predicted Value')""]"
SETUP,0,"['import pandas as pd\n', 'import numpy as np\n', 'import seaborn as sns\n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline']"
"CHECKPOINT ASSIGN = model17.predict(x_es) ASSIGN = mean_squared_error(Y_es, y_pred17, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, ASSIGN)))",0,"['y_pred17 = model17.predict(x_es)\n', '\n', 'val = mean_squared_error(Y_es, y_pred17, squared=False)\n', 'val17 = str(round(val, 4))\n', '\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred17)))\n""]"
"New_York_lines=lines[lines.city_id==206] New_York_track_lines=track_lines[track_lines.city_id==206] New_York_tracks=tracks[tracks.city_id==206].drop(columns=['buildstart','opening','closure','city_id']) New_York_tracks.columns=['section_id','geometry','length'] New_York_track_lines=pd.merge(New_York_track_lines,New_York_tracks) New_York_track_lines=New_York_track_lines.drop(columns=['id','created_at','updated_at','city_id']) New_York_track_lines.columns=['section_id','id','geometry','length'] New_York_lines=pd.merge(New_York_track_lines,New_York_lines) New_York_stations=stations[stations['city_id']==206] New_York_stations.head()",0,"['New_York_lines=lines[lines.city_id==206]\n', 'New_York_track_lines=track_lines[track_lines.city_id==206]\n', ""New_York_tracks=tracks[tracks.city_id==206].drop(columns=['buildstart','opening','closure','city_id'])\n"", ""New_York_tracks.columns=['section_id','geometry','length']\n"", 'New_York_track_lines=pd.merge(New_York_track_lines,New_York_tracks)\n', ""New_York_track_lines=New_York_track_lines.drop(columns=['id','created_at','updated_at','city_id'])\n"", ""New_York_track_lines.columns=['section_id','id','geometry','length']\n"", 'New_York_lines=pd.merge(New_York_track_lines,New_York_lines)\n', ""New_York_stations=stations[stations['city_id']==206]\n"", 'New_York_stations.head()']"
"CHECKPOINT ASSIGN = len(countries) print(ASSIGN) ASSIGN = range(N) ASSIGN = 0.8 plt.figure(figsize=(100,150)) ASSIGN = plt.barh(ind, df['Chemistry'], width, color = ' ASSIGN = plt.barh(ind, df['Literature'], width, df['Chemistry'], color = ' ASSIGN = plt.barh(ind, df['Medicine'], width, df['Chemistry'] + df['Literature'], color = ' ASSIGN = plt.barh(ind, df['Peace'], width, df['Chemistry'] + df['Literature'] + df['Medicine'], color = ' ASSIGN = plt.barh(ind, df['Physics'], width, df['Chemistry'] + df['Literature'] + df['Medicine'] + df['Peace'], color = ' ASSIGN = plt.barh(ind, df['Economics'], width, df['Chemistry'] + df['Literature'] + df['Medicine'] + df['Peace'] + df['Physics'], color = ' plt.xticks(np.arange(0, 280, 1)) plt.yticks(ASSIGN,df['country'], fontsize=56) plt.show() plt.clf()",1,"['N = len(countries)\n', 'print(N)\n', 'ind = range(N)  \n', 'width = 0.8\n', 'plt.figure(figsize=(100,150))\n', '\n', ""p1 = plt.barh(ind, df['Chemistry'], width, color = '#137e6d')\n"", ""p2 = plt.barh(ind, df['Literature'], width, df['Chemistry'],  color = '#95d0fc')\n"", ""p3 = plt.barh(ind, df['Medicine'], width, df['Chemistry'] + df['Literature'], color = '#03719c')\n"", ""p4 = plt.barh(ind, df['Peace'], width, df['Chemistry'] + df['Literature'] + df['Medicine'], color = '#6a79f7')\n"", ""p5 = plt.barh(ind, df['Physics'], width, df['Chemistry'] + df['Literature'] + df['Medicine'] + df['Peace'], color = '#137e6d')\n"", ""p6 = plt.barh(ind, df['Economics'], width, df['Chemistry'] + df['Literature'] + df['Medicine'] + df['Peace'] + df['Physics'], color = '#95d0fc')\n"", '\n', 'plt.xticks(np.arange(0, 280, 1))\n', ""plt.yticks(ind,df['country'],  fontsize=56)\n"", '\n', 'plt.show()\n', 'plt.clf()']"
"ASSIGN=train_datagen.flow_from_dataframe(test,directory='path', ASSIGN=(384,384), ASSIGN=""image_id"", ASSIGN=None, ASSIGN=None, ASSIGN=False, ASSIGN=32)",0,"[""test_generator=train_datagen.flow_from_dataframe(test,directory='/kaggle/input/plant-pathology-2020-fgvc7/images/',\n"", '                                                      target_size=(384,384),\n', '                                                      x_col=""image_id"",\n', '                                                      y_col=None,\n', '                                                      class_mode=None,\n', '                                                      shuffle=False,\n', '                                                      batch_size=32)']"
"ASSIGN = kickstarters_2017.usd_pledged_real > 0 ASSIGN = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges] ASSIGN = stats.boxcox(positive_pledges)[0] ASSIGN=plt.subplots(1,2) sns.distplot(ASSIGN, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Normalized data"")",1,"['# get the index of all positive pledges (Box-Cox only takes postive values)\n', 'index_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0\n', '\n', '# get only positive pledges (using their indexes)\n', 'positive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]\n', '\n', '# normalize the pledges (w/ Box-Cox)\n', 'normalized_pledges = stats.boxcox(positive_pledges)[0]\n', '\n', '# plot both together to compare\n', 'fig, ax=plt.subplots(1,2)\n', 'sns.distplot(positive_pledges, ax=ax[0])\n', 'ax[0].set_title(""Original Data"")\n', 'sns.distplot(normalized_pledges, ax=ax[1])\n', 'ax[1].set_title(""Normalized data"")']"
"CHECKPOINT ASSIGN = (data_new.loc[:, data_new.columns != 'Churn']) ASSIGN = (data_new.loc[:, data_new.columns == 'Churn']) print('Shape of ASSIGN: {}'.format(ASSIGN.shape)) print('Shape of ASSIGN: {}'.format(ASSIGN.shape)) X_train, X_test, y_train, y_test = train_test_split(ASSIGN, ASSIGN, test_size=0.2, random_state=0) ASSIGN = X_train.ASSIGN",0,"['# Splitiing to x and y\n', '\n', ""X = (data_new.loc[:, data_new.columns != 'Churn'])\n"", ""y = (data_new.loc[:, data_new.columns == 'Churn'])\n"", ""print('Shape of X: {}'.format(X.shape))\n"", ""print('Shape of y: {}'.format(y.shape))\n"", '\n', 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n', 'columns = X_train.columns']"
"SETUP CHECKPOINT ASSIGN = np.array([1,1,2,3,5,8,13,21,34,55,79]) print(ASSIGN)",0,"['# 1.Create 1D,2D and boolean array using numpy.\n', '#1D array\n', 'import numpy as np\n', 'a = np.array([1,1,2,3,5,8,13,21,34,55,79])\n', 'print(a)']"
"CHECKPOINT ASSIGN=[] ASSIGN=[] ASSIGN=[] for i in range (2,20): ASSIGN=tree.DecisionTreeClassifier(max_depth=i) ASSIGN=ASSIGN.fit(train_x,train_y) ASSIGN = dtree.score(train_x, train_y) print('max_depth=%d ' % i,'training accuracy: %.5f' % ASSIGN) ASSIGN = dtree.predict(test_x) X=accuracy_score(test_y, ASSIGN) print('\t\ttest accuracy: %.5f' % X) ASSIGN.append(ASSIGN) ASSIGN.append(X) ASSIGN.append(i)",0,"['tree_train_acc=[]   \n', 'tree_test_acc=[]    \n', 'tree_depth=[]       \n', '\n', 'for i in range (2,20):\n', '    dtree=tree.DecisionTreeClassifier(max_depth=i)\n', '    dtree=dtree.fit(train_x,train_y)\n', '    acc_log = dtree.score(train_x, train_y)\n', ""    print('max_depth=%d ' % i,'training accuracy: %.5f' % acc_log)\n"", '    \n', '    predict_y = dtree.predict(test_x)    \n', '    X=accuracy_score(test_y, predict_y)\n', ""    print('\\t\\ttest accuracy: %.5f' % X)\n"", '    \n', '    tree_train_acc.append(acc_log)\n', '    tree_test_acc.append(X)\n', '    tree_depth.append(i)\n', '    ']"
"model.score(X_test,y_test)",0,"['model.score(X_test,y_test)']"
"ASSIGN=plt.subplots(2,2,figsize=(15,16)) data.Risk.value_counts().plot(kind='bar',color=['red','yellow','green'],ax=ax[0,0]) ax[0,0].tick_params(axis='x',labelrotation=360) ax[0,0].set_title(""The counts of Risk"",size=20) ax[0,0].set_ylabel('counts',size=18) data.groupby(['year','Risk'])['Inspection ID'].agg('count').unstack('Risk').plot(ax=ax[0,1],color=['red','yellow','green']) ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,0.75)) ax[0,1].set_title(""The counts of Risk by year"",size=20) ax[0,1].set_ylabel('counts',size=18) data.groupby(['month','Risk'])['Inspection ID'].agg('count').unstack('Risk').plot(ax=ax[1,0],color=['red','yellow','green']) ax[1,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(-0.25,0.75)) ax[1,0].set_title(""The counts of Risk by month"",size=20) ax[1,0].set_ylabel('counts',size=18) sns.scatterplot(x='Longitude',y='Latitude',hue='Risk' ,data=data, ax=ax[1,1]) ax[1,1].set_title(""The distribution of inspections by risk"",size=20) ax[1,1].set_xlabel('Longitude') ax[1,1].set_ylabel('LATITUDE')",1,"['fig,ax=plt.subplots(2,2,figsize=(15,16))\n', ""data.Risk.value_counts().plot(kind='bar',color=['red','yellow','green'],ax=ax[0,0])\n"", ""ax[0,0].tick_params(axis='x',labelrotation=360)\n"", 'ax[0,0].set_title(""The counts of Risk"",size=20)\n', ""ax[0,0].set_ylabel('counts',size=18)\n"", '\n', '\n', ""data.groupby(['year','Risk'])['Inspection ID'].agg('count').unstack('Risk').plot(ax=ax[0,1],color=['red','yellow','green'])\n"", 'ax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,0.75))\n', 'ax[0,1].set_title(""The counts of Risk by year"",size=20)\n', ""ax[0,1].set_ylabel('counts',size=18)\n"", '\n', ""data.groupby(['month','Risk'])['Inspection ID'].agg('count').unstack('Risk').plot(ax=ax[1,0],color=['red','yellow','green'])\n"", 'ax[1,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(-0.25,0.75))\n', 'ax[1,0].set_title(""The counts of Risk by month"",size=20)\n', ""ax[1,0].set_ylabel('counts',size=18)\n"", '\n', ""sns.scatterplot(x='Longitude',y='Latitude',hue='Risk' ,data=data, ax=ax[1,1])\n"", 'ax[1,1].set_title(""The distribution of inspections by risk"",size=20)\n', ""ax[1,1].set_xlabel('Longitude')\n"", ""ax[1,1].set_ylabel('LATITUDE')\n""]"
"X_TRAIN_FILE=""path"" X_TEST_FILE=""path"" Y_TRAIN_FILE=""path"" Y_TEST_FILE=""path""",0,"['X_TRAIN_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_X_train.npz""\n', 'X_TEST_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_X_test.npy""\n', 'Y_TRAIN_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_Y_train.npz""\n', 'Y_TEST_FILE=""/kaggle/input/cat-dog-numpy/CAT_DOG_Y_test.npy""']"
"CHECKPOINT ASSIGN = pd.DataFrame(last_two, index=index) print([ j for (i,j) in zip(last_two, index) if i >= 4 ])",0,"['last_two_df = pd.DataFrame(last_two, index=index)\n', 'print([ j for (i,j) in zip(last_two, index) if i >= 4 ])']"
"CHECKPOINT ASSIGN = LinearRegression() ASSIGN.fit(X_train, y_train) ASSIGN = model1.score(X_test,y_test) print(ASSIGN*100,'%')",0,"['model1 = LinearRegression()\n', 'model1.fit(X_train, y_train)\n', '\n', 'accuracy1 = model1.score(X_test,y_test)\n', ""print(accuracy1*100,'%')""]"
"model.compile(loss='categorical_crossentropy', ASSIGN = adam(lr=0.001), metrics = [""accuracy""])",0,"[""model.compile(loss='categorical_crossentropy', \n"", '              optimizer = adam(lr=0.001), metrics = [""accuracy""])']"
SETUP ASSIGN = plot_confusion_matrix(conf_mat=cm) plt.show(),1,"['import matplotlib.pyplot as plt\n', 'from mlxtend.plotting import plot_confusion_matrix\n', '\n', 'fig, ax = plot_confusion_matrix(conf_mat=cm)\n', 'plt.show()']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
"CHECKPOINT ASSIGN = np.diag(cm) path(cm, axis = 0) precision",0,"['precision = np.diag(cm) / np.sum(cm, axis = 0)\n', 'precision']"
"CHECKPOINT ASSIGN = train_df.groupby('ImageId').agg(np.sum).reset_index() ASSIGN.sort_values('hasMask', ascending=False, inplace=True) print(ASSIGN.shape) ASSIGN.head()",0,"[""mask_count_df = train_df.groupby('ImageId').agg(np.sum).reset_index()\n"", ""mask_count_df.sort_values('hasMask', ascending=False, inplace=True)\n"", 'print(mask_count_df.shape)\n', 'mask_count_df.head()']"
match_stats_df.possession.unique()[1],0,"['#inspecting possession values\n', 'match_stats_df.possession.unique()[1]']"
len(li),0,['len(li)']
CHECKPOINT SETUP 1,0,"['1\n', 'import pandas as pd']"
SETUP,0,"['import gc\n', 'import os\n', 'import pickle\n', 'import random\n', 'import re\n', 'import sklearn.metrics\n', 'import sklearn.model_selection\n', 'import time\n', 'import warnings\n', 'import numpy as np\n', 'import pandas as pd\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', 'from collections import OrderedDict\n', 'from tqdm import tqdm, tqdm_notebook\n', 'import functools\n', 'import multiprocessing\n', 'import sklearn.preprocessing\n', 'import unicodedata\n', 'import copy\n', 'import time\n', 'from sklearn.model_selection import train_test_split, StratifiedKFold\n', 'import torch\n', 'import torch.nn as nn\n', 'import torch.nn.functional as F\n', 'import torch.utils.data']"
"ASSIGN = df['TOEFL Score'] sns.distplot(ASSIGN , kde= True,rug = False, bins = 30)",1,"[""x = df['TOEFL Score']\n"", 'sns.distplot(x , kde= True,rug = False, bins = 30)']"
"SETUP CHECKPOINT ASSIGN = sns.color_palette() warnings.filterwarnings(""ignore"") print(os.listdir())",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '%matplotlib inline\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns # for making plots with seaborn\n', 'color = sns.color_palette()\n', 'from PIL import Image\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n', 'import warnings\n', 'warnings.filterwarnings(""ignore"")\n', 'import os\n', 'print(os.listdir(""../input""))\n', '\n', '# Any results you write to the current directory are saved as output.']"
"ASSIGN=pd.DataFrame(train_identity,columns=['TransactionID','id_01','id_12','id_38','id_37','id_36','id_35','id_15','id_29','id_28','id_11','id_02','DeviceType','id_31','id_17','id_19','id_20']) ASSIGN=ASSIGN.dropna(subset=['id_38','id_37','id_36','id_35','id_15','id_29','id_28','id_11','id_02','DeviceType','id_31','id_17','id_19','id_20']) ASSIGN.head()",0,"[""train_identity_new=pd.DataFrame(train_identity,columns=['TransactionID','id_01','id_12','id_38','id_37','id_36','id_35','id_15','id_29','id_28','id_11','id_02','DeviceType','id_31','id_17','id_19','id_20'])\n"", ""train_identity_new=train_identity_new.dropna(subset=['id_38','id_37','id_36','id_35','id_15','id_29','id_28','id_11','id_02','DeviceType','id_31','id_17','id_19','id_20'])\n"", 'train_identity_new.head()']"
"CHECKPOINT print('Graph') print('Do we have a fully connected graph? ',nx.is_connected(g)) ASSIGN = g.to_directed() N, K = ASSIGN.order(), ASSIGN.size() ASSIGN= float(K) path print (""# Nodes: "", N) print (""# Edges: "", K) print (""Average connectivity degree: "", ASSIGN)",0,"['# Extract reference graph facts & metrics \n', ""print('Graph')\n"", ""print('Do we have a fully connected graph? ',nx.is_connected(g))\n"", 'h = g.to_directed()\n', 'N, K = h.order(), h.size()\n', 'avg_deg= float(K) / N\n', 'print (""# Nodes: "", N)\n', 'print (""# Edges: "", K)\n', 'print (""Average connectivity degree: "", avg_deg)']"
"RF.fit(x_train,x_test) RF.score(y_train,y_test)",0,"['RF.fit(x_train,x_test)\n', 'RF.score(y_train,y_test)']"
"CHECKPOINT ASSIGN = torch.zeros(NUM_CLASSES, NUM_CLASSES) with torch.no_grad(): ASSIGN = 0 ASSIGN = [] ASSIGN = [] ASSIGN = [] for i, (inputs, labels) in enumerate(test_loader): ASSIGN = ASSIGN.to(device) ASSIGN = ASSIGN.to(device) ASSIGN = model(inputs) ASSIGN = torch.max(outputs, 1) ASSIGN = torch.sum(preds == labels.data) ASSIGN += ASSIGN ASSIGN.append(preds) ASSIGN.append(ASSIGN) for t, p in zip(ASSIGN.view(-1), preds.view(-1)): ASSIGN[t.long(), p.long()] += 1 for dir in os.listdir(TEST_DIR): for file in os.listdir(os.path.join(TEST_DIR, dir)): ASSIGN = os.path.splitext(file)[0] ASSIGN.append(ASSIGN) print('Accuracy =====>>', ASSIGN.item()path(test_dataset))",0,"['confusion_matrix = torch.zeros(NUM_CLASSES, NUM_CLASSES)\n', '\n', 'with torch.no_grad():\n', '    accuracy = 0\n', '    pred_labels = []\n', '    pred_img_ids = []\n', '    true_labels = []\n', '    for i, (inputs, labels) in enumerate(test_loader):\n', '        inputs = inputs.to(device)\n', '        labels = labels.to(device)\n', '        outputs = model(inputs)\n', '        _, preds = torch.max(outputs, 1)\n', '        running_acc = torch.sum(preds == labels.data)\n', '        accuracy += running_acc\n', '        pred_labels.append(preds)\n', '        true_labels.append(labels)\n', '        \n', '        for t, p in zip(labels.view(-1), preds.view(-1)):\n', '            confusion_matrix[t.long(), p.long()] += 1\n', '        \n', '    for dir in os.listdir(TEST_DIR):\n', '        for file in os.listdir(os.path.join(TEST_DIR, dir)):\n', '            img_id = os.path.splitext(file)[0]\n', '            pred_img_ids.append(img_id)\n', '            \n', ""    print('Accuracy =====>>', accuracy.item()/len(test_dataset))""]"
"ASSIGN = CustomDataset(df['image_path'], labels, train=True) ASSIGN = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)",0,"[""train_dataset = CustomDataset(df['image_path'], labels, train=True)\n"", 'train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n']"
SETUP CHECKPOINT print(os.listdir()),0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n', '\n', 'import os\n', 'print(os.listdir(""../input""))\n', '\n', '# Any results you write to the current directory are saved as output.']"
"ASSIGN=plt.subplots(2,2,figsize=(20,16)) ASSIGN=data['Facility Type'].value_counts()[:10].index ASSIGN=data['Facility Type'].value_counts()[:10] sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,0]) ax[0,0].set_title(""Top 10 Facility Type by the counts of inspection "",size=20) ax[0,0].set_xlabel('counts',size=18) ax[0,0].set_ylabel('') sns.scatterplot(ASSIGN='Longitude',ASSIGN='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=data[data['Facility Type']=='Restaurant'], ax=ax[0,1]) ax[0,1].set_title(""The distribution of inspections for restaurant"",size=20) ax[0,1].set_xlabel('Longitude') ax[0,1].set_ylabel('LATITUDE') sns.scatterplot(ASSIGN='Longitude',ASSIGN='Latitude',hue='Risk' ,hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'],data=data[data['Facility Type']=='Grocery Store'], ax=ax[1,0]) ax[1,0].set_title(""The distribution of inspections for Grocery Store"",size=20) ax[1,0].set_xlabel('Longitude') ax[1,0].set_ylabel('LATITUDE') sns.scatterplot(ASSIGN='Longitude',ASSIGN='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=data[data['Facility Type']=='School'], ax=ax[1,1]) ax[1,1].set_title(""The distribution of inspections for School"",size=20) ax[1,1].set_xlabel('Longitude') ax[1,1].set_ylabel('LATITUDE')",1,"['fig,ax=plt.subplots(2,2,figsize=(20,16))\n', ""y=data['Facility Type'].value_counts()[:10].index\n"", ""x=data['Facility Type'].value_counts()[:10]\n"", 'sns.barplot(x=x,y=y,ax=ax[0,0])\n', 'ax[0,0].set_title(""Top 10 Facility Type by the counts of inspection "",size=20)\n', ""ax[0,0].set_xlabel('counts',size=18)\n"", ""ax[0,0].set_ylabel('')\n"", '\n', ""sns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=data[data['Facility Type']=='Restaurant'], ax=ax[0,1])\n"", 'ax[0,1].set_title(""The distribution of inspections for restaurant"",size=20)\n', ""ax[0,1].set_xlabel('Longitude')\n"", ""ax[0,1].set_ylabel('LATITUDE')\n"", '\n', ""sns.scatterplot(x='Longitude',y='Latitude',hue='Risk' ,hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'],data=data[data['Facility Type']=='Grocery Store'], ax=ax[1,0])\n"", 'ax[1,0].set_title(""The distribution of inspections for Grocery Store"",size=20)\n', ""ax[1,0].set_xlabel('Longitude')\n"", ""ax[1,0].set_ylabel('LATITUDE')\n"", '\n', ""sns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=data[data['Facility Type']=='School'], ax=ax[1,1])\n"", 'ax[1,1].set_title(""The distribution of inspections for School"",size=20)\n', ""ax[1,1].set_xlabel('Longitude')\n"", ""ax[1,1].set_ylabel('LATITUDE')""]"
ASSIGN = player_attr_df.describe().loc['mean'] ASSIGN = player_attr_df.describe().loc['std'],0,"[""old_mean = player_attr_df.describe().loc['mean']\n"", ""old_std = player_attr_df.describe().loc['std']""]"
"ASSIGN = ASSIGN.reshape(1,224,224,3)",0,"['test_x = test_x.reshape(1,224,224,3)']"
"MNB.fit(x_train,x_test) ASSIGN = MNB.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('MultinomialNB(No HT).csv',index = False)",0,"['MNB.fit(x_train,x_test)\n', 'model6pred = MNB.predict(y_train)\n', ""submission6 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission6['PassengerId'] = result['PassengerId']\n"", ""submission6['Survived'] = model6pred\n"", ""submission6.to_csv('MultinomialNB(No HT).csv',index = False)""]"
SETUP ASSIGN = MinMaxScaler() ASSIGN = trans.fit_transform(df) ASSIGN = DataFrame(dat) df.head(),0,"['from pandas import DataFrame\n', 'trans = MinMaxScaler()\n', 'dat = trans.fit_transform(df)\n', 'dataset = DataFrame(dat)\n', '\n', 'df.head()']"
"ASSIGN = tb.groupby(['Year'], as_index = False).mean() ASSIGN = plt.figure(figsize = (8,6)) ASSIGN = fig.add_subplot(2,2,1) ASSIGN = fig.add_subplot(2,2,2) ASSIGN = fig.add_subplot(2,2,3) ASSIGN = fig.add_subplot(2,2,4) ASSIGN.head() ASSIGN = 'Percentage of people' ASSIGN = 'Year' ASSIGN.set(title = 'Smoke everyday', ylabel = ASSIGN, xlabel = ASSIGN) ASSIGN.set(title = 'Smoke some days', ylabel = ASSIGN, xlabel = ASSIGN) ASSIGN.set(title = 'Former smoker', ylabel = ASSIGN, xlabel = ASSIGN) ASSIGN.set(title = 'Never smoked', ylabel = ASSIGN, xlabel = ASSIGN) ASSIGN.scatter(ASSIGN.Year, ASSIGN['Smoke everyday'], ) ASSIGN.scatter(ASSIGN.Year, ASSIGN['Smoke some days']) ASSIGN.scatter(ASSIGN.Year, ASSIGN['Former smoker']) ASSIGN.scatter(ASSIGN.Year, ASSIGN['Never smoked']) ASSIGN.tight_layout() ASSIGN.autofmt_xdate() plt.show()",1,"[""tb_group = tb.groupby(['Year'], as_index = False).mean()\n"", '\n', 'fig = plt.figure(figsize = (8,6))\n', 'ax1 = fig.add_subplot(2,2,1)\n', 'ax2 = fig.add_subplot(2,2,2)\n', 'ax3 = fig.add_subplot(2,2,3)\n', 'ax4 = fig.add_subplot(2,2,4)\n', '\n', 'tb_group.head()\n', '\n', ""y = 'Percentage of people'\n"", ""x = 'Year'\n"", '\n', ""ax1.set(title = 'Smoke everyday', ylabel = y, xlabel = x)\n"", ""ax2.set(title = 'Smoke some days', ylabel = y, xlabel = x)\n"", ""ax3.set(title = 'Former smoker', ylabel = y, xlabel = x)\n"", ""ax4.set(title = 'Never smoked', ylabel = y, xlabel = x)\n"", ""ax1.scatter(tb_group.Year, tb_group['Smoke everyday'], )\n"", ""ax2.scatter(tb_group.Year, tb_group['Smoke some days'])\n"", ""ax3.scatter(tb_group.Year, tb_group['Former smoker'])\n"", ""ax4.scatter(tb_group.Year, tb_group['Never smoked'])\n"", '\n', 'fig.tight_layout()\n', 'fig.autofmt_xdate()\n', 'plt.show()']"
SETUP,0,"['MIN_BOUND = -1000.0\n', 'MAX_BOUND = 400.0\n', '    \n']"
CHECKPOINT ASSIGN = pd.read_csv('..path') ASSIGN=ASSIGN.dropna(how='any') print(ASSIGN.shape),0,"[""pd_data = pd.read_csv('../input/weatherAUS.csv')\n"", ""pd_data=pd_data.dropna(how='any')\n"", 'print(pd_data.shape)\n']"
CHECKPOINT df_final,0,"['#display the dataframe again\n', 'df_final']"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") print() print(ASSIGN.shape) print() print(type(ASSIGN)) print() print(ASSIGN.head(3))",0,"['#4\n', 'import pandas as pd\n', 'data = pd.read_csv(""../input/iris-dataset/iris.data.csv"")\n', 'print(""Shape of the data:"")\n', 'print(data.shape)\n', 'print(""\\nData Type:"")\n', 'print(type(data))\n', 'print(""\\nFirst 3 rows:"")\n', 'print(data.head(3))']"
"plt.figure(3,figsize=(25,25)) ASSIGN = g.ASSIGN() ASSIGN = [g[u][v]['color'] for u,v in edges] nx.draw(g, node_color = color_map, edge_color = ASSIGN, with_labels = True)",1,"['# Drawing Graph\n', 'plt.figure(3,figsize=(25,25))  \n', 'edges = g.edges()\n', ""colors = [g[u][v]['color'] for u,v in edges]\n"", 'nx.draw(g, node_color = color_map, edge_color = colors, with_labels = True)']"
SETUP,0,"['import pandas as pd\n', 'import numpy as np\n', 'import matplotlib\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', 'import warnings\n', 'import itertools\n', '%matplotlib inline']"
"ASSIGN = preprocessing.LabelEncoder() ASSIGN = ['YearBuilt','YearRemodAdd','GarageYrBlt'] for var in ASSIGN: data_features[var] = ASSIGN.fit_transform(data_features[var]) data_features[ASSIGN]",0,"['#For variables about years,we use labelencoder method to transform. \n', ""#You can also use one-hot encode but I don't like the dimensional disaster...\n"", 'le = preprocessing.LabelEncoder()\n', '#ohe = preprocessing.OneHotEncoder()\n', ""Year_var = ['YearBuilt','YearRemodAdd','GarageYrBlt']\n"", 'for var in Year_var:\n', '    data_features[var] = le.fit_transform(data_features[var])\n', 'data_features[Year_var]']"
"ASSIGN = [] for x in range(100): ASSIGN.append(x) ASSIGN = plt.figure() ASSIGN = fig.add_axes([0,0,1,1]) ASSIGN = .35 ASSIGN = np.arange(len(index)) plt.xticks(ASSIGN, ASSIGN) ASSIGN.bar(ASSIGN - widthpath, last_two, ASSIGN= ASSIGN) plt.title('Last Two Digit Count') plt.show()",1,"['index = []\n', 'for x in range(100):\n', '    index.append(x)\n', '\n', '#Plotting\n', 'fig = plt.figure()\n', 'ax = fig.add_axes([0,0,1,1])\n', 'width = .35\n', 'x = np.arange(len(index))\n', 'plt.xticks(x, index)\n', 'ax.bar(x - width/2, last_two, width= width)\n', ""plt.title('Last Two Digit Count')\n"", 'plt.show()']"
"ASSIGN = [2016, 2017, 2020] ASSIGN = vg_sales.groupby(['Year']).sum().drop(years) ASSIGN = vg_sales.groupby(['Year']).mean().drop(years) ASSIGN = vg_sales.replace(0, np.nan).groupby(['Year']).count().drop(years)",0,"['years = [2016, 2017, 2020]\n', ""total_sales_group = vg_sales.groupby(['Year']).sum().drop(years)\n"", ""average_sales_group = vg_sales.groupby(['Year']).mean().drop(years)\n"", ""count_sales_group = vg_sales.replace(0, np.nan).groupby(['Year']).count().drop(years)""]"
"ASSIGN = {} for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']: ASSIGN = ['nunique'] ASSIGN['purchase_amount'] = ['sum','max','min','mean','var'] ASSIGN = ['sum','max','min','mean','var'] ASSIGN['purchase_date'] = ['max','min'] ASSIGN['month_lag'] = ['max','min','mean','var'] ASSIGN['month_diff'] = ['mean'] ASSIGN = ['sum', 'mean'] ASSIGN['category_1'] = ['sum', 'mean'] ASSIGN['card_id'] = ['size'] for col in ['category_2','category_3']: df_new_merchant_trans[col+'_mean'] = df_new_merchant_trans.groupby([col])['purchase_amount'].transform('mean') ASSIGN[col+'_mean'] = ['mean'] ASSIGN = get_new_columns('new_hist',aggs) ASSIGN = df_new_merchant_trans.groupby('card_id').agg(aggs) ASSIGN.columns = ASSIGN ASSIGN.reset_index(drop=False,inplace=True) ASSIGN['new_hist_purchase_date_diff'] = (ASSIGN['new_hist_purchase_date_max'] - ASSIGN['new_hist_purchase_date_min']).dt.days ASSIGN['new_hist_purchase_date_average'] = ASSIGN['new_hist_purchase_date_diff']path['new_hist_card_id_size'] ASSIGN['new_hist_purchase_date_uptonow'] = (datetime.datetime.today() - ASSIGN['new_hist_purchase_date_max']).dt.days ASSIGN = ASSIGN.merge(df_hist_trans_group,on='card_id',how='left') ASSIGN = ASSIGN.merge(df_hist_trans_group,on='card_id',how='left') del ASSIGN;gc.collect() del df_new_merchant_trans;gc.collect()",0,"['aggs = {}\n', ""for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n"", ""    aggs[col] = ['nunique']\n"", ""aggs['purchase_amount'] = ['sum','max','min','mean','var']\n"", ""aggs['installments'] = ['sum','max','min','mean','var']\n"", ""aggs['purchase_date'] = ['max','min']\n"", ""aggs['month_lag'] = ['max','min','mean','var']\n"", ""aggs['month_diff'] = ['mean']\n"", ""aggs['weekend'] = ['sum', 'mean']\n"", ""aggs['category_1'] = ['sum', 'mean']\n"", ""aggs['card_id'] = ['size']\n"", '\n', ""for col in ['category_2','category_3']:\n"", ""    df_new_merchant_trans[col+'_mean'] = df_new_merchant_trans.groupby([col])['purchase_amount'].transform('mean')\n"", ""    aggs[col+'_mean'] = ['mean']\n"", '    \n', ""new_columns = get_new_columns('new_hist',aggs)\n"", ""df_hist_trans_group = df_new_merchant_trans.groupby('card_id').agg(aggs)\n"", 'df_hist_trans_group.columns = new_columns\n', 'df_hist_trans_group.reset_index(drop=False,inplace=True)\n', ""df_hist_trans_group['new_hist_purchase_date_diff'] = (df_hist_trans_group['new_hist_purchase_date_max'] - df_hist_trans_group['new_hist_purchase_date_min']).dt.days\n"", ""df_hist_trans_group['new_hist_purchase_date_average'] = df_hist_trans_group['new_hist_purchase_date_diff']/df_hist_trans_group['new_hist_card_id_size']\n"", ""df_hist_trans_group['new_hist_purchase_date_uptonow'] = (datetime.datetime.today() - df_hist_trans_group['new_hist_purchase_date_max']).dt.days\n"", ""df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n"", ""df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n"", 'del df_hist_trans_group;gc.collect()\n', 'del df_new_merchant_trans;gc.collect()']"
ASSIGN = df1.copy() df1.describe(),0,"['df = df1.copy()\n', 'df1.describe()']"
CHECKPOINT print(rimage.shape),0,['print(rimage.shape)']
"SETUP torch_seed(42) ASSIGN = QuoraData() ASSIGN.convert_start(['glove', 'wiki']) ASSIGN = QuoraPreprocessor() ASSIGN = qd.read_input() ASSIGN['question_text'] = ASSIGN.transform(ASSIGN.question_text)",0,"['%%time\n', 'torch_seed(42)\n', '\n', 'qd = QuoraData()\n', ""qd.convert_start(['glove', 'wiki'])\n"", '\n', 'prep = QuoraPreprocessor()\n', 'input_df = qd.read_input()\n', ""input_df['question_text'] = prep.transform(input_df.question_text)""]"
SETUP,0,"['import numpy as np\n', 'import pandas as pd \n', 'import matplotlib.pyplot as plt\n', 'import os \n', 'import seaborn as sns\n', 'import geopandas as gpd\n', 'import folium\n', 'from folium import plugins\n', 'import datetime\n', 'import math']"
"plt.figure(figsize=(8,8)) train.isnull().sum().sort_values(ascending=False)[:19].sort_values().plot.barh(color='plum') plt.title('counts of missing value in the train data',size=20) plt.xlabel('counts')",1,"['plt.figure(figsize=(8,8))\n', ""train.isnull().sum().sort_values(ascending=False)[:19].sort_values().plot.barh(color='plum')\n"", ""plt.title('counts of missing value in the train data',size=20)\n"", ""plt.xlabel('counts')""]"
"ASSIGN = DT.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('Decision Tree(No HT).csv',index = False)",0,"['model4pred = DT.predict(y_train)\n', ""submission4 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission4['PassengerId'] = result['PassengerId']\n"", ""submission4['Survived'] = model4pred\n"", ""submission4.to_csv('Decision Tree(No HT).csv',index = False)""]"
CHECKPOINT test_data.shape,0,['test_data.shape']
"ASSIGN = model.fit(X_train, y_ohe, ASSIGN = 0.05, batch_size = 128, epochs = 8)",0,"['hist = model.fit(X_train, y_ohe,\n', '          validation_split = 0.05, batch_size = 128, epochs = 8)']"
my_submission.head(),0,['my_submission.head()']
"sns.set(font_scale=1.4) sns.heatmap(confusion_matrix[:10, :10], annot=True,annot_kws={""size"": 16})# font size",1,"['sns.set(font_scale=1.4)#for label size\n', 'sns.heatmap(confusion_matrix[:10, :10], annot=True,annot_kws={""size"": 16})# font size']"
"ASSIGN=plt.subplots(1,2,figsize=(25,16)) sns.barplot(x=data.Size.value_counts()[:10],y=data.Size.value_counts()[:10].index,ax=ax[0]) ax[0].set_title(""Top 10 Size by counts"",size=20) ax[0].set_xlabel("""") sns.boxplot(y=""Size"",x=""Rating"",data=data[data.Size.isin(list(data.Size.value_counts()[:10].index))],ax=ax[1]) ax[1].set_ylabel("""") ax[1].set_title(""Distribution of rating by Size for Top 10"",size=20)",1,"['fig,ax=plt.subplots(1,2,figsize=(25,16))\n', 'sns.barplot(x=data.Size.value_counts()[:10],y=data.Size.value_counts()[:10].index,ax=ax[0])\n', 'ax[0].set_title(""Top 10 Size by counts"",size=20)\n', 'ax[0].set_xlabel("""")\n', '\n', 'sns.boxplot(y=""Size"",x=""Rating"",data=data[data.Size.isin(list(data.Size.value_counts()[:10].index))],ax=ax[1])\n', 'ax[1].set_ylabel("""")\n', 'ax[1].set_title(""Distribution of rating by Size for Top 10"",size=20)']"
"SETUP ASSIGN=RandomForestRegressor(n_estimators=5,random_state=329,min_samples_leaf=.0001)",0,"['from sklearn.ensemble import RandomForestRegressor\n', 'RForest=RandomForestRegressor(n_estimators=5,random_state=329,min_samples_leaf=.0001)']"
"ASSIGN = np.random.exponential(size = 1000) ASSIGN = minmax_scaling(original_data, columns = [0]) ASSIGN=plt.subplots(1,2) sns.distplot(ASSIGN, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Scaled data"")",1,"['# generate 1000 data points randomly drawn from an exponential distribution\n', 'original_data = np.random.exponential(size = 1000)\n', '\n', '# mix-max scale the data between 0 and 1\n', 'scaled_data = minmax_scaling(original_data, columns = [0])\n', '\n', '# plot both together to compare\n', 'fig, ax=plt.subplots(1,2)\n', 'sns.distplot(original_data, ax=ax[0])\n', 'ax[0].set_title(""Original Data"")\n', 'sns.distplot(scaled_data, ax=ax[1])\n', 'ax[1].set_title(""Scaled data"")']"
"precision_score(y_train, predictions,average='macro')",0,"[""precision_score(y_train, predictions,average='macro')""]"
"SETUP ASSIGN = pd.read_csv(""..path"") np.random.seed(0)",0,"[""# modules we'll use\n"", 'import pandas as pd\n', 'import numpy as np\n', '\n', '# for Box-Cox Transformation\n', 'from scipy import stats\n', '\n', '# for min_max scaling\n', 'from mlxtend.preprocessing import minmax_scaling\n', '\n', '# plotting modules\n', 'import seaborn as sns\n', 'import matplotlib.pyplot as plt\n', '\n', '# read in all our data\n', 'kickstarters_2017 = pd.read_csv(""../input/kickstarter-projects/ks-projects-201801.csv"")\n', '\n', '# set seed for reproducibility\n', 'np.random.seed(0)']"
(match_df.country_id == match_df.league_id).all(),0,['(match_df.country_id == match_df.league_id).all()']
SETUP,0,['from fastai.widgets import *']
"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)",0,"['# Splitting the data into train and test\n', 'X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)']"
"ASSIGN = plt.subplots(1,3, figsize=(10,10)) axes[0].imshow(np.squeeze(image.numpy()), cmap='gray') axes[0].set_title('image') axes[1].imshow(np.squeeze(lossy_image.numpy()), cmap='gray') axes[1].set_title('lossy image'); axes[2].imshow(np.squeeze(lossy_image.numpy() - image.numpy()), cmap='gray') axes[2].set_title('diff bpath');",1,"['fig, axes = plt.subplots(1,3, figsize=(10,10))\n', '\n', ""axes[0].imshow(np.squeeze(image.numpy()), cmap='gray')\n"", ""axes[0].set_title('image')\n"", ""axes[1].imshow(np.squeeze(lossy_image.numpy()), cmap='gray')\n"", ""axes[1].set_title('lossy image');\n"", ""axes[2].imshow(np.squeeze(lossy_image.numpy() - image.numpy()), cmap='gray')\n"", ""axes[2].set_title('diff b/w images');""]"
"CHECKPOINT ASSIGN = ['address','activities','nursery','higher','internet','romantic'] for column in ASSIGN: print(column,,data_mat[column].unique())",0,"[""binary_features = ['address','activities','nursery','higher','internet','romantic']\n"", 'for column in binary_features:\n', '    print(column,""-"",data_mat[column].unique())']"
"data_features['FireplaceQu'].groupby([data_features['Fireplaces'],data_features['FireplaceQu']]).count()",0,"[""data_features['FireplaceQu'].groupby([data_features['Fireplaces'],data_features['FireplaceQu']]).count()""]"
"ASSIGN=ASSIGN.dropna(subset=['Rating','Current Ver','Android Ver','Content Rating'])",0,"[""data=data.dropna(subset=['Rating','Current Ver','Android Ver','Content Rating'])""]"
ASSIGN = ASSIGN[:10],0,['idxs = idxs[:10]']
"SETUP CHECKPOINT ASSIGN = np.ASSIGN(6) print(, ASSIGN) ASSIGN = sparse.csr_matrix(eye) print(, ASSIGN)",0,"['#3 Write a python program to create a 2-D array with ones on the diagonal and zeroes elsewhere.\n', 'import numpy as np\n', 'from scipy import sparse\n', 'eye = np.eye(6)\n', 'print(""NumPy array:\\n"", eye)\n', 'sparse_matrix = sparse.csr_matrix(eye)\n', 'print(""\\nSciPy sparse CSR matrix:\\n"", sparse_matrix)']"
relevant.notnull().sum(),0,"['## Eliminamos todos os valores nulos ou negativos\n', 'relevant.notnull().sum()']"
ASSIGN = mnist['label'] ASSIGN.head(),0,"['# Putting response variable to y\n', ""y = mnist['label']\n"", '\n', 'y.head()']"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# Any results you write to the current directory are saved as output.']"
ASSIGN = {} ASSIGN = {} ASSIGN = {} for country in countries: ASSIGN = covid_data[covid_data['Countrypath'] == country] ASSIGN = country_data['ObservationDate'].max() ASSIGN = country_data[country_data['ObservationDate'] == max_date] ASSIGN = sub['Confirmed'].sum() ASSIGN = sub['Deaths'].sum() ASSIGN = sub['Recovered'].sum() ASSIGN[country] = ASSIGN ASSIGN[country] = ASSIGN ASSIGN[country] = ASSIGN,0,"['confirm_dict = {}\n', 'deaths_dict = {}\n', 'recover_dict = {}\n', 'for country in countries:\n', ""    country_data = covid_data[covid_data['Country/Region'] == country]\n"", '    #cummulative, so we can simply take the latest date for final result\n', ""    max_date = country_data['ObservationDate'].max()\n"", ""    sub = country_data[country_data['ObservationDate'] == max_date]\n"", ""    confirm = sub['Confirmed'].sum()\n"", ""    death = sub['Deaths'].sum()\n"", ""    recover = sub['Recovered'].sum()\n"", '    \n', '    confirm_dict[country] = confirm\n', '    deaths_dict[country] = death\n', '    recover_dict[country] = recover\n']"
"SETUP ASSIGN = RandomForestClassifier().fit(X, y.reshape(-1)) ASSIGN = rfc.predict(X)",0,"['from sklearn.model_selection import cross_val_score\n', 'from sklearn.metrics import accuracy_score, classification_report, plot_confusion_matrix\n', '\n', '\n', 'rfc = RandomForestClassifier().fit(X, y.reshape(-1))\n', 'y_prima = rfc.predict(X)\n']"
data.iloc[:3],0,['data.iloc[:3]']
ASSIGN = val_counts_image_id['image_id'][cond_1 | cond_2].tolist(),0,"[""image_names_list = val_counts_image_id['image_id'][cond_1 | cond_2].tolist()""]"
"formated_gdf['size'] = formated_gdf['Confirmed'].pow(0.3) ASSIGN = px.scatter_geo(formated_gdf, locations=""Countrypath"", locationmode='country names', ASSIGN=""Confirmed"", size='size', hover_name=""Countrypath"", ASSIGN= [0, max(formated_gdf['Confirmed'])+2], ASSIGN=""natural earth"", animation_frame=""date"", ASSIGN='Confirmed for each day') ASSIGN.show()",1,"[""formated_gdf['size'] = formated_gdf['Confirmed'].pow(0.3)\n"", '\n', 'fig = px.scatter_geo(formated_gdf, locations=""Country/Region"", locationmode=\'country names\', \n', '                     color=""Confirmed"", size=\'size\', hover_name=""Country/Region"", \n', ""                     range_color= [0, max(formated_gdf['Confirmed'])+2], \n"", '                     projection=""natural earth"", animation_frame=""date"", \n', ""                     title='Confirmed for each day')\n"", 'fig.show()']"
"ASSIGN = final.copy() ASSIGN = ASSIGN[:891] ASSIGN = pd.concat([ASSIGN,df1['Survived']],axis = 1) ASSIGN.head()",0,"['df3 = final.copy()\n', 'df3 =  df3[:891]\n', ""df3 = pd.concat([df3,df1['Survived']],axis = 1)\n"", 'df3.head()']"
"ASSIGN=train_datagen.flow_from_dataframe(train,directory='path', ASSIGN=(384,384), ASSIGN=""image_id"", ASSIGN=['healthy','multiple_diseases','rust','scab'], ASSIGN='raw', ASSIGN=False, ASSIGN='training', ASSIGN=32)",0,"[""train_generator=train_datagen.flow_from_dataframe(train,directory='/kaggle/input/plant-pathology-2020-fgvc7/images/',\n"", '                                                      target_size=(384,384),\n', '                                                      x_col=""image_id"",\n', ""                                                      y_col=['healthy','multiple_diseases','rust','scab'],\n"", ""                                                      class_mode='raw',\n"", '                                                      shuffle=False,\n', ""                                                       subset='training',\n"", '                                                      batch_size=32)']"
ASSIGN = m.plot_components(forecast),1,['forecast_components = m.plot_components(forecast)']
"sns.axes_style('white') sns.jointplot(x=x, y=y, kind = 'hex', color = 'green')",1,"[""sns.axes_style('white')\n"", ""sns.jointplot(x=x, y=y, kind = 'hex', color = 'green')""]"
"CHECKPOINT ASSIGN = model9.predict(x_test) ASSIGN = mean_squared_error(Y_test, y_pred9, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, ASSIGN)))",0,"['y_pred9 = model9.predict(x_test)\n', '\n', 'val = mean_squared_error(Y_test, y_pred9, squared=False)\n', 'val9 = str(round(val, 4))\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred9)))\n""]"
"def CalculateNewY(X_orig, slope, intercept): ASSIGN = np.empty_like(X_orig, dtype=""float64"") ASSIGN = X_orig*slope+intercept return Y_calc",0,"['def CalculateNewY(X_orig, slope, intercept):\n', '    \n', '    Y_calc = np.empty_like(X_orig, dtype=""float64"")\n', '\n', '    Y_calc = X_orig*slope+intercept\n', '        \n', '    return Y_calc']"
CHECKPOINT ASSIGN = suicide_attacks['City'].unique() ASSIGN.sort() cities,0,"[""# get all the unique values in the 'City' column\n"", ""cities = suicide_attacks['City'].unique()\n"", '\n', '# sort them alphabetically and then take a closer look\n', 'cities.sort()\n', 'cities']"
"ASSIGN = Lasso() ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]} ASSIGN = GridSearchCV(L, parameters, scoring='neg_mean_squared_error', cv = 100) ASSIGN.fit(x_train, Y_train) ASSIGN.best_params_",0,"['L = Lasso()\n', '\n', ""parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n"", '\n', ""LR = GridSearchCV(L, parameters, scoring='neg_mean_squared_error', cv = 100)\n"", '\n', 'LR.fit(x_train, Y_train)\n', 'LR.best_params_']"
"ggplot(recpies, aes(x = calories, y = dessert)) + geom_point()",1,"[""# plot calories by whether or not it's a dessert\n"", 'ggplot(recpies, aes(x = calories, y = dessert)) + # draw a \n', '    geom_point()  # add points']"
SETUP,0,['DATA_PATH = Path(DATA_PATH)']
SETUP,0,"['import numpy as np\n', 'import pandas as pd ']"
"ASSIGN=[] ASSIGN=[] ASSIGN=[] for i in range(len(New_York_lines)): ASSIGN=New_York_lines.iloc[i].geometry.split('(')[1].split(')')[0].split(',') for j in range(len(ASSIGN)): ASSIGN.append(ASSIGN[j].split(' ')[0]) ASSIGN.append(ASSIGN[j].split(' ')[1]) ASSIGN.append(New_York_lines.url_name[i]) ASSIGN=pd.DataFrame({'x':x,'y':y,'z':z}) SLICE=SLICE.astype(float) SLICE=SLICE.astype(float) plt.figure(figsize=(27, 27)) plt.subplot(2, 2, 1) ASSIGN=sns.scatterplot(x=""x"", y=""y"", hue=""z"",data=fix) plt.legend(loc=0, bbox_to_anchor=(1.05,0.6)) plt.title(""lines for New_York"",size=20) ASSIGN.get_legend().remove() ASSIGN.set(xlabel='Longitude', ylabel='LATITUDE') plt.subplot(2,2,2) (New_York_lines.groupby(['url_name'])['length'].sum()path).sort_values(ascending= False)[:10].sort_values().plot.barh() plt.ylabel(' ') plt.xlabel('length(km)') plt.title(""Top 10 track by length"",size=20) plt.subplot(2,2,3) New_York_stations.groupby(['opening'])['id'].agg('count').plot() plt.xlabel(' ') plt.ylabel('stations') plt.title(""Number of opening stations by year"",size=20) plt.subplot(2,2,4) New_York_lines.name.value_counts()[:10].sort_values().plot.barh() plt.xlabel('counts') plt.title(""Top 10 line by number"",size=20)",1,"['x=[]\n', 'y=[]\n', 'z=[]\n', 'for i in range(len(New_York_lines)):\n', ""    sp=New_York_lines.iloc[i].geometry.split('(')[1].split(')')[0].split(',')\n"", '    for j in range(len(sp)):\n', ""        x.append(sp[j].split(' ')[0])\n"", ""        y.append(sp[j].split(' ')[1])\n"", '        z.append(New_York_lines.url_name[i])\n', ""fix=pd.DataFrame({'x':x,'y':y,'z':z})\n"", ""fix['x']=fix['x'].astype(float)\n"", ""fix['y']=fix['y'].astype(float)\n"", 'plt.figure(figsize=(27, 27))\n', 'plt.subplot(2, 2, 1) \n', 'ax=sns.scatterplot(x=""x"", y=""y"", hue=""z"",data=fix)\n', 'plt.legend(loc=0, bbox_to_anchor=(1.05,0.6))\n', 'plt.title(""lines for New_York"",size=20)\n', 'ax.get_legend().remove()\n', ""ax.set(xlabel='Longitude', ylabel='LATITUDE')\n"", 'plt.subplot(2,2,2)\n', ""(New_York_lines.groupby(['url_name'])['length'].sum()/1000).sort_values(ascending= False)[:10].sort_values().plot.barh()\n"", ""plt.ylabel(' ')\n"", ""plt.xlabel('length(km)')\n"", 'plt.title(""Top 10 track by length"",size=20)\n', 'plt.subplot(2,2,3)\n', ""New_York_stations.groupby(['opening'])['id'].agg('count').plot()\n"", ""plt.xlabel(' ')\n"", ""plt.ylabel('stations')\n"", 'plt.title(""Number of opening stations by year"",size=20)\n', 'plt.subplot(2,2,4)\n', 'New_York_lines.name.value_counts()[:10].sort_values().plot.barh()\n', ""plt.xlabel('counts')\n"", 'plt.title(""Top 10 line by number"",size=20)']"
for i in range(len(train)): if train.Province[i] is np.NaN: train.Province[i] = train.Country[i] for i in range(len(test)): if test.Province[i] is np.NaN: test.Province[i] = test.Country[i],0,"['for i in range(len(train)):\n', '    if train.Province[i] is np.NaN:\n', '        train.Province[i] = train.Country[i]\n', 'for i in range(len(test)):\n', '    if test.Province[i] is np.NaN:\n', '        test.Province[i] = test.Country[i]']"
"ASSIGN = data.groupby(['date','Countrypath']).agg('sum') ASSIGN = ASSIGN.reset_index() ASSIGN=ASSIGN.drop(columns='SNo')",0,"[""formated_gdf = data.groupby(['date','Country/Region']).agg('sum')\n"", 'formated_gdf = formated_gdf.reset_index()\n', ""formated_gdf=formated_gdf.drop(columns='SNo')\n""]"
"ASSIGN = pd.DataFrame(columns = ['Id_x', 'Province', 'Country', 'Date', 'ConfirmedCases', 'Fatalities', 'Days_After_1stJan', 'Dayofweek', 'Month', 'Day', 'Population', 'Density', 'Land_Area', 'Migrants', 'MedAge', 'UrbanPopRate', 'Id_y', 'Lat', 'Long', 'temp', 'min', 'max', 'stp', 'slp', 'dewp', 'rh', 'ah', 'wdsp', 'prcp', 'fog', 'API_beds']) for country in train.Country.unique(): for province in train.query(f""Country=='{country}'"").Province.unique(): ASSIGN = train.query(f""Country=='{country}' and Province=='{province}'"") ASSIGN = province_df.ConfirmedCases ASSIGN = province_df.Fatalities ASSIGN = np.array(ASSIGN).reshape(-1,1) ASSIGN = np.array(province_confirm).reshape(-1,1) ASSIGN= preprocessing.MinMaxScaler() ASSIGN = scaler1.fit_transform(province_confirm) ASSIGN = preprocessing.MinMaxScaler() ASSIGN = scaler2.fit_transform(province_fatalities) ASSIGN['ConfirmedCases'] = ASSIGN ASSIGN['Fatalities'] = ASSIGN ASSIGN = pd.concat((ASSIGN,province_df),axis = 0,sort=True)",0,"[""scale_train = pd.DataFrame(columns = ['Id_x', 'Province', 'Country', 'Date', 'ConfirmedCases', 'Fatalities',\n"", ""       'Days_After_1stJan', 'Dayofweek', 'Month', 'Day', 'Population',\n"", ""       'Density', 'Land_Area', 'Migrants', 'MedAge', 'UrbanPopRate', 'Id_y',\n"", ""       'Lat', 'Long', 'temp', 'min', 'max', 'stp', 'slp', 'dewp', 'rh', 'ah',\n"", ""       'wdsp', 'prcp', 'fog', 'API_beds'])\n"", 'for country in train.Country.unique():\n', '    for province in train.query(f""Country==\'{country}\'"").Province.unique():\n', '        province_df = train.query(f""Country==\'{country}\' and Province==\'{province}\'"")\n', '        province_confirm = province_df.ConfirmedCases\n', '        province_fatalities = province_df.Fatalities\n', '        province_confirm = np.array(province_confirm).reshape(-1,1)\n', '        province_fatalities = np.array(province_confirm).reshape(-1,1)\n', '        scaler1= preprocessing.MinMaxScaler()\n', '        scaled_confirm = scaler1.fit_transform(province_confirm)\n', '        scaler2 = preprocessing.MinMaxScaler()\n', '        scaled_fata = scaler2.fit_transform(province_fatalities)\n', ""        province_df['ConfirmedCases'] = scaled_confirm\n"", ""        province_df['Fatalities'] = scaled_fata\n"", '        scale_train = pd.concat((scale_train,province_df),axis = 0,sort=True)']"
"CHECKPOINT ASSIGN = os.listdir(""..path"") print(ASSIGN)",0,"['folder = os.listdir(""../input/lung-colon-normal/trainable_normal"")\n', 'print(folder)']"
"CHECKPOINT def train_model(model): ASSIGN = ASSIGN.cuda() ASSIGN = 0.0 ASSIGN = copy.deepcopy(model.state_dict()) for epoch in range(int(epochs)): ASSIGN = 0 ASSIGN = 0 ASSIGN = 0 ASSIGN.train() for inputs, labels in train_dataloader: ASSIGN = inputs.cuda(), labels.cuda() optimizer.zero_grad() ASSIGN = model(inputs) ASSIGN = criterion(outputs.squeeze(), labels) ASSIGN.backward() optimizer.step() ASSIGN += ASSIGN.item() else: ASSIGN.eval() ASSIGN = 0 for ASSIGN in val_dataloader: ASSIGN = inputs.cuda(), labels.cuda() ASSIGN = model(inputs) ASSIGN = torch.round(outputs.squeeze()) ASSIGN = criterion(predictions, labels) ASSIGN += ASSIGN.item() ASSIGN = (predictions == labels.data) ASSIGN += torch.sum(ASSIGN.data).item() ASSIGN = num_correct path(val_dataset) if ASSIGN > ASSIGN: ASSIGN = val_acc ASSIGN = copy.deepcopy(model.state_dict()) print('---------Epoch {} -----------'.format(epoch)) print('Train Loss: {:.6f} Val Loss: {:.6f} Val Accuracy: {:.6f}'.format( train_losspath(train_dataset), val_losspath(val_dataset), ASSIGN)) ASSIGN.load_state_dict(ASSIGN) return model",0,"['def train_model(model):\n', '    model = model.cuda()\n', '    best_acc = 0.0\n', '    best_model_wts = copy.deepcopy(model.state_dict())\n', '    \n', '    for epoch in range(int(epochs)):\n', '        train_loss = 0\n', '        val_loss = 0\n', '        val_acc = 0\n', '        model.train()\n', '        for inputs, labels in train_dataloader:\n', '            inputs, labels = inputs.cuda(), labels.cuda()\n', '            optimizer.zero_grad()\n', '            outputs = model(inputs)\n', '            loss = criterion(outputs.squeeze(), labels)\n', '            loss.backward()\n', '            optimizer.step()\n', '            \n', '            train_loss += loss.item()\n', '        else:\n', '            model.eval()\n', '            num_correct = 0\n', '            for inputs, labels in val_dataloader:\n', '                inputs, labels = inputs.cuda(), labels.cuda()\n', '                outputs = model(inputs)\n', '                predictions = torch.round(outputs.squeeze())\n', '                loss = criterion(predictions, labels)\n', '                \n', '                val_loss += loss.item()\n', '                equals = (predictions == labels.data)\n', '    \n', '                num_correct += torch.sum(equals.data).item()\n', '            \n', '            val_acc = num_correct / len(val_dataset)\n', '            if val_acc > best_acc:\n', '                best_acc = val_acc\n', '                best_model_wts = copy.deepcopy(model.state_dict())\n', ""        print('---------Epoch {} -----------'.format(epoch))\n"", ""        print('Train Loss: {:.6f} Val Loss: {:.6f} Val Accuracy: {:.6f}'.format(\n"", '                 train_loss/len(train_dataset), val_loss/len(val_dataset), val_acc))\n', '        \n', '    model.load_state_dict(best_model_wts)\n', '    return model']"
"CHECKPOINT ASSIGN = pd.DataFrame(data=X_train, columns=['PC1', 'PC2']) ASSIGN = pd.DataFrame(data=X_test, columns=['PC1', 'PC2']) ASSIGN = pd.concat([new_dataset_train.reset_index (drop=True), new_dataset_test], axis=1) new_dataset.shape",0,"[""new_dataset_train = pd.DataFrame(data=X_train, columns=['PC1', 'PC2'])\n"", ""new_dataset_test = pd.DataFrame(data=X_test, columns=['PC1', 'PC2'])\n"", '# Con-catenating test and train datasets\n', 'new_dataset = pd.concat([new_dataset_train.reset_index (drop=True), new_dataset_test], axis=1)\n', 'new_dataset.shape']"
"ASSIGN = ASSIGN.drop(['PassengerId','Name','Ticket'], axis=1) ASSIGN  = ASSIGN.drop(['Name','Ticket'], axis=1)",0,"[""# drop unnecessary columns, these columns won't be useful in analysis and prediction\n"", ""titanic_df = titanic_df.drop(['PassengerId','Name','Ticket'], axis=1)\n"", ""test_df    = test_df.drop(['Name','Ticket'], axis=1)""]"
learn.lr_find() learn.recorder.plot(),1,"['learn.lr_find()\n', 'learn.recorder.plot()']"
CHECKPOINT X_test_full.shape,0,['X_test_full.shape']
"SETUP ASSIGN = plt.figure(figsize = (10,10)) plt.scatter(pcs_df.PC1, pcs_df.PC2) plt.xlabel('Principal Component 1') plt.ylabel('Principal Component 2') for i, txt in enumerate(pcs_df.Feature): plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i])) plt.tight_layout() plt.show()",1,"['%matplotlib inline\n', 'fig = plt.figure(figsize = (10,10))\n', 'plt.scatter(pcs_df.PC1, pcs_df.PC2)\n', ""plt.xlabel('Principal Component 1')\n"", ""plt.ylabel('Principal Component 2')\n"", 'for i, txt in enumerate(pcs_df.Feature):\n', '    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\n', 'plt.tight_layout()\n', 'plt.show()']"
"CHECKPOINT SETUP ASSIGN=pd.read_csv(""..path"") data",0,"['import pandas as pd\n', 'data=pd.read_csv(""../input/titanic/train_and_test2.csv"")\n', 'data\n']"
"ggplot(recipes, aes(x='calories', y='dessert')) + geom_point()",1,"[""# plot calories by whether or not it's a dessert\n"", ""ggplot(recipes, aes(x='calories', y='dessert')) + geom_point()""]"
"ASSIGN = ASSIGN.drop(['Patient ID'], axis=1, inplace=True) ASSIGN = relevant",0,"[""covid = covid.drop(['Patient ID'], axis=1, inplace=True)\n"", 'covid = relevant']"
X.head(),0,['X.head()']
"plotScatterMatrix(df2, 20, 10)",1,"['plotScatterMatrix(df2, 20, 10)']"
df.head(),0,['df.head()']
"ASSIGN=['V108', 'id_22', 'id_26', 'id_25', 'C3', 'V14', 'id_24', 'id_07', 'V121', 'V65', 'V305', 'V311', 'V286', 'V111', 'V300', 'V115', 'V113', 'V301', 'V25', 'V123', 'V118', 'V109', 'id_23', 'V112', 'V114', 'V120', 'V88', 'V107', 'V117', 'V122', 'V116', 'id_08', 'id_27', 'id_21', 'V110', 'V119'] ASSIGN = ASSIGN.drop(cols_to_drop, axis=1) ASSIGN = ASSIGN.drop(cols_to_drop, axis=1)",0,"[""cols_to_drop=['V108', 'id_22', 'id_26', 'id_25', 'C3', 'V14', 'id_24', 'id_07', 'V121', 'V65', 'V305', 'V311', 'V286', 'V111', 'V300', 'V115', 'V113', 'V301',  'V25', 'V123', 'V118', 'V109', 'id_23', 'V112', 'V114', 'V120', 'V88', 'V107', 'V117', 'V122', 'V116', 'id_08', 'id_27', 'id_21', 'V110', 'V119']\n"", '\n', 'train = train.drop(cols_to_drop, axis=1)\n', 'X_test = X_test.drop(cols_to_drop, axis=1)']"
"ASSIGN=irishtimes[:100000] ASSIGN=[] for i in range(len(ASSIGN)): ASSIGN=re.sub('[^a-zA-Z]',' ',irishtimes_headline_text['headline_text'][i]) ASSIGN=ASSIGN.lower() ASSIGN=ASSIGN.split() ASSIGN=PorterStemmer() ASSIGN=[ps.stem(word) for word in ASSIGN if not word in set(stopwords.words('english'))] ASSIGN.extend(ASSIGN)",0,"['irishtimes_headline_text=irishtimes[:100000]\n', 'headline_text_new=[]#Initialize empty array to append clean text\n', 'for i in range(len(irishtimes_headline_text)):\n', ""\theadline=re.sub('[^a-zA-Z]',' ',irishtimes_headline_text['headline_text'][i]) \n"", '\theadline=headline.lower() #convert to lower case\n', '\theadline=headline.split() #split to array(default delimiter is "" "")\n', '\tps=PorterStemmer() #creating porterStemmer object to take main stem of each word\n', ""\theadline=[ps.stem(word) for word in headline if not word in set(stopwords.words('english'))] #loop for stemming each word  in string array at ith row\n"", '\theadline_text_new.extend(headline)\n']"
"CHECKPOINT ASSIGN=stations[stations['city_id']==114][0:2000] ASSIGN=139.75 ASSIGN=35.67 ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(tokyo_map) for lat,lon,label in zip(ASSIGN.ASSIGN,ASSIGN.ASSIGN,ASSIGN.stations_name): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN) tokyo_map",1,"['\n', ""stations_tokyo_2000=stations[stations['city_id']==114][0:2000]\n"", 'Long=139.75\n', 'Lat=35.67\n', 'tokyo_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'tokyo_stations_map=plugins.MarkerCluster().add_to(tokyo_map)\n', 'for lat,lon,label in zip(stations_tokyo_2000.Lat,stations_tokyo_2000.Long,stations_tokyo_2000.stations_name):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(tokyo_stations_map)\n', 'tokyo_map.add_child(tokyo_stations_map)\n', '\n', 'tokyo_map\n']"
"'''cols_to_drop=['V108', 'id_22', 'id_26', 'id_25', 'C3', 'V14', 'id_24', 'id_07', 'V121', 'V65', 'V305', 'V311', 'V286', 'V111', 'V300', 'V115', 'V113', 'V301',  'V25', 'V123', 'V118', 'V109', 'id_23', 'V112', 'V114', 'V120', 'V88', 'V107', 'V117', 'V122', 'V116', 'id_08', 'id_27', 'id_21', 'V110', 'V119'] ASSIGN = ASSIGN.drop(cols_to_drop, axis=1) ASSIGN = ASSIGN.drop(cols_to_drop, axis=1)'''",0,"['\n', ""'''cols_to_drop=['V108', 'id_22', 'id_26', 'id_25', 'C3', 'V14', 'id_24', 'id_07', 'V121', 'V65', 'V305', 'V311', 'V286', 'V111', 'V300', 'V115', 'V113', 'V301',  'V25', 'V123', 'V118', 'V109', 'id_23', 'V112', 'V114', 'V120', 'V88', 'V107', 'V117', 'V122', 'V116', 'id_08', 'id_27', 'id_21', 'V110', 'V119']\n"", '\n', 'train = train.drop(cols_to_drop, axis=1)\n', ""test = test.drop(cols_to_drop, axis=1)'''""]"
test.head(),0,['test.head()']
"CHECKPOINT ASSIGN = AR(train['Total']) ASSIGN = model.fit(maxlag=2,method='cmle') print(f'Lag: {ASSIGN.k_ar}') print(f'Coefficients:\n{ASSIGN.params}')",0,"[""model = AR(train['Total'])\n"", ""AR1fit = model.fit(maxlag=2,method='cmle')\n"", ""print(f'Lag: {AR1fit.k_ar}')\n"", ""print(f'Coefficients:\\n{AR1fit.params}')""]"
ht[10]=172 ht[11]=172 ht[22]=2 ht[49]=2,0,"['ht[10]=172\n', 'ht[11]=172\n', 'ht[22]=2\n', 'ht[49]=2']"
CHECKPOINT ASSIGN = df['Chance of Admit '] y,0,"[""y = df['Chance of Admit ']\n"", 'y']"
zomato_en.isna().sum(),0,['zomato_en.isna().sum()']
"ASSIGN=plt.subplots(1,2,figsize=(15,8)) ASSIGN = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",'cadetblue','hotpink','orange','darksalmon','brown') data_manha.neighbourhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=ASSIGN,ax=ax[0]) ax[0].set_title(""Top 10 neighbourhood by the number of rooms"",size=20) ax[0].set_xlabel('rooms',size=18) ASSIGN=data_manha['neighbourhood'].value_counts() ASSIGN=list(data_manha['neighbourhood'].value_counts().index)[:10] ASSIGN=list(count[:10]) ASSIGN.append(ASSIGN.agg(sum)-ASSIGN[:10].agg('sum')) ASSIGN.append('Other') ASSIGN=pd.DataFrame({""group"":groups,""counts"":counts}) ASSIGN=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum') ASSIGN = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1]) plt.legend(loc=0, bbox_to_anchor=(1.15,0.4)) plt.subplots_adjust(wspace =0.5, hspace =0) plt.ylabel('')",1,"['fig,ax=plt.subplots(1,2,figsize=(15,8))\n', 'clr = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",\'cadetblue\',\'hotpink\',\'orange\',\'darksalmon\',\'brown\')\n', ""data_manha.neighbourhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n"", 'ax[0].set_title(""Top 10 neighbourhood by the number of rooms"",size=20)\n', ""ax[0].set_xlabel('rooms',size=18)\n"", '\n', '\n', ""count=data_manha['neighbourhood'].value_counts()\n"", ""groups=list(data_manha['neighbourhood'].value_counts().index)[:10]\n"", 'counts=list(count[:10])\n', ""counts.append(count.agg(sum)-count[:10].agg('sum'))\n"", ""groups.append('Other')\n"", 'type_dict=pd.DataFrame({""group"":groups,""counts"":counts})\n', ""clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n"", ""qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n"", 'plt.legend(loc=0, bbox_to_anchor=(1.15,0.4)) \n', 'plt.subplots_adjust(wspace =0.5, hspace =0)\n', ""plt.ylabel('')""]"
df.head(),0,['df.head()']
CHECKPOINT hbfd,0,['hbfd']
"ASSIGN = type_stat.plot(kind='pie', figsize=(10,7), y='value', labels=list(type_stat.type),autopct='%1.1f%%', pctdistance=0.9, radius=1.2) plt.legend(loc=0, bbox_to_anchor=(1.10,1.0)) plt.title('Top 10 Statistic by number', weight='bold', size=14,y=1.08) plt.axis('equal') plt.ylabel('') plt.show() plt.clf() plt.close()",1,"['\n', ""qx = type_stat.plot(kind='pie', figsize=(10,7), y='value', labels=list(type_stat.type),autopct='%1.1f%%', pctdistance=0.9, radius=1.2)\n"", 'plt.legend(loc=0, bbox_to_anchor=(1.10,1.0)) \n', ""plt.title('Top 10 Statistic by number', weight='bold', size=14,y=1.08)\n"", ""plt.axis('equal')\n"", ""plt.ylabel('')\n"", 'plt.show()\n', 'plt.clf()\n', 'plt.close()']"
"ASSIGN = covid_data[covid_data['Countrypath'] == 'US'] ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for dat in ASSIGN['ObservationDate'].unique(): ASSIGN = us_data[us_data['ObservationDate'] == dat] ASSIGN = sub['Confirmed'].sum() ASSIGN = sub['Deaths'].sum() ASSIGN = sub['Recovered'].sum() ASSIGN.append(dat) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN = pd.Series(ASSIGN) ASSIGN =pd.Series(ASSIGN) ASSIGN = pd.Series(ASSIGN) ASSIGN = pd.Series(ASSIGN) ASSIGN = [date.min(), date[len(date)path], date.max()] plt.figure(figsize=(8,8)) plt.plot(ASSIGN, ASSIGN, color = 'yellow') plt.plot(ASSIGN, ASSIGN, color = 'red') plt.plot(ASSIGN, ASSIGN, color = 'green') plt.xticks(ASSIGN, ASSIGN) plt.xlabel('Date') plt.ylabel('Cummulative Count cases') plt.title('Trend Curve of Confirmed Cases in US') plt.legend(['Confirmed', 'Death', 'Recovered']) plt.show()",1,"[""us_data = covid_data[covid_data['Country/Region'] == 'US']\n"", 'date = []\n', 'c = []\n', 'd = []\n', 'r = []\n', ""for dat in us_data['ObservationDate'].unique():\n"", ""    sub = us_data[us_data['ObservationDate'] == dat]\n"", ""    confirm = sub['Confirmed'].sum()\n"", ""    death = sub['Deaths'].sum()\n"", ""    recover = sub['Recovered'].sum()\n"", '    date.append(dat)\n', '    c.append(confirm)\n', '    d.append(death)\n', '    r.append(recover)\n', '\n', 'date = pd.Series(date)\n', 'c  =pd.Series(c)\n', 'd = pd.Series(d)\n', 'r = pd.Series(r)\n', '\n', 't = [date.min(), date[len(date)//2], date.max()]\n', 'plt.figure(figsize=(8,8))\n', ""plt.plot(date, c, color = 'yellow')\n"", ""plt.plot(date, d, color = 'red')\n"", ""plt.plot(date, r, color = 'green')\n"", 'plt.xticks(t, t)\n', ""plt.xlabel('Date')\n"", ""plt.ylabel('Cummulative Count cases')\n"", ""plt.title('Trend Curve of Confirmed Cases in US')\n"", ""plt.legend(['Confirmed', 'Death', 'Recovered'])\n"", 'plt.show()']"
"X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)",0,"['X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)']"
CHECKPOINT df_plot_top,0,"['#displaying the data frame\n', 'df_plot_top']"
"SETUP CHECKPOINT ASSIGN = confusion_matrix(y_test, y_pred) sns.heatmap(ASSIGN, annot = True, fmt = 'd') print(classification_report(y_test, y_pred))",1,"['#Import and run confusion matrix & classification report\n', 'from sklearn.metrics import confusion_matrix\n', 'from sklearn.metrics import classification_report\n', 'cm = confusion_matrix(y_test, y_pred)\n', ""sns.heatmap(cm, annot = True, fmt = 'd')\n"", 'print(classification_report(y_test, y_pred))']"
CHECKPOINT print(img.shape),0,['print(img.shape)']
"ASSIGN=0 ASSIGN = pd.DataFrame() for idx,row in trend_df.iterrows(): if sum(row.infection_trend)>0: ASSIGN = ASSIGN.append(row) else: if ASSIGN<25: ASSIGN = ASSIGN.append(row) ASSIGN+=1 ASSIGN = temp_df",0,"['# Only keeping 25 sequences where the number of cases stays at 0, as there were way too many of these samples in our dataset.\n', 'i=0\n', 'temp_df = pd.DataFrame()\n', 'for idx,row in trend_df.iterrows():\n', '    if sum(row.infection_trend)>0:\n', '        temp_df = temp_df.append(row)\n', '    else:\n', '        if i<25:\n', '            temp_df = temp_df.append(row)\n', '            i+=1\n', 'trend_df = temp_df']"
del country_df,0,"['#country df deletion\n', 'del country_df']"
SETUP ASSIGN = 200,0,"['IMG_WIDTH = 1280//2\n', 'IMG_HEIGHT = 720//2\n', 'latent_size = 200']"
"SETUP CHECKPOINT ASSIGN = np.array([0, 10, 20, 40, 60]) print(,ASSIGN) ASSIGN = [10, 30, 40] print(,ASSIGN) print() print(np.intersect1d(ASSIGN, ASSIGN))",0,"['import numpy as np\n', 'array1 = np.array([0, 10, 20, 40, 60])\n', 'print(""Array1: "",array1)\n', 'array2 = [10, 30, 40]\n', 'print(""Array2: "",array2)\n', 'print(""Common values between two arrays:"")\n', 'print(np.intersect1d(array1, array2))']"
CHECKPOINT total.shape,0,['total.shape']
for val in range(len(finalData)): if str(finalData[val])[-2:] == '.0': ASSIGN = str(ASSIGN)[:-2],0,"[""#this for and if loop evaluates if the value ends with '.0' and if it does, the last two digits are removed\n"", 'for val in range(len(finalData)):\n', ""    if str(finalData[val])[-2:] == '.0':\n"", '        finalData[val] = str(finalData[val])[:-2]']"
testset.isnull().sum().sort_values(ascending = False).head(33)path(trainset),0,"['#summing percentage missing value\n', 'testset.isnull().sum().sort_values(ascending = False).head(33)/len(trainset)']"
"CHECKPOINT ASSIGN = torch.ASSIGN(""cuda"" if torch.cuda.is_available() else ""cpu"") print(ASSIGN)",0,"['device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n', 'print(device)']"
"plt.subplots(figsize=(10,15)) sns.heatmap(correlation, annot = True)",1,"['plt.subplots(figsize=(10,15))\n', 'sns.heatmap(correlation, annot = True)']"
"CHECKPOINT ASSIGN = logisticRegr.fit(X_train_pca, y_train) model_pca",0,"['model_pca = logisticRegr.fit(X_train_pca, y_train)\n', 'model_pca']"
CHECKPOINT train.dtypes,0,['train.dtypes']
ASSIGN = ClassificationInterpretation.from_learner(learn),0,['interp = ClassificationInterpretation.from_learner(learn)']
SETUP,0,['!pip install pmdarima']
"ASSIGN = ASSIGN[['Image_Label', 'EncodedPixels']] ASSIGN.to_csv('submission.csv', index=False) display(ASSIGN.head(10))",0,"[""sub_df = sub_df[['Image_Label', 'EncodedPixels']]\n"", ""sub_df.to_csv('submission.csv', index=False)\n"", 'display(sub_df.head(10))']"
"CHECKPOINT ASSIGN = next(iter(dataloader)) print(ASSIGN[0].shape) ASSIGN = a[15] ASSIGN = ASSIGN *0.5 + 0.5 plt.imshow(ASSIGN.permute(1,2,0))",1,"['a = next(iter(dataloader))\n', 'print(a[0].shape)\n', 'img = a[15]\n', 'img = img *0.5 + 0.5\n', 'plt.imshow(img.permute(1,2,0))']"
"CHECKPOINT ASSIGN = KNeighborsClassifier(n_neighbors = 11) Estimator.append(('ASSIGN',KNeighborsClassifier(n_neighbors = 13))) ASSIGN = cross_val_score(knn,x_train1,x_test,ASSIGN=10) Accuracy9 = ASSIGN.mean() Accuracy.append(Accuracy9) print(ASSIGN) print(ASSIGN.mean())",0,"['knn = KNeighborsClassifier(n_neighbors = 11)\n', ""Estimator.append(('knn',KNeighborsClassifier(n_neighbors = 13)))\n"", 'cv = cross_val_score(knn,x_train1,x_test,cv=10)\n', 'Accuracy9 = cv.mean()\n', 'Accuracy.append(Accuracy9)\n', 'print(cv)\n', 'print(cv.mean())']"
CHECKPOINT train_data.shape,0,['train_data.shape']
data.isnull().sum(),0,['data.isnull().sum()']
CHECKPOINT data.shape,0,['data.shape']
sns.set() sns.countplot(train_data['color']),1,"['#Visualize categorical variables\n', 'sns.set()\n', ""sns.countplot(train_data['color'])""]"
"SETUP ASSIGN=[85,87,92,98,80,83] ASSIGN=['English','Bengali','Hindi','Maths','History','Geography'] plt.pie(ASSIGN,labels=ASSIGN,startangle=90,shadow=True,explode=(0.08,0.08,0.08,0.08,0.5,0.08),autopct='%1.1f%%')",1,"['from matplotlib import pyplot as plt\n', 'slices=[85,87,92,98,80,83]\n', ""Subject=['English','Bengali','Hindi','Maths','History','Geography']\n"", ""plt.pie(slices,labels=Subject,startangle=90,shadow=True,explode=(0.08,0.08,0.08,0.08,0.5,0.08),autopct='%1.1f%%')\n""]"
"Images, Classes = get_images() Images.shape, Classes.shape",0,"['## get images / labels\n', '\n', 'Images, Classes = get_images()\n', '\n', 'Images.shape, Classes.shape']"
"SETUP CHECKPOINT ASSIGN = np.ASSIGN(5) print(, ASSIGN) ASSIGN = sparse.csr_matrix(eye) print(, ASSIGN)",0,"['#Q3.create a 2-D array with ones on the diagonal and zeros elsewhere.\n', 'import numpy as np\n', 'from scipy import sparse\n', 'eye = np.eye(5)\n', 'print(""NumPy array:\\n"", eye)\n', 'sparse_matrix = sparse.csr_matrix(eye)\n', 'print(""\\nSciPy sparse CSR matrix:\\n"", sparse_matrix)']"
"SETUP CHECKPOINT print(classification_report(y_train, predictions))",0,"['from sklearn.metrics import classification_report\n', 'print(classification_report(y_train, predictions))']"
menu.Category.value_counts(),0,['menu.Category.value_counts()']
"CHECKPOINT ASSIGN = Images.shape[0] for target_cls in [0, 1, 2]: ASSIGN = np.where(Classes == target_cls)[0] ASSIGN = indices.shape[0] ASSIGN = class_to_label[target_cls] print(ASSIGN, ASSIGN, n_target_clspath) ASSIGN = 10 ASSIGN = plt.subplots(ncols=n_cols, figsize=(25, 3)) for i in range(ASSIGN): axs[i].imshow(np.uint8(Images[ASSIGN[i]])) axs[i].axis('off') axs[i].set_title(ASSIGN) plt.show()",1,"['## visualize some images / labels\n', '\n', 'n_total_images = Images.shape[0]\n', '\n', 'for target_cls in [0, 1, 2]:\n', '    \n', '    indices = np.where(Classes == target_cls)[0] # get target class indices on Images / Classes\n', '    n_target_cls = indices.shape[0]\n', '    label = class_to_label[target_cls]\n', '    print(label, n_target_cls, n_target_cls/n_total_images)\n', '\n', '    n_cols = 10 # # of sample plot\n', '    fig, axs = plt.subplots(ncols=n_cols, figsize=(25, 3))\n', '\n', '    for i in range(n_cols):\n', '\n', '        axs[i].imshow(np.uint8(Images[indices[i]]))\n', ""        axs[i].axis('off')\n"", '        axs[i].set_title(label)\n', '\n', '    plt.show()']"
"ASSIGN = pd.read_excel('path', index_col=0)",0,"[""df = pd.read_excel('/kaggle/input/covid19/dataset.xlsx', index_col=0)\n""]"
"SETUP ASSIGN = Fold(input_df, input_X, seed=1111); torch_seed(ASSIGN.seed) ASSIGN = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)",0,"['%%time\n', 'fold = Fold(input_df, input_X, seed=1111); torch_seed(fold.seed)\n', 'learn3 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)']"
ASSIGN = 'teddy' ASSIGN = 'teddy_bear.txt',0,"[""folder = 'teddy'\n"", ""file = 'teddy_bear.txt'""]"
"ASSIGN=32 ASSIGN = model.fit_generator( data_gen(X_train, target_label_map, ASSIGN, augment=True), ASSIGN=data_gen(X_valid, target_label_map, batch_size), ASSIGN=50, ASSIGN = 1, ASSIGN= int(len(X_train)path), ASSIGN= int(len(X_valid)path) )",0,"['batch_size=32\n', 'history = model.fit_generator(\n', '    data_gen(X_train, target_label_map, batch_size, augment=True),\n', '    validation_data=data_gen(X_valid, target_label_map, batch_size),\n', '    epochs=50, \n', '    verbose = 1,\n', '    #callbacks=callbacks,\n', '    steps_per_epoch=  int(len(X_train)//batch_size),\n', '    validation_steps= int(len(X_valid)// batch_size)\n', ')']"
recpies <- recpies %>% filter(calories < 10000) %>% na.omit(),0,"['# quickly clean our dataset\n', 'recpies <- recpies %>%\n', '    filter(calories < 10000) %>% # remove outliers\n', '    na.omit() # remove rows with NA values']"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") ASSIGN = train.drop(""Survived"",axis=1) ASSIGN = train[""Survived""] X_train, X_test, y_train, y_test = train_test_split(ASSIGN, ASSIGN, test_size = 0.3, random_state = 101) ASSIGN = LogisticRegression() ASSIGN.fit(X_train,y_train) ASSIGN = logmodel.predict(X_test) print(,f1_score(y_test, ASSIGN)) print() confusion_matrix(y_test, ASSIGN)",0,"['from sklearn.model_selection import train_test_split\n', 'from sklearn.metrics import confusion_matrix\n', 'from sklearn.model_selection import cross_val_predict\n', 'from sklearn.linear_model import LogisticRegression\n', 'from sklearn.metrics import f1_score\n', '\n', 'train = pd.read_csv(""../input/titanic/train_data.csv"")\n', '\n', '\n', 'X = train.drop(""Survived"",axis=1)\n', 'y = train[""Survived""]\n', '\n', 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n', '\n', 'logmodel = LogisticRegression()\n', 'logmodel.fit(X_train,y_train)\n', '\n', 'predictions = logmodel.predict(X_test)\n', '\n', 'print(""F1 Score:"",f1_score(y_test, predictions))\n', ' \n', 'print(""\\nConfusion Matrix(below):\\n"")\n', 'confusion_matrix(y_test, predictions)']"
SETUP,0,"['import numpy as np\n', 'import random as rn']"
"ASSIGN = 0.0002 ASSIGN = nn.BCELoss() ASSIGN = nn.MSELoss() ASSIGN = nn.L1Loss() ASSIGN = 1 ASSIGN = 0 ASSIGN = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))",0,"['lr = 0.0002\n', '# Initialize BCELoss function\n', 'criterion = nn.BCELoss()\n', 'msecriterion = nn.MSELoss()\n', 'l1criterion = nn.L1Loss()\n', '# Establish convention for real and fake labels during training\n', 'real_label = 1\n', 'fake_label = 0\n', '\n', '# Setup Adam optimizers for both G and D\n', '# optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n', 'optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))']"
"train_data[numerical].hist(bins=15, figsize=(15, 6), layout=(2, 4),rwidth=0.9,grid=False,color='purple');",1,"['#Check if numerical attributes have normal distributions\n', ""train_data[numerical].hist(bins=15, figsize=(15, 6), layout=(2, 4),rwidth=0.9,grid=False,color='purple');""]"
learn.load('stage-2');,0,"[""learn.load('stage-2');""]"
CHECKPOINT df.shape,0,['df.shape']
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path""); print(ASSIGN[ASSIGN[:].isnull()])",0,"['#Q.5 write a pandas program to select the rows where the score is mising.\n', 'import pandas as pd\n', 'df = pd.read_csv(""../input/prediction-of-asteroid-diameter/Asteroid.csv"");\n', 'print(df[df[:].isnull()])']"
zomato_orgnl.head(),0,['zomato_orgnl.head()']
"df.loc[idx_val, dep_var].value_counts()[1]path(199522-159619)",0,"['# check % positive values in validation set\n', 'df.loc[idx_val, dep_var].value_counts()[1]/(199522-159619)']"
"sns.set(rc={'figure.figsize':(40,5)}) sns.countplot(x = 'Age', data = df1)",1,"[""sns.set(rc={'figure.figsize':(40,5)})\n"", ""sns.countplot(x = 'Age', data = df1)""]"
ASSIGN = pd.DataFrame(ForecastId) SLICE=pred_confirm SLICE=pred_fata,0,"['submit = pd.DataFrame(ForecastId)\n', ""submit['ConfirmedCases']=pred_confirm\n"", ""submit['Fatalities']=pred_fata""]"
"SETUP def scorer(model, X,  train_y): ASSIGN = model.predict(X) return metrics.accuracy_score( train_y, ASSIGN) ASSIGN = [1,2,4,8,16,32,64,128, 256] ASSIGN = [] for estimator in ASSIGN: ASSIGN = RandomForestClassifier(n_estimators=estimator) ASSIGN = model_selection.cross_val_score(rf, train_x, train_y, cv=5, scoring=scorer) ASSIGN.append(ASSIGN.mean())",0,"['#Parameters:決定最佳的n_estimators\n', '#目前只算到n_estimators=256，太大需要時間過長\n', '\n', 'from sklearn import model_selection, metrics\n', '\n', 'def scorer(model, X,  train_y):\n', '    preds = model.predict(X)\n', '    return metrics.accuracy_score( train_y, preds)\n', '\n', 'n_estimators = [1,2,4,8,16,32,64,128, 256]  ## try different n_estimators\n', 'cv_results = []\n', '\n', 'for estimator in n_estimators:\n', '    rf = RandomForestClassifier(n_estimators=estimator)\n', '    acc = model_selection.cross_val_score(rf, train_x,  train_y, cv=5, scoring=scorer)\n', '    cv_results.append(acc.mean())']"
"ASSIGN=20 ASSIGN = ImageData(is_train=False) ASSIGN = DataLoader(dataset, batch_size=batch_size, shuffle=True) ASSIGN = 'cuda'",0,"['batch_size=20\n', 'dataset = ImageData(is_train=False)\n', 'dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n', ""device = 'cuda'""]"
SETUP ASSIGN = 200,0,"['IMG_WIDTH = 178\n', 'IMG_HEIGHT = 218\n', 'latent_size = 200']"
"CHECKPOINT print('Skew\n', final_df.skew(), '\nKurtosis:\n', final_df.kurt())",0,"[""print('Skew\\n', final_df.skew(), '\\nKurtosis:\\n', final_df.kurt())""]"
trainset.isnull().sum().sort_values(ascending = False).head(20)path(trainset),0,"['#summing missing value trainset\n', 'trainset.isnull().sum().sort_values(ascending = False).head(20)/len(trainset)']"
"ASSIGN = pd.DataFrame(ASSIGN,dtype=float) ASSIGN = pd.DataFrame(ASSIGN,dtype=float)",0,"['X = pd.DataFrame(X,dtype=float)\n', 'X_test = pd.DataFrame(X_test,dtype=float)']"
"def get_img_coords(input_item, input_type=str, output_z=False): ''' Input is a PredictionString (e.g. from train dataframe) Output is two arrays: xs: x coordinates in the image (row) ys: y coordinates in the image (column) ''' ASSIGN == str: ASSIGN = str2coords(input_item) else: ASSIGN = input_item ASSIGN = [c['x'] for c in coords] ASSIGN = [c['y'] for c in coords] ASSIGN = [c['z'] for c in coords] ASSIGN = np.array(list(zip(xs, ys, zs))).T ASSIGN = np.dot(camera_matrix, P).T ASSIGN[:, 0] path= ASSIGN[:, 2] ASSIGN[:, 1] path= ASSIGN[:, 2] ASSIGN = img_p[:, 0] ASSIGN = img_p[:, 1] ASSIGN = img_p[:, 2] if output_z: return img_xs, img_ys, img_zs return img_xs, img_ys plt.figure(figsize=(14,14)) plt.imshow(imread(PATH + 'train_imagespath' + train['ImageId'][2217] + '.jpg')) plt.scatter(*get_img_coords(train['PredictionString'][2217]), color='red', s=100);",1,"['def get_img_coords(input_item, input_type=str, output_z=False):\n', ""    '''\n"", '    Input is a PredictionString (e.g. from train dataframe)\n', '    Output is two arrays:\n', '        xs: x coordinates in the image (row)\n', '        ys: y coordinates in the image (column)\n', ""    '''\n"", '    if input_type == str:\n', '        coords = str2coords(input_item)\n', '    else:\n', '        coords = input_item\n', '    \n', ""    xs = [c['x'] for c in coords]\n"", ""    ys = [c['y'] for c in coords]\n"", ""    zs = [c['z'] for c in coords]\n"", '    P = np.array(list(zip(xs, ys, zs))).T\n', '    img_p = np.dot(camera_matrix, P).T\n', '    img_p[:, 0] /= img_p[:, 2]\n', '    img_p[:, 1] /= img_p[:, 2]\n', '    img_xs = img_p[:, 0]\n', '    img_ys = img_p[:, 1]\n', '    img_zs = img_p[:, 2] # z = Distance from the camera\n', '    if output_z:\n', '        return img_xs, img_ys, img_zs\n', '    return img_xs, img_ys\n', '\n', 'plt.figure(figsize=(14,14))\n', ""plt.imshow(imread(PATH + 'train_images/' + train['ImageId'][2217] + '.jpg'))\n"", ""plt.scatter(*get_img_coords(train['PredictionString'][2217]), color='red', s=100);""]"
data.keys(),0,['data.keys()']
"data_features['MSZoning'].groupby([data_features['MSSubClass'],data_features['MSZoning']]).count() data_features['MSZoning'] = data_features['MSZoning'].groupby(data_features['MSSubClass']).transform(lambda x:x.fillna(x.mode()[0]))",0,"[""data_features['MSZoning'].groupby([data_features['MSSubClass'],data_features['MSZoning']]).count()\n"", ""data_features['MSZoning'] = data_features['MSZoning'].groupby(data_features['MSSubClass']).transform(lambda x:x.fillna(x.mode()[0]))""]"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
SETUP,0,"['from sklearn.metrics import precision_score, recall_score']"
ASSIGN=max(city_vs_count),0,"['#lets check max count\n', 'count_max=max(city_vs_count)']"
CHECKPOINT submission,0,['submission']
"x_t, x_es, Y_t, Y_es = train_test_split(ex, ey, test_size = 0.20,shuffle = False)",0,"['x_t, x_es, Y_t, Y_es = train_test_split(ex, ey, test_size = 0.20,shuffle = False)']"
"data_features['MasVnrScore'] = data_features['MasVnrType'] * data_features['MasVnrArea'] data_features['BsmtScore2'] = data_features['BsmtQual'] * data_features['BsmtCond'] * data_features['BsmtExposure'] data_features['TotalSF'] = data_features['TotalBsmtSF'] + data_features['1stFlrSF'] + data_features['2ndFlrSF'] data_features[""AllSF""] = data_features[""GrLivArea""] + data_features[""TotalBsmtSF""] data_features['TotalBath'] = data_features['FullBath'] + 0.5*data_features['HalfBath'] + data_features['BsmtFullBath'] + 0.5*data_features['BsmtHalfBath'] data_features['Total_porch_sf'] = data_features['OpenPorchSF'] + data_features['3SsnPorch'] + data_features['EnclosedPorch'] + data_features['ScreenPorch'] + data_features['WoodDeckSF']",0,"[""data_features['MasVnrScore'] = data_features['MasVnrType'] * data_features['MasVnrArea']\n"", ""data_features['BsmtScore2'] = data_features['BsmtQual'] * data_features['BsmtCond'] * data_features['BsmtExposure']\n"", ""data_features['TotalSF'] = data_features['TotalBsmtSF'] + data_features['1stFlrSF'] + data_features['2ndFlrSF']\n"", 'data_features[""AllSF""] = data_features[""GrLivArea""] + data_features[""TotalBsmtSF""]\n', ""data_features['TotalBath'] = data_features['FullBath'] + 0.5*data_features['HalfBath'] + data_features['BsmtFullBath'] + 0.5*data_features['BsmtHalfBath']\n"", ""data_features['Total_porch_sf'] = data_features['OpenPorchSF'] + data_features['3SsnPorch'] + data_features['EnclosedPorch'] + data_features['ScreenPorch'] + data_features['WoodDeckSF']""]"
ASSIGN = train.ASSIGN().sum(axis=1) ASSIGN = test.isna().sum(axis=1) SLICE=train.ASSIGN().sum(axis=1) SLICE=test.ASSIGN().sum(axis=1),0,"['isna = train.isna().sum(axis=1)\n', 'isna_test = test.isna().sum(axis=1)\n', ""train['isna']=train.isna().sum(axis=1)\n"", ""test['isna']=test.isna().sum(axis=1)""]"
"CHECKPOINT ASSIGN = model3.predict(X_test) ASSIGN = mean_squared_error(y_test, y_pred3, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ASSIGN)))",0,"['y_pred3 = model3.predict(X_test)\n', '\n', 'val = mean_squared_error(y_test, y_pred3, squared=False)\n', 'val3 = str(round(val, 4))\n', '\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred3)))\n""]"
CHECKPOINT ASSIGN = data_features.isnull().sum().sort_values(ascending = False) ASSIGN = ASSIGN[ASSIGN > 0] missing_data,0,"['missing_data = data_features.isnull().sum().sort_values(ascending = False)\n', 'missing_data = missing_data[missing_data > 0]\n', 'missing_data']"
"ASSIGN = LogisticRegression() ASSIGN.fit(X_train,y_train)",0,"['logmodel = LogisticRegression()\n', 'logmodel.fit(X_train,y_train)']"
def get_str_date(x): ASSIGN = str(ASSIGN)[0:10] return x scale_train.Date = scale_train.Date.apply(lambda ASSIGN: get_str_date(ASSIGN)),0,"['# In order to use query function,transform datetime to string\n', 'def get_str_date(x):\n', '    x = str(x)[0:10]\n', '    return x\n', '\n', 'scale_train.Date = scale_train.Date.apply(lambda x: get_str_date(x))']"
"ASSIGN = (data_features.isnull().sum()path().count()).sort_values(ascending=False) ASSIGN = ASSIGN[ASSIGN>0] pd.concat([data_features_na, ASSIGN],axis=1,keys=['total', 'ASSIGN'])",0,"['percent = (data_features.isnull().sum()/data_features.isnull().count()).sort_values(ascending=False)\n', 'percent = percent[percent>0]\n', ""pd.concat([data_features_na, percent],axis=1,keys=['total', 'percent'])""]"
final.head(),0,['final.head()']
"ASSIGN=plt.subplots(2,2,figsize=(20,20)) ASSIGN=train_identity_new.id_01.value_counts().index ASSIGN=train_identity_new.id_01.value_counts() sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,0],orient='h') ax[0,0].set_title(""Bar chart for id_01"",size=20) ax[0,0].set_xlabel('counts',size=18) ax[0,0].set_ylabel('') ASSIGN=train_identity_new.id_12.value_counts().index ASSIGN=train_identity_new.id_12.value_counts() sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[0,1]) ax[0,1].set_title(""Bar chart for id_12"",size=20) ax[0,1].set_xlabel('counts',size=18) ax[0,1].set_ylabel('') ASSIGN=train_identity_new.id_38.value_counts().index ASSIGN=train_identity_new.id_38.value_counts() sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[1,0],order=['T','F']) ax[1,0].set_title(""Bar chart for id_38"",size=20) ax[1,0].set_xlabel('counts',size=18) ax[1,0].set_ylabel('') ASSIGN=train_identity_new.id_37.value_counts().index ASSIGN=train_identity_new.id_37.value_counts() sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN,ax=ax[1,1],order=['T','F']) ax[1,1].set_title(""Bar chart for id_37"",size=20) ax[1,1].set_xlabel('counts',size=18) ax[1,1].set_ylabel('')",1,"['fig,ax=plt.subplots(2,2,figsize=(20,20))\n', 'y=train_identity_new.id_01.value_counts().index\n', 'x=train_identity_new.id_01.value_counts()\n', ""sns.barplot(x=x,y=y,ax=ax[0,0],orient='h')\n"", 'ax[0,0].set_title(""Bar chart for id_01"",size=20)\n', ""ax[0,0].set_xlabel('counts',size=18)\n"", ""ax[0,0].set_ylabel('')\n"", '\n', 'y=train_identity_new.id_12.value_counts().index\n', 'x=train_identity_new.id_12.value_counts()\n', 'sns.barplot(x=x,y=y,ax=ax[0,1])\n', 'ax[0,1].set_title(""Bar chart for id_12"",size=20)\n', ""ax[0,1].set_xlabel('counts',size=18)\n"", ""ax[0,1].set_ylabel('')\n"", '\n', 'y=train_identity_new.id_38.value_counts().index\n', 'x=train_identity_new.id_38.value_counts()\n', ""sns.barplot(x=x,y=y,ax=ax[1,0],order=['T','F'])\n"", 'ax[1,0].set_title(""Bar chart for id_38"",size=20)\n', ""ax[1,0].set_xlabel('counts',size=18)\n"", ""ax[1,0].set_ylabel('')\n"", '\n', 'y=train_identity_new.id_37.value_counts().index\n', 'x=train_identity_new.id_37.value_counts()\n', ""sns.barplot(x=x,y=y,ax=ax[1,1],order=['T','F'])\n"", 'ax[1,1].set_title(""Bar chart for id_37"",size=20)\n', ""ax[1,1].set_xlabel('counts',size=18)\n"", ""ax[1,1].set_ylabel('')""]"
"def pyth_test (x1, x2): print (x1 + x2) pyth_test(1,2)",0,"['def pyth_test (x1, x2):\n', '   \n', '    print (x1 + x2)\n', '\n', 'pyth_test(1,2)']"
SETUP,0,"['import torch\n', 'import torch.nn as nn\n', 'import torch.nn.functional as F\n', 'import torch.optim as optim\n', 'from torch.utils.data import Dataset, DataLoader\n', 'from torchvision import transforms\n', 'from torchvision.utils import make_grid\n', 'import torchvision.utils as vutils\n', 'import matplotlib.animation as animation\n', 'from IPython.display import HTML\n', '\n', 'import matplotlib.pyplot as plt\n', 'from PIL import Image\n', 'import numpy as np\n', 'import pandas as pd\n', 'import copy\n', 'import math\n', 'import torch.utils.checkpoint as cp\n', '\n', 'import time\n', 'import cv2 as cv\n', 'from tqdm import tqdm_notebook as tqdm\n', 'import matplotlib.image as mpimg\n', 'from math import exp\n', '\n', 'import torchvision.transforms.functional as TF\n', 'from collections import OrderedDict\n']"
"ASSIGN = pathpath ASSIGN.mkdir(parents=True, exist_ok=True) download_images(pathpath, ASSIGN, max_pics=200)",0,"['# 创建各个类别图像下载的目录\n', 'dest = path/folder\n', 'dest.mkdir(parents=True, exist_ok=True)\n', '# 下载图像\n', 'download_images(path/file, dest, max_pics=200)']"
test_data['color'].unique(),0,"['#Check if train and test data have the same categories\n', ""test_data['color'].unique()""]"
"sub_df.to_csv('my_submission.csv', index=False)",0,"[""sub_df.to_csv('my_submission.csv', index=False)\n""]"
ASSIGN = pd.read_csv('..path'),0,"[""data_mat = pd.read_csv('../input/student-mat.csv')""]"
ASSIGN = np.zeros(xtest_tfidf.shape[0]),0,['test_y = np.zeros(xtest_tfidf.shape[0])']
"SETUP CHECKPOINT ASSIGN = input(""Enter first array:"") ASSIGN = inp1.split() ASSIGN = [int(i) for i in ASSIGN] ASSIGN = input(""Enter second array:"") ASSIGN = inp2.split() ASSIGN = [int(i) for i in ASSIGN] Arr1 = np.array(ASSIGN) Arr2 = np.array(ASSIGN) print() print(Arr1) print() print(Arr2) print() print(id(Arr1)==id(Arr2)) ASSIGN = Arr1%3==0 ASSIGN = Arr2%3==0 print() print(Arr1[ASSIGN]) print() print(Arr2[ASSIGN]) print() Arr2.sort() print(Arr2) print() print (Arr1.sum())",0,"['#Q1create two NumPy array by takin user input of data stored in array, check if they have views to same memory, check if elements of arrays are divisible by 3 or not sort 2nd array and find sum of all elements of 1st array\n', 'import numpy as np\n', 'inp1 = input(""Enter first array:"")\n', 'a = inp1.split()\n', 'a = [int(i) for i in a]\n', 'inp2 = input(""Enter second array:"")\n', 'b = inp2.split()\n', 'b = [int(i) for i in b]\n', 'Arr1 = np.array(a)\n', 'Arr2 = np.array(b)\n', 'print(""Array 1 :"")\n', 'print(Arr1)\n', 'print(""Array 2 :"")\n', 'print(Arr2)\n', 'print(""Do both of these arrays share the same memory :"")\n', 'print(id(Arr1)==id(Arr2))\n', 'div1 = Arr1%3==0\n', 'div2 = Arr2%3==0\n', 'print(""elements of array 1 divisible by 3 are :"")\n', 'print(Arr1[div1])\n', 'print(""elements of array 2 divisible by 3 are :"")\n', 'print(Arr2[div2])\n', 'print(""Array 2 after sorting is :"")\n', 'Arr2.sort()\n', 'print(Arr2)\n', 'print(""Sum of elements of array 1 is :"")\n', 'print (Arr1.sum())']"
CHECKPOINT print(os.listdir('..path')),0,"[""print(os.listdir('../input'))""]"
"final_df.plot(kind='box', rot=-30)",1,"[""final_df.plot(kind='box', rot=-30)""]"
"ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"")",0,"['cbd = pd.read_csv(""../input/crimeanalysis/crime_by_district.csv"")\n', 'cbdr = pd.read_csv(""../input/crimeanalysis/crime_by_district_rt.csv"")\n', 'cbs = pd.read_csv(""../input/crimeanalysis/crime_by_state.csv"")\n', 'cbsr = pd.read_csv(""../input/crimeanalysis/crime_by_state_rt.csv"")']"
"SETUP ASSIGN = random.sample(list(range(len(points_df))), 5000) ASSIGN = np.vstack([points_df['x'][sample_index_list], points_df['y'][sample_index_list], points_df['z'][ASSIGN], points_df['yaw'][ASSIGN], points_df['pitch'][ASSIGN], points_df['roll'][ASSIGN]]) ASSIGN = np.corrcoef(v) ASSIGN = plt.subplots(figsize=(7, 7)) ASSIGN = ax.imshow(CM) ax.set_xticks(np.arange(6)) ax.set_yticks(np.arange(6)) ax.set_xticklabels(['x', 'y', 'z', 'yaw', 'pitch', 'roll']) ax.set_yticklabels(['x', 'y', 'z', 'yaw', 'pitch', 'roll']) for i in range(6): for j in range(6): ASSIGN = ax.ASSIGN(j, i, round(CM[i, j], 2), ASSIGN=""center"", va=""center"", color=""w"") fig.tight_layout() plt.show()",1,"['import random\n', 'sample_index_list = random.sample(list(range(len(points_df))), 5000)\n', ""v = np.vstack([points_df['x'][sample_index_list], points_df['y'][sample_index_list], \n"", ""               points_df['z'][sample_index_list], points_df['yaw'][sample_index_list], \n"", ""               points_df['pitch'][sample_index_list], points_df['roll'][sample_index_list]])\n"", 'CM = np.corrcoef(v)\n', '\n', 'fig, ax = plt.subplots(figsize=(7, 7))\n', 'im = ax.imshow(CM)\n', 'ax.set_xticks(np.arange(6))\n', 'ax.set_yticks(np.arange(6))\n', ""ax.set_xticklabels(['x', 'y', 'z', 'yaw', 'pitch', 'roll'])\n"", ""ax.set_yticklabels(['x', 'y', 'z', 'yaw', 'pitch', 'roll'])\n"", 'for i in range(6):\n', '    for j in range(6):\n', '        text = ax.text(j, i, round(CM[i, j], 2),\n', '                       ha=""center"", va=""center"", color=""w"")\n', 'fig.tight_layout()\n', 'plt.show()']"
"plt.figure(figsize=(12,5)) plt.title(""Distribution of train image locations(Train location Variable)"") ASSIGN = sns.distplot(train[""location""])",1,"['plt.figure(figsize=(12,5))\n', 'plt.title(""Distribution of train image locations(Train location Variable)"")\n', 'ax = sns.distplot(train[""location""])']"
"df.drop(['Serial No.'], axis = 1, inplace = True)",0,"[""df.drop(['Serial No.'], axis = 1, inplace = True)""]"
"CHECKPOINT ASSIGN = vg_sales[['Name', 'JP_Sales', 'NA_Sales']][(vg_sales.Year>=1992) & (vg_sales.Year<=1996) & (vg_sales.JP_Sales > vg_sales.NA_Sales)].sort(columns = 'JP_Sales', ascending = False) print(ASSIGN.head(20))",0,"[""japan1992_1996 = vg_sales[['Name', 'JP_Sales', 'NA_Sales']][(vg_sales.Year>=1992) & (vg_sales.Year<=1996) & (vg_sales.JP_Sales > vg_sales.NA_Sales)].sort(columns = 'JP_Sales', ascending = False)\n"", 'print(japan1992_1996.head(20))']"
"CHECKPOINT ASSIGN = LinearRegression() ASSIGN.fit(x_t, Y_t) ASSIGN = model14.score(x_es,Y_es) print(ASSIGN*100,'%')",0,"['model14 = LinearRegression()\n', 'model14.fit(x_t, Y_t)\n', '\n', 'accuracy14 = model14.score(x_es,Y_es)\n', ""print(accuracy14*100,'%')""]"
"MNB.fit(x_train,x_test) MNB.score(y_train,y_test)",0,"['MNB.fit(x_train,x_test)\n', 'MNB.score(y_train,y_test)']"
CHECKPOINT cbd.shape,0,['cbd.shape']
"plotCorrelationMatrix(df1, 196)",1,"['plotCorrelationMatrix(df1, 196)']"
"ASSIGN=plt.subplots(1,2,figsize=(20,10)) sns.barplot(x=data2.sex.value_counts().sort_values(ascending=False),y=data2.sex.value_counts().sort_values(ascending=False).index,ax=ax[0]) ax[0].set_title(""Number of patient by sex"",size=20) ax[0].set_xlabel('patient',size=18) sns.barplot(x=data2.country.value_counts().sort_values(ascending=False),y=data2.country.value_counts().sort_values(ascending=False).index,ax=ax[1]) ax[1].set_title(""Number of patient by country"",size=20) ax[1].set_xlabel('patient',size=18)",1,"['fig,ax=plt.subplots(1,2,figsize=(20,10))\n', 'sns.barplot(x=data2.sex.value_counts().sort_values(ascending=False),y=data2.sex.value_counts().sort_values(ascending=False).index,ax=ax[0])\n', 'ax[0].set_title(""Number of patient by sex"",size=20)\n', ""ax[0].set_xlabel('patient',size=18)\n"", 'sns.barplot(x=data2.country.value_counts().sort_values(ascending=False),y=data2.country.value_counts().sort_values(ascending=False).index,ax=ax[1])\n', 'ax[1].set_title(""Number of patient by country"",size=20)\n', ""ax[1].set_xlabel('patient',size=18)""]"
tf.isnull().sum(),0,['tf.isnull().sum()']
"df_all['date_account_created'] = pd.to_datetime(df_all['date_account_created'],format='%Y-%m-%d')",0,"[""df_all['date_account_created'] = pd.to_datetime(df_all['date_account_created'],format='%Y-%m-%d')""]"
SLICE=df.Attrition.eq('Yes').mul(1),0,"[""df['Attrition']=df.Attrition.eq('Yes').mul(1)""]"
covid.tail(),0,"['# verificação final\n', 'covid.tail()']"
"plt.figure(figsize=(25,35)) ASSIGN=1 for k in ['01','02','03','04','05','06','07','08']: ASSIGN=TOTAvgRank_DD_DAP_TE_TB('ASSIGN Im',str(k)) ASSIGN=[] ASSIGN=[] ASSIGN=[] for i in range(4): r1,r2=ASSIGN['rank'+str(i)].split('path') R=float(r1)path(r2) R=1-R ASSIGN.append(1.5+R*math.sin(math.pipath+i*math.pipath)) ASSIGN.append(1.5+R*math.cos(math.pipath+i*math.pipath)) ASSIGN.append(ASSIGN['type'+str(i)]+"" rank: ""+ASSIGN['rank'+str(i)]) ASSIGN.append(ASSIGN[0]) ASSIGN.append(ASSIGN[0]) plt.subplot(4,2,ASSIGN) plt.plot(ASSIGN,ASSIGN, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2) for i, txt in enumerate(ASSIGN): plt.annotate(txt, (ASSIGN[i]-0.2, ASSIGN[i])) plt.xlim(0.5,2.5) plt.ylim(0.5,2.5) plt.fill(ASSIGN, ASSIGN,""greenyellow"") plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2) plt.title(""ASSIGN Im's performance in month ""+str(k),size=18)",1,"['plt.figure(figsize=(25,35))\n', 'j=1\n', ""for k in ['01','02','03','04','05','06','07','08']:\n"", ""    Sungjae=TOTAvgRank_DD_DAP_TE_TB('Sungjae Im',str(k))\n"", '    y=[]\n', '    x=[]\n', '    n=[]\n', '    for i in range(4):\n', ""        r1,r2=Sungjae['rank'+str(i)].split('/')\n"", '        R=float(r1)/float(r2)\n', '        R=1-R\n', '        y.append(1.5+R*math.sin(math.pi/4+i*math.pi/2))\n', '        x.append(1.5+R*math.cos(math.pi/4+i*math.pi/2))\n', '        n.append(Sungjae[\'type\'+str(i)]+"" rank: ""+Sungjae[\'rank\'+str(i)])\n', '\n', '    x.append(x[0])\n', '    y.append(y[0])\n', '    plt.subplot(4,2,j)\n', ""    plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n"", '    for i, txt in enumerate(n):\n', '        plt.annotate(txt, (x[i]-0.2, y[i]))\n', '    plt.xlim(0.5,2.5)\n', '    plt.ylim(0.5,2.5)\n', '    plt.fill(x, y,""greenyellow"")\n', ""    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n"", '    plt.title(""Sungjae Im\'s performance in month ""+str(k),size=18)\n']"
ASSIGN = ASSIGN.sort_values(by=['image-names']) ASSIGN = ASSIGN.sort_values(by=['image-names']),0,"[""pred_df_with_species_name = pred_df_with_species_name.sort_values(by=['image-names'])\n"", ""pred_df_with_cat_number = pred_df_with_cat_number.sort_values(by=['image-names'])""]"
"CHECKPOINT ASSIGN=Sequential() ASSIGN.add(Convolution2D(filters=32,kernel_size=(3,3),input_shape=(32,32,3),padding='same')) ASSIGN.add(BatchNormalization()) ASSIGN.add(Activation('relu')) ASSIGN.add(Dropout(rate=0.35)) ASSIGN.add(MaxPool2D(pool_size=(2,2),padding='same')) ASSIGN.add(Convolution2D(filters=64,kernel_size=(3,3),padding='same')) ASSIGN.add(BatchNormalization()) ASSIGN.add(Activation('relu')) ASSIGN.add(Dropout(rate=0.45)) ASSIGN.add(MaxPool2D(pool_size=(2,2),padding='same')) ASSIGN.add(Flatten()) ASSIGN.add(Dense(1024,activation='relu')) ASSIGN.add(BatchNormalization()) ASSIGN.add(Activation('relu')) ASSIGN.add(Dropout(rate=0.75)) ASSIGN.add(Dense(max(label)+1,activation='softmax')) ASSIGN.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) ASSIGN=model.fit(X_train,y_train,validation_split=0.2,epochs=20,batch_size=128,verbose=1) ASSIGN=model.evaluate(X_test,y_test,verbose=1) print(,accuracy[1])#accuracy for test set def show_train_history(ASSIGN,train,validation): plt.plot(ASSIGN.history[train]) plt.plot(ASSIGN.history[validation]) plt.title('Train History') plt.ylabel('train') plt.xlabel('Epoch') plt.legend(['train','validation'],loc='upper left') plt.show() show_train_history(ASSIGN,'acc','val_acc')",1,"['model=Sequential()\n', ""model.add(Convolution2D(filters=32,kernel_size=(3,3),input_shape=(32,32,3),padding='same'))\n"", 'model.add(BatchNormalization())\n', ""model.add(Activation('relu'))\n"", 'model.add(Dropout(rate=0.35))\n', '\n', ""model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n"", '\n', ""model.add(Convolution2D(filters=64,kernel_size=(3,3),padding='same'))\n"", 'model.add(BatchNormalization())\n', ""model.add(Activation('relu'))\n"", 'model.add(Dropout(rate=0.45))\n', '\n', ""model.add(MaxPool2D(pool_size=(2,2),padding='same'))\n"", '\n', 'model.add(Flatten())\n', '\n', ""model.add(Dense(1024,activation='relu'))\n"", 'model.add(BatchNormalization())\n', ""model.add(Activation('relu'))\n"", 'model.add(Dropout(rate=0.75))\n', '\n', ""model.add(Dense(max(label)+1,activation='softmax'))\n"", '\n', ""model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n"", '\n', 'train_history=model.fit(X_train,y_train,validation_split=0.2,epochs=20,batch_size=128,verbose=1)\n', 'accuracy=model.evaluate(X_test,y_test,verbose=1)\n', 'print(""test accuracy:"",accuracy[1])#accuracy for test set\n', '\n', '\n', '\n', 'def show_train_history(train_history,train,validation):\n', '\tplt.plot(train_history.history[train])\n', '\tplt.plot(train_history.history[validation])\n', ""\tplt.title('Train History')\n"", ""\tplt.ylabel('train')\n"", ""\tplt.xlabel('Epoch')\n"", ""\tplt.legend(['train','validation'],loc='upper left')\n"", '\tplt.show()\n', '\n', ""show_train_history(train_history,'acc','val_acc') #acc:accuracy for training set. val_acc:accuracy for validation.""]"
submission_df['income class'] = submission_df['income class'] + correction submission_df['income class'] = submission_df['income class'].apply(np.round) submission_df['income class'] = submission_df['income class'].astype(int),0,"['# apply correction, classify 0/1 and make it int\n', ""submission_df['income class'] = submission_df['income class'] + correction # tensor correction\n"", ""submission_df['income class'] = submission_df['income class'].apply(np.round)\n"", ""submission_df['income class'] = submission_df['income class'].astype(int)""]"
"data_features.loc[condition6,'MasVnrType'] = 'Stone' data_features['MasVnrType'] = data_features['MasVnrType'].fillna('None') data_features['MasVnrArea'] = data_features['MasVnrArea'].fillna(0)",0,"[""data_features.loc[condition6,'MasVnrType'] = 'Stone'\n"", ""data_features['MasVnrType'] = data_features['MasVnrType'].fillna('None')\n"", ""data_features['MasVnrArea'] = data_features['MasVnrArea'].fillna(0)""]"
train.head(),0,['train.head()']
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n"", '#MAYUKH GHOSH 18BCE0417']"
CHECKPOINT all_to_investigate,0,['all_to_investigate']
covid['SARS-Cov-2 exam result'].value_counts(),0,"['# Verificando os valores da coluna target\n', ""covid['SARS-Cov-2 exam result'].value_counts()""]"
df.tail(),0,['df.tail()']
"X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)",0,"['X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)']"
"CHECKPOINT print('Cross_val Score RandomForestClassifier = ', cross_val_score(rfc, X, y.reshape(-1), cv=5).mean())",0,"[""print('Cross_val Score RandomForestClassifier = ', cross_val_score(rfc, X, y.reshape(-1), cv=5).mean())""]"
"ASSIGN=torch.FloatTensor(create_class_weights()).to(device) ASSIGN = nn.NLLLoss() ASSIGN = optim.Adam(model.parameters(), lr=0.001) ASSIGN = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)",0,"['weight=torch.FloatTensor(create_class_weights()).to(device)\n', 'criterion = nn.NLLLoss()\n', 'optimizer = optim.Adam(model.parameters(), lr=0.001)\n', 'exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)']"
"data.iloc[1000,:]",0,"['data.iloc[1000,:]']"
"CHECKPOINT ASSIGN = dataset.drop(['Chance of Admit'], axis = 1) ex",0,"[""ex = dataset.drop(['Chance of Admit'], axis = 1)\n"", 'ex']"
SETUP,0,['import matplotlib.pyplot as plt']
"sns.catplot(x ='Survived', y ='Age', data = df1)",1,"[""sns.catplot(x ='Survived', y ='Age', data = df1)""]"
ASSIGN = df1['Survived'],0,"[""x_test = df1['Survived']""]"
"X_train.drop(['gender','PhoneService','TotalCharges','tenure'], axis=1, inplace=True) X_test.drop(['gender','PhoneService','TotalCharges','tenure'], axis=1, inplace=True)",0,"[""# Removing unimportant features ['gender','PhoneService','TotalCharges','tenure']\n"", ""X_train.drop(['gender','PhoneService','TotalCharges','tenure'], axis=1, inplace=True)\n"", ""X_test.drop(['gender','PhoneService','TotalCharges','tenure'], axis=1, inplace=True)""]"
"ASSIGN=folium.Map() ASSIGN=pd.DataFrame({""Lat"":stations['Lat'],""Long"":stations['Long']}) ASSIGN.add_child(plugins.HeatMap(data=ASSIGN))",1,"['map_all=folium.Map()\n', 'stations_new=pd.DataFrame({""Lat"":stations[\'Lat\'],""Long"":stations[\'Long\']})\n', 'map_all.add_child(plugins.HeatMap(data=stations_new))\n', '\n']"
"test_df['prediction'] = pp.predict(test_df.question_text.to_numpy()) test_df[['qid','prediction']].to_csv(""submission.csv"", index=False)",0,"[""test_df['prediction'] = pp.predict(test_df.question_text.to_numpy())\n"", 'test_df[[\'qid\',\'prediction\']].to_csv(""submission.csv"", index=False)']"
"ASSIGN = pd.concat([data,dummy1,dummy2,dummy3], axis = 'columns')",0,"[""merge = pd.concat([data,dummy1,dummy2,dummy3], axis = 'columns')""]"
"ASSIGN=plt.subplots(2,1,figsize=(15,15)) sns.boxplot(x=""MSSubClass"", y=""SalePrice"", data=train,ax=ax[0]) ax[0].set_title(""Boxplot of Price for MSSubClass"",size=20) ASSIGN=ASSIGN[ASSIGN.SalePrice<=400000] sns.boxplot(x=""MSSubClass"", y=""SalePrice"", data=ASSIGN,ax=ax[1]) ax[1].set_title(""Boxplot of Price for MSSubClass(price<=400000)"",size=20)",1,"['fig,ax=plt.subplots(2,1,figsize=(15,15))\n', 'sns.boxplot(x=""MSSubClass"", y=""SalePrice"", data=train,ax=ax[0])\n', 'ax[0].set_title(""Boxplot of Price for MSSubClass"",size=20)\n', '\n', 'train=train[train.SalePrice<=400000]\n', 'sns.boxplot(x=""MSSubClass"", y=""SalePrice"", data=train,ax=ax[1])\n', 'ax[1].set_title(""Boxplot of Price for MSSubClass(price<=400000)"",size=20)']"
"CHECKPOINT model2.save(""model2.h5"") print()",0,"['model2.save(""model2.h5"")\n', 'print(""Saved model to disk"")']"
"ASSIGN = pd.concat([output1,output2],axis = 1) ASSIGN.sort_values(by=['Accuracy after HT'], inplace=True, ascending=False) ASSIGN.head(10)",0,"['output = pd.concat([output1,output2],axis = 1)\n', ""output.sort_values(by=['Accuracy after HT'], inplace=True, ascending=False)\n"", 'output.head(10)']"
"plt.figure(figsize=(12,5)) plt.title(""Distribution of test image locations(Test location Variable)"") ASSIGN = sns.distplot(test[""location""])",1,"['plt.figure(figsize=(12,5))\n', 'plt.title(""Distribution of test image locations(Test location Variable)"")\n', 'ax = sns.distplot(test[""location""])']"
SETUP,0,['import matplotlib.pyplot as plt']
"ASSIGN = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers']] ASSIGN = df_train['ASSIGN'] del df_train['ASSIGN']",0,"[""df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n"", ""target = df_train['target']\n"", ""del df_train['target']""]"
"CHECKPOINT ASSIGN = sum(list(confirm_dict.values())) ASSIGN = sum(list(deaths_dict.values())) ASSIGN = sum(list(recover_dict.values())) ASSIGN = total_confirmed -(total_deaths+total_recovered) print(, ASSIGN)",0,"['total_confirmed = sum(list(confirm_dict.values()))\n', 'total_deaths = sum(list(deaths_dict.values()))\n', 'total_recovered = sum(list(recover_dict.values()))\n', '\n', 'total_still_affected = total_confirmed -(total_deaths+total_recovered)\n', 'print(""World Population affectedas of 22nd March 2020: "", total_confirmed)']"
headline_by_year(2005),0,['headline_by_year(2005)']
SETUP ASSIGN = load_breast_cancer() ASSIGN.keys(),0,"['from sklearn.datasets import load_breast_cancer\n', 'cancer = load_breast_cancer()\n', 'cancer.keys()']"
"titanic_df.drop(""Cabin"",axis=1,inplace=True) test_df.drop(""Cabin"",axis=1,inplace=True)",0,"['# Cabin\n', ""# It has a lot of NaN values, so it won't cause a remarkable impact on prediction\n"", 'titanic_df.drop(""Cabin"",axis=1,inplace=True)\n', 'test_df.drop(""Cabin"",axis=1,inplace=True)']"
data.carsList.apply(lambda x:x.strip('[]')) SLICE=data.carsList.apply(lambda x:x.strip('[]')) data.head(),0,"[""data.carsList.apply(lambda x:x.strip('[]'))\n"", ""data['carsList']=data.carsList.apply(lambda x:x.strip('[]'))\n"", 'data.head()']"
"CHECKPOINT def plotCorrelationMatrix(df, graphWidth): ASSIGN = df.dataframeName ASSIGN = ASSIGN.dropna('columns') ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]] if ASSIGN.shape[1] < 2: print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2') return ASSIGN = df.ASSIGN() plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k') ASSIGN = plt.matshow(corr, fignum = 1) plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90) plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns) plt.gca().xaxis.tick_bottom() plt.colorbar(ASSIGN) plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15) plt.show()",1,"['# Correlation matrix\n', 'def plotCorrelationMatrix(df, graphWidth):\n', '    filename = df.dataframeName\n', ""    df = df.dropna('columns') # drop columns with NaN\n"", '    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n', '    if df.shape[1] < 2:\n', ""        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n"", '        return\n', '    corr = df.corr()\n', ""    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n"", '    corrMat = plt.matshow(corr, fignum = 1)\n', '    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n', '    plt.yticks(range(len(corr.columns)), corr.columns)\n', '    plt.gca().xaxis.tick_bottom()\n', '    plt.colorbar(corrMat)\n', ""    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n"", '    plt.show()\n']"
"def preprocess_img(img): ASSIGN=(100,100) ASSIGN = cv2.resize(img, dim, interpolation=cv2.INTER_LINEAR) return res",0,"['def preprocess_img(img):\n', '    dim=(100,100)\n', '    res = cv2.resize(img, dim, interpolation=cv2.INTER_LINEAR)\n', '    return res\n', '    ']"
ASSIGN = confirm_dict_sorted[:10] ASSIGN = deaths_dict_sorted[:10] ASSIGN = recover_dict_sorted[:10] ASSIGN = dict(ASSIGN) ASSIGN = dict(ASSIGN) ASSIGN = dict(ASSIGN),0,"['top10_confirm = confirm_dict_sorted[:10]\n', 'top10_deaths = deaths_dict_sorted[:10]\n', 'top10_recover = recover_dict_sorted[:10]\n', 'top10_confirm = dict(top10_confirm)\n', 'top10_deaths = dict(top10_deaths)\n', 'top10_recover = dict(top10_recover)']"
"my_submission['target'] = my_submission['target'].map({0:0, 1:4})",0,"[""my_submission['target'] = my_submission['target'].map({0:0, 1:4})""]"
"SETUP CHECKPOINT ASSIGN = dtree.score(train_x, train_y) print('training accuracy: %.5f' % ASSIGN) ASSIGN=accuracy_score(test_y, predict_y) print('test accuracy: %.5f' % ASSIGN)",0,"['#計算訓練數據與測試數據的正確率\n', 'from sklearn.metrics import accuracy_score\n', 'acc_log = dtree.score(train_x, train_y)\n', ""print('training accuracy: %.5f' % acc_log)\n"", 'x=accuracy_score(test_y, predict_y)\n', ""print('test accuracy: %.5f' % x)""]"
"ASSIGN=pd.read_csv(""..path"") ASSIGN=pd.read_csv(""..path"") ASSIGN['image_id']=ASSIGN['image_id']+'.jpg' ASSIGN['image_id']=ASSIGN['image_id']+'.jpg' ASSIGN.head()",0,"['train=pd.read_csv(""../input/plant-pathology-2020-fgvc7/train.csv"")\n', 'test=pd.read_csv(""../input/plant-pathology-2020-fgvc7/test.csv"")\n', ""train['image_id']=train['image_id']+'.jpg'\n"", ""test['image_id']=test['image_id']+'.jpg'\n"", 'train.head()']"
"learn.fit_one_cycle(1,max_lr=1e-8)",0,"['learn.fit_one_cycle(1,max_lr=1e-8)']"
"CHECKPOINT ASSIGN = ""..path"" ASSIGN = 0 ASSIGN =[] for n in range(len(folder)): ASSIGN = os.path.join(base_path, folder[n]) print(ASSIGN) ASSIGN = os.listdir(image_path) print(len(ASSIGN)) ASSIGN.append(len(ASSIGN)) ASSIGN += len(ASSIGN) print(.format(ASSIGN)) print(ASSIGN)",0,"['base_path = ""../input/lung-colon-normal/trainable_normal""\n', 'total_images = 0\n', 'image_class =[]\n', 'for n in range(len(folder)):\n', '  image_path = os.path.join(base_path, folder[n]) \n', '  print(image_path)\n', '  # class_path = patient_path + ""/"" + str(c) + ""/""\n', '  subfiles = os.listdir(image_path)\n', '  print(len(subfiles))\n', '  image_class.append(len(subfiles))\n', '  total_images += len(subfiles)\n', 'print(""The number of total images are:{}"".format(total_images))  \n', 'print(image_class)']"
"CHECKPOINT ASSIGN=data_risk1[:2000] ASSIGN=data_risk1_2000.Longitude.mean() ASSIGN=data_risk1_2000.Latitude.mean() ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(risk1_map) for lat,lon,label in zip(ASSIGN.Latitude,ASSIGN.Longitude,ASSIGN['AKA Name']): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN) risk1_map",1,"['data_risk1_2000=data_risk1[:2000]\n', 'Long=data_risk1_2000.Longitude.mean()\n', 'Lat=data_risk1_2000.Latitude.mean()\n', 'risk1_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'risk1_distribution_map=plugins.MarkerCluster().add_to(risk1_map)\n', ""for lat,lon,label in zip(data_risk1_2000.Latitude,data_risk1_2000.Longitude,data_risk1_2000['AKA Name']):\n"", '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(risk1_distribution_map)\n', 'risk1_map.add_child(risk1_distribution_map)\n', '\n', 'risk1_map']"
"def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow): ASSIGN = df.ASSIGN() ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]] nRow, nCol = ASSIGN.shape ASSIGN = list(df) ASSIGN = (nCol + nGraphPerRow - 1) path plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k') for i in range(min(nCol, nGraphShown)): plt.subplot(ASSIGN, nGraphPerRow, i + 1) ASSIGN = df.iloc[:, i] if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)): ASSIGN = columnDf.value_counts() ASSIGN.plot.bar() else: ASSIGN.hist() plt.ylabel('counts') plt.xticks(rotation = 90) plt.title(f'{ASSIGN[i]} (column {i})') plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0) plt.show()",1,"['# Distribution graphs (histogram/bar graph) of column data\n', 'def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n', '    nunique = df.nunique()\n', '    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n', '    nRow, nCol = df.shape\n', '    columnNames = list(df)\n', '    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n', ""    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n"", '    for i in range(min(nCol, nGraphShown)):\n', '        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n', '        columnDf = df.iloc[:, i]\n', '        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n', '            valueCounts = columnDf.value_counts()\n', '            valueCounts.plot.bar()\n', '        else:\n', '            columnDf.hist()\n', ""        plt.ylabel('counts')\n"", '        plt.xticks(rotation = 90)\n', ""        plt.title(f'{columnNames[i]} (column {i})')\n"", '    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n', '    plt.show()\n']"
"X=train_data.drop(['id','type'],axis=1) ASSIGN=pd.get_dummies(train_data['type'])",0,"[""X=train_data.drop(['id','type'],axis=1)\n"", ""y=pd.get_dummies(train_data['type'])""]"
"sns.scatterplot(data = data, x = '401K Savings', y = 'Age', hue = 'Retire')",1,"['#Scatterplot\n', ""sns.scatterplot(data = data, x = '401K Savings', y = 'Age', hue = 'Retire')""]"
"ASSIGN = train_test_split(list(range(Images.shape[0])), train_size=0.8, test_size=0.2, shuffle=False) ASSIGN = Images[indices_train] ASSIGN = Classes[indices_train] ASSIGN = Images[indices_test] ASSIGN = Classes[indices_test] x_train.shape, y_train.shape, x_test.shape, y_test.shape",0,"['## split train / test\n', '\n', 'indices_train, indices_test = train_test_split(list(range(Images.shape[0])), train_size=0.8, test_size=0.2, shuffle=False)\n', '\n', 'x_train = Images[indices_train]\n', 'y_train = Classes[indices_train]\n', 'x_test = Images[indices_test]\n', 'y_test = Classes[indices_test]\n', '\n', 'x_train.shape, y_train.shape, x_test.shape, y_test.shape']"
"ASSIGN = tabular_learner(data, layers=[200,100], metrics=[accuracy, AUROC()],callback_fns=ShowGraph) ASSIGN.lr_find() ASSIGN.recorder.plot(suggestion=True)",1,"['# build NN learner and look at learning rate curve\n', 'learn = tabular_learner(data, layers=[200,100], metrics=[accuracy, AUROC()],callback_fns=ShowGraph)\n', '\n', 'learn.lr_find()\n', 'learn.recorder.plot(suggestion=True)']"
"ASSIGN = Image.open('..path') ASSIGN = ASSIGN.resize((224,224)) plt.imshow(ASSIGN)",1,"[""img = Image.open('../input/naruto/naruto.jpg')\n"", 'img = img.resize((224,224))\n', 'plt.imshow(img)']"
SETUP,0,['from sklearn.tree import DecisionTreeRegressor']
SETUP,0,['from sklearn.linear_model import LogisticRegression']
learn.predict(inception[0])[0],0,['learn.predict(inception[0])[0]']
plt.savefig('results1.png') f.savefig('results1.png'),0,"[""plt.savefig('results1.png')\n"", ""f.savefig('results1.png')""]"
"LR.fit(x_train,x_test) ASSIGN = LR.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('LogisticRegression(No HT).csv',index = False)",0,"['LR.fit(x_train,x_test)\n', 'model1pred = LR.predict(y_train)\n', ""submission1 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission1['PassengerId'] = result['PassengerId']\n"", ""submission1['Survived'] = model1pred\n"", ""submission1.to_csv('LogisticRegression(No HT).csv',index = False)""]"
"CHECKPOINT ASSIGN = pd.read_excel('..path', sheet_name=0,dtype=dtypes) print(.format(ASSIGN.shape))",0,"['# source : https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/\n', ""df = pd.read_excel('../input/WA_Fn-UseC_-HR-Employee-Attrition.xlsx', sheet_name=0,dtype=dtypes)\n"", 'print(""Shape of dataframe is: {}"".format(df.shape))']"
"SETUP ASSIGN = torch.ASSIGN(""cuda"" if torch.cuda.is_available() else ""cpu"") model.to(ASSIGN)",0,"['import torch\n', 'device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n', '# Move model to the device specified above\n', 'model.to(device)\n']"
"plt.figure(figsize=(15,6)) sns.distplot(points_df['yaw'], bins=500); plt.xlabel('yaw') plt.show()",1,"['plt.figure(figsize=(15,6))\n', ""sns.distplot(points_df['yaw'], bins=500);\n"", ""plt.xlabel('yaw')\n"", 'plt.show()']"
"CHECKPOINT ASSIGN = pd.DataFrame(ASSIGN, columns=['Survived']) ASSIGN = pd.read_csv('path') ASSIGN = pd.concat((test.iloc[:, 0], ASSIGN), axis = 1) ASSIGN.to_csv('submission1.csv', sep="","", index = False) print('end')",0,"[""predictions = pd.DataFrame(predictions, columns=['Survived'])\n"", ""test = pd.read_csv('/kaggle/input/titanic/test.csv')\n"", 'predictions = pd.concat((test.iloc[:, 0], predictions), axis = 1)\n', 'predictions.to_csv(\'submission1.csv\', sep="","", index = False)\n', '\n', ""print('end')""]"
"df_all['age'].fillna(-1, inplace=True)",0,"[""df_all['age'].fillna(-1, inplace=True)""]"
ASSIGN = pd.read_csv('path'),0,"[""country_info = pd.read_csv('/kaggle/input/population/population_by_country_2020.csv')""]"
"ASSIGN = pd.get_dummies(ASSIGN, columns = [""Sex""],prefix=""Gender_"")",0,"['final = pd.get_dummies(final, columns = [""Sex""],prefix=""Gender_"")']"
"SETUP CHECKPOINT def challenge_data_conversion(challenge_data): ASSIGN = [] ASSIGN.append(challenge_data['id']) ASSIGN.append(1 if len(challenge_data['winners']) > 0 else 0) ASSIGN.append(len(challenge_data['winners'])) return output def data_conversion(training_file_path): ASSIGN = pd.DataFrame(columns=['id', 'hasWinner', 'numOfWinners']) ASSIGN = [] ASSIGN = [""json""] for extension in ASSIGN: ASSIGN = glob.glob(training_file_path+""path*.""+extension) ASSIGN.extend(ASSIGN) print(str(len(ASSIGN))+' files') for file_path in ASSIGN: with open(file_path,'r') as f: ASSIGN = json.load(f) for challenge_data in ASSIGN: ASSIGN.loc[len(ASSIGN)] = challenge_data_conversion(challenge_data) return data_df ASSIGN = data_conversion('..path') ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path')",0,"['import numpy as np\n', 'import pandas as pd\n', 'import joblib\n', 'import lightgbm as lgb\n', 'import time\n', 'import pickle\n', 'import math\n', 'import string\n', 'import datetime\n', 'from sklearn.model_selection import KFold\n', 'from sklearn.metrics import f1_score, mean_squared_error\n', 'import glob\n', 'import json\n', '\n', 'def challenge_data_conversion(challenge_data):\n', '    output = []\n', ""    output.append(challenge_data['id'])\n"", ""    output.append(1 if len(challenge_data['winners']) > 0 else 0)\n"", ""    output.append(len(challenge_data['winners']))\n"", '    \n', '    return output\n', '\n', 'def data_conversion(training_file_path):\n', ""    data_df = pd.DataFrame(columns=['id', 'hasWinner', 'numOfWinners'])\n"", '    file_list = []\n', '    extensions = [""json""]\n', '    for extension in extensions:\n', '        file_glob = glob.glob(training_file_path+""/*.""+extension)\n', '        file_list.extend(file_glob)\n', ""    print(str(len(file_list))+' files')\n"", '        \n', '    for file_path in file_list:\n', ""        with open(file_path,'r') as f:\n"", '            data_dict = json.load(f)\n', '        for challenge_data in data_dict:\n', '            #try:\n', '            data_df.loc[len(data_df)] = challenge_data_conversion(challenge_data)\n', '            #except:\n', '            #    print(challenge_data_conversion(challenge_data))\n', '            \n', '            \n', '    return data_df\n', '\n', ""test_data = data_conversion('../input/challenge-health-notification-test-data/')\n"", ""reg_output = pd.read_csv('../input/challenge-health-notification-reg-output/lightgbm_numOfWinners_prediction.csv')\n"", ""cls_output = pd.read_csv('../input/challenge-health-notification-cls-output/lightgbm_hasWinner_prediction.csv')""]"
"relevant.apply(falta_numero, axis=0)",0,"['relevant.apply(falta_numero, axis=0)']"
"for index,row in train.iterrows(): if train.iloc[index].Province == train.iloc[index - 1].Province and train.iloc[index].ConfirmedCases < train.iloc[index-1].ConfirmedCases: train.iloc[index,4] = train.iloc[index-1,4] if train.iloc[index].Province == train.iloc[index - 1].Province and train.iloc[index].Fatalities < train.iloc[index-1].Fatalities: train.iloc[index,5] = train.iloc[index-1,5]",0,"['for index,row in train.iterrows():\n', '    if train.iloc[index].Province == train.iloc[index - 1].Province and train.iloc[index].ConfirmedCases < train.iloc[index-1].ConfirmedCases:\n', '        train.iloc[index,4] = train.iloc[index-1,4]\n', '    if train.iloc[index].Province == train.iloc[index - 1].Province and train.iloc[index].Fatalities < train.iloc[index-1].Fatalities:\n', '        train.iloc[index,5] = train.iloc[index-1,5]']"
"ASSIGN = plt.subplots(2,2) axarr[0,0].title.set_text('Original \n Image') axarr[0,1].title.set_text('Reconstructed Image with \n 43% Compression') axarr[1,0].title.set_text('Reconstructed Image with \n 68% Compression') axarr[1,1].title.set_text('Reconstructed Image with \n 84% Compression') for i in range(2): for j in range(2): axarr[i,j].title.set_fontsize(40) ASSIGN = 0 ASSIGN = (valid_batch_1[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5 ASSIGN = (reconstructed_img_28_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5 ASSIGN = (reconstructed_img_16_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5 ASSIGN = (reconstructed_img_8_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5 axarr[0,0].imshow(ASSIGN) axarr[0,1].imshow(ASSIGN) axarr[1,0].imshow(ASSIGN) axarr[1,1].imshow(ASSIGN) f.set_figheight(50) f.set_figwidth(50) plt.show()",1,"['f, axarr = plt.subplots(2,2)\n', '\n', ""axarr[0,0].title.set_text('Original \\n Image')\n"", ""axarr[0,1].title.set_text('Reconstructed Image with \\n 43% Compression')\n"", ""axarr[1,0].title.set_text('Reconstructed Image with \\n 68% Compression')\n"", ""axarr[1,1].title.set_text('Reconstructed Image with \\n 84% Compression')\n"", '\n', 'for i in range(2):\n', '    for j in range(2):\n', '        axarr[i,j].title.set_fontsize(40)\n', 'i = 0\n', '\n', '\n', 'reimg = (valid_batch_1[i].cpu().detach().permute(1, 2, 0) * 0.5) + 0.5\n', 'reimg_28 = (reconstructed_img_28_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\n', 'reimg_16 = (reconstructed_img_16_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\n', 'reimg_8 = (reconstructed_img_8_1[i].cpu().detach().permute(1, 2, 0) *0.5) + 0.5\n', '\n', '\n', '\n', 'axarr[0,0].imshow(reimg)\n', 'axarr[0,1].imshow(reimg_28)\n', 'axarr[1,0].imshow(reimg_16)\n', 'axarr[1,1].imshow(reimg_8)\n', 'f.set_figheight(50)\n', 'f.set_figwidth(50)\n', 'plt.show()']"
df.head(5),0,['df.head(5)']
"ASSIGN = ElasticNet() ASSIGN = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]} ASSIGN = GridSearchCV(Elasticnet, parameters, scoring='neg_mean_squared_error', cv = 100) ASSIGN.fit(X_train, y_train) ASSIGN.best_params_",0,"['Elasticnet = ElasticNet()\n', '\n', ""parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n"", '\n', ""en_regressor = GridSearchCV(Elasticnet, parameters, scoring='neg_mean_squared_error', cv = 100)\n"", '\n', 'en_regressor.fit(X_train, y_train)\n', 'en_regressor.best_params_']"
"CHECKPOINT print('Best amount of true classes should be', np.argmax(y)*100,'with expected AUC arround',y[np.argmax(y)])",0,"[""print('Best amount of true classes should be', np.argmax(y)*100,'with expected AUC arround',y[np.argmax(y)])""]"
"ASSIGN = plt.figure(figsize = (20, 5)) ASSIGN = fig.add_subplot(121) ASSIGN = fig.add_subplot(122) ASSIGN.set_title('Values without normalization') ASSIGN.set_title('Values with normalization') sns.scatterplot(x = X_train['texture_mean'], y = X_train['area_mean'], hue = Y_train, ax = ASSIGN) sns.scatterplot(x = X_train_scaled['texture_mean'], y = X_train_scaled['area_mean'], hue = Y_train, ax = ASSIGN)",1,"['fig = plt.figure(figsize = (20, 5))\n', 'ax1 = fig.add_subplot(121)\n', 'ax2 = fig.add_subplot(122)\n', ""ax1.set_title('Values without normalization')\n"", ""ax2.set_title('Values with normalization')\n"", ""sns.scatterplot(x = X_train['texture_mean'], y = X_train['area_mean'], hue = Y_train, ax = ax1)\n"", ""sns.scatterplot(x = X_train_scaled['texture_mean'], y = X_train_scaled['area_mean'], hue = Y_train, ax = ax2)""]"
"def concatzeroes(max_height,max_width,b): ASSIGN = 2 ASSIGN = int((max_height-b.shape[dim]) path) ASSIGN = (max_height-b.shape[dim]) - pad_size_1 ASSIGN = F.pad(input=ASSIGN, pad=(0, 0, pad_size_1, pad_size_2), mode='constant', value=0) ASSIGN = 3 ASSIGN = int((max_width-b.shape[dim]) path) ASSIGN = (max_width-b.shape[dim]) - pad_size_1 ASSIGN = F.pad(input=ASSIGN, pad=(pad_size_1, pad_size_2,0,0), mode='constant', value=0) return b def downsizetensors(max_height,max_width,ASSIGN): ASSIGN = (max_height,max_width) return F.interpolate(ASSIGN, ASSIGN=ASSIGN, mode='bilinear', align_corners=False) def concatenate2(a,ASSIGN,increase_size): if(increase_size): ASSIGN = max(a.shape[2],b.shape[2]) ASSIGN = max(a.shape[3],b.shape[3]) ASSIGN = concatzeroes(max_height,max_width,ASSIGN) ASSIGN = concatzeroes(max_height,max_width,ASSIGN) return torch.cat((ASSIGN, ASSIGN), 1) else: ASSIGN = min(a.shape[2],b.shape[2]) ASSIGN = min(a.shape[3],b.shape[3]) ASSIGN = downsizetensors(max_height,max_width,ASSIGN) ASSIGN = downsizetensors(max_height,max_width,ASSIGN) return torch.cat((ASSIGN, ASSIGN), 1) def concatenate3(ASSIGN,ASSIGN,c,increase_size): if(increase_size): ASSIGN = max(a.shape[2],b.shape[2],c.shape[2]) ASSIGN = max(a.shape[3],b.shape[3],c.shape[3]) ASSIGN = concatzeroes(max_height,max_width,ASSIGN) ASSIGN = concatzeroes(max_height,max_width,ASSIGN) ASSIGN = concatzeroes(max_height,max_width,ASSIGN) return torch.cat((torch.cat((ASSIGN, ASSIGN), 1), ASSIGN), 1) else: ASSIGN = min(a.shape[2],b.shape[2],c.shape[2]) ASSIGN = min(a.shape[3],b.shape[3],c.shape[3]) ASSIGN = downsizetensors(max_height,max_width,ASSIGN) ASSIGN = downsizetensors(max_height,max_width,ASSIGN) ASSIGN = downsizetensors(max_height,max_width,ASSIGN) return torch.cat((torch.cat((ASSIGN, ASSIGN), 1), ASSIGN), 1)",0,"['def concatzeroes(max_height,max_width,b):\n', '    dim = 2\n', '    pad_size_1 = int((max_height-b.shape[dim]) / 2)\n', '    pad_size_2 = (max_height-b.shape[dim]) - pad_size_1\n', ""    b = F.pad(input=b, pad=(0, 0, pad_size_1, pad_size_2), mode='constant', value=0)\n"", '\n', '    dim = 3\n', '    pad_size_1 = int((max_width-b.shape[dim]) / 2)\n', '    pad_size_2 = (max_width-b.shape[dim]) - pad_size_1\n', ""    b = F.pad(input=b, pad=(pad_size_1, pad_size_2,0,0), mode='constant', value=0)\n"", '    \n', '    return b\n', '\n', 'def downsizetensors(max_height,max_width,b):\n', '    size = (max_height,max_width)\n', ""    return F.interpolate(b, size=size, mode='bilinear', align_corners=False)\n"", '\n', 'def concatenate2(a,b,increase_size):\n', '    if(increase_size):\n', '        max_height = max(a.shape[2],b.shape[2])\n', '        max_width = max(a.shape[3],b.shape[3])\n', '        a = concatzeroes(max_height,max_width,a)\n', '        b = concatzeroes(max_height,max_width,b)\n', '        return torch.cat((a, b), 1)\n', '    else:\n', '        max_height = min(a.shape[2],b.shape[2])\n', '        max_width = min(a.shape[3],b.shape[3])\n', '        a = downsizetensors(max_height,max_width,a)\n', '        b = downsizetensors(max_height,max_width,b)\n', '        return torch.cat((a, b), 1)\n', '\n', '\n', 'def concatenate3(a,b,c,increase_size):\n', '    if(increase_size):\n', '        max_height = max(a.shape[2],b.shape[2],c.shape[2])\n', '        max_width = max(a.shape[3],b.shape[3],c.shape[3])\n', '        a = concatzeroes(max_height,max_width,a)\n', '        b = concatzeroes(max_height,max_width,b)\n', '        c = concatzeroes(max_height,max_width,c)\n', '        return torch.cat((torch.cat((a, b), 1), c), 1)\n', '    else:\n', '        \n', '        max_height = min(a.shape[2],b.shape[2],c.shape[2])\n', '        max_width = min(a.shape[3],b.shape[3],c.shape[3])\n', '        a = downsizetensors(max_height,max_width,a)\n', '        b = downsizetensors(max_height,max_width,b)\n', '        c = downsizetensors(max_height,max_width,c)\n', '        return torch.cat((torch.cat((a, b), 1), c), 1)']"
SETUP sns.set(),0,"['import os\n', 'import pandas as pd\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline\n', 'import seaborn as sns\n', 'sns.set()\n', 'from PIL import Image\n', 'from glob import glob\n', 'from skimage.io import imread\n', 'from os import listdir\n', 'from sklearn.preprocessing import LabelEncoder\n', 'import time\n', 'import cv2\n', 'import copy\n', 'from random import shuffle\n', 'from tqdm import tqdm_notebook as tqdm\n', 'from sklearn.model_selection import train_test_split\n', 'from keras.utils.np_utils import to_categorical\n', 'from sklearn.metrics import accuracy_score\n', 'from sklearn.metrics import confusion_matrix\n', 'from sklearn.metrics import classification_report\n', 'from sklearn.metrics import plot_roc_curve\n', 'from sklearn.metrics import precision_recall_fscore_support\n', 'from imblearn.metrics import sensitivity_specificity_support\n', 'from imgaug import augmenters as iaa\n', 'import imgaug as ia\n', 'import tensorflow as tf\n', '\n', '# import numpy as np\n', '# import matplotlib.pyplot as plt\n', 'from itertools import cycle\n', '\n', '# from sklearn import svm, datasets\n', 'from sklearn.metrics import roc_curve, auc\n', '# from sklearn.model_selection import train_test_split\n', '# from sklearn.preprocessing import label_binarize\n', '# from sklearn.multiclass import OneVsRestClassifier\n', 'from scipy import interp\n', 'from sklearn.metrics import roc_auc_score\n', '\n', 'from keras.applications.resnet50 import ResNet50, preprocess_input\n', 'from keras.utils.vis_utils import plot_model\n', 'from keras.optimizers import SGD,Adam\n', 'import numpy as np\n', '\n', '# from keras.applications.vgg16 import VGG16\n', 'from keras.layers import Dense, GlobalAveragePooling2D, Dropout,concatenate\n', 'from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, BatchNormalization\n', 'from keras.layers import Input\n', 'from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard,CSVLogger\n', '# import tools\n', 'import gc\n', 'from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix\n', 'from keras.models import Model\n', 'import keras\n', '# import channel_attention']"
"CHECKPOINT with open(""..path(30-November-2017).csv"", 'rb') as rawdata: ASSIGN = chardet.detect(rawdata.read(100000)) print(ASSIGN)",0,"['# look at the first ten thousand bytes to guess the character encoding\n', 'with open(""../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv"", \'rb\') as rawdata:\n', '    result = chardet.detect(rawdata.read(100000))\n', '\n', '# check what the character encoding might be\n', 'print(result)']"
ASSIGN = ASSIGN.assign(Attrition = Y_res),0,['smote_df = smote_df.assign(Attrition = Y_res)']
SETUP,0,"['from torchvision import transforms\n', 'from torchvision.datasets import ImageFolder\n', 'from torch.utils.data import DataLoader\n', 'from torch.utils.data.dataset import Dataset\n', 'from PIL import Image\n', '\n', 'import torchvision.models as models\n', 'import torch.nn as nn\n', 'import torch.optim as optim']"
"data_features['MSSubClass'].groupby((data_features['BsmtFinType1'],data_features['BsmtFinType2'])).count() data_features.loc[condition5, 'BsmtFinType2'] = 'Unf'",0,"[""data_features['MSSubClass'].groupby((data_features['BsmtFinType1'],data_features['BsmtFinType2'])).count()\n"", ""#I guess even if Type1 is good , the Type 2 is more likely to be Unf. So fill 'BsmtFinType2' by Unf\n"", ""data_features.loc[condition5, 'BsmtFinType2'] = 'Unf'""]"
"SETUP plt.figure(figsize=(20,4)) for index, (image, label) in enumerate(zip(X_train[0:5], y_train[0:5])): plt.subplot(1, 5, index + 1) plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray) plt.title('Training: %i\n' % label, fontsize = 20)",1,"['import numpy as np\n', 'import matplotlib.pyplot as plt\n', 'plt.figure(figsize=(20,4))\n', 'for index, (image, label) in enumerate(zip(X_train[0:5], y_train[0:5])):\n', ' plt.subplot(1, 5, index + 1)\n', ' plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n', "" plt.title('Training: %i\\n' % label, fontsize = 20)""]"
"''' ASSIGN = [250,500,750,1000] ASSIGN = ['gini','entropy'] ASSIGN = [5,10,15,20,25] ASSIGN = [2,3,4,5] ASSIGN = [True,False] ASSIGN = [True,False] ASSIGN = ['balanced','balanced_subsample','dict'] ASSIGN = ['auto','sqrt','log2'] ASSIGN = RandomForestClassifier() ASSIGN = {'n_estimators': [250,500,750,1000],'criterion': ['gini','entropy'],'max_depth':[5,10,15,20,25],'min_samples_split':[2,3,4,5],'bootstrap':[True,False] ,'ASSIGN':[True,False],'ASSIGN':['balanced','balanced_subsample','dict'],'ASSIGN':['auto','sqrt','log2']} ASSIGN = RandomizedSearchCV(RF, parameters, scoring='accuracy' ,cv =50) ASSIGN.fit(x_train, x_test) ASSIGN.best_params_ '''",0,"[""'''\n"", 'n_estimators = [250,500,750,1000]\n', ""criterion = ['gini','entropy']\n"", 'max_depth = [5,10,15,20,25]\n', 'min_samples_split = [2,3,4,5]\n', 'bootstrap = [True,False]\n', 'oob_score = [True,False]\n', ""class_weight = ['balanced','balanced_subsample','dict']\n"", ""max_features = ['auto','sqrt','log2']\n"", '\n', 'RF = RandomForestClassifier()\n', '\n', ""parameters = {'n_estimators': [250,500,750,1000],'criterion': ['gini','entropy'],'max_depth':[5,10,15,20,25],'min_samples_split':[2,3,4,5],'bootstrap':[True,False]\n"", ""              ,'oob_score':[True,False],'class_weight':['balanced','balanced_subsample','dict'],'max_features':['auto','sqrt','log2']}\n"", '\n', ""RFClassifier = RandomizedSearchCV(RF, parameters, scoring='accuracy' ,cv =50)\n"", 'RFClassifier.fit(x_train, x_test)\n', 'RFClassifier.best_params_\n', ""'''""]"
SETUP,0,['from sklearn.linear_model import LogisticRegression']
"CHECKPOINT ASSIGN = ['WindGustDir', 'WindDir9am', 'WindDir3pm','Date','Location','RISK_MM'] ASSIGN = ASSIGN.drop(drop_columns_list, axis=1) print(ASSIGN.shape) ASSIGN.head()",0,"['#drop something column\n', ""drop_columns_list = ['WindGustDir', 'WindDir9am', 'WindDir3pm','Date','Location','RISK_MM']\n"", 'pd_data = pd_data.drop(drop_columns_list, axis=1)\n', 'print(pd_data.shape)\n', 'pd_data.head()']"
"plt.plot(tree_depth,tree_train_acc,'b', label=""training accuracy"") plt.plot(tree_depth,tree_test_acc,'r', label=""test accuracy"") plt.ylabel('accuracy (%)') plt.xlabel('max depth ') plt.legend() plt.show()",1,"['plt.plot(tree_depth,tree_train_acc,\'b\', label=""training accuracy"")\n', 'plt.plot(tree_depth,tree_test_acc,\'r\', label=""test accuracy"")\n', ""plt.ylabel('accuracy (%)')\n"", ""plt.xlabel('max depth ')\n"", 'plt.legend()\n', 'plt.show()']"
SETUP warnings.simplefilter(action='ignore') ASSIGN = 1 random.seed(ASSIGN) np.random.seed(ASSIGN),0,"['import os\n', 'import random\n', 'import warnings\n', ""warnings.simplefilter(action='ignore')\n"", 'import numpy as np\n', 'import pandas as pd\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', '%matplotlib inline\n', 'from sklearn.linear_model import LogisticRegression\n', 'from sklearn.svm import SVC, LinearSVC\n', 'from sklearn.neighbors import KNeighborsClassifier\n', 'from sklearn.linear_model import SGDClassifier\n', 'from sklearn.tree import DecisionTreeClassifier\n', 'from sklearn.model_selection import KFold\n', 'from sklearn.ensemble import RandomForestClassifier\n', 'from sklearn.metrics import make_scorer, accuracy_score\n', 'from sklearn.model_selection import RandomizedSearchCV\n', 'import lightgbm\n', 'import matplotlib.pyplot as plt\n', 'from sklearn.mixture import GaussianMixture\n', 'from lightgbm import LGBMClassifier\n', 'from mlxtend.classifier import StackingCVClassifier\n', 'from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier\n', 'from sklearn.preprocessing import MinMaxScaler\n', 'from sklearn.model_selection import cross_val_score\n', 'from sklearn.tree import DecisionTreeClassifier\n', '\n', 'random_state = 1\n', 'random.seed(random_state)\n', 'np.random.seed(random_state)']"
"ASSIGN = [(x path(first_Digit)) for x in first_Digit] ASSIGN.insert(0, 0) ASSIGN = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] ASSIGN = {'First Digit Test': first_digit_percentile, 'Second Digit Test': second_Digit_percentile, 'Third Digit Test': third_Digit_percentile} ASSIGN = pd.DataFrame(data=final_series, index=index) ASSIGN = pd.merge(digit_df, Benford_percentiles, on=digit_df.index, how='outer')",0,"['#reinitialize these variables before I alter them just to keep the code clean\n', 'first_digit_percentile = [(x / sum(first_Digit)) for x in first_Digit]\n', 'first_digit_percentile.insert(0, 0)\n', ""index = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"", '\n', ""final_series = {'First Digit Test': first_digit_percentile,\n"", ""                'Second Digit Test': second_Digit_percentile,\n"", ""                'Third Digit Test': third_Digit_percentile}\n"", 'digit_df = pd.DataFrame(data=final_series, index=index)\n', ""final_df = pd.merge(digit_df, Benford_percentiles, on=digit_df.index, how='outer')""]"
os.listdir('..path'),0,"[""os.listdir('../input/new-york-city-airbnb-open-data/')""]"
ASSIGN = pca.transform(scaled_data),0,['x_pca = pca.transform(scaled_data)']
"SETUP ASSIGN = Fold(input_df, input_X, seed=1111); torch_seed(ASSIGN.seed) ASSIGN = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)",0,"['%%time\n', 'fold = Fold(input_df, input_X, seed=1111); torch_seed(fold.seed)\n', 'learn3 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)']"
cleaning_test.isnull().sum().sort_values(),0,['cleaning_test.isnull().sum().sort_values()']
"train.Country.replace('Cote d\'Ivoire','Cote d Ivoire',inplace=True) train.Province.replace('Cote d\'Ivoire','Cote d Ivoire',inplace=True) train.replace(np.inf,0,inplace=True) def get_percent(x): ASSIGN = str(ASSIGN) ASSIGN = ASSIGN.strip('%') ASSIGN = int(ASSIGN)path return x train.UrbanPopRate = train.UrbanPopRate.apply(lambda ASSIGN:get_percent(ASSIGN)) def get_dt(ASSIGN): return datetime.strptime(ASSIGN,'%Y-%m-%d') train.Date = train.Date.apply(lambda ASSIGN:get_dt(ASSIGN))",0,"[""# The character ' will make later query function report an error,so it's replaced by a space\n"", ""train.Country.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n"", ""train.Province.replace('Cote d\\'Ivoire','Cote d Ivoire',inplace=True)\n"", '\n', ""# There are few infinite values in the weather data,it will cause the training loss become NAN.Since the amount of np.inf is very few,it's simply replace by 0.\n"", 'train.replace(np.inf,0,inplace=True)\n', '\n', '# Transform percentage data to float\n', 'def get_percent(x):\n', '    x = str(x)\n', ""    x = x.strip('%')\n"", '    x = int(x)/100\n', '    return x\n', '\n', 'train.UrbanPopRate = train.UrbanPopRate.apply(lambda x:get_percent(x))\n', '\n', '# Transform date type\n', 'def get_dt(x):\n', ""    return datetime.strptime(x,'%Y-%m-%d')\n"", '\n', 'train.Date = train.Date.apply(lambda x:get_dt(x))']"
"plt.figure(figsize=(15,15)) sns.heatmap(data_mat.corr(),annot = True,fmt = "".2f"",cbar = True) plt.xticks(rotation=90) plt.yticks(rotation = 0)",1,"['plt.figure(figsize=(15,15))\n', 'sns.heatmap(data_mat.corr(),annot = True,fmt = "".2f"",cbar = True)\n', 'plt.xticks(rotation=90)\n', 'plt.yticks(rotation = 0)']"
"plotScatterMatrix(df3, 9, 10)",1,"['plotScatterMatrix(df3, 9, 10)']"
"ASSIGN=data[data['YEAR']==2017][0:2000] ASSIGN=""Crime2017"" ASSIGN=folium.Map([Lat,Lon],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(boston_map) for lat,lon,label in zip(ASSIGN.Lat,ASSIGN.Long,ASSIGN.OFFENSE_CODE_GROUP): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN)",1,"[""data1=data[data['YEAR']==2017][0:2000]\n"", 'filename=""Crime2017""\n', 'boston_map=folium.Map([Lat,Lon],zoom_start=12)\n', 'incidents2=plugins.MarkerCluster().add_to(boston_map)\n', 'for lat,lon,label in zip(data1.Lat,data1.Long,data1.OFFENSE_CODE_GROUP):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(incidents2)\n', 'boston_map.add_child(incidents2)\n']"
"ASSIGN = TPOTClassifier(generations=5, population_size=20, cv=5, n_jobs=-1,random_state=42, verbosity=2, early_stop=5)",0,"['pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5, n_jobs=-1,random_state=42, verbosity=2, early_stop=5)']"
"ASSIGN = pd.get_dummies(ASSIGN, columns = [""Embarked""], prefix=""Embarked_from_"")",0,"['final = pd.get_dummies(final, columns = [""Embarked""], prefix=""Embarked_from_"")']"
"ASSIGN = {} for f in categorical: train_df[f] = train_df[f].replace(""nan"", ""other"") train_df[f] = train_df[f].replace(np.nan, ""other"") test_df[f] = test_df[f].replace(""nan"", ""other"") test_df[f] = test_df[f].replace(np.nan, ""other"") ASSIGN = preprocessing.LabelEncoder() ASSIGN.fit(list(train_df[f].values) + list(test_df[f].values)) train_df[f] = ASSIGN.transform(list(train_df[f].values)) test_df[f] = ASSIGN.transform(list(test_df[f].values)) ASSIGN[f] = len(list(ASSIGN.classes_)) + 1",0,"['# Label Encoding\n', 'category_counts = {}\n', 'for f in categorical:\n', '    train_df[f] = train_df[f].replace(""nan"", ""other"")\n', '    train_df[f] = train_df[f].replace(np.nan, ""other"")\n', '    test_df[f] = test_df[f].replace(""nan"", ""other"")\n', '    test_df[f] = test_df[f].replace(np.nan, ""other"")\n', '    lbl = preprocessing.LabelEncoder()\n', '    lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n', '    train_df[f] = lbl.transform(list(train_df[f].values))\n', '    test_df[f] = lbl.transform(list(test_df[f].values))\n', '    category_counts[f] = len(list(lbl.classes_)) + 1\n', '# train_df = train_df.reset_index()\n', '# test_df = test_df.reset_index()']"
"plt.figure(figsize=(15,15)) ASSIGN = train.ASSIGN() sns.heatmap(ASSIGN, xticklabels=ASSIGN.columns,yticklabels=ASSIGN.columns) plt.title(""correlation plot"",size=28)",1,"['plt.figure(figsize=(15,15))\n', 'corr = train.corr()\n', 'sns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns)\n', 'plt.title(""correlation plot"",size=28)']"
"SETUP random.seed( 199 ) def randomcolor(): ASSIGN = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F'] ASSIGN = """" for i in range(6): ASSIGN += ASSIGN[random.randint(0,14)] return ""#""+color ASSIGN=[] for i in range(len(US['Provincepath'].unique())): ASSIGN.append(randomcolor())",0,"['import random\n', 'random.seed( 199 )\n', 'def randomcolor():\n', ""    colorArr = ['1','2','3','4','5','6','7','8','9','A','B','C','D','E','F']\n"", '    color = """"\n', '    for i in range(6):\n', '        color += colorArr[random.randint(0,14)]\n', '    return ""#""+color\n', 'color_US=[]\n', ""for i in range(len(US['Province/State'].unique())):\n"", '    color_US.append(randomcolor())']"
"ASSIGN = covid_data[covid_data['Countrypath'] == 'Mainland China'] ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for dat in ASSIGN['ObservationDate'].unique(): ASSIGN = china_data[china_data['ObservationDate'] == dat] ASSIGN = sub['Confirmed'].sum() ASSIGN = sub['Deaths'].sum() ASSIGN = sub['Recovered'].sum() ASSIGN.append(dat) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN = pd.Series(ASSIGN) ASSIGN =pd.Series(ASSIGN) ASSIGN = pd.Series(ASSIGN) ASSIGN = pd.Series(ASSIGN) ASSIGN = [date.min(), date[len(date)path], date.max()] plt.figure(figsize=(8,8)) plt.plot(ASSIGN, ASSIGN, color = 'yellow') plt.plot(ASSIGN, ASSIGN, color = 'red') plt.plot(ASSIGN, ASSIGN, color = 'green') plt.xticks(ASSIGN, ASSIGN) plt.xlabel('Date') plt.ylabel('Cummulative Count cases') plt.title('Trend Curve of Confirmed Cases in China') plt.legend(['Confirmed', 'Death', 'Recovered']) plt.show()",1,"[""china_data = covid_data[covid_data['Country/Region'] == 'Mainland China']\n"", 'date = []\n', 'c = []\n', 'd = []\n', 'r = []\n', ""for dat in china_data['ObservationDate'].unique():\n"", ""    sub = china_data[china_data['ObservationDate'] == dat]\n"", ""    confirm = sub['Confirmed'].sum()\n"", ""    death = sub['Deaths'].sum()\n"", ""    recover = sub['Recovered'].sum()\n"", '    date.append(dat)\n', '    c.append(confirm)\n', '    d.append(death)\n', '    r.append(recover)\n', '    \n', 'date = pd.Series(date)\n', 'c  =pd.Series(c)\n', 'd = pd.Series(d)\n', 'r = pd.Series(r)\n', '\n', 't = [date.min(), date[len(date)//2], date.max()]\n', 'plt.figure(figsize=(8,8))\n', ""plt.plot(date, c, color = 'yellow')\n"", ""plt.plot(date, d, color = 'red')\n"", ""plt.plot(date, r, color = 'green')\n"", 'plt.xticks(t, t)\n', ""plt.xlabel('Date')\n"", ""plt.ylabel('Cummulative Count cases')\n"", ""plt.title('Trend Curve of Confirmed Cases in China')\n"", ""plt.legend(['Confirmed', 'Death', 'Recovered'])\n"", 'plt.show()']"
relevant[col[6:21]] = relevant[col[6:21]].fillna(9),0,['relevant[col[6:21]] = relevant[col[6:21]].fillna(9)']
"(x_train, y_train), (x_test, y_test) = mnist.load_data()",0,"['# Load Dataset\n', '(x_train, y_train), (x_test, y_test) = mnist.load_data()']"
"RForest.fit(x_train,y_train) ASSIGN=RForest.predict(x_test)",0,"['RForest.fit(x_train,y_train)\n', 'y_predict=RForest.predict(x_test)']"
"ASSIGN = plt.subplots(1,3,figsize=(20,5)) sns.countplot(y_train, ax=ax[0], palette=""Reds"") ax[0].set_title(""Train data"") sns.countplot(y_valid, ax=ax[1], palette=""Blues"") ax[1].set_title(""Dev data"") sns.countplot(y_test, ax=ax[2], palette=""Greens""); ax[2].set_title(""Test data"");",1,"['fig, ax = plt.subplots(1,3,figsize=(20,5))\n', 'sns.countplot(y_train, ax=ax[0], palette=""Reds"")\n', 'ax[0].set_title(""Train data"")\n', 'sns.countplot(y_valid, ax=ax[1], palette=""Blues"")\n', 'ax[1].set_title(""Dev data"")\n', 'sns.countplot(y_test, ax=ax[2], palette=""Greens"");\n', 'ax[2].set_title(""Test data"");']"
SETUP ASSIGN = PCA(n_components=2) ASSIGN.fit(scaled_data),0,"['from sklearn.decomposition import PCA\n', 'pca = PCA(n_components=2)\n', 'pca.fit(scaled_data)']"
"ASSIGN = VotingClassifier(estimators = Estimator, voting ='hard') ASSIGN.fit(x_train, x_test) ASSIGN = vot_hard1.predict(y_train) ASSIGN.score(y_train,y_test) ASSIGN = vot_hard1.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('HardVoting(HT).csv',index = False)",0,"[""vot_hard1 = VotingClassifier(estimators = Estimator, voting ='hard') \n"", 'vot_hard1.fit(x_train, x_test) \n', 'y_pred = vot_hard1.predict(y_train)\n', 'vot_hard1.score(y_train,y_test)\n', '\n', 'modelpred4 = vot_hard1.predict(y_train)\n', ""sub4 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""sub4['PassengerId'] = result['PassengerId']\n"", ""sub4['Survived'] = modelpred4\n"", ""sub4.to_csv('HardVoting(HT).csv',index = False)""]"
"ASSIGN = tourney_result.drop('result', axis=1) ASSIGN = tourney_result.result ASSIGN = shuffle(ASSIGN)",0,"[""X_train = tourney_result.drop('result', axis=1)\n"", 'y_train = tourney_result.result\n', 'X_train, y_train = shuffle(X_train, y_train)']"
"SETUP ASSIGN = RandomForestClassifier(bootstrap=True, n_estimators=1000, max_depth=7) ASSIGN.fit(train_x, train_y)",0,"['from sklearn.ensemble import RandomForestClassifier\n', 'rdf = RandomForestClassifier(bootstrap=True, n_estimators=1000, max_depth=7)\n', 'rdf.fit(train_x, train_y)  ']"
"final_submit.to_csv('path',index=False)",0,"[""final_submit.to_csv('/kaggle/working/submission.csv',index=False)""]"
"CHECKPOINT ASSIGN = LogisticRegression(C = 100,penalty = 'l2', solver = 'newton-cg',class_weight = 'dict', max_iter = 900) Estimator.append(('ASSIGN',LogisticRegression(C = 1,penalty = 'l2', solver = 'newton-cg',class_weight = 'dict', max_iter = 900))) ASSIGN = cross_val_score(lr,x_train,x_test,ASSIGN=10) Accuracy1 = ASSIGN.mean() Accuracy.append(Accuracy1) print(ASSIGN) print(ASSIGN.mean())",0,"[""lr = LogisticRegression(C = 100,penalty = 'l2', solver = 'newton-cg',class_weight = 'dict', max_iter = 900)\n"", ""Estimator.append(('lr',LogisticRegression(C = 1,penalty = 'l2', solver = 'newton-cg',class_weight = 'dict', max_iter = 900)))\n"", 'cv = cross_val_score(lr,x_train,x_test,cv=10)\n', 'Accuracy1 = cv.mean()\n', 'Accuracy.append(Accuracy1)\n', 'print(cv)\n', 'print(cv.mean())']"
cbdr['Total Atrocities'] = cbdr['Murder'] +cbdr['Assault on women']+cbdr['Kidnapping and Abduction']+cbdr['Dacoity']+cbdr['Robbery']+cbdr['Arson']+cbdr['Hurt']+cbdr['Prevention of atrocities (POA) Act']+cbdr['Protection of Civil Rights (PCR) Act']+cbdr['Other Crimes Against SCs'],0,"[""cbdr['Total Atrocities'] = cbdr['Murder'] +cbdr['Assault on women']+cbdr['Kidnapping and Abduction']+cbdr['Dacoity']+cbdr['Robbery']+cbdr['Arson']+cbdr['Hurt']+cbdr['Prevention of atrocities (POA) Act']+cbdr['Protection of Civil Rights (PCR) Act']+cbdr['Other Crimes Against SCs']\n""]"
SETUP ASSIGN = InceptionV3(weights='imagenet'),0,"['# Import Model\n', '#from tensorflow.keras.applications import VGG16\n', '#from tensorflow.keras.applications import ResNet101V2\n', 'from tensorflow.keras.applications import InceptionV3\n', '\n', '#from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n', '#from tensorflow.keras.applications.resnet import preprocess_input, decode_predictions\n', 'from tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\n', '\n', '# Load Model\n', ""#model = VGG16(weights='imagenet')\n"", ""#model = ResNet101V2(weights='imagenet')\n"", ""model = InceptionV3(weights='imagenet')""]"
"plt.figure(figsize=(8,6)) plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target']) plt.xlabel('First principal component') plt.ylabel('Second Principal Component')",1,"['plt.figure(figsize=(8,6))\n', ""plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'])\n"", ""plt.xlabel('First principal component')\n"", ""plt.ylabel('Second Principal Component')""]"
SETUP,0,['from sklearn.preprocessing import StandardScaler']
"SETUP CHECKPOINT ASSIGN=np.array([33,33,13,44,55,66,77,55,12,23,21,34,59]) ASSIGN=np.array([21,34,55,77]) ASSIGN=b.argsort() ASSIGN=a[b[x[np.searchsorted(b,a,sorter=x)]]!=a] print(ASSIGN)",0,"['import numpy as np\n', 'a=np.array([33,33,13,44,55,66,77,55,12,23,21,34,59])\n', 'b=np.array([21,34,55,77])\n', 'x=b.argsort()\n', 'out=a[b[x[np.searchsorted(b,a,sorter=x)]]!=a]\n', 'print(out)']"
ASSIGN = df2.groupby('Date').sum()['ConfirmedCases'].reset_index() ASSIGN = df2.groupby('Date').sum()['Fatalities'].reset_index(),0,"[""confirmed = df2.groupby('Date').sum()['ConfirmedCases'].reset_index()\n"", ""death = df2.groupby('Date').sum()['Fatalities'].reset_index()""]"
"ASSIGN = dict(zip( ['Anhui', 'Beijing', 'Chongqing', 'Fujian', 'Gansu', 'Guangdong', 'Guangxi', 'Guizhou', 'Hainan', 'Hebei', 'Heilongjiang', 'Henan', 'Hubei', 'Hunan', 'Inner Mongolia', 'Jiangsu', 'Jiangxi', 'Jilin', 'Liaoning', 'Ningxia', 'Qinghai', 'Shaanxi', 'Shandong', 'Shanghai', 'Shanxi', 'Sichuan', 'Tianjin', 'Tibet', 'Xinjiang', 'Yunnan', 'Zhejiang'], [' ' ' ' )) def race_barchart(date): ASSIGN = china[china['date'].eq(date)].sort_values(by='Confirmed', ascending=True).tail(10) ax.clear() ax.barh(ASSIGN['Provincepath'], ASSIGN['Confirmed'], color=[ASSIGN[x] for x in ASSIGN['Provincepath']],height=0.8) ASSIGN = dff['Confirmed'].max() path for i, (value, name) in enumerate(zip(ASSIGN['Confirmed'], ASSIGN['Provincepath'])): ax.text(0, i,name+' ',size=16, weight=600, ha='right', va='center') ax.text(value+ASSIGN, i,f'{value:,.0f}', size=16, ha='left', va='center') ax.text(0.9, 0.2, date, transform=ax.transAxes, color=' ax.text(0.59, 0.14, 'Total Confirmed:'+str(int(ASSIGN['Confirmed'].sum())), transform=ax.transAxes, size=24, color=' ax.tick_params(axis='x', ASSIGN=' ax.xaxis.set_ticks_position('top') ax.set_yticks([]) ax.margins(0, 0.01) ax.grid(which='major', axis='x', linestyle='-') ax.text(0, 1.15, 'Confirmed for each date in China ', ASSIGN=ax.transAxes, size=24, weight=600, ha='left', va='top') plt.box(False) ASSIGN = list(set(china.date.values)) ASSIGN.sort() ASSIGN = plt.subplots(figsize=(16, 9)) HTML(animation.FuncAnimation(fig, race_barchart, frames=ASSIGN).to_jshtml())",1,"['colors = dict(zip(\n', ""    ['Anhui', 'Beijing', 'Chongqing', 'Fujian', 'Gansu', 'Guangdong',\n"", ""       'Guangxi', 'Guizhou', 'Hainan', 'Hebei', 'Heilongjiang', 'Henan',\n"", ""       'Hubei', 'Hunan', 'Inner Mongolia', 'Jiangsu', 'Jiangxi', 'Jilin',\n"", ""       'Liaoning', 'Ningxia', 'Qinghai', 'Shaanxi', 'Shandong',\n"", ""       'Shanghai', 'Shanxi', 'Sichuan', 'Tianjin', 'Tibet', 'Xinjiang',\n"", ""       'Yunnan', 'Zhejiang'],\n"", ""    ['#800000', '#8B0000', '#A52A2A', '#B22222', '#DC143C', '#FF0000', '#FF6347','#FF7F50','#CD5C5C','#F08080',\n"", ""    '#E9967A','#FA8072','#FFA07A','#FF4500','#FF8C00','#FFA500','#FFD700','#B8860B','#DAA520','#EEE8AA',\n"", ""    '#BDB76B','#F0E68C','#808000','#FFFF00','#9ACD32','#556B2F','#6B8E23','#7CFC00','#7FFF00','#ADFF2F',\n"", ""    '#006400']\n"", '))\n', '\n', '\n', 'def race_barchart(date):\n', ""    dff = china[china['date'].eq(date)].sort_values(by='Confirmed', ascending=True).tail(10)\n"", '    ax.clear()\n', ""    ax.barh(dff['Province/State'], dff['Confirmed'], color=[colors[x] for x in dff['Province/State']],height=0.8)\n"", ""    dx = dff['Confirmed'].max() / 200\n"", '    \n', ""    for i, (value, name) in enumerate(zip(dff['Confirmed'], dff['Province/State'])):\n"", ""        ax.text(0, i,name+' ',size=16, weight=600, ha='right', va='center') \n"", ""        ax.text(value+dx, i,f'{value:,.0f}',  size=16, ha='left',  va='center') \n"", '            \n', ""    ax.text(0.9, 0.2, date, transform=ax.transAxes, color='#777777', size=72, ha='right', weight=1000) \n"", ""    ax.text(0.59, 0.14, 'Total Confirmed:'+str(int(dff['Confirmed'].sum())), transform=ax.transAxes, size=24, color='#000000',ha='left')\n"", ""    ax.tick_params(axis='x', colors='#777777', labelsize=12) \n"", ""    ax.xaxis.set_ticks_position('top') \n"", '    ax.set_yticks([])\n', '    ax.margins(0, 0.01)\n', ""    ax.grid(which='major', axis='x', linestyle='-') \n"", ""    ax.text(0, 1.15, 'Confirmed for each date in China ',\n"", ""                transform=ax.transAxes, size=24, weight=600, ha='left', va='top') \n"", '\n', '    plt.box(False)\n', '    \n', '\n', 'day = list(set(china.date.values))\n', 'day.sort()\n', '\n', 'fig, ax = plt.subplots(figsize=(16, 9))\n', '\n', 'HTML(animation.FuncAnimation(fig, race_barchart, frames=day).to_jshtml())']"
"ASSIGN = pd.read_csv(""path"")",0,"['data = pd.read_csv(""/kaggle/input/wildlife-strikes/database.csv"")']"
covid_data['Countrypath'].value_counts(),0,"[""covid_data['Country/Region'].value_counts()""]"
SETUP,0,"['import numpy as np\n', 'import pandas as pd \n', 'import matplotlib.pyplot as plt\n', 'import os\n', 'import seaborn as sns\n', 'import math\n', 'import datetime']"
"def rank_import(country): ASSIGN=['2010','2011','2012','2013','2014','2015','2016','2017','2018'] B={} for i in range(len(ASSIGN)): A=data_import[data_import.ASSIGN==int(ASSIGN[i])] ASSIGN=A.groupby(['country'])['ASSIGN'].agg('sum') ASSIGN=A.groupby(['country'])['value'].agg('sum').ASSIGN(method='min',ascending=False) ASSIGN=pd.DataFrame({'rank':rank,'value':value}) B['ASSIGN '+ASSIGN[i]]=str(ASSIGN[ASSIGN.index==country].iloc[0,0])+""path""+str(max(ASSIGN)) B['ASSIGN '+ASSIGN[i]]=str(ASSIGN[ASSIGN.index==country].iloc[0,1]) return B",0,"['def rank_import(country):\n', ""    year=['2010','2011','2012','2013','2014','2015','2016','2017','2018']\n"", '    B={}\n', '    for i in range(len(year)):\n', '        A=data_import[data_import.year==int(year[i])]\n', ""        value=A.groupby(['country'])['value'].agg('sum')\n"", ""        rank=A.groupby(['country'])['value'].agg('sum').rank(method='min',ascending=False)\n"", ""        new=pd.DataFrame({'rank':rank,'value':value})\n"", '        B[\'rank \'+year[i]]=str(new[new.index==country].iloc[0,0])+""/""+str(max(rank))\n', ""        B['value '+year[i]]=str(new[new.index==country].iloc[0,1])\n"", '\n', '    return B']"
"plot_figures( counts, ASSIGN='The percentage of the number of cars of each model type', ASSIGN=170, ASSIGN='Distribution of cars of each model type', ASSIGN='Frequency', ASSIGN=[label for label in unique], ASSIGN=np.zeros(len(unique)) )",1,"['plot_figures(\n', '    counts,\n', ""    pie_title='The percentage of the number of cars of each model type',\n"", '    start_angle=170,\n', ""    bar_title='Distribution of cars of each model type',\n"", ""    bar_ylabel='Frequency',\n"", '    labels=[label for label in unique],\n', '    explode=np.zeros(len(unique))\n', ')']"
CHECKPOINT print(type(player_df.player_name[0])) print(type(player_df.birthday[0])),0,"['print(type(player_df.player_name[0]))\n', 'print(type(player_df.birthday[0]))']"
"ASSIGN=plt.subplots(1,2,figsize=(15,8)) ASSIGN = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",'cadetblue','hotpink','orange','darksalmon','brown') train.Neighborhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=ASSIGN,ax=ax[0]) ax[0].set_title(""Top 10 Neighborhood by counts"",size=20) ax[0].set_xlabel('counts',size=18) ASSIGN=train.Neighborhood.value_counts() ASSIGN=list(train.Neighborhood.value_counts().index)[:10] ASSIGN=list(count[:10]) ASSIGN.append(ASSIGN.agg(sum)-ASSIGN[:10].agg('sum')) ASSIGN.append('Other') ASSIGN=pd.DataFrame({""group"":groups,""counts"":counts}) ASSIGN=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum') ASSIGN = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1]) plt.legend(loc=0, bbox_to_anchor=(1.15,0.8)) plt.subplots_adjust(wspace =0.5, hspace =0) plt.ylabel('')",1,"['fig,ax=plt.subplots(1,2,figsize=(15,8))\n', 'clr = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",\'cadetblue\',\'hotpink\',\'orange\',\'darksalmon\',\'brown\')\n', ""train.Neighborhood.value_counts().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color=clr,ax=ax[0])\n"", 'ax[0].set_title(""Top 10 Neighborhood by counts"",size=20)\n', ""ax[0].set_xlabel('counts',size=18)\n"", '\n', '\n', 'count=train.Neighborhood.value_counts()\n', 'groups=list(train.Neighborhood.value_counts().index)[:10]\n', 'counts=list(count[:10])\n', ""counts.append(count.agg(sum)-count[:10].agg('sum'))\n"", ""groups.append('Other')\n"", 'type_dict=pd.DataFrame({""group"":groups,""counts"":counts})\n', ""clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n"", ""qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n"", 'plt.legend(loc=0, bbox_to_anchor=(1.15,0.8)) \n', 'plt.subplots_adjust(wspace =0.5, hspace =0)\n', ""plt.ylabel('')""]"
"ASSIGN = {'train': len(trainset), 'valid': len(valset), 'test': len(test_dataset)}",0,"[""dataset_sizes = {'train': len(trainset), 'valid': len(valset), 'test': len(test_dataset)}""]"
CHECKPOINT print() print(iris_data.count().sum()) print() print(iris_data.isnull().sum()),0,"['#Q2.get the number of observations, missing values and nan values.\n', 'print(""No.of Observations are:"")\n', 'print(iris_data.count().sum())\n', 'print(""No. of Nan is:"")\n', 'print(iris_data.isnull().sum())']"
"ASSIGN = pd.DataFrame(X_res, columns = x_col)",0,"['smote_df = pd.DataFrame(X_res, columns = x_col)']"
"plt.figure(figsize=(10,10)) sns.barplot(x=train_identity.isnull().sum().sort_values(ascending=False),y=train_identity.isnull().sum().sort_values(ascending=False).index) plt.title(""counts of missing value for train_identity"",size=20)",1,"['plt.figure(figsize=(10,10))\n', 'sns.barplot(x=train_identity.isnull().sum().sort_values(ascending=False),y=train_identity.isnull().sum().sort_values(ascending=False).index)\n', 'plt.title(""counts of missing value for train_identity"",size=20)']"
"ASSIGN = dataset.iloc[:,0:13] ASSIGN = dataset.iloc[:, 13]",0,"['X = dataset.iloc[:,0:13]\n', 'y = dataset.iloc[:, 13]']"
"data.show_batch(rows=3, figsize=(15,11))",1,"['data.show_batch(rows=3, figsize=(15,11))']"
ASSIGN = total,0,"[""artime['Total'] = total""]"
"plt.figure(figsize=(15,6)) sns.distplot(points_df['y'], bins=500); plt.xlabel('y') plt.show()",1,"['plt.figure(figsize=(15,6))\n', ""sns.distplot(points_df['y'], bins=500);\n"", ""plt.xlabel('y')\n"", 'plt.show()']"
"ASSIGN =(6, 10, 8)",0,"['#12 16 24\n', 'block_config =(6, 10, 8)']"
learn.lr_find() learn.recorder.plot(),1,"['learn.lr_find()\n', 'learn.recorder.plot()']"
"ASSIGN = LogisticRegression() ASSIGN.fit(X_train, Y_train) ASSIGN = logreg.predict(X_test) ASSIGN.score(X_train, Y_train)",0,"['# Logistic Regression\n', '\n', 'logreg = LogisticRegression()\n', '\n', 'logreg.fit(X_train, Y_train)\n', '\n', 'Y_pred = logreg.predict(X_test)\n', '\n', 'logreg.score(X_train, Y_train)']"
"CHECKPOINT print(classification_report(Y_test, Y_predicted))",0,"['print(classification_report(Y_test, Y_predicted))']"
ASSIGN=pd.read_csv('..path'),0,"[""data2=pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')""]"
"def weights_init(m): ASSIGN = m.__class__.__name__ if ASSIGN.find('Conv') != -1: nn.init.normal_(m.weight.data, 0.0, 0.02) elif ASSIGN.find('BatchNorm') != -1: nn.init.normal_(m.weight.data, 1.0, 0.02) nn.init.constant_(m.bias.data, 0)",0,"['# custom weights initialization called on netG and netD\n', 'def weights_init(m):\n', '    classname = m.__class__.__name__\n', ""    if classname.find('Conv') != -1:\n"", '        nn.init.normal_(m.weight.data, 0.0, 0.02)\n', ""    elif classname.find('BatchNorm') != -1:\n"", '        nn.init.normal_(m.weight.data, 1.0, 0.02)\n', '        nn.init.constant_(m.bias.data, 0)\n', '    ']"
"data.drop(['Names','emails','Country'], axis = 1, inplace = True) data.head()",0,"['#Drop name, email, country\n', ""data.drop(['Names','emails','Country'], axis = 1, inplace = True)\n"", 'data.head()']"
data.count(),0,['data.count()']
pca_last.fit(X_train),0,"['#Doing the PCA on the train data\n', 'pca_last.fit(X_train)']"
CHECKPOINT X_train.shape,0,"['X_train.shape\n', '# We have 30 variables after creating our dummy variables for our categories']"
"ASSIGN = train_test_split(trainDF['review'], trainDF['sentiment'], test_size=0.1, shuffle=True)",0,"[""train_x, val_x, train_y, val_y = train_test_split(trainDF['review'], trainDF['sentiment'], test_size=0.1, shuffle=True)""]"
"ASSIGN = [10, 100,1000] ASSIGN = 0 for i in ASSIGN: ASSIGN+=1 plt.subplot(310+(ASSIGN)) plt.imshow(x_train[i][:,:,0]) plt.title(""this is {}"".format(y_label[i]))",1,"['n = [10, 100,1000]\n', 'j = 0\n', 'for i in n:\n', '    j+=1\n', '    plt.subplot(310+(j))\n', '    plt.imshow(x_train[i][:,:,0])\n', '    plt.title(""this is {}"".format(y_label[i]))']"
ASSIGN = 'cpu' if torch.cuda.is_available() : ASSIGN = 'cuda',0,"['\n', ""device = 'cpu'\n"", 'if torch.cuda.is_available() :\n', ""    device = 'cuda'""]"
"CHECKPOINT ASSIGN = GradientBoostingClassifier(loss = 'exponential',n_estimators =200 ,min_samples_split = 4,max_features = 'auto',max_depth =9,learning_rate = .01,subsample = .1) Estimator.append(('ASSIGN',GradientBoostingClassifier(loss = 'exponential',n_estimators =200 ,min_samples_split = 4,max_features = 'auto',max_depth =9,learning_rate = .01,subsample = .1))) ASSIGN = cross_val_score(gbc,x_train,x_test,ASSIGN=10) Accuracy7 = ASSIGN.mean() Accuracy.append(Accuracy7) print(ASSIGN) print(ASSIGN.mean())",0,"[""gbc = GradientBoostingClassifier(loss = 'exponential',n_estimators =200 ,min_samples_split = 4,max_features = 'auto',max_depth =9,learning_rate = .01,subsample = .1)\n"", ""Estimator.append(('gbc',GradientBoostingClassifier(loss = 'exponential',n_estimators =200 ,min_samples_split = 4,max_features = 'auto',max_depth =9,learning_rate = .01,subsample = .1)))\n"", 'cv = cross_val_score(gbc,x_train,x_test,cv=10)\n', 'Accuracy7 = cv.mean()\n', 'Accuracy.append(Accuracy7)\n', 'print(cv)\n', '\n', '\n', 'print(cv.mean())']"
df1.head(5),0,['df1.head(5)']
"SETUP X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.25, random_state = 42)",0,"['#Train test split\n', 'from sklearn.model_selection import train_test_split\n', 'X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.25, random_state = 42)']"
"CHECKPOINT ASSIGN=min(city_vs_count) for x,y in city_vs_count.items(): if(y==ASSIGN): print(x)",0,"['#lets find for city count is min\n', '\n', 'min_count=min(city_vs_count)\n', '\n', 'for x,y in city_vs_count.items():\n', '    if(y==min_count):\n', '        print(x)']"
data.head(),0,['data.head()']
"ASSIGN = df.sample(frac=1, random_state=0) ASSIGN.sort_values(""EmployeeNumber"", inplace=True) ASSIGN = KFold(n_splits=4, shuffle=False, random_state=0) ASSIGN = ""Attrition"" ASSIGN = [col for col in df.columns if col != target] ASSIGN = LOFOImportance(sample_df, features, target, cv=cv, scoring=""roc_auc"") ASSIGN= lofo.get_importance()",0,"['sample_df = df.sample(frac=1, random_state=0)\n', 'sample_df.sort_values(""EmployeeNumber"", inplace=True)\n', '\n', 'cv = KFold(n_splits=4, shuffle=False, random_state=0)\n', 'target = ""Attrition""\n', 'features = [col for col in df.columns if col != target]\n', '\n', 'lofo = LOFOImportance(sample_df, features, target, cv=cv, scoring=""roc_auc"")\n', 'importance_df= lofo.get_importance()']"
"ASSIGN = stats.boxcox(original_data) ASSIGN=plt.subplots(1,2) sns.distplot(original_data, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN[0], ax=ax[1]) ax[1].set_title(""Normalized data"")",1,"['# normalize the exponential data with boxcox\n', 'normalized_data = stats.boxcox(original_data)\n', '\n', '# plot both together to compare\n', 'fig, ax=plt.subplots(1,2)\n', 'sns.distplot(original_data, ax=ax[0])\n', 'ax[0].set_title(""Original Data"")\n', 'sns.distplot(normalized_data[0], ax=ax[1])\n', 'ax[1].set_title(""Normalized data"")']"
ASSIGN = ASSIGN.toarray() ASSIGN = ASSIGN.toarray() ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN),0,"['# Converting the Sparse matrix into a numpy array\n', 'xtrain_tfidf = xtrain_tfidf.toarray()\n', 'xval_tfidf = xval_tfidf.toarray()\n', '# Converting pandas Series into numpy array\n', 'train_y = np.array(train_y)\n', 'val_y = np.array(val_y)']"
"ASSIGN = pd.read_csv('path') ASSIGN = data['Class'] ASSIGN = data.drop(['Class','Time'], axis=1)",0,"[""data = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\n"", ""Y = data['Class']\n"", ""X = data.drop(['Class','Time'], axis=1)""]"
"CHECKPOINT ASSIGN = svm.SVC(kernel = 'ASSIGN', gamma = 'scale') ASSIGN = cross_val_score(poly,x_train,x_test,ASSIGN=10) ASSIGN = cv.mean() accuracy.append(ASSIGN) print(ASSIGN) print(ASSIGN.mean())",0,"[""poly = svm.SVC(kernel = 'poly', gamma = 'scale')\n"", ""#estimator.append(('PSVC',svm.SVC(kernel = 'poly', gamma = 'scale')))\n"", 'cv = cross_val_score(poly,x_train,x_test,cv=10)\n', 'accuracy3 = cv.mean()\n', 'accuracy.append(accuracy3)\n', 'print(cv)\n', 'print(cv.mean())']"
"SETUP CHECKPOINT warnings.filterwarnings(""ignore"") os.environ['OMP_NUM_THREADS'] = '4' print(os.listdir())",0,"['import pandas as pd\n', 'import numpy as np\n', 'from tpot import TPOTClassifier\n', 'from sklearn.model_selection import StratifiedKFold,train_test_split\n', 'import warnings\n', 'warnings.filterwarnings(""ignore"")\n', 'import os\n', ""os.environ['OMP_NUM_THREADS'] = '4'\n"", 'print(os.listdir(""../input""))\n', '# Any results you write to the current directory are saved as output.']"
"for c1, c2 in train.dtypes.reset_index().values: ASSIGN=='O': train[c1] = train[c1].map(lambda x: labels[str(x).lower()]) test[c1] = test[c1].map(lambda x: labels[str(x).lower()]) train.fillna(-999, inplace=True) test.fillna(-999, inplace=True)",0,"['for c1, c2 in train.dtypes.reset_index().values:\n', ""    if c2=='O':\n"", '        train[c1] = train[c1].map(lambda x: labels[str(x).lower()])\n', '        test[c1] = test[c1].map(lambda x: labels[str(x).lower()])\n', 'train.fillna(-999, inplace=True)\n', 'test.fillna(-999, inplace=True)']"
"CHECKPOINT ASSIGN = RandomForestRegressor(n_estimators = 100) ASSIGN.fit(x_t, Y_t) ASSIGN = model19.score(x_es,Y_es) print(ASSIGN*100,'%')",0,"['model19 = RandomForestRegressor(n_estimators = 100)\n', 'model19.fit(x_t, Y_t)\n', '\n', 'accuracy19 = model19.score(x_es,Y_es)\n', ""print(accuracy19*100,'%')""]"
"ASSIGN = lambda x: x.replace('path', '') if type(x) == np.str else x zomato.rate = zomato.rate.apply(ASSIGN).str.strip().astype('float')",0,"[""remove_slash = lambda x: x.replace('/5', '') if type(x) == np.str else x\n"", ""zomato.rate = zomato.rate.apply(remove_slash).str.strip().astype('float')""]"
SETUP ipd.Audio('path'),0,"['import IPython.display as ipd\n', ""ipd.Audio('/kaggle/input/urbansound8k/fold1/108041-9-0-5.wav')""]"
"data_features['MSSubClass'].groupby((data_features['BsmtQual'],data_features['BsmtExposure'])).count()",0,"[""data_features['MSSubClass'].groupby((data_features['BsmtQual'],data_features['BsmtExposure'])).count()""]"
"SETUP ASSIGN = Fold(input_df, input_X, seed=1005); torch_seed(ASSIGN.seed) ASSIGN = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)",0,"['%%time\n', 'fold = Fold(input_df, input_X, seed=1005); torch_seed(fold.seed)\n', 'learn1 = Learner(fold, Arch1b_LstmGru(emb_glovewiki)).train(5)']"
"CHECKPOINT ASSIGN = df_hist_trans.groupby(""card_id"") print(type(ASSIGN)) ASSIGN = ASSIGN.agg({'merchant_category_id':['min']}).reset_index() print(type(ASSIGN)) ASSIGN.columns = [""card_id"", ""merchant_category_id""] ASSIGN = pd.merge(ASSIGN, gdf, on=""card_id"", how=""left"") ASSIGN = pd.merge(ASSIGN, gdf, on=""card_id"", how=""left"")",0,"['gdf = df_hist_trans.groupby(""card_id"")\n', 'print(type(gdf))\n', ""gdf = gdf.agg({'merchant_category_id':['min']}).reset_index()\n"", 'print(type(gdf))\n', 'gdf.columns = [""card_id"", ""merchant_category_id""]\n', 'df_train = pd.merge(df_train, gdf, on=""card_id"", how=""left"")\n', 'df_test = pd.merge(df_test, gdf, on=""card_id"", how=""left"")']"
train['Days_After_1stJan'] = train.Date.apply(lambda x :get_days(x)) test['Days_After_1stJan'] = test.Date.apply(lambda x :get_days(x)),0,"[""train['Days_After_1stJan'] = train.Date.apply(lambda x :get_days(x))\n"", ""test['Days_After_1stJan'] = test.Date.apply(lambda x :get_days(x))""]"
CHECKPOINT print(len(train.ImageId)),0,['print(len(train.ImageId))']
train[target_col].unique(),0,['train[target_col].unique()']
"SETUP ASSIGN=pd.read_csv('..path') ASSIGN=os.listdir(""..path"") ASSIGN.sort(key=lambda x:int(x[:-4])) ASSIGN=[] for file in ASSIGN: ASSIGN.append(np.array(Image.open(""..path""+file))) ASSIGN=np.array(ASSIGN)",0,"['import requests\n', 'from bs4 import BeautifulSoup\n', 'import lxml\n', 'import os\n', 'import urllib\n', 'import sys\n', 'import pandas as pd\n', 'import numpy as np\n', 'from PIL import Image\n', 'import cv2\n', 'import csv\n', 'import multiprocessing\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', '\n', '##https://bulkresizephotos.com/zh-tw <- This website can change your image to 32*32 pixels\n', ""new_train=pd.read_csv('../input/landmark-id-from-0-to-499/new_train_id0_499.csv')\n"", 'filename=os.listdir(""../input/graph-id0-499/landgraphnew_0_499"")\n', 'filename.sort(key=lambda x:int(x[:-4]))\n', 'img=[]\n', 'for file in filename:\n', '\timg.append(np.array(Image.open(""../input/graph-id0-499/landgraphnew_0_499/""+file)))\n', 'img=np.array(img)']"
"train.drop('SalePrice', inplace=True, axis=1) ASSIGN = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],test.loc[:,'MSSubClass':'SaleCondition'])).reset_index(drop=True)",0,"[""#we don't need SalePrice so drop it\n"", ""train.drop('SalePrice', inplace=True, axis=1)\n"", ""data_features = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],test.loc[:,'MSSubClass':'SaleCondition'])).reset_index(drop=True)""]"
tumor_data.describe(include='all'),0,"[""tumor_data.describe(include='all')""]"
"CHECKPOINT ASSIGN = model.fit(maxlag=2,method='cmle') print(f'Lag: {ASSIGN.k_ar}') print(f'Coefficients:\n{ASSIGN.params}')",0,"[""AR2fit = model.fit(maxlag=2,method='cmle')\n"", ""print(f'Lag: {AR2fit.k_ar}')\n"", ""print(f'Coefficients:\\n{AR2fit.params}')""]"
"ASSIGN = train_data_copy.ASSIGN.unique() ASSIGN = pd.DataFrame(predict_y,index=test_id,columns=sorted(species)) ASSIGN['predicted ASSIGN'] = ASSIGN.idxmax(axis=1)",0,"['species = train_data_copy.species.unique()\n', 'predict_out = pd.DataFrame(predict_y,index=test_id,columns=sorted(species))\n', ""predict_out['predicted species'] = predict_out.idxmax(axis=1)""]"
data_tree_for_test.MSZoning.value_counts(),0,['data_tree_for_test.MSZoning.value_counts()']
"ASSIGN=pd.read_csv(""..path"") ASSIGN=pd.read_csv(""..path"") ASSIGN.head()",0,"['train=pd.read_csv(""../input/iwildcam-2019-fgvc6/train.csv"")\n', 'test=pd.read_csv(""../input/iwildcam-2019-fgvc6/test.csv"")\n', 'train.head()']"
"CHECKPOINT print(m, c) print(, Ymin) print(, Ymax - Ymin) ASSIGN = (c * (Ymax - Ymin)) + Ymin print(.format(m),.format(ASSIGN)) ASSIGN = np.arange(0, Xmax+1, 0.1, dtype=""float64"") ASSIGN = np.empty_like(x, dtype=""float64"") ASSIGN = m*x + c_final ASSIGN = np.arange(Xmin-1, Xmax+1, 0.1) plt.plot(ASSIGN, m*ASSIGN+ASSIGN, 'g--') plt.scatter(X,Y, 10, color = 'blue') plt.show()",1,"['print(m, c)\n', 'print(""Ymin:"", Ymin)\n', 'print(""Y Range:"", Ymax - Ymin)\n', '\n', 'c_final = (c * (Ymax - Ymin)) + Ymin\n', '\n', 'print(""m {:6.4f}"".format(m),""\\t final c {:6.4f}"".format(c_final))\n', '\n', '# plt.xlim(-1,Xmax+1)\n', '# plt.ylim(2,Ymax+1)\n', '\n', 'x = np.arange(0, Xmax+1, 0.1, dtype=""float64"")\n', 'y = np.empty_like(x, dtype=""float64"") \n', 'y = m*x + c_final\n', '\n', ""# plt.plot(x, y, 'black')\n"", '# Plot the original points and the final line.\n', '\n', 'points = np.arange(Xmin-1, Xmax+1, 0.1)\n', ""plt.plot(points, m*points+c_final, 'g--')\n"", '\n', '\n', ""plt.scatter(X,Y, 10, color = 'blue')\n"", 'plt.show()']"
ASSIGN = pd.DataFrame(forecastD),0,['death_predict = pd.DataFrame(forecastD)']
"ASSIGN = pd.get_dummies(data_mat['ASSIGN'], drop_first = True) ASSIGN = pd.get_dummies(data_mat['ASSIGN'],drop_first = True) ASSIGN = pd.get_dummies(data_mat['ASSIGN'],drop_first = True) data_mat.drop(['ASSIGN','ASSIGN','ASSIGN'], axis =1, inplace = True) ASSIGN = pd.concat([ASSIGN,absences, failures,studytime],axis = 1) ASSIGN.head()",0,"[""absences = pd.get_dummies(data_mat['absences'], drop_first = True)\n"", ""failures = pd.get_dummies(data_mat['failures'],drop_first = True)\n"", ""studytime = pd.get_dummies(data_mat['studytime'],drop_first = True)\n"", '\n', ""data_mat.drop(['absences','failures','studytime'], axis =1, inplace = True)\n"", 'data_mat = pd.concat([data_mat,absences, failures,studytime],axis = 1)\n', 'data_mat.head()']"
SETUP ASSIGN = LabelEncoder() for i in list(train_transaction_new.select_dtypes(include=['object']).columns): test_transaction_new[i] = ASSIGN.fit_transform(test_transaction_new[i].astype('str')) train_transaction_new[i] = ASSIGN.fit_transform(train_transaction_new[i].astype('str')) test_transaction_new.ProductCD[:5],0,"['from sklearn.preprocessing import LabelEncoder\n', 'labelencoder = LabelEncoder()\n', ""for i in list(train_transaction_new.select_dtypes(include=['object']).columns):\n"", ""    test_transaction_new[i] = labelencoder.fit_transform(test_transaction_new[i].astype('str'))\n"", ""    train_transaction_new[i] = labelencoder.fit_transform(train_transaction_new[i].astype('str'))\n"", 'test_transaction_new.ProductCD[:5]\n']"
"def rle2mask(rle,img): ASSIGN=img.shape[0] ASSIGN=img.shape[1] ASSIGN= np.zeros(width*length).astype(np.uint8) ASSIGN=ASSIGN.split() ASSIGN = rle[0::2] ASSIGN = rle[1::2] for i in range(len(ASSIGN)): ASSIGN[int(ASSIGN[i]):(int(ASSIGN[i])+int(ASSIGN[i]))]=1 return np.flipud(np.rot90(ASSIGN.reshape(ASSIGN, ASSIGN), k=1 ) )",0,"['def rle2mask(rle,img):\n', '\twidth=img.shape[0]\n', '\tlength=img.shape[1]\n', '\tmask= np.zeros(width*length).astype(np.uint8)\n', '\trle=rle.split()\n', '\tstarts = rle[0::2]\n', '\tlengths = rle[1::2]\n', '\tfor i in range(len(starts)):\n', '\t\tmask[int(starts[i]):(int(starts[i])+int(lengths[i]))]=1\n', '\treturn np.flipud(np.rot90(mask.reshape(length, width), k=1 ) )\n']"
CHECKPOINT df_np.shape,0,"['#no of rows is represents the total no restaurants ,now of coloumns(12) is columns for the dataframe\n', 'df_np.shape']"
"SETUP OWGR_TP.Value=OWGR_TP.Value.astype(float) ASSIGN = plt.subplots(1, 2,figsize=(18,8)) OWGR_TP.groupby('Player Name')['Value'].mean().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color='brown',ax=axes[0]) axes[0].set_title('Top 10 player by Average of Total Points',size=20) axes[0].set_xlabel('points') axes[0].set_ylabel('') OWGE_TP10.groupby(['Player Name','Date'])['Value'].agg('mean').unstack('Player Name').plot(ax=axes[1]) axes[1].set_title('Official World Golf Ranking - (TOTAL POINTS)',size=20) axes[1].set_ylabel('Total points') axes[1].set_xlim([0,30]) plt.xticks([0,5,10,15,20,25,29], ['2019-01-27','2019-03-03','2019-04-07','2019-05-12','2019-06-16','2019-07-21','2019-08-18'], rotation=0)",1,"[""OWGR_TP=data_2019[data_2019.Variable=='Official World Golf Ranking - (TOTAL POINTS)']\n"", 'OWGR_TP.Value=OWGR_TP.Value.astype(float)\n', '\n', 'fig, axes = plt.subplots(1, 2,figsize=(18,8))\n', '\n', ""OWGR_TP.groupby('Player Name')['Value'].mean().sort_values(ascending=False)[:10].sort_values().plot(kind='barh',color='brown',ax=axes[0])\n"", ""axes[0].set_title('Top 10 player by Average of Total Points',size=20)\n"", ""axes[0].set_xlabel('points')\n"", ""axes[0].set_ylabel('')\n"", '\n', ""OWGE_TP10=OWGR_TP[OWGR_TP['Player Name'].isin(list(OWGR_TP.groupby('Player Name')['Value'].mean().sort_values(ascending=False)[:10].index[:10]))]\n"", ""OWGE_TP10.groupby(['Player Name','Date'])['Value'].agg('mean').unstack('Player Name').plot(ax=axes[1])\n"", ""axes[1].set_title('Official World Golf Ranking - (TOTAL POINTS)',size=20)\n"", ""axes[1].set_ylabel('Total points')\n"", 'axes[1].set_xlim([0,30])\n', ""plt.xticks([0,5,10,15,20,25,29], ['2019-01-27','2019-03-03','2019-04-07','2019-05-12','2019-06-16','2019-07-21','2019-08-18'], rotation=0)""]"
ASSIGN = train.Date.apply(lambda x:get_dt(x)) ASSIGN = test.Date.apply(lambda x:get_dt(x)) ASSIGN = train.Date.apply(lambda x:get_dayofweek(x)) ASSIGN = test.Date.apply(lambda x:get_dayofweek(x)) ASSIGN = train.Date.apply(lambda x:get_month(x)) ASSIGN = test.Date.apply(lambda x:get_month(x)) ASSIGN = train.Date.apply(lambda x:get_day(x)) ASSIGN = test.Date.apply(lambda x:get_day(x)),0,"[""train['Date'] = train.Date.apply(lambda x:get_dt(x))\n"", ""test['Date'] = test.Date.apply(lambda x:get_dt(x))\n"", ""train['Dayofweek'] = train.Date.apply(lambda x:get_dayofweek(x))\n"", ""test['Dayofweek'] = test.Date.apply(lambda x:get_dayofweek(x))\n"", ""train['Month'] = train.Date.apply(lambda x:get_month(x))\n"", ""test['Month'] = test.Date.apply(lambda x:get_month(x))\n"", ""train['Day'] = train.Date.apply(lambda x:get_day(x))\n"", ""test['Day'] = test.Date.apply(lambda x:get_day(x))""]"
SETUP,0,"['#Display the Principal components that are calculated on the predictor variables and target variables. \n', 'import matplotlib.pyplot as plt\n', 'import pandas as pd\n', 'import numpy as np\n', 'import seaborn as sns']"
"''' ASSIGN = [1,5,10] ASSIGN = [.5,1,1.5,2,2.5] ASSIGN = [.6,.8,1] ASSIGN = [.6,.8,1] ASSIGN = [.01,.05,.1,.5,.2] ASSIGN = [3,4,5,6,7,8,9,10] ASSIGN = XGBClassifier() ASSIGN = {'min_child_weight': [1,5,10],'gamma': [.5,1,1.5,2,2.5],'subsample':[.6,.8,1],'colsample_bytree':[.6,.8,1],'subsample':[.6,.8,1], 'ASSIGN':[.01,.05,.1,.5,.2],'ASSIGN':[3,4,5,6,7,8,9,10]} ASSIGN = RandomizedSearchCV(XB, parameters, scoring='accuracy' ,cv =50) ASSIGN.fit(x_train, x_test) ASSIGN.best_params_ '''",0,"[""'''\n"", 'min_child_weight = [1,5,10]\n', 'gamma = [.5,1,1.5,2,2.5]\n', 'subsample = [.6,.8,1]\n', 'colsample_bytree = [.6,.8,1]\n', 'eta = [.01,.05,.1,.5,.2]\n', 'max_depth = [3,4,5,6,7,8,9,10]\n', '\n', 'XB = XGBClassifier()\n', '\n', ""parameters = {'min_child_weight': [1,5,10],'gamma': [.5,1,1.5,2,2.5],'subsample':[.6,.8,1],'colsample_bytree':[.6,.8,1],'subsample':[.6,.8,1],\n"", ""             'eta':[.01,.05,.1,.5,.2],'max_depth':[3,4,5,6,7,8,9,10]}\n"", '\n', ""XBClassifier = RandomizedSearchCV(XB, parameters, scoring='accuracy' ,cv =50)\n"", 'XBClassifier.fit(x_train, x_test)\n', 'XBClassifier.best_params_\n', ""'''""]"
"np.random.seed(42) ASSIGN = ImageDataBunch.from_folder(path, train=""."", valid_pct=0.2, ASSIGN=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)",0,"['# 置固定的随机数种子，保证每次创建相同的验证集，以便调整超参数\n', 'np.random.seed(42)\n', '# 默认训练集会在train目录下查找。用.设置为当前目录，并且划分验证集\n', 'data = ImageDataBunch.from_folder(path, train=""."", valid_pct=0.2,\n', '        ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)']"
SETUP,0,"['#importing the libraries for map ploting\n', 'from matplotlib import cm\n', 'from matplotlib.dates import date2num\n', 'from mpl_toolkits.basemap import Basemap\n', '\n', '# for date and time processing\n', 'import datetime']"
CHECKPOINT data.columns,0,['data.columns']
CHECKPOINT train.dtypes,0,['train.dtypes']
train_data.info(),0,['train_data.info()']
"ASSIGN = cont ASSIGN = 'Attrition' ASSIGN = df.drop('Attrition', axis=1) ASSIGN = df.Attrition X_res, Y_res = SMOTE().fit_resample(ASSIGN, ASSIGN)",0,"['#save the column name\n', 'x_col = cont\n', ""y_col = 'Attrition'\n"", '\n', ""X = df.drop('Attrition', axis=1)\n"", 'Y = df.Attrition\n', 'X_res, Y_res = SMOTE().fit_resample(X, Y)']"
CHECKPOINT print(os.listdir()),0,"['print(os.listdir(""../input/city-lines/""))']"
CHECKPOINT print(os.listdir('..path')),0,"[""print(os.listdir('../input/'))""]"
SETUP CHECKPOINT print(os.listdir('..path')),0,"['import os\n', ""print(os.listdir('../input/sasuke'))""]"
"formated_gdf['size'] = formated_gdf['Recovered'].pow(0.3) ASSIGN = px.scatter_geo(formated_gdf, locations=""Countrypath"", locationmode='country names', ASSIGN=""Recovered"", size='size', hover_name=""Countrypath"", ASSIGN= [0, max(formated_gdf['Recovered'])+2], ASSIGN=""natural earth"", animation_frame=""date"", ASSIGN='Recovered for each day') ASSIGN.show()",1,"[""formated_gdf['size'] = formated_gdf['Recovered'].pow(0.3)\n"", '\n', 'fig = px.scatter_geo(formated_gdf, locations=""Country/Region"", locationmode=\'country names\', \n', '                     color=""Recovered"", size=\'size\', hover_name=""Country/Region"", \n', ""                     range_color= [0, max(formated_gdf['Recovered'])+2], \n"", '                     projection=""natural earth"", animation_frame=""date"", \n', ""                     title='Recovered for each day')\n"", 'fig.show()']"
"ASSIGN= cbdr.groupby(['STATEpath','DISTRICT','Year'])['Hurt'].sum().reset_index().sort_values(by='Hurt',ascending=False) ASSIGN.head(10).style.background_gradient(cmap='Greys')",1,"[""s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Hurt'].sum().reset_index().sort_values(by='Hurt',ascending=False)\n"", ""s.head(10).style.background_gradient(cmap='Greys')""]"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# Any results you write to the current directory are saved as output.']"
CHECKPOINT x_pca.shape,0,['x_pca.shape']
CHECKPOINT df_map_final,0,"['#displaying the new data frame this frame will be used for map ploting\n', 'df_map_final']"
ASSIGN = [] for x in Benford_percentiles['First Digit Expected']: if x > 0: ASSIGN.append(x),0,"['#This is just a quick script to seperate out the expected first digit from the rest of the Data Frame as these are on a\n', '#scale of 1-9 instead of 0-9 like the second and third digits\n', 'First_digit_benfords = []\n', ""for x in Benford_percentiles['First Digit Expected']:\n"", '    if x > 0:\n', '        First_digit_benfords.append(x)']"
tf1.head(),0,['tf1.head()']
"ggplot(bikes,aes(x='Low Temp (°F)', y='Total')) + geom_point() + \ stat_smooth(method=""loess"", color='blue')",1,"[""ggplot(bikes,aes(x='Low Temp (°F)', y='Total')) + geom_point() + \\\n"", 'stat_smooth(method=""loess"", color=\'blue\')']"
draw_df.drawing.values[0],0,['draw_df.drawing.values[0]']
CHECKPOINT ASSIGN = 'path' ASSIGN = os.listdir(img_dir) print(len(ASSIGN)) ASSIGN = 0.8,0,"['# Code -- https://github.com/alexandru-dinu/cae\n', '# DataBase -- https://www.kaggle.com/hsankesara/flickr-image-dataset\n', '\n', '\n', '\n', '\n', ""img_dir = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images/'\n"", 'img_list = os.listdir(img_dir)\n', 'print(len(img_list))\n', 'valid_ratio = 0.8']"
test.head(),0,['test.head()']
headline_by_year(2000),0,['headline_by_year(2000)']
"ASSIGN = merge.drop(['Car_Name','Fuel_Type','Seller_Type','Transmission','CNG','Individual','Automatic','Owner','Kms_Driven'], axis = 'columns')",0,"[""final = merge.drop(['Car_Name','Fuel_Type','Seller_Type','Transmission','CNG','Individual','Automatic','Owner','Kms_Driven'], axis = 'columns')""]"
"def parse_goal(goal, home_id, away_id): ''' The function parses the goal values which is xml text into more convenient tuble Args: goal -> xml text with multiple tags and goal info home_id -> the id of the home team of the match that goal was scored in away_id -> the id of the away team Returns: a tuble of two lists: the first one is the home goals list and the second is the away goals list. each list consists of a number of tubles correspond to each goal. tuble format: (time of the goal in mins-int, scorer id-int, assisstant id-int, goal type-string) ''' if pd.notna(goal): if xmltodict.parse(goal)['goal'] != None: ASSIGN = xmltodict.parse(goal)['goal']['value'] ASSIGN = list() ASSIGN = list() if type(ASSIGN) == collections.OrderedDict: ASSIGN = [ASSIGN] for g in ASSIGN: try: ASSIGN = int(g['player1']) except: ASSIGN = 0 try: ASSIGN = int(g['player2']) except: ASSIGN = 0 ASSIGN = (int(g['elapsed']),p1, p2, g['comment']) if 'del' not in g.keys(): if int(g['team']) == home_id: ASSIGN.append(ASSIGN) else: ASSIGN.append(ASSIGN) return home_goals, away_goals",0,"['def parse_goal(goal, home_id, away_id):\n', ""    '''\n"", '    The function parses the goal values which is xml text into more convenient tuble\n', '    Args:\n', '        goal -> xml text with multiple tags and goal info\n', '        home_id -> the id of the home team of the match that goal was scored in\n', '        away_id -> the id of the away team\n', '    Returns:\n', '        a tuble of two lists: the first one is the home goals list and the second is the away goals list.\n', '        each list consists of a number of tubles correspond to each goal.\n', '        tuble format: (time of the goal in mins-int, scorer id-int, assisstant id-int, goal type-string)\n', ""    '''\n"", '    if pd.notna(goal):\n', ""        if xmltodict.parse(goal)['goal'] != None:\n"", ""            goal_dict = xmltodict.parse(goal)['goal']['value']\n"", '            home_goals = list()\n', '            away_goals = list()\n', '            if type(goal_dict) == collections.OrderedDict:\n', '                goal_dict = [goal_dict]\n', '            for g in goal_dict:\n', '                try:\n', ""                    p1 = int(g['player1'])\n"", '                except:\n', '                    p1 = 0\n', '                try:\n', ""                    p2 = int(g['player2'])\n"", '                except:\n', '                    p2 = 0                \n', ""                g_info = (int(g['elapsed']),p1, p2, g['comment'])\n"", ""                if 'del' not in g.keys():\n"", ""                    if int(g['team']) == home_id:\n"", '                        home_goals.append(g_info)\n', '                    else:\n', '                        away_goals.append(g_info)\n', '            return home_goals, away_goals']"
"ASSIGN = rfc.predict(test.iloc[:,test.columns != 'Survived'])",0,"['\n', ""predictions = rfc.predict(test.iloc[:,test.columns != 'Survived'])\n"", '\n']"
"plt.figure(figsize=(5,5)) plt.plot(train.history['accuracy'],'r',label='Training accuracy') plt.plot(train.history['val_accuracy'],'b',label='Validation accuracy') plt.legend()",1,"['plt.figure(figsize=(5,5))\n', ""plt.plot(train.history['accuracy'],'r',label='Training accuracy')\n"", ""plt.plot(train.history['val_accuracy'],'b',label='Validation accuracy')\n"", 'plt.legend()']"
match_lineup_df.isnull().sum(axis= 1).max(),0,"['#max number of missing values per lineup\n', 'match_lineup_df.isnull().sum(axis= 1).max()']"
"SETUP CHECKPOINT plt.rcParams['figure.figsize']=(20,10) print(os.listdir()) py.init_notebook_mode(connected=False)",0,"['import matplotlib.pyplot as plt\n', 'import matplotlib.image as mpimg\n', '%matplotlib inline\n', 'import numpy as np\n', 'import os\n', 'import pandas as pd\n', 'import plotly.offline as py\n', 'import plotly.graph_objs as go\n', 'import seaborn as sns\n', '\n', ""plt.rcParams['figure.figsize']=(20,10)\n"", 'print(os.listdir(""../input""))\n', 'py.init_notebook_mode(connected=False)\n', '\n', '%env JOBLIB_TEMP_FOLDER=/tmp']"
"ASSIGN= cbdr.groupby(['STATEpath','DISTRICT','Year'])['Arson'].sum().reset_index().sort_values(by='Arson',ascending=False) ASSIGN.head(10).style.background_gradient(cmap='RdPu')",0,"[""s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Arson'].sum().reset_index().sort_values(by='Arson',ascending=False)\n"", ""s.head(10).style.background_gradient(cmap='RdPu')""]"
"CHECKPOINT ASSIGN = DecisionTreeClassifier(class_weight = 'balanced',criterion = 'entropy',max_depth = 5,min_samples_split = 2,splitter = 'best',random_state = 6) Estimator.append(('ASSIGN',DecisionTreeClassifier(class_weight = 'balanced',criterion = 'entropy',max_depth = 5,min_samples_split = 2,splitter = 'best',random_state = 6))) ASSIGN = cross_val_score(dt,x_train,x_test,ASSIGN=10) Accuracy4 = ASSIGN.mean() Accuracy.append(Accuracy4) print(ASSIGN) print(ASSIGN.mean())",0,"[""dt = DecisionTreeClassifier(class_weight = 'balanced',criterion = 'entropy',max_depth = 5,min_samples_split = 2,splitter = 'best',random_state = 6)\n"", ""Estimator.append(('dt',DecisionTreeClassifier(class_weight = 'balanced',criterion = 'entropy',max_depth = 5,min_samples_split = 2,splitter = 'best',random_state = 6)))\n"", 'cv = cross_val_score(dt,x_train,x_test,cv=10)\n', 'Accuracy4 = cv.mean()\n', 'Accuracy.append(Accuracy4)\n', 'print(cv)\n', 'print(cv.mean())']"
"Krange1 = range(1,20) ASSIGN = {} ASSIGN = [] for k in Krange1: ASSIGN = KNeighborsClassifier(n_neighbors = k) ASSIGN.fit(x_train1,x_test) ASSIGN = knn.predict(y_train1) ASSIGN[k] = metrics.accuracy_score(result['Survived'],ASSIGN) ASSIGN.append(metrics.accuracy_score(result['Survived'],ASSIGN)) plt.plot(Krange,scores_list) plt.xlabel(""Value of K"") plt.ylabel(""Accuracy"")",1,"['Krange1 = range(1,20)\n', 'scores1 = {}\n', 'scores_list1 = []\n', 'for k in Krange1:\n', '    knn = KNeighborsClassifier(n_neighbors = k)\n', '    knn.fit(x_train1,x_test)\n', '    y_pred = knn.predict(y_train1)\n', ""    scores1[k] = metrics.accuracy_score(result['Survived'],y_pred)\n"", ""    scores_list1.append(metrics.accuracy_score(result['Survived'],y_pred))\n"", '    \n', 'plt.plot(Krange,scores_list)\n', 'plt.xlabel(""Value of K"")\n', 'plt.ylabel(""Accuracy"")']"
"test['Total'].plot(legend=True) predictions1.plot(legend=True) predictions2.plot(legend=True) predictions11.plot(legend=True,figsize=(12,6));",1,"[""test['Total'].plot(legend=True)\n"", 'predictions1.plot(legend=True)\n', 'predictions2.plot(legend=True)\n', 'predictions11.plot(legend=True,figsize=(12,6));']"
ASSIGN = (data_features['BsmtCond'].isnull() & data_features['BsmtExposure'].notnull()) data_features[ASSIGN],0,"[""condition1 = (data_features['BsmtCond'].isnull() & data_features['BsmtExposure'].notnull())\n"", 'data_features[condition1]']"
df['breed'].describe(),0,"[""df['breed'].describe()""]"
"SETUP CHECKPOINT warnings.filterwarnings(""ignore"") for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import warnings\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns\n', 'from fastai.tabular import *\n', '%matplotlib inline\n', 'warnings.filterwarnings(""ignore"")\n', '\n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# Any results you write to the current directory are saved as output.']"
CHECKPOINT ASSIGN = logisticRegr.predict(X_train_pca) predictions,0,"['predictions = logisticRegr.predict(X_train_pca)\n', 'predictions']"
"ASSIGN=data[data['date']==max(data['date'])].groupby(['Countrypath']).agg('sum').sort_values('Confirmed',ascending=False)[:10] ASSIGN=ASSIGN.reset_index() ASSIGN=ASSIGN.drop(columns='SNo') ASSIGN['Recovered rate']=ASSIGN['Recovered']path['Confirmed'] ASSIGN.style.background_gradient(cmap='Blues')",1,"[""Confirmed_last=data[data['date']==max(data['date'])].groupby(['Country/Region']).agg('sum').sort_values('Confirmed',ascending=False)[:10]\n"", 'Confirmed_last=Confirmed_last.reset_index()\n', ""Confirmed_last=Confirmed_last.drop(columns='SNo')\n"", ""Confirmed_last['Recovered rate']=Confirmed_last['Recovered']/Confirmed_last['Confirmed']\n"", ""Confirmed_last.style.background_gradient(cmap='Blues')""]"
ASSIGN=ASSIGN.dropna(subset=['name']),0,"[""data=data.dropna(subset=['name'])""]"
"sns.catplot(x='Year', y='Robbery', data=cbdr,height = 5, aspect = 4)",1,"[""sns.catplot(x='Year', y='Robbery', data=cbdr,height = 5, aspect = 4)""]"
"SETUP CHECKPOINT def train_model(epoch): model.train() for batch_idx, (trend, stable, target) in enumerate(train_loader): ASSIGN = Variable(trend), Variable(stable),Variable(target) optimizer.zero_grad() ASSIGN = model(trend,stable) ASSIGN = criterion(output, target) ASSIGN.backward() optimizer.step() if batch_idx % 300 == 0: print('Train Epoch: {} [{}path{} ({:.0f}%)]\tLoss: {:.6f}'.format( epoch, batch_idx * len(trend), len(train_loader.dataset), 100. * batch_idx path(train_loader), ASSIGN.data)) def test_model(epoch): model.eval() ASSIGN = 0 ASSIGN = [] ASSIGN = [] for ASSIGN in test_loader: ASSIGN = Variable(trend),Variable(stable),Variable(target) ASSIGN = model(trend,stable) ASSIGN += criterion(ASSIGN, target).data ASSIGN.append(ASSIGN) ASSIGN.append(target) ASSIGN = torch.cat(ASSIGN, dim=0) ASSIGN = torch.cat(ASSIGN, dim=0) ASSIGN = ASSIGN ASSIGN= len(test_loader) print('\nTest set: Average ASSIGN: {:.4f}, MSE: {} \n'.format( test_loss, MSE ))",0,"['# Training process\n', 'def train_model(epoch):\n', '    model.train()\n', '    for batch_idx, (trend, stable, target) in enumerate(train_loader):\n', '        trend, stable, target = Variable(trend), Variable(stable),Variable(target)\n', '        optimizer.zero_grad()\n', '        output = model(trend,stable)\n', '        loss = criterion(output, target)  \n', '        loss.backward()\n', '        optimizer.step()\n', '        if batch_idx % 300 == 0:\n', ""            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n"", '                epoch, batch_idx * len(trend), len(train_loader.dataset),\n', '                100. * batch_idx / len(train_loader), loss.data))\n', '\n', 'def test_model(epoch):\n', '    model.eval()\n', '    test_loss = 0\n', '    y_pred = []\n', '    y_true = []\n', '    for trend, stable, target in test_loader:\n', '        trend,stable, target = Variable(trend),Variable(stable),Variable(target)\n', '        output = model(trend,stable)\n', '        test_loss += criterion(output, target).data\n', '        y_pred.append(output)\n', '        y_true.append(target)\n', '\n', '    y_pred = torch.cat(y_pred, dim=0)\n', '    y_true = torch.cat(y_true, dim=0)\n', '    test_loss = test_loss\n', '    test_loss /= len(test_loader) # loss function already averages over batch size\n', '    MSE = mean_squared_error(y_true.detach().numpy(), y_pred.detach().numpy())\n', ""    print('\\nTest set: Average loss: {:.4f}, MSE: {} \\n'.format(\n"", '        test_loss, MSE \n', '        ))']"
"ASSIGN = RandomForestRegressor(random_state = 1) ASSIGN.fit(train_X, train_y)",0,"['# To improve accuracy, create a new Random Forest model which you will train on all training data\n', 'rf_model_on_full_data = RandomForestRegressor(random_state = 1)\n', '\n', '# fit rf_model_on_full_data on all data from the training data\n', 'rf_model_on_full_data.fit(train_X, train_y)\n']"
"CHECKPOINT ASSIGN = .2 while len(submission_df[submission_df['income class']>=ASSIGN])< 19100: ASSIGN = ASSIGN-.000001 ASSIGN = .5 - pivot print('Pivot is',ASSIGN,'- tensor ASSIGN is', ASSIGN)",0,"[""# should have more positive classes, let's align...\n"", 'pivot = .2\n', ""while len(submission_df[submission_df['income class']>=pivot])< 19100: #(np.argmax(y)*100):\n"", '    pivot = pivot-.000001\n', 'correction = .5 - pivot\n', ""print('Pivot is',pivot,'- tensor correction is', correction)""]"
"data_features['MSSubClass'].groupby((data_features['BsmtExposure'],data_features['BsmtQual'])).count()",0,"[""data_features['MSSubClass'].groupby((data_features['BsmtExposure'],data_features['BsmtQual'])).count()""]"
"ASSIGN = 1e-03 learn.fit_one_cycle(5, ASSIGN)",0,"['# Initial learning phase using suggested learning rate during 3 cycles\n', 'lr = 1e-03\n', 'learn.fit_one_cycle(5, lr)']"
"SETUP CHECKPOINT ASSIGN = score(y_train, predictions) print('precision: {}'.format(precision)) print('recall: {}'.format(recall)) print('fscore: {}'.format(fscore)) print('support: {}'.format(support))",0,"['from sklearn.metrics import precision_recall_fscore_support as score\n', '\n', 'precision, recall, fscore, support = score(y_train, predictions)\n', '\n', ""print('precision: {}'.format(precision))\n"", ""print('recall: {}'.format(recall))\n"", ""print('fscore: {}'.format(fscore))\n"", ""print('support: {}'.format(support))""]"
netG.eval() ASSIGN = next(iter(dataloader)) ASSIGN = valid_batch[0].to(device) ASSIGN = netG(blur_images) ASSIGN = (output_heatmap[0].cpu()),1,"['netG.eval()\n', 'valid_batch = next(iter(dataloader))\n', 'blur_images = valid_batch[0].to(device)\n', 'output_heatmap = netG(blur_images)\n', 'rec_img =  (output_heatmap[0].cpu())']"
model.summary(),0,"['\n', 'model.summary()\n']"
"SVC.fit(x_train,x_test) ASSIGN = SVC.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('LinearSVC(No HT).csv',index = False)",0,"['SVC.fit(x_train,x_test)\n', 'model2pred = SVC.predict(y_train)\n', ""submission2 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission2['PassengerId'] = result['PassengerId']\n"", ""submission2['Survived'] = model2pred\n"", ""submission2.to_csv('LinearSVC(No HT).csv',index = False)""]"
test_data.info(),0,['test_data.info()']
"CHECKPOINT ForestTreesPerformance = [] for n_trees in range(1,11,1): ASSIGN=RandomForestClassifier(n_estimators=n_trees) ASSIGN=[] for train_id, test_id in kf.split(data_matf,data_matl): X_train, X_test = data_matf.values[train_id], data_matf.values[test_id] ASSIGN = data_matl.values[train_id], data_matl.values[test_id] Rf.fit(X_train,y_train) ASSIGN = Rf.predict(X_test) ASSIGN = accuracy_score(y_test, predictions) ASSIGN.append(ASSIGN) ForestTreesPerformance.append(np.mean(ASSIGN)) plt.plot(range(1,11,1),ForestTreesPerformance) plt.show() if (min(ForestTreesPerformance) > average_error_Dt): print() else: print(,np.argmin(ForestTreesPerformance)+1,)",1,"['ForestTreesPerformance = []\n', 'for n_trees in range(1,11,1):\n', '    pRf=RandomForestClassifier(n_estimators=n_trees)\n', '    outcomesRfs=[]\n', '    for train_id, test_id in kf.split(data_matf,data_matl):\n', '        X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]\n', '        y_train, y_test = data_matl.values[train_id], data_matl.values[test_id]\n', '        Rf.fit(X_train,y_train)\n', '        predictions = Rf.predict(X_test)\n', '        accuracy = accuracy_score(y_test, predictions)\n', '        outcomesRfs.append(accuracy)\n', '    ForestTreesPerformance.append(np.mean(outcomesRfs))\n', 'plt.plot(range(1,11,1),ForestTreesPerformance)\n', 'plt.show()\n', 'if (min(ForestTreesPerformance) > average_error_Dt):\n', '    print(""A decision tree works better than a random forest with respect to the probability error"")\n', 'else:\n', '    print(""A random forest with"",np.argmin(ForestTreesPerformance)+1,""trees works better than a decision tree with respect to the probability error"")']"
"CHECKPOINT ASSIGN = df_hist_trans.groupby(""card_id"") print(type(ASSIGN)) ASSIGN = ASSIGN.agg({'merchant_category_id':['max']}).reset_index() print(type(ASSIGN)) ASSIGN.columns = [""card_id"", ""max_merchant_category_id""] ASSIGN = pd.merge(ASSIGN, gdf, on=""card_id"", how=""left"") ASSIGN = pd.merge(ASSIGN, gdf, on=""card_id"", how=""left"")",0,"['gdf = df_hist_trans.groupby(""card_id"")\n', 'print(type(gdf))\n', ""gdf = gdf.agg({'merchant_category_id':['max']}).reset_index()\n"", 'print(type(gdf))\n', 'gdf.columns = [""card_id"", ""max_merchant_category_id""]\n', 'df_train = pd.merge(df_train, gdf, on=""card_id"", how=""left"")\n', 'df_test = pd.merge(df_test, gdf, on=""card_id"", how=""left"")']"
CHECKPOINT df_plot_bottom,0,"['#displaying the data frame\n', 'df_plot_bottom']"
ASSIGN = [col for col in ASSIGN if col in train_df.columns] ASSIGN = [col for col in ASSIGN if col in train_df.columns],0,"['numerical = [col for col in numerical if col in train_df.columns]\n', 'categorical = [col for col in categorical if col in train_df.columns]']"
"ASSIGN=plt.subplots(1,2,figsize=(15,8)) ASSIGN = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",'cadetblue','hotpink','orange','darksalmon','brown') train.groupby(['OverallQual'])['Id'].agg('count').plot(kind='bar',color=ASSIGN,ax=ax[0]) ax[0].set_title(""Top 10 OverallQual by counts"",size=20) ax[0].set_xlabel('counts',size=18) ASSIGN=train.groupby(['OverallQual'])['Id'].agg('ASSIGN') ASSIGN=list(train.groupby(['OverallQual'])['Id'].agg('count').index) ASSIGN=list(count) ASSIGN=pd.DataFrame({""group"":groups,""counts"":counts}) ASSIGN=(""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",'cadetblue','hotpink','orange','darksalmon','brown') ASSIGN = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1]) plt.legend(loc=0, bbox_to_anchor=(1.20,0.8)) plt.subplots_adjust(wspace =0.5, hspace =0) plt.ylabel('')",1,"['fig,ax=plt.subplots(1,2,figsize=(15,8))\n', 'clr = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",\'cadetblue\',\'hotpink\',\'orange\',\'darksalmon\',\'brown\')\n', ""train.groupby(['OverallQual'])['Id'].agg('count').plot(kind='bar',color=clr,ax=ax[0])\n"", 'ax[0].set_title(""Top 10 OverallQual by counts"",size=20)\n', ""ax[0].set_xlabel('counts',size=18)\n"", '\n', '\n', ""count=train.groupby(['OverallQual'])['Id'].agg('count')\n"", ""groups=list(train.groupby(['OverallQual'])['Id'].agg('count').index)\n"", 'counts=list(count)\n', 'type_dict=pd.DataFrame({""group"":groups,""counts"":counts})\n', 'clr1=(""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",\'cadetblue\',\'hotpink\',\'orange\',\'darksalmon\',\'brown\')\n', ""qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n"", 'plt.legend(loc=0, bbox_to_anchor=(1.20,0.8)) \n', 'plt.subplots_adjust(wspace =0.5, hspace =0)\n', ""plt.ylabel('')""]"
"test_df['ConfirmedCases'] = np.NAN test_df['Fatalities'] = np.NAN test_df['Id_x'] = 0 test_df['Id_y'] = 0 ASSIGN = ASSIGN[list(scale_train.columns)] ASSIGN = pd.concat((scale_train,test_df),axis = 0)",0,"['# make train dataframe and test dataframe have same columns\n', ""test_df['ConfirmedCases'] = np.NAN\n"", ""test_df['Fatalities'] = np.NAN\n"", ""test_df['Id_x'] = 0\n"", ""test_df['Id_y'] = 0\n"", 'test_df = test_df[list(scale_train.columns)]\n', '# merge scale_train and test\n', 'total_df = pd.concat((scale_train,test_df),axis = 0)']"
ASSIGN = reg_fata.predict(X_test),0,['pred_fata = reg_fata.predict(X_test)']
"ASSIGN = ['Smoke everyday', 'Smoke some days', 'Former smoker', 'Never smoked'] for x in ASSIGN: ASSIGN = ASSIGN.str.strip('%').astype('float')",0,"[""columns = ['Smoke everyday', 'Smoke some days', 'Former smoker', 'Never smoked']\n"", '\n', 'for x in columns:\n', ""    tb[x] = tb[x].str.strip('%').astype('float')\n"", '    ']"
"CHECKPOINT ASSIGN = intersection(first_numbers, second_numbers) ASSIGN = intersection(first_numbers, third_numbers) ASSIGN = intersection(second_numbers, third_numbers) ASSIGN = intersection(inter_one_two, third_numbers) print(ASSIGN) print(ASSIGN) print(ASSIGN) print(ASSIGN)",0,"['inter_one_two = intersection(first_numbers, second_numbers)\n', 'inter_one_three = intersection(first_numbers, third_numbers)\n', 'inter_two_three = intersection(second_numbers, third_numbers)\n', 'interfinal = intersection(inter_one_two, third_numbers)\n', 'print(inter_one_two)\n', 'print(inter_one_three)\n', 'print(inter_two_three)\n', 'print(interfinal)']"
"ASSIGN = corr15.drop(['SalePrice','AllSF','1stFlrSF','GarageFinish','GarageArea']) ASSIGN = corr10.index.tolist() for col in ASSIGN: data_features[col + '-2'] = data_features[col] **2 data_features[col + '-3'] = data_features[col] **3 data_features[col + '-sqrt'] = np.sqrt(data_features[col])",0,"['#Polynomial on the top 10 features.For the features have connection themself, only use one to do polynomail.\n', ""corr10 = corr15.drop(['SalePrice','AllSF','1stFlrSF','GarageFinish','GarageArea'])\n"", 'corr10_var = corr10.index.tolist()\n', 'for col in corr10_var:\n', ""    data_features[col + '-2'] = data_features[col] **2\n"", ""    data_features[col + '-3'] = data_features[col] **3\n"", ""    data_features[col + '-sqrt'] = np.sqrt(data_features[col])\n"", '\n']"
"CHECKPOINT ASSIGN=data_Queens.loc[(data_Queens['price'] <100)][2801:-1] ASSIGN['label']=ASSIGN.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1) ASSIGN=-73.80 ASSIGN=40.70 ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(data_Queens_100_3_map) for lat,lon,label in zip(ASSIGN.latitude,ASSIGN.longitude,ASSIGN.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN) data_Queens_100_3_map",1,"[""data_Queens_100_3=data_Queens.loc[(data_Queens['price'] <100)][2801:-1]\n"", ""data_Queens_100_3['label']=data_Queens_100_3.apply(lambda x: (x['name'],'price:'+str(x['price'])),axis=1)\n"", 'Long=-73.80\n', 'Lat=40.70\n', 'data_Queens_100_3_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'data_Queens_100_3_rooms_map=plugins.MarkerCluster().add_to(data_Queens_100_3_map)\n', 'for lat,lon,label in zip(data_Queens_100_3.latitude,data_Queens_100_3.longitude,data_Queens_100_3.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_Queens_100_3_rooms_map)\n', 'data_Queens_100_3_map.add_child(data_Queens_100_3_rooms_map)\n', '\n', 'data_Queens_100_3_map']"
"sns.relplot(x ='LOR ', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')",1,"[""sns.relplot(x ='LOR ', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')""]"
def WH4(df): df['Hydro_high'] = df.Vertical_Distance_To_Hydrology.apply(lambda x: x > 3 ) df['Hydro_Euclidean'] = (df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2).apply(np.sqrt) df['Hydro_Fire_road'] = (df.Horizontal_Distance_To_Roadways + df.Horizontal_Distance_To_Fire_Points)path(df.Hydro_Euclideanpath+1) df['Hydro_Fire_sum'] = (df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']) df['Hydro_Elevation_diff'] = (df['Elevation'] - df['Vertical_Distance_To_Hydrology']) df['Soil_Type12_32'] = df['Soil_Type_32'] + df['Soil_Type_12'] df['Soil_Type23_22_32_33'] = df['Soil_Type_23'] + df['Soil_Type_22'] + df['Soil_Type_32'] + df['Soil_Type_33'] df['Hydro_Fire_diff'] = (df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points']).abs() df['Hydro_Road_sum'] = (df['Horizontal_Distance_To_Hydrology'] +df['Horizontal_Distance_To_Roadways']) df['Hydro_Road_diff'] = (df['Horizontal_Distance_To_Hydrology'] -df['Horizontal_Distance_To_Roadways']).abs() df['Road_Fire_sum'] = (df['Horizontal_Distance_To_Roadways'] + df['Horizontal_Distance_To_Fire_Points']) df['Road_Fire_diff'] = (df['Horizontal_Distance_To_Roadways'] - df['Horizontal_Distance_To_Fire_Points']).abs(),0,"['def WH4(df):\n', ""    df['Hydro_high'] = df.Vertical_Distance_To_Hydrology.apply(lambda x: x > 3 )\n"", ""    df['Hydro_Euclidean'] = (df['Horizontal_Distance_To_Hydrology']**2 +\n"", ""                            df['Vertical_Distance_To_Hydrology']**2).apply(np.sqrt)\n"", ""    #df.drop(['Vertical_Distance_To_Hydrology'], axis=1, inplace=True)\n"", ""    #df.drop(['Horizontal_Distance_To_Hydrology'], axis=1, inplace=True)\n"", ""    df['Hydro_Fire_road'] = (df.Horizontal_Distance_To_Roadways + df.Horizontal_Distance_To_Fire_Points)/(df.Hydro_Euclidean/20000+1)\n"", ""    df['Hydro_Fire_sum'] = (df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points'])\n"", ""    #df.drop(['Soil_Type15'], axis=1, inplace=True)\n"", ""    #df.drop(['Soil_Type7'], axis=1, inplace=True)\n"", ""    df['Hydro_Elevation_diff'] = (df['Elevation'] - df['Vertical_Distance_To_Hydrology'])\n"", '    \n', ""    df['Soil_Type12_32'] = df['Soil_Type_32'] + df['Soil_Type_12']\n"", ""    df['Soil_Type23_22_32_33'] = df['Soil_Type_23'] + df['Soil_Type_22'] + df['Soil_Type_32'] + df['Soil_Type_33']\n"", '      \n', ""    df['Hydro_Fire_diff'] = (df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points']).abs()\n"", ""    df['Hydro_Road_sum'] = (df['Horizontal_Distance_To_Hydrology'] +df['Horizontal_Distance_To_Roadways'])\n"", ""    df['Hydro_Road_diff'] = (df['Horizontal_Distance_To_Hydrology'] -df['Horizontal_Distance_To_Roadways']).abs()\n"", ""    df['Road_Fire_sum'] = (df['Horizontal_Distance_To_Roadways'] + df['Horizontal_Distance_To_Fire_Points'])\n"", ""    df['Road_Fire_diff'] = (df['Horizontal_Distance_To_Roadways'] - df['Horizontal_Distance_To_Fire_Points']).abs()\n"", '    #df.loc[:, :] = np.floor(MinMaxScaler((0, 100)).fit_transform(df))\n', ""    #df = df.astype('int8')\n"", '    #df.fillna(0)\n', '    \n']"
ASSIGN=pd.read_csv('..path'),0,"[""data=pd.read_csv('../input/iris/Iris.csv')""]"
"sns.jointplot(x = 'TOEFL Score', y = 'University Rating', data=df)",1,"[""sns.jointplot(x = 'TOEFL Score', y = 'University Rating', data=df)""]"
"ASSIGN = pd.DataFrame(holdout.isnull().sum().sort_values(ascending=False), columns=['Total']) ASSIGN = pd.DataFrame(round(100*(holdout.isnull().sum()path[0]),2).sort_values(ascending=False)\ ,columns=['Percentage']) pd.concat([ASSIGN, ASSIGN], axis = 1).head()",0,"['# Checking for total count and percentage of null values in all columns of the dataframe.\n', '\n', ""total = pd.DataFrame(holdout.isnull().sum().sort_values(ascending=False), columns=['Total'])\n"", 'percentage = pd.DataFrame(round(100*(holdout.isnull().sum()/holdout.shape[0]),2).sort_values(ascending=False)\\\n', ""                          ,columns=['Percentage'])\n"", 'pd.concat([total, percentage], axis = 1).head()']"
"SETUP CHECKPOINT ASSIGN = plt.figure(figsize=(15,5)) plt.subplot(1,2,1) sns.distplot(train[""SalePrice""], fit=norm) ASSIGN = norm.fit(train['SalePrice']) plt.legend(['Norm dist. mu={:.2f}, sigma={:.2f}'.format(mu,sigma)]) plt.subplot(1,2,2) stats.probplot(train['SalePrice'],plot=plt) plt.title('Before transfomation') print('mu = {:.2f},\nsigma = {:.2f}'.format(mu,sigma)) train.SalePrice = np.log1p(train.SalePrice) ASSIGN = train.SalePrice.values ASSIGN = train.SalePrice ASSIGN = plt.figure(figsize=(15,5)) plt.subplot(1,2,1) sns.distplot(train.SalePrice, fit=norm) ASSIGN = norm.fit(train.SalePrice) plt.legend(['Norm distribution.(mu={:.2f}, sigma={:.2f})'.format(ASSIGN)]) plt.subplot(1,2,2) plt.ylabel('Frequency') stats.probplot(train.SalePrice, plot=plt) plt.title('After transformation') print('\n mu={:.2f}, sigma={:.2f}'.format(mu,sigma))",1,"['#the qq-plot before log-transfomation\n', 'from scipy import stats\n', 'from scipy.stats import norm\n', 'fig = plt.figure(figsize=(15,5)) #这里要记得写等于\n', 'plt.subplot(1,2,1) #如果需要一行多幅图 需要在这里先制定是哪一幅图\n', 'sns.distplot(train[""SalePrice""], fit=norm)\n', ""mu, sigma = norm.fit(train['SalePrice'])\n"", ""#plt.legend(['Norm dist. mu = %f, sigma = %f' %(mu,sigma)], loc='upper right')\n"", ""plt.legend(['Norm dist. mu={:.2f}, sigma={:.2f}'.format(mu,sigma)])\n"", 'plt.subplot(1,2,2)\n', ""stats.probplot(train['SalePrice'],plot=plt)\n"", ""plt.title('Before transfomation')\n"", ""print('mu = {:.2f},\\nsigma = {:.2f}'.format(mu,sigma))\n"", '\n', '#Do the transformation\n', 'train.SalePrice = np.log1p(train.SalePrice)\n', 'y_train = train.SalePrice.values\n', 'y_train_orig = train.SalePrice\n', '#the reason why we do this is because the models like linear regression and SVM need the data to be norm distribution.\n', '\n', '#after the transformation\n', 'fig = plt.figure(figsize=(15,5))\n', 'plt.subplot(1,2,1)\n', 'sns.distplot(train.SalePrice, fit=norm)\n', 'mu, sigma = norm.fit(train.SalePrice)\n', ""plt.legend(['Norm distribution.(mu={:.2f}, sigma={:.2f})'.format(mu, sigma)])\n"", 'plt.subplot(1,2,2)\n', ""plt.ylabel('Frequency')\n"", 'stats.probplot(train.SalePrice, plot=plt)\n', ""plt.title('After transformation')\n"", ""print('\\n mu={:.2f}, sigma={:.2f}'.format(mu,sigma))""]"
SETUP,0,"['from mpl_toolkits.mplot3d import Axes3D\n', 'from sklearn.preprocessing import StandardScaler\n', 'import matplotlib.pyplot as plt # plotting\n', 'import numpy as np # linear algebra\n', 'import os # accessing directory structure\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n']"
"for i in [train, test]: ASSIGN = ASSIGN.fillna('S')",0,"['for i in [train, test]:\n', ""    i['Embarked'] = i['Embarked'].fillna('S')""]"
os.listdir('..path'),0,"[""os.listdir('../input/plant-pathology-2020-fgvc7')""]"
CHECKPOINT ASSIGN = logisticRegr.predict(X_test_pca) predict_test,0,"['predict_test = logisticRegr.predict(X_test_pca)\n', 'predict_test']"
"ASSIGN = pd.get_dummies(final.Ticket_length, prefix = 'Ticket Length') ASSIGN = pd.concat([ASSIGN , conversion], axis = 1) ASSIGN.drop(['Ticket','Ticket_length'],axis = 1, inplace = True)",0,"[""conversion = pd.get_dummies(final.Ticket_length, prefix = 'Ticket Length')\n"", 'final = pd.concat([final , conversion], axis = 1)\n', ' \n', ""final.drop(['Ticket','Ticket_length'],axis = 1, inplace = True)""]"
"ASSIGN = transforms.Compose([ transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)), transforms.RandomRotation(degrees=15), transforms.RandomHorizontalFlip(), transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) ASSIGN = transforms.Compose([transforms.CenterCrop(size=224), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) ASSIGN = datasets.ImageFolder(TRAIN_DIR, train_transforms) ASSIGN = datasets.ImageFolder(TEST_DIR, valid_transforms)",0,"['train_transforms = transforms.Compose([\n', '    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n', '    transforms.RandomRotation(degrees=15),\n', '    transforms.RandomHorizontalFlip(),\n', '    transforms.CenterCrop(size=224),\n', '    transforms.ToTensor(),\n', '    transforms.Normalize([0.485, 0.456, 0.406],\n', '                         [0.229, 0.224, 0.225])])\n', '\n', 'valid_transforms = transforms.Compose([transforms.CenterCrop(size=224),\n', '    transforms.ToTensor(),\n', '    transforms.Normalize([0.485, 0.456, 0.406],\n', '                         [0.229, 0.224, 0.225])])\n', '\n', 'dataset = datasets.ImageFolder(TRAIN_DIR, train_transforms)\n', 'test_dataset = datasets.ImageFolder(TEST_DIR, valid_transforms)']"
cbdr.head(),0,['cbdr.head()']
"ASSIGN=data.groupby('date')['timestamp'].agg('count').ASSIGN plt.figure(figsize=(15,100)) for i in range(len(ASSIGN)): plt.subplot(15,2,i+1) sns.scatterplot(x='latitude',y='longitude',alpha=0.01,data=data[data.date==ASSIGN[i]]) plt.title('total_car '+ASSIGN[i],size=20) plt.xlim(32,32.17) plt.ylim(34.74,34.85)",1,"[""index=data.groupby('date')['timestamp'].agg('count').index\n"", 'plt.figure(figsize=(15,100))\n', 'for i in range(len(index)):\n', '    plt.subplot(15,2,i+1)\n', ""    sns.scatterplot(x='latitude',y='longitude',alpha=0.01,data=data[data.date==index[i]])\n"", ""    plt.title('total_car '+index[i],size=20)\n"", '    plt.xlim(32,32.17)\n', '    plt.ylim(34.74,34.85)\n', '    ']"
"SVC.fit(x_train,x_test) SVC.score(y_train,y_test)",0,"['SVC.fit(x_train,x_test)\n', 'SVC.score(y_train,y_test)']"
ASSIGN=data[data.total_cars==2].drop_duplicates(subset=['latitude']) ASSIGN.head(),0,"[""data_total_cars_6=data[data.total_cars==2].drop_duplicates(subset=['latitude'])\n"", 'data_total_cars_6.head()\n']"
SETUP sns.set(),0,"['import os\n', 'import pandas as pd\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline\n', 'import seaborn as sns\n', 'sns.set()\n', 'from PIL import Image\n', 'from glob import glob\n', 'from skimage.io import imread\n', 'from os import listdir\n', 'from sklearn.preprocessing import LabelEncoder\n', 'import time\n', 'import cv2\n', 'import copy\n', 'from random import shuffle\n', 'from tqdm import tqdm_notebook as tqdm\n', 'from sklearn.model_selection import train_test_split\n', 'from keras.utils.np_utils import to_categorical\n', 'from sklearn.metrics import accuracy_score\n', 'from sklearn.metrics import confusion_matrix\n', 'from sklearn.metrics import classification_report\n', 'from sklearn.metrics import plot_roc_curve\n', 'from sklearn.metrics import precision_recall_fscore_support\n', 'from imblearn.metrics import sensitivity_specificity_support\n', 'from imgaug import augmenters as iaa\n', 'import imgaug as ia\n', '\n', '\n', '# import numpy as np\n', '# import matplotlib.pyplot as plt\n', 'from itertools import cycle\n', '\n', '# from sklearn import svm, datasets\n', 'from sklearn.metrics import roc_curve, auc\n', '# from sklearn.model_selection import train_test_split\n', '# from sklearn.preprocessing import label_binarize\n', '# from sklearn.multiclass import OneVsRestClassifier\n', 'from scipy import interp\n', 'from sklearn.metrics import roc_auc_score\n', '\n', 'from keras.applications.resnet50 import ResNet50, preprocess_input\n', 'from keras.utils.vis_utils import plot_model\n', 'from keras.optimizers import SGD,Adam\n', 'import numpy as np\n', 'from keras.applications.vgg16 import VGG16\n', 'from keras.layers import Dense, GlobalAveragePooling2D, Dropout,concatenate\n', 'from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, BatchNormalization\n', 'from keras.layers import Input\n', 'from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard,CSVLogger\n', '# import tools\n', 'import gc\n', 'from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix\n', 'from keras.models import Model\n', 'import keras\n', '# import channel_attention']"
"ASSIGN = pd.read_csv('..path') ASSIGN = best_submission.merge(outlier_id,how='right') ASSIGN.head()",0,"[""best_submission = pd.read_csv('../input/finaldata/submission_ashish.csv')\n"", ""most_likely_liers = best_submission.merge(outlier_id,how='right')\n"", 'most_likely_liers.head()']"
"plotPerColumnDistribution(df1, 10, 5)",1,"['plotPerColumnDistribution(df1, 10, 5)']"
sns.pairplot(data = data),1,['sns.pairplot(data = data)']
SETUP,0,['import pandas as pd']
CHECKPOINT Benford_percentiles,0,['Benford_percentiles']
ASSIGN = pd.read_csv('path') ASSIGN = pd.read_csv('path'),0,"['# read test_df and a new train_df\n', ""test_df = pd.read_csv('/kaggle/input/covid-with-weather-and-population/test_processed.csv')\n"", ""train_df2 =  pd.read_csv('/kaggle/input/covid19-with-population/update_train_processed.csv')""]"
relevant.head(),0,['relevant.head()']
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") print() print(ASSIGN.keys()) print() print(ASSIGN.shape)",0,"['#Write a Python program using Scikit-learn to print the keys, number of rows-columns, feature names and the description of the Iris data.\n', 'import pandas as pd\n', 'iris_data = pd.read_csv(""../input/iriscsv/Iris.csv"")\n', 'print(""\\nKeys of Iris dataset:"")\n', 'print(iris_data.keys())\n', 'print(""\\nNumber of rows and columns of Iris dataset:"")\n', 'print(iris_data.shape) ']"
"SETUP CHECKPOINT ASSIGN = np.split(X,2)[0] ASSIGN = np.split(X,2)[1] ASSIGN = np.split(train[""label""],2)[0] ASSIGN = np.split(train[""label""],2)[1] ASSIGN = DecisionTreeClassifier() ASSIGN.fit(ASSIGN,ASSIGN) ASSIGN = model.predict(X1) ASSIGN = 0 for i in range(200): if ASSIGN[i] == ASSIGN[i]: ASSIGN += 1 print(totalpath)",0,"['# create model\n', 'X1 = np.split(X,2)[0]\n', 'X2 = np.split(X,2)[1]\n', '\n', 'y1 = np.split(train[""label""],2)[0]\n', 'y2 = np.split(train[""label""],2)[1]\n', '\n', 'from sklearn.tree import DecisionTreeClassifier\n', 'from sklearn.tree import DecisionTreeRegressor\n', '\n', 'model = DecisionTreeClassifier()\n', '\n', 'model.fit(X2,y2) # easier to use 2 first than fix indexing\n', '\n', 'p = model.predict(X1)\n', '\n', 'total = 0\n', 'for i in range(200):\n', '    if y1[i] == p[i]:\n', '        total += 1\n', '\n', 'print(total/200) # print accuracy']"
"CHECKPOINT print('size of train data',train.shape) print('size of test data',test.shape)",0,"[""print('size of train data',train.shape)\n"", ""print('size of test data',test.shape)""]"
ASSIGN = get_cat_to_name_data(CAT_TO_NAME_PATH),0,['cat_to_names = get_cat_to_name_data(CAT_TO_NAME_PATH)']
"plt.figure(figsize=(30,30)) ASSIGN=0 for i in ['2010','2011','2012','2013','2014','2015','2016','2017','2018']: ASSIGN+=1 plt.subplot(3,3,ASSIGN) ASSIGN=data_export[data_export.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10].index ASSIGN=data_export[data_export.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10] sns.barplot(ASSIGN=ASSIGN,ASSIGN=ASSIGN) plt.title('Top 10 value of export in '+i,size=24) plt.xlabel('million US$')",1,"['plt.figure(figsize=(30,30))\n', 'j=0\n', ""for i in ['2010','2011','2012','2013','2014','2015','2016','2017','2018']:\n"", '    j+=1\n', '    plt.subplot(3,3,j)\n', ""    y=data_export[data_export.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10].index\n"", ""    x=data_export[data_export.year==int(i)].groupby('country')['value'].agg('sum').sort_values(ascending=False)[:10]\n"", '    sns.barplot(x=x,y=y)\n', ""    plt.title('Top 10 value of export in '+i,size=24)\n"", ""    plt.xlabel('million US$')""]"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
"ASSIGN=train_test_split(x,y,test_size=.1,random_state=105)",0,"['x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=105)']"
ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path') ASSIGN['wheezy-copper-turtle-magic'] = ASSIGN['wheezy-copper-turtle-magic'].astype('category') ASSIGN['wheezy-copper-turtle-magic'] = ASSIGN['wheezy-copper-turtle-magic'].astype('category'),0,"[""train = pd.read_csv('../input/train.csv')\n"", ""test = pd.read_csv('../input/test.csv')\n"", ""train['wheezy-copper-turtle-magic'] = train['wheezy-copper-turtle-magic'].astype('category')\n"", ""test['wheezy-copper-turtle-magic'] = test['wheezy-copper-turtle-magic'].astype('category')""]"
"class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.lstm = nn.LSTM(2,32,1,dropout = 0.5) self.stable_full = nn.Linear(7,16) nn.init.kaiming_normal_(self.stable_full.weight) self.BN1 = nn.BatchNorm1d(16) self.stable_dropout = nn.Dropout(0.5) self.merge_full = nn.Linear(16+13*32,64) nn.init.kaiming_normal_(self.merge_full.weight) self.BN2 = nn.BatchNorm1d(64) self.merge_dropout = nn.Dropout(0.3) self.merge_full2 = nn.Linear(64,2) nn.init.kaiming_normal_(self.merge_full2.weight) def reset_hidden(self): self.hidden = (torch.zeros(self.hidden[0].shape), torch.zeros(self.hidden[1].shape)) def forward(self, x_trend,x_stable): ASSIGN = x_trend.reshape(13,-1,2).size(1) ASSIGN = ASSIGN.reshape(13,batch_size,2) ASSIGN, self.hidden = self.lstm(ASSIGN) ASSIGN = self.stable_dropout(F.relu(self.BN1(self.stable_full(ASSIGN)))) ASSIGN = x_trend.shape ASSIGN = ASSIGN.view(b, s*h) ASSIGN = torch.cat((x_trend,x_stable),axis = 1) ASSIGN = F.relu(self.merge_full2(self.merge_dropout(F.relu(self.BN2(self.merge_full(ASSIGN)))))) return x_merge",0,"['# Define Model\n', 'class Net(nn.Module):\n', '    def __init__(self):\n', '            super(Net, self).__init__()\n', '            self.lstm = nn.LSTM(2,32,1,dropout = 0.5)\n', '            \n', '            self.stable_full = nn.Linear(7,16)\n', '            nn.init.kaiming_normal_(self.stable_full.weight)\n', '            self.BN1 = nn.BatchNorm1d(16)\n', '            self.stable_dropout = nn.Dropout(0.5)\n', '            \n', '            self.merge_full = nn.Linear(16+13*32,64)# stable:（5*16）  lstm:（13，5，32）\n', '            nn.init.kaiming_normal_(self.merge_full.weight)\n', '            self.BN2 = nn.BatchNorm1d(64)\n', '            self.merge_dropout = nn.Dropout(0.3)\n', '            self.merge_full2 = nn.Linear(64,2)\n', '            nn.init.kaiming_normal_(self.merge_full2.weight)\n', '\n', '    def reset_hidden(self):\n', '        self.hidden = (torch.zeros(self.hidden[0].shape), torch.zeros(self.hidden[1].shape))\n', '        \n', '    def forward(self, x_trend,x_stable):\n', '        batch_size = x_trend.reshape(13,-1,2).size(1)\n', '        x_trend = x_trend.reshape(13,batch_size,2)\n', '        x_trend, self.hidden = self.lstm(x_trend)\n', '        \n', '        x_stable = self.stable_dropout(F.relu(self.BN1(self.stable_full(x_stable))))\n', '        \n', '        s, b, h = x_trend.shape  #(seq, batch, hidden)\n', '        x_trend = x_trend.view(b, s*h)\n', '        x_merge = torch.cat((x_trend,x_stable),axis = 1)\n', '        x_merge = F.relu(self.merge_full2(self.merge_dropout(F.relu(self.BN2(self.merge_full(x_merge))))))\n', '        return x_merge']"
ASSIGN = '..path' ASSIGN = pd.read_csv(lowafilepath),0,"['#Importing data\n', ""lowafilepath = '../input/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n"", 'data = pd.read_csv(lowafilepath)']"
"ASSIGN= cbdr.groupby(['STATEpath','DISTRICT','Year'])['Assault on women'].sum().reset_index().sort_values(by='Assault on women',ascending=False)",0,"[""s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Assault on women'].sum().reset_index().sort_values(by='Assault on women',ascending=False)\n""]"
"plt.figure(figsize=(50,60)) ASSIGN=Basemap(width=120000,height=900000,projection=""lcc"",resolution=""l"",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77) ASSIGN.drawcountries() ASSIGN.drawmapboundary(color=' ASSIGN.drawcoastlines() ASSIGN=np.array(df_plot_top[""lng""]) ASSIGN=np.array(df_plot_top[""ASSIGN""]) ASSIGN=np.array(df_plot_top[""COUNT""]) ASSIGN=np.array(df_plot_top[""CITY""]) ASSIGN=map(lg,lat) ASSIGN=df_plot_top[""COUNT""].apply(lambda x: int(x)path) plt.scatter(ASSIGN,s=ASSIGN,marker=""o"",c=ASSIGN) for a,b ,c,d in zip(ASSIGN,ASSIGN,ASSIGN): plt.text(a,b,c,fontsize=30,color=""r"") plt.text(a+60000,b+30000,d,fontsize=30) plt.title(""TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO"",fontsize=30,color='RED')",1,"['#lets plot with the city names and restaurants count inside the map corresponding to the cities exact co-ordinates which we received from google api ,here marker color will be different as per marker size\n', '#plt.subplots(figsize=(20,50))\n', 'plt.figure(figsize=(50,60))\n', 'map=Basemap(width=120000,height=900000,projection=""lcc"",resolution=""l"",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n', 'map.drawcountries()\n', ""map.drawmapboundary(color='#f2f2f2')\n"", '\n', 'map.drawcoastlines()\n', '\n', '\n', '\n', 'lg=np.array(df_plot_top[""lng""])\n', 'lat=np.array(df_plot_top[""lat""])\n', 'pt=np.array(df_plot_top[""COUNT""])\n', 'city_name=np.array(df_plot_top[""CITY""])\n', '\n', 'x,y=map(lg,lat)\n', '\n', '#using lambda function to create different sizes of marker as per thecount \n', '\n', 'p_s=df_plot_top[""COUNT""].apply(lambda x: int(x)/2)\n', '\n', '#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is different.\n', 'plt.scatter(x,y,s=p_s,marker=""o"",c=p_s)\n', '\n', 'for a,b ,c,d in zip(x,y,city_name,pt):\n', '    #plt.text takes x position , y position ,text(city name) ,font size and color as arguments\n', '    plt.text(a,b,c,fontsize=30,color=""r"")\n', '    #plt.text takes x position , y position ,text(restaurant counts) ,font size and color as arguments, like above . but only i have changed the x and y position to make it more clean and easier to read\n', '    plt.text(a+60000,b+30000,d,fontsize=30)\n', '   \n', '    \n', '    \n', 'plt.title(""TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO"",fontsize=30,color=\'RED\')']"
"CHECKPOINT ASSIGN=pd.read_csv(""..path"") ASSIGN=pd.read_csv(""..path"") ASSIGN=pd.read_csv(""..path"") ASSIGN=pd.read_csv(""..path"") ASSIGN=pd.read_csv(""..path"") print(,len(ASSIGN)) print(,len(ASSIGN)) print(,len(ASSIGN)) print(,len(ASSIGN)) print(,len(ASSIGN))",0,"['cities=pd.read_csv(""../input/city-lines/cities.csv"")\n', 'stations=pd.read_csv(""../input/city-lines/stations.csv"")\n', 'tracks=pd.read_csv(""../input/city-lines/tracks.csv"")\n', 'lines=pd.read_csv(""../input/city-lines/lines.csv"")\n', 'track_lines=pd.read_csv(""../input/city-lines/track_lines.csv"")\n', 'print(""cities size:"",len(cities))\n', 'print(""stations size:"",len(stations))\n', 'print(""tracks size:"",len(tracks))\n', 'print(""lines size:"",len(lines))\n', 'print(""track_lines size:"",len(track_lines))']"
"def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test): ASSIGN=normalize_X(ASSIGN) ASSIGN=normalize_X(ASSIGN) ASSIGN=normalize_X(ASSIGN) ASSIGN = shuffle(ASSIGN, random_state=42) ASSIGN = shuffle(ASSIGN, random_state=42) ASSIGN = shuffle(ASSIGN, random_state=42) return X_train,X_val,X_test,y_train,y_val,y_test",0,"['def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test):\n', '    X_train=normalize_X(X_train)\n', '    X_val=normalize_X(X_val)\n', '    X_test=normalize_X(X_test)\n', '    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n', '    X_val, y_val = shuffle(X_val, y_val, random_state=42)\n', '    X_test, y_test = shuffle(X_test, y_test, random_state=42)\n', '    return X_train,X_val,X_test,y_train,y_val,y_test']"
player_attr_df.date = pd.to_datetime(player_attr_df.date),0,['player_attr_df.date = pd.to_datetime(player_attr_df.date)']
"SETUP CHECKPOINT ASSIGN = np.array([2,3,5,7,11]) print(ASSIGN) ASSIGN = [1,2,3,4,5,6,7,8,9] print(ASSIGN) print() print(np.intersect1d(ASSIGN,ASSIGN))",0,"['# 4. How to get the common items between two python numpy arrays?\n', 'import numpy as np\n', 'a1 = np.array([2,3,5,7,11])\n', 'print(a1)\n', 'a2 = [1,2,3,4,5,6,7,8,9]\n', 'print(a2)\n', 'print(""common values between two arrays are"")\n', 'print(np.intersect1d(a1,a2))']"
"def save_checkpoint(): ASSIGN = {'model': model, 'state_dict': model.state_dict(), 'optimizer' : optimizer.state_dict()} torch.save(ASSIGN, 'checkpoint1.pth')",0,"['def save_checkpoint():\n', ""    checkpoint = {'model': model,\n"", ""                  'state_dict': model.state_dict(),\n"", ""                  'optimizer' : optimizer.state_dict()}\n"", '\n', ""    torch.save(checkpoint, 'checkpoint1.pth')""]"
"CHECKPOINT ASSIGN = linear_model.Ridge(alpha=1) ASSIGN.fit(X_train,y_train) ASSIGN = model5.score(X_test,y_test) print(ASSIGN*100,'%')",0,"['model5 = linear_model.Ridge(alpha=1)\n', 'model5.fit(X_train,y_train)\n', '\n', 'accuracy5 = model5.score(X_test,y_test)\n', ""print(accuracy5*100,'%')""]"
"df_final.drop(columns=[""PAGE NO""],axis=1,inplace=True)",0,"['#header column ""PAGE NO"" is not required ,i used it while scraping the data from zomato to do some sort of validation,lets remove the column\n', 'df_final.drop(columns=[""PAGE NO""],axis=1,inplace=True)']"
"sns.catplot(x='Year', y='Kidnapping and Abduction', data=cbdr,height = 5, aspect = 4)",1,"[""sns.catplot(x='Year', y='Kidnapping and Abduction', data=cbdr,height = 5, aspect = 4)""]"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load\n"", '\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', '\n', '# Input data files are available in the read-only ""../input/"" directory\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n', '\n', 'import os\n', ""for dirname, _, filenames in os.walk('/kaggle/input'):\n"", '    for filename in filenames:\n', '        print(os.path.join(dirname, filename))\n', '\n', '# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" \n', ""# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session""]"
"ASSIGN = df1['Fare'] sns.distplot(ASSIGN, hist=True, rug=True)",1,"[""x = df1['Fare']\n"", 'sns.distplot(x, hist=True, rug=True)']"
zomato['rate'].head(),0,"[""zomato['rate'].head()""]"
"CHECKPOINT ASSIGN=pd.read_csv(""..path"") ASSIGN=pd.read_csv(""..path"") ASSIGN=pd.read_csv(""..path"") ASSIGN=pd.read_csv(""..path"") print(,len(ASSIGN)) print(,len(ASSIGN)) print(,len(ASSIGN)) print(,len(ASSIGN))",0,"['train_identity=pd.read_csv(""../input/ieee-fraud-detection/train_identity.csv"")\n', 'train_transaction=pd.read_csv(""../input/ieee-fraud-detection/train_transaction.csv"")\n', 'test_transaction=pd.read_csv(""../input/ieee-fraud-detection/test_transaction.csv"")\n', 'test_identity=pd.read_csv(""../input/ieee-fraud-detection/test_identity.csv"")\n', 'print(""train_identity_data_size: "",len(train_identity))\n', 'print(""train_transaction_data_size: "",len(train_transaction))\n', 'print(""test_transaction_data_size: "",len(test_transaction))\n', 'print(""test_identity_data_size: "",len(test_identity))']"
SETUP CHECKPOINT ASSIGN=load_model(WEIGHTS_FILE) print(),0,"['from keras.models import load_model\n', '# load model\n', 'loaded_model=load_model(WEIGHTS_FILE)\n', 'print(""Loaded model from disk"")']"
headline_by_year(2010),0,['headline_by_year(2010)']
ASSIGN=ASSIGN.dropna(subset=['value']),0,"[""data_import=data_import.dropna(subset=['value'])""]"
"ASSIGN = pd.merge(ASSIGN,temperature,on=['Country','Province','Date'],how='left')",0,"[""train = pd.merge(train,temperature,on=['Country','Province','Date'],how='left')""]"
SETUP,0,['!tar -zxvf ../input/cifar10-python/cifar-10-python.tar.gz\n']
"lr.fit(x_train,x_test) lr.score(y_train,y_test)",0,"['lr.fit(x_train,x_test)\n', 'lr.score(y_train,y_test)']"
covid['SARS-Cov-2 exam result'].value_counts(),0,"[""covid['SARS-Cov-2 exam result'].value_counts()""]"
df1.head(),0,['df1.head()']
"for df in [df_hist_trans,df_new_merchant_trans]: df['purchase_date'] = pd.to_datetime(df['purchase_date']) ASSIGN = ASSIGN.dt.year ASSIGN = ASSIGN.dt.weekofyear ASSIGN = ASSIGN.dt.month ASSIGN = ASSIGN.dt.dayofweek ASSIGN = (df.purchase_date.dt.weekday >=5).astype(int) ASSIGN = ASSIGN.dt.hour df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0}) df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)path df['month_diff'] += df['month_lag']",0,"['for df in [df_hist_trans,df_new_merchant_trans]:\n', ""    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n"", ""    df['year'] = df['purchase_date'].dt.year\n"", ""    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n"", ""    df['month'] = df['purchase_date'].dt.month\n"", ""    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n"", ""    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n"", ""    df['hour'] = df['purchase_date'].dt.hour\n"", ""    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n"", ""    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n"", '    #https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/73244\n', ""    df['month_diff'] = ((datetime.datetime.today() - df['purchase_date']).dt.days)//30\n"", ""    df['month_diff'] += df['month_lag']""]"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") print(ASSIGN.info())",0,"['#2\n', 'import pandas as pd\n', 'iris = pd.read_csv(""../input/iris-dataset/iris.data.csv"")\n', 'print(iris.info())\n']"
"odd_goals[odd_goals.home_team_goal+odd_goals.away_team_goal != odd_goals.goals_info.apply(lambda x: len(x[0])+len(x[1]))][['goals_info', 'home_team_goal', 'away_team_goal']].head(10)",0,"['#confirmation step\n', ""odd_goals[odd_goals.home_team_goal+odd_goals.away_team_goal != odd_goals.goals_info.apply(lambda x: len(x[0])+len(x[1]))][['goals_info', 'home_team_goal', 'away_team_goal']].head(10)""]"
SETUP CHECKPOINT print(os.listdir()),0,"['# This Python 3 environment comes with many helpful analytics libraries installed\n', '# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n', ""# For example, here's several helpful packages to load in \n"", '%matplotlib inline\n', 'import numpy as np # linear algebra\n', 'import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n', 'import matplotlib.pyplot as plt \n', '#from collections import Counter\n', 'import networkx as nx \n', '# Input data files are available in the ""../input/"" directory.\n', '# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n', '\n', 'import os\n', 'print(os.listdir(""../input""))\n', '\n', '# Any results you write to the current directory are saved as output.']"
"SETUP ASSIGN=folium.map.FeatureGroup() ASSIGN=data2.latitude.mean() ASSIGN=data2.longitude.mean() ASSIGN=folium.Map([Lat,Lon],zoom_start=3) ASSIGN=plugins.MarkerCluster().add_to(map1) for lat,lon,label in zip(data2.latitude,data2.longitude,data2.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN) ASSIGN.save(""COVID""+"".html"")",1,"['incidents=folium.map.FeatureGroup()\n', '\n', 'Lat=data2.latitude.mean()\n', 'Lon=data2.longitude.mean()\n', 'from folium import plugins\n', '\n', 'map1=folium.Map([Lat,Lon],zoom_start=3)\n', '\n', 'COVID_map=plugins.MarkerCluster().add_to(map1)\n', 'for lat,lon,label in zip(data2.latitude,data2.longitude,data2.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(COVID_map)\n', 'map1.add_child(COVID_map)\n', 'map1.save(""COVID""+"".html"")']"
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'fashion-mnist_train.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,"[""nRowsRead = 1000 # specify 'None' if want to read whole file\n"", '# fashion-mnist_train.csv has 60000 rows in reality, but we are only loading/previewing the first 1000 rows\n', ""df2 = pd.read_csv('../input/fashion-mnist_train.csv', delimiter=',', nrows = nRowsRead)\n"", ""df2.dataframeName = 'fashion-mnist_train.csv'\n"", 'nRow, nCol = df2.shape\n', ""print(f'There are {nRow} rows and {nCol} columns')""]"
"sns.countplot(x = 'Parch', data = df1)",1,"[""sns.countplot(x = 'Parch', data = df1)""]"
"ASSIGN = confusion_matrix(Y_test, Y_predicted) sns.heatmap(ASSIGN, annot = True, cmap=""Blues"")",1,"['cm = confusion_matrix(Y_test, Y_predicted)\n', 'sns.heatmap(cm, annot = True, cmap=""Blues"")']"
"ASSIGN=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[8000:-1] ASSIGN=34.78 ASSIGN=32.05 ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN['label']=ASSIGN.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1) ASSIGN=plugins.MarkerCluster().add_to(data_total_cars_5_map) for lat,lon,label in zip(ASSIGN.latitude,ASSIGN.longitude,ASSIGN.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN)",1,"[""data_total_cars_5=data[data.total_cars==1].drop_duplicates(subset=['latitude'])[8000:-1]\n"", 'Long=34.78\n', 'Lat=32.05\n', 'data_total_cars_5_map=folium.Map([Lat,Long],zoom_start=12)\n', ""data_total_cars_5['label']=data_total_cars_5.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n"", '\n', 'data_total_cars_5_cars_map=plugins.MarkerCluster().add_to(data_total_cars_5_map)\n', 'for lat,lon,label in zip(data_total_cars_5.latitude,data_total_cars_5.longitude,data_total_cars_5.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_5_cars_map)\n', 'data_total_cars_5_map.add_child(data_total_cars_5_cars_map)\n', '\n']"
"CHECKPOINT for i, col in enumerate(match_df.columns): print(i, col)",0,"['#columns names\n', 'for i, col in enumerate(match_df.columns):\n', '    print(i, col)']"
ASSIGN = final['Selling_Price'],0,"[""y = final['Selling_Price']""]"
data_2019.Statistic.unique(),0,['data_2019.Statistic.unique()']
plt.boxplot(ht) plt.title('Height') plt.xlabel('X-axis') plt.ylabel('Y-axis') plt.show(),1,"['plt.boxplot(ht)\n', ""plt.title('Height')\n"", ""plt.xlabel('X-axis')\n"", ""plt.ylabel('Y-axis')\n"", 'plt.show()']"
"ASSIGN = [overall_data, suicide_data, homicide_data] ASSIGN = ['Overall', 'Suicide', 'Homicide'] ASSIGN = ['year', 'month', 'intent', 'sex', 'age', 'race', 'place', 'education'] for i, d in enumerate(ASSIGN): for col in ASSIGN: ASSIGN == 'age' or col == 'month': d[col].value_counts().sort_index().plot(kind = 'line') ASSIGN == 'year' or col == 'education': d[col].value_counts().sort_index().plot(kind = 'bar') else: d[col].value_counts().plot(kind = 'bar') plt.title(ASSIGN[i] + ': ' + col) plt.show()",1,"['data_sources = [overall_data, suicide_data, homicide_data]\n', ""titles = ['Overall', 'Suicide', 'Homicide']\n"", '\n', ""interested_columns = ['year', 'month', 'intent', 'sex', 'age', 'race', 'place', 'education']\n"", 'for i, d in enumerate(data_sources):\n', '    for col in interested_columns:\n', ""        if col == 'age' or col == 'month':\n"", ""            d[col].value_counts().sort_index().plot(kind = 'line')\n"", ""        elif col == 'year' or col == 'education':\n"", ""            d[col].value_counts().sort_index().plot(kind = 'bar')\n"", '        else:\n', ""            d[col].value_counts().plot(kind = 'bar')\n"", ""        plt.title(titles[i] + ': ' + col)\n"", '        plt.show()']"
"ASSIGN = go.Bar(x=temp['breed'], y=temp['instances']) ASSIGN = [trace] ASSIGN = go.Layout( ASSIGN='Breed Counts', ASSIGN=False, ASSIGN=5000, ASSIGN=500, ASSIGN=dict( ASSIGN=100, ASSIGN=100, ASSIGN=100, ASSIGN=100 ) ) ASSIGN = go.Figure(data=data, layout=layout) py.iplot(ASSIGN)",1,"[""trace = go.Bar(x=temp['breed'], y=temp['instances'])\n"", 'data = [trace]\n', 'layout = go.Layout(\n', ""        title='Breed Counts',\n"", '        autosize=False,\n', '        width=5000,\n', '        height=500,\n', '        margin=dict(\n', '            l=100,\n', '            r=100,\n', '            b=100,\n', '            t=100\n', '        )\n', '    )\n', 'fig = go.Figure(data=data, layout=layout)\n', 'py.iplot(fig)']"
ASSIGN= gs_1.predict(X_test),0,"['#predict based on gs\n', 'y_pred_gs1= gs_1.predict(X_test)']"
learn.recorder.plot_losses(),1,['learn.recorder.plot_losses()']
"submissions.to_csv(""DR.csv"", index=False, header=True)",0,"['submissions.to_csv(""DR.csv"", index=False, header=True)']"
"sns.catplot(x='Owner', y ='Selling_Price', kind = 'violin',data= data)",1,"[""sns.catplot(x='Owner', y ='Selling_Price', kind = 'violin',data= data)""]"
"SETUP r2_score(y_test,y_predict)",0,"['from sklearn.metrics import r2_score\n', 'r2_score(y_test,y_predict)']"
"ASSIGN=data[data.carsList.apply(lambda x:re.match('37',x)!=None)] ASSIGN=data_37[data.date==data_37.date.unique()[0]].drop_duplicates(subset=['longitude']) for i in range(1,len(ASSIGN.date.unique())): ASSIGN=data_37[data.date==data_37.date.unique()[i]].drop_duplicates(subset=['longitude']) ASSIGN=pd.concat([ASSIGN,data_37_concate],axis=0) ASSIGN=34.78 ASSIGN=32.05 ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(data_37_map) for lat,lon,label in zip(ASSIGN.latitude,ASSIGN.longitude,ASSIGN.timestamp): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN)",1,"[""data_37=data[data.carsList.apply(lambda x:re.match('37',x)!=None)]\n"", ""data_37_new=data_37[data.date==data_37.date.unique()[0]].drop_duplicates(subset=['longitude'])\n"", 'for i in range(1,len(data_37.date.unique())):\n', ""    data_37_concate=data_37[data.date==data_37.date.unique()[i]].drop_duplicates(subset=['longitude'])\n"", '    data_37_new=pd.concat([data_37_new,data_37_concate],axis=0)\n', '\n', 'Long=34.78\n', 'Lat=32.05\n', 'data_37_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'data_37_cars_map=plugins.MarkerCluster().add_to(data_37_map)\n', 'for lat,lon,label in zip(data_37_new.latitude,data_37_new.longitude,data_37_new.timestamp):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_37_cars_map)\n', 'data_37_map.add_child(data_37_cars_map)\n', '\n']"
SETUP,0,['from sklearn.metrics import r2_score']
"ASSIGN = len(dataset) ASSIGN = 0.2 ASSIGN = random_split(dataset, [size - int(size * val_split), int(size * val_split)]) ASSIGN = DataLoader(trainset, batch_size=64, shuffle=True) ASSIGN = DataLoader(valset, batch_size=64, shuffle=True) ASSIGN = DataLoader(test_dataset, batch_size=64, shuffle=False)",0,"['size = len(dataset)\n', 'val_split = 0.2\n', 'trainset, valset = random_split(dataset, [size - int(size * val_split), int(size * val_split)])\n', '\n', 'train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n', 'val_loader = DataLoader(valset, batch_size=64, shuffle=True)\n', 'test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)']"
"CHECKPOINT for i, col in enumerate(match_df.columns): print(i, col)",0,"['#columns names\n', 'for i, col in enumerate(match_df.columns):\n', '    print(i, col)']"
SETUP SETUP,0,"['# 在kaggle中，将导入的数据，从只读目录复制到工作区可写目录下\n', '!cp /kaggle/input/* {path}/\n', '# 安装tree命令\n', '!apt get tree']"
"def superimpose(img, cam): """"""superimpose original image and cam heatmap"""""" ASSIGN = cv2.resize(cam, (img.shape[1], img.shape[0])) ASSIGN = np.uint8(255 * ASSIGN) ASSIGN = cv2.applyColorMap(ASSIGN, cv2.COLORMAP_JET) ASSIGN = heatmap * .45 + img * 1.2 ASSIGN = np.minimum(ASSIGN, 255.0).astype(np.uint8) ASSIGN = cv2.cvtColor(ASSIGN, cv2.COLOR_BGR2RGB) return img, heatmap, superimposed_img",1,"['def superimpose(img, cam):\n', '    """"""superimpose original image and cam heatmap""""""\n', '    \n', '    heatmap = cv2.resize(cam, (img.shape[1], img.shape[0]))\n', '    heatmap = np.uint8(255 * heatmap)\n', '    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n', '\n', '    superimposed_img = heatmap * .45 + img * 1.2\n', '    superimposed_img = np.minimum(superimposed_img, 255.0).astype(np.uint8)  # scale 0 to 255  \n', '    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n', '    \n', '    return img, heatmap, superimposed_img']"
"dataset.columns = ['GRE Score','TOEFL Score','University Rating','SOP','LOR','CGPA','Research','Chance of Admit']",0,"[""dataset.columns = ['GRE Score','TOEFL Score','University Rating','SOP','LOR','CGPA','Research','Chance of Admit']\n""]"
CHECKPOINT smote_df.shape,0,['smote_df.shape']
"my_submission.to_csv('my_sub.csv', index=False)",0,"[""my_submission.to_csv('my_sub.csv', index=False)""]"
ASSIGN=pd.read_csv('..path') ASSIGN.head(),0,"[""data=pd.read_csv('../input/denver-crime-data/crime.csv')\n"", 'data.head()']"
"ASSIGN=plt.subplots(figsize=(10,6)) stations.groupby(['opening','country'])['stations_name'].agg('count').unstack().plot(ax=ax) plt.legend(loc=0, bbox_to_anchor=(1.05,1.1)) plt.xlabel('') plt.ylabel('stations') plt.title(""The number of opening stations by year (all country)"",size=18)",1,"['fig,ax=plt.subplots(figsize=(10,6))\n', ""stations.groupby(['opening','country'])['stations_name'].agg('count').unstack().plot(ax=ax)\n"", 'plt.legend(loc=0, bbox_to_anchor=(1.05,1.1))\n', ""plt.xlabel('')\n"", ""plt.ylabel('stations')\n"", 'plt.title(""The number of opening stations by year (all country)"",size=18)']"
ASSIGN = 128 ASSIGN = 28 ASSIGN = 10000,0,"['batch_size = 128\n', 'img_size = 28 #224\n', 'trainset_size = 10000']"
data2.loc[data2.serum_creatinine.isnull()],0,['data2.loc[data2.serum_creatinine.isnull()]']
"learn.fit(1,lr=1e-3)",0,"['learn.fit(1,lr=1e-3)']"
final.head(),0,['final.head()']
ASSIGN = ClassificationInterpretation.from_learner(learn) ASSIGN = interp.top_losses() len(data.valid_ds)==len(losses)==len(idxs),0,"['interp = ClassificationInterpretation.from_learner(learn)\n', 'losses,idxs = interp.top_losses()\n', 'len(data.valid_ds)==len(losses)==len(idxs)']"
"data_features.loc[(data_features['GarageYrBlt'].isnull() & data_features['GarageType'].notnull()) ,['GarageYrBlt','GarageType','GarageCond','GarageFinish','GarageQual']] ASSIGN = ['GarageYrBlt','GarageType','GarageCond','GarageFinish','GarageQual'] ASSIGN = (data_features['GarageYrBlt'].isnull() & data_features['GarageType'].notnull()) for col in ASSIGN: data_features.loc[ASSIGN,col] = data_features[(data_features['GarageType'] == 'Detchd')][col].mode()[0]",0,"[""data_features.loc[(data_features['GarageYrBlt'].isnull() & data_features['GarageType'].notnull())\n"", ""                  ,['GarageYrBlt','GarageType','GarageCond','GarageFinish','GarageQual']]\n"", '#so we use the value of GarageType to fill the other four variables\n', ""garage_var = ['GarageYrBlt','GarageType','GarageCond','GarageFinish','GarageQual']\n"", ""condition1 = (data_features['GarageYrBlt'].isnull() & data_features['GarageType'].notnull())\n"", 'for col in garage_var:\n', ""    data_features.loc[condition1,col] = data_features[(data_features['GarageType'] == 'Detchd')][col].mode()[0]\n"", '#Note that we still have 156 missing values to fill for all 5 variables']"
"ASSIGN = pd.concat((df_train, df_test), axis=0, ignore_index=True) ASSIGN.head(n=5)",0,"['df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n', 'df_all.head(n=5)']"
"ASSIGN = Sequential() ASSIGN.add(Convolution2D(20, 5, 5, input_shape = (28, 28, 1), activation=""relu"")) ASSIGN.add(MaxPooling2D(pool_size=(2,2))) ASSIGN.add(Dropout(0.5)) ASSIGN.add(Convolution2D(40, 5, 5, activation=""relu"")) ASSIGN.add(MaxPooling2D(pool_size=(2,2))) ASSIGN.add(Dropout(0.5)) ASSIGN.add(Flatten()) ASSIGN.add(Dense(100, activation = ""relu"")) ASSIGN.add(Dropout(0.5)) ASSIGN.add(Dense(100, activation = ""relu"")) ASSIGN.add(Dense(10, activation=""softmax""))",0,"['model = Sequential()\n', '\n', 'model.add(Convolution2D(20, 5, 5, input_shape = (28, 28, 1), activation=""relu""))\n', 'model.add(MaxPooling2D(pool_size=(2,2)))\n', 'model.add(Dropout(0.5))\n', '\n', 'model.add(Convolution2D(40, 5, 5, activation=""relu""))\n', 'model.add(MaxPooling2D(pool_size=(2,2)))\n', 'model.add(Dropout(0.5))\n', '\n', 'model.add(Flatten())\n', 'model.add(Dense(100, activation = ""relu""))\n', 'model.add(Dropout(0.5))\n', 'model.add(Dense(100, activation = ""relu""))\n', 'model.add(Dense(10, activation=""softmax""))\n', '\n']"
"ASSIGN = WordCloud(background_color=""black"",max_words=200,max_font_size=40,random_state=10).generate(str(headline_text_new)) plt.figure(figsize=(20,15)) plt.imshow(ASSIGN, interpolation='bilinear') plt.axis(""off"") plt.show()",1,"['wordcloud = WordCloud(background_color=""black"",max_words=200,max_font_size=40,random_state=10).generate(str(headline_text_new))\n', '\n', 'plt.figure(figsize=(20,15))\n', ""plt.imshow(wordcloud, interpolation='bilinear')\n"", 'plt.axis(""off"")\n', 'plt.show()']"
"SETUP ASSIGN = plt.figure(figsize = (12,8)) plt.plot(np.cumsum(pca.explained_variance_ratio_)) plt.xlabel('number of components') plt.ylabel('cumulative explained variance') plt.show()",1,"['#Making the screeplot - plotting the cumulative variance against the number of components\n', '%matplotlib inline\n', 'fig = plt.figure(figsize = (12,8))\n', 'plt.plot(np.cumsum(pca.explained_variance_ratio_))\n', ""plt.xlabel('number of components')\n"", ""plt.ylabel('cumulative explained variance')\n"", 'plt.show()']"
"learn.unfreeze() learn.fit_one_cycle(2, max_lr=slice(1e-6,3e-4)) ASSIGN = ClassificationInterpretation.from_learner(learn) ASSIGN.plot_top_losses(9, figsize=(12,8)) ASSIGN.plot_top_losses(9, figsize=(12,8),heatmap=False) ASSIGN.plot_confusion_matrix()",1,"['learn.unfreeze()\n', 'learn.fit_one_cycle(2, max_lr=slice(1e-6,3e-4))\n', 'interp = ClassificationInterpretation.from_learner(learn)\n', 'interp.plot_top_losses(9, figsize=(12,8))\n', 'interp.plot_top_losses(9, figsize=(12,8),heatmap=False)\n', 'interp.plot_confusion_matrix()']"
ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path'),0,"[""train_df = pd.read_csv('../input/nlp-hack/train.csv')\n"", ""test_df = pd.read_csv('../input/nlp-hack/test.csv')""]"
"ASSIGN=plt.subplots(1,2,figsize=(15,8)) sns.barplot(x=data_risk2['Facility Type'].value_counts()[:10],y=data_risk2['Facility Type'].value_counts()[:10].index,ax=ax[0]) ax[0].set_title(""Top 10 Facility Type by the counts of risk 2 "",size=20) ax[0].set_xlabel('counts',size=18) ASSIGN=data_risk2.groupby(['Facility Type'])['Inspection ID'].agg('ASSIGN').sort_values(ascending=False) ASSIGN=list(data_risk2.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10]) ASSIGN=list(count[:10]) ASSIGN.append(ASSIGN.agg(sum)-ASSIGN[:10].agg('sum')) ASSIGN.append('Other') ASSIGN=pd.DataFrame({""group"":groups,""counts"":counts}) ASSIGN=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum') ASSIGN.plot(kind='pie', y='ASSIGN', labels=ASSIGN,colors=ASSIGN,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1]) ax[1].set_ylabel('') ax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))",1,"['fig,ax=plt.subplots(1,2,figsize=(15,8))\n', ""sns.barplot(x=data_risk2['Facility Type'].value_counts()[:10],y=data_risk2['Facility Type'].value_counts()[:10].index,ax=ax[0])\n"", 'ax[0].set_title(""Top 10 Facility Type by the counts of risk 2 "",size=20)\n', ""ax[0].set_xlabel('counts',size=18)\n"", '\n', '\n', ""count=data_risk2.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False)\n"", ""groups=list(data_risk2.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10])\n"", 'counts=list(count[:10])\n', ""counts.append(count.agg(sum)-count[:10].agg('sum'))\n"", ""groups.append('Other')\n"", 'type_dict=pd.DataFrame({""group"":groups,""counts"":counts})\n', ""clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n"", ""type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\n"", ""ax[1].set_ylabel('')\n"", 'ax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))']"
ASSIGN = tf1.copy() tf1.describe(),0,"['tf = tf1.copy()\n', 'tf1.describe()']"
"ASSIGN = TfidfVectorizer() ASSIGN = vect.build_tokenizer() ASSIGN = set(stopwords.words(""english"")) ASSIGN = df.target.to_numpy() ASSIGN=TfidfVectorizer(tokenizer = sklearn_tokenizer,stop_words='english',ngram_range=(1, 1), norm='l2') ASSIGN=SGDClassifier(alpha=0.0001,epsilon=0.1, eta0=0.0, ASSIGN=0.1, learning_rate='optimal', ASSIGN='modified_huber', penalty='l2',class_weight = 'balanced') ASSIGN = Pipeline([('vect',vect),('clf',clf )])",0,"['vect = TfidfVectorizer()\n', 'sklearn_tokenizer = vect.build_tokenizer()\n', 'stop_words = set(stopwords.words(""english""))\n', 'y = df.target.to_numpy()\n', ""vect=TfidfVectorizer(tokenizer = sklearn_tokenizer,stop_words='english',ngram_range=(1, 1), norm='l2')\n"", 'clf=SGDClassifier(alpha=0.0001,epsilon=0.1, eta0=0.0,\n', ""                               l1_ratio=0.1, learning_rate='optimal',\n"", ""                               loss='modified_huber', penalty='l2',class_weight =  'balanced')\n"", ""pp = Pipeline([('vect',vect),('clf',clf )])""]"
ASSIGN = pd.read_csv('path'),0,"[""original_train = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/train.csv')""]"
"new_train.groupby(['landmark_id']).agg('count').sort_values(by='id',ascending=False).style.background_gradient(cmap='Blues')",0,"[""new_train.groupby(['landmark_id']).agg('count').sort_values(by='id',ascending=False).style.background_gradient(cmap='Blues')\n""]"
"ASSIGN = titanic_df.drop(""Survived"",axis=1) ASSIGN = titanic_df[""Survived""] ASSIGN = test_df.drop(""PassengerId"",axis=1).copy()",0,"['# define training and testing sets\n', '\n', 'X_train = titanic_df.drop(""Survived"",axis=1)\n', 'Y_train = titanic_df[""Survived""]\n', 'X_test  = test_df.drop(""PassengerId"",axis=1).copy()']"
"def dsc(y_true, y_pred): ASSIGN = 1. ASSIGN = tf.keras.layers.Flatten()(y_true) ASSIGN = tf.keras.layers.Flatten()(y_pred) ASSIGN = reduce_sum(y_true_f * y_pred_f) ASSIGN = (2. * intersection + smooth) path(reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth) return score def dice_loss(y_true, y_pred): ASSIGN = 1 - dsc(y_true, y_pred) return loss def bce_dice_loss(y_true, y_pred): ASSIGN = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred) return loss def tversky(y_true, y_pred, ASSIGN=1e-6): ASSIGN = tf.keras.layers.Flatten()(y_true) ASSIGN = tf.keras.layers.Flatten()(y_pred) ASSIGN = tf.reduce_sum(y_true_pos * y_pred_pos) ASSIGN = tf.reduce_sum(y_true_pos * (1-y_pred_pos)) ASSIGN = tf.reduce_sum((1-y_true_pos)*y_pred_pos) ASSIGN = 0.7 return (ASSIGN + ASSIGN)path(ASSIGN + ASSIGN*ASSIGN + (1-ASSIGN)*ASSIGN + ASSIGN) def tversky_loss(y_true, y_pred): return 1 - tversky(y_true,y_pred) def focal_tversky_loss(y_true,y_pred): ASSIGN = tversky(y_true, y_pred) ASSIGN = 0.75 return tf.keras.backend.pow((1-ASSIGN), ASSIGN)",0,"['def dsc(y_true, y_pred):\n', '    smooth = 1.\n', '    y_true_f = tf.keras.layers.Flatten()(y_true)\n', '    y_pred_f = tf.keras.layers.Flatten()(y_pred)\n', '    intersection = reduce_sum(y_true_f * y_pred_f)\n', '    score = (2. * intersection + smooth) / (reduce_sum(y_true_f) + reduce_sum(y_pred_f) + smooth)\n', '    return score\n', '\n', 'def dice_loss(y_true, y_pred):\n', '    loss = 1 - dsc(y_true, y_pred)\n', '    return loss\n', '\n', 'def bce_dice_loss(y_true, y_pred):\n', '    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n', '    return loss\n', 'def tversky(y_true, y_pred, smooth=1e-6):\n', '    y_true_pos = tf.keras.layers.Flatten()(y_true)\n', '    y_pred_pos = tf.keras.layers.Flatten()(y_pred)\n', '    true_pos = tf.reduce_sum(y_true_pos * y_pred_pos)\n', '    false_neg = tf.reduce_sum(y_true_pos * (1-y_pred_pos))\n', '    false_pos = tf.reduce_sum((1-y_true_pos)*y_pred_pos)\n', '    alpha = 0.7\n', '    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n', '\n', 'def tversky_loss(y_true, y_pred):\n', '    return 1 - tversky(y_true,y_pred)\n', '\n', 'def focal_tversky_loss(y_true,y_pred):\n', '    pt_1 = tversky(y_true, y_pred)\n', '    gamma = 0.75\n', '    return tf.keras.backend.pow((1-pt_1), gamma)']"
"ASSIGN=train_test_split(x,y,test_size=.1,random_state=353)",0,"['x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)']"
"ASSIGN=plt.subplots(9,2,figsize=(20,55)) for i in range(9): ASSIGN=data_export[data_export.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False) ASSIGN=list(data_export[data_export.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False).index[:10]) ASSIGN=list(count[:10]) ASSIGN.append(ASSIGN.agg(sum)-ASSIGN[:10].agg('sum')) ASSIGN.append('Other') ASSIGN=pd.DataFrame({""group"":groups,""counts"":counts}) ASSIGN=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum') ASSIGN.plot(kind='pie', y='ASSIGN', labels=ASSIGN,colors=ASSIGN,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[i,0]) ax[i,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1)) ax[i,0].set_title(""Top 10 export of commodity in ""+str(i+2010)) ax[i,0].set_ylabel('') ASSIGN=data_import[data_import.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False) ASSIGN=list(data_import[data_import.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False).index[:10]) ASSIGN=list(count[:10]) ASSIGN.append(ASSIGN.agg(sum)-ASSIGN[:10].agg('sum')) ASSIGN.append('Other') ASSIGN=pd.DataFrame({""group"":groups,""counts"":counts}) ASSIGN=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum') ASSIGN = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[i,1]) ax[i,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1)) ax[i,1].set_title(""Top 10 import of commodity in ""+str(i+2010)) ax[i,1].set_ylabel('')",1,"['fig,ax=plt.subplots(9,2,figsize=(20,55))\n', 'for i in range(9):\n', ""    count=data_export[data_export.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False)\n"", ""    groups=list(data_export[data_export.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False).index[:10])\n"", '    counts=list(count[:10])\n', ""    counts.append(count.agg(sum)-count[:10].agg('sum'))\n"", ""    groups.append('Other')\n"", '    type_dict=pd.DataFrame({""group"":groups,""counts"":counts})\n', ""    clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n"", ""    type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[i,0])\n"", '    ax[i,0].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n', '    ax[i,0].set_title(""Top 10 export of commodity in ""+str(i+2010))\n', ""    ax[i,0].set_ylabel('')\n"", '\n', ""    count=data_import[data_import.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False)\n"", ""    groups=list(data_import[data_import.year==i+2010].groupby(['HSCode'])['value'].agg('sum').sort_values(ascending=False).index[:10])\n"", '    counts=list(count[:10])\n', ""    counts.append(count.agg(sum)-count[:10].agg('sum'))\n"", ""    groups.append('Other')\n"", '    type_dict=pd.DataFrame({""group"":groups,""counts"":counts})\n', ""    clr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\n"", ""    qx = type_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[i,1])\n"", '    ax[i,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.10,1))\n', '    ax[i,1].set_title(""Top 10 import of commodity in ""+str(i+2010))\n', ""    ax[i,1].set_ylabel('')""]"
"ASSIGN = 6 for idx in range(ASSIGN): ASSIGN = plt.subplots(1, 2, figsize=(20,20)) ASSIGN = imread(PATH + 'train_imagespath' + train['ImageId'].iloc[idx] + '.jpg') axes[0].imshow(ASSIGN) ASSIGN = visualize(img, str2coords(train['PredictionString'].iloc[idx])) axes[1].imshow(ASSIGN) plt.show()",1,"['n_rows = 6\n', '\n', 'for idx in range(n_rows):\n', '    fig, axes = plt.subplots(1, 2, figsize=(20,20))\n', ""    img = imread(PATH + 'train_images/' + train['ImageId'].iloc[idx] + '.jpg')\n"", '    axes[0].imshow(img)\n', ""    img_vis = visualize(img, str2coords(train['PredictionString'].iloc[idx]))\n"", '    axes[1].imshow(img_vis)\n', '    plt.show()']"
CHECKPOINT final_list,0,['final_list']
"data['REPORTED_DATE']=data.REPORTED_DATE.apply(lambda x:datetime.datetime.strptime(x,'%mpath%dpath%Y %I:%M:%S %p')) SLICE=data.REPORTED_DATE.apply(lambda x:x.strftime('%Y')) SLICE=data.REPORTED_DATE.apply(lambda x:x.strftime('%m')) SLICE=data.REPORTED_DATE.apply(lambda x:x.strftime('%H')) data.head()",0,"[""data['REPORTED_DATE']=data.REPORTED_DATE.apply(lambda x:datetime.datetime.strptime(x,'%m/%d/%Y %I:%M:%S %p'))\n"", ""data['year']=data.REPORTED_DATE.apply(lambda x:x.strftime('%Y'))\n"", ""data['month']=data.REPORTED_DATE.apply(lambda x:x.strftime('%m'))\n"", ""data['hour']=data.REPORTED_DATE.apply(lambda x:x.strftime('%H'))\n"", 'data.head()']"
"CHECKPOINT ASSIGN=np.array(lst) ASSIGN=np.array(y) print(ASSIGN.shape) print(ASSIGN.shape) np.save(""CAT_DOG_X_test"",ASSIGN) np.save(""CAT_DOG_Y_test"",ASSIGN)",0,"['X_test=np.array(lst)\n', 'Y_test=np.array(y)\n', 'print(X_test.shape)\n', 'print(Y_test.shape)\n', 'np.save(""CAT_DOG_X_test"",X_test)\n', 'np.save(""CAT_DOG_Y_test"",Y_test)']"
CHECKPOINT damage_x,0,['damage_x']
"ASSIGN = [] for sample_index in sample_index_list: ASSIGN = {} ASSIGN = ASSIGN ASSIGN = ASSIGN ASSIGN = ASSIGN ASSIGN.append(ASSIGN) ASSIGN = get_img_coords(coords, input_type=list, output_z=True) ASSIGN = np.vstack([points_df['x'][sample_index_list], points_df['y'][sample_index_list], points_df['z'][sample_index_list], ASSIGN]) ASSIGN = np.corrcoef(v) ASSIGN = plt.subplots(figsize=(7, 7)) ASSIGN = ax.imshow(CM) ax.set_xticks(np.arange(6)) ax.set_yticks(np.arange(6)) ax.set_xticklabels(['x', 'y', 'z', 'img_x', 'img_y', 'img_z']) ax.set_yticklabels(['x', 'y', 'z', 'img_x', 'img_y', 'img_z']) for i in range(6): for j in range(6): ASSIGN = ax.ASSIGN(j, i, round(CM[i, j], 2), ASSIGN=""center"", va=""center"", color=""w"") fig.tight_layout() plt.show()",1,"['coords = []\n', 'for sample_index in sample_index_list:\n', '    coord = {}\n', ""    coord['x'] = points_df['x'][sample_index] \n"", ""    coord['y'] = points_df['y'][sample_index] \n"", ""    coord['z'] = points_df['z'][sample_index]\n"", '    coords.append(coord)\n', 'img_x_list, img_y_list, img_z_list = get_img_coords(coords, input_type=list, output_z=True)\n', '\n', ""v = np.vstack([points_df['x'][sample_index_list], points_df['y'][sample_index_list], \n"", ""               points_df['z'][sample_index_list], img_x_list, img_y_list, img_z_list])\n"", 'CM = np.corrcoef(v)\n', '\n', 'fig, ax = plt.subplots(figsize=(7, 7))\n', 'im = ax.imshow(CM)\n', 'ax.set_xticks(np.arange(6))\n', 'ax.set_yticks(np.arange(6))\n', ""ax.set_xticklabels(['x', 'y', 'z', 'img_x', 'img_y', 'img_z'])\n"", ""ax.set_yticklabels(['x', 'y', 'z', 'img_x', 'img_y', 'img_z'])\n"", 'for i in range(6):\n', '    for j in range(6):\n', '        text = ax.text(j, i, round(CM[i, j], 2),\n', '                       ha=""center"", va=""center"", color=""w"")\n', 'fig.tight_layout()\n', 'plt.show()']"
ASSIGN = train_data.ASSIGN(),0,['corr = train_data.corr()\n']
"CHECKPOINT ASSIGN = go.Figure(data=[ go.Line(name='Confirmed', x=formated_gdf[formated_gdf['Countrypath']=='Mainland China']['date'], y=formated_gdf[formated_gdf['Countrypath']=='Mainland China']['Confirmed']), go.Line(name='Deaths', x=formated_gdf[formated_gdf['Countrypath']=='Mainland China']['date'], y=formated_gdf[formated_gdf['Countrypath']=='Mainland China']['Deaths']), go.Line(name='Recovered', x=formated_gdf[formated_gdf['Countrypath']=='Mainland China']['date'], y=formated_gdf[formated_gdf['Countrypath']=='Mainland China']['Recovered']), ]) ASSIGN.update_layout( ASSIGN=""Number of Confirmed,Recovered,death in china for each day"", ASSIGN=""date"", ASSIGN=""People"", ASSIGN=dict( ASSIGN=""Courier New, monospace"", ASSIGN=18, ASSIGN=""#7f7f7f"" )) fig",1,"['fig = go.Figure(data=[\n', ""    go.Line(name='Confirmed', x=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['Confirmed']),\n"", ""    go.Line(name='Deaths', x=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['Deaths']),\n"", ""    go.Line(name='Recovered', x=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['date'], y=formated_gdf[formated_gdf['Country/Region']=='Mainland China']['Recovered']),\n"", '])\n', '\n', 'fig.update_layout(\n', '    title=""Number of Confirmed,Recovered,death in china for each day"",\n', '    xaxis_title=""date"",\n', '    yaxis_title=""People"",\n', '    font=dict(\n', '        family=""Courier New, monospace"",\n', '        size=18,\n', '        color=""#7f7f7f""\n', '    ))\n', 'fig']"
for c in cols: match_bets_df[c] = match_bets_df[c].apply(lambda x: 1path),0,"['for c in cols:\n', '    match_bets_df[c] = match_bets_df[c].apply(lambda x: 1/x)']"
ASSIGN=pd.read_csv('..path') ASSIGN.head(),0,"[""data2=pd.read_csv('../input/novel-corona-virus-2019-dataset/COVID19_open_line_list.csv')\n"", 'data2.head()']"
"ASSIGN = df.iloc[199523:].index ASSIGN = df.iloc[159619:199522].index idx_val, idx_test",0,"['# split by index\n', 'idx_test = df.iloc[199523:].index # last N rows\n', 'idx_val  = df.iloc[159619:199522].index # last 20% of train rows\n', 'idx_val, idx_test']"
"for c1, c2 in train_df.dtypes.reset_index().values: ASSIGN=='O': train_df[c1] = train_df[c1].map(lambda x: str(x).lower()) test_df[c1] = test_df[c1].map(lambda x: str(x).lower())",0,"['for c1, c2 in train_df.dtypes.reset_index().values:\n', ""    if c2=='O':\n"", '        train_df[c1] = train_df[c1].map(lambda x: str(x).lower())\n', '        test_df[c1] = test_df[c1].map(lambda x: str(x).lower())']"
CHECKPOINT final.shape,0,['final.shape']
"cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']] = cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']].fillna(cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']].median())",0,"['# clean with median on numerical\n', ""cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']] = cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']].fillna(cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']].median())""]"
country_df.info(),0,['country_df.info()']
"ASSIGN = pd.merge(forecastC, forecastDeath, how='inner')",0,"[""submission = pd.merge(forecastC, forecastDeath, how='inner')""]"
"plt.figure(figsize=(10,10)) sns.barplot(x=data.isnull().sum().sort_values(ascending=False),y=data.isnull().sum().sort_values(ascending=False).index) plt.title(""counts of missing value"",size=20)",1,"['plt.figure(figsize=(10,10))\n', 'sns.barplot(x=data.isnull().sum().sort_values(ascending=False),y=data.isnull().sum().sort_values(ascending=False).index)\n', 'plt.title(""counts of missing value"",size=20)\n']"
gs.best_params_,0,['gs.best_params_']
"data.classes, data.c, len(data.train_ds), len(data.valid_ds)",0,"['data.classes, data.c, len(data.train_ds), len(data.valid_ds)']"
ASSIGN = scaler.transform(ASSIGN) pd.DataFrame(ASSIGN).head(),0,"['holdout = scaler.transform(holdout)\n', '\n', 'pd.DataFrame(holdout).head()']"
"ASSIGN = ['Calories', 'Total Fat', 'Cholesterol','Sodium', 'Sugars', 'Carbohydrates'] for m in ASSIGN: ASSIGN = sns.swarmplot(x=""Category"", y=m, data=menu) plt.setp(ASSIGN.get_xticklabels(), rotation=45) plt.title(m) plt.show()",1,"[""measures = ['Calories', 'Total Fat', 'Cholesterol','Sodium', 'Sugars', 'Carbohydrates']\n"", '\n', 'for m in measures:   \n', '    plot = sns.swarmplot(x=""Category"", y=m, data=menu)\n', '    plt.setp(plot.get_xticklabels(), rotation=45)\n', '    plt.title(m)\n', '    plt.show()']"
"ASSIGN=34.78 ASSIGN=32.05 ASSIGN=folium.Map([Lat,Long],zoom_start=12) data_total_cars_6['label']=data_total_cars_6.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1) ASSIGN=plugins.MarkerCluster().add_to(data_total_cars_6_map) for lat,lon,label in zip(data_total_cars_6.latitude,data_total_cars_6.longitude,data_total_cars_6.label): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN)",1,"['Long=34.78\n', 'Lat=32.05\n', 'data_total_cars_6_map=folium.Map([Lat,Long],zoom_start=12)\n', ""data_total_cars_6['label']=data_total_cars_6.apply(lambda x: (x['timestamp'],'car:'+str(x['carsList'])),axis=1)\n"", '\n', 'data_total_cars_6_cars_map=plugins.MarkerCluster().add_to(data_total_cars_6_map)\n', 'for lat,lon,label in zip(data_total_cars_6.latitude,data_total_cars_6.longitude,data_total_cars_6.label):\n', '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(data_total_cars_6_cars_map)\n', 'data_total_cars_6_map.add_child(data_total_cars_6_cars_map)\n', '\n']"
SETUP,0,"['import numpy as np\n', 'import pandas as pd \n', 'import re #library to clean data\n', 'import nltk #Natural Language tool kit\n', 'import matplotlib.pyplot as plt\n', 'import seaborn as sns \n', 'import os \n', 'import datetime\n', 'from nltk.corpus import stopwords #to remove stopword\n', 'from nltk.stem.porter import PorterStemmer \n', 'from PIL import Image\n', 'from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator']"
"ASSIGN = pd.concat([cbd,cbdr]).drop_duplicates(keep = False)",0,"['total1 = pd.concat([cbd,cbdr]).drop_duplicates(keep = False)']"
X.isnull().sum().sort_values(ascending = False),0,['X.isnull().sum().sort_values(ascending = False)']
ForecastId = X_test.ForecastId,0,['ForecastId = X_test.ForecastId']
"ASSIGN = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'} ASSIGN = ['gmail', 'net', 'edu'] for c in ['P_emaildomain', 'R_emaildomain']: train_df[c + '_bin'] = train_df[c].map(ASSIGN) test_df[c + '_bin'] = test_df[c].map(ASSIGN) train_df[c + '_suffix'] = train_df[c].map(lambda x: str(x).split('.')[-1]) test_df[c + '_suffix'] = test_df[c].map(lambda x: str(x).split('.')[-1]) train_df[c + '_suffix'] = train_df[c + '_suffix'].map(lambda x: x if str(x) not in ASSIGN else 'us') test_df[c + '_suffix'] = test_df[c + '_suffix'].map(lambda x: x if str(x) not in ASSIGN else 'us')",0,"[""emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n"", ""us_emails = ['gmail', 'net', 'edu']\n"", '#https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499#latest_df-579654\n', ""for c in ['P_emaildomain', 'R_emaildomain']:\n"", ""    train_df[c + '_bin'] = train_df[c].map(emails)\n"", ""    test_df[c + '_bin'] = test_df[c].map(emails)\n"", '    \n', ""    train_df[c + '_suffix'] = train_df[c].map(lambda x: str(x).split('.')[-1])\n"", ""    test_df[c + '_suffix'] = test_df[c].map(lambda x: str(x).split('.')[-1])\n"", '    \n', ""    train_df[c + '_suffix'] = train_df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n"", ""    test_df[c + '_suffix'] = test_df[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')""]"
CHECKPOINT print(data['OFFENSE_CODE_GROUP'].value_counts()),0,"[""print(data['OFFENSE_CODE_GROUP'].value_counts())""]"
CHECKPOINT print(result),0,['print(result)']
player_df.info(),0,['player_df.info()']
"plt.figure(figsize=[12,5]) ASSIGN = plt.subplots(2) sns.distplot(data['Time Spent on Site'], ax = ax[0]) sns.boxplot(x='Clicked', y='Time Spent on Site', data = data, ax = ax[1]) plt.show()",1,"['#Distribution of time spent\n', 'plt.figure(figsize=[12,5])\n', 'fig, ax = plt.subplots(2)\n', ""sns.distplot(data['Time Spent on Site'], ax = ax[0])\n"", ""sns.boxplot(x='Clicked', y='Time Spent on Site', data = data, ax = ax[1])\n"", 'plt.show()']"
SETUP,0,"['import pandas as pd\n', 'import numpy as np']"
SETUP,0,"['from datetime import datetime\n', 'from sklearn.preprocessing import RobustScaler\n', 'from sklearn.model_selection import KFold, cross_val_score\n', 'from sklearn.metrics import mean_squared_error , make_scorer\n', 'from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n', 'from sklearn.pipeline import make_pipeline\n', 'from sklearn.linear_model import LinearRegression\n', '\n', 'from sklearn.ensemble import GradientBoostingRegressor\n', 'from sklearn.svm import SVR\n', 'from mlxtend.regressor import StackingCVRegressor\n', 'from sklearn.linear_model import LinearRegression\n', '\n', 'from xgboost import XGBRegressor\n', 'from lightgbm import LGBMRegressor']"
"""""""Q3.Store data of heights of 50 students with 4 mistakes and plot them in a graph and segregate normal data from abnormal one"""""" ASSIGN=[72,71,56,45,67,89,54,58,67,77,77,78,77,73,73,172,72,71,56, 45,67,89,54,58,67,172,77,78,77,73,73,172,12,54,64,75,75,77, 88,66,70,12,54,64,75,75,77,88,66,70] def plot_his(ASSIGN): ASSIGN=min(heights)-min(heights)%10 ASSIGN=max(heights)+10 ASSIGN=list(range(start,end,5)) plt.hist(ASSIGN,ASSIGN,histtype='bar',rwidth=0.5,color='c') plt.xlabel('height of students (inches)') plt.ylabel('No.of Students') plt.show() plot_his(ASSIGN) ASSIGN=list(filter(lambda x: not x==172 and not x==12, ASSIGN)) plot_his(ASSIGN)",1,"['""""""Q3.Store data of heights of 50 students with 4 mistakes and plot \n', 'them in a graph and segregate normal data from abnormal one""""""\n', '\n', 'heights=[72,71,56,45,67,89,54,58,67,77,77,78,77,73,73,172,72,71,56,\n', '         45,67,89,54,58,67,172,77,78,77,73,73,172,12,54,64,75,75,77,\n', '         88,66,70,12,54,64,75,75,77,88,66,70]\n', 'def plot_his(heights):\n', '    start=min(heights)-min(heights)%10\n', '    end=max(heights)+10\n', '    bins=list(range(start,end,5))\n', ""    plt.hist(heights,bins,histtype='bar',rwidth=0.5,color='c')\n"", ""    plt.xlabel('height of students (inches)')\n"", ""    plt.ylabel('No.of Students')\n"", '    plt.show()\n', 'plot_his(heights)\n', 'heights=list(filter(lambda x: not x==172 and not x==12, heights))\n', 'plot_his(heights)']"
"def infer_model(df, features, target, n_jobs): ASSIGN = LGBMRegressor if len(df[target].value_counts()) == 2: ASSIGN = LabelEncoder().fit_transform(ASSIGN) ASSIGN = LGBMClassifier ASSIGN = [] for f in features: if df[f].dtype == object: ASSIGN = LabelEncoder().fit_transform(ASSIGN.apply(str)) ASSIGN.append(f) ASSIGN = int(0.01*df.shape[0]) ASSIGN = model_class(min_child_samples=min_child_samples, n_jobs=n_jobs) return model, df, categoricals",0,"['def infer_model(df, features, target, n_jobs):\n', '    model_class = LGBMRegressor\n', '    if len(df[target].value_counts()) == 2:\n', '        df[target] = LabelEncoder().fit_transform(df[target])\n', '        model_class = LGBMClassifier\n', '\n', '    categoricals = []\n', '    for f in features:\n', '        if df[f].dtype == object:\n', '            df[f] = LabelEncoder().fit_transform(df[f].apply(str))\n', '            categoricals.append(f)\n', '\n', '    min_child_samples = int(0.01*df.shape[0])\n', '\n', '    model = model_class(min_child_samples=min_child_samples, n_jobs=n_jobs)\n', '\n', '    return model, df, categoricals']"
CHECKPOINT ASSIGN = pd.read_csv('..path') print(ASSIGN.head()),0,"[""tb = pd.read_csv('../input/tobacco.csv')\n"", 'print(tb.head())']"
"interp.plot_top_losses(9, figsize=(15,11))",1,"['interp.plot_top_losses(9, figsize=(15,11))']"
SETUP,0,"['# Import Libraries\n', 'import numpy as np\n', '\n', 'import tensorflow as tf\n', 'import tensorflow.keras as keras\n', 'from tensorflow.keras.datasets import mnist\n', 'from tensorflow.keras.models import Sequential\n', 'from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n', '\n', 'import matplotlib.pyplot as plt']"
"SETUP ASSIGN = SVC() ASSIGN.fit(X_train, y_train)",0,"['#Fit model\n', 'from sklearn.svm import SVC\n', 'svc_model = SVC()\n', 'svc_model.fit(X_train, y_train)']"
"plt.figure(figsize = (7,6)) ASSIGN = plt.bar(top10_confirm.keys(), top10_confirm.values()) plt.xlabel('Country') plt.ylabel('Count') plt.title('Highest Confirmed Cases in 10 countries') plt.xticks(list(top10_confirm.keys()), rotation = 90) for bar in ASSIGN: ASSIGN = bar.get_height() plt.text(bar.get_x(), ASSIGN + .005, ASSIGN, rotation = 5) plt.show()",1,"['plt.figure(figsize = (7,6))\n', 'bars = plt.bar(top10_confirm.keys(), top10_confirm.values())\n', ""plt.xlabel('Country')\n"", ""plt.ylabel('Count')\n"", ""plt.title('Highest Confirmed Cases in 10 countries')\n"", 'plt.xticks(list(top10_confirm.keys()), rotation = 90)\n', 'for bar in bars:\n', '    yval = bar.get_height()\n', '    plt.text(bar.get_x(), yval + .005, yval, rotation = 5)\n', 'plt.show()']"
"sns.heatmap(data.isnull(), yticklabels = False, cmap = 'Blues', cbar = False)",1,"['#Check on missing data\n', ""sns.heatmap(data.isnull(), yticklabels = False, cmap = 'Blues', cbar = False)""]"
final['Age'].isnull().sum(),0,"[""final['Age'].isnull().sum()""]"
SETUP,0,"['%matplotlib inline\n', 'import numpy as np\n', 'import pandas as pd\n', 'import matplotlib.pyplot as plt\n', 'import matplotlib\n', 'from sklearn import metrics\n', 'from sklearn.neighbors import NearestNeighbors\n', '\n', 'from keras.models import Sequential, Model\n', 'from keras.layers import Dense, Dropout, Convolution2D, MaxPooling2D, Flatten, Input\n', 'from keras.optimizers import adam\n', 'from keras.utils.np_utils import to_categorical\n', '\n', 'import seaborn as sns\n', '\n', ""%config InlineBackend.figure_format = 'retina'""]"
"ASSIGN = ASSIGN.fillna(ASSIGN.median()) ASSIGN = ASSIGN.fillna(ASSIGN.median()) sns.heatmap(test.isnull(), cbar=False)",1,"[""test['Age'] = test['Age'].fillna(test['Age'].median())\n"", ""test['Fare'] = test['Fare'].fillna(test['Fare'].median())\n"", '\n', 'sns.heatmap(test.isnull(), cbar=False)']"
df_all.sample(n=5),0,['df_all.sample(n=5)']
"df_all.drop('date_first_booking', axis=1, inplace=True)",0,"[""df_all.drop('date_first_booking', axis=1, inplace=True)""]"
"sns.countplot(x = 'Embarked', data = df1)",1,"[""sns.countplot(x = 'Embarked', data = df1)""]"
"ASSIGN = Sequential() ASSIGN.add(Conv2D(32, kernel_size = (3, 3), padding = 'same', activation = 'relu', input_shape = input_shape)) ASSIGN.add(MaxPool2D(pool_size = (2, 2))) ASSIGN.add(Dropout(0.25)) ASSIGN.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu', padding = 'same')) ASSIGN.add(MaxPool2D(pool_size = (2, 2))) ASSIGN.add(Dropout(0.25)) ASSIGN.add(Flatten()) ASSIGN.add(Dense(128, activation = 'relu')) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Dense(num_classes, activation = 'softmax')) ASSIGN.summary()",0,"['# Build Model\n', 'model = Sequential()\n', '# 1st Conv layer\n', ""model.add(Conv2D(32, kernel_size = (3, 3), padding = 'same', activation = 'relu', input_shape = input_shape))\n"", 'model.add(MaxPool2D(pool_size = (2, 2)))\n', 'model.add(Dropout(0.25))\n', '# 2nd Conv layer        \n', ""model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu', padding = 'same'))\n"", 'model.add(MaxPool2D(pool_size = (2, 2)))\n', 'model.add(Dropout(0.25))\n', '# Fully Connected layer        \n', 'model.add(Flatten())\n', ""model.add(Dense(128, activation = 'relu'))\n"", 'model.add(Dropout(0.2))\n', ""model.add(Dense(num_classes, activation = 'softmax'))\n"", '\n', 'model.summary()']"
df[features].describe(),0,['df[features].describe()']
final.head(),0,['final.head()']
"sns.pairplot(train_data,hue='type')",1,"[""sns.pairplot(train_data,hue='type')""]"
"SETUP X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,random_state = 20)",0,"['from sklearn.model_selection import train_test_split\n', 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,random_state = 20)']"
"plotPerColumnDistribution(df3, 10, 5)",1,"['plotPerColumnDistribution(df3, 10, 5)']"
CHECKPOINT print(os.listdir()),0,"[""# let's check\n"", 'print(os.listdir(""../working""))']"
"sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))",1,"['#heatmap gives a visual representation of correlation between different attributes of the data\n', 'sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))']"
"ASSIGN = pd.merge(ASSIGN,weather_pred,on = ['Province','Date'],how = 'left')",0,"['#adding weather feature to test data\n', ""test = pd.merge(test,weather_pred,on = ['Province','Date'],how = 'left')""]"
zomato.info(),0,['zomato.info()']
data_features['Functional'] = data_features['Functional'].fillna('Typ') data_features['Utilities'] = data_features['Utilities'].fillna('None'),0,"[""data_features['Functional'] = data_features['Functional'].fillna('Typ')\n"", ""data_features['Utilities'] = data_features['Utilities'].fillna('None')""]"
"CHECKPOINT ASSIGN=pd.DataFrame({'type':data_2019.Statistic.value_counts().sort_values(ascending=False)[:10].index,'value':data_2019.Statistic.value_counts().sort_values(ascending=False)[:10].values}) type_stat",0,"[""type_stat=pd.DataFrame({'type':data_2019.Statistic.value_counts().sort_values(ascending=False)[:10].index,'value':data_2019.Statistic.value_counts().sort_values(ascending=False)[:10].values})\n"", 'type_stat']"
"ASSIGN = model.predict_classes(test,verbose=0) ASSIGN=pd.DataFrame({""ImageId"": list(range(1,len(pred)+1)), ""Label"": ASSIGN})",0,"['pred = model.predict_classes(test,verbose=0)\n', '\n', 'submissions=pd.DataFrame({""ImageId"": list(range(1,len(pred)+1)),\n', '                         ""Label"": pred})']"
"SETUP ASSIGN=pd.DataFrame({'TransactionID':ID,'LR_TEST':LR_TEST,'LGBM_TEST':LGBM_TEST}) ASSIGN.to_csv('ASSIGN.csv',index=False) ASSIGN=pd.DataFrame({'TransactionID':ID,'isFraud':LGBM_TEST}) ASSIGN.to_csv('ASSIGN.csv',index=False)",0,"['LR_TEST=lr.predict_proba(test_transaction_new)[:, 1]\n', 'LGBM_TEST= gbm.predict(test_transaction_new, num_iteration=gbm.best_iteration) \n', '\n', ""prediction=pd.DataFrame({'TransactionID':ID,'LR_TEST':LR_TEST,'LGBM_TEST':LGBM_TEST})\n"", '\n', ""prediction.to_csv('prediction.csv',index=False)\n"", '\n', ""submission=pd.DataFrame({'TransactionID':ID,'isFraud':LGBM_TEST})\n"", '\n', '\n', ""submission.to_csv('submission.csv',index=False)""]"
"def save(model, path): torch.save(model, path) def load(path): return torch.load(path)",0,"['def save(model, path):\n', '    torch.save(model, path)\n', '\n', 'def load(path):\n', '    return torch.load(path)']"
"ASSIGN = torch.ASSIGN(""cuda"" if torch.cuda.is_available() else ""cpu"") ASSIGN = torch.cuda.device_count() torch.cuda.get_device_name(0)",0,"['device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n', 'n_gpu = torch.cuda.device_count()\n', 'torch.cuda.get_device_name(0)']"
"CHECKPOINT print('data size: ',len(data)) print('data_review size: ',len(data_review))",0,"[""print('data size: ',len(data))\n"", ""print('data_review size: ',len(data_review))""]"
SETUP ASSIGN = pd.read_csv('..path') ASSIGN.head(),0,"['import pandas as pd\n', ""sub_sample = pd.read_csv('../input/virtual-hack/sampleSubmission.csv')\n"", 'sub_sample.head()']"
missing_data[11:],0,"['#handle them in the same way as garage\n', 'missing_data[11:]']"
"XGB.fit(x_train,x_test) ASSIGN = XGB.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('XGBoosting(No HT).csv',index = False)",0,"['XGB.fit(x_train,x_test)\n', 'model9pred = XGB.predict(y_train)\n', ""submission9 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission9['PassengerId'] = result['PassengerId']\n"", ""submission9['Survived'] = model9pred\n"", ""submission9.to_csv('XGBoosting(No HT).csv',index = False)""]"
"ASSIGN = ASSIGN.query(""Date > '2020-04-25'"")",0,"['test_df = test_df.query(""Date > \'2020-04-25\'"")']"
"data_features['MSSubClass'].groupby((data_features['BsmtCond'],data_features['BsmtExposure'])).count()",0,"[""data_features['MSSubClass'].groupby((data_features['BsmtCond'],data_features['BsmtExposure'])).count()\n"", ""#When Con is TA and Qual is Gd we shuold choose No to fill the missing value in 'BsmtExposure'""]"
"plt.figure(figsize=(25,35)) ASSIGN=1 for k in ['01','02','03','04','05','06','07','08']: ASSIGN=TOTAvgRank_DD_DAP_TE_TB('ASSIGN Baddeley',str(k)) ASSIGN=[] ASSIGN=[] ASSIGN=[] for i in range(4): r1,r2=ASSIGN['rank'+str(i)].split('path') R=float(r1)path(r2) R=1-R ASSIGN.append(1.5+R*math.sin(math.pipath+i*math.pipath)) ASSIGN.append(1.5+R*math.cos(math.pipath+i*math.pipath)) ASSIGN.append(ASSIGN['type'+str(i)]+"" rank: ""+ASSIGN['rank'+str(i)]) ASSIGN.append(ASSIGN[0]) ASSIGN.append(ASSIGN[0]) plt.subplot(4,2,ASSIGN) plt.plot(ASSIGN,ASSIGN, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2) for i, txt in enumerate(ASSIGN): plt.annotate(txt, (ASSIGN[i]-0.2, ASSIGN[i])) plt.xlim(0.5,2.5) plt.ylim(0.5,2.5) plt.fill(ASSIGN, ASSIGN,""coral"") plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2) plt.title(""ASSIGN Baddeley's performance in month ""+str(k),size=18)",1,"['plt.figure(figsize=(25,35))\n', 'j=1\n', ""for k in ['01','02','03','04','05','06','07','08']:\n"", ""    Aaron=TOTAvgRank_DD_DAP_TE_TB('Aaron Baddeley',str(k))\n"", '    y=[]\n', '    x=[]\n', '    n=[]\n', '    for i in range(4):\n', ""        r1,r2=Aaron['rank'+str(i)].split('/')\n"", '        R=float(r1)/float(r2)\n', '        R=1-R\n', '        y.append(1.5+R*math.sin(math.pi/4+i*math.pi/2))\n', '        x.append(1.5+R*math.cos(math.pi/4+i*math.pi/2))\n', '        n.append(Aaron[\'type\'+str(i)]+"" rank: ""+Aaron[\'rank\'+str(i)])\n', '\n', '    x.append(x[0])\n', '    y.append(y[0])\n', '    plt.subplot(4,2,j)\n', ""    plt.plot(x,y, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=2)\n"", '    for i, txt in enumerate(n):\n', '        plt.annotate(txt, (x[i]-0.2, y[i]))\n', '    plt.xlim(0.5,2.5)\n', '    plt.ylim(0.5,2.5)\n', '    plt.fill(x, y,""coral"")\n', ""    plt.plot( 1.5, 1.5, marker='o', markerfacecolor='blue', markersize=8, linewidth=2)\n"", '    plt.title(""Aaron Baddeley\'s performance in month ""+str(k),size=18)\n']"
"CHECKPOINT ASSIGN = model11.predict(x_test) ASSIGN = mean_squared_error(Y_test, y_pred11, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, ASSIGN)))",0,"['y_pred11 = model11.predict(x_test)\n', '\n', 'val = mean_squared_error(Y_test, y_pred11, squared=False)\n', 'val11 = str(round(val, 4))\n', '\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred11)))\n""]"
"US=data[data['Countrypath']=='US'] US= US.groupby(['date','Provincepath'])['Confirmed', 'Deaths', 'Recovered'].max() ASSIGN =ASSIGN.reset_index() ASSIGN=US[US['date']==max(US['date'])].sort_values('Confirmed',ascending=False)[:10] ASSIGN=ASSIGN.drop(columns=['date']) ASSIGN['Recovered rate']=ASSIGN['Recovered']path['Confirmed'] ASSIGN.style.background_gradient(cmap='Purples')",1,"[""US=data[data['Country/Region']=='US']\n"", ""US= US.groupby(['date','Province/State'])['Confirmed', 'Deaths', 'Recovered'].max()\n"", 'US =US.reset_index()\n', '\n', ""US_table=US[US['date']==max(US['date'])].sort_values('Confirmed',ascending=False)[:10]\n"", ""US_table=US_table.drop(columns=['date'])\n"", ""US_table['Recovered rate']=US_table['Recovered']/US_table['Confirmed']\n"", ""US_table.style.background_gradient(cmap='Purples')""]"
scores.mean(),0,['scores.mean()']
"my_submission.to_csv('my_submission.csv', index=False)",0,"[""my_submission.to_csv('my_submission.csv', index=False)""]"
"SETUP CHECKPOINT ASSIGN = [] for l in pred_labels: for l1 in l: ASSIGN.append(l1.item()) ASSIGN = [] for l in true_labels: for l1 in l: ASSIGN.append(l1.item()) def multiclass_roc_auc_score(truth, pred, average=""macro""): ASSIGN = LabelBinarizer() ASSIGN.fit(truth) ASSIGN = lb.transform(ASSIGN) ASSIGN = lb.transform(ASSIGN) return roc_auc_score(ASSIGN, ASSIGN, average=average) print(, multiclass_roc_auc_score(ASSIGN, ASSIGN))",0,"['pred_labels_expanded = []\n', 'for l in pred_labels:\n', '    for l1 in l:\n', '        pred_labels_expanded.append(l1.item())\n', '\n', 'true_labels_expanded = []\n', 'for l in true_labels:\n', '    for l1 in l:\n', '        true_labels_expanded.append(l1.item())\n', '\n', 'from sklearn.metrics import roc_auc_score\n', 'from sklearn.preprocessing import LabelBinarizer\n', '\n', 'def multiclass_roc_auc_score(truth, pred, average=""macro""):\n', '\n', '    lb = LabelBinarizer()\n', '    lb.fit(truth)\n', '\n', '    truth = lb.transform(truth)\n', '    pred = lb.transform(pred)\n', '\n', '    return roc_auc_score(truth, pred, average=average)\n', '\n', 'print(""ROC AUC SCORE: =========>"", multiclass_roc_auc_score(true_labels_expanded, pred_labels_expanded))']"
"SETUP ASSIGN = read_csv(""..path"") ASSIGN = read_csv(""..path"") ASSIGN = read_csv(""..path"")",0,"[""# We'll need these libraries\n"", 'import numpy as np\n', 'import pandas as pd \n', 'from pandas import read_csv\n', '\n', '# Plotting libraries\n', 'import seaborn as sns\n', 'from ggplot import *\n', '\n', 'recipes = read_csv(""../input/epirecipes/epi_r.csv"")\n', 'bikes = read_csv(""../input/nyc-east-river-bicycle-crossings/nyc-east-river-bicycle-counts.csv"")\n', 'weather = read_csv(""../input/szeged-weather/weatherHistory.csv"")']"
"ASSIGN = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001) ASSIGN = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)",0,"['optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n', 'scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)']"
CHECKPOINT X_train.shape,0,"['X_train.shape\n', '# We have 30 variables after creating our dummy variables for our categories']"
"SETUP ASSIGN=df_final[""CITY""].unique() ASSIGN=[] ASSIGN ='https:path' for i in range(len(ASSIGN)): ASSIGN = {'address': city_name[i], 'key': 'AIzaSyD-kYTK-8FQGueJqA2028t2YHbUX96V0vk'} ASSIGN = requests.get(geo_s, params=param) ASSIGN=ASSIGN.text ASSIGN=json.loads(response) ASSIGN=data[""results""][0][""geometry""][""location""][""ASSIGN""] ASSIGN=data[""results""][0][""geometry""][""location""][""ASSIGN""] ASSIGN=pd.DataFrame([[city_name[i],lat,lng]]) ASSIGN.append(ASSIGN.values)",0,"['# import json and requests library to use googl apis to get the longitude ant latituide values\n', 'import requests\n', 'import json\n', '\n', '#creating a separate array with all city names as elements of array\n', 'city_name=df_final[""CITY""].unique()\n', 'li1=[]\n', '\n', '#googlemap api calling url \n', ""geo_s ='https://maps.googleapis.com/maps/api/geocode/json'\n"", '#iterating through a for loop for each city names \n', 'for i in range(len(city_name)):\n', '\n', '#i have used my own google map api, please use ypur own api     \n', "" param = {'address': city_name[i], 'key': 'AIzaSyD-kYTK-8FQGueJqA2028t2YHbUX96V0vk'}\n"", ' \n', ' response = requests.get(geo_s, params=param)\n', ' \n', ' response=response.text\n', '\n', ' data=json.loads(response)\n', '\n', '#setting up the variable with corresponding city longitude and latitude\n', ' lat=data[""results""][0][""geometry""][""location""][""lat""]\n', ' lng=data[""results""][0][""geometry""][""location""][""lng""]\n', '\n', '#creating a new data frame with city , latitude and longitude as columns\n', ' df2=pd.DataFrame([[city_name[i],lat,lng]])\n', ' li1.append(df2.values)']"
"ASSIGN = GNB.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('Gaussian NB(No HT).csv',index = False)",0,"['model5pred = GNB.predict(y_train)\n', ""submission5 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""submission5['PassengerId'] = result['PassengerId']\n"", ""submission5['Survived'] = model5pred\n"", ""submission5.to_csv('Gaussian NB(No HT).csv',index = False)""]"
learn.save('stage-1'),0,"[""learn.save('stage-1')""]"
"library(tidyverse) recpies <- read_csv(""..path"") bikes <- read_csv(""..path"") weather <- read_csv(""..path"")",0,"[""# library we'll need\n"", 'library(tidyverse)\n', '\n', ""# read in all three datasets (you'll pick one to use later)\n"", 'recpies <- read_csv(""../input/epirecipes/epi_r.csv"")\n', 'bikes <- read_csv(""../input/nyc-east-river-bicycle-crossings/nyc-east-river-bicycle-counts.csv"")\n', 'weather <- read_csv(""../input/szeged-weather"")']"
CHECKPOINT ASSIGN = tourney_win_result.copy() ASSIGN['Seed1'] = tourney_win_result['Seed2'] ASSIGN['Seed2'] = tourney_win_result['Seed1'] ASSIGN['ScoreT1'] = tourney_win_result['ScoreT2'] ASSIGN['ScoreT2'] = tourney_win_result['ScoreT1'] tourney_lose_result,0,"['tourney_lose_result = tourney_win_result.copy()\n', ""tourney_lose_result['Seed1'] = tourney_win_result['Seed2']\n"", ""tourney_lose_result['Seed2'] = tourney_win_result['Seed1']\n"", ""tourney_lose_result['ScoreT1'] = tourney_win_result['ScoreT2']\n"", ""tourney_lose_result['ScoreT2'] = tourney_win_result['ScoreT1']\n"", 'tourney_lose_result']"
df1.head(5),0,['df1.head(5)']
"ASSIGN=plt.subplots(1,2,figsize=(15,8)) ASSIGN = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",'cadetblue','hotpink','orange','darksalmon','brown') train.MSZoning.value_counts().sort_values(ascending=False).plot(kind='bar',color=ASSIGN,ax=ax[0]) ax[0].set_title(""bar chart for MSZoning"",size=20) ax[0].set_xlabel('counts',size=18) ax[0].tick_params(axis='x',rotation=360) sns.boxplot(x=""MSZoning"", y=""SalePrice"", data=train,ax=ax[1]) ax[1].set_title(""Boxplot of Price for MSZoning"",size=20)",1,"['fig,ax=plt.subplots(1,2,figsize=(15,8))\n', 'clr = (""blue"", ""forestgreen"", ""gold"", ""red"", ""purple"",\'cadetblue\',\'hotpink\',\'orange\',\'darksalmon\',\'brown\')\n', ""train.MSZoning.value_counts().sort_values(ascending=False).plot(kind='bar',color=clr,ax=ax[0])\n"", 'ax[0].set_title(""bar chart for MSZoning"",size=20)\n', ""ax[0].set_xlabel('counts',size=18)\n"", ""ax[0].tick_params(axis='x',rotation=360)\n"", '\n', 'sns.boxplot(x=""MSZoning"", y=""SalePrice"", data=train,ax=ax[1])\n', 'ax[1].set_title(""Boxplot of Price for MSZoning"",size=20)']"
train_df.head(),0,['train_df.head()']
"ASSIGN = ASSIGN[['address','studytime','failures','activities','nursery','higher','internet','famrel','freetime','absences','G3','romantic']]",0,"[""data_mat = data_mat[['address','studytime','failures','activities','nursery','higher','internet','famrel','freetime','absences','G3','romantic']]""]"
total.head(20),0,['total.head(20)']
logisticRegr.predict(X_test_pca),0,['logisticRegr.predict(X_test_pca)']
ASSIGN = data_features[num_features] ASSIGN = data_features[cat_features] ASSIGN.head() ASSIGN.head(),0,"['data_num = data_features[num_features]\n', 'data_cat = data_features[cat_features]\n', 'data_num.head()\n', 'data_cat.head()']"
"CHECKPOINT """"""dtree=tree.DecisionTreeClassifier(min_samples_split=1000,min_samples_leaf =570) ASSIGN=ASSIGN.fit(train_x,train_y) ASSIGN = dtree.predict(test_x) ASSIGN=accuracy_score(test_y, predict_y) print('%d' % i,'test accuracy: %.5f' %ASSIGN)""""""",0,"['#test\n', '#測試不同的參數，發現並沒有太大改變\n', '#for i in range(400,601,5):    \n', '    """"""dtree=tree.DecisionTreeClassifier(min_samples_split=1000,min_samples_leaf =570)\n', '    dtree=dtree.fit(train_x,train_y)\n', '    predict_y = dtree.predict(test_x)\n', '    x=accuracy_score(test_y, predict_y)\n', '    print(\'%d\' % i,\'test accuracy: %.5f\'  %x)""""""']"
"CHECKPOINT print('We have following categorical features:') print() ASSIGN = [] ASSIGN = [] for i in test[1:]: ASSIGN = train_df[i].unique() ASSIGN = test_df[i].unique() if len(ASSIGN) == len(ASSIGN): print(i,':',len(ASSIGN),'unique values') ASSIGN.append(i) else: ASSIGN.append(i) print() print('And we have',len(ASSIGN), 'of following continuous features:') print(ASSIGN)",0,"[""print('We have following categorical features:')\n"", 'print()\n', 'cat = []\n', 'cont = []\n', 'for i in test[1:]:\n', '    temp1 = train_df[i].unique()\n', '    temp2 = test_df[i].unique()\n', '    if len(temp1) == len(temp2):\n', ""        print(i,':',len(temp1),'unique values')\n"", '        cat.append(i)\n', '    else:\n', '        cont.append(i)\n', 'print()\n', ""print('And we have',len(cont), 'of following continuous features:') \n"", 'print(cont)']"
"CHECKPOINT def DrawLineFromFormula(slope, intercept, color): plt.xlim(-0.05, 1.05) plt.ylim(-0.05, 1.05) ASSIGN = np.arange(-100, 100, 0.1) plt.plot(ASSIGN, slope*ASSIGN+intercept, color) return",1,"['# Helper function\n', '\n', 'def DrawLineFromFormula(slope, intercept, color):\n', '    plt.xlim(-0.05, 1.05)\n', '    plt.ylim(-0.05, 1.05)\n', '    x = np.arange(-100, 100, 0.1)\n', '    plt.plot(x, slope*x+intercept, color)\n', '    return']"
"ASSIGN=pd.read_csv(""..path"") ASSIGN.head()",0,"['data_2019=pd.read_csv(""../input/pga-tour-20102018-data/2019_data.csv"")\n', 'data_2019.head()']"
suicide_attacks['Province'] = suicide_attacks['Province'].str.lower() suicide_attacks['Province'] = suicide_attacks['Province'].str.strip(),0,"['# convert to lower case\n', ""suicide_attacks['Province'] = suicide_attacks['Province'].str.lower()\n"", '# remove trailing white spaces\n', ""suicide_attacks['Province'] = suicide_attacks['Province'].str.strip()""]"
"CHECKPOINT tf.keras.backend.clear_session() print('Building model and restoring weights for fine-tuning...', flush=True) ASSIGN = 1 ASSIGN = 'path' ASSIGN = 'path'",0,"['tf.keras.backend.clear_session()\n', '\n', ""print('Building model and restoring weights for fine-tuning...', flush=True)\n"", 'num_classes = 1\n', ""pipeline_config = '/kaggle/working/models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n"", ""checkpoint_path = '/kaggle/working/models/research/object_detection/test_data/checkpoint/ckpt-0'\n""]"
"plt.figure(figsize=(18, 6)) plt.subplot(1, 2, 1) stations.name.value_counts()[:10].sort_values().plot.barh() plt.title(""Top 10 city by stations"",size=18) plt.xlabel(""stations"") plt.subplot(1, 2, 2) stations.country.value_counts()[:10].sort_values().plot.barh() plt.title(""Top 10 country by stations"",size=18) plt.xlabel(""stations"")",1,"['plt.figure(figsize=(18, 6))\n', 'plt.subplot(1, 2, 1) \n', 'stations.name.value_counts()[:10].sort_values().plot.barh()\n', 'plt.title(""Top 10 city by stations"",size=18)\n', 'plt.xlabel(""stations"")\n', 'plt.subplot(1, 2, 2) \n', 'stations.country.value_counts()[:10].sort_values().plot.barh()\n', 'plt.title(""Top 10 country by stations"",size=18)\n', 'plt.xlabel(""stations"")']"
'''columns_np=train.columns.values ASSIGN=list(train) ASSIGN=train[train.month==12] ASSIGN=train[train.month==1] ASSIGN=train[train.month==2] ASSIGN=train[train.month==3] ASSIGN=train[train.month==4] ASSIGN=train[train.month==5] ASSIGN=train[train.month==6]''' ''' ASSIGN=test[test.month==7] ASSIGN=test[test.month==8] ASSIGN=test[test.month==9] ASSIGN=test[test.month==10] ASSIGN=test[test.month==11] ASSIGN=test[test.month==12]''',0,"[""'''columns_np=train.columns.values\n"", 'columns_list=list(train)\n', 'train_12=train[train.month==12]\n', 'train_1=train[train.month==1]\n', 'train_2=train[train.month==2]\n', 'train_3=train[train.month==3]\n', 'train_4=train[train.month==4]\n', 'train_5=train[train.month==5]\n', ""train_6=train[train.month==6]'''\n"", ""'''\n"", 'test_7=test[test.month==7]\n', 'test_8=test[test.month==8]\n', 'test_9=test[test.month==9]\n', 'test_10=test[test.month==10]\n', 'test_11=test[test.month==11]\n', ""test_12=test[test.month==12]'''""]"
"CHECKPOINT ASSIGN = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0009) ASSIGN = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False) ASSIGN = Input(shape=(224,224, 3)) ASSIGN = tf.keras.applications.ResNet50(input_tensor= input_tensor, weights='imagenet', include_top=False) ASSIGN = base_model.output print(ASSIGN.shape) ASSIGN = GlobalAveragePooling2D()(base_output) ASSIGN = Flatten()(base_output) ASSIGN = concatenate([gap,ASSIGN]) ASSIGN = Dense(512, activation='relu')(ASSIGN) ASSIGN = BatchNormalization()(ASSIGN) ASSIGN = Dense(512, activation='relu')(ASSIGN) ASSIGN = BatchNormalization()(ASSIGN) ASSIGN = Dense(5, activation='softmax')(x) ASSIGN = Model(inputs=input_tensor, outputs=predict) for layer in (ASSIGN.layers): layer.trainable = False",0,"['adam = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0009)\n', 'sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n', '\n', 'input_tensor = Input(shape=(224,224, 3))\n', '#backbone\n', '\n', ""base_model = tf.keras.applications.ResNet50(input_tensor= input_tensor, weights='imagenet', include_top=False)\n"", 'base_output = base_model.output\n', 'print(base_output.shape)\n', '# channel-attention\n', '# x = squeeze_excitation_layer(base_output, 2048, ratio=4, concate=False)\n', '# x = BatchNormalization()(x)\n', '\n', '# #concat\n', '# x = concatenate([base_output, x], axis=3)\n', '# spp\n', '\n', 'gap = GlobalAveragePooling2D()(base_output)\n', 'x = Flatten()(base_output)\n', 'x = concatenate([gap,x])\n', ""x = Dense(512, activation='relu')(x)\n"", 'x = BatchNormalization()(x)\n', ""x = Dense(512, activation='relu')(x)\n"", 'x = BatchNormalization()(x)\n', ""predict = Dense(5, activation='softmax')(x)\n"", 'model = Model(inputs=input_tensor, outputs=predict)\n', '\n', 'for layer in (base_model.layers):\n', '    layer.trainable = False\n', '\n', '# for l in model.layers:\n', '#   print(l.name)']"
"for df in [df_train,df_test]: df['first_active_month'] = pd.to_datetime(df['first_active_month']) ASSIGN = ASSIGN.dt.dayofweek ASSIGN = ASSIGN.dt.weekofyear ASSIGN = ASSIGN.dt.month df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\ 'new_hist_purchase_date_min']: ASSIGN = ASSIGN.astype(np.int64) * 1e-9 df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size'] df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum'] for f in ['feature_1','feature_2','feature_3']: ASSIGN = df_train.groupby([f])['outliers'].mean() df_train[f] = df_train[f].map(ASSIGN) df_test[f] = df_test[f].map(ASSIGN)",0,"['for df in [df_train,df_test]:\n', ""    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n"", ""    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n"", ""    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n"", ""    df['month'] = df['first_active_month'].dt.month\n"", ""    df['elapsed_time'] = (datetime.datetime.today() - df['first_active_month']).dt.days\n"", ""    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n"", ""    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n"", ""    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n"", ""                     'new_hist_purchase_date_min']:\n"", '        df[f] = df[f].astype(np.int64) * 1e-9\n', ""    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n"", ""    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n"", '\n', ""for f in ['feature_1','feature_2','feature_3']:\n"", ""    order_label = df_train.groupby([f])['outliers'].mean()\n"", '    df_train[f] = df_train[f].map(order_label)\n', '    df_test[f] = df_test[f].map(order_label)\n']"
"ASSIGN=train['P_emaildomain']=='protonmail.com' ASSIGN=train['P_emaildomain'][index_pro] SLICE=ASSIGN del index_pro del index_pro_data ASSIGN=test['P_emaildomain']=='protonmail.com' ASSIGN=test['P_emaildomain'][index_pro] SLICE=ASSIGN del index_pro del index_pro_data train.loc[ (train.isprotonmail.isnull()), 'isprotonmail' ] = 0 train['isprotonmail'].describe(include=""all"") test.loc[ (test.isprotonmail.isnull()), 'isprotonmail' ] = 0 test['isprotonmail'].describe(include=""all"")",0,"[""index_pro=train['P_emaildomain']=='protonmail.com'\n"", ""index_pro_data=train['P_emaildomain'][index_pro]\n"", ""train['isprotonmail']=index_pro_data\n"", 'del index_pro\n', 'del index_pro_data\n', ""index_pro=test['P_emaildomain']=='protonmail.com'\n"", ""index_pro_data=test['P_emaildomain'][index_pro]\n"", ""test['isprotonmail']=index_pro_data\n"", 'del index_pro\n', 'del index_pro_data\n', '\n', ""train.loc[ (train.isprotonmail.isnull()), 'isprotonmail' ] = 0\n"", 'train[\'isprotonmail\'].describe(include=""all"")\n', '\n', ""test.loc[ (test.isprotonmail.isnull()), 'isprotonmail' ] = 0\n"", 'test[\'isprotonmail\'].describe(include=""all"")']"
train.describe(),0,['train.describe()']
"CHECKPOINT ASSIGN = model7.predict(x_test) ASSIGN = mean_squared_error(Y_test, y_pred7, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, ASSIGN)))",0,"['y_pred7 = model7.predict(x_test)\n', '\n', 'val = mean_squared_error(Y_test, y_pred7, squared=False)\n', 'val7 = str(round(val, 4))\n', '\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred7)))\n""]"
"svc_model.fit(X_train_scaled, Y_train) ASSIGN = (X_test - X_test.min())path(X_test - X_test.min()).max() ASSIGN = svc_model.predict(X_test_scaled) ASSIGN = confusion_matrix(Y_test, Y_predicted) sns.heatmap(ASSIGN, annot = True, cmap='Blues')",1,"['# Train the model again\n', 'svc_model.fit(X_train_scaled, Y_train)\n', '# Create scaled test data\n', 'X_test_scaled = (X_test - X_test.min())/(X_test - X_test.min()).max()\n', '# Calculate new predictions\n', 'Y_predicted = svc_model.predict(X_test_scaled)\n', '# Draw confusion matrix\n', 'cm = confusion_matrix(Y_test, Y_predicted)\n', ""sns.heatmap(cm, annot = True, cmap='Blues')""]"
learn.fit_one_cycle(4),0,['learn.fit_one_cycle(4)']
"ASSIGN=[] ASSIGN=[] ASSIGN=[] for i in range(len(osaka_lines)): ASSIGN=osaka_lines.iloc[i].geometry.split('(')[1].split(')')[0].split(',') for j in range(len(ASSIGN)): ASSIGN.append(ASSIGN[j].split(' ')[0]) ASSIGN.append(ASSIGN[j].split(' ')[1]) ASSIGN.append(osaka_lines.url_name[i]) ASSIGN=pd.DataFrame({'x':x,'y':y,'z':z}) SLICE=SLICE.astype(float) SLICE=SLICE.astype(float) plt.figure(figsize=(27, 27)) plt.subplot(2, 2, 1) ASSIGN=sns.scatterplot(x=""x"", y=""y"", hue=""z"",data=fix) plt.legend(loc=0, bbox_to_anchor=(1.05,0.6)) plt.title(""lines for osaka"",size=20) ASSIGN.get_legend().remove() ASSIGN.set(xlabel='Longitude', ylabel='LATITUDE') plt.subplot(2,2,2) (osaka_lines.groupby(['url_name'])['length'].sum()path).sort_values(ascending= False)[:10].sort_values().plot.barh() plt.ylabel(' ') plt.xlabel('length(km)') plt.title(""Top 10 track by length"",size=20) plt.subplot(2,2,3) osaka_stations.groupby(['opening'])['id'].agg('count').plot() plt.xlabel(' ') plt.ylabel('stations') plt.title(""Number of opening stations by year"",size=20) plt.subplot(2,2,4) osaka_lines.name.value_counts()[:10].sort_values().plot.barh() plt.xlabel('counts') plt.title(""Top 10 line by number"",size=20)",1,"['x=[]\n', 'y=[]\n', 'z=[]\n', 'for i in range(len(osaka_lines)):\n', ""    sp=osaka_lines.iloc[i].geometry.split('(')[1].split(')')[0].split(',')\n"", '    for j in range(len(sp)):\n', ""        x.append(sp[j].split(' ')[0])\n"", ""        y.append(sp[j].split(' ')[1])\n"", '        z.append(osaka_lines.url_name[i])\n', ""fix=pd.DataFrame({'x':x,'y':y,'z':z})\n"", ""fix['x']=fix['x'].astype(float)\n"", ""fix['y']=fix['y'].astype(float)\n"", 'plt.figure(figsize=(27, 27))\n', 'plt.subplot(2, 2, 1) \n', 'ax=sns.scatterplot(x=""x"", y=""y"", hue=""z"",data=fix)\n', 'plt.legend(loc=0, bbox_to_anchor=(1.05,0.6))\n', 'plt.title(""lines for osaka"",size=20)\n', 'ax.get_legend().remove()\n', ""ax.set(xlabel='Longitude', ylabel='LATITUDE')\n"", 'plt.subplot(2,2,2)\n', ""(osaka_lines.groupby(['url_name'])['length'].sum()/1000).sort_values(ascending= False)[:10].sort_values().plot.barh()\n"", ""plt.ylabel(' ')\n"", ""plt.xlabel('length(km)')\n"", 'plt.title(""Top 10 track by length"",size=20)\n', 'plt.subplot(2,2,3)\n', ""osaka_stations.groupby(['opening'])['id'].agg('count').plot()\n"", ""plt.xlabel(' ')\n"", ""plt.ylabel('stations')\n"", 'plt.title(""Number of opening stations by year"",size=20)\n', 'plt.subplot(2,2,4)\n', 'osaka_lines.name.value_counts()[:10].sort_values().plot.barh()\n', ""plt.xlabel('counts')\n"", 'plt.title(""Top 10 line by number"",size=20)']"
"CHECKPOINT ASSIGN = grid.predict(X_test_scaled) ASSIGN = confusion_matrix(Y_test, grid_predicted) sns.heatmap(ASSIGN, annot = True, cmap = 'Blues') print(classification_report(Y_test, ASSIGN))",1,"['# We can use the optimized grid object directly to get predictions\n', 'grid_predicted = grid.predict(X_test_scaled)\n', '\n', 'cm = confusion_matrix(Y_test, grid_predicted)\n', ""sns.heatmap(cm, annot = True, cmap = 'Blues')\n"", 'print(classification_report(Y_test, grid_predicted))']"
"ASSIGN = vot_hard.predict(y_train) ASSIGN = pd.DataFrame(columns = ['PassengerId','Survived']) ASSIGN['PassengerId'] = result['PassengerId'] ASSIGN['Survived'] = ASSIGN ASSIGN.to_csv('HardVoting(NO HT).csv',index = False)",0,"['modelpred2 = vot_hard.predict(y_train)\n', ""sub2 = pd.DataFrame(columns = ['PassengerId','Survived'])\n"", ""sub2['PassengerId'] = result['PassengerId']\n"", ""sub2['Survived'] = modelpred2\n"", ""sub2.to_csv('HardVoting(NO HT).csv',index = False)""]"
"ASSIGN = plt.figure(figsize=(25, 60)) ASSIGN = [np.random.choice(night.loc[night['category_id'] == i, 'file_name'], 4) for i in night.category_id.unique()] ASSIGN = [i for j in ASSIGN for i in j] ASSIGN = [[i] * 4 for i in train.category_id.unique()] ASSIGN = [i for j in ASSIGN for i in j] for idx, img in enumerate(ASSIGN): ASSIGN = fig.add_subplot(14, 4, idx + 1, xticks=[], yticks=[]) ASSIGN = Image.open(""..path"" + img) plt.imshow(ASSIGN) ASSIGN.set_title(f'Label: {ASSIGN[idx]}')",1,"['# sample night images\n', 'fig = plt.figure(figsize=(25, 60))\n', ""imgs = [np.random.choice(night.loc[night['category_id'] == i, 'file_name'], 4) for i in night.category_id.unique()]\n"", 'imgs = [i for j in imgs for i in j]\n', 'labels = [[i] * 4 for i in train.category_id.unique()]\n', 'labels = [i for j in labels for i in j]\n', 'for idx, img in enumerate(imgs):\n', '    ax = fig.add_subplot(14, 4, idx + 1, xticks=[], yticks=[])\n', '    im = Image.open(""../input/train_images/"" + img)\n', '    plt.imshow(im)\n', ""    ax.set_title(f'Label: {labels[idx]}')""]"
SETUP,0,['!tree']
"ASSIGN = [] ASSIGN= h.in_degree() for i in ASSIGN: ASSIGN.append(i[1]) ASSIGN = plt.figure(figsize=(20,20)); plt.title('Degree Distribution per day'); plt.grid(True); plt.xlabel('Days'); plt.ylabel(' plt.plot(ASSIGN[0:99],color='orange',alpha=0.90);",1,"['dis = []\n', 'in_degrees= h.in_degree() \n', 'for i in in_degrees:\n', '    dis.append(i[1])\n', 'fig = plt.figure(figsize=(20,20));\n', ""plt.title('Degree Distribution per day');\n"", 'plt.grid(True);\n', ""plt.xlabel('Days');\n"", ""plt.ylabel('# of Demand');\n"", ""plt.plot(dis[0:99],color='orange',alpha=0.90);""]"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") print(ASSIGN.info())",0,"['#3. Write a Python program to get the number of observations, missing values and nan values.\n', 'import pandas as pd\n', 'iris = pd.read_csv(""../input/iris-dataset/iris.data.csv"")\n', 'print(iris.info())']"
CHECKPOINT data.path.values,0,['data.path.values']
"ASSIGN = TabularList.from_df(df.loc[idx_test].copy(), path='', cat_names=cat, cont_names=cont)",0,"['# prepare databunch ingestion of test set\n', ""test = TabularList.from_df(df.loc[idx_test].copy(), path='', cat_names=cat, cont_names=cont)""]"
"SETUP ASSIGN = Imputer(missing_values='NaN', strategy='most_frequent', axis=0) test[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']]=ASSIGN.fit_transform(test[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']]) train[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']]=ASSIGN.fit_transform(train[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']])",0,"['from sklearn.preprocessing import Imputer\n', ""imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n"", ""test[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']]=imp.fit_transform(test[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']])\n"", ""train[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']]=imp.fit_transform(train[['dist1','dist2','C1','C2','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14']])""]"
sub_df.head(),0,['sub_df.head()']
"ASSIGN = pd.concat([df,tf],axis = 0) ASSIGN.drop(['Survived'],axis = 1,inplace = True)",0,"['final = pd.concat([df,tf],axis = 0)\n', ""final.drop(['Survived'],axis = 1,inplace = True)""]"
"ASSIGN = create_pred_dataframe(image_names, predictions)",0,"['pred_df_with_species_name, pred_df_with_cat_number = create_pred_dataframe(image_names, predictions)']"
"ASSIGN=model.predict(test_data.drop('id',axis=1))",0,"[""pred=model.predict(test_data.drop('id',axis=1))""]"
"CHECKPOINT ASSIGN = Sequential() ASSIGN.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3))) ASSIGN.add(MaxPooling2D((2, 2))) ASSIGN.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) ASSIGN.add(MaxPooling2D((2, 2))) ASSIGN.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same')) ASSIGN.add(MaxPooling2D((2, 2))) ASSIGN.add(Flatten()) ASSIGN.add(Dense(128, activation='relu', kernel_initializer='he_uniform')) ASSIGN.add(Dense(1, activation='sigmoid')) print(ASSIGN.summary()) input_and_run(ASSIGN,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.0001,num_epochs=20)",0,"['##BUILDING THE MODEL 1\n', '\n', 'model1 = Sequential()#add model layers\n', '\n', ""model1.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)))\n"", 'model1.add(MaxPooling2D((2, 2)))\n', ""model1.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n"", 'model1.add(MaxPooling2D((2, 2)))\n', ""model1.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n"", 'model1.add(MaxPooling2D((2, 2)))\n', 'model1.add(Flatten())\n', ""model1.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n"", ""model1.add(Dense(1, activation='sigmoid'))\n"", '\n', 'print(model1.summary())\n', 'input_and_run(model1,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.0001,num_epochs=20)']"
"SETUP ASSIGN = plt.figure(figsize=(8,8)) ASSIGN = Axes3D(fig) ASSIGN = plt.axes(projection='3d') ASSIGN.scatter(X_train_pca[:,2], X_train_pca[:,0], X_train_pca[:,1],zdir='z', s=20, marker = 'o', c=y_train) ASSIGN.set_xlabel('Principal Component 1') ASSIGN.set_ylabel('Principal Component 2') ASSIGN.set_zlabel('Principal Component 3') plt.tight_layout() plt.show()",1,"['%matplotlib notebook\n', 'from mpl_toolkits.mplot3d import Axes3D\n', 'fig = plt.figure(figsize=(8,8))\n', 'ax = Axes3D(fig)\n', ""ax = plt.axes(projection='3d')\n"", ""ax.scatter(X_train_pca[:,2], X_train_pca[:,0], X_train_pca[:,1],zdir='z', s=20, marker = 'o', c=y_train)\n"", ""ax.set_xlabel('Principal Component 1')\n"", ""ax.set_ylabel('Principal Component 2')\n"", ""ax.set_zlabel('Principal Component 3')\n"", 'plt.tight_layout()\n', 'plt.show()']"
SETUP,0,"['import pandas as pd\n', 'import numpy as np\n', 'import matplotlib.pyplot as plt\n', '%matplotlib inline\n', 'import seaborn as sns']"
"CHECKPOINT tumor_data.drop(['diagnosis'], axis = 1, inplace = True) tumor_data.columns",0,"[""tumor_data.drop(['diagnosis'], axis = 1, inplace = True)\n"", 'tumor_data.columns']"
"ASSIGN=model.fit(x=X_train,y=y_train,batch_size=10,epochs=10,verbose=2,validation_data=(X_test,y_test))",0,"['train=model.fit(x=X_train,y=y_train,batch_size=10,epochs=10,verbose=2,validation_data=(X_test,y_test))']"
CHECKPOINT ASSIGN = dataset['Chance of Admit'] ey,0,"[""ey = dataset['Chance of Admit']\n"", 'ey']"
"ASSIGN = 28, 28 if keras.backend.image_data_format() == 'channels_first': ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], 1, img_rows, img_cols) ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], 1, img_rows, img_cols) ASSIGN = (1, img_rows, img_cols) else: ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], img_rows, img_cols, 1) ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], img_rows, img_cols, 1) ASSIGN = (img_rows, img_cols, 1)",0,"['img_rows, img_cols = 28, 28 # input image dimensions\n', '\n', ""if keras.backend.image_data_format() == 'channels_first':\n"", '    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n', '    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n', '    input_shape = (1, img_rows, img_cols)\n', 'else:\n', '    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n', '    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n', '    input_shape = (img_rows, img_cols, 1)']"
"ASSIGN = [file for file in os.listdir(JSON_DIR) if os.path.isfile(os.path.join(JSON_DIR, file))] for file in ASSIGN: ASSIGN = os.path.join(JSON_DIR, file) with open(ASSIGN) as src: ASSIGN = json.load(src) ASSIGN = data['ASSIGN'] ASSIGN = data['ASSIGN'] ASSIGN = np.array(data['ASSIGN']) ASSIGN = np.array(faces) - 1 ASSIGN = plt.figure(figsize=(16, 5)) ASSIGN = fig.add_subplot(1, 2, 1, projection='3d') ASSIGN.set_title('Model: {} | Type: {}'.format(file.split('.')[0], ASSIGN)) ASSIGN.set_xlim([-2, 3]) ASSIGN.set_ylim([-3, 2]) ASSIGN.set_zlim([0, 3]) ASSIGN.view_init(30, -50) ASSIGN.plot_trisurf(ASSIGN[:,0], ASSIGN[:,2], ASSIGN, -ASSIGN[:,1], shade=True, color='lime') ASSIGN = fig.add_subplot(1, 2, 2, projection='3d') ASSIGN.set_title('Model: {} | Type: {}'.format(file.split('.')[0], ASSIGN)) ASSIGN.set_xlim([-2, 3]) ASSIGN.set_ylim([-3, 2]) ASSIGN.set_zlim([0, 3]) ASSIGN.view_init(30, 40) ASSIGN.plot_trisurf(ASSIGN[:,0], ASSIGN[:,2], ASSIGN, -ASSIGN[:,1], shade=True, color='lime')",1,"['# Get all json files\n', 'files = [file for file in os.listdir(JSON_DIR) if os.path.isfile(os.path.join(JSON_DIR, file))]\n', '\n', '# For each json file, plot figure\n', 'for file in files:\n', '    model_path = os.path.join(JSON_DIR, file)\n', '    with open(model_path) as src:\n', '        data = json.load(src)\n', ""        car_type = data['car_type']\n"", ""        faces = data['faces']\n"", ""        vertices = np.array(data['vertices'])\n"", '        triangles = np.array(faces) - 1\n', '\n', '        fig = plt.figure(figsize=(16, 5))\n', ""        ax11 = fig.add_subplot(1, 2, 1, projection='3d')\n"", ""        ax11.set_title('Model: {} | Type: {}'.format(file.split('.')[0], car_type))\n"", '        ax11.set_xlim([-2, 3])\n', '        ax11.set_ylim([-3, 2])\n', '        ax11.set_zlim([0, 3])\n', '        ax11.view_init(30, -50)\n', ""        ax11.plot_trisurf(vertices[:,0], vertices[:,2], triangles, -vertices[:,1], shade=True, color='lime')\n"", '        \n', ""        ax12 = fig.add_subplot(1, 2, 2, projection='3d')\n"", ""        ax12.set_title('Model: {} | Type: {}'.format(file.split('.')[0], car_type))\n"", '        ax12.set_xlim([-2, 3])\n', '        ax12.set_ylim([-3, 2])\n', '        ax12.set_zlim([0, 3])\n', '        ax12.view_init(30, 40)\n', ""        ax12.plot_trisurf(vertices[:,0], vertices[:,2], triangles, -vertices[:,1], shade=True, color='lime')""]"
"CHECKPOINT ASSIGN = logisticRegr.ASSIGN(X_train_pca, y_train) print(ASSIGN)",0,"['score = logisticRegr.score(X_train_pca, y_train)\n', 'print(score)']"
SETUP ASSIGN = MinMaxScaler() ASSIGN = sacalar.fit_transform(train[digital_cols]) ASSIGN = sacalar.transform(test[digital_cols]),0,"['from sklearn.preprocessing import MinMaxScaler\n', '\n', 'sacalar = MinMaxScaler()\n', 'train_digital = sacalar.fit_transform(train[digital_cols])\n', 'test_digital = sacalar.transform(test[digital_cols])']"
"CHECKPOINT print('f1 score:') print(f1_score(test_data['hasWinner'].astype().values, cls_output['hasWinner'].values)) print('') print('mean squared error:') print(mean_squared_error(test_data['numOfWinners'].values, reg_output['numOfWinners'].values))",0,"[""print('f1 score:')\n"", 'print(f1_score(test_data[\'hasWinner\'].astype(""int"").values, cls_output[\'hasWinner\'].values))\n', '\n', ""print('')\n"", '\n', ""print('mean squared error:')\n"", ""print(mean_squared_error(test_data['numOfWinners'].values, reg_output['numOfWinners'].values))""]"
"ASSIGN = (data_features['BsmtFinType2'].isnull() & data_features['BsmtFinType1'].notnull()) data_features[ASSIGN][['BsmtFinType1','BsmtFinType2']]",0,"['#The last one \n', ""condition5 = (data_features['BsmtFinType2'].isnull() & data_features['BsmtFinType1'].notnull())\n"", ""data_features[condition5][['BsmtFinType1','BsmtFinType2']]\n"", '#From the data description ']"
"ASSIGN = pd.read_excel('..path', sheet_name=0,dtype=dtypes)",0,"['# source : https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/\n', ""df = pd.read_excel('../input/WA_Fn-UseC_-HR-Employee-Attrition.xlsx', sheet_name=0,dtype=dtypes)""]"
"CHECKPOINT ASSIGN = model18.predict(x_es) ASSIGN = mean_squared_error(Y_es, y_pred8, squared=False) ASSIGN = str(round(val, 4)) print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, ASSIGN)))",0,"['y_pred18 = model18.predict(x_es)\n', '\n', 'val = mean_squared_error(Y_es, y_pred8, squared=False)\n', 'val18 = str(round(val, 4))\n', '\n', ""print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred18)))\n""]"
ASSIGN = create_model(),0,['model = create_model()']
"CHECKPOINT ASSIGN = model.predict(test_x) ASSIGN = decode_predictions(preds, top=3)[0] print('Predicted:', ASSIGN)",0,"['# model prediction\n', 'preds = model.predict(test_x)\n', '# decode prediction\n', 'dec_preds =  decode_predictions(preds, top=3)[0]\n', ""print('Predicted:', dec_preds)""]"
"ASSIGN = np.array(df['ImageId']) ASSIGN = np.array(df['PredictionString']) ASSIGN = [ np.array(prediction_string.split(' ')).astype(np.float32).reshape(-1, 7) \ for prediction_string in prediction_strings ]",0,"[""image_ids = np.array(df['ImageId'])\n"", ""prediction_strings = np.array(df['PredictionString'])\n"", 'prediction_strings = [\n', ""    np.array(prediction_string.split(' ')).astype(np.float32).reshape(-1, 7) \\\n"", '    for prediction_string in prediction_strings\n', ']']"
CHECKPOINT ASSIGN = data_features.isnull().sum().sort_values(ascending = False) ASSIGN = ASSIGN[ASSIGN > 0] missing_data,0,"['missing_data = data_features.isnull().sum().sort_values(ascending = False)\n', 'missing_data = missing_data[missing_data > 0]\n', 'missing_data']"
"ASSIGN = final.drop(['Selling_Price'],axis = 'columns')",0,"[""X = final.drop(['Selling_Price'],axis = 'columns')""]"
"tf.keras.models.save_model( model, filepath, ASSIGN=True, ASSIGN=True, ASSIGN=None, ASSIGN=None, ASSIGN=None )",0,"['tf.keras.models.save_model(\n', '    model,\n', '    filepath,\n', '    overwrite=True,\n', '    include_optimizer=True,\n', '    save_format=None,\n', '    signatures=None,\n', '    options=None\n', ')']"
sns.pairplot(df),1,['sns.pairplot(df)']
"plotCorrelationMatrix(df2, 196)",1,"['plotCorrelationMatrix(df2, 196)']"
"ASSIGN = mnist.drop(['label'], axis=1) ASSIGN.head()",0,"['# Putting feature variable to X\n', ""X = mnist.drop(['label'], axis=1)\n"", '\n', 'X.head()']"
"CHECKPOINT ASSIGN = Sequential() ASSIGN.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', ASSIGN ='relu', input_shape = (28,28,1))) ASSIGN.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', ASSIGN ='relu')) ASSIGN.add(MaxPool2D(pool_size=(2,2))) ASSIGN.add(Dropout(0.25)) print(, ASSIGN.output_shape) ASSIGN.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', ASSIGN ='relu')) ASSIGN.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', ASSIGN ='relu')) ASSIGN.add(MaxPool2D(pool_size=(2,2), strides=(2,2))) ASSIGN.add(Dropout(0.25)) print(, ASSIGN.output_shape) ASSIGN.add(Flatten()) print(, ASSIGN.output_shape) ASSIGN.add(Dense(64, ASSIGN = ""relu"")) ASSIGN.add(Dropout(0.5)) ASSIGN.add(Dense(10, ASSIGN = ""softmax"")) ASSIGN.compile( ASSIGN='adam', ASSIGN='categorical_crossentropy', ASSIGN=['accuracy'] ) ASSIGN = ReduceLROnPlateau( ASSIGN='val_acc', ASSIGN=3, ASSIGN=1, ASSIGN=0.5, ASSIGN=0.00001 )",0,"['model = Sequential()\n', '\n', ""model.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n"", ""                 activation ='relu', input_shape = (28,28,1)))\n"", ""model.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n"", ""                 activation ='relu'))\n"", 'model.add(MaxPool2D(pool_size=(2,2)))\n', 'model.add(Dropout(0.25))\n', '\n', 'print(""model output shape: "", model.output_shape)\n', '\n', ""model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n"", ""                 activation ='relu'))\n"", ""model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n"", ""                 activation ='relu'))\n"", 'model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n', 'model.add(Dropout(0.25))\n', '\n', 'print(""model output shape: "", model.output_shape)\n', '\n', 'model.add(Flatten())\n', '\n', 'print(""model output shape: "", model.output_shape)\n', 'model.add(Dense(64, activation = ""relu""))\n', 'model.add(Dropout(0.5))\n', 'model.add(Dense(10, activation = ""softmax""))\n', '\n', 'model.compile(\n', ""    optimizer='adam',\n"", ""    loss='categorical_crossentropy',\n"", ""    metrics=['accuracy']\n"", ')\n', 'lr_reduction = ReduceLROnPlateau(\n', ""    monitor='val_acc',\n"", '    patience=3,\n', '    verbose=1,\n', '    factor=0.5,\n', '    min_lr=0.00001\n', ')']"
os.listdir('..path'),0,"[""os.listdir('../input/iwildcam-2019-fgvc6')""]"
"CHECKPOINT ASSIGN=data_risk2[:2000] ASSIGN=data_risk2_2000.Longitude.mean() ASSIGN=data_risk2_2000.Latitude.mean() ASSIGN=folium.Map([Lat,Long],zoom_start=12) ASSIGN=plugins.MarkerCluster().add_to(risk2_map) for lat,lon,label in zip(ASSIGN.Latitude,ASSIGN.Longitude,ASSIGN['AKA Name']): folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(ASSIGN) ASSIGN.add_child(ASSIGN) risk2_map",1,"['data_risk2_2000=data_risk2[:2000]\n', 'Long=data_risk2_2000.Longitude.mean()\n', 'Lat=data_risk2_2000.Latitude.mean()\n', 'risk2_map=folium.Map([Lat,Long],zoom_start=12)\n', '\n', 'risk2_distribution_map=plugins.MarkerCluster().add_to(risk2_map)\n', ""for lat,lon,label in zip(data_risk2_2000.Latitude,data_risk2_2000.Longitude,data_risk2_2000['AKA Name']):\n"", '    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(risk2_distribution_map)\n', 'risk2_map.add_child(risk2_distribution_map)\n', '\n', 'risk2_map']"
len(data_features),0,['len(data_features)']
"ASSIGN = pd.merge(ASSIGN,population,left_on = 'Country',right_on='Country',how='left') ASSIGN = pd.merge(ASSIGN,population,left_on = 'Country',right_on='Country',how = 'left')",0,"[""train = pd.merge(train,population,left_on = 'Country',right_on='Country',how='left')\n"", ""test = pd.merge(test,population,left_on = 'Country',right_on='Country',how = 'left')""]"
SETUP ASSIGN = pd.read_csv('path') ASSIGN = pd.read_csv('path'),0,"['from sklearn.ensemble import RandomForestClassifier\n', 'from sklearn.ensemble import RandomForestRegressor\n', '\n', ""train = pd.read_csv('/kaggle/input/titanic/train.csv')\n"", ""test = pd.read_csv('/kaggle/input/titanic/test.csv')""]"
"CHECKPOINT with pd.option_context('display.max_rows', None, 'display.max_columns', None): print(pred_df_with_species_name)",0,"[""with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n"", '    print(pred_df_with_species_name)']"
"sns.scatterplot(data = X, x = 'Age', y = '401K Savings')",1,"['#Inspect normalization\n', ""sns.scatterplot(data = X, x = 'Age', y = '401K Savings')""]"
SETUP,0,"['import numpy as np\n', 'import pandas as pd \n', 'import matplotlib.pyplot as plt\n', 'import os \n', 'import seaborn as sns\n', 'import geopandas as gpd\n', 'import folium\n', 'from folium import plugins\n', 'import datetime\n', 'import math']"
CHECKPOINT df_final2,0,"['#display the contents , it will have longitude and latitude now\n', 'df_final2']"
ASSIGN = StandardScaler() ASSIGN = scaler.fit_transform(ASSIGN) pd.DataFrame(ASSIGN).head(),0,"['scaler = StandardScaler()\n', '\n', 'X = scaler.fit_transform(X)\n', '\n', 'pd.DataFrame(X).head()']"
"ASSIGN = pd.read_csv('path') for i in range(len(ASSIGN)): if ASSIGN.iloc[i]['Province_State'] is np.NaN: ASSIGN.iloc[i,1] = ASSIGN.iloc[i,2] ASSIGN.rename(columns = {'Country_Region':'Country','Province_State':'Province'},inplace = True)",0,"[""original_test = pd.read_csv('/kaggle/input/covid19-global-forecasting-week-4/test.csv')\n"", 'for i in range(len(original_test)):\n', ""    if original_test.iloc[i]['Province_State'] is np.NaN:\n"", '        original_test.iloc[i,1] = original_test.iloc[i,2]\n', '        \n', ""original_test.rename(columns = {'Country_Region':'Country','Province_State':'Province'},inplace = True)""]"
"ASSIGN = plt.subplots(1,3,figsize=(20,5)) sns.countplot(y_train, ax=ax[0], palette=""Reds"") ax[0].set_title(""Train data"") sns.countplot(y_valid, ax=ax[1], palette=""Blues"") ax[1].set_title(""Dev data"") sns.countplot(y_test, ax=ax[2], palette=""Greens""); ax[2].set_title(""Test data"");",1,"['fig, ax = plt.subplots(1,3,figsize=(20,5))\n', 'sns.countplot(y_train, ax=ax[0], palette=""Reds"")\n', 'ax[0].set_title(""Train data"")\n', 'sns.countplot(y_valid, ax=ax[1], palette=""Blues"")\n', 'ax[1].set_title(""Dev data"")\n', 'sns.countplot(y_test, ax=ax[2], palette=""Greens"");\n', 'ax[2].set_title(""Test data"");']"
"plt.figure(figsize=(10,10)) sns.barplot(x=train.ClassId.value_counts().index,y=train.ClassId.value_counts()) plt.ylabel('') plt.xlabel('ClassId') plt.title(""Number of images for each class"",size=20)",1,"['plt.figure(figsize=(10,10))\n', 'sns.barplot(x=train.ClassId.value_counts().index,y=train.ClassId.value_counts())\n', ""plt.ylabel('')\n"", ""plt.xlabel('ClassId')\n"", 'plt.title(""Number of images for each class"",size=20)']"
pca_last.components_,0,['pca_last.components_']
"del train_transaction, train_identity, test_transaction, test_identity",0,"['del train_transaction, train_identity, test_transaction, test_identity']"
"ASSIGN = pd.read_csv('..path', index_col='TransactionID') ASSIGN = pd.read_csv('..path', index_col='TransactionID') ASSIGN = pd.read_csv('..path', index_col='TransactionID') ASSIGN = pd.read_csv('..path', index_col='TransactionID') ASSIGN = pd.read_csv('..path', index_col='TransactionID')",0,"[""train_transaction = pd.read_csv('../input/train_transaction.csv', index_col='TransactionID')\n"", ""test_transaction = pd.read_csv('../input/test_transaction.csv', index_col='TransactionID')\n"", ""train_identity = pd.read_csv('../input/train_identity.csv', index_col='TransactionID')\n"", ""test_identity = pd.read_csv('../input/test_identity.csv', index_col='TransactionID')\n"", ""sample_submission = pd.read_csv('../input/sample_submission.csv', index_col='TransactionID')""]"
"plt.figure(figsize=(18,70)) sns.barplot(x=train_transaction.isnull().sum().sort_values(ascending=False),y=train_transaction.isnull().sum().sort_values(ascending=False).index) plt.title(""counts of missing value for train_transaction"",size=20)",1,"['plt.figure(figsize=(18,70))\n', 'sns.barplot(x=train_transaction.isnull().sum().sort_values(ascending=False),y=train_transaction.isnull().sum().sort_values(ascending=False).index)\n', 'plt.title(""counts of missing value for train_transaction"",size=20)']"
len(data_2019),0,['len(data_2019)']
data.head(),0,['data.head()']
"SETUP ASSIGN = lgb.Dataset(X_train, y_train) ASSIGN = lgb.Dataset(X_test, y_test, reference=lgb_train) ASSIGN = { 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': {'binary_logloss', 'auc'}, 'num_leaves':240, 'max_depth': 15, 'min_data_in_leaf': 100, 'learning_rate': 0.05, 'feature_fraction': 0.95, 'bagging_fraction': 0.95, 'bagging_freq': 5, 'lambda_l1': 0, 'lambda_l2': 0, 'min_gain_to_split': 0.1, 'verbose': 0, 'is_unbalance': True }",0,"['import lightgbm as lgb  \n', 'import pickle  \n', 'from sklearn.metrics import roc_auc_score  \n', 'lgb_train = lgb.Dataset(X_train, y_train)  \n', 'lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train) \n', 'params = {  \n', ""    'boosting_type': 'gbdt',  \n"", ""    'objective': 'binary',  \n"", ""    'metric': {'binary_logloss', 'auc'},  \n"", ""    'num_leaves':240,  \n"", ""    'max_depth': 15,  \n"", ""    'min_data_in_leaf': 100,  \n"", ""    'learning_rate': 0.05,  \n"", ""    'feature_fraction': 0.95,  \n"", ""    'bagging_fraction': 0.95,  \n"", ""    'bagging_freq': 5,  \n"", ""    'lambda_l1': 0,    \n"", ""    'lambda_l2': 0, \n"", ""    'min_gain_to_split': 0.1,  \n"", ""    'verbose': 0,  \n"", ""    'is_unbalance': True  \n"", '}  ']"
"ASSIGN = Sequential() ASSIGN.add(Dense(100,input_shape=(X.shape[1],))) ASSIGN.add(Dense(100,activation='relu')) ASSIGN.add(Dense(100,activation='relu')) ASSIGN.add(Dense(3,activation='softmax')) ASSIGN.summary()",0,"['model = Sequential()\n', 'model.add(Dense(100,input_shape=(X.shape[1],)))\n', ""model.add(Dense(100,activation='relu'))\n"", ""model.add(Dense(100,activation='relu'))\n"", ""model.add(Dense(3,activation='softmax'))\n"", 'model.summary()']"
CHECKPOINT test,0,['test']
os.listdir('..path'),0,"[""os.listdir('../input/denver-crime-data')""]"
CHECKPOINT print(x_test.shape),0,['print(x_test.shape)']
