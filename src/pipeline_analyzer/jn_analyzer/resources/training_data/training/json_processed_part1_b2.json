{
    "content": [
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "overall_data = pd.read_csv('../input/guns.csv')\n",
                "suicide_data = overall_data[overall_data.intent == 'Suicide']\n",
                "homicide_data = overall_data[overall_data.intent == 'Homicide']"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "data_sources = [overall_data, suicide_data, homicide_data]\n",
                "titles = ['Overall', 'Suicide', 'Homicide']\n",
                "\n",
                "interested_columns = ['year', 'month', 'intent', 'sex', 'age', 'race', 'place', 'education']\n",
                "for i, d in enumerate(data_sources):\n",
                "    for col in interested_columns:\n",
                "        if col == 'age' or col == 'month':\n",
                "            d[col].value_counts().sort_index().plot(kind = 'line')\n",
                "        elif col == 'year' or col == 'education':\n",
                "            d[col].value_counts().sort_index().plot(kind = 'bar')\n",
                "        else:\n",
                "            d[col].value_counts().plot(kind = 'bar')\n",
                "        plt.title(titles[i] + ': ' + col)\n",
                "        plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "menu = pd.read_csv('../input/menu.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "menu.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "menu.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "menu.Category.value_counts()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "measures = ['Calories', 'Total Fat', 'Cholesterol','Sodium', 'Sugars', 'Carbohydrates']\n",
                "\n",
                "for m in measures:   \n",
                "    plot = sns.swarmplot(x=\"Category\", y=m, data=menu)\n",
                "    plt.setp(plot.get_xticklabels(), rotation=45)\n",
                "    plt.title(m)\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('Highest Calorie meal: {} \\n\\\n",
                "       Highest Fat meal: {} \\n\\\n",
                "       Highest Cholesterol meal: {} \\n\\\n",
                "       Highest Sodium meal: {} \\n\\\n",
                "       Highest Sugar meal: {} \\n\\\n",
                "       Highest Carb meal: {}'.format(menu.Item[menu['Calories'].idxmax()],\\\n",
                "                                     menu.Item[menu['Total Fat'].idxmax()],\\\n",
                "                                     menu.Item[menu['Cholesterol'].idxmax()],\\\n",
                "                                     menu.Item[menu['Sodium'].idxmax()],\\\n",
                "                                     menu.Item[menu['Sugars'].idxmax()],\\\n",
                "                                     menu.Item[menu['Carbohydrates'].idxmax()]))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import warnings\n",
                "import seaborn as sns\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "tb = pd.read_csv('../input/tobacco.csv')\n",
                "print(tb.head())"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tb.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "columns = ['Smoke everyday', 'Smoke some days', 'Former smoker', 'Never smoked']\n",
                "\n",
                "for x in columns:\n",
                "    tb[x] = tb[x].str.strip('%').astype('float')\n",
                "    "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tb.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "tb_group = tb.groupby(['Year'], as_index = False).mean()\n",
                "\n",
                "fig = plt.figure(figsize = (8,6))\n",
                "ax1 = fig.add_subplot(2,2,1)\n",
                "ax2 = fig.add_subplot(2,2,2)\n",
                "ax3 = fig.add_subplot(2,2,3)\n",
                "ax4 = fig.add_subplot(2,2,4)\n",
                "\n",
                "tb_group.head()\n",
                "\n",
                "y = 'Percentage of people'\n",
                "x = 'Year'\n",
                "\n",
                "ax1.set(title = 'Smoke everyday', ylabel = y, xlabel = x)\n",
                "ax2.set(title = 'Smoke some days', ylabel = y, xlabel = x)\n",
                "ax3.set(title = 'Former smoker', ylabel = y, xlabel = x)\n",
                "ax4.set(title = 'Never smoked', ylabel = y, xlabel = x)\n",
                "ax1.scatter(tb_group.Year, tb_group['Smoke everyday'], )\n",
                "ax2.scatter(tb_group.Year, tb_group['Smoke some days'])\n",
                "ax3.scatter(tb_group.Year, tb_group['Former smoker'])\n",
                "ax4.scatter(tb_group.Year, tb_group['Never smoked'])\n",
                "\n",
                "fig.tight_layout()\n",
                "fig.autofmt_xdate()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from scipy import stats\n",
                "\n",
                "states = set(tb.State)\n",
                "\n",
                "slope_dict = {}\n",
                "\n",
                "for state in states:\n",
                "    slope, intercept, r_value, p_value, std_err = stats.linregress(tb.Year[tb.State == state], tb['Never smoked'][tb.State == state])\n",
                "    slope_dict[state] = slope\n",
                "    \n",
                "slope_df = pd.DataFrame([slope_dict]).transpose()\n",
                "slope_df.columns = ['slope']\n",
                "slope_df.sort(columns = 'slope', ascending = True, inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "slope_dict1 = {}\n",
                "\n",
                "for state in states:\n",
                "    slope, intercept, r_value, p_value, std_err = stats.linregress(tb.Year[tb.State == state], tb['Smoke everyday'][tb.State == state])\n",
                "    slope_dict1[state] = slope\n",
                "    \n",
                "slope_df1 = pd.DataFrame([slope_dict1]).transpose()\n",
                "slope_df1.columns = ['slope']\n",
                "slope_df1.sort(columns = 'slope',ascending = False, inplace = True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "slope_df.plot(kind = 'bar', figsize = (10,6), title = 'Never Smoked: % Changes from 1994 to 2010')\n",
                "slope_df1.plot(kind = 'bar', figsize = (10,6), title = 'Smoke everyday: % Changes from 1994 to 2010')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "import seaborn as sns\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "vg_sales = pd.read_csv('../input/vgsales.csv')\n",
                "print(vg_sales.head())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "years = [2016, 2017, 2020]\n",
                "total_sales_group = vg_sales.groupby(['Year']).sum().drop(years)\n",
                "average_sales_group = vg_sales.groupby(['Year']).mean().drop(years)\n",
                "count_sales_group = vg_sales.replace(0, np.nan).groupby(['Year']).count().drop(years)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def lineplot(df, title = 'Sales by Year', ylabel ='Sales' , legendsize = 10, legendloc = 'upper left'):\n",
                "\n",
                "    year = df.index.values\n",
                "    na = df.NA_Sales\n",
                "    eu = df.EU_Sales\n",
                "    jp = df.JP_Sales\n",
                "    other = df.Other_Sales\n",
                "    global_ = df.Global_Sales\n",
                "    \n",
                "    if df is count_sales_group:\n",
                "        region_list = [na, eu, jp, other]\n",
                "        columns = ['NA', 'EU', 'JP', 'OTHER']\n",
                "    else:\n",
                "        region_list = [na, eu, jp, other, global_]\n",
                "        columns = ['NA', 'EU', 'JP', 'OTHER', 'WORLD WIDE']\n",
                "\n",
                "    for i, region in enumerate(region_list):\n",
                "        plt.plot(year, region, label = columns[i])\n",
                "\n",
                "    plt.ylabel(ylabel)\n",
                "    plt.xlabel('Year')\n",
                "    plt.title(title)\n",
                "    plt.legend(loc=legendloc, prop = {'size':legendsize})\n",
                "    plt.show()\n",
                "    plt.clf()\n",
                "\n",
                "    for i, region in enumerate(region_list):\n",
                "        plt.plot(year, region, label = columns[i])\n",
                "\n",
                "    plt.yscale('log')\n",
                "    plt.ylabel(ylabel)\n",
                "    plt.xlabel('Year')\n",
                "    plt.title(title + '(Log)')\n",
                "    plt.legend(loc=legendloc, prop = {'size':legendsize})\n",
                "    plt.show()\n",
                "    plt.clf()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "lineplot(total_sales_group, title = 'Sales by Year', ylabel ='Sales (In Millions)', legendsize = 8)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "japan1992_1996 = vg_sales[['Name', 'JP_Sales', 'NA_Sales']][(vg_sales.Year>=1992) & (vg_sales.Year<=1996) & (vg_sales.JP_Sales > vg_sales.NA_Sales)].sort(columns = 'JP_Sales', ascending = False)\n",
                "print(japan1992_1996.head(20))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "lineplot(average_sales_group, title = 'Average Revenue per Game per Year', ylabel ='Sales (In Millions)', legendsize = 8, legendloc = 'upper right')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "lineplot(count_sales_group, title = 'Of All Games Produced Did Every Region Sell Every Game?\\n\\nGames for Sale by Region by Year', ylabel ='Count', legendsize = 8, legendloc = 'upper left')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "Top_games = vg_sales[['Name', 'Year','Global_Sales']].sort(columns = 'Global_Sales', ascending = False)\n",
                "print(Top_games.head(20))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns \n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "data_mat = pd.read_csv('../input/student-mat.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_mat.dtypes"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_mat = data_mat[['address','studytime','failures','activities','nursery','higher','internet','famrel','freetime','absences','G3','romantic']]"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "binary_features = ['address','activities','nursery','higher','internet','romantic']\n",
                "for column in binary_features:\n",
                "    print(column,\"-\",data_mat[column].unique())"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "for column in binary_features:\n",
                "    if (column == 'address'):\n",
                "        data_mat[column] = data_mat[column].apply(lambda x: 1 if (x == 'U') else 0)\n",
                "    else:\n",
                "        data_mat[column] = data_mat[column].apply(lambda x: 1 if (x =='yes') else 0)\n",
                "    print(column,\"-\",data_mat[column].unique())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,15))\n",
                "sns.heatmap(data_mat.corr(),annot = True,fmt = \".2f\",cbar = True)\n",
                "plt.xticks(rotation=90)\n",
                "plt.yticks(rotation = 0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_mat = data_mat[data_mat.columns.drop(['address','activities','nursery','freetime'])]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(15,15))\n",
                "sns.heatmap(data_mat.corr(),annot = True,fmt = \".2f\",cbar = True)\n",
                "plt.xticks(rotation=90)\n",
                "plt.yticks(rotation = 0)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "absences = pd.get_dummies(data_mat['absences'], drop_first = True)\n",
                "failures = pd.get_dummies(data_mat['failures'],drop_first = True)\n",
                "studytime = pd.get_dummies(data_mat['studytime'],drop_first = True)\n",
                "\n",
                "data_mat.drop(['absences','failures','studytime'], axis =1, inplace = True)\n",
                "data_mat = pd.concat([data_mat,absences, failures,studytime],axis = 1)\n",
                "data_mat.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_matf = data_mat.drop('romantic', axis = 1)\n",
                "data_matl = data_mat['romantic']"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "check_results",
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "from sklearn.model_selection import KFold\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "kf=KFold(n_splits=10, shuffle=True, random_state=False)\n",
                "dtree = DecisionTreeClassifier()\n",
                "\n",
                "outcomesDt = []\n",
                "for train_id, test_id in kf.split(data_matf,data_matl):\n",
                "    X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]\n",
                "    y_train, y_test = data_matl.values[train_id], data_matl.values[test_id]\n",
                "    dtree.fit(X_train,y_train)\n",
                "    predictions = dtree.predict(X_test)\n",
                "    accuracy = accuracy_score(y_test, predictions)\n",
                "    outcomesDt.append(accuracy)\n",
                "plt.plot(range(10),outcomesDt)\n",
                "plt.show()\n",
                "average_error_Dt = np.mean(outcomesDt)\n",
                "print(\"the average error is equal to \",average_error_Dt)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "check_results",
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "Rf=RandomForestClassifier(n_estimators=10)\n",
                "outcomesRf=[]\n",
                "for train_id, test_id in kf.split(data_matf,data_matl):\n",
                "    X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]\n",
                "    y_train, y_test = data_matl.values[train_id], data_matl.values[test_id]\n",
                "    Rf.fit(X_train,y_train)\n",
                "    predictions = Rf.predict(X_test)\n",
                "    accuracy = accuracy_score(y_test, predictions)\n",
                "    outcomesRf.append(accuracy)\n",
                "plt.plot(range(10),outcomesRf)\n",
                "plt.show()\n",
                "print(\"the average error is equal to \",np.mean(outcomesRf))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results",
                "visualize_data"
            ],
            "source": [
                "ForestTreesPerformance = []\n",
                "for n_trees in range(1,11,1):\n",
                "    pRf=RandomForestClassifier(n_estimators=n_trees)\n",
                "    outcomesRfs=[]\n",
                "    for train_id, test_id in kf.split(data_matf,data_matl):\n",
                "        X_train, X_test = data_matf.values[train_id], data_matf.values[test_id]\n",
                "        y_train, y_test = data_matl.values[train_id], data_matl.values[test_id]\n",
                "        Rf.fit(X_train,y_train)\n",
                "        predictions = Rf.predict(X_test)\n",
                "        accuracy = accuracy_score(y_test, predictions)\n",
                "        outcomesRfs.append(accuracy)\n",
                "    ForestTreesPerformance.append(np.mean(outcomesRfs))\n",
                "plt.plot(range(1,11,1),ForestTreesPerformance)\n",
                "plt.show()\n",
                "if (min(ForestTreesPerformance) > average_error_Dt):\n",
                "    print(\"A decision tree works better than a random forest with respect to the probability error\")\n",
                "else:\n",
                "    print(\"A random forest with\",np.argmin(ForestTreesPerformance)+1,\"trees works better than a decision tree with respect to the probability error\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "# Setting up some global values and also plotting the data to get a feel for it.\n",
                "\n",
                "Xmin=np.min(X)\n",
                "Xmax=np.max(X)\n",
                "Ymin=np.min(Y)\n",
                "Ymax=np.max(Y)\n",
                "print(\"X min & max:\", Xmin, Xmax)\n",
                "print(\"Y min & max:\", Ymin, Ymax)\n",
                "\n",
                "\n",
                "# plt.xlim(Xmin-1,Xmax+1)\n",
                "plt.xlim(0,Xmax+1)\n",
                "plt.ylim(0,Ymax+1)\n",
                "\n",
                "plt.scatter(X,Y, 10, color = 'blue')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "X_s=np.empty_like(X, dtype=\"float64\")     # X_s  X standardised\n",
                "Y_s=np.empty_like(Y, dtype=\"float64\")     # Y_s  Y standardised\n",
                "\n",
                "X_s = (X - Xmin)/(Xmax-Xmin)\n",
                "Y_s = (Y - Ymin)/(Ymax-Ymin)\n",
                "\n",
                "\n",
                "for i in range(0, X.size):\n",
                "    print(\"i= \", i, \"\\t\\tx {:6.2f}\".format(X_s[i]),\"\\ty {:6.2f}\".format(Y_s[i]))\n",
                "\n",
                "\n",
                "\n",
                "plt.scatter(X_s,Y_s, 10, color = 'blue')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def CalculateNewY(X_orig, slope, intercept):\n",
                "    \n",
                "    Y_calc = np.empty_like(X_orig, dtype=\"float64\")\n",
                "\n",
                "    Y_calc = X_orig*slope+intercept\n",
                "        \n",
                "    return Y_calc"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def CalculateSSE(original_Y, predicted_Y):\n",
                "    theSSE=0.0\n",
                "    \n",
                "    for i in range(0, original_Y.size):\n",
                "        theSSE += (original_Y[i]-predicted_Y[i])**2\n",
                "        \n",
                "    theSSE = theSSE/2\n",
                "    \n",
                "    return theSSE"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Helper function\n",
                "\n",
                "def DrawLineFromFormula(slope, intercept, color):\n",
                "    plt.xlim(-0.05, 1.05)\n",
                "    plt.ylim(-0.05, 1.05)\n",
                "    x = np.arange(-100, 100, 0.1)\n",
                "    plt.plot(x, slope*x+intercept, color)\n",
                "    return"
            ]
        },
        {
            "tags": [
                "check_results",
                "visualize_data"
            ],
            "source": [
                "print(m, c)\n",
                "print(\"Ymin:\", Ymin)\n",
                "print(\"Y Range:\", Ymax - Ymin)\n",
                "\n",
                "c_final = (c * (Ymax - Ymin)) + Ymin\n",
                "\n",
                "print(\"m {:6.4f}\".format(m),\"\\t final c {:6.4f}\".format(c_final))\n",
                "\n",
                "# plt.xlim(-1,Xmax+1)\n",
                "# plt.ylim(2,Ymax+1)\n",
                "\n",
                "x = np.arange(0, Xmax+1, 0.1, dtype=\"float64\")\n",
                "y = np.empty_like(x, dtype=\"float64\") \n",
                "y = m*x + c_final\n",
                "\n",
                "# plt.plot(x, y, 'black')\n",
                "# Plot the original points and the final line.\n",
                "\n",
                "points = np.arange(Xmin-1, Xmax+1, 0.1)\n",
                "plt.plot(points, m*points+c_final, 'g--')\n",
                "\n",
                "\n",
                "plt.scatter(X,Y, 10, color = 'blue')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import csv\n",
                "import os"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.chdir('/kaggle/input/novel-corona-virus-2019-dataset/')\n",
                "os.listdir()"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "covid_data = pd.read_csv(\"covid_19_data.csv\")\n",
                "covid_data.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "states = covid_data['Province/State'].unique()\n",
                "countries = covid_data['Country/Region'].unique()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "covid_data['Country/Region'].value_counts()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# to get the last updated date of the dataset\n",
                "\n",
                "covid_data['Last Update'].unique()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "confirm_dict = {}\n",
                "deaths_dict = {}\n",
                "recover_dict = {}\n",
                "for country in countries:\n",
                "    country_data = covid_data[covid_data['Country/Region'] == country]\n",
                "    #cummulative, so we can simply take the latest date for final result\n",
                "    max_date = country_data['ObservationDate'].max()\n",
                "    sub = country_data[country_data['ObservationDate'] == max_date]\n",
                "    confirm = sub['Confirmed'].sum()\n",
                "    death = sub['Deaths'].sum()\n",
                "    recover = sub['Recovered'].sum()\n",
                "    \n",
                "    confirm_dict[country] = confirm\n",
                "    deaths_dict[country] = death\n",
                "    recover_dict[country] = recover\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "confirm_dict_sorted = sorted(confirm_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\n",
                "deaths_dict_sorted = sorted(deaths_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)\n",
                "recover_dict_sorted = sorted(recover_dict.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "top10_confirm = confirm_dict_sorted[:10]\n",
                "top10_deaths = deaths_dict_sorted[:10]\n",
                "top10_recover = recover_dict_sorted[:10]\n",
                "top10_confirm = dict(top10_confirm)\n",
                "top10_deaths = dict(top10_deaths)\n",
                "top10_recover = dict(top10_recover)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize = (7,6))\n",
                "bars = plt.bar(top10_confirm.keys(), top10_confirm.values())\n",
                "plt.xlabel('Country')\n",
                "plt.ylabel('Count')\n",
                "plt.title('Highest Confirmed Cases in 10 countries')\n",
                "plt.xticks(list(top10_confirm.keys()), rotation = 90)\n",
                "for bar in bars:\n",
                "    yval = bar.get_height()\n",
                "    plt.text(bar.get_x(), yval + .005, yval, rotation = 5)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize = (7,6))\n",
                "bars = plt.bar(top10_deaths.keys(), top10_deaths.values())\n",
                "plt.xlabel('Country')\n",
                "plt.ylabel('Count')\n",
                "plt.title('Highest Death Cases in 10 countries')\n",
                "plt.xticks(list(top10_deaths.keys()), rotation = 90)\n",
                "for bar in bars:\n",
                "    yval = bar.get_height()\n",
                "    plt.text(bar.get_x(), yval + .005, yval, rotation = 5)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize = (7,6))\n",
                "bars = plt.bar(top10_recover.keys(), top10_recover.values())\n",
                "plt.xlabel('Country')\n",
                "plt.ylabel('Count')\n",
                "plt.title('Highest Recovered Cases in 10 countries')\n",
                "plt.xticks(list(top10_recover.keys()), rotation = 90)\n",
                "for bar in bars:\n",
                "    yval = bar.get_height()\n",
                "    plt.text(bar.get_x(), yval + .005, yval, rotation = 5)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "china_data = covid_data[covid_data['Country/Region'] == 'Mainland China']\n",
                "date = []\n",
                "c = []\n",
                "d = []\n",
                "r = []\n",
                "for dat in china_data['ObservationDate'].unique():\n",
                "    sub = china_data[china_data['ObservationDate'] == dat]\n",
                "    confirm = sub['Confirmed'].sum()\n",
                "    death = sub['Deaths'].sum()\n",
                "    recover = sub['Recovered'].sum()\n",
                "    date.append(dat)\n",
                "    c.append(confirm)\n",
                "    d.append(death)\n",
                "    r.append(recover)\n",
                "    \n",
                "date = pd.Series(date)\n",
                "c  =pd.Series(c)\n",
                "d = pd.Series(d)\n",
                "r = pd.Series(r)\n",
                "\n",
                "t = [date.min(), date[len(date)//2], date.max()]\n",
                "plt.figure(figsize=(8,8))\n",
                "plt.plot(date, c, color = 'yellow')\n",
                "plt.plot(date, d, color = 'red')\n",
                "plt.plot(date, r, color = 'green')\n",
                "plt.xticks(t, t)\n",
                "plt.xlabel('Date')\n",
                "plt.ylabel('Cummulative Count cases')\n",
                "plt.title('Trend Curve of Confirmed Cases in China')\n",
                "plt.legend(['Confirmed', 'Death', 'Recovered'])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "italy_data = covid_data[covid_data['Country/Region'] == 'Italy']\n",
                "date = []\n",
                "c = []\n",
                "d = []\n",
                "r = []\n",
                "for dat in italy_data['ObservationDate'].unique():\n",
                "    sub = italy_data[italy_data['ObservationDate'] == dat]\n",
                "    confirm = sub['Confirmed'].sum()\n",
                "    death = sub['Deaths'].sum()\n",
                "    recover = sub['Recovered'].sum()\n",
                "    date.append(dat)\n",
                "    c.append(confirm)\n",
                "    d.append(death)\n",
                "    r.append(recover)\n",
                "\n",
                "date = pd.Series(date)\n",
                "c  =pd.Series(c)\n",
                "d = pd.Series(d)\n",
                "r = pd.Series(r)\n",
                "\n",
                "t = [date.min(), date[len(date)//2], date.max()]\n",
                "plt.figure(figsize=(8,8))\n",
                "plt.plot(date, c, color = 'yellow')\n",
                "plt.plot(date, d, color = 'red')\n",
                "plt.plot(date, r, color = 'green')\n",
                "plt.xticks(t, t)\n",
                "plt.xlabel('Date')\n",
                "plt.ylabel('Cummulative Count cases')\n",
                "plt.title('Trend Curve of Confirmed Cases in Italy')\n",
                "plt.legend(['Confirmed', 'Death', 'Recovered'])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "us_data = covid_data[covid_data['Country/Region'] == 'US']\n",
                "date = []\n",
                "c = []\n",
                "d = []\n",
                "r = []\n",
                "for dat in us_data['ObservationDate'].unique():\n",
                "    sub = us_data[us_data['ObservationDate'] == dat]\n",
                "    confirm = sub['Confirmed'].sum()\n",
                "    death = sub['Deaths'].sum()\n",
                "    recover = sub['Recovered'].sum()\n",
                "    date.append(dat)\n",
                "    c.append(confirm)\n",
                "    d.append(death)\n",
                "    r.append(recover)\n",
                "\n",
                "date = pd.Series(date)\n",
                "c  =pd.Series(c)\n",
                "d = pd.Series(d)\n",
                "r = pd.Series(r)\n",
                "\n",
                "t = [date.min(), date[len(date)//2], date.max()]\n",
                "plt.figure(figsize=(8,8))\n",
                "plt.plot(date, c, color = 'yellow')\n",
                "plt.plot(date, d, color = 'red')\n",
                "plt.plot(date, r, color = 'green')\n",
                "plt.xticks(t, t)\n",
                "plt.xlabel('Date')\n",
                "plt.ylabel('Cummulative Count cases')\n",
                "plt.title('Trend Curve of Confirmed Cases in US')\n",
                "plt.legend(['Confirmed', 'Death', 'Recovered'])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "germany_data = covid_data[covid_data['Country/Region'] == 'Germany']\n",
                "date = []\n",
                "c = []\n",
                "d = []\n",
                "r = []\n",
                "for dat in germany_data['ObservationDate'].unique():\n",
                "    sub = germany_data[germany_data['ObservationDate'] == dat]\n",
                "    confirm = sub['Confirmed'].sum()\n",
                "    death = sub['Deaths'].sum()\n",
                "    recover = sub['Recovered'].sum()\n",
                "    date.append(dat)\n",
                "    c.append(confirm)\n",
                "    d.append(death)\n",
                "    r.append(recover)\n",
                "\n",
                "date = pd.Series(date)\n",
                "c  =pd.Series(c)\n",
                "d = pd.Series(d)\n",
                "r = pd.Series(r)\n",
                "\n",
                "t = [date.min(), date[len(date)//2], date.max()]\n",
                "plt.figure(figsize=(8,8))\n",
                "plt.plot(date, c, color = 'yellow')\n",
                "plt.plot(date, d, color = 'red')\n",
                "plt.plot(date, r, color = 'green')\n",
                "plt.xticks(t, t)\n",
                "plt.xlabel('Date')\n",
                "plt.ylabel('Cummulative Count cases')\n",
                "plt.title('Trend Curve of Confirmed Cases in Germany')\n",
                "plt.legend(['Confirmed', 'Death', 'Recovered'])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "total_confirmed = sum(list(confirm_dict.values()))\n",
                "total_deaths = sum(list(deaths_dict.values()))\n",
                "total_recovered = sum(list(recover_dict.values()))\n",
                "\n",
                "total_still_affected = total_confirmed -(total_deaths+total_recovered)\n",
                "print(\"World Population affectedas of 22nd March 2020: \", total_confirmed)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "groups = ['Affected and Uncured', 'Deaths', 'Recovered']\n",
                "sizes = [total_still_affected, total_deaths, total_recovered]\n",
                "colours = ['Yellow', 'Red', 'Green']\n",
                "explode = (0, 0.2, 0)\n",
                "col_labels = ['Count']\n",
                "row_labels = ['Affected and Uncured', 'Deaths', 'Recovered']\n",
                "table_values = [[total_still_affected],[total_deaths], [total_recovered]]\n",
                "\n",
                "\n",
                "fig, axs = plt.subplots(1,2, figsize = (9,9))\n",
                "axs[0].axis('tight')\n",
                "axs[0].axis('off')\n",
                "the_table = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')\n",
                "the_table.set_fontsize(14)\n",
                "the_table.scale(1.5, 1.5)\n",
                "axs[1].pie(sizes, labels = groups, explode = explode, colors=colours, shadow=True, autopct='%1.1f%%')\n",
                "plt.title('Distribution at world level')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "china_c_number = confirm_dict['Mainland China']\n",
                "italy_c_number = confirm_dict['Italy']\n",
                "others_c = 0\n",
                "for key in confirm_dict:\n",
                "    if key != 'Mainland China' and key != 'Italy':\n",
                "        others_c+=confirm_dict[key]\n",
                "        \n",
                "groups = ['China', 'Italy', 'Others']\n",
                "sizes = [china_c_number, italy_c_number, others_c]\n",
                "colours = ['Red', 'Green', 'Grey']\n",
                "explode = (0.1, 0, 0)\n",
                "col_labels = ['Count']\n",
                "row_labels = ['China', 'Italy', 'Others']\n",
                "table_values = [[china_c_number], [italy_c_number], [others_c]]\n",
                "\n",
                "\n",
                "fig, axs = plt.subplots(1,2, figsize = (8,8))\n",
                "axs[0].axis('tight')\n",
                "axs[0].axis('off')\n",
                "the_table = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')\n",
                "the_table.set_fontsize(14)\n",
                "the_table.scale(1.5, 1.5)\n",
                "axs[1].pie(sizes, labels = groups, explode = explode, colors=colours, shadow=True, autopct='%1.1f%%')\n",
                "plt.title('Global Proportions of 2 severely striken countries')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "china_r_number = recover_dict['Mainland China']\n",
                "italy_r_number = recover_dict['Italy']\n",
                "others_r = 0\n",
                "for key in recover_dict:\n",
                "    if key != 'Mainland China' and key != 'Italy':\n",
                "        others_r+=recover_dict[key]\n",
                "        \n",
                "groups = ['China', 'Italy', 'Others']\n",
                "sizes = [china_r_number, italy_r_number, others_r]\n",
                "colours = ['Red', 'Green', 'Grey']\n",
                "explode = (0.1, 0, 0)\n",
                "col_labels = ['Count']\n",
                "row_labels = ['China', 'Italy', 'Others']\n",
                "table_values = [[china_r_number], [italy_r_number], [others_r]]\n",
                "\n",
                "\n",
                "fig, axs = plt.subplots(1,2, figsize = (8,8))\n",
                "axs[0].axis('tight')\n",
                "axs[0].axis('off')\n",
                "the_table = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')\n",
                "the_table.set_fontsize(14)\n",
                "the_table.scale(1.5, 1.5)\n",
                "axs[1].pie(sizes, labels = groups, explode = explode, colors=colours, shadow=True, autopct='%1.1f%%')\n",
                "plt.title('Global Proportions of 2 severely striken countries')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "italian_death_perc = (deaths_dict['Italy']/total_deaths)*100\n",
                "print('Death Percentage in Italy: ', italian_death_perc)\n",
                "\n",
                "china_death_perc = (deaths_dict['Mainland China']/total_deaths)*100\n",
                "print('Death Percentage in China: ', china_death_perc)\n",
                "\n",
                "print(total_deaths)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "china_d_number = deaths_dict['Mainland China']\n",
                "italy_d_number = deaths_dict['Italy']\n",
                "others_d = 0\n",
                "for key in deaths_dict:\n",
                "    if key != 'Mainland China' and key != 'Italy':\n",
                "        others_d+=deaths_dict[key]\n",
                "        \n",
                "groups = ['China', 'Italy', 'Others']\n",
                "sizes = [china_d_number, italy_d_number, others_d]\n",
                "colours = ['Red', 'Green', 'Grey']\n",
                "explode = (0.1, 0, 0)\n",
                "col_labels = ['Count']\n",
                "row_labels = ['China', 'Italy', 'Others']\n",
                "table_values = [[china_d_number], [italy_d_number], [others_d]]\n",
                "\n",
                "\n",
                "fig, axs = plt.subplots(1,2, figsize = (8,8))\n",
                "axs[0].axis('tight')\n",
                "axs[0].axis('off')\n",
                "the_table = axs[0].table(cellText=table_values,colWidths = [0.5], colLabels=col_labels, rowLabels = row_labels, loc='center')\n",
                "the_table.set_fontsize(14)\n",
                "the_table.scale(1.5, 1.5)\n",
                "axs[1].pie(sizes, labels = groups, explode = explode, colors=colours, shadow=True, autopct='%1.1f%%')\n",
                "plt.title('Global Proportions of 2 severely striken countries')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "# -*- coding: utf-8 -*-\n",
                "\"\"\"\n",
                "Created on Sat Sep  2 20:13:30 2017\n",
                "\n",
                "\"\"\"\n",
                "import numpy as np\n",
                "#计算hurst的值\n",
                "def hurst(data):\n",
                "    RS = list()\n",
                "    ARS = []\n",
                "    N = len(data)\n",
                "    ranges = [2,4,8,16,32,64]\n",
                "    L = N/np.array(ranges)\n",
                "    for i in range(len(ranges)):\n",
                "        for r in range(ranges[i]):\n",
                "            Range = data[int(L[i]*r):int(L[i]*(r+1))]\n",
                "            meanvalue = np.mean(Range)\n",
                "            Deviation = np.subtract(Range,meanvalue)\n",
                "            sigma = np.sqrt((sum(Deviation*Deviation))/(L[i]-1))\n",
                "            Deviation = Deviation.cumsum()\n",
                "            maxi = max(Deviation)\n",
                "            mini = min(Deviation)\n",
                "        RS.append((maxi-mini)/sigma)\n",
                "        ARS.append(np.mean(RS))\n",
                "    GAP = np.log(L)\n",
                "    a = np.log(ARS)\n",
                "    hurst_exponent = np.polyfit(GAP,a,1)[0]*2\n",
                "    return(hurst_exponent)\n",
                "#滚动窗口,N为窗口大小,\n",
                "def rolling(close_data,N):\n",
                "    hurst_value = close_data.rolling(window=N).apply(hurst)\n",
                "    return(hurst_value)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "#Import packages\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "#Load dataset\n",
                "data = pd.read_csv('../input/facebook-ads-2/Facebook_Ads_2.csv', encoding='latin-1')\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(\"The total number of users in this dataset is: \", len(data))\n",
                "print(\"The number of users who clicked through the ads is: \", len(data[data['Clicked']==1]))\n",
                "print(\"The number of users who did not click through the ads is: \", len(data[data['Clicked']==0]))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#Count by country\n",
                "data['Country'].value_counts()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#Distribution of time spent\n",
                "plt.figure(figsize=[12,5])\n",
                "fig, ax = plt.subplots(2)\n",
                "sns.distplot(data['Time Spent on Site'], ax = ax[0])\n",
                "sns.boxplot(x='Clicked', y='Time Spent on Site', data = data, ax = ax[1])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize = [12,5])\n",
                "sns.distplot(data[data['Clicked']==0]['Time Spent on Site'], label = 'Clicked==0')\n",
                "sns.distplot(data[data['Clicked']==1]['Time Spent on Site'], label = 'Clicked==1')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#Distribution of salary\n",
                "plt.figure(figsize = (12,5))\n",
                "fig, ax = plt.subplots(1,2)\n",
                "sns.distplot(data['Salary'], ax = ax[0])\n",
                "sns.boxplot(data = data, x = 'Clicked', y = 'Salary', ax = ax[1])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize = [12,5])\n",
                "sns.distplot(data[data['Clicked']==0]['Salary'], label = 'Clicked==0')\n",
                "sns.distplot(data[data['Clicked']==1]['Salary'], label = 'Clicked==1')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#Drop name, email, country\n",
                "data.drop(['Names','emails','Country'], axis = 1, inplace = True)\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#Check on missing data\n",
                "sns.heatmap(data.isnull(), yticklabels = False, cmap = 'Blues', cbar = False)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#Split data into dependent and independent variables\n",
                "X = data.drop(['Clicked'], axis = 1).values\n",
                "y = data['Clicked'].values"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "#Scale X\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "sc = StandardScaler()\n",
                "X = sc.fit_transform(X)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "#Split data into training and testing sets\n",
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "#Import and fit logistic regression mode\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "LR = LogisticRegression(random_state = 0)\n",
                "LR.fit(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "y_pred = LR.predict(X_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "#Import and run confusion matrix & classification report\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import classification_report\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "sns.heatmap(cm, annot = True, fmt = 'd')\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "#Import packages\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "data = pd.read_csv('../input/bank-customer-retirement/Bank_Customer_retirement.csv')\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.tail()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#Retirement\n",
                "print(\"The total number of customers in this dataset is: \", len(data))\n",
                "print(\"Out of those customers, the number of retired customers is: \", len(data[data['Retire']==1]))\n",
                "print(\"Out of those customers, the number of not retired customers is: \", len(data[data['Retire']==0]))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#Distribution of age\n",
                "sns.distplot(data[data['Retire']==1]['Age'], label = 'Retire == 1')\n",
                "sns.distplot(data[data['Retire']==0]['Age'], label = 'Retire == 0')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#Distribution of 401k savings\n",
                "sns.distplot(data[data['Retire']==1]['401K Savings'], label = 'Retire == 1')\n",
                "sns.distplot(data[data['Retire']==0]['401K Savings'], label = 'Retire == 0')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#Scatterplot\n",
                "sns.scatterplot(data = data, x = '401K Savings', y = 'Age', hue = 'Retire')"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#Drop customer ID\n",
                "data = data.drop(['Customer ID'], axis = 1)\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#Split into X and y\n",
                "X = data.drop(['Retire'], axis = 1)\n",
                "y = data['Retire']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#Normalize X\n",
                "X_min = X.min()\n",
                "X_range = (X - X_min).max()\n",
                "X_scaled = (X - X_min)/X_range"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#Inspect normalization\n",
                "sns.scatterplot(data = X, x = 'Age', y = '401K Savings')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.scatterplot(data = X_scaled, x = 'Age', y = '401K Savings')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "#Train test split\n",
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.25, random_state = 42)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "#Fit model\n",
                "from sklearn.svm import SVC\n",
                "svc_model = SVC()\n",
                "svc_model.fit(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "#Evaluate model\n",
                "y_pred = svc_model.predict(X_test)\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "source": [
                "sns.heatmap(confusion_matrix(y_test, y_pred), annot = True, fmt = 'd')\n",
                "plt.xlabel('Actual Value')\n",
                "plt.ylabel('Predicted Value')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from sklearn.model_selection import GridSearchCV"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "#fit gs\n",
                "gs.fit(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "gs.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "gs_1.fit(X_train, y_train)\n",
                "gs_1.best_params_"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "#predict based on gs\n",
                "y_pred_gs1= gs_1.predict(X_test)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "#evaluate gs results\n",
                "print(classification_report(y_test, y_pred_gs1))"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "sns.heatmap(confusion_matrix(y_test, y_pred_gs1), annot = True)\n",
                "plt.xlabel('Actual Value')\n",
                "plt.ylabel('Predicted Value')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "import os\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
                "from keras.models import Sequential                                   # model\n",
                "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D   # layers\n",
                "from keras.optimizers import RMSprop                                  # optimizer\n",
                "from keras.preprocessing.image import ImageDataGenerator              \n",
                "from keras.callbacks import ReduceLROnPlateau                         # callback function\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))                #list the input folder files"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "# create the training & test sets, skipping the header row with [1:]\n",
                "train = pd.read_csv(\"../input/train.csv\")\n",
                "test= pd.read_csv(\"../input/test.csv\")\n",
                "print('train datasets shape: ',train.shape)\n",
                "print('test datasets shape: ',test.shape)\n",
                "train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "y_label = train[\"label\"]\n",
                "x_train = train.drop(labels = [\"label\"], axis =1)\n",
                "del train\n",
                "print('x train datasets shape: ',x_train.shape)\n",
                "print('y label datasets shape: ',y_label.shape)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "x_train = x_train/255.0\n",
                "y_label = to_categorical(y_label, 10)\n",
                "print('x train datasets shape: ',x_train.shape)\n",
                "print('y label datasets shape: ',y_label.shape)\n",
                "x_train.head()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "x_train = x_train.values.reshape(-1, 28, 28, 1)\n",
                "print('x train datasets shape: ',x_train.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "n = [10, 100,1000]\n",
                "j = 0\n",
                "for i in n:\n",
                "    j+=1\n",
                "    plt.subplot(310+(j))\n",
                "    plt.imshow(x_train[i][:,:,0])\n",
                "    plt.title(\"this is {}\".format(y_label[i]))"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "model = Sequential()\n",
                "\n",
                "model.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n",
                "                 activation ='relu', input_shape = (28,28,1)))\n",
                "model.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n",
                "                 activation ='relu'))\n",
                "model.add(MaxPool2D(pool_size=(2,2)))\n",
                "model.add(Dropout(0.25))\n",
                "\n",
                "print(\"model output shape: \", model.output_shape)\n",
                "\n",
                "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
                "                 activation ='relu'))\n",
                "model.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n",
                "                 activation ='relu'))\n",
                "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
                "model.add(Dropout(0.25))\n",
                "\n",
                "print(\"model output shape: \", model.output_shape)\n",
                "\n",
                "model.add(Flatten())\n",
                "\n",
                "print(\"model output shape: \", model.output_shape)\n",
                "model.add(Dense(64, activation = \"relu\"))\n",
                "model.add(Dropout(0.5))\n",
                "model.add(Dense(10, activation = \"softmax\"))\n",
                "\n",
                "model.compile(\n",
                "    optimizer='adam',\n",
                "    loss='categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "lr_reduction = ReduceLROnPlateau(\n",
                "    monitor='val_acc',\n",
                "    patience=3,\n",
                "    verbose=1,\n",
                "    factor=0.5,\n",
                "    min_lr=0.00001\n",
                ")"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "history = model.fit(\n",
                "    x_train,\n",
                "    y_label,\n",
                "    batch_size=1000,\n",
                "    epochs=20,\n",
                "    validation_split=0.2,\n",
                "    verbose=2,\n",
                "    callbacks=[lr_reduction]\n",
                ")"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "history_dict = history.history\n",
                "loss_values = history_dict['loss']\n",
                "val_loss_values = history_dict['val_loss']\n",
                "acc_values = history_dict['acc']\n",
                "val_acc_values = history_dict['val_acc']\n",
                "epochs = range(1,len(loss_values)+1)\n",
                "plt.clf()\n",
                "plt.subplot(311)\n",
                "plt.plot(epochs, loss_values,'bo-')\n",
                "plt.plot(epochs, acc_values,'rs-')\n",
                "plt.xlabel('Iterations')\n",
                "plt.ylabel('Loss & Accuracy ')\n",
                "plt.title(\"For Train Data\")\n",
                "\n",
                "plt.subplot(313)\n",
                "plt.plot(epochs, val_loss_values,'bo-')\n",
                "plt.plot(epochs, val_acc_values,'rs-')\n",
                "plt.xlabel('Iterations')\n",
                "plt.ylabel('Loss & Accuracy')\n",
                "plt.title(\"For validation Data\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "from glob import glob as gb"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#list all the directories\n",
                "dirs=os.listdir(\"../input/zomato_data/\")\n",
                "dirs"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(dirs)"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "#storing all the files from every directory\n",
                "li=[]\n",
                "for dir1 in dirs:\n",
                "    files=os.listdir(r\"../input/zomato_data/\"+dir1)\n",
                "    #reading each file from list of files from previous step and creating pandas data fame    \n",
                "    for file in files:\n",
                "        \n",
                "        df_file=pd.read_csv(\"../input/zomato_data/\"+dir1+\"/\"+file,quotechar='\"',delimiter=\"|\")\n",
                "#appending the dataframe into a list\n",
                "        li.append(df_file.values)\n",
                "    \n",
                "    "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(li)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#numpys vstack method to append all the datafames to stack the sequence of input vertically to make a single array\n",
                "df_np=np.vstack(li)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#no of rows is represents the total no restaurants ,now of coloumns(12) is columns for the dataframe\n",
                "df_np.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#creating final dataframe from the numpy array\n",
                "df_final=pd.DataFrame(df_np)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#adding the header columns\n",
                "df_final=pd.DataFrame(df_final.values, columns =[\"NAME\",\"PRICE\",\"CUSINE_CATEGORY\",\"CITY\",\"REGION\",\"URL\",\"PAGE NO\",\"CUSINE TYPE\",\"TIMING\",\"RATING_TYPE\",\"RATING\",\"VOTES\"])\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#displaying the dataframe\n",
                "df_final"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#header column \"PAGE NO\" is not required ,i used it while scraping the data from zomato to do some sort of validation,lets remove the column\n",
                "df_final.drop(columns=[\"PAGE NO\"],axis=1,inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#display the dataframe again\n",
                "df_final"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#lets count how many unique cities are there \n",
                "\n",
                "df_final[\"CITY\"].unique()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(df_final[\"CITY\"].unique())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#lets check city wise restaurant counts and save it in ascending order\n",
                "\n",
                "city_vs_count=df_final[\"CITY\"].value_counts().sort_values(ascending=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "city_vs_count"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#lets check max count\n",
                "count_max=max(city_vs_count)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#lets find for city count is max\n",
                "\n",
                "for x,y in city_vs_count.items():\n",
                "    if(y==count_max):\n",
                "        print(x)\n",
                "    "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#lets find for city count is min\n",
                "\n",
                "min_count=min(city_vs_count)\n",
                "\n",
                "for x,y in city_vs_count.items():\n",
                "    if(y==min_count):\n",
                "        print(x)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#lets plot citywise restaurant count in barh form\n",
                "\n",
                "fig=plt.figure(figsize=(20,40))\n",
                "city_vs_count.plot(kind=\"barh\",fontsize=30)\n",
                "plt.grid(b=True, which='both', color='Black',linestyle='-')\n",
                "plt.ylabel(\"city names\",fontsize=50,color=\"red\",fontweight='bold')\n",
                "plt.title(\"CITY VS RESTAURANT COUNT GRAPH\",fontsize=50,color=\"BLUE\",fontweight='bold')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#lets plot citywise restaurant count in barh form,and each bar should display the count of the corresponding restuants for that city\n",
                "\n",
                "fig=plt.figure(figsize=(20,40))\n",
                "city_vs_count.plot(kind=\"barh\",fontsize=30)\n",
                "plt.grid(b=True, which='both', color='Black',linestyle='-')\n",
                "plt.ylabel(\"city names\",fontsize=50,color=\"red\",fontweight='bold')\n",
                "plt.title(\"CITY VS RESTAURANT COUNT GRAPH\",fontsize=50,color=\"BLUE\",fontweight='bold')\n",
                "for v in range(len(city_vs_count)):\n",
                "    #plt.text(x axis location ,y axis location ,text value ,other parameters......)\n",
                "    plt.text(v+city_vs_count[v],v,city_vs_count[v],fontsize=20,color=\"BLUE\",fontweight='bold')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "from glob import glob as gb"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#list all the directories\n",
                "dirs=os.listdir(\"../input/zomato_data/\")\n",
                "dirs"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(dirs)"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "#storing all the files from every directory\n",
                "li=[]\n",
                "for dir1 in dirs:\n",
                "    files=os.listdir(r\"../input/zomato_data/\"+dir1)\n",
                "    #reading each file from list of files from previous step and creating pandas data fame    \n",
                "    for file in files:\n",
                "        \n",
                "        df_file=pd.read_csv(\"../input/zomato_data/\"+dir1+\"/\"+file,quotechar='\"',delimiter=\"|\")\n",
                "#appending the dataframe into a list\n",
                "        li.append(df_file.values)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(li)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#numpys vstack method to append all the datafames to stack the sequence of input vertically to make a single array\n",
                "df_np=np.vstack(li)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#no of rows is represents the total no restaurants ,now of coloumns(12) is columns for the dataframe\n",
                "df_np.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#creating final dataframe from the numpy array\n",
                "df_final=pd.DataFrame(df_np)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#adding the header columns\n",
                "df_final=pd.DataFrame(df_final.values, columns =[\"NAME\",\"PRICE\",\"CUSINE_CATEGORY\",\"CITY\",\"REGION\",\"URL\",\"PAGE NO\",\"CUSINE TYPE\",\"TIMING\",\"RATING_TYPE\",\"RATING\",\"VOTES\"])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#displaying the dataframe\n",
                "df_final"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#header column \"PAGE NO\" is not required ,i used it while scraping the data from zomato to do some sort of validation,lets remove the column\n",
                "df_final.drop(columns=[\"PAGE NO\"],axis=1,inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#display the dataframe again\n",
                "df_final"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#lets count how many unique cities are there \n",
                "\n",
                "df_final[\"CITY\"].unique()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data"
            ],
            "source": [
                "# import json and requests library to use googl apis to get the longitude ant latituide values\n",
                "import requests\n",
                "import json\n",
                "\n",
                "#creating a separate array with all city names as elements of array\n",
                "city_name=df_final[\"CITY\"].unique()\n",
                "li1=[]\n",
                "\n",
                "#googlemap api calling url \n",
                "geo_s ='https://maps.googleapis.com/maps/api/geocode/json'\n",
                "#iterating through a for loop for each city names \n",
                "for i in range(len(city_name)):\n",
                "\n",
                "#i have used my own google map api, please use ypur own api     \n",
                " param = {'address': city_name[i], 'key': 'AIzaSyD-kYTK-8FQGueJqA2028t2YHbUX96V0vk'}\n",
                " \n",
                " response = requests.get(geo_s, params=param)\n",
                " \n",
                " response=response.text\n",
                "\n",
                " data=json.loads(response)\n",
                "\n",
                "#setting up the variable with corresponding city longitude and latitude\n",
                " lat=data[\"results\"][0][\"geometry\"][\"location\"][\"lat\"]\n",
                " lng=data[\"results\"][0][\"geometry\"][\"location\"][\"lng\"]\n",
                "\n",
                "#creating a new data frame with city , latitude and longitude as columns\n",
                " df2=pd.DataFrame([[city_name[i],lat,lng]])\n",
                " li1.append(df2.values)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#numpys vstack method to append all the datafames to stack the sequence of input vertically to make a single array\n",
                "df_np=np.vstack(li1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#creating a second dataframe with city name, latitude and longitude\n",
                "df_sec=pd.DataFrame(df_np,columns=[\"CITY\",\"lat\",\"lng\"])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#display the second dataframe contents\n",
                "df_sec"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#merge this data frame to the existing df_final data frame using merge and join features from pandas,and creating a new data frame\n",
                "df_final2=df_final.merge(df_sec,on=\"CITY\",how=\"left\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#display the contents , it will have longitude and latitude now\n",
                "df_final2"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#creating pandas series to hold the citynames and corresponding count of restuarnats in ascending order\n",
                "li2=df_final[\"CITY\"].value_counts().sort_values(ascending=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "li2"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#displaying the dictionary\n",
                "dc"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#creating another data frame from the above dictionary\n",
                "df_map=pd.DataFrame.from_dict(dc,orient=\"index\",columns=[\"CITY\",\"COUNT\"])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#displaying the data frame\n",
                "df_map"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#merging this data frame with df_sec data frame(which we created using city names,longitude and latitude)\n",
                "df_map_final=df_map.merge(df_sec,on=\"CITY\",how=\"left\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#displaying the new data frame this frame will be used for map ploting\n",
                "df_map_final"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "#importing the libraries for map ploting\n",
                "from matplotlib import cm\n",
                "from matplotlib.dates import date2num\n",
                "from mpl_toolkits.basemap import Basemap\n",
                "\n",
                "# for date and time processing\n",
                "import datetime"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#lets take one data frame for top 20 cities with most retaurants counts \n",
                "df_plot_top=df_map_final.tail(20)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#displaying the data frame\n",
                "df_plot_top"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#lets plot this inside the map corresponding to the cities exact co-ordinates which we received from google api \n",
                "#plt.subplots(figsize=(20,50))\n",
                "plt.figure(figsize=(50,60))\n",
                "map=Basemap(width=120000,height=900000,projection=\"lcc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n",
                "map.drawcountries()\n",
                "map.drawmapboundary(color='#f2f2f2')\n",
                "\n",
                "map.drawcoastlines()\n",
                "\n",
                "\n",
                "\n",
                "lg=np.array(df_plot_top[\"lng\"])\n",
                "lat=np.array(df_plot_top[\"lat\"])\n",
                "pt=np.array(df_plot_top[\"COUNT\"])\n",
                "city_name=np.array(df_plot_top[\"CITY\"])\n",
                "\n",
                "x,y=map(lg,lat)\n",
                "\n",
                "#using lambda function to create different sizes of marker as per thecount \n",
                "\n",
                "p_s=df_plot_top[\"COUNT\"].apply(lambda x: int(x)/2)\n",
                "\n",
                "#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is always blue.\n",
                "plt.scatter(x,y,s=p_s,marker=\"o\",c='BLUE')\n",
                "plt.title(\"TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#lets plot this inside the map corresponding to the cities exact co-ordinates which we received from google api ,here marker color will be different as per marker size\n",
                "#plt.subplots(figsize=(20,50))\n",
                "plt.figure(figsize=(50,60))\n",
                "map=Basemap(width=120000,height=900000,projection=\"lcc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n",
                "map.drawcountries()\n",
                "map.drawmapboundary(color='#f2f2f2')\n",
                "\n",
                "map.drawcoastlines()\n",
                "\n",
                "\n",
                "\n",
                "lg=np.array(df_plot_top[\"lng\"])\n",
                "lat=np.array(df_plot_top[\"lat\"])\n",
                "pt=np.array(df_plot_top[\"COUNT\"])\n",
                "city_name=np.array(df_plot_top[\"CITY\"])\n",
                "\n",
                "x,y=map(lg,lat)\n",
                "\n",
                "#using lambda function to create different sizes of marker as per thecount \n",
                "\n",
                "p_s=df_plot_top[\"COUNT\"].apply(lambda x: int(x)/2)\n",
                "\n",
                "#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is different.\n",
                "plt.scatter(x,y,s=p_s,marker=\"o\",c=p_s)\n",
                "plt.title(\"TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#lets plot with the city names inside the map corresponding to the cities exact co-ordinates which we received from google api ,here marker color will be different as per marker size\n",
                "#plt.subplots(figsize=(20,50))\n",
                "plt.figure(figsize=(50,60))\n",
                "map=Basemap(width=120000,height=900000,projection=\"lcc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n",
                "map.drawcountries()\n",
                "map.drawmapboundary(color='#f2f2f2')\n",
                "\n",
                "map.drawcoastlines()\n",
                "\n",
                "\n",
                "\n",
                "lg=np.array(df_plot_top[\"lng\"])\n",
                "lat=np.array(df_plot_top[\"lat\"])\n",
                "pt=np.array(df_plot_top[\"COUNT\"])\n",
                "city_name=np.array(df_plot_top[\"CITY\"])\n",
                "\n",
                "x,y=map(lg,lat)\n",
                "\n",
                "#using lambda function to create different sizes of marker as per thecount \n",
                "\n",
                "p_s=df_plot_top[\"COUNT\"].apply(lambda x: int(x)/2)\n",
                "\n",
                "#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is different.\n",
                "plt.scatter(x,y,s=p_s,marker=\"o\",c=p_s)\n",
                "\n",
                "for a,b ,c,d in zip(x,y,city_name,pt):\n",
                "    #plt.text takes x position , y position ,text ,font size and color as arguments\n",
                "    plt.text(a,b,c,fontsize=30,color=\"r\")\n",
                "   \n",
                "    \n",
                "    \n",
                "plt.title(\"TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#lets plot with the city names and restaurants count inside the map corresponding to the cities exact co-ordinates which we received from google api ,here marker color will be different as per marker size\n",
                "#plt.subplots(figsize=(20,50))\n",
                "plt.figure(figsize=(50,60))\n",
                "map=Basemap(width=120000,height=900000,projection=\"lcc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=28,lon_0=77)\n",
                "map.drawcountries()\n",
                "map.drawmapboundary(color='#f2f2f2')\n",
                "\n",
                "map.drawcoastlines()\n",
                "\n",
                "\n",
                "\n",
                "lg=np.array(df_plot_top[\"lng\"])\n",
                "lat=np.array(df_plot_top[\"lat\"])\n",
                "pt=np.array(df_plot_top[\"COUNT\"])\n",
                "city_name=np.array(df_plot_top[\"CITY\"])\n",
                "\n",
                "x,y=map(lg,lat)\n",
                "\n",
                "#using lambda function to create different sizes of marker as per thecount \n",
                "\n",
                "p_s=df_plot_top[\"COUNT\"].apply(lambda x: int(x)/2)\n",
                "\n",
                "#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is different.\n",
                "plt.scatter(x,y,s=p_s,marker=\"o\",c=p_s)\n",
                "\n",
                "for a,b ,c,d in zip(x,y,city_name,pt):\n",
                "    #plt.text takes x position , y position ,text(city name) ,font size and color as arguments\n",
                "    plt.text(a,b,c,fontsize=30,color=\"r\")\n",
                "    #plt.text takes x position , y position ,text(restaurant counts) ,font size and color as arguments, like above . but only i have changed the x and y position to make it more clean and easier to read\n",
                "    plt.text(a+60000,b+30000,d,fontsize=30)\n",
                "   \n",
                "    \n",
                "    \n",
                "plt.title(\"TOP 20 INDIAN CITIES RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#lets take one data frame for bottom 15 cities with minimum retaurants counts \n",
                "df_plot_bottom=df_map_final.head(15)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#displaying the data frame\n",
                "df_plot_bottom"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#lets plot with the city names and restaurants count inside the map corresponding to the cities exact co-ordinates which we received from google api ,here marker color will be different as per marker size\n",
                "#plt.subplots(figsize=(20,50))\n",
                "plt.figure(figsize=(50,50))\n",
                "map=Basemap(width=120000,height=900000,projection=\"tmerc\",resolution=\"l\",llcrnrlon=67,llcrnrlat=5,urcrnrlon=99,urcrnrlat=37,lat_0=20,lon_0=88)\n",
                "map.drawcountries()\n",
                "map.drawmapboundary(color='#f2f2f2')\n",
                "\n",
                "map.drawcoastlines()\n",
                "\n",
                "\n",
                "\n",
                "lg=np.array(df_plot_bottom[\"lng\"])\n",
                "lat=np.array(df_plot_bottom[\"lat\"])\n",
                "pt=np.array(df_plot_bottom[\"COUNT\"])\n",
                "city_name=np.array(df_plot_bottom[\"CITY\"])\n",
                "\n",
                "x,y=map(lg,lat)\n",
                "\n",
                "#using lambda function to create different sizes of marker as per thecount \n",
                "\n",
                "p_s=df_plot_bottom[\"COUNT\"].apply(lambda x: int(x)*50)\n",
                "\n",
                "#plt.scatter takes logitude ,latitude, marker size,shape,and color as parameter in the below , in this plot marker color is different.\n",
                "plt.scatter(x,y,s=p_s,marker=\"o\",c=p_s)\n",
                "\n",
                "for a,b ,c,d in zip(x,y,city_name,pt):\n",
                "    #plt.text takes x position , y position ,text(city name) ,font size and color as arguments\n",
                "    plt.text(a-3000,b,c,fontsize=30,color=\"r\")\n",
                "    #plt.text takes x position , y position ,text(restaurant counts) ,font size and color as arguments, like above . but only i have changed the x and y position to make it more clean and easier to read\n",
                "    plt.text(a+60000,b+30000,d,fontsize=30)\n",
                "   \n",
                "    \n",
                "    \n",
                "plt.title(\"BOTTOM 15 INDIAN CITIES MINIMUM RESTAURANT COUNTS PLOT AS PER ZOMATO\",fontsize=30,color='RED')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from fbprophet import Prophet"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "df2 = pd.read_csv('../input/covid19-global-forecasting-week-1/train.csv', sep = ',')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "## I will use train set only\n",
                "df2.tail()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "confirmed = df2.groupby('Date').sum()['ConfirmedCases'].reset_index()\n",
                "death = df2.groupby('Date').sum()['Fatalities'].reset_index()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "confirmed.columns = ['ds','y']\n",
                "confirmed['ds'] = pd.to_datetime(confirmed['ds'])"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "m = Prophet(interval_width=0.97)\n",
                "m.fit(confirmed)\n",
                "future = m.make_future_dataframe(periods=29)\n",
                "future_confirmed = future.copy() \n",
                "future.tail()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "forecast = m.predict(future)\n",
                "forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "forecast_components = m.plot_components(forecast)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "forecast1=pd.DataFrame(forecast)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "## These are the predictions for Confirmed Covid-19 cases until 2020 April 22\n",
                "forecastC= forecast1[['ds', 'yhat']]\n",
                "forecastC.columns = [['ForecastId', 'ConfirmedCases']]\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "death.columns = ['ds','y']\n",
                "death['ds'] = pd.to_datetime(death['ds'])"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "m = Prophet(interval_width=0.97)\n",
                "m.fit(death)\n",
                "future = m.make_future_dataframe(periods=29)\n",
                "future_deaths = future.copy() \n",
                "future.tail()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "forecastD = m.predict(future)\n",
                "forecastD[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "death_predict = pd.DataFrame(forecastD)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "## These are the predictions for Deaths by Covid-19 cases until 2020 April 24\n",
                "\n",
                "forecastDeath= death_predict[['ds', 'yhat']]\n",
                "forecastDeath.columns = [['ForecastId', 'Fatalities']]\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "death_forecast = m.plot(forecastD)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "submission = pd.merge(forecastC, forecastDeath, how='inner')"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submission.to_csv('submission.csv', index=False)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "tscovid = pd.read_csv('/kaggle/input/novel-corona-virus-2019-dataset/time_series_covid_19_deaths.csv', sep=',')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "artime = tscovid.iloc[0:405, 4:54]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "days = pd.date_range('2020/1/22', periods=50, freq='D')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "artime = artime.T"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "artime.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "artime = pd.DataFrame(artime)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df = tscovid.iloc[0:405, 4:54]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "total = df.sum(axis=0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "total"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "artime['Total'] = total"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "artime['Day'] = days"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "artime.set_index('Day', inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "artime['Total']"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from statsmodels.tsa.ar_model import AR,ARResults"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train = artime.iloc[:42]\n",
                "test = artime.iloc[42:]"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "model = AR(train['Total'])\n",
                "AR1fit = model.fit(maxlag=2,method='cmle')\n",
                "print(f'Lag: {AR1fit.k_ar}')\n",
                "print(f'Coefficients:\\n{AR1fit.params}')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "process_data"
            ],
            "source": [
                "start=len(train)\n",
                "end=len(train)+len(test)-1\n",
                "predictions1 = AR1fit.predict(start=start, end=end).rename('AR(1) Predictions')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "for i in range(len(predictions1)):\n",
                "    print(f\"predicted={predictions1[i]}, expected={test['Total'][i]}\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "test['Total'].plot(legend=True)\n",
                "predictions1.plot(legend=True,figsize=(12,6));"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "AR2fit = model.fit(maxlag=2,method='cmle')\n",
                "print(f'Lag: {AR2fit.k_ar}')\n",
                "print(f'Coefficients:\\n{AR2fit.params}')"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "source": [
                "start=len(train)\n",
                "end=len(train)+len(test)-1\n",
                "predictions2 = AR2fit.predict(start=start, end=end).rename('AR(2) Predictions')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "test['Total'].plot(legend=True)\n",
                "predictions1.plot(legend=True)\n",
                "predictions2.plot(legend=True,figsize=(12,6));"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "ARfit = model.fit(maxlag=2,method='cmle')\n",
                "print(f'Lag: {ARfit.k_ar}')\n",
                "print(f'Coefficients:\\n{ARfit.params}')"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "source": [
                "start = len(train)\n",
                "end = len(train)+len(test)-1\n",
                "rename = f'AR(12) Predictions'\n",
                "\n",
                "predictions11 = ARfit.predict(start=start,end=end).rename(rename)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "test['Total'].plot(legend=True)\n",
                "predictions1.plot(legend=True)\n",
                "predictions2.plot(legend=True)\n",
                "predictions11.plot(legend=True,figsize=(12,6));"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "labels = ['AR(1)','AR(2)','AR(11)']\n",
                "preds = [predictions1, predictions2, predictions11]  # these are variables, not strings!\n",
                "\n",
                "for i in range(3):\n",
                "    error = mean_squared_error(test['Total'], preds[i])\n",
                "    print(f'{labels[i]} Error: {error:11.10}')"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "model = AR(artime['Total'])\n",
                "\n",
                "# Next, fit the model\n",
                "ARfit = model.fit(maxlag=8)\n",
                "\n",
                "# Make predictions\n",
                "fcast = ARfit.predict(start=len(artime), end=len(artime)+20).rename('Forecast')\n",
                "\n",
                "# Plot the results\n",
                "artime['Total'].plot(legend=True)\n",
                "fcast.plot(legend=True, grid=True, figsize=(12,6));"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('Expectative for coronavirus deaths till April 01 is', fcast)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "import itertools\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "df = pd.read_excel('/kaggle/input/covid19/dataset.xlsx', index_col=0)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df.reset_index(drop=False, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.tail()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.isnull().sum()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# o número de células com valores NaN é muito grande. Exposição gráfica para melhor visualização\n",
                "plt.figure(figsize=(14,9))\n",
                "sns.heatmap(df.isnull(), cbar=False, yticklabels=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.notnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cols=df.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "relevant = df[cols[0:20]]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "relevant['Neutrophils'] = df['Neutrophils']"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "relevant.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "col = relevant.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "relevant.apply(falta_numero, axis=0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "relevant.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "relevant[col[6:21]] = relevant[col[6:21]].apply(normaliza)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "relevant[col[6:21]] = relevant[col[6:21]].fillna(9)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(14,9))\n",
                "sns.heatmap(relevant.isnull(), cbar=False, yticklabels=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "## Eliminamos todos os valores nulos ou negativos\n",
                "relevant.notnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "covid = relevant"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "covid.reset_index(drop=True, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "covid.tail()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "covid = covid.drop(['Patient ID'], axis=1, inplace=True)\n",
                "covid = relevant"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "covid = covid.drop(['Patient addmited to regular ward (1=yes, 0=no)'], axis=1, inplace=True)\n",
                "covid = relevant"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "covid = covid.drop(['Patient addmited to semi-intensive unit (1=yes, 0=no)'], axis=1, inplace=True)\n",
                "covid = relevant"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "covid = covid.drop(['Patient addmited to intensive care unit (1=yes, 0=no)'], axis=1, inplace=True)\n",
                "covid = relevant"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "covid.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# Verificando os valores da coluna target\n",
                "covid['SARS-Cov-2 exam result'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# convertendo celulas categóricas em numericas\n",
                "covid['SARS-Cov-2 exam result'] = pd.get_dummies(covid['SARS-Cov-2 exam result'])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "covid['SARS-Cov-2 exam result'].value_counts()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "# verificação final\n",
                "covid.tail()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(covid.drop('SARS-Cov-2 exam result',axis=1), covid['SARS-Cov-2 exam result'], test_size=0.25)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "logmodel = LogisticRegression()\n",
                "logmodel.fit(X_train,y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "predictions = logmodel.predict(X_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from sklearn.metrics import confusion_matrix"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "reg_cm=confusion_matrix(y_test,predictions)\n",
                "print(reg_cm)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def matriz_deconfusao(cm, target_names,\n",
                "                          title='Matriz de Confusão',\n",
                "                          cmap=None,\n",
                "                          normalize=True):\n",
                "    \n",
                "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
                "    misclass = 1 - accuracy\n",
                "\n",
                "    if cmap is None:\n",
                "        cmap = plt.get_cmap('Blues')\n",
                "\n",
                "    plt.figure(figsize=(8, 6))\n",
                "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
                "    plt.title(title)\n",
                "    plt.colorbar()\n",
                "\n",
                "    if target_names is not None:\n",
                "        tick_marks = np.arange(len(target_names))\n",
                "        plt.xticks(tick_marks, target_names, rotation=45)\n",
                "        plt.yticks(tick_marks, target_names)\n",
                "\n",
                "    if normalize:\n",
                "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "\n",
                "\n",
                "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
                "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
                "        if normalize:\n",
                "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
                "                     horizontalalignment=\"center\",\n",
                "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
                "        else:\n",
                "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
                "                     horizontalalignment=\"center\",\n",
                "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
                "\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.ylabel('True label')\n",
                "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "matriz_deconfusao(cm = reg_cm, normalize  = False,\n",
                "                      target_names = ['Positivo - COVID-19', 'Não Infectado'],\n",
                "                      title        = \"Matriz de Confusão\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "#generalization and overfitting\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline  "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def make_poly(X, deg):\n",
                "    n = len(X)\n",
                "    data = [np.ones(n)]\n",
                "    for d in range(deg):\n",
                "        data.append(X**(d+1))\n",
                "    return np.vstack(data).T\n",
                "\n",
                "def fit(X, Y):\n",
                "    return np.linalg.solve(X.T.dot(X), X.T.dot(Y))\n",
                "\n",
                "def fit_and_display(X, Y, sample, deg):\n",
                "    N = len(X)\n",
                "    train_idx = np.random.choice(N, sample)\n",
                "    Xtrain = X[train_idx]\n",
                "    Ytrain = Y[train_idx]\n",
                "    \n",
                "    plt.scatter(Xtrain, Ytrain)\n",
                "    plt.show()\n",
                "    \n",
                "    #fit poly\n",
                "    Xtrain_poly = make_poly(Xtrain, deg)\n",
                "    w = fit(Xtrain_poly, Ytrain)\n",
                "    \n",
                "    # display the polynomial\n",
                "    X_poly = make_poly(X, deg)\n",
                "    Y_hat = X_poly.dot(w)\n",
                "    plt.plot(X, Y)\n",
                "    plt.plot(X, Y_hat)\n",
                "    plt.scatter(Xtrain, Ytrain)\n",
                "    plt.title(\"deg = %d\" %deg)\n",
                "    plt.show()\n",
                "    \n",
                "#for deg in range(5,10):\n",
                " #   fit_and_display(X, Y, 10, deg)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def get_mse(Y, Yhat):\n",
                "    d = Y - Yhat\n",
                "    return d.dot(d) / len(d)\n",
                "\n",
                "def plot_train_vs_test_curves(X, Y, sample=20, max_deg=20):\n",
                "    N = len(X)\n",
                "    train_idx = np.random.choice(N, sample)\n",
                "    Xtrain = X[train_idx]\n",
                "    Ytrain = Y[train_idx]\n",
                "    \n",
                "    test_idx = [idx for idx in range(N) if idx not in train_idx]\n",
                "    Xtest = X[test_idx]\n",
                "    Ytest = Y[test_idx]\n",
                "    \n",
                "    mse_trains = []\n",
                "    mse_tests = []\n",
                "    for deg in range(max_deg+1):\n",
                "        Xtrain_poly = make_poly(Xtrain, deg)\n",
                "        w = fit(Xtrain_poly, Ytrain)\n",
                "        Yhat_train = Xtrain_poly.dot(w)\n",
                "        mse_train = get_mse(Ytrain, Yhat_train)\n",
                "\n",
                "        Xtest_poly = make_poly(Xtest, deg)\n",
                "        Yhat_test = Xtest_poly.dot(w)\n",
                "        mse_test = get_mse(Ytest, Yhat_test)\n",
                "\n",
                "        mse_trains.append(mse_train)\n",
                "        mse_tests.append(mse_test)\n",
                "\n",
                "    plt.plot(mse_trains, label=\"train mse\")\n",
                "    plt.plot(mse_tests, label=\"test mse\")\n",
                "    plt.legend()\n",
                "    plt.show()\n",
                "\n",
                "    plt.plot(mse_trains, label=\"train mse\")\n",
                "    plt.legend()\n",
                "    plt.show()\n",
                "        "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "if __name__ == \"__main__\":\n",
                "    # make up some data and plot it\n",
                "    N = 100\n",
                "    X = np.linspace(0, 6*np.pi, N)\n",
                "    Y = np.sin(X)\n",
                "\n",
                "    plt.plot(X, Y)\n",
                "    plt.show()\n",
                "\n",
                "    for deg in (5, 6, 7, 8, 9):\n",
                "        fit_and_display(X, Y, 10, deg)\n",
                "    plot_train_vs_test_curves(X, Y)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# credits.csv has 45476 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df1 = pd.read_csv('../input/credits.csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'credits.csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df1.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# keywords.csv has 46419 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df2 = pd.read_csv('../input/keywords.csv', delimiter=',', nrows = nRowsRead)\n",
                "df2.dataframeName = 'keywords.csv'\n",
                "nRow, nCol = df2.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df2.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotPerColumnDistribution(df2, 10, 5)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# links_small.csv has 9125 rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df3 = pd.read_csv('../input/links_small.csv', delimiter=',', nrows = nRowsRead)\n",
                "df3.dataframeName = 'links_small.csv'\n",
                "nRow, nCol = df3.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df3.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotPerColumnDistribution(df3, 10, 5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotCorrelationMatrix(df3, 8)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plotScatterMatrix(df3, 9, 10)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "1\n",
                "import pandas as pd"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "data=pd.read_csv('../input/flight-route-database/routes.csv')\n",
                "nan=data[data.isnull().any(axis=1)].head()\n",
                "nan.iloc[0:3]"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "data2=data.fillna(0)\n",
                "data2.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "2\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "team=['CSK','KKR','DC','MI']\n",
                "score=[149,218,188,143]\n",
                "plt.bar(team,score,color=['gold','purple','gold','gold'])\n",
                "plt.title('IPL TEAM SCORE GRAPH')\n",
                "plt.xlabel('TEAMS')\n",
                "plt.ylabel('SCORE')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "3\n",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "a1=np.array([5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100])\n",
                "a2=np.array([2,5,6,10,80])\n",
                "temp=[m for m, val in enumerate(a1) if val in set(a2)]\n",
                "new_arr=np.delete(a1,temp)\n",
                "print(\"ARRAY 1:\",a1)\n",
                "print(\"ARRAY 2:\",a2)\n",
                "print(\"NEW ARRAY:\",new_arr)\n",
                "print(\"ARRAY 2:\",a2)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "4\n",
                "import pandas as pd\n",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.model_selection import cross_val_predict\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import f1_score\n",
                "\n",
                "train = pd.read_csv(\"../input/titanic/train_data.csv\")\n",
                "\n",
                "\n",
                "X = train.drop(\"Survived\",axis=1)\n",
                "y = train[\"Survived\"]\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
                "\n",
                "logmodel = LogisticRegression()\n",
                "logmodel.fit(X_train,y_train)\n",
                "\n",
                "predictions = logmodel.predict(X_test)\n",
                "\n",
                "print(\"F1 Score:\",f1_score(y_test, predictions))\n",
                " \n",
                "print(\"\\nConfusion Matrix(below):\\n\")\n",
                "confusion_matrix(y_test, predictions)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "bg=['O+','A+','B+','AB+','O-','A-','B-','AB-']\n",
                "no=[12,11,2,15,22,14,34,21]\n",
                "plt.bar(bg,no,color=['red','red','red','red','black','red','red','red'])\n",
                "plt.title('BLOOD GROUP')\n",
                "plt.xlabel('X-axis')\n",
                "plt.ylabel(\"Y-axis\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.pie(mrks,labels=slc,explode=(0,0,0,0,0,0.1))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import random as rn"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "ht"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "ht[10]=172\n",
                "ht[11]=172\n",
                "ht[22]=2\n",
                "ht[49]=2"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.boxplot(ht)\n",
                "plt.title('Height')\n",
                "plt.xlabel('X-axis')\n",
                "plt.ylabel('Y-axis')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "from sklearn.datasets import load_breast_cancer\n",
                "cancer_data = load_breast_cancer()\n",
                "cancer_data.keys()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "CC = pd.DataFrame(cancer_data['data'],columns=cancer_data['feature_names'])\n",
                "CC.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import StandardScaler\n",
                "scaler = StandardScaler()\n",
                "scaler.fit(CC)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "scaled_data = scaler.transform(CC)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.decomposition import PCA\n",
                "pca = PCA(n_components=2)\n",
                "pca.fit(scaled_data)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x_pca = pca.transform(scaled_data)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "scaled_data.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "x_pca.shape"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(8,6))\n",
                "plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'])\n",
                "plt.xlabel('First principal component')\n",
                "plt.ylabel('Second Principal Component')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "data=pd.read_csv('../input/iris/Iris.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.dtypes"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data.iloc[:3]"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import numpy as np\n",
                "from scipy import sparse\n",
                "sz = int(input(\"Enter the size of the matrix here:-\"))\n",
                "arr = np.eye(sz)\n",
                "print(\"NumPy array:\", arr)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.count()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.keys()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.columns"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.describe()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "data=pd.read_csv('../input/iris/Iris.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.columns"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.iloc[0:10]#selecting 6 cloumns and 10 rows"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import random as rn\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.boxplot(ht)\n",
                "plt.title('Height')\n",
                "plt.xlabel('X-axis')\n",
                "plt.ylabel('Y-axis')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.loc[data.SepalWidthCm.isnull()]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data#total no. of observations"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df=pd.DataFrame({'Students':['A','B','C','D'],'Subjects':['DSD','AI/ML','COA','ALGORITHMS'],'Marks':[50,90,80,80]})"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "data=pd.read_csv('../input/iris/Iris.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.iloc[:3]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.loc[:3,'SepalLengthCm']"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "data2=pd.read_csv('../input/heart-failure-clinical-data/heart_failure_clinical_records_dataset.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data2.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data2.loc[data2.serum_creatinine.isnull()]"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# Other Imports\n",
                "import json\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import os\n",
                "import pandas as pd\n",
                "import plotly.graph_objs as go\n",
                "import plotly.offline as py\n",
                "import seaborn as sns\n",
                "\n",
                "# Pytorch Imports\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "import torch.utils.data as data\n",
                "import torchvision\n",
                "from torchvision import transforms\n",
                "import torchvision.models as models\n",
                "\n",
                "print(os.listdir('../input'))\n",
                "%matplotlib inline\n",
                "py.init_notebook_mode(connected=False)\n",
                "sns.set(rc={'figure.figsize':(20,10)})\n",
                "\n",
                "%env JOBLIB_TEMP_FOLDER=/tmp"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "CAT_TO_NAME_PATH = '../input/hackathon-blossom-flower-classification/cat_to_name.json'\n",
                "TRAIN_DATA_PATH = \"../input/hackathon-blossom-flower-classification/flower_data/flower_data/train\"\n",
                "VAL_DATA_PATH = \"../input/hackathon-blossom-flower-classification/flower_data/flower_data/valid\"\n",
                "TEST_DATA_PATH = '../input/hackathon-blossom-flower-classification/test set/'\n",
                "CHECKPOINT_PATH = '../input/model-checkpoints/'\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "def get_cat_to_name_data(file_path):\n",
                "    \"\"\" Imports the cat_to_name.json file and returns a Pandas DataFrame \"\"\"\n",
                "    with open(file_path, 'r') as f:\n",
                "        cat_to_names = json.load(f)\n",
                "    return cat_to_names"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cat_to_names = get_cat_to_name_data(CAT_TO_NAME_PATH)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cat_to_names"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "for i in cat_to_names:\n",
                "    if i == '11':\n",
                "        print(cat_to_names['11'])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def get_data_loaders(train_data_path, val_data_path):\n",
                "    transform = transforms.Compose([\n",
                "        transforms.Resize(256),\n",
                "        transforms.CenterCrop(256),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
                "        ])\n",
                "\n",
                "    train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
                "    train_loader = data.DataLoader(train_data, batch_size=32, shuffle=True,  num_workers=4)\n",
                "    val_data = torchvision.datasets.ImageFolder(root=val_data_path, transform=transform)\n",
                "    val_loader  = data.DataLoader(val_data, batch_size=32, shuffle=True, num_workers=4) \n",
                "    \n",
                "    train_class_names = train_data.classes\n",
                "    val_class_names = val_data.classes\n",
                "    \n",
                "    return train_loader, val_loader, train_class_names, val_class_names"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_loader, val_loader, train_class_names, val_class_names = get_data_loaders(TRAIN_DATA_PATH, VAL_DATA_PATH)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def create_model():\n",
                "    model = models.densenet161(pretrained=True)\n",
                "\n",
                "    for param in model.parameters():\n",
                "        param.requires_grad = False\n",
                "\n",
                "    num_filters = model.classifier.in_features\n",
                "    model.classifier = nn.Sequential(nn.Linear(num_filters, 2048),\n",
                "                               nn.ReLU(),\n",
                "                               nn.Linear(2048, 512),\n",
                "                               nn.ReLU(),\n",
                "                               nn.Linear(512, 102),\n",
                "                               nn.LogSoftmax(dim=1))\n",
                "\n",
                "    # Move model to the device specified above\n",
                "    model.to(device)\n",
                "    return model"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model = create_model()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "criterion = nn.NLLLoss()\n",
                "# Set the optimizer function using torch.optim as optim library\n",
                "optimizer = optim.Adam(model.parameters())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "def train(epochs):\n",
                "    train_losses = []\n",
                "    valid_losses = []\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        train_loss = 0\n",
                "        val_loss = 0\n",
                "        accuracy = 0\n",
                "\n",
                "        # Training the model\n",
                "        model.train()\n",
                "        counter = 0\n",
                "        for inputs, labels in train_loader:\n",
                "            # Move to device\n",
                "            inputs, labels = inputs.to(device), labels.to(device)\n",
                "            # Clear optimizers\n",
                "            optimizer.zero_grad()\n",
                "            # Forward pass\n",
                "            output = model.forward(inputs)\n",
                "            # Loss\n",
                "            loss = criterion(output, labels)\n",
                "            # Calculate gradients (backpropogation)\n",
                "            loss.backward()\n",
                "            # Adjust parameters based on gradients\n",
                "            optimizer.step()\n",
                "            # Add the loss to the training set's rnning loss\n",
                "            train_loss += loss.item()\n",
                "\n",
                "            # Print the progress of our training\n",
                "            counter += 1\n",
                "            #print(counter, \"/\", len(train_loader))\n",
                "\n",
                "            # Evaluating the model\n",
                "        model.eval()\n",
                "        counter = 0\n",
                "        # Tell torch not to calculate gradients\n",
                "        with torch.no_grad():\n",
                "            for inputs, labels in val_loader:\n",
                "                # Move to device\n",
                "                inputs, labels = inputs.to(device), labels.to(device)\n",
                "                # Forward pass\n",
                "                output = model.forward(inputs)\n",
                "                # Calculate Loss\n",
                "                valloss = criterion(output, labels)\n",
                "                # Add loss to the validation set's running loss\n",
                "                val_loss += valloss.item()\n",
                "\n",
                "                # Since our model outputs a LogSoftmax, find the real \n",
                "                # percentages by reversing the log function\n",
                "                output = torch.exp(output)\n",
                "                # Get the top class of the output\n",
                "                top_p, top_class = output.topk(1, dim=1)\n",
                "                # See how many of the classes were correct?\n",
                "                equals = top_class == labels.view(*top_class.shape)\n",
                "                # Calculate the mean (get the accuracy for this batch)\n",
                "                # and add it to the running accuracy for this epoch\n",
                "                accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
                "\n",
                "                # Print the progress of our evaluation\n",
                "                counter += 1\n",
                "                #print(counter, \"/\", len(val_loader))\n",
                "\n",
                "        # Get the average loss for the entire epoch\n",
                "        train_loss = train_loss/len(train_loader)\n",
                "        valid_loss = val_loss/len(val_loader)\n",
                "\n",
                "        train_losses.append(train_loss)\n",
                "        valid_losses.append(valid_loss)\n",
                "        # Print out the information\n",
                "        print('Accuracy: ', accuracy/len(val_loader))\n",
                "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
                "\n",
                "    return train_losses, valid_losses\n"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "def save_checkpoint():\n",
                "    checkpoint = {'model': model,\n",
                "                  'state_dict': model.state_dict(),\n",
                "                  'optimizer' : optimizer.state_dict()}\n",
                "\n",
                "    torch.save(checkpoint, 'checkpoint1.pth')"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "def predict(test_loader):\n",
                "    model.eval()\n",
                "    \n",
                "    predictions = []\n",
                "    with torch.no_grad():\n",
                "        for images, _ in test_loader:\n",
                "            images = images.to(device)\n",
                "            output = model(images)\n",
                "            ps = torch.exp(output)\n",
                "            top_p, top_class = ps.topk(1, dim=1)\n",
                "            predictions += [int(i) for i in list(top_class.data.cpu().numpy())]\n",
                "        \n",
                "    return predictions"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "predictions = predict(test_loader)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "predictions"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "pred_df_with_species_name, pred_df_with_cat_number = create_pred_dataframe(image_names, predictions)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "pred_df_with_species_name = pred_df_with_species_name.sort_values(by=['image-names'])\n",
                "pred_df_with_cat_number = pred_df_with_cat_number.sort_values(by=['image-names'])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
                "    print(pred_df_with_cat_number)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
                "    print(pred_df_with_species_name)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "pred_df_with_species_name.to_csv('my_predictions.csv', sep='\\t', encoding='utf-8')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.image as mpimg\n",
                "%matplotlib inline\n",
                "import numpy as np\n",
                "import os\n",
                "import pandas as pd\n",
                "import plotly.offline as py\n",
                "import plotly.graph_objs as go\n",
                "import seaborn as sns\n",
                "\n",
                "plt.rcParams['figure.figsize']=(20,10)\n",
                "print(os.listdir(\"../input\"))\n",
                "py.init_notebook_mode(connected=False)\n",
                "\n",
                "%env JOBLIB_TEMP_FOLDER=/tmp"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "df = pd.read_csv('../input/labels.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df['breed'].describe()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "temp = pd.DataFrame({'breed': df['breed'].value_counts().index, 'instances': df['breed'].value_counts().values})\n",
                "temp = temp.sort_values(by=['breed'])\n",
                "temp.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "trace = go.Bar(x=temp['breed'], y=temp['instances'])\n",
                "data = [trace]\n",
                "layout = go.Layout(\n",
                "        title='Breed Counts',\n",
                "        autosize=False,\n",
                "        width=5000,\n",
                "        height=500,\n",
                "        margin=dict(\n",
                "            l=100,\n",
                "            r=100,\n",
                "            b=100,\n",
                "            t=100\n",
                "        )\n",
                "    )\n",
                "fig = go.Figure(data=data, layout=layout)\n",
                "py.iplot(fig)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df['breed'] = pd.get_dummies(df['breed']).values.tolist()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from torchvision import transforms\n",
                "from torchvision.datasets import ImageFolder\n",
                "from torch.utils.data import DataLoader\n",
                "from torch.utils.data.dataset import Dataset\n",
                "from PIL import Image\n",
                "\n",
                "import torchvision.models as models\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df['image_path'] = '../input/train/' + df['id'].astype(str) + '.jpg'"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "labels = torch.tensor(df['breed'].tolist())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_dataset = CustomDataset(df['image_path'], labels, train=True)\n",
                "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=1)\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model = models.densenet161(pretrained=True)\n",
                "# Turn off training for their parameters\n",
                "for param in model.parameters():\n",
                "    param.requires_grad = False"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "classifier = nn.Sequential(nn.Linear(255, 1024),\n",
                "                           nn.ReLU(),\n",
                "                           nn.Linear(1024, 512),\n",
                "                           nn.ReLU(),\n",
                "                           nn.Linear(512, 120),\n",
                "                           nn.LogSoftmax(dim=1))\n",
                "# Replace default classifier with new classifier\n",
                "model.classifier = classifier"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import torch\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "# Move model to the device specified above\n",
                "model.to(device)\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "criterion = nn.NLLLoss()\n",
                "# Set the optimizer function using torch.optim as optim library\n",
                "optimizer = optim.Adam(model.classifier.parameters())"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import re\n",
                "import os\n",
                "%matplotlib inline\n",
                "\n",
                "\n",
                "import argparse\n",
                "import pickle\n",
                "\n",
                "import numpy as np; np.seterr(invalid='ignore')\n",
                "import pandas as pd"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(os.listdir('../input/'))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train = pd.read_csv('../input/web-traffic-time-series-forecasting/train_1.csv').fillna(0)\n",
                "train.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "def get_language(page):\n",
                "    res = re.search('[a-z][a-z].wikipedia.org', page)\n",
                "    if res:\n",
                "        return res[0][0:2]\n",
                "    return 'na'\n",
                "\n",
                "train['lang'] = train.Page.map(get_language)\n",
                "\n",
                "from collections import Counter\n",
                "\n",
                "print(Counter(train.lang))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "lang_sets = {}\n",
                "lang_sets['en'] = train[train.lang=='en'].iloc[:,0:-1]\n",
                "lang_sets['ja'] = train[train.lang=='ja'].iloc[:,0:-1]\n",
                "lang_sets['de'] = train[train.lang=='de'].iloc[:,0:-1]\n",
                "lang_sets['na'] = train[train.lang=='na'].iloc[:,0:-1]\n",
                "lang_sets['fr'] = train[train.lang=='fr'].iloc[:,0:-1]\n",
                "lang_sets['zh'] = train[train.lang=='zh'].iloc[:,0:-1]\n",
                "lang_sets['ru'] = train[train.lang=='ru'].iloc[:,0:-1]\n",
                "lang_sets['es'] = train[train.lang=='es'].iloc[:,0:-1]\n",
                "\n",
                "sums = {}\n",
                "for key in lang_sets:\n",
                "    sums[key] = lang_sets[key].iloc[:,1:].sum(axis=0) / lang_sets[key].shape[0]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "days = [r for r in range(sums['en'].shape[0])]\n",
                "\n",
                "fig = plt.figure(1, figsize=[10, 10])\n",
                "plt.ylabel('View Per Page')\n",
                "plt.xlabel('Day')\n",
                "plt.title('Pages in Different Languages')\n",
                "labels={'en':'English','ja':'Japanese','de':'German',\n",
                "        'na':'Media','fr':'French','zh':'Chinese',\n",
                "        'ru':'Russian','es':'Spanish'\n",
                "       }\n",
                "\n",
                "for key in sums:\n",
                "    plt.plot(days,sums[key],label = labels[key] )\n",
                "    \n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "print(\"Getting data...\")\n",
                "full = pd.read_csv('../input/web-traffic-time-series-forecasting/train_2.csv')\n",
                "full.iloc[:, 1:] = full.iloc[:, 1:].fillna(method='ffill', axis=1).fillna(\n",
                "        method='bfill', axis=1)\n",
                "datetime_list = pd.date_range(args.forecast_start, args.forecast_end)\n",
                "for datetime in datetime_list:\n",
                "    full[datetime.date().isoformat()] = 0\n",
                "\n",
                "print(\"Constructing test set...\")\n",
                "test = pd.melt(full[list(\n",
                "    full.columns[args.offset+1:args.offset+args.val_len+1])+['Page']],\n",
                "    id_vars='Page', var_name='Date', value_name=\"Visits\")\n",
                "test['Date'] = test['Date'].astype('datetime64[ns]')\n",
                "test['Weekend'] = test['Date'].dt.dayofweek >= 5\n",
                "\n",
                "print(\"Constructing train set...\")\n",
                "train = full.iloc[:, :args.offset+1]\n",
                "\n",
                "print(\"Getting medians...\")\n",
                "for i in args.windows:\n",
                "    print(i, end=' ')\n",
                "    val = 'MW'+str(i)\n",
                "    tmp = pd.melt(train[list(train.columns[-i:])+['Page']],\n",
                "                  id_vars='Page', var_name='Date', value_name=val)\n",
                "    tmp['Date'] = tmp['Date'].astype('datetime64[ns]')\n",
                "    tmp['Weekend']= tmp['Date'].dt.dayofweek >= 5           \n",
                "    tmp1 = tmp.groupby(['Page', 'Weekend']).median().reset_index()\n",
                "    test = test.merge(tmp1, how='left')\n",
                "print(\"\\n\")\n",
                "\n",
                "print(\"Getting median of medians...\")\n",
                "test['Predict'] = test[[\"MW7\", \"MW7\", \"MW14\", \"MW21\", \"MW35\", \"MW56\", \"MW91\",\n",
                "    \"MW147\", \"MW238\", \"MW385\", \"MW623\"]].median(axis=1)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(os.listdir('../working'))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import os\n",
                "import argparse\n",
                "import pickle\n",
                "import time\n",
                "\n",
                "import numpy as np; np.seterr(invalid='ignore')\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.autograd import Variable\n",
                "import torch.optim as optim\n",
                "from torch.optim.lr_scheduler import MultiStepLR\n",
                "from torch.utils.data import TensorDataset, DataLoader"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "class DenseLSTMForecast(nn.Module):\n",
                "    def __init__(self, hidden_size):\n",
                "        super(DenseLSTMForecast, self).__init__()\n",
                "        self.lstm1 = nn.LSTMCell(1, hidden_size)\n",
                "        self.lstm2 = nn.LSTMCell(hidden_size+1, hidden_size)\n",
                "        self.lstm3 = nn.LSTMCell(2*hidden_size+1, hidden_size)\n",
                "        self.linear = nn.Linear(3*hidden_size+1, 1)\n",
                "        self.hidden_size = hidden_size\n",
                "\n",
                "    def forward(self, x, future=0):\n",
                "        o = []\n",
                "        tt = torch.cuda if args.cuda else torch\n",
                "        h1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
                "        c1_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
                "        h2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
                "        c2_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
                "        h3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
                "        c3_t = Variable(tt.FloatTensor(x.size(0), self.hidden_size).zero_())\n",
                "        \n",
                "        for x_t in x.chunk(x.size(1), dim=1):\n",
                "            x_t = x_t.squeeze(dim=1)\n",
                "            h1_t, c1_t = self.lstm1(x_t, (h1_t, c1_t))\n",
                "            h1d_t = torch.cat([x_t, h1_t], dim=1)\n",
                "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
                "            h2d_t = torch.cat([x_t, h1_t, h2_t], dim=1)\n",
                "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
                "            h3d_t = torch.cat([x_t, h1_t, h2_t, h3_t], dim=1)\n",
                "            o_t = self.linear(h3d_t)\n",
                "            o.append(o_t)\n",
                "\n",
                "            \n",
                "        for i in range(future):\n",
                "            h1_t, c1_t = self.lstm1(o_t, (h1_t, c1_t))\n",
                "            h1d_t = torch.cat([o_t, h1_t], dim=1)\n",
                "            h2_t, c2_t = self.lstm2(h1d_t, (h2_t, c2_t))\n",
                "            h2d_t = torch.cat([o_t, h1_t, h2_t], dim=1)\n",
                "            h3_t, c3_t = self.lstm3(h2d_t, (h3_t, c3_t))\n",
                "            h3d_t = torch.cat([o_t, h1_t, h2_t, h3_t], dim=1)\n",
                "            o_t = self.linear(h3d_t)\n",
                "            o.append(o_t)\n",
                "\n",
                "        return torch.stack(o, dim=1)"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data",
                "transfer_results"
            ],
            "source": [
                "def get_data():\n",
                "    raw_data_file = os.path.join(args.intermediate_path, 'raw_data.pkl')\n",
                "    scaled_data_file = os.path.join(args.intermediate_path,\n",
                "                                    'scaled_data.pkl')\n",
                "    scaler_file = os.path.join(args.intermediate_path, 'scaler.pkl')\n",
                "    \n",
                "    if not args.read_from_file:\n",
                "        data_df = pd.read_csv(os.path.join(args.data_path, args.train_file),\n",
                "                              index_col='Page')\n",
                "        raw_data = data_df.values.copy()\n",
                "        data_df = data_df.fillna(method='ffill', axis=1).fillna(\n",
                "            method='bfill', axis=1)\n",
                "        data = np.nan_to_num(data_df.values.astype('float32'))\n",
                "        data = np.log1p(data)\n",
                "        scaler = StandardScaler()\n",
                "        scaler.fit(np.swapaxes(data, 0, 1))\n",
                "        scaled_data = scaler.transform(np.swapaxes(data, 0, 1))\n",
                "        scaled_data = np.swapaxes(scaled_data, 0, 1)\n",
                "        \n",
                "        with open(raw_data_file, 'wb') as f:\n",
                "            pickle.dump(raw_data, f)\n",
                "        with open(scaled_data_file, 'wb') as f:\n",
                "            pickle.dump(scaled_data, f)\n",
                "        with open(scaler_file, 'wb') as f:\n",
                "            pickle.dump(scaler, f)\n",
                "    else:\n",
                "        with open(raw_data_file, 'rb') as f:\n",
                "            raw_data = pickle.load(f)\n",
                "        with open(scaled_data_file, 'rb') as f:\n",
                "            scaled_data = pickle.load(f)\n",
                "        with open(scaler_file, 'rb') as f:\n",
                "            scaler = pickle.load(f)\n",
                "    return raw_data, scaled_data, scaler"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "def save_model(model, epoch, loss):\n",
                "    model_file = os.path.join(args.intermediate_path,\n",
                "                              \"model_{}_epoch{}_loss{:.4f}.pth\"\n",
                "                              .format(args.seed, epoch, loss))\n",
                "    torch.save(model.state_dict(), os.path.join(model_file))\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
                "scheduler = MultiStepLR(optimizer, milestones=[2, 4])"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "full = pd.read_csv('../input/web-traffic-time-series-forecasting/train_2.csv', index_col='Page')\n",
                "y_true = full.iloc[:, -args.test_len:].values"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "test = pd.DataFrame()\n",
                "test[\"Page\"] = full.index\n",
                "datetime_list = pd.date_range(args.forecast_start, args.forecast_end)\n",
                "for datetime in datetime_list:\n",
                "    test[datetime.date().isoformat()] = 0\n",
                "test.iloc[:, 1:] = np.around(prediction[:, 2:])"
            ]
        },
        {
            "tags": [
                "process_data",
                "transfer_results",
                "ingest_data"
            ],
            "source": [
                "test = pd.melt(test, id_vars='Page', var_name='Date', value_name=\"Visits\")\n",
                "\n",
                "key_df = pd.read_csv('../input/web-traffic-time-series-forecasting/key_2.csv')\n",
                "key_df['Date'] = key_df['Page'].apply(lambda a: a[-10:])\n",
                "key_df['Page'] = key_df['Page'].apply(lambda a: a[:-11])\n",
                "key_df = key_df.merge(test, how=\"left\")\n",
                "\n",
                "key_df[['Id', 'Visits']].to_csv(\n",
                "    '../working/{}/submission.csv'.format(args.seed), index=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "key_df.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import plotly.offline as py\n",
                "import plotly.graph_objs as go\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.metrics import roc_auc_score\n",
                " \n",
                "import cv2\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader, sampler, TensorDataset, random_split\n",
                "from torch.optim import lr_scheduler\n",
                "from torchvision import datasets, transforms, models\n",
                "from torch import utils\n",
                "\n",
                "import  time, copy, glob, torchvision, torch, os, json, re\n",
                "from collections import Counter, OrderedDict\n",
                "\n",
                "from PIL import Image\n",
                "from sklearn.metrics import classification_report\n",
                "\n",
                "\n",
                "plt.rcParams['figure.figsize']=(20,10)\n",
                "py.init_notebook_mode(connected=False)\n",
                "%matplotlib inline\n",
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "ROOT_DIR = '../input/virtual-hack/car_data/car_data'\n",
                "TRAIN_DIR = '../input/virtual-hack/car_data/car_data/train'\n",
                "TEST_DIR = '../input/virtual-hack/car_data/car_data/test'\n",
                "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
                "NUM_CLASSES = 196"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_transforms = transforms.Compose([\n",
                "    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),\n",
                "    transforms.RandomRotation(degrees=15),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.CenterCrop(size=224),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406],\n",
                "                         [0.229, 0.224, 0.225])])\n",
                "\n",
                "valid_transforms = transforms.Compose([transforms.CenterCrop(size=224),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406],\n",
                "                         [0.229, 0.224, 0.225])])\n",
                "\n",
                "dataset = datasets.ImageFolder(TRAIN_DIR, train_transforms)\n",
                "test_dataset = datasets.ImageFolder(TEST_DIR, valid_transforms)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "size = len(dataset)\n",
                "val_split = 0.2\n",
                "trainset, valset = random_split(dataset, [size - int(size * val_split), int(size * val_split)])\n",
                "\n",
                "train_loader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
                "val_loader = DataLoader(valset, batch_size=64, shuffle=True)\n",
                "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "def imshow(img):\n",
                "    inverse_transform = transforms.Compose([\n",
                "      transforms.Normalize(mean = [ 0., 0., 0. ],\n",
                "                           std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
                "      transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
                "                           std = [ 1., 1., 1. ]),\n",
                "      ])\n",
                "\n",
                "    unnormalized_img = inverse_transform(img)\n",
                "    npimg = img.numpy()\n",
                "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
                "    plt.show()\n",
                "  \n",
                "\n",
                "def visualize(): \n",
                "    dataiter = iter(train_loader)\n",
                "    images, labels = dataiter.next()\n",
                "\n",
                "    classes = dataset.classes\n",
                "    imshow(torchvision.utils.make_grid(images[:4]))\n",
                "    for j in range(4):\n",
                "        print(\"label: {},  name: {}\".format(labels[j].item(),\n",
                "                                        classes[labels[j]]))\n",
                "visualize()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def create_model():\n",
                "    model = models.densenet121(pretrained=True)\n",
                "\n",
                "    for param in model.parameters():\n",
                "        param.requires_grad = False\n",
                "\n",
                "    n_inputs = model.classifier.in_features\n",
                "    model.classifier = nn.Sequential(\n",
                "                    nn.Linear(n_inputs, NUM_CLASSES),\n",
                "                    nn.LogSoftmax(dim=1))\n",
                "    # Move model to the device specified above\n",
                "\n",
                "    model.to(device)\n",
                "    return model"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model = create_model()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "weight=torch.FloatTensor(create_class_weights()).to(device)\n",
                "criterion = nn.NLLLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "dataset_sizes = {'train': len(trainset), 'valid': len(valset), 'test': len(test_dataset)}"
            ]
        },
        {
            "tags": [
                "train_model",
                "ingest_data",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "def train_model(dataloaders, model, criterion, optimizer, scheduler, num_epochs=5):\n",
                "    since = time.time()\n",
                "\n",
                "    best_model_wts = copy.deepcopy(model.state_dict())\n",
                "    best_acc = 0.0\n",
                "    losses = {'train': [], 'valid':[]}\n",
                "    acc = {'train': [], 'valid': []}\n",
                "  \n",
                "    for epoch in range(num_epochs):\n",
                "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
                "        print('-' * 10)\n",
                "\n",
                "    # Each epoch has a training and validation phase\n",
                "        for phase in ['train', 'valid']:\n",
                "            if phase == 'train':\n",
                "                scheduler.step()\n",
                "                model.train()  # Set model to training mode\n",
                "            else:\n",
                "                model.eval()   # Set model to evaluate mode\n",
                "\n",
                "            running_loss = 0.0\n",
                "            running_corrects = 0\n",
                "\n",
                "\n",
                "            for inputs, labels in dataloaders[phase]:\n",
                "                inputs = inputs.to(device)\n",
                "                labels = labels.to(device)\n",
                "\n",
                "                # zero the parameter gradients\n",
                "                optimizer.zero_grad()\n",
                "\n",
                "            # forward\n",
                "            # track history if only in train\n",
                "                with torch.set_grad_enabled(phase == 'train'):\n",
                "                    outputs = model(inputs)\n",
                "                    _, preds = torch.max(outputs, 1)\n",
                "                    loss = criterion(outputs, labels)\n",
                "\n",
                "                    # backward + optimize only if in training phase\n",
                "                    if phase == 'train':\n",
                "                        loss.backward()\n",
                "                        optimizer.step()\n",
                "\n",
                "                    running_loss += loss.item() * inputs.size(0)\n",
                "                    running_corrects += torch.sum(preds == labels.data)\n",
                "\n",
                "            epoch_loss = running_loss / dataset_sizes[phase]\n",
                "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
                "            losses[phase].append(epoch_loss)\n",
                "            acc[phase].append(epoch_acc)\n",
                "\n",
                "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
                "                      phase, epoch_loss, epoch_acc))\n",
                "\n",
                "            # deep copy the model\n",
                "            if phase == 'valid' and epoch_acc > best_acc:\n",
                "                best_acc = epoch_acc\n",
                "                best_model_wts = copy.deepcopy(model.state_dict())\n",
                "\n",
                "        print()\n",
                "\n",
                "    time_elapsed = time.time() - since\n",
                "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
                "      time_elapsed // 60, time_elapsed % 60))\n",
                "    print('Best val Acc: {:4f}'.format(best_acc))\n",
                "\n",
                "    # load best model weights\n",
                "    model.load_state_dict(best_model_wts)\n",
                "    return model"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "ingest_data"
            ],
            "source": [
                "def save(model, path):\n",
                "    torch.save(model, path)\n",
                "\n",
                "def load(path):\n",
                "    return torch.load(path)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "confusion_matrix = torch.zeros(NUM_CLASSES, NUM_CLASSES)\n",
                "\n",
                "with torch.no_grad():\n",
                "    accuracy = 0\n",
                "    pred_labels = []\n",
                "    pred_img_ids = []\n",
                "    true_labels = []\n",
                "    for i, (inputs, labels) in enumerate(test_loader):\n",
                "        inputs = inputs.to(device)\n",
                "        labels = labels.to(device)\n",
                "        outputs = model(inputs)\n",
                "        _, preds = torch.max(outputs, 1)\n",
                "        running_acc = torch.sum(preds == labels.data)\n",
                "        accuracy += running_acc\n",
                "        pred_labels.append(preds)\n",
                "        true_labels.append(labels)\n",
                "        \n",
                "        for t, p in zip(labels.view(-1), preds.view(-1)):\n",
                "            confusion_matrix[t.long(), p.long()] += 1\n",
                "        \n",
                "    for dir in os.listdir(TEST_DIR):\n",
                "        for file in os.listdir(os.path.join(TEST_DIR, dir)):\n",
                "            img_id = os.path.splitext(file)[0]\n",
                "            pred_img_ids.append(img_id)\n",
                "            \n",
                "    print('Accuracy =====>>', accuracy.item()/len(test_dataset))"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "sns.set(font_scale=1.4)#for label size\n",
                "sns.heatmap(confusion_matrix[:10, :10], annot=True,annot_kws={\"size\": 16})# font size"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "pred_labels_expanded = []\n",
                "for l in pred_labels:\n",
                "    for l1 in l:\n",
                "        pred_labels_expanded.append(l1.item())\n",
                "\n",
                "true_labels_expanded = []\n",
                "for l in true_labels:\n",
                "    for l1 in l:\n",
                "        true_labels_expanded.append(l1.item())\n",
                "\n",
                "from sklearn.metrics import roc_auc_score\n",
                "from sklearn.preprocessing import LabelBinarizer\n",
                "\n",
                "def multiclass_roc_auc_score(truth, pred, average=\"macro\"):\n",
                "\n",
                "    lb = LabelBinarizer()\n",
                "    lb.fit(truth)\n",
                "\n",
                "    truth = lb.transform(truth)\n",
                "    pred = lb.transform(pred)\n",
                "\n",
                "    return roc_auc_score(truth, pred, average=average)\n",
                "\n",
                "print(\"ROC AUC SCORE: =========>\", multiclass_roc_auc_score(true_labels_expanded, pred_labels_expanded))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "import pandas as pd\n",
                "sub_sample = pd.read_csv('../input/virtual-hack/sampleSubmission.csv')\n",
                "sub_sample.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "id_label_dict = {}\n",
                "for id, label in zip(pred_img_ids, pred_labels_expanded):\n",
                "    id_label_dict[id] = label"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "my_submission = pd.DataFrame()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "my_submission['Id'] = od.keys()\n",
                "my_submission['Predicted'] = od.values()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "my_submission.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "my_submission.to_csv('my_submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# General Imports\n",
                "import copy\n",
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Pytorch Imports\n",
                "import torch\n",
                "from torch import nn\n",
                "import torch.nn.functional as F\n",
                "from torch.autograd import Variable\n",
                "import torchvision.transforms as transforms\n",
                "from torch.utils.data import TensorDataset, DataLoader\n",
                "\n",
                "print(os.listdir('../input/'))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "trainDF = pd.read_csv('../input/train.csv')\n",
                "trainDF.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_x, val_x, train_y, val_y = train_test_split(trainDF['review'], trainDF['sentiment'], test_size=0.1, shuffle=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# word level tf-idf\n",
                "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=25000)\n",
                "tfidf_vect.fit(trainDF['review'])\n",
                "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
                "xval_tfidf = tfidf_vect.transform(val_x)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Converting the Sparse matrix into a numpy array\n",
                "xtrain_tfidf = xtrain_tfidf.toarray()\n",
                "xval_tfidf = xval_tfidf.toarray()\n",
                "# Converting pandas Series into numpy array\n",
                "train_y = np.array(train_y)\n",
                "val_y = np.array(val_y)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model = nn.Sequential(nn.Linear(input_dim, 2048),\n",
                "                      nn.Dropout(0.5),\n",
                "                      nn.ReLU(),\n",
                "                      nn.Linear(2048, 256),\n",
                "                      nn.Dropout(0.5),\n",
                "                      nn.ReLU(),\n",
                "                      nn.Linear(256, output_dim),\n",
                "                      nn.Sigmoid()).double()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# Criterion and Optimizer\n",
                "criterion = torch.nn.BCEWithLogitsLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=lr_rate)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model",
                "ingest_data"
            ],
            "source": [
                "def train_model(model):\n",
                "    model = model.cuda()\n",
                "    best_acc = 0.0\n",
                "    best_model_wts = copy.deepcopy(model.state_dict())\n",
                "    \n",
                "    for epoch in range(int(epochs)):\n",
                "        train_loss = 0\n",
                "        val_loss = 0\n",
                "        val_acc = 0\n",
                "        model.train()\n",
                "        for inputs, labels in train_dataloader:\n",
                "            inputs, labels = inputs.cuda(), labels.cuda()\n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(inputs)\n",
                "            loss = criterion(outputs.squeeze(), labels)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            train_loss += loss.item()\n",
                "        else:\n",
                "            model.eval()\n",
                "            num_correct = 0\n",
                "            for inputs, labels in val_dataloader:\n",
                "                inputs, labels = inputs.cuda(), labels.cuda()\n",
                "                outputs = model(inputs)\n",
                "                predictions = torch.round(outputs.squeeze())\n",
                "                loss = criterion(predictions, labels)\n",
                "                \n",
                "                val_loss += loss.item()\n",
                "                equals = (predictions == labels.data)\n",
                "    \n",
                "                num_correct += torch.sum(equals.data).item()\n",
                "            \n",
                "            val_acc = num_correct / len(val_dataset)\n",
                "            if val_acc > best_acc:\n",
                "                best_acc = val_acc\n",
                "                best_model_wts = copy.deepcopy(model.state_dict())\n",
                "        print('---------Epoch {} -----------'.format(epoch))\n",
                "        print('Train Loss: {:.6f} Val Loss: {:.6f} Val Accuracy: {:.6f}'.format(\n",
                "                 train_loss/len(train_dataset), val_loss/len(val_dataset), val_acc))\n",
                "        \n",
                "    model.load_state_dict(best_model_wts)\n",
                "    return model"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "test_df = pd.read_csv('../input/test.csv')\n",
                "xtest_tfidf =  tfidf_vect.transform(test_df['review'])\n",
                "xtest_tfidf = xtest_tfidf.toarray()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "test_y = np.zeros(xtest_tfidf.shape[0])"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "def predict(model, test_dataloader):\n",
                "    model.eval()\n",
                "    predictions = []\n",
                "    for inputs, _ in test_dataloader:\n",
                "        inputs = inputs.cuda()\n",
                "        output = model(inputs)\n",
                "        preds = torch.round(output)\n",
                "        predictions.extend([p.item() for p in preds])\n",
                "    return predictions\n",
                "\n",
                "predictions = predict(model, test_dataloader)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "sub_df = pd.DataFrame()\n",
                "sub_df['Id'] = test_df['Id']\n",
                "sub_df['sentiment'] = [int(p) for p in predictions]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "sub_df.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "sub_df.to_csv('my_submission.csv', index=False)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import torch\n",
                "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
                "from keras.preprocessing.sequence import pad_sequences\n",
                "from sklearn.model_selection import train_test_split\n",
                "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
                "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
                "from tqdm import tqdm, trange\n",
                "import pandas as pd\n",
                "import io\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "\n",
                "import torch\n",
                "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
                "import os\n",
                "print(os.listdir('../input/'))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "n_gpu = torch.cuda.device_count()\n",
                "torch.cuda.get_device_name(0)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train_df = pd.read_csv('../input/nlp-hack/train.csv')\n",
                "test_df = pd.read_csv('../input/nlp-hack/test.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model"
            ],
            "source": [
                "for param in model.bert.parameters():\n",
                "  param.requires_grad = False\n",
                "\n",
                "for name, param in model.named_parameters():                \n",
                "    if param.requires_grad:\n",
                "        print(name)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "sentences = test_df.text.values\n",
                "sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
                "labels = np.random.rand(len(sentences))"
            ]
        },
        {
            "tags": [
                "process_data",
                "setup_notebook"
            ],
            "source": [
                "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
                "MAX_LEN = 128\n",
                "# Pad our input tokens\n",
                "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
                "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
                "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
                "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
                "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
                "# Create attention masks\n",
                "attention_masks = []\n",
                "\n",
                "# Create a mask of 1s for each token followed by 0s for padding\n",
                "for seq in input_ids:\n",
                "  seq_mask = [float(i>0) for i in seq]\n",
                "  attention_masks.append(seq_mask) \n",
                "\n",
                "prediction_inputs = torch.tensor(input_ids)\n",
                "prediction_masks = torch.tensor(attention_masks)\n",
                "prediction_labels = torch.tensor(labels)\n",
                "  \n",
                "batch_size = 32  \n",
                "\n",
                "\n",
                "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
                "prediction_sampler = SequentialSampler(prediction_data)\n",
                "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "process_data"
            ],
            "source": [
                "model.eval()\n",
                "\n",
                "# Tracking variables \n",
                "predictions , true_labels = [], []\n",
                "\n",
                "# Predict \n",
                "for batch in prediction_dataloader:\n",
                "  # Add batch to GPU\n",
                "  batch = tuple(t.to(device) for t in batch)\n",
                "  # Unpack the inputs from our dataloader\n",
                "  b_input_ids, b_input_mask, b_labels = batch\n",
                "  # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
                "  with torch.no_grad():\n",
                "    # Forward pass, calculate logit predictions\n",
                "    logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
                "\n",
                "  # Move logits and labels to CPU\n",
                "  logits = logits.detach().cpu().numpy()\n",
                "  label_ids = b_labels.to('cpu').numpy()\n",
                "  \n",
                "  # Store predictions and true labels\n",
                "  predictions.append(logits)\n",
                "  true_labels.append(label_ids)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "my_submission = pd.DataFrame()\n",
                "my_submission['Id'] = test_df['Id']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "my_submission['target'] = final_preds"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "my_submission['target'] = my_submission['target'].map({0:0, 1:4})"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "my_submission.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "my_submission.to_csv('my_sub.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "from sklearn import model_selection, preprocessing, linear_model, metrics\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
                "from sklearn import decomposition, ensemble\n",
                "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Matplotlib forms basis for visualization in Python\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.ticker as ticker\n",
                "\n",
                "# We will use the Seaborn library\n",
                "import seaborn as sns\n",
                "sns.set()\n",
                "\n",
                "import os, string\n",
                "# Graphics in SVG format are more sharp and legible\n",
                "%config InlineBackend.figure_format = 'svg'\n",
                "print(os.listdir('../input/'))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "df = pd.read_csv('../input/train.csv')\n",
                "df.head()"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "test_df = pd.read_csv('../input/test.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df['char_count'] = df['review'].apply(len)\n",
                "df['word_count'] = df['review'].apply(lambda x: len(x.split()))\n",
                "df['word_density'] = df['char_count'] / (df['word_count']+1)\n",
                "df['punctuation_count'] = df['review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#df.hist(column=['char_count', 'word_count'])\n",
                "features = ['char_count', 'word_count', 'word_density', 'punctuation_count']\n",
                "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 6), sharey=True)\n",
                "\n",
                "for i, feature in enumerate(features):\n",
                "    df.hist(column=feature, ax=axes.flatten()[i])\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df[features].describe()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig, ax = plt.subplots()\n",
                "sns.boxplot(df['word_count'], order=range(0,max(df['word_count'])), ax=ax)\n",
                "\n",
                "ax.xaxis.set_major_locator(ticker.MultipleLocator(200))\n",
                "ax.xaxis.set_major_formatter(ticker.ScalarFormatter())\n",
                "fig.set_size_inches(8, 4)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "\n",
                "X_train, X_val, y_train, y_val = train_test_split(df['review'], df['sentiment'], test_size=0.3,\n",
                "random_state=17)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Converting X_train and X_val to tfidf vectors (since out models can't take text data is input)\n",
                "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
                "tfidf_vect.fit(df['review'])\n",
                "xtrain_tfidf =  tfidf_vect.transform(X_train)\n",
                "xvalid_tfidf =  tfidf_vect.transform(X_val)\n",
                "\n",
                "# ngram level tf-idf \n",
                "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
                "tfidf_vect_ngram.fit(df['review'])\n",
                "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(X_train)\n",
                "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(X_val)\n",
                "\n",
                "# characters level tf-idf\n",
                "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
                "tfidf_vect_ngram_chars.fit(df['review'])\n",
                "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
                "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_val) \n",
                "\n",
                "# Also creating for the X_test which is essentially test_df['review'] column\n",
                "xtest_tfidf =  tfidf_vect.transform(test_df['review'])\n",
                "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_df['review'])\n",
                "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_df['review']) "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# create a count vectorizer object \n",
                "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
                "count_vect.fit(df['review'])\n",
                "\n",
                "# transform the training and validation data using count vectorizer object\n",
                "xtrain_count =  count_vect.transform(X_train)\n",
                "xvalid_count =  count_vect.transform(X_val)\n",
                "xtest_count = count_vect.transform(test_df['review'])"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "model1 = linear_model.LogisticRegression()\n",
                "model1.fit(xtrain_count, y_train)\n",
                "accuracy=model1.score(xvalid_count, y_val)\n",
                "print('Accuracy Count LR:', accuracy)\n",
                "test_pred1=model1.predict(xtest_count)\n",
                "\n",
                "model2 = linear_model.LogisticRegression()\n",
                "model2.fit(xtrain_tfidf, y_train)\n",
                "accuracy=model2.score(xvalid_tfidf, y_val)\n",
                "print('Accuracy TFIDF LR:', accuracy)\n",
                "test_pred2=model2.predict(xtest_tfidf)\n",
                "\n",
                "model3 = linear_model.LogisticRegression()\n",
                "model3.fit(xtrain_tfidf_ngram, y_train)\n",
                "accuracy = model3.score(xvalid_tfidf_ngram, y_val)\n",
                "print('Accuracy TFIDF NGRAM LR:', accuracy)\n",
                "test_pred3 = model3.predict(xtest_tfidf_ngram)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "sub_df = pd.DataFrame()\n",
                "sub_df['Id'] = test_df['Id']\n",
                "sub_df['sentiment'] = [int(i) for i in final_pred]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "sub_df.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "sub_df.to_csv('my_submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import os\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import keras\n",
                "import cv2\n",
                "from keras.datasets import fashion_mnist#download mnist data and split into train and test sets\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.utils import shuffle\n",
                "import matplotlib.pyplot as plt\n",
                "from keras.utils import to_categorical\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout#create model\n",
                "from keras.optimizers import SGD\n",
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "\n",
                "from keras.preprocessing.image import load_img\n",
                "from keras.preprocessing.image import img_to_array\n",
                "from keras.applications.vgg16 import VGG16\n",
                "\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "CAT_TRAIN_PATH=\"/kaggle/input/cat-and-dog/training_set/training_set/cats/\"\n",
                "DOG_TRAIN_PATH=\"/kaggle/input/cat-and-dog/training_set/training_set/dogs/\"\n",
                "CAT_TEST_PATH=\"/kaggle/input/cat-and-dog/test_set/test_set/cats/\"\n",
                "DOG_TEST_PATH=\"/kaggle/input/cat-and-dog/test_set/test_set/dogs/\""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def preprocess_img(img):\n",
                "    dim=(100,100)\n",
                "    res = cv2.resize(img, dim, interpolation=cv2.INTER_LINEAR)\n",
                "    return res\n",
                "    "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "\n",
                "print(np.array(lst).shape)\n",
                "print(np.array(y).shape)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data",
                "transfer_results"
            ],
            "source": [
                "X_train=np.array(lst)\n",
                "Y_train=np.array(y)\n",
                "np.savez(\"CAT_DOG_X_train\",X_train)\n",
                "np.savez(\"CAT_DOG_Y_train\",Y_train)\n",
                "print(X_train.shape)\n",
                "print(Y_train.shape)\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results",
                "transfer_results"
            ],
            "source": [
                "X_test=np.array(lst)\n",
                "Y_test=np.array(y)\n",
                "print(X_test.shape)\n",
                "print(Y_test.shape)\n",
                "np.save(\"CAT_DOG_X_test\",X_test)\n",
                "np.save(\"CAT_DOG_Y_test\",Y_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "X_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_train.npz\"\n",
                "X_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_test.npy\"\n",
                "Y_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_train.npz\"\n",
                "Y_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_test.npy\""
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(X_train.shape)\n",
                "print(Y_train.shape)\n",
                "print(X_test.shape)\n",
                "print(Y_test.shape)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(\"Distribution of cats and dogs in the different sets\")\n",
                "print(\"TRAIN  :  \"+str(sum(Y_train==1))+\" cats vs \"+str(sum(Y_train==0))+\" dogs\")\n",
                "print(\"TEST  :  \"+str(sum(Y_test==1))+\" cats vs \"+str(sum(Y_test==0))+\" dogs\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(X_train.shape)\n",
                "print(Y_train.shape)\n",
                "print(X_val.shape)\n",
                "print(Y_val.shape)\n",
                "print(X_test.shape)\n",
                "print(Y_test.shape)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(\"Distribution of cats and dogs in the different sets\")\n",
                "print(\"TRAIN  :  \"+str(sum(Y_train==1))+\" cats vs \"+str(sum(Y_train==0))+\" dogs\")\n",
                "print(\"VAL  :  \"+str(sum(Y_val==1))+\" cats vs \"+str(sum(Y_val==0))+\" dogs\")\n",
                "print(\"TEST  :  \"+str(sum(Y_test==1))+\" cats vs \"+str(sum(Y_test==0))+\" dogs\")"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "\n",
                "print(\"Images 1 to 5 :\")\n",
                "for i in range(0,5):\n",
                "    plt.imshow(X_train[i])\n",
                "    print(Y_train[i])\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test):\n",
                "    X_train=normalize_X(X_train)\n",
                "    X_val=normalize_X(X_val)\n",
                "    X_test=normalize_X(X_test)\n",
                "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
                "    X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
                "    X_test, y_test = shuffle(X_test, y_test, random_state=42)\n",
                "    return X_train,X_val,X_test,y_train,y_val,y_test"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "\n",
                "\n",
                "def input_and_run(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10):\n",
                "    \n",
                "    #compile model using accuracy to measure model performance\n",
                "    opt = keras.optimizers.Adam(learning_rate=alpha)\n",
                "    opt2=SGD(lr=alpha, momentum=0.9)\n",
                "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
                "    \n",
                "    #train the model\n",
                "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=num_epochs)\n",
                "    \n",
                "    #Getting results\n",
                "    result = model.evaluate(X_train,y_train)\n",
                "    #print(result)\n",
                "    print(\"Training accuracy = \"+str(result[1]*100))\n",
                "    result = model.evaluate(X_val,y_val)\n",
                "    #print(result)\n",
                "    print(\"Validation accuracy = \"+str(result[1]*100))\n",
                "    result = model.evaluate(X_test,y_test)\n",
                "    #print(result)\n",
                "    print(\"Test accuracy = \"+str(result[1]*100))\n",
                "\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "##BUILDING THE MODEL 1\n",
                "\n",
                "model1 = Sequential()#add model layers\n",
                "\n",
                "model1.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)))\n",
                "model1.add(MaxPooling2D((2, 2)))\n",
                "model1.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model1.add(MaxPooling2D((2, 2)))\n",
                "model1.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model1.add(MaxPooling2D((2, 2)))\n",
                "model1.add(Flatten())\n",
                "model1.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
                "model1.add(Dense(1, activation='sigmoid'))\n",
                "\n",
                "print(model1.summary())\n",
                "input_and_run(model1,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.0001,num_epochs=20)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "##BUILDING THE MODEL 1\n",
                "\n",
                "model2 = Sequential()#add model layers\n",
                "\n",
                "model2.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)))\n",
                "model2.add(MaxPooling2D((2, 2)))\n",
                "model2.add(Dropout(0.2))\n",
                "model2.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model2.add(MaxPooling2D((2, 2)))\n",
                "model2.add(Dropout(0.2))\n",
                "model2.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model2.add(MaxPooling2D((2, 2)))\n",
                "model2.add(Dropout(0.2))\n",
                "model2.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model2.add(MaxPooling2D((2, 2)))\n",
                "model2.add(Dropout(0.2))\n",
                "model2.add(Flatten())\n",
                "model2.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
                "model2.add(Dropout(0.5))\n",
                "model2.add(Dense(1, activation='sigmoid'))\n",
                "\n",
                "print(model2.summary())\n",
                "input_and_run(model2,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.002,num_epochs=200)"
            ]
        },
        {
            "tags": [
                "check_results",
                "transfer_results"
            ],
            "source": [
                "model2.save(\"model2.h5\")\n",
                "print(\"Saved model to disk\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "WEIGHTS_FILE=\"/kaggle/input/cat-dog-numpy/model2.h5\""
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "from keras.models import load_model\n",
                "# load model\n",
                "loaded_model=load_model(WEIGHTS_FILE)\n",
                "print(\"Loaded model from disk\")"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "#Getting results\n",
                "opt = keras.optimizers.Adam(learning_rate=0.002)\n",
                "loaded_model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
                "result = loaded_model.evaluate(X_train,Y_train)\n",
                "#print(result)\n",
                "print(\"Training accuracy = \"+str(result[1]*100))\n",
                "result = loaded_model.evaluate(X_val,Y_val)\n",
                "#print(result)\n",
                "print(\"Validation accuracy = \"+str(result[1]*100))\n",
                "result = loaded_model.evaluate(X_test,Y_test)\n",
                "#print(result)\n",
                "print(\"Test accuracy = \"+str(result[1]*100))\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "X_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_train.npz\"\n",
                "X_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_test.npy\"\n",
                "Y_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_train.npz\"\n",
                "Y_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_test.npy\""
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "a=np.load(X_TRAIN_FILE)\n",
                "X_train=a.f.arr_0\n",
                "a=np.load(Y_TRAIN_FILE)\n",
                "Y_train=a.f.arr_0\n",
                "a=np.load(X_TEST_FILE)\n",
                "X_test=a\n",
                "a=np.load(Y_TEST_FILE)\n",
                "Y_test=a\n",
                "\n",
                "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(X_train.shape)\n",
                "print(Y_train.shape)\n",
                "print(X_val.shape)\n",
                "print(Y_val.shape)\n",
                "print(X_test.shape)\n",
                "print(Y_test.shape)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test):\n",
                "    X_train=normalize_X(X_train)\n",
                "    X_val=normalize_X(X_val)\n",
                "    X_test=normalize_X(X_test)\n",
                "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
                "    X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
                "    X_test, y_test = shuffle(X_test, y_test, random_state=42)\n",
                "    return X_train,X_val,X_test,y_train,y_val,y_test"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "\n",
                "\n",
                "def input_and_run2(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10):\n",
                "    \n",
                "    datagen = ImageDataGenerator(width_shift_range=0.2,height_shift_range=0.2,horizontal_flip=True)\n",
                "    datagen.fit(X_train)\n",
                "    \n",
                "    #compile model using accuracy to measure model performance\n",
                "    opt = keras.optimizers.Adam(learning_rate=alpha)\n",
                "    opt2=SGD(lr=alpha, momentum=0.9)\n",
                "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
                "    \n",
                "    #train the model\n",
                "    model.fit(datagen.flow(X_train, y_train),validation_data=(X_val, y_val), epochs=num_epochs)\n",
                "    \n",
                "    #Getting results\n",
                "    result = model.evaluate(X_train,y_train)\n",
                "    #print(result)\n",
                "    print(\"Training accuracy = \"+str(result[1]*100))\n",
                "    result = model.evaluate(X_val,y_val)\n",
                "    #print(result)\n",
                "    print(\"Validation accuracy = \"+str(result[1]*100))\n",
                "    result = model.evaluate(X_test,y_test)\n",
                "    #print(result)\n",
                "    print(\"Test accuracy = \"+str(result[1]*100))\n",
                "\n",
                "\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "##BUILDING THE MODEL 1\n",
                "\n",
                "model4 = Sequential()#add model layers\n",
                "\n",
                "model4.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(100, 100, 3)))\n",
                "model4.add(MaxPooling2D((2, 2)))\n",
                "model4.add(Dropout(0.2))\n",
                "model4.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model4.add(MaxPooling2D((2, 2)))\n",
                "model4.add(Dropout(0.2))\n",
                "model4.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model4.add(MaxPooling2D((2, 2)))\n",
                "model4.add(Dropout(0.2))\n",
                "model4.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
                "model4.add(MaxPooling2D((2, 2)))\n",
                "model4.add(Dropout(0.2))\n",
                "model4.add(Flatten())\n",
                "model4.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
                "model4.add(Dropout(0.5))\n",
                "model4.add(Dense(1, activation='sigmoid'))\n",
                "\n",
                "print(model4.summary())\n",
                "input_and_run2(model4,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.001,num_epochs=200)"
            ]
        },
        {
            "tags": [
                "check_results",
                "transfer_results"
            ],
            "source": [
                "model4.save(\"model4.h5\")\n",
                "print(\"Saved model to disk\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "X_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_train.npz\"\n",
                "X_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_test.npy\"\n",
                "Y_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_train.npz\"\n",
                "Y_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_test.npy\""
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "a=np.load(X_TRAIN_FILE)\n",
                "X_train=a.f.arr_0\n",
                "a=np.load(Y_TRAIN_FILE)\n",
                "Y_train=a.f.arr_0\n",
                "a=np.load(X_TEST_FILE)\n",
                "X_test=a\n",
                "a=np.load(Y_TEST_FILE)\n",
                "Y_test=a\n",
                "\n",
                "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(X_train.shape)\n",
                "print(Y_train.shape)\n",
                "print(X_val.shape)\n",
                "print(Y_val.shape)\n",
                "print(X_test.shape)\n",
                "print(Y_test.shape)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def preprocess_data(X_train,X_val,X_test,y_train,y_val,y_test):\n",
                "    X_train=normalize_X(X_train)\n",
                "    X_val=normalize_X(X_val)\n",
                "    X_test=normalize_X(X_test)\n",
                "    X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
                "    X_val, y_val = shuffle(X_val, y_val, random_state=42)\n",
                "    X_test, y_test = shuffle(X_test, y_test, random_state=42)\n",
                "    return X_train,X_val,X_test,y_train,y_val,y_test"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "\n",
                "\n",
                "def input_and_run3(model,X_train,X_val,X_test,y_train,y_val,y_test,alpha=0.01,num_epochs=10):\n",
                "    \n",
                "    #datagen = ImageDataGenerator(width_shift_range=0.2,height_shift_range=0.2,horizontal_flip=True)\n",
                "    #datagen.fit(X_train)\n",
                "    \n",
                "    #compile model using accuracy to measure model performance\n",
                "    opt = keras.optimizers.Adam(learning_rate=alpha)\n",
                "    opt2=SGD(lr=alpha, momentum=0.9)\n",
                "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
                "    \n",
                "    #train the model\n",
                "    model.fit(X_train,y_train,validation_data=(X_val, y_val), epochs=num_epochs)\n",
                "    #model.fit(datagen.flow(X_train, y_train),validation_data=(X_val, y_val), epochs=num_epochs)\n",
                "    \n",
                "    #Getting results\n",
                "    result = model.evaluate(X_train,y_train)\n",
                "    #print(result)\n",
                "    print(\"Training accuracy = \"+str(result[1]*100))\n",
                "    result = model.evaluate(X_val,y_val)\n",
                "    #print(result)\n",
                "    print(\"Validation accuracy = \"+str(result[1]*100))\n",
                "    result = model.evaluate(X_test,y_test)\n",
                "    #print(result)\n",
                "    print(\"Test accuracy = \"+str(result[1]*100))\n",
                "\n",
                "\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "model5 = Sequential()\n",
                "model5.add(VGG16(include_top=False, input_shape=(100, 100, 3)))\n",
                "# mark loaded layers as not trainable\n",
                "for layer in model5.layers:\n",
                "    layer.trainable = False\n",
                "# add new classifier layers\n",
                "#flat1 = Flatten()(model.layers[-1].output)\n",
                "#class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
                "#output = Dense(1, activation='sigmoid')(class1)\n",
                "\n",
                "model5.add(Flatten())\n",
                "model5.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
                "model5.add(Dropout(0.5))\n",
                "model5.add(Dense(1, activation='sigmoid'))\n",
                "print(model5.summary())\n",
                "input_and_run3(model5,X_train,X_val,X_test,Y_train,Y_val,Y_test,alpha=0.001,num_epochs=200)\n",
                "\n",
                "\n",
                "# define new model\n",
                "#model = Model(inputs=model.inputs, outputs=output)\n",
                "# compile model\n",
                "#opt = SGD(lr=0.001, momentum=0.9)\n",
                "#model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
                "#return model"
            ]
        },
        {
            "tags": [
                "check_results",
                "transfer_results"
            ],
            "source": [
                "model5.save(\"model5.h5\")\n",
                "print(\"Saved model to disk\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "X_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_train.npz\"\n",
                "X_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_X_test.npy\"\n",
                "Y_TRAIN_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_train.npz\"\n",
                "Y_TEST_FILE=\"/kaggle/input/cat-dog-numpy/CAT_DOG_Y_test.npy\""
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "a=np.load(X_TRAIN_FILE)\n",
                "X_train=a.f.arr_0\n",
                "a=np.load(Y_TRAIN_FILE)\n",
                "Y_train=a.f.arr_0\n",
                "a=np.load(X_TEST_FILE)\n",
                "X_test=a\n",
                "a=np.load(Y_TEST_FILE)\n",
                "Y_test=a\n",
                "\n",
                "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.25, random_state=42)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(X_train.shape)\n",
                "print(Y_train.shape)\n",
                "print(X_val.shape)\n",
                "print(Y_val.shape)\n",
                "print(X_test.shape)\n",
                "print(Y_test.shape)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train,X_val,X_test,Y_train,Y_val,Y_test=preprocess_data(X_train,X_val,X_test,Y_train,Y_val,Y_test)"
            ]
        },
        {
            "tags": [
                "check_results",
                "transfer_results"
            ],
            "source": [
                "model6.save(\"model6.h5\")\n",
                "print(\"Saved model to disk\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "from keras.datasets import mnist#download mnist data and split into train and test sets\n",
                "from sklearn.model_selection import train_test_split\n",
                "import matplotlib.pyplot as plt\n",
                "from keras.utils import to_categorical\n",
                "\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "\n",
                "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
                "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=10000, random_state=42)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "###Seeing dimensions of the different sets\n",
                "print(X_train.shape)\n",
                "print(y_train.shape)\n",
                "print(X_val.shape)\n",
                "print(y_val.shape)\n",
                "print(X_test.shape)\n",
                "print(y_test.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "##Seeing example of random images\n",
                "image_no=5\n",
                "img=X_train[image_no]\n",
                "plt.imshow(img)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "##BUILDING THE MODEL\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Dense, Conv2D, Flatten#create model\n",
                "\n",
                "model = Sequential()#add model layers\n",
                "\n",
                "model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
                "model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(10, activation='softmax'))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(model.summary())"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "#compile model using accuracy to measure model performance\n",
                "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#Pre processing the data\n",
                "X_train=reshape_X(normalize_X(X_train))\n",
                "X_val=reshape_X(normalize_X(X_val))\n",
                "X_test=reshape_X(normalize_X(X_test))\n",
                "y_train=to_categorical(y_train)\n",
                "y_val=to_categorical(y_val)\n",
                "y_test=to_categorical(y_test)\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "#train the model\n",
                "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "result = model.evaluate(X_train,y_train)\n",
                "#print(result)\n",
                "print(\"Training accuracy = \"+str(result[1]*100))\n",
                "result = model.evaluate(X_val,y_val)\n",
                "#print(result)\n",
                "print(\"Validation accuracy = \"+str(result[1]*100))\n",
                "#result = model.evaluate(X_test,y_test)\n",
                "#print(result)\n",
                "#print(\"Test accuracy = \"+str(result[1]*100))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "from datetime import datetime as dt\n",
                "\n",
                "# For Visualisation\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "%matplotlib inline\n",
                "\n",
                "# To Scale our data\n",
                "from sklearn.preprocessing import scale\n",
                "\n",
                "# Supress Warnings\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from sklearn.datasets import fetch_mldata\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn import metrics\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "import os\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "mnist = pd.read_csv(\"../input/train.csv\",  sep = ',',encoding = \"ISO-8859-1\", header= 0)\n",
                "holdout = pd.read_csv(\"../input/test.csv\",  sep = ',',encoding = \"ISO-8859-1\", header= 0)\n",
                "\n",
                "print(\"Dimensions of train: {}\".format(mnist.shape))\n",
                "print(\"Dimensions of test: {}\".format(holdout.shape))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "mnist.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# Checking for total count and percentage of null values in all columns of the dataframe.\n",
                "\n",
                "total = pd.DataFrame(mnist.isnull().sum().sort_values(ascending=False), columns=['Total'])\n",
                "percentage = pd.DataFrame(round(100*(mnist.isnull().sum()/mnist.shape[0]),2).sort_values(ascending=False)\\\n",
                "                          ,columns=['Percentage'])\n",
                "pd.concat([total, percentage], axis = 1).head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# Putting feature variable to X\n",
                "X = mnist.drop(['label'], axis=1)\n",
                "\n",
                "X.head()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# Putting response variable to y\n",
                "y = mnist['label']\n",
                "\n",
                "y.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from sklearn.preprocessing import StandardScaler"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "scaler = StandardScaler()\n",
                "\n",
                "X = scaler.fit_transform(X)\n",
                "\n",
                "pd.DataFrame(X).head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Splitting the data into train and test\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pd.DataFrame(X_test).head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "plt.figure(figsize=(20,4))\n",
                "for index, (image, label) in enumerate(zip(X_train[0:5], y_train[0:5])):\n",
                " plt.subplot(1, 5, index + 1)\n",
                " plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
                " plt.title('Training: %i\\n' % label, fontsize = 20)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X_train.shape\n",
                "# We have 30 variables after creating our dummy variables for our categories"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "#Improting the PCA module\n",
                "from sklearn.decomposition import PCA\n",
                "pca = PCA(svd_solver='randomized', random_state=42)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#Doing the PCA on the train data\n",
                "pca.fit(X_train)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pca.n_components_"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pca.components_"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "colnames = list(pd.DataFrame(X_train).columns)\n",
                "pcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\n",
                "pcs_df.head()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "setup_notebook"
            ],
            "source": [
                "%matplotlib inline\n",
                "fig = plt.figure(figsize = (10,10))\n",
                "plt.scatter(pcs_df.PC1, pcs_df.PC2)\n",
                "plt.xlabel('Principal Component 1')\n",
                "plt.ylabel('Principal Component 2')\n",
                "for i, txt in enumerate(pcs_df.Feature):\n",
                "    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pd.options.display.float_format = '{:.2f}'.format\n",
                "pca.explained_variance_ratio_"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "setup_notebook"
            ],
            "source": [
                "#Making the screeplot - plotting the cumulative variance against the number of components\n",
                "%matplotlib inline\n",
                "fig = plt.figure(figsize = (12,8))\n",
                "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
                "plt.xlabel('number of components')\n",
                "plt.ylabel('cumulative explained variance')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train_pca = pca.transform(X_train)\n",
                "X_test_pca = pca.transform(X_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from sklearn.linear_model import LogisticRegression"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "logisticRegr = LogisticRegression(solver = 'lbfgs')"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "logisticRegr.fit(X_train_pca, y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "logisticRegr.predict(X_train_pca[0:10])"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "logisticRegr.predict(X_train_pca)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "score = logisticRegr.score(X_train_pca, y_train)\n",
                "print(score)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "logisticRegr.predict(X_test_pca)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "score = logisticRegr.score(X_test_pca, y_test)\n",
                "print(score)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X_train.shape\n",
                "# We have 30 variables after creating our dummy variables for our categories"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#Doing the PCA on the train data\n",
                "pca_last.fit(X_train)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pca_last.n_components_"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "pca_last.components_"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "colnames = list(pd.DataFrame(X_train).columns)\n",
                "pcs_df = pd.DataFrame({'PC1':pca_last.components_[0],'PC2':pca_last.components_[1], 'Feature':colnames})\n",
                "pcs_df.head()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "setup_notebook"
            ],
            "source": [
                "%matplotlib inline\n",
                "fig = plt.figure(figsize = (10,10))\n",
                "plt.scatter(pcs_df.PC1, pcs_df.PC2)\n",
                "plt.xlabel('Principal Component 1')\n",
                "plt.ylabel('Principal Component 2')\n",
                "for i, txt in enumerate(pcs_df.Feature):\n",
                "    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train_pca = pca_last.transform(X_train)\n",
                "X_test_pca = pca_last.transform(X_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from sklearn.linear_model import LogisticRegression"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "logisticRegr = LogisticRegression(solver = 'lbfgs')"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "model_pca = logisticRegr.fit(X_train_pca, y_train)\n",
                "model_pca"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "logisticRegr.predict(X_train_pca[0:10])"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "predictions = logisticRegr.predict(X_train_pca)\n",
                "predictions"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "score = logisticRegr.score(X_train_pca, y_train)\n",
                "print(score)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "setup_notebook"
            ],
            "source": [
                "%matplotlib inline\n",
                "fig = plt.figure(figsize = (8,8))\n",
                "plt.scatter(X_train_pca[:,0], X_train_pca[:,1], c = y_train)\n",
                "plt.xlabel('Principal Component 1')\n",
                "plt.ylabel('Principal Component 2')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "%matplotlib notebook\n",
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "fig = plt.figure(figsize=(8,8))\n",
                "ax = Axes3D(fig)\n",
                "ax = plt.axes(projection='3d')\n",
                "ax.scatter(X_train_pca[:,2], X_train_pca[:,0], X_train_pca[:,1],zdir='z', s=20, marker = 'o', c=y_train)\n",
                "ax.set_xlabel('Principal Component 1')\n",
                "ax.set_ylabel('Principal Component 2')\n",
                "ax.set_zlabel('Principal Component 3')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "check_results"
            ],
            "source": [
                "import statsmodels.api as sm\n",
                "# Logistic regression model\n",
                "logpca = sm.GLM(y_train,(sm.add_constant(X_train_pca)), family = sm.families.Binomial())\n",
                "logpca.fit().summary()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn import metrics"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "from mlxtend.plotting import plot_confusion_matrix\n",
                "\n",
                "fig, ax = plot_confusion_matrix(conf_mat=cm)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(metrics.accuracy_score(y_train, predictions))"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "recall = np.diag(cm) / np.sum(cm, axis = 1)\n",
                "recall"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from sklearn.metrics import precision_score, recall_score"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "precision = np.diag(cm) / np.sum(cm, axis = 0)\n",
                "precision"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "precision_score(y_train, predictions,average='macro')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "from sklearn.metrics import precision_recall_fscore_support as score\n",
                "\n",
                "precision, recall, fscore, support = score(y_train, predictions)\n",
                "\n",
                "print('precision: {}'.format(precision))\n",
                "print('recall: {}'.format(recall))\n",
                "print('fscore: {}'.format(fscore))\n",
                "print('support: {}'.format(support))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "from sklearn.metrics import classification_report\n",
                "print(classification_report(y_train, predictions))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "predict_test = logisticRegr.predict(X_test_pca)\n",
                "predict_test"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "score = logisticRegr.score(X_test_pca, y_test)\n",
                "print(score)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(metrics.accuracy_score(y_test, predict_test))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "from sklearn.metrics import classification_report\n",
                "print(classification_report(y_test, predict_test))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "holdout.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "holdout.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Checking for total count and percentage of null values in all columns of the dataframe.\n",
                "\n",
                "total = pd.DataFrame(holdout.isnull().sum().sort_values(ascending=False), columns=['Total'])\n",
                "percentage = pd.DataFrame(round(100*(holdout.isnull().sum()/holdout.shape[0]),2).sort_values(ascending=False)\\\n",
                "                          ,columns=['Percentage'])\n",
                "pd.concat([total, percentage], axis = 1).head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from sklearn.preprocessing import StandardScaler"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "holdout = scaler.transform(holdout)\n",
                "\n",
                "pd.DataFrame(holdout).head()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "holdout_pca = pca_last.transform(holdout)\n",
                "pd.DataFrame(holdout_pca).head()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "predict_holdout = logisticRegr.predict(holdout_pca)\n",
                "predict_holdout"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submission.to_csv(\"submission.csv\",index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "\n",
                "train = pd.read_csv('/kaggle/input/titanic/train.csv')\n",
                "test = pd.read_csv('/kaggle/input/titanic/test.csv')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.describe()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.dtypes"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "source": [
                "import seaborn as sns\n",
                "\n",
                "sns.heatmap(train.isnull(), cbar=False)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "test['Age'] = test['Age'].fillna(test['Age'].median())\n",
                "test['Fare'] = test['Fare'].fillna(test['Fare'].median())\n",
                "\n",
                "sns.heatmap(test.isnull(), cbar=False)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train = train.drop('Name',axis = 1)\n",
                "train = train.drop('Sex',axis = 1)\n",
                "train = train.drop('Ticket',axis = 1)\n",
                "train = train.drop('Cabin',axis = 1)\n",
                "\n",
                "\n",
                "test = test.drop('Name',axis = 1)\n",
                "test = test.drop('Sex',axis = 1)\n",
                "test = test.drop('Ticket',axis = 1)\n",
                "test = test.drop('Cabin',axis = 1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.heatmap(train.isnull(), cbar=False)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.heatmap(test.isnull(), cbar=False)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for i in [train, test]:\n",
                "    i['Embarked'] = i['Embarked'].fillna('S')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.preprocessing import OneHotEncoder\n",
                "\n",
                "enc = OneHotEncoder(handle_unknown='ignore')\n",
                "\n",
                "\n",
                "enc_df = pd.DataFrame(enc.fit_transform(train[['Embarked']]).toarray())\n",
                "train = train.join(enc_df)\n",
                "\n",
                "enc_df = pd.DataFrame(enc.fit_transform(test[['Embarked']]).toarray())\n",
                "test = test.join(enc_df)\n",
                "\n",
                "\n",
                "train = train.drop('Embarked',axis = 1)\n",
                "test = test.drop('Embarked',axis = 1)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.describe()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.dtypes"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X = np.array(train.iloc[:,train.columns != 'Survived'])\n",
                "y = np.array(train.Survived).reshape(-1,1)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.metrics import accuracy_score, classification_report, plot_confusion_matrix\n",
                "\n",
                "\n",
                "rfc = RandomForestClassifier().fit(X, y.reshape(-1))\n",
                "y_prima = rfc.predict(X)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print('Cross_val Score RandomForestClassifier = ', cross_val_score(rfc, X, y.reshape(-1), cv=5).mean())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "disp = plot_confusion_matrix(rfc, X, y,\n",
                "                         display_labels=class_names,\n",
                "                         cmap=plt.cm.Blues,\n",
                "                         normalize=None)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "\n",
                "predictions = rfc.predict(test.iloc[:,test.columns != 'Survived'])\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(test)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "transfer_results",
                "process_data",
                "check_results"
            ],
            "source": [
                "predictions = pd.DataFrame(predictions, columns=['Survived'])\n",
                "test = pd.read_csv('/kaggle/input/titanic/test.csv')\n",
                "predictions = pd.concat((test.iloc[:, 0], predictions), axis = 1)\n",
                "predictions.to_csv('submission1.csv', sep=\",\", index = False)\n",
                "\n",
                "print('end')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "sns.set_style('whitegrid')\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "data = pd.read_csv('../input/archive.csv')\n",
                "women = data[(data.Sex == 'Female')]\n",
                "by_category = women['Category'].value_counts()\n",
                "percent = by_category/50\n",
                "categories = data['Category'].unique() \n",
                "\n",
                "#title\n",
                "plt.text(0.1, 1, 'Women Laureates Category',\n",
                "        horizontalalignment='center',\n",
                "        verticalalignment='center',\n",
                "        fontsize=12, color='k')\n",
                "\n",
                "# Plot\n",
                "plt.pie(percent.values, labels = categories)\n",
                "plt.axis('equal')\n",
                "plt.show()\n",
                "plt.clf()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "countries = data['Birth Country'].unique()\n",
                "\n",
                "dict = {}\n",
                "\n",
                "for category in categories:\n",
                "    for country in countries:\n",
                "        x = data.Category[(data['Category'] == category) & (data['Birth Country'] == country)].count()\n",
                "        #print(category, country, x)\n",
                "        dict.setdefault(category, []).append(x)\n",
                "        \n",
                "for country in countries:\n",
                "    dict.setdefault('country', []).append(country)\n",
                "\n",
                "df = pd.DataFrame(dict)\n",
                "\n",
                "for lab, row in df.iterrows() :\n",
                "    df.loc[lab, 'total'] = row['Economics'] + row['Chemistry'] + row['Literature'] + row['Medicine'] + row['Peace'] + row['Physics']\n",
                "   \n",
                "df = df.sort_values('total')\n",
                "        "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "N = len(countries)\n",
                "print(N)\n",
                "ind = range(N)  \n",
                "width = 0.8\n",
                "plt.figure(figsize=(100,150))\n",
                "\n",
                "p1 = plt.barh(ind, df['Chemistry'], width, color = '#137e6d')\n",
                "p2 = plt.barh(ind, df['Literature'], width, df['Chemistry'],  color = '#95d0fc')\n",
                "p3 = plt.barh(ind, df['Medicine'], width, df['Chemistry'] + df['Literature'], color = '#03719c')\n",
                "p4 = plt.barh(ind, df['Peace'], width, df['Chemistry'] + df['Literature'] + df['Medicine'], color = '#6a79f7')\n",
                "p5 = plt.barh(ind, df['Physics'], width, df['Chemistry'] + df['Literature'] + df['Medicine'] + df['Peace'], color = '#137e6d')\n",
                "p6 = plt.barh(ind, df['Economics'], width, df['Chemistry'] + df['Literature'] + df['Medicine'] + df['Peace'] + df['Physics'], color = '#95d0fc')\n",
                "\n",
                "plt.xticks(np.arange(0, 280, 1))\n",
                "plt.yticks(ind,df['country'],  fontsize=56)\n",
                "\n",
                "plt.show()\n",
                "plt.clf()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "trainset = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
                "testset = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#build function to count caolumn that has missing value\n",
                "def column_nan(missing):\n",
                "    s = []\n",
                "    for i in missing:\n",
                "        if i > 0:\n",
                "            s.append(i)\n",
                "    print(len(s))\n",
                "train = trainset.isnull().sum().sort_values(ascending = False)\n",
                "test = testset.isnull().sum().sort_values(ascending = False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(\"Sum of column has Nan in trainset:\")\n",
                "column_nan(train)\n",
                "print(\"Sum of column has Nan in testset:\")\n",
                "column_nan(test)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#summing missing value trainset\n",
                "trainset.isnull().sum().sort_values(ascending = False).head(20)/len(trainset)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#summing percentage missing value\n",
                "testset.isnull().sum().sort_values(ascending = False).head(33)/len(trainset)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#clean columns 'PoolQC','MiscFeature','Alley','Fence' because columns has missing values more than 80%\n",
                "cleaning_train = trainset.drop(['PoolQC','MiscFeature','Alley','Fence'], axis= 1)\n",
                "cleaning_test = testset.drop(['PoolQC','MiscFeature','Alley','Fence'], axis= 1)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#cleaning trainig_set\n",
                "s = cleaning_train.isnull().sum(axis=0).reset_index().sort_values(0,ascending=False).head(15)\n",
                "s.columns = ['variable','missing']\n",
                "col_miss_train = s['variable'].tolist()\n",
                "miss = cleaning_train[col_miss_train]\n",
                "miss.describe()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# see distribution\n",
                "fig,axes = plt.subplots(1,3, figsize = (20,6))\n",
                "sns.distplot(miss['LotFrontage'], color = 'b',ax = axes[0])\n",
                "sns.distplot(miss['GarageYrBlt'], color = 'r', ax = axes[1])\n",
                "sns.distplot(miss['MasVnrArea'], color = 'y',ax = axes[2])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# clean with median on numerical\n",
                "cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']] = cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']].fillna(cleaning_train[['LotFrontage','GarageYrBlt','MasVnrArea']].median())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# clean with mode on categorical\n",
                "list_miss = cleaning_train.isnull().sum().sort_values(ascending = False).head(12).index.values.tolist()\n",
                "cleaning_train[list_miss] = cleaning_train[list_miss].fillna(cleaning_train[list_miss].mode().iloc[0])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Cleaning testset in numerical\n",
                "list_miss_test = cleaning_test.isnull().sum().sort_values(ascending = False).head(29).index.values.tolist()\n",
                "list_numeric = cleaning_test[list_miss_test].describe().columns.values.tolist()\n",
                "# handling with median\n",
                "cleaning_test[list_numeric] = cleaning_test[list_numeric].fillna(cleaning_test[list_numeric].median())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# cleaning testset in categorical\n",
                "lst_categ = cleaning_test.isnull().sum().sort_values(ascending = False).head(18).index.values.tolist()\n",
                "cleaning_test[lst_categ] = cleaning_test[lst_categ].fillna(cleaning_test[lst_categ].mode().iloc[0])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cleaning_train.isnull().sum().sort_values()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cleaning_test.isnull().sum().sort_values()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import os\n",
                "from shutil import copyfile\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.image import imread\n",
                "import time\n",
                "\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense,Dropout,Activation\n",
                "\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import StratifiedShuffleSplit"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "def plot_img(img):\n",
                "    plt.imshow(img, cmap='gray')\n",
                "    plt.axis(\"off\")\n",
                "    plt.show()\n",
                "    \n",
                "def plot_img_by_id(id, species = ''):\n",
                "    src = './LeafClassification/' + str(id) + '.jpg'\n",
                "    img = imread(src)\n",
                "    plt.imshow(img, cmap='gray')\n",
                "    plt.suptitle('Predicted species: ' + species)\n",
                "    plt.axis(\"off\")\n",
                "    plt.show()\n",
                "    \n",
                "def plot_img_by_species(species):\n",
                "    ldir = './training_data/' + str(species) + '/'\n",
                "    plt.figure(figsize=(28,28))\n",
                "    #plt.suptitle('Predicted species: ' + species)\n",
                "    x, y = len(os.listdir(ldir)), 1\n",
                "     \n",
                "    i = 1\n",
                "    print(species)\n",
                "    for d in os.listdir(ldir):\n",
                "        src = ldir + d\n",
                "        img = imread(src)\n",
                "        \n",
                "        plt.subplot(y, x, i)\n",
                "        plt.imshow(img, cmap='gray')\n",
                "        plt.axis(\"off\")\n",
                "        i += 1\n",
                "            \n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "source": [
                "# method to reload data\n",
                "def reload_data():\n",
                "    # Load test & train datasets\n",
                "    train_data = pd.read_csv(\"train.csv\")\n",
                "    test_data = pd.read_csv(\"test.csv\")\n",
                "    df = [train_data, test_data]\n",
                "    df = pd.concat(df, axis=0, sort=False)\n",
                "    \n",
                "    return train_data, test_data, df\n",
                "\n",
                "# load predictions\n",
                "def load_pred():\n",
                "    # id should be index\n",
                "    pred = pd.read_csv(\"predictions.csv\", index_col='id')\n",
                "    return pred\n",
                "\n",
                "train_data, test_data, df = reload_data()\n",
                "# for backup reason\n",
                "train_data_copy, test_data_copy, df = reload_data()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print(train_data.shape)\n",
                "train_data.describe()\n",
                "train_data.head()\n",
                "\n",
                "print(df.shape)\n",
                "df.describe()\n",
                "df.head()\n",
                "\n",
                "print(test_data.shape)\n",
                "test_data.describe()\n",
                "test_data.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# remove column 'id' from train data und save in variable train_id\n",
                "train_id = train_data.pop('id')\n",
                "test_id = test_data.pop('id')\n",
                "\n",
                "# remove column 'species' from train data und save in variable train_y, then transform into categorical\n",
                "train_y = train_data.pop('species')\n",
                "train_y = LabelEncoder().fit(train_y).transform(train_y)\n",
                "train_y = to_categorical(train_y)\n",
                "\n",
                "#scale training data\n",
                "train_x = StandardScaler().fit(train_data).transform(train_data)\n",
                "test_x = StandardScaler().fit(test_data).transform(test_data)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "## retain class balances\n",
                "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2,random_state=12345)\n",
                "train_index, val_index = next(iter(sss.split(train_x, train_y)))\n",
                "x_train, x_val = train_x[train_index], train_x[val_index]\n",
                "y_train, y_val = train_y[train_index], train_y[val_index]\n",
                "print(\"x_train dim: \",x_train.shape)\n",
                "print(\"x_val dim:   \",x_val.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "input_dim = train_x.shape[1]\n",
                "EPOCHS = 100\n",
                "batch_size = 128"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model = Sequential()\n",
                "model.add(Dense(1024,input_dim=input_dim))\n",
                "model.add(Dropout(0.2))\n",
                "model.add(Activation('sigmoid'))\n",
                "model.add(Dense(512))\n",
                "model.add(Dropout(0.3))\n",
                "model.add(Activation('sigmoid'))\n",
                "model.add(Dense(99))\n",
                "model.add(Activation('softmax'))"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# compile model\n",
                "model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "# fit model\n",
                "start = time.time()\n",
                "history = model.fit(train_x,train_y,validation_data=(x_val, y_val),batch_size=batch_size,epoch=EPOCHS,verbose=0)\n",
                "end = time.time()\n",
                "print(round((end-start),2), \"seconds\")"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "check_results"
            ],
            "source": [
                "plt.plot(history.history['acc'])\n",
                "plt.plot(history.history['val_acc'])\n",
                "#plt.title('model accuracy')\n",
                "plt.ylabel('accuracy')\n",
                "plt.xlabel('epoch')\n",
                "plt.legend(['train', 'valid'], loc='lower right')\n",
                "plt.show()\n",
                "\n",
                "print('-'*50)\n",
                "print('Training accuracy: ' + str(max(history.history['acc'])))\n",
                "print('Validation accuracy: ' + str(max(history.history['val_acc'])))\n",
                "print('-'*50)\n",
                "\n",
                "plt.plot(history.history['loss'])\n",
                "plt.plot(history.history['val_loss'])\n",
                "#plt.title('model loss')\n",
                "plt.ylabel('loss')\n",
                "plt.xlabel('epoch')\n",
                "plt.legend(['train', 'valid'], loc='upper right')\n",
                "plt.show()\n",
                "\n",
                "print('-'*50)\n",
                "print('Training loss: ' + str(min(history.history['loss'])))\n",
                "print('Validation loss: ' + str(min(history.history['val_loss'])))\n",
                "print('-'*50)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "predict_y = model.predict_proba(test_x)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "species = train_data_copy.species.unique()\n",
                "predict_out = pd.DataFrame(predict_y,index=test_id,columns=sorted(species))\n",
                "predict_out['predicted species'] = predict_out.idxmax(axis=1)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "predict_out.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "#model.save_weights('./models/leaf_classification_weights_best.h5')\n",
                "model.save('./models/leaf_classification_model_best.h5')"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "# write file to csv\n",
                "fp = open('predictions_neuralnetwork_1.csv','w')\n",
                "fp.write(predict_out.to_csv())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "%matplotlib inline\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import dicom\n",
                "import os\n",
                "import scipy.ndimage\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from skimage import measure, morphology\n",
                "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
                "\n",
                "# Some constants \n",
                "INPUT_FOLDER = '../input/sample_images/'\n",
                "patients = os.listdir(INPUT_FOLDER)\n",
                "patients.sort()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "source": [
                "first_patient = load_scan(INPUT_FOLDER + patients[0])\n",
                "first_patient_pixels = get_pixels_hu(first_patient)\n",
                "plt.hist(first_patient_pixels.flatten(), bins=180, color='c')\n",
                "plt.xlabel(\"Hounsfield Units (HU)\")\n",
                "plt.ylabel(\"Frequency\")\n",
                "plt.show()\n",
                "\n",
                "# Show some slice in the middle\n",
                "plt.imshow(first_patient_pixels[80], cmap=plt.cm.gray)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "pix_resampled, spacing = resample(first_patient_pixels, first_patient, [1,1,1])\n",
                "print(\"Shape before resampling\\t\", first_patient_pixels.shape)\n",
                "print(\"Shape after resampling\\t\", pix_resampled.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def plot_3d(image, threshold=-300):\n",
                "    \n",
                "    # Position the scan upright, \n",
                "    # so the head of the patient would be at the top facing the camera\n",
                "    p = image.transpose(2,1,0)\n",
                "    \n",
                "    verts, faces = measure.marching_cubes(p, threshold)\n",
                "\n",
                "    fig = plt.figure(figsize=(10, 10))\n",
                "    ax = fig.add_subplot(111, projection='3d')\n",
                "\n",
                "    # Fancy indexing: `verts[faces]` to generate a collection of triangles\n",
                "    mesh = Poly3DCollection(verts[faces], alpha=0.1)\n",
                "    face_color = [0.5, 0.5, 1]\n",
                "    mesh.set_facecolor(face_color)\n",
                "    ax.add_collection3d(mesh)\n",
                "\n",
                "    ax.set_xlim(0, p.shape[0])\n",
                "    ax.set_ylim(0, p.shape[1])\n",
                "    ax.set_zlim(0, p.shape[2])\n",
                "\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "segmented_lungs = segment_lung_mask(pix_resampled, False)\n",
                "segmented_lungs_fill = segment_lung_mask(pix_resampled, True)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "MIN_BOUND = -1000.0\n",
                "MAX_BOUND = 400.0\n",
                "    \n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "PIXEL_MEAN = 0.25\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "check_results",
                "ingest_data"
            ],
            "source": [
                "# Code you have previously used to load data\n",
                "import pandas as pd\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from learntools.core import *\n",
                "import numpy as np\n",
                "\n",
                "\n",
                "\n",
                "# Path of the file to read. We changed the directory structure to simplify submitting to a competition\n",
                "iowa_file_path = '../input/train.csv'\n",
                "\n",
                "home_data = pd.read_csv(iowa_file_path)\n",
                "# Create target object and call it y\n",
                "y = home_data.SalePrice\n",
                "# Create X\n",
                "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
                "X = home_data[features]\n",
                "\n",
                "# Split into validation and training data\n",
                "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
                "\n",
                "# Specify Model\n",
                "iowa_model = DecisionTreeRegressor(random_state=1)\n",
                "# Fit Model\n",
                "iowa_model.fit(train_X, train_y)\n",
                "\n",
                "# Make validation predictions and calculate mean absolute error\n",
                "val_predictions = iowa_model.predict(val_X)\n",
                "val_mae = mean_absolute_error(val_predictions, val_y)\n",
                "print(\"Validation MAE when not specifying max_leaf_nodes: {:,.0f}\".format(val_mae))\n",
                "\n",
                "# Using best value for max_leaf_nodes\n",
                "iowa_model = DecisionTreeRegressor(max_leaf_nodes=100, random_state=1)\n",
                "iowa_model.fit(train_X, train_y)\n",
                "val_predictions = iowa_model.predict(val_X)\n",
                "val_mae = mean_absolute_error(val_predictions, val_y)\n",
                "print(\"Validation MAE for best value of max_leaf_nodes: {:,.0f}\".format(val_mae))\n",
                "\n",
                "# Define the model. Set random_state to 1\n",
                "rf_model = RandomForestRegressor(random_state=1)\n",
                "rf_model.fit(train_X, train_y)\n",
                "rf_val_predictions = rf_model.predict(val_X)\n",
                "rf_val_mae = mean_absolute_error(rf_val_predictions, val_y)\n",
                "\n",
                "print(\"Validation MAE for Random Forest Model: {:,.0f}\".format(rf_val_mae))\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "# To improve accuracy, create a new Random Forest model which you will train on all training data\n",
                "rf_model_on_full_data = RandomForestRegressor(random_state = 1)\n",
                "\n",
                "# fit rf_model_on_full_data on all data from the training data\n",
                "rf_model_on_full_data.fit(train_X, train_y)\n"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results",
                "ingest_data"
            ],
            "source": [
                "# path to file you will use for predictions\n",
                "test_data_path = '../input/test.csv'\n",
                "\n",
                "# read test data file using pandas\n",
                "test_data = pd.read_csv(test_data_path)\n",
                "\n",
                "# create test_X which comes from test_data but includes only the columns you used for prediction.\n",
                "# The list of columns is stored in a variable called features\n",
                "test_X = test_data[features]\n",
                "test_X = np.nan_to_num(test_X)\n",
                "\n",
                "# make predictions which we will submit. \n",
                "test_preds = rf_model_on_full_data.predict(test_X)\n",
                "\n",
                "# The lines below shows how to save predictions in format used for competition scoring\n",
                "# Just uncomment them.\n",
                "\n",
                "output = pd.DataFrame({'Id': test_data.Id,\n",
                "                       'SalePrice': test_preds})\n",
                "output.to_csv('submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import sys\n",
                "import os\n",
                "\n",
                "sys.path.append(os.path.abspath('./ivis-explain'))\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import confusion_matrix, average_precision_score, roc_auc_score, classification_report, roc_curve, precision_recall_curve\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "from ivis import Ivis\n",
                "from ivis_explanations import LinearExplainer\n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data"
            ],
            "source": [
                "data = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\n",
                "Y = data['Class']\n",
                "X = data.drop(['Class','Time'], axis=1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, stratify=Y, test_size=0.8, random_state=1234)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# standard_scaler = StandardScaler().fit(train_X[['Time', 'Amount']])\n",
                "# train_X.loc[:, ['Time', 'Amount']] = standard_scaler.transform(train_X[['Time', 'Amount']])\n",
                "# test_X.loc[:, ['Time', 'Amount']] = standard_scaler.transform(test_X[['Time', 'Amount']])\n",
                "\n",
                "standard_scaler = StandardScaler().fit(train_X[['Amount']])\n",
                "train_X.loc[:, ['Amount']] = standard_scaler.transform(train_X[['Amount']])\n",
                "test_X.loc[:, ['Amount']] = standard_scaler.transform(test_X[['Amount']])\n",
                "minmax_scaler = MinMaxScaler().fit(train_X)\n",
                "train_X = minmax_scaler.transform(train_X)\n",
                "test_X = minmax_scaler.transform(test_X)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "ivis = Ivis(embedding_dims=2, model='maaten',\n",
                "            k=15, n_epochs_without_progress=5,\n",
                "            supervision_weight=0.95,\n",
                "            verbose=0)\n",
                "ivis.fit(train_X, train_Y.values)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "ivis.save_model('ivis-supervised-fraud', overwrite=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_embeddings = ivis.transform(train_X)\n",
                "test_embeddings = ivis.transform(test_X)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig, ax = plt.subplots(1, 2, figsize=(17, 7), dpi=200)\n",
                "ax[0].scatter(x=train_embeddings[:, 0], y=train_embeddings[:, 1], c=train_Y, s=3, cmap='RdYlBu_r')\n",
                "ax[0].set_xlabel('ivis 1')\n",
                "ax[0].set_ylabel('ivis 2')\n",
                "ax[0].set_title('Training Set')\n",
                "\n",
                "ax[1].scatter(x=test_embeddings[:, 0], y=test_embeddings[:, 1], c=test_Y, s=3, cmap='RdYlBu_r')\n",
                "ax[1].set_xlabel('ivis 1')\n",
                "ax[1].set_ylabel('ivis 2')\n",
                "ax[1].set_title('Testing Set')"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "clf = LogisticRegression(solver=\"lbfgs\").fit(train_embeddings, train_Y)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "labels = clf.predict(test_embeddings)\n",
                "proba = clf.predict_proba(test_embeddings)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "print(classification_report(test_Y, labels))\n",
                "\n",
                "print('Confusion Matrix')\n",
                "print(confusion_matrix(test_Y, labels))\n",
                "print('Average Precision: '+str(average_precision_score(test_Y, proba[:, 1])))\n",
                "print('ROC AUC: '+str(roc_auc_score(test_Y, labels)))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# retrieve just the probabilities for the positive class\n",
                "pos_probs = proba[:, 1]\n",
                "\n",
                "# calculate roc curve for model\n",
                "fpr, tpr, thresholds = roc_curve(test_Y, pos_probs)\n",
                "\n",
                "# plot no skill roc curve\n",
                "plt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
                "# plot model roc curve\n",
                "plt.plot(fpr, tpr, marker='.', label='Logistic')\n",
                "# axis labels\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "# show the legend\n",
                "plt.legend()\n",
                "# show the plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# calculate the no skill line as the proportion of the positive class\n",
                "no_skill = len(Y[Y==1]) / len(Y)\n",
                "# plot the no skill precision-recall curve\n",
                "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
                "# calculate model precision-recall curve\n",
                "precision, recall, _ = precision_recall_curve(test_Y, pos_probs)\n",
                "# plot the model precision-recall curve\n",
                "plt.plot(recall, precision, marker='.', label='Logistic')\n",
                "# axis labels\n",
                "plt.xlabel('Recall')\n",
                "plt.ylabel('Precision')\n",
                "# show the legend\n",
                "plt.legend()\n",
                "# show the plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# create a histogram of the predicted probabilities\n",
                "plt.hist(pos_probs, bins=100)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import os\n",
                "\n",
                "\n",
                "import copy\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "import cv2\n",
                "import keras\n",
                "from keras import backend as K\n",
                "from keras.models import Model, Sequential\n",
                "from keras.layers import Dense, Dropout, BatchNormalization, Flatten, Input\n",
                "from keras.layers import Conv2D, Activation, GlobalAveragePooling2D\n",
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "from keras.preprocessing.image import load_img, img_to_array\n",
                "from keras.applications.resnet50 import preprocess_input, ResNet50\n",
                "import matplotlib\n",
                "import matplotlib.pylab as plt\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import shap\n",
                "from sklearn.utils import shuffle\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "## get images / labels\n",
                "\n",
                "Images, Classes = get_images()\n",
                "\n",
                "Images.shape, Classes.shape"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "## visualize some images / labels\n",
                "\n",
                "n_total_images = Images.shape[0]\n",
                "\n",
                "for target_cls in [0, 1, 2]:\n",
                "    \n",
                "    indices = np.where(Classes == target_cls)[0] # get target class indices on Images / Classes\n",
                "    n_target_cls = indices.shape[0]\n",
                "    label = class_to_label[target_cls]\n",
                "    print(label, n_target_cls, n_target_cls/n_total_images)\n",
                "\n",
                "    n_cols = 10 # # of sample plot\n",
                "    fig, axs = plt.subplots(ncols=n_cols, figsize=(25, 3))\n",
                "\n",
                "    for i in range(n_cols):\n",
                "\n",
                "        axs[i].imshow(np.uint8(Images[indices[i]]))\n",
                "        axs[i].axis('off')\n",
                "        axs[i].set_title(label)\n",
                "\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "## split train / test\n",
                "\n",
                "indices_train, indices_test = train_test_split(list(range(Images.shape[0])), train_size=0.8, test_size=0.2, shuffle=False)\n",
                "\n",
                "x_train = Images[indices_train]\n",
                "y_train = Classes[indices_train]\n",
                "x_test = Images[indices_test]\n",
                "y_test = Classes[indices_test]\n",
                "\n",
                "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "## to one-hot\n",
                "\n",
                "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
                "y_test = keras.utils.to_categorical(y_test, n_classes)\n",
                "\n",
                "y_train.shape, y_test.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "import keras\n",
                "import tensorflow as tf\n",
                "from keras.applications.mobilenet_v2 import MobileNetV2\n",
                "\n",
                "def build_model():\n",
                "    \"\"\"build model function\"\"\"\n",
                "    \n",
                "    # Resnet\n",
                "    input_tensor = Input(shape=(W, H, 3)) # To change input shape\n",
                "    densenet_121 = MobileNetV2(\n",
                "        include_top=False,                # To change output shape\n",
                "        weights='imagenet',               # Use pre-trained model\n",
                "        input_tensor=input_tensor,        # Change input shape for this task\n",
                "    )\n",
                "    \n",
                "    # fc layer\n",
                "    top_model = Sequential()\n",
                "    top_model.add(GlobalAveragePooling2D())               # Add GAP for cam\n",
                "    top_model.add(Dense(n_classes, activation='softmax')) # Change output shape for this task\n",
                "    \n",
                "    # model\n",
                "    model = Model(input=densenet_121.input, output=top_model(densenet_121.output))\n",
                "    \n",
                "    # frozen weights\n",
                "    for layer in model.layers[:-11]:\n",
                "        layer.trainable = False or isinstance(layer, BatchNormalization) # If Batch Normalization layer, it should be trainable\n",
                "        \n",
                "    # compile\n",
                "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
                "    \n",
                "    return model"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model = build_model()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "model.summary()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "## finetuning\n",
                "\n",
                "batch_size = 32\n",
                "\n",
                "history = model.fit_generator(\n",
                "    datagen_train.flow(x_train, y_train, batch_size=32),\n",
                "    epochs= 50,\n",
                "    validation_data=datagen_test.flow(x_test, y_test, batch_size=32),\n",
                "    verbose = 1,\n",
                "    #callbacks=callbacks,\n",
                "    steps_per_epoch=  int(len(x_train)//batch_size),\n",
                "    validation_steps= int(len(x_test)// batch_size)\n",
                ")"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "source": [
                "## plot confusion matrix\n",
                "\n",
                "x = preprocess_input(copy.deepcopy(x_test))\n",
                "y_preds = model.predict(x)\n",
                "y_preds = np.argmax(y_preds, axis=1)\n",
                "y_trues = np.argmax(y_test, axis=1)\n",
                "cm = confusion_matrix(y_trues, y_preds)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(7, 6))\n",
                "\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'shrink': .3}, linewidths=.1, ax=ax)\n",
                "\n",
                "ax.set(\n",
                "    xticklabels=list(label_to_class.keys()),\n",
                "    yticklabels=list(label_to_class.keys()),\n",
                "    title='confusion matrix',\n",
                "    ylabel='True label',\n",
                "    xlabel='Predicted label'\n",
                ")\n",
                "params = dict(rotation=45, ha='center', rotation_mode='anchor')\n",
                "plt.setp(ax.get_yticklabels(), **params)\n",
                "plt.setp(ax.get_xticklabels(), **params)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def superimpose(img, cam):\n",
                "    \"\"\"superimpose original image and cam heatmap\"\"\"\n",
                "    \n",
                "    heatmap = cv2.resize(cam, (img.shape[1], img.shape[0]))\n",
                "    heatmap = np.uint8(255 * heatmap)\n",
                "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
                "\n",
                "    superimposed_img = heatmap * .45 + img * 1.2\n",
                "    superimposed_img = np.minimum(superimposed_img, 255.0).astype(np.uint8)  # scale 0 to 255  \n",
                "    superimposed_img = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n",
                "    \n",
                "    return img, heatmap, superimposed_img"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def _plot(model, cam_func, img, cls_true):\n",
                "    \"\"\"plot original image, heatmap from cam and superimpose image\"\"\"\n",
                "    \n",
                "    # for cam\n",
                "    x = np.expand_dims(img, axis=0)\n",
                "    x = preprocess_input(copy.deepcopy(x))\n",
                "\n",
                "    # for superimpose\n",
                "    img = np.uint8(img)\n",
                "\n",
                "    # cam / superimpose\n",
                "    cls_pred, cam = cam_func(model=model, x=x, layer_name=model.layers[-2].name)\n",
                "    img, heatmap, superimposed_img = superimpose(img, cam)\n",
                "\n",
                "    fig, axs = plt.subplots(ncols=2, figsize=(8, 6))\n",
                "\n",
                "    axs[0].imshow(img)\n",
                "    #axs[0].set_title('True label: ' + class_to_label[cls_true] + ' / Predicted label : ' + class_to_label[cls_pred])\n",
                "    axs[0].axis('off')\n",
                "\n",
                "    #axs[1].imshow(heatmap)\n",
                "    #axs[1].set_title('heatmap')\n",
                "    #axs[1].axis('off')\n",
                "\n",
                "    axs[1].imshow(superimposed_img)\n",
                "    #axs[1].set_title(class_to_label[cls_true])\n",
                "    axs[1].axis('off')\n",
                "\n",
                "    plt.suptitle('True label: ' + class_to_label[cls_true] + ' / Predicted label : ' + class_to_label[cls_pred])\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    #fig.savefig(\"colon_aca_prewitt.jpeg\",bbox_inches='tight', pad_inches=0)\n",
                "    \n",
                "    "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "for i in range(200):\n",
                "\n",
                "    _plot(model=model, cam_func=grad_cam, img=Images[i], cls_true=Classes[i])\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "sns.set()\n",
                "from PIL import Image\n",
                "from glob import glob\n",
                "from skimage.io import imread\n",
                "from os import listdir\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "import time\n",
                "import cv2\n",
                "import copy\n",
                "from random import shuffle\n",
                "from tqdm import tqdm_notebook as tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from keras.utils.np_utils import to_categorical\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import classification_report\n",
                "from sklearn.metrics import plot_roc_curve\n",
                "from sklearn.metrics import precision_recall_fscore_support\n",
                "from imblearn.metrics import sensitivity_specificity_support\n",
                "from imgaug import augmenters as iaa\n",
                "import imgaug as ia\n",
                "\n",
                "\n",
                "# import numpy as np\n",
                "# import matplotlib.pyplot as plt\n",
                "from itertools import cycle\n",
                "\n",
                "# from sklearn import svm, datasets\n",
                "from sklearn.metrics import roc_curve, auc\n",
                "# from sklearn.model_selection import train_test_split\n",
                "# from sklearn.preprocessing import label_binarize\n",
                "# from sklearn.multiclass import OneVsRestClassifier\n",
                "from scipy import interp\n",
                "from sklearn.metrics import roc_auc_score\n",
                "\n",
                "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
                "from keras.utils.vis_utils import plot_model\n",
                "from keras.optimizers import SGD,Adam\n",
                "import numpy as np\n",
                "from keras.applications.vgg16 import VGG16\n",
                "from keras.layers import Dense, GlobalAveragePooling2D, Dropout,concatenate\n",
                "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, BatchNormalization\n",
                "from keras.layers import Input\n",
                "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard,CSVLogger\n",
                "# import tools\n",
                "import gc\n",
                "from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix\n",
                "from keras.models import Model\n",
                "import keras\n",
                "# import channel_attention"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "folder = os.listdir(\"../input/lung-colon-sobel/trainable_sobel\")\n",
                "print(folder)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "base_path = \"../input/lung-colon-sobel/trainable_sobel\"\n",
                "total_images = 0\n",
                "image_class =[]\n",
                "for n in range(len(folder)):\n",
                "  image_path = os.path.join(base_path, folder[n]) \n",
                "  print(image_path)\n",
                "  # class_path = patient_path + \"/\" + str(c) + \"/\"\n",
                "  subfiles = os.listdir(image_path)\n",
                "  print(len(subfiles))\n",
                "  image_class.append(len(subfiles))\n",
                "  total_images += len(subfiles)\n",
                "print(\"The number of total images are:{}\".format(total_images))  \n",
                "print(image_class)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data = pd.DataFrame(index=np.arange(0, total_images), columns=[\"path\", \"target\"])\n",
                "\n",
                "k = 0\n",
                "for n in range(len(folder)):\n",
                "    class_id = folder[n]\n",
                "    final_path = os.path.join(base_path,class_id) \n",
                "    subfiles = os.listdir(final_path)\n",
                "    for m in range(len(subfiles)):\n",
                "      image_path = subfiles[m]\n",
                "      data.iloc[k][\"path\"] = os.path.join(final_path,image_path)\n",
                "      data.iloc[k][\"target\"] = class_id\n",
                "      k += 1  \n",
                "\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data['target'].unique()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# creating instance of labelencoder\n",
                "labelencoder = LabelEncoder()\n",
                "# Assigning numerical values and storing in another column\n",
                "data['target_label'] = labelencoder.fit_transform(data['target'])\n",
                "data = data.sample(frac=1).reset_index(drop=True)\n",
                "data"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# cancer_perc = data.groupby(\"patient_id\").target.value_counts()/ data.groupby(\"patient_id\").target.size()\n",
                "# cancer_perc = cancer_perc.unstack()\n",
                "\n",
                "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
                "sns.distplot(data.groupby(\"target_label\").size(), ax=ax[0], color=\"Orange\", kde=False)\n",
                "ax[0].set_xlabel(\"Number of images\")\n",
                "ax[0].set_ylabel(\"Frequency\");\n",
                "\n",
                "sns.countplot(data.target, palette=\"Set2\", ax=ax[1]);\n",
                "ax[1].set_xlabel(\"Names of Class\")\n",
                "ax[1].set_title(\"Data Distribution\");"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "X = data.path\n",
                "y = data.target_label\n",
                "X_train, X_test_sub ,y_train,y_test_sub= train_test_split(X,y, test_size=0.3, random_state=0,shuffle = True)\n",
                "print(X_train.shape)\n",
                "print(X_test_sub.shape)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "X_test,X_valid,y_test,y_valid = train_test_split(X_test_sub, y_test_sub, test_size=0.5, random_state=0 , shuffle =False)\n",
                "print(X_test.shape)\n",
                "print(X_valid.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig, ax = plt.subplots(1,3,figsize=(20,5))\n",
                "sns.countplot(y_train, ax=ax[0], palette=\"Reds\")\n",
                "ax[0].set_title(\"Train data\")\n",
                "sns.countplot(y_valid, ax=ax[1], palette=\"Blues\")\n",
                "ax[1].set_title(\"Dev data\")\n",
                "sns.countplot(y_test, ax=ax[2], palette=\"Greens\");\n",
                "ax[2].set_title(\"Test data\");"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.path.values"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "target_label_map = {k:v for k,v in zip(data.path.values,data.target_label.values)}"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "from keras.layers import Activation, Reshape, Lambda, dot, add\n",
                "from keras.layers import Conv1D, Conv2D, Conv3D\n",
                "from keras.layers import MaxPool1D,GlobalAveragePooling2D,Dense,multiply,Activation,concatenate\n",
                "from keras import backend as K\n",
                "\n",
                "\n",
                "def squeeze_excitation_layer(x, out_dim, ratio = 4, concate = True):\n",
                "    '''\n",
                "    SE module performs inter-channel weighting.\n",
                "    '''\n",
                "    squeeze = GlobalAveragePooling2D()(x)\n",
                "\n",
                "    excitation = Dense(units=out_dim //ratio)(squeeze)\n",
                "    excitation = Activation('relu')(excitation)\n",
                "    excitation = Dense(units=out_dim)(excitation)\n",
                "    excitation = Activation('sigmoid')(excitation)\n",
                "    print(excitation.shape)\n",
                "    excitation = Reshape((1, 1, out_dim))(excitation)\n",
                "\n",
                "    scale = multiply([x, excitation])\n",
                "\n",
                "    if concate:\n",
                "        scale = concatenate([scale, x],axis=3)\n",
                "    return scale"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "\n",
                "\n",
                "adam = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0009)\n",
                "sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
                "\n",
                "input_tensor = Input(shape=(224,224, 3))\n",
                "#backbone\n",
                "base_model = ResNet50(input_tensor= input_tensor, weights='imagenet', include_top=False)\n",
                "base_output = base_model.output\n",
                "print(base_output.shape)\n",
                "# channel-attention\n",
                "x = squeeze_excitation_layer(base_output, 2048, ratio=4, concate=False)\n",
                "x = BatchNormalization()(x)\n",
                "\n",
                "# #concat\n",
                "x = concatenate([base_output, x], axis=3)\n",
                "# spp\n",
                "\n",
                "gap = GlobalAveragePooling2D()(x)\n",
                "x = Flatten()(x)\n",
                "x = concatenate([gap,x])\n",
                "x = Dense(512, activation='relu')(x)\n",
                "x = BatchNormalization()(x)\n",
                "x = Dense(512, activation='relu')(x)\n",
                "x = BatchNormalization()(x)\n",
                "predict = Dense(5, activation='softmax')(x)\n",
                "model = Model(inputs=input_tensor, outputs=predict)\n",
                "\n",
                "for layer in (base_model.layers):\n",
                "    layer.trainable = False\n",
                "\n",
                "model.compile(optimizer=adam,\n",
                "                      loss='categorical_crossentropy',\n",
                "                      metrics=[keras.metrics.categorical_accuracy])    \n",
                "\n",
                "# for l in model.layers:\n",
                "#   print(l.name)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "\n",
                "model.summary()\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "batch_size=32\n",
                "history = model.fit_generator(\n",
                "    data_gen(X_train, target_label_map, batch_size, augment=True),\n",
                "    validation_data=data_gen(X_valid, target_label_map, batch_size),\n",
                "    epochs=50, \n",
                "    verbose = 1,\n",
                "    #callbacks=callbacks,\n",
                "    steps_per_epoch=  int(len(X_train)//batch_size),\n",
                "    validation_steps= int(len(X_valid)// batch_size)\n",
                ")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential, save_model, load_model\n",
                "\n",
                "filepath = './'"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "tf.keras.models.save_model(\n",
                "    model,\n",
                "    filepath,\n",
                "    overwrite=True,\n",
                "    include_optimizer=True,\n",
                "    save_format=None,\n",
                "    signatures=None,\n",
                "    options=None\n",
                ")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "sns.set()\n",
                "from PIL import Image\n",
                "from glob import glob\n",
                "from skimage.io import imread\n",
                "from os import listdir\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "import time\n",
                "import cv2\n",
                "import copy\n",
                "from random import shuffle\n",
                "from tqdm import tqdm_notebook as tqdm\n",
                "from sklearn.model_selection import train_test_split\n",
                "from keras.utils.np_utils import to_categorical\n",
                "from sklearn.metrics import accuracy_score\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import classification_report\n",
                "from sklearn.metrics import plot_roc_curve\n",
                "from sklearn.metrics import precision_recall_fscore_support\n",
                "from imblearn.metrics import sensitivity_specificity_support\n",
                "from imgaug import augmenters as iaa\n",
                "import imgaug as ia\n",
                "import tensorflow as tf\n",
                "\n",
                "# import numpy as np\n",
                "# import matplotlib.pyplot as plt\n",
                "from itertools import cycle\n",
                "\n",
                "# from sklearn import svm, datasets\n",
                "from sklearn.metrics import roc_curve, auc\n",
                "# from sklearn.model_selection import train_test_split\n",
                "# from sklearn.preprocessing import label_binarize\n",
                "# from sklearn.multiclass import OneVsRestClassifier\n",
                "from scipy import interp\n",
                "from sklearn.metrics import roc_auc_score\n",
                "\n",
                "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
                "from keras.utils.vis_utils import plot_model\n",
                "from keras.optimizers import SGD,Adam\n",
                "import numpy as np\n",
                "\n",
                "# from keras.applications.vgg16 import VGG16\n",
                "from keras.layers import Dense, GlobalAveragePooling2D, Dropout,concatenate\n",
                "from keras.layers import Conv2D, MaxPooling2D, Input, Flatten, BatchNormalization\n",
                "from keras.layers import Input\n",
                "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard,CSVLogger\n",
                "# import tools\n",
                "import gc\n",
                "from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix\n",
                "from keras.models import Model\n",
                "import keras\n",
                "# import channel_attention"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "folder = os.listdir(\"../input/lung-colon-normal/trainable_normal\")\n",
                "print(folder)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "base_path = \"../input/lung-colon-normal/trainable_normal\"\n",
                "total_images = 0\n",
                "image_class =[]\n",
                "for n in range(len(folder)):\n",
                "  image_path = os.path.join(base_path, folder[n]) \n",
                "  print(image_path)\n",
                "  # class_path = patient_path + \"/\" + str(c) + \"/\"\n",
                "  subfiles = os.listdir(image_path)\n",
                "  print(len(subfiles))\n",
                "  image_class.append(len(subfiles))\n",
                "  total_images += len(subfiles)\n",
                "print(\"The number of total images are:{}\".format(total_images))  \n",
                "print(image_class)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data = pd.DataFrame(index=np.arange(0, total_images), columns=[\"path\", \"target\"])\n",
                "\n",
                "k = 0\n",
                "for n in range(len(folder)):\n",
                "    class_id = folder[n]\n",
                "    final_path = os.path.join(base_path,class_id) \n",
                "    subfiles = os.listdir(final_path)\n",
                "    for m in range(len(subfiles)):\n",
                "      image_path = subfiles[m]\n",
                "      data.iloc[k][\"path\"] = os.path.join(final_path,image_path)\n",
                "      data.iloc[k][\"target\"] = class_id\n",
                "      k += 1  \n",
                "\n",
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data['target'].unique()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# creating instance of labelencoder\n",
                "labelencoder = LabelEncoder()\n",
                "# Assigning numerical values and storing in another column\n",
                "data['target_label'] = labelencoder.fit_transform(data['target'])\n",
                "data = data.sample(frac=1).reset_index(drop=True)\n",
                "data"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.iloc[1000,:]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.groupby(\"target_label\").size()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "# cancer_perc = data.groupby(\"patient_id\").target.value_counts()/ data.groupby(\"patient_id\").target.size()\n",
                "# cancer_perc = cancer_perc.unstack()\n",
                "\n",
                "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
                "sns.distplot(data.groupby(\"target_label\").size(), ax=ax[0], color=\"Orange\", kde=False)\n",
                "ax[0].set_xlabel(\"Number of images\")\n",
                "ax[0].set_ylabel(\"Frequency\");\n",
                "\n",
                "sns.countplot(data.target, palette=\"Set2\", ax=ax[1]);\n",
                "ax[1].set_xlabel(\"Names of Class\")\n",
                "ax[1].set_title(\"Data Distribution\");"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.describe()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.target_label"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "X = data.path\n",
                "y = data.target_label\n",
                "X_train, X_test_sub ,y_train,y_test_sub= train_test_split(X,y, test_size=0.3, random_state=0,shuffle = True)\n",
                "print(X_train.shape)\n",
                "print(X_test_sub.shape)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "X_test,X_valid,y_test,y_valid = train_test_split(X_test_sub, y_test_sub, test_size=0.5, random_state=0 , shuffle =False)\n",
                "print(X_test.shape)\n",
                "print(X_valid.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig, ax = plt.subplots(1,3,figsize=(20,5))\n",
                "sns.countplot(y_train, ax=ax[0], palette=\"Reds\")\n",
                "ax[0].set_title(\"Train data\")\n",
                "sns.countplot(y_valid, ax=ax[1], palette=\"Blues\")\n",
                "ax[1].set_title(\"Dev data\")\n",
                "sns.countplot(y_test, ax=ax[2], palette=\"Greens\");\n",
                "ax[2].set_title(\"Test data\");"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.path.values"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "target_label_map = {k:v for k,v in zip(data.path.values,data.target_label.values)}"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "adam = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0009)\n",
                "sgd = SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=False)\n",
                "\n",
                "input_tensor = Input(shape=(224,224, 3))\n",
                "#backbone\n",
                "\n",
                "base_model = tf.keras.applications.ResNet50(input_tensor= input_tensor, weights='imagenet', include_top=False)\n",
                "base_output = base_model.output\n",
                "print(base_output.shape)\n",
                "# channel-attention\n",
                "# x = squeeze_excitation_layer(base_output, 2048, ratio=4, concate=False)\n",
                "# x = BatchNormalization()(x)\n",
                "\n",
                "# #concat\n",
                "# x = concatenate([base_output, x], axis=3)\n",
                "# spp\n",
                "\n",
                "gap = GlobalAveragePooling2D()(base_output)\n",
                "x = Flatten()(base_output)\n",
                "x = concatenate([gap,x])\n",
                "x = Dense(512, activation='relu')(x)\n",
                "x = BatchNormalization()(x)\n",
                "x = Dense(512, activation='relu')(x)\n",
                "x = BatchNormalization()(x)\n",
                "predict = Dense(5, activation='softmax')(x)\n",
                "model = Model(inputs=input_tensor, outputs=predict)\n",
                "\n",
                "for layer in (base_model.layers):\n",
                "    layer.trainable = False\n",
                "\n",
                "# for l in model.layers:\n",
                "#   print(l.name)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "model.compile(optimizer=adam,\n",
                "              \n",
                "                  loss='categorical_crossentropy',\n",
                "                  metrics=[keras.metrics.categorical_accuracy])\n",
                "model.summary()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "batch_size=32\n",
                "history = model.fit_generator(\n",
                "    data_gen(X_train, target_label_map, batch_size, augment=True),\n",
                "    validation_data=data_gen(X_valid, target_label_map, batch_size),\n",
                "    epochs=50, \n",
                "    verbose = 1,\n",
                "    #callbacks=callbacks,\n",
                "    steps_per_epoch=  int(len(X_train)//batch_size),\n",
                "    validation_steps= int(len(X_valid)// batch_size)\n",
                ")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import tensorflow as tf\n",
                "from tensorflow.keras.models import Sequential, save_model, load_model\n",
                "\n",
                "filepath = './'"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "tf.keras.models.save_model(\n",
                "    model,\n",
                "    filepath,\n",
                "    overwrite=True,\n",
                "    include_optimizer=True,\n",
                "    save_format=None,\n",
                "    signatures=None,\n",
                "    options=None\n",
                ")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "from scipy import stats\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve\n",
                "from sklearn.metrics import  recall_score, classification_report\n",
                "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
                "from statsmodels.tools.tools import add_constant\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn import metrics\n",
                "\n",
                "# Plotting the graphs\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "import plotly.graph_objs as go\n",
                "import plotly.offline as py\n",
                "py.init_notebook_mode(connected=True)\n",
                "\n",
                "import os\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "#Importing data\n",
                "lowafilepath = '../input/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
                "data = pd.read_csv(lowafilepath)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "print (\"Rows     : \" ,data.shape[0])\n",
                "print (\"Columns  : \" ,data.shape[1])\n",
                "print (\"\\nFeatures : \\n\" ,data.columns.tolist())\n",
                "print(\"\\nData Information : \\n\",data.info())"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "# Removing the missing values\n",
                "data = data[pd.notnull(data['TotalCharges'])]\n",
                "print(\"Number of null values in total charges:\",data['TotalCharges'].isna().sum())\n",
                "data['TotalCharges'] = pd.to_numeric(data['TotalCharges'],errors='coerce')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.pairplot(data,vars = ['tenure','MonthlyCharges','TotalCharges'], hue=\"Churn\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Getting the categorical variables \n",
                "all_cat_var = data.nunique()[data.nunique()<5].keys().tolist()\n",
                "\n",
                "# Getting the categorical variables without churn\n",
                "cat_var = all_cat_var[:-1]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "def pie_plot(Column):    \n",
                "    ct1 = pd.crosstab(data[Column],data['Churn'])\n",
                "    trace1 = go.Pie(labels = ct1.index,\n",
                "                    values = ct1.iloc[:,0],\n",
                "                    hole=0.3,\n",
                "                    domain=dict(x=[0,.45]))\n",
                "    trace2 = go.Pie(labels = ct1.index,\n",
                "                    values = ct1.iloc[:,1],\n",
                "                    domain=dict(x=[.55,1]),\n",
                "                    hole=0.3)\n",
                "\n",
                "    layout = go.Layout(dict(title = Column + \" distribution in customer attrition \",\n",
                "                                plot_bgcolor  = \"rgb(243,243,243)\",\n",
                "                                paper_bgcolor = \"rgb(243,243,243)\",\n",
                "                                annotations = [dict(text = \"churn customers\",\n",
                "                                                    font = dict(size = 13),\n",
                "                                                    showarrow = False,\n",
                "                                                    x = .15, y = 1),\n",
                "                                               dict(text = \"Non churn customers\",\n",
                "                                                    font = dict(size = 13),\n",
                "                                                    showarrow = False,\n",
                "                                                    x = .88,y = 1)\n",
                "\n",
                "                                              ]\n",
                "                               )\n",
                "                          )\n",
                "\n",
                "    fig = go.Figure(data=[trace1,trace2],layout=layout)\n",
                "    py.iplot(fig)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Removing the customer id\n",
                "del data['customerID'] #customerID is a uninque id so it dosn't give any information"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X = add_constant(data_new)\n",
                "pd.Series([variance_inflation_factor(X.values, i) \n",
                "           for i in range(X.shape[1])], index=X.columns)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "# Splitiing to x and y\n",
                "\n",
                "X = (data_new.loc[:, data_new.columns != 'Churn'])\n",
                "y = (data_new.loc[:, data_new.columns == 'Churn'])\n",
                "print('Shape of X: {}'.format(X.shape))\n",
                "print('Shape of y: {}'.format(y.shape))\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
                "columns = X_train.columns"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "# Logistic regression Model\n",
                "\n",
                "logreg = LogisticRegression()\n",
                "logreg.fit(X_train, y_train)\n",
                "\n",
                "#  Model metrics\n",
                "y_pred = logreg.predict(X_test)\n",
                "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
                "print(confusion_matrix(y_test, y_pred))\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "# Removing unimportant features ['gender','PhoneService','TotalCharges','tenure']\n",
                "X_train.drop(['gender','PhoneService','TotalCharges','tenure'], axis=1, inplace=True)\n",
                "X_test.drop(['gender','PhoneService','TotalCharges','tenure'], axis=1, inplace=True)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "# New model\n",
                "logreg_new = LogisticRegression()\n",
                "logreg_new.fit(X_train, y_train)\n",
                "\n",
                "#  Model metrics\n",
                "y_pred = logreg_new.predict(X_test)\n",
                "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg_new.score(X_test, y_test)))\n",
                "print(confusion_matrix(y_test, y_pred))\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "# Create Decision Tree classifer object\n",
                "clf = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100,max_depth = 3)\n",
                "\n",
                "# Train Decision Tree Classifer\n",
                "clf = clf.fit(X_train,y_train)\n",
                "\n",
                "#Predict the response for test dataset\n",
                "y_pred = clf.predict(X_test)\n",
                "\n",
                "# Model Accuracy, how often is the classifier correct?\n",
                "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
                "print(\"Report : \",  classification_report(y_test, y_pred)) "
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
                "#MAYUKH GHOSH 18BCE0417"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import sklearn as sklearn\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import sys\n",
                "import re"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "data = pd.read_csv(\"/kaggle/input/wildlife-strikes/database.csv\")"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "species = data[\"Species Name\"]\n",
                "species_count=species.value_counts()\n",
                "print(species_count)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "species_count=species_count[species_count>4000]\n",
                "print(species_count)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "top_species = [\"UNKNOWN MEDIUM BIRD\",\"UNKNOWN SMALL BIRD\",\"MOURNING DOVE\", \"GULL\",\"UNKNOWN BIRD\",\"KILLDEER\", \"AMERICAN KESTREL\",\"BARN SWALLOW\"]\n",
                "top_species = species[species.isin(top_species)]\n",
                "print(top_species.value_counts())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(top_species)\n",
                "plt.title(\"Top Species That Impact with Aircraft\")\n",
                "plt.xticks(rotation='vertical')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "top_known_species = [\"MOURNING DOVE\", \"GULL\",\"KILLDEER\", \"AMERICAN KESTREL\",\"BARN SWALLOW\"]\n",
                "top_known_species = species[species.isin(top_known_species)]\n",
                "print(top_known_species.value_counts())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(top_known_species)\n",
                "plt.title(\"Top Known Species That Impact with Aircraft\")\n",
                "plt.xticks(rotation='vertical')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "damage_x"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.bar(damage_x,damage_y)\n",
                "plt.title(\"Parts Damaged in the Aircraft\")\n",
                "plt.xticks(rotation='vertical')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.bar(strike_x,strike_y,color='orange')\n",
                "plt.title(\"Parts Striked in the Aircraft\")\n",
                "plt.xticks(rotation='vertical')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.bar(parts,damage_per_strike,color='red')\n",
                "plt.title(\"Parts Damage per strike in the Aircraft\")\n",
                "plt.xticks(rotation='vertical')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import os\n",
                "import pathlib\n",
                "\n",
                "# Clone the tensorflow models repository if it doesn't already exist\n",
                "if \"models\" in pathlib.Path.cwd().parts:\n",
                "    while \"models\" in pathlib.Path.cwd().parts:\n",
                "        os.chdir('..')\n",
                "elif not pathlib.Path('models').exists():\n",
                "    !git clone --depth 1 https://github.com/tensorflow/models"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import matplotlib\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import os\n",
                "import random\n",
                "import io\n",
                "import imageio\n",
                "import glob\n",
                "import scipy.misc\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "from six import BytesIO\n",
                "from PIL import Image, ImageDraw, ImageFont\n",
                "from IPython.display import display, Javascript\n",
                "from IPython.display import Image as IPyImage\n",
                "\n",
                "import tensorflow as tf\n",
                "\n",
                "from object_detection.utils import label_map_util\n",
                "from object_detection.utils import config_util\n",
                "from object_detection.utils import visualization_utils as viz_utils\n",
                "# from object_detection.utils import colab_utils\n",
                "# this was for the annotator can be done away with in this kaggle env\n",
                "from object_detection.builders import model_builder\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "train_df = pd.read_csv('/kaggle/input/global-wheat-detection/train.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_df['bboxs'] = train_df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=','))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_df.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_df.width.nunique(),train_df.height.nunique()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_df['bbox_adj'] = train_df['bboxs'].map(xyxy_to_yxyx)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "val_counts_image_id = pd.DataFrame(train_df['image_id'].value_counts())"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "val_counts_image_id.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "val_counts_image_id['bbox_count'] = val_counts_image_id['image_id'] "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "val_counts_image_id['image_id']  = val_counts_image_id.index"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "val_counts_image_id.head() # "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cond_1 = val_counts_image_id['bbox_count']==1\n",
                "cond_2 = val_counts_image_id['bbox_count']==2\n",
                "cond_3 = val_counts_image_id['bbox_count']==3"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "val_counts_image_id['image_id'][cond_1 | cond_2]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "image_names_list = val_counts_image_id['image_id'][cond_1 | cond_2].tolist()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "image_names_list # only images with one or 2  or 3 wheat heads higher numbers dont converge well... for now..."
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_df.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tf.keras.backend.clear_session()\n",
                "\n",
                "print('Building model and restoring weights for fine-tuning...', flush=True)\n",
                "num_classes = 1\n",
                "pipeline_config = '/kaggle/working/models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
                "checkpoint_path = '/kaggle/working/models/research/object_detection/test_data/checkpoint/ckpt-0'\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model"
            ],
            "source": [
                "%%time\n",
                "tf.keras.backend.set_learning_phase(True)\n",
                "\n",
                "# These parameters can be tuned; since our training set has 5 images\n",
                "# it doesn't make sense to have a much larger batch size, though we could\n",
                "# fit more examples in memory if we wanted to.\n",
                "batch_size = 4\n",
                "learning_rate = 0.001\n",
                "num_batches = 200\n",
                "\n",
                "# optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
                "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,  decay=learning_rate / num_batches)\n",
                "train_step_fn = get_model_train_step_function(\n",
                "    detection_model, optimizer, to_fine_tune)\n",
                "\n",
                "\n",
                "print('Start fine-tuning!', flush=True)\n",
                "for idx in range(num_batches):\n",
                "  # Grab keys for a random subset of examples\n",
                "  all_keys = list(range(len(train_images_np)))\n",
                "  random.shuffle(all_keys)\n",
                "  example_keys = all_keys[:batch_size]\n",
                "\n",
                "  # Note that we do not do data augmentation in this demo.  If you want a\n",
                "  # a fun exercise, we recommend experimenting with random horizontal flipping\n",
                "  # and random cropping :)\n",
                "  gt_boxes_list = [gt_box_tensors[key] for key in example_keys]\n",
                "  gt_classes_list = [gt_classes_one_hot_tensors[key] for key in example_keys]\n",
                "  image_tensors = [train_image_tensors[key] for key in example_keys]\n",
                "\n",
                "  # Training step (forward pass + backwards pass)\n",
                "  total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)\n",
                "\n",
                "  if idx % 10 == 0:\n",
                "    print('batch ' + str(idx) + ' of ' + str(num_batches)\n",
                "    + ', loss=' +  str(total_loss.numpy()), flush=True)\n",
                "\n",
                "print('Done fine-tuning!')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "os.listdir(test_image_dir)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "#Import library\n",
                "from IPython.display import Image# Load image from local storage\n",
                "Image(filename = 'inf_348a992bb.jpg', width = 512, height = 512)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "# source https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/dicom.ipynb#scrollTo=WodUv8O1VKmr"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "list(os.listdir(\"../input/rsna-str-pulmonary-embolism-detection\"))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "setup_notebook"
            ],
            "source": [
                "%%time\n",
                "DATA_PATH = \"../input/rsna-str-pulmonary-embolism-detection\"\n",
                "\n",
                "train = pd.read_csv(f\"{DATA_PATH}/train.csv\")\n",
                "test = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
                "sample_submission = pd.read_csv(f\"{DATA_PATH}/sample_submission.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.head(10)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.info()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import tensorflow as tf\n",
                "import tensorflow_io as tfio\n",
                "from pathlib import Path\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "DATA_PATH = Path(DATA_PATH)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "type(skipped), type(skipped.numpy()), skipped.numpy() "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "image.numpy().shape, lossy_image.numpy().shape"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig, axes = plt.subplots(1,3, figsize=(10,10))\n",
                "\n",
                "axes[0].imshow(np.squeeze(image.numpy()), cmap='gray')\n",
                "axes[0].set_title('image')\n",
                "axes[1].imshow(np.squeeze(lossy_image.numpy()), cmap='gray')\n",
                "axes[1].set_title('lossy image');\n",
                "axes[2].imshow(np.squeeze(lossy_image.numpy() - image.numpy()), cmap='gray')\n",
                "axes[2].set_title('diff b/w images');"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "fig, axes = plt.subplots(3,1, figsize=(20,20))\n",
                "\n",
                "axes[0].imshow(np.squeeze(image.numpy()), cmap='gray')\n",
                "axes[0].set_title('image')\n",
                "axes[1].imshow(np.squeeze(lossy_image.numpy()), cmap='gray')\n",
                "axes[1].set_title('lossy image');"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "np.sum(image.numpy() - lossy_image.numpy())"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "#     for filename in filenames:\n",
                "#         print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torchvision import datasets, transforms\n",
                "from torch.utils.data import Dataset\n",
                "import torchaudio\n",
                "import pandas as pd\n",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(device)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "csvData = pd.read_csv('/kaggle/input/urbansound8k/UrbanSound8K.csv')\n",
                "print(csvData.iloc[0, :])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "csvData.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "import IPython.display as ipd\n",
                "ipd.Audio('/kaggle/input/urbansound8k/fold1/108041-9-0-5.wav')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                " sound, sound [0] , sound [1] # an array and freq"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "soundData = torch.mean(sound[0], dim=0).unsqueeze(1)\n",
                "# was as below\n",
                "# soundData = torch.mean(sound[0], dim=0).unsqueeze(0)# add a dim at idx 0 <-unsqueeze? \n",
                "# dim 0 is where the second channel comes in\n",
                "soundData, soundData.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "soundData.numel()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tempData.shape"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "soundData = tempData\n",
                "soundFormatted = torch.zeros([32000, 1])\n",
                "soundFormatted[:32000] = soundData[::5] #take every fifth sample of soundData\n",
                "soundFormatted.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "soundFormatted = soundFormatted.permute(1, 0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "soundFormatted.shape"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "class Net(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(Net, self).__init__()\n",
                "        self.conv1 = nn.Conv1d(1, 128, 80, 4)\n",
                "        self.bn1 = nn.BatchNorm1d(128)\n",
                "        self.pool1 = nn.MaxPool1d(4)\n",
                "        self.conv2 = nn.Conv1d(128, 128, 3)\n",
                "        self.bn2 = nn.BatchNorm1d(128)\n",
                "        self.pool2 = nn.MaxPool1d(4)\n",
                "        self.conv3 = nn.Conv1d(128, 256, 3)\n",
                "        self.bn3 = nn.BatchNorm1d(256)\n",
                "        self.pool3 = nn.MaxPool1d(4)\n",
                "        self.conv4 = nn.Conv1d(256, 512, 3)\n",
                "        self.bn4 = nn.BatchNorm1d(512)\n",
                "        self.pool4 = nn.MaxPool1d(4)\n",
                "        self.avgPool = nn.AvgPool1d(30) #input should be 512x30 so this outputs a 512x1\n",
                "        self.fc1 = nn.Linear(512, 10)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = self.conv1(x)\n",
                "        x = F.relu(self.bn1(x))\n",
                "        x = self.pool1(x)\n",
                "        x = self.conv2(x)\n",
                "        x = F.relu(self.bn2(x))\n",
                "        x = self.pool2(x)\n",
                "        x = self.conv3(x)\n",
                "        x = F.relu(self.bn3(x))\n",
                "        x = self.pool3(x)\n",
                "        x = self.conv4(x)\n",
                "        x = F.relu(self.bn4(x))\n",
                "        x = self.pool4(x)\n",
                "        x = self.avgPool(x)\n",
                "        x = x.permute(0, 2, 1) #change the 512x1 to 1x512\n",
                "        x = self.fc1(x)\n",
                "        return F.log_softmax(x, dim = 2)\n",
                "\n",
                "model = Net()\n",
                "model.to(device)\n",
                "print(model)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 0.0001)\n",
                "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 0.1)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "def train(model, epoch):\n",
                "    model.train()\n",
                "    for batch_idx, (data, target) in enumerate(train_loader):\n",
                "        optimizer.zero_grad()\n",
                "        data = data.to(device)\n",
                "        target = target.to(device)\n",
                "        data = data.requires_grad_() #set requires_grad to True for training\n",
                "        output = model(data)\n",
                "        output = output.permute(1, 0, 2) #original output dimensions are batchSizex1x10 \n",
                "        loss = F.nll_loss(output[0], target) #the loss functions expects a batchSizex10 input\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        if batch_idx % log_interval == 0: #print training stats\n",
                "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
                "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
                "                100. * batch_idx / len(train_loader), loss))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "def test(model, epoch):\n",
                "    model.eval()\n",
                "    correct = 0\n",
                "    for data, target in test_loader:\n",
                "        data = data.to(device)\n",
                "        target = target.to(device)\n",
                "        output = model(data)\n",
                "        output = output.permute(1, 0, 2)\n",
                "        pred = output.max(2)[1] # get the index of the max log-probability\n",
                "        correct += pred.eq(target).cpu().sum().item()\n",
                "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
                "        correct, len(test_loader.dataset),\n",
                "        100. * correct / len(test_loader.dataset)))"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "log_interval = 20\n",
                "for epoch in range(1, 2): # use 41\n",
                "    if epoch == 31:\n",
                "        print(\"First round of training complete. Setting learn rate to 0.001.\")\n",
                "    scheduler.step()\n",
                "    train(model, epoch)\n",
                "    test(model, epoch)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import os\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "df_train = pd.read_csv(\"../input/train_users_2.csv\")\n",
                "df_train.sample(n=5) "
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "df_test = pd.read_csv(\"../input/test_users.csv\")\n",
                "df_test.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
                "df_all.head(n=5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_all.drop('date_first_booking', axis=1, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_all['date_account_created'] = pd.to_datetime(df_all['date_account_created'],format='%Y-%m-%d')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "df_all['timestamp_first_active']=pd.to_datetime(df_all['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "def remove_age_outliers(x, min_value=15, max_value=90):\n",
                "    if np.logical_or(x<=min_value, x>=max_value):\n",
                "        return np.nan\n",
                "    else:\n",
                "        return x"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_all['age'] = df_all['age'].apply(lambda x: remove_age_outliers(x) if(not np.isnan(x)) else x)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_all['age'].fillna(-1, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "df_all.age = df_all.age.astype(int)\n",
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "def check_NaN_Values_in_df(df):\n",
                "    for col in df:\n",
                "        nan_count = df[col].isnull().sum()\n",
                "        \n",
                "        if nan_count != 0:\n",
                "            print (col + \" => \" + str(nan_count) + \" NaN Values\")\n",
                "        "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "check_NaN_Values_in_df(df_all)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_all['first_affiliate_tracked'].fillna(-1, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_all.drop('timestamp_first_active', axis=1, inplace=True)\n",
                "df_all.drop('language', axis=1, inplace=True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df_all = df_all[df_all['date_account_created'] > '2013-02-01']"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df_all.sample(n=5)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "if not os.path.exists(\"output\"):\n",
                "    os.makedirs(\"output\")\n",
                "    \n",
                "df_all.to_csv(\"output/cleaned.csv\", sep=',', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "#For data processing\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "#For visualizations\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "#For ignoring warnings\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "df1 = pd.read_csv(\"../input/titanic/train.csv\")\n",
                "tf1 = pd.read_csv(\"../input/titanic/test.csv\")\n",
                "result = pd.read_csv(\"../input/titanic/gender_submission.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df1.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tf1.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df = df1.copy()\n",
                "df1.describe()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tf = tf1.copy()\n",
                "tf1.describe()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tf.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.dtypes"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tf.dtypes"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "tf.isnull().sum()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.set(rc={'figure.figsize':(15,5)})\n",
                "sns.heatmap(df.isnull(),yticklabels=False)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.heatmap(tf.isnull(),yticklabels=False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df['Survived'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final = pd.concat([df,tf],axis = 0)\n",
                "final.drop(['Survived'],axis = 1,inplace = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final['Age'].isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final['Age'].fillna(final['Age'].median(),inplace = True)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final['Fare'].isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final[\"Fare\"] = final[\"Fare\"].fillna(final[\"Fare\"].median())\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final['Fare'].dtypes"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final[\"Fare\"] = final[\"Fare\"].map(lambda n: np.log(n) if n > 0 else 0)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "new = final['Name'].str.split('.', n=1, expand = True)\n",
                "final['First'] = new[0]\n",
                "final['Last'] = new[1]\n",
                "new1 = final['First'].str.split(',', n=1, expand = True)\n",
                "final['Last Name'] = new1[0]\n",
                "final['Title'] = new1[1]\n",
                "new2 = final['Title'].str.split('', n=1, expand = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final['Title'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final.drop(['First','Last','Name','Last Name'],axis = 1,inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final.replace(to_replace = [ ' Don', ' Rev', ' Dr', ' Mme',\n",
                "        ' Major', ' Sir', ' Col', ' Capt',' Jonkheer'], value = ' Honorary(M)', inplace = True)\n",
                "final.replace(to_replace = [ ' Ms', ' Lady', ' Mlle',' the Countess', ' Dona'], value = ' Honorary(F)', inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "df3 = final.copy()\n",
                "df3 =  df3[:891]\n",
                "df3 = pd.concat([df3,df1['Survived']],axis = 1)\n",
                "df3.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final['Title'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final = pd.get_dummies(final, columns = [\"Title\"])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final[\"Family\"] = final[\"SibSp\"] + final[\"Parch\"] + 1"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final['Single'] = final['Family'].map(lambda s: 1 if s == 1 else 0)\n",
                "final['SmallF'] = final['Family'].map(lambda s: 1 if  s == 2  else 0)\n",
                "final['MedF'] = final['Family'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n",
                "final['LargeF'] = final['Family'].map(lambda s: 1 if s >= 5 else 0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final['Embarked'].fillna(\"S\",inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final = pd.get_dummies(final, columns = [\"Embarked\"], prefix=\"Embarked_from_\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.Cabin.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.Cabin.value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final['Cabin_final'] = df['Cabin'].str[0]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final['Cabin_final'].fillna('Unknown',inplace = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final['Cabin_final'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final.drop(['Cabin'],axis = 1,inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final = pd.get_dummies(final, columns = [\"Cabin_final\"],prefix=\"Cabin_\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.Ticket.unique()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.Ticket.value_counts()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "final['Ticket'] = final['Ticket'].astype(str)\n",
                "final['Ticket_length'] = final.Ticket.apply(len)\n",
                "final['Ticket_length'].astype(int)\n",
                "final['Ticket_length'].unique()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final['Ticket_length'] = np.where(((final.Ticket_length == 3) | (final.Ticket_length == 4) | (final.Ticket_length == 5)),4,final.Ticket_length)\n",
                "\n",
                "final['Ticket_length'] = np.where(((final.Ticket_length == 6)),5,final.Ticket_length)\n",
                "\n",
                "final['Ticket_length'] = np.where(((final.Ticket_length == 7) | (final.Ticket_length == 8) | (final.Ticket_length == 9) | (final.Ticket_length == 10) | (final.Ticket_length == 13)\n",
                "                                 | (final.Ticket_length == 17)| (final.Ticket_length == 16)| (final.Ticket_length == 13)| (final.Ticket_length == 12) | (final.Ticket_length == 15)\n",
                "                                 | (final.Ticket_length == 11)| (final.Ticket_length == 18)),12,final.Ticket_length)\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final['Ticket_length'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final['Ticket_length'] = final['Ticket_length'].astype(str)\n",
                "\n",
                "final['Ticket_length'] = np.where(((final.Ticket_length == '4')),'Below 6',final.Ticket_length)\n",
                "final['Ticket_length'] = np.where(((final.Ticket_length == '5')),'At 6',final.Ticket_length)\n",
                "final['Ticket_length'] = np.where(((final.Ticket_length == '12')),'Above 6',final.Ticket_length)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "conversion = pd.get_dummies(final.Ticket_length, prefix = 'Ticket Length')\n",
                "final = pd.concat([final , conversion], axis = 1)\n",
                " \n",
                "final.drop(['Ticket','Ticket_length'],axis = 1, inplace = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final = pd.get_dummies(final, columns = [\"Sex\"],prefix=\"Gender_\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final.head()\n",
                "final.drop(['PassengerId'],axis = 1,inplace = True)\n",
                "final.drop(['SibSp','Parch','Family'],axis = 1,inplace = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.dtypes"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final.isnull().sum()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(x = 'Survived', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(x = 'Pclass', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(x = 'Title', data = df3)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(x = 'Sex', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.set(rc={'figure.figsize':(40,5)})\n",
                "sns.countplot(x = 'Age', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "x = df1['Age']\n",
                "sns.distplot(x, hist=True, rug=True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "x = df1['Fare']\n",
                "sns.distplot(x, hist=True, rug=True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "x = final['Fare']\n",
                "sns.distplot(x, hist=True, rug=True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.set(rc={'figure.figsize':(15,5)})\n",
                "sns.countplot(x = 'SibSp', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(x = 'Parch', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.countplot(x = 'Embarked', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x ='Survived', y ='Age', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x ='Survived', y ='SibSp', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x ='Survived', y ='Parch', data = df1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x = 'Sex',y='Survived',hue = 'Pclass', kind = 'bar', data = df1, col = 'Pclass', color = 'purple')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x = 'Title',y='Survived',hue = 'Sex', kind = 'bar', data = df3, col = 'Sex', palette = 'GnBu_d',aspect =2)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x = 'SibSp',y='Survived',hue = 'Pclass',kind = 'violin', data = df3, palette = 'BuGn_r', col = 'Pclass')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x = 'Parch',y='Survived',hue = 'Pclass',kind = 'violin', data = df3, palette = 'cubehelix', col = 'Pclass')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x = 'Embarked',y='Survived',kind = 'point', data = df3, hue = 'Pclass', col = 'Pclass')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.jointplot(x=df1['Age'], y=df1['SibSp'], kind = 'kde')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "correlation = final.copy()\n",
                "sur = pd.concat([df['Survived'],result['Survived']],axis = 0)\n",
                "correlation = pd.concat([correlation,sur],axis = 1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(30,30))\n",
                "sns.heatmap(correlation.corr(), annot=True, linewidth=0.5, cmap='coolwarm')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "#The models trained\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import LinearSVC\n",
                "from sklearn import svm\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from xgboost import XGBClassifier\n",
                "from sklearn.naive_bayes import BernoulliNB\n",
                "\n",
                "#For Scaling and Hyperparameter Tuning\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.model_selection import RandomizedSearchCV\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn.metrics import accuracy_score,confusion_matrix\n",
                "from sklearn import metrics\n",
                "\n",
                "#Voting Classifier\n",
                "from sklearn.ensemble import VotingClassifier "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x_train = final[:891]\n",
                "feature_scaler = MinMaxScaler()\n",
                "x_train = feature_scaler.fit_transform(x_train)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "y_train = final[891:]\n",
                "feature_scaler = MinMaxScaler()\n",
                "y_train = feature_scaler.fit_transform(y_train)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x_test = df1['Survived']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "y_test = result['Survived']"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results",
                "process_data"
            ],
            "source": [
                "LR.fit(x_train,x_test)\n",
                "model1pred = LR.predict(y_train)\n",
                "submission1 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission1['PassengerId'] = result['PassengerId']\n",
                "submission1['Survived'] = model1pred\n",
                "submission1.to_csv('LogisticRegression(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "LR.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "SVC = LinearSVC()\n",
                "#estimator.append(('LSVC',LinearSVC()))\n",
                "cv = cross_val_score(SVC,x_train,x_test,cv=10)\n",
                "accuracy2 = cv.mean()\n",
                "accuracy.append(accuracy2)\n",
                "print(cv)\n",
                "print(cv.mean())\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "SVC.fit(x_train,x_test)\n",
                "SVC.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "SVC.fit(x_train,x_test)\n",
                "model2pred = SVC.predict(y_train)\n",
                "submission2 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission2['PassengerId'] = result['PassengerId']\n",
                "submission2['Survived'] = model2pred\n",
                "submission2.to_csv('LinearSVC(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "poly = svm.SVC(kernel = 'poly', gamma = 'scale')\n",
                "#estimator.append(('PSVC',svm.SVC(kernel = 'poly', gamma = 'scale')))\n",
                "cv = cross_val_score(poly,x_train,x_test,cv=10)\n",
                "accuracy3 = cv.mean()\n",
                "accuracy.append(accuracy3)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "poly.fit(x_train,x_test)\n",
                "poly.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "model3pred = poly.predict(y_train)\n",
                "submission3 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission3['PassengerId'] = result['PassengerId']\n",
                "submission3['Survived'] = model3pred\n",
                "submission3.to_csv('PolynomialSVC(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "DT = DecisionTreeClassifier(random_state = 5)\n",
                "estimator.append(('DT',DecisionTreeClassifier(random_state = 5)))\n",
                "cv = cross_val_score(DT,x_train,x_test,cv=10)\n",
                "accuracy4 = cv.mean()\n",
                "accuracy.append(accuracy4)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "DT.fit(x_train,x_test)\n",
                "DT.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "model4pred = DT.predict(y_train)\n",
                "submission4 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission4['PassengerId'] = result['PassengerId']\n",
                "submission4['Survived'] = model4pred\n",
                "submission4.to_csv('Decision Tree(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "GNB = GaussianNB()\n",
                "estimator.append(('GNB',GaussianNB()))\n",
                "cv = cross_val_score(GNB,x_train,x_test,cv=10)\n",
                "accuracy5 = cv.mean()\n",
                "accuracy.append(accuracy5)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "GNB.fit(x_train,x_test)\n",
                "GNB.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "model5pred = GNB.predict(y_train)\n",
                "submission5 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission5['PassengerId'] = result['PassengerId']\n",
                "submission5['Survived'] = model5pred\n",
                "submission5.to_csv('Gaussian NB(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "MNB = MultinomialNB()\n",
                "estimator.append(('MNB',MultinomialNB()))\n",
                "cv = cross_val_score(MNB,x_train,x_test,cv=10)\n",
                "accuracy6 = cv.mean()\n",
                "accuracy.append(accuracy6)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "MNB.fit(x_train,x_test)\n",
                "MNB.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "MNB.fit(x_train,x_test)\n",
                "model6pred = MNB.predict(y_train)\n",
                "submission6 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission6['PassengerId'] = result['PassengerId']\n",
                "submission6['Survived'] = model6pred\n",
                "submission6.to_csv('MultinomialNB(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "RF = RandomForestClassifier(random_state = 5)\n",
                "estimator.append(('RF',RandomForestClassifier(random_state = 5)))\n",
                "cv = cross_val_score(RF,x_train,x_test,cv=10)\n",
                "accuracy7 = cv.mean()\n",
                "accuracy.append(accuracy7)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "RF.fit(x_train,x_test)\n",
                "RF.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "RF.fit(x_train,x_test)\n",
                "model7pred = RF.predict(y_train)\n",
                "submission7 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission7['PassengerId'] = result['PassengerId']\n",
                "submission7['Survived'] = model7pred\n",
                "submission7.to_csv('RandomForest(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "GBC = GradientBoostingClassifier(random_state = 5)\n",
                "estimator.append(('GBC',GradientBoostingClassifier(random_state = 5)))\n",
                "cv = cross_val_score(GBC,x_train,x_test,cv=10)\n",
                "accuracy8 = cv.mean()\n",
                "accuracy.append(accuracy8)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "GBC.fit(x_train,x_test)\n",
                "GBC.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "GBC.fit(x_train,x_test)\n",
                "model8pred = GBC.predict(y_train)\n",
                "submission8 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission8['PassengerId'] = result['PassengerId']\n",
                "submission8['Survived'] = model8pred\n",
                "submission8.to_csv('GradientBoosting(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "XGB = XGBClassifier(random_state = 5)\n",
                "estimator.append(('XGB', XGBClassifier(random_state = 5)))\n",
                "cv = cross_val_score(XGB,x_train,x_test,cv=10)\n",
                "accuracy9 = cv.mean()\n",
                "accuracy.append(accuracy9)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "XGB.fit(x_train,x_test)\n",
                "XGB.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "XGB.fit(x_train,x_test)\n",
                "model9pred = XGB.predict(y_train)\n",
                "submission9 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission9['PassengerId'] = result['PassengerId']\n",
                "submission9['Survived'] = model9pred\n",
                "submission9.to_csv('XGBoosting(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data",
                "train_model"
            ],
            "source": [
                "Krange = range(1,20)\n",
                "scores = {}\n",
                "scores_list = []\n",
                "for k in Krange:\n",
                "    knn = KNeighborsClassifier(n_neighbors = k)\n",
                "    knn.fit(x_train,x_test)\n",
                "    y_pred = knn.predict(y_train)\n",
                "    scores[k] = metrics.accuracy_score(result['Survived'],y_pred)\n",
                "    scores_list.append(metrics.accuracy_score(result['Survived'],y_pred))\n",
                "    \n",
                "plt.plot(Krange,scores_list)\n",
                "plt.xlabel(\"Value of K\")\n",
                "plt.ylabel(\"Accuracy\")"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "KNN = KNeighborsClassifier(n_neighbors = 11)\n",
                "estimator.append(('KNN',KNeighborsClassifier(n_neighbors = 11)))\n",
                "cv = cross_val_score(KNN,x_train,x_test,cv=10)\n",
                "accuracy10 = cv.mean()\n",
                "accuracy.append(accuracy10)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "KNN.fit(x_train,x_test)\n",
                "KNN.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "KNN.fit(x_train,x_test)\n",
                "model10pred = KNN.predict(y_train)\n",
                "submission10 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission10['PassengerId'] = result['PassengerId']\n",
                "submission10['Survived'] = model10pred\n",
                "submission10.to_csv('KNN(No HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "o = output1.groupby(['Models'])['Accuracy'].mean().reset_index().sort_values(by='Accuracy',ascending=False)\n",
                "o.head(10).style.background_gradient(cmap='Reds')\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "vot_soft = VotingClassifier(estimators = estimator, voting ='soft') \n",
                "vot_soft.fit(x_train, x_test) \n",
                "y_pred = vot_soft.predict(y_train)\n",
                "vot_soft.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "modelpred1 = vot_soft.predict(y_train)\n",
                "sub1 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "sub1['PassengerId'] = result['PassengerId']\n",
                "sub1['Survived'] = modelpred1\n",
                "sub1.to_csv('SoftVoting(NO HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "vot_hard = VotingClassifier(estimators = estimator, voting ='hard') \n",
                "vot_hard.fit(x_train, x_test) \n",
                "y_pred = vot_hard.predict(y_train)\n",
                "vot_hard.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "modelpred2 = vot_hard.predict(y_train)\n",
                "sub2 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "sub2['PassengerId'] = result['PassengerId']\n",
                "sub2['Survived'] = modelpred2\n",
                "sub2.to_csv('HardVoting(NO HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "\"\"\"\n",
                "C = [0.01,0.1, 1, 10,50, 100]\n",
                "penalty = ['l2']\n",
                "solver = ['newton-cg','lbfgs','liblinear']\n",
                "class_weight = ['dict','balanced','None']\n",
                "max_iter = [900,1000,1100,1200]\n",
                "\n",
                "Log = LogisticRegression()\n",
                "\n",
                "parameters = {'C': [0.01,0.1, 1, 10,50, 100],'penalty' : ['l2'],'solver' : ['newton-cg','lbfgs','liblinear'],'class_weight':['dict','balanced','None'],'max_iter':[900,1000,1100,1200]}\n",
                "\n",
                "log_regressor = GridSearchCV(Log, parameters, scoring='accuracy',cv =10)\n",
                "log_regressor.fit(x_train, x_test)\n",
                "log_regressor.best_params_\n",
                "\"\"\""
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "lr = LogisticRegression(C = 100,penalty = 'l2', solver = 'newton-cg',class_weight = 'dict', max_iter = 900)\n",
                "Estimator.append(('lr',LogisticRegression(C = 1,penalty = 'l2', solver = 'newton-cg',class_weight = 'dict', max_iter = 900)))\n",
                "cv = cross_val_score(lr,x_train,x_test,cv=10)\n",
                "Accuracy1 = cv.mean()\n",
                "Accuracy.append(Accuracy1)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "lr.fit(x_train,x_test)\n",
                "lr.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "model11pred = lr.predict(y_train)\n",
                "submission11 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission11['PassengerId'] = result['PassengerId']\n",
                "submission11['Survived'] = model11pred\n",
                "submission11.to_csv('LogisticRegression(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "'''\n",
                "penalty = ['l1','l2']\n",
                "loss = ['hinge','squared_hinge']\n",
                "class_weight = ['dict','balanced','None']\n",
                "C = [.1,1,10,50,100,150]\n",
                "\n",
                "SVM = LinearSVC()\n",
                "\n",
                "parameters = {'penalty':['l1','l2'],'loss':['hinge','squared_hinge'],'class_weight':['dict','balanced','None'] ,'C': [.1,1,10,50,100,150]}\n",
                "\n",
                "SVM_classifier = GridSearchCV(SVM, parameters, scoring='accuracy' ,cv =10)\n",
                "SVM_classifier.fit(x_train, x_test)\n",
                "SVM_classifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "svc = LinearSVC(C = 0.1,penalty = 'l2', loss = 'hinge',class_weight = 'balanced')\n",
                "cv = cross_val_score(svc,x_train,x_test,cv=10)\n",
                "Accuracy2 = cv.mean()\n",
                "Accuracy.append(Accuracy2)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "svc.fit(x_train,x_test)\n",
                "svc.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "model12pred = svc.predict(y_train)\n",
                "submission12 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission12['PassengerId'] = result['PassengerId']\n",
                "submission12['Survived'] = model12pred\n",
                "submission12.to_csv('SVCLinear(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "'''\n",
                "kernel = ['poly']\n",
                "degree = [1,2,3]\n",
                "class_weight = ['balanced','dict']\n",
                "C = [.1,1,10,]\n",
                "gamma = ['scale','auto']\n",
                "\n",
                "s = svm.SVC()\n",
                "\n",
                "parameters = {'kernel':['poly'],'class_weight':['balanced','dict'] ,'C': [.1,1,10],'degree':[1,2,3],'gamma':['scale','auto']}\n",
                "\n",
                "svcc = GridSearchCV(s, parameters, scoring='accuracy' ,cv =10)\n",
                "svcc.fit(x_train, x_test)\n",
                "svcc.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "SVM_all = svm.SVC(C = 1,degree = 2, kernel = 'poly',class_weight = 'balanced',gamma = 'scale')\n",
                "cv = cross_val_score(svc,x_train,x_test,cv=10)\n",
                "Accuracy3 = cv.mean()\n",
                "Accuracy.append(Accuracy3)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "SVM_all.fit(x_train,x_test)\n",
                "SVM_all.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "model13pred = SVM_all.predict(y_train)\n",
                "submission13 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission13['PassengerId'] = result['PassengerId']\n",
                "submission13['Survived'] = model13pred\n",
                "submission13.to_csv('PolynomialSVM(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "'''\n",
                "criterion = ['gini','entropy']\n",
                "splitter = ['best','random']\n",
                "max_depth = [5,10,15,20,25]\n",
                "min_samples_split = [2,3,4,5]\n",
                "class_weight = ['dict','balanced','None']\n",
                "random_state = [5,6]\n",
                "\n",
                "\n",
                "Tree = DecisionTreeClassifier()\n",
                "\n",
                "parameters = {'criterion': ['gini','entropy'],'splitter': ['best','random'], 'max_depth':[5,10,15,20,25],'min_samples_split':[2,3,4,5],'class_weight':['dict','balanced','None'],'random_state':[5,6]}\n",
                "\n",
                "tree_classifier = GridSearchCV(Tree, parameters, scoring='accuracy' ,cv = 10)\n",
                "tree_classifier.fit(x_train, x_test)\n",
                "tree_classifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "dt = DecisionTreeClassifier(class_weight = 'balanced',criterion = 'entropy',max_depth = 5,min_samples_split = 2,splitter = 'best',random_state = 6)\n",
                "Estimator.append(('dt',DecisionTreeClassifier(class_weight = 'balanced',criterion = 'entropy',max_depth = 5,min_samples_split = 2,splitter = 'best',random_state = 6)))\n",
                "cv = cross_val_score(dt,x_train,x_test,cv=10)\n",
                "Accuracy4 = cv.mean()\n",
                "Accuracy.append(Accuracy4)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "dt.fit(x_train,x_test)\n",
                "dt.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "model14pred = SVM_all.predict(y_train)\n",
                "submission14 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission14['PassengerId'] = result['PassengerId']\n",
                "submission14['Survived'] = model14pred\n",
                "submission14.to_csv('DecisionTrees(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "'''\n",
                "alpha = [0.01,0.1, 1, 10, 100]\n",
                "fit_prior = [True,False]\n",
                "\n",
                "mnb = MultinomialNB()\n",
                "\n",
                "parameters = {'alpha': [0.01,0.1, 1, 10, 100],'fit_prior' : [True,False]}\n",
                "\n",
                "mn = GridSearchCV(mnb, parameters, scoring='accuracy',cv =10)\n",
                "mn.fit(x_train, x_test)\n",
                "mn.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "mnb = MultinomialNB(alpha = 1,fit_prior = True)\n",
                "Estimator.append(('mnb',MultinomialNB(alpha = 1,fit_prior = True)))\n",
                "cv = cross_val_score(mnb,x_train,x_test,cv=10)\n",
                "Accuracy5 = cv.mean()\n",
                "Accuracy.append(Accuracy5)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "mnb.fit(x_train,x_test)\n",
                "mnb.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "model15pred = mnb.predict(y_train)\n",
                "submission15 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission15['PassengerId'] = result['PassengerId']\n",
                "submission15['Survived'] = model15pred\n",
                "submission15.to_csv('MultinomialNB(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "'''\n",
                "n_estimators = [250,500,750,1000]\n",
                "criterion = ['gini','entropy']\n",
                "max_depth = [5,10,15,20,25]\n",
                "min_samples_split = [2,3,4,5]\n",
                "bootstrap = [True,False]\n",
                "oob_score = [True,False]\n",
                "class_weight = ['balanced','balanced_subsample','dict']\n",
                "max_features = ['auto','sqrt','log2']\n",
                "\n",
                "RF = RandomForestClassifier()\n",
                "\n",
                "parameters = {'n_estimators': [250,500,750,1000],'criterion': ['gini','entropy'],'max_depth':[5,10,15,20,25],'min_samples_split':[2,3,4,5],'bootstrap':[True,False]\n",
                "              ,'oob_score':[True,False],'class_weight':['balanced','balanced_subsample','dict'],'max_features':['auto','sqrt','log2']}\n",
                "\n",
                "RFClassifier = RandomizedSearchCV(RF, parameters, scoring='accuracy' ,cv =50)\n",
                "RFClassifier.fit(x_train, x_test)\n",
                "RFClassifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "\"\"\"\n",
                "n_estimators = [650,700,750,800,850]\n",
                "criterion = ['gini']\n",
                "max_depth = [4,5]\n",
                "min_samples_split = [5,6]\n",
                "bootstrap = [False,True]\n",
                "oob_score = [False,True]\n",
                "class_weight = ['balanced_subsample']\n",
                "max_features = ['log2']\n",
                "\n",
                "rF = RandomForestClassifier()\n",
                "\n",
                "parameters = {'n_estimators': [650,700,750,800,850],'criterion': ['gini'],'max_depth':[5,6],'min_samples_split':[4,5],'bootstrap':[False,True]\n",
                "              ,'oob_score':[False,True],'class_weight':['balanced_subsample'],'max_features':['log2']}\n",
                "\n",
                "RClassifier = GridSearchCV(rF, parameters, scoring='accuracy' ,cv =5)\n",
                "RClassifier.fit(x_train,x_test)\n",
                "RClassifier.best_params_\n",
                "\"\"\""
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "rf = RandomForestClassifier(oob_score = True,n_estimators =650 ,min_samples_split = 4,max_features = 'log2',max_depth =6,criterion = 'gini',class_weight = 'balanced_subsample',bootstrap = True)\n",
                "Estimator.append(('rf',RandomForestClassifier(oob_score = True,n_estimators =650 ,min_samples_split = 4,max_features = 'log2',max_depth =6,criterion = 'gini',class_weight = 'balanced_subsample',bootstrap = True)))\n",
                "cv = cross_val_score(rf,x_train,x_test,cv=10)\n",
                "Accuracy6 = cv.mean()\n",
                "Accuracy.append(Accuracy6)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "rf.fit(x_train,x_test)\n",
                "rf.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "model16pred = rf.predict(y_train)\n",
                "submission16 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission16['PassengerId'] = result['PassengerId']\n",
                "submission16['Survived'] = model16pred\n",
                "submission16.to_csv('RandomForest(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "'''\n",
                "n_estimators = [250,500,750,1000]\n",
                "learning_rate = [.01,.1,1,5]\n",
                "subsample = [.01,.1,1,5]\n",
                "min_samples_split = [2,3,4,5]\n",
                "max_depth = [5,10,15,20,25]\n",
                "loss = ['deviance','exponential']\n",
                "max_features = ['auto','sqrt','log2']\n",
                "\n",
                "GB = GradientBoostingClassifier()\n",
                "\n",
                "parameters = {'n_estimators': [250,500,750,1000],'loss': ['deviance','exponential'],'max_features':['auto','sqrt','log2'],'learning_rate':[.01,.1,1,5],'subsample':[.01,.1,1,5],\n",
                "             'min_samples_split':[2,3,4,5],'max_depth':[5,10,15,20,25]}\n",
                "\n",
                "GBClassifier = RandomizedSearchCV(GB, parameters, scoring='accuracy' ,cv =50)\n",
                "GBClassifier.fit(x_train, x_test)\n",
                "GBClassifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "'''\n",
                "n_estimators = [150,200,250,300,350]\n",
                "learning_rate = [.01,.1]\n",
                "subsample = [.05,.1]\n",
                "min_samples_split = [3,4,5]\n",
                "max_depth = [9,10,11]\n",
                "loss = ['exponential']\n",
                "max_features = ['auto']\n",
                "\n",
                "GB = GradientBoostingClassifier()\n",
                "\n",
                "parameters = {'n_estimators': [150,200,250,300,350],'loss': ['exponential'],'max_features':['auto'],'learning_rate':[.01,.1],'subsample':[.05,.1],\n",
                "             'min_samples_split':[3,4,5],'max_depth':[9,10,11]}\n",
                "\n",
                "GBClassifier = GridSearchCV(GB, parameters, scoring='accuracy' ,cv =5)\n",
                "GBClassifier.fit(x_train, x_test)\n",
                "GBClassifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "gbc = GradientBoostingClassifier(loss = 'exponential',n_estimators =200 ,min_samples_split = 4,max_features = 'auto',max_depth =9,learning_rate = .01,subsample = .1)\n",
                "Estimator.append(('gbc',GradientBoostingClassifier(loss = 'exponential',n_estimators =200 ,min_samples_split = 4,max_features = 'auto',max_depth =9,learning_rate = .01,subsample = .1)))\n",
                "cv = cross_val_score(gbc,x_train,x_test,cv=10)\n",
                "Accuracy7 = cv.mean()\n",
                "Accuracy.append(Accuracy7)\n",
                "print(cv)\n",
                "\n",
                "\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "gbc.fit(x_train,x_test)\n",
                "gbc.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "model17pred = gbc.predict(y_train)\n",
                "submission17 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission17['PassengerId'] = result['PassengerId']\n",
                "submission17['Survived'] = model17pred\n",
                "submission17.to_csv('GradientBoosting(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model"
            ],
            "source": [
                "'''\n",
                "min_child_weight = [1,5,10]\n",
                "gamma = [.5,1,1.5,2,2.5]\n",
                "subsample = [.6,.8,1]\n",
                "colsample_bytree = [.6,.8,1]\n",
                "eta = [.01,.05,.1,.5,.2]\n",
                "max_depth = [3,4,5,6,7,8,9,10]\n",
                "\n",
                "XB = XGBClassifier()\n",
                "\n",
                "parameters = {'min_child_weight': [1,5,10],'gamma': [.5,1,1.5,2,2.5],'subsample':[.6,.8,1],'colsample_bytree':[.6,.8,1],'subsample':[.6,.8,1],\n",
                "             'eta':[.01,.05,.1,.5,.2],'max_depth':[3,4,5,6,7,8,9,10]}\n",
                "\n",
                "XBClassifier = RandomizedSearchCV(XB, parameters, scoring='accuracy' ,cv =50)\n",
                "XBClassifier.fit(x_train, x_test)\n",
                "XBClassifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "train_model"
            ],
            "source": [
                "'''\n",
                "min_child_weight = [4,5,6]\n",
                "gamma = [1,1.5,2.0,2.5,3]\n",
                "subsample = [.6,.8,1,1.2]\n",
                "colsample_bytree = [.6,.8,1,1.2]\n",
                "eta = [.5,.01]\n",
                "\n",
                "max_depth = [5,6,7,8]\n",
                "\n",
                "XB = XGBClassifier()\n",
                "\n",
                "parameters = {'min_child_weight': [4,5,6],'gamma': [1,1.5,2.0,2.5,3],'subsample':[.6,.8,1,1.2],'colsample_bytree':[.6,.8,1,1.2],\n",
                "             'eta':[.5,.01],'max_depth':[5,6,7,8]}\n",
                "\n",
                "XBClassifier = GridSearchCV(XB, parameters, scoring='accuracy' ,cv =5)\n",
                "XBClassifier.fit(x_train, x_test)\n",
                "XBClassifier.best_params_\n",
                "'''"
            ]
        },
        {
            "tags": [
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "xgb = XGBClassifier(colsample_bytree = .6,eta = 0.5,gamma = 1,max_depth = 5,min_child_weight = 6,subsample = 1)\n",
                "Estimator.append(('xgb',XGBClassifier(colsample_bytree = .6,eta = 0.5,gamma = 1,max_depth = 5,min_child_weight = 6,subsample = 1)))\n",
                "cv = cross_val_score(xgb,x_train,x_test,cv=10)\n",
                "Accuracy8 = cv.mean()\n",
                "Accuracy.append(Accuracy8)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "xgb.fit(x_train,x_test)\n",
                "gbc.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "model18pred = xgb.predict(y_train)\n",
                "submission18 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission18['PassengerId'] = result['PassengerId']\n",
                "submission18['Survived'] = model18pred\n",
                "submission18.to_csv('XGBoosting(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x_train1 = final[:891]\n",
                "feature_scaler = StandardScaler()\n",
                "x_train1 = feature_scaler.fit_transform(x_train1)\n",
                "y_train1 = final[891:]\n",
                "feature_scaler = StandardScaler()\n",
                "y_train1 = feature_scaler.fit_transform(y_train1)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data",
                "train_model"
            ],
            "source": [
                "Krange1 = range(1,20)\n",
                "scores1 = {}\n",
                "scores_list1 = []\n",
                "for k in Krange1:\n",
                "    knn = KNeighborsClassifier(n_neighbors = k)\n",
                "    knn.fit(x_train1,x_test)\n",
                "    y_pred = knn.predict(y_train1)\n",
                "    scores1[k] = metrics.accuracy_score(result['Survived'],y_pred)\n",
                "    scores_list1.append(metrics.accuracy_score(result['Survived'],y_pred))\n",
                "    \n",
                "plt.plot(Krange,scores_list)\n",
                "plt.xlabel(\"Value of K\")\n",
                "plt.ylabel(\"Accuracy\")"
            ]
        },
        {
            "tags": [
                "check_results",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "knn = KNeighborsClassifier(n_neighbors = 11)\n",
                "Estimator.append(('knn',KNeighborsClassifier(n_neighbors = 13)))\n",
                "cv = cross_val_score(knn,x_train1,x_test,cv=10)\n",
                "Accuracy9 = cv.mean()\n",
                "Accuracy.append(Accuracy9)\n",
                "print(cv)\n",
                "print(cv.mean())"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "knn.fit(x_train1,x_test)\n",
                "knn.score(y_train1,y_test)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "evaluate_model"
            ],
            "source": [
                "model19pred = knn.predict(y_train)\n",
                "submission19 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "submission19['PassengerId'] = result['PassengerId']\n",
                "submission19['Survived'] = model19pred\n",
                "submission19.to_csv('KNN(StdScaler).csv',index = False)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "r = output2.groupby(['Models after Hyperparameter Tuning'])['Accuracy after HT'].mean().reset_index().sort_values(by='Accuracy after HT',ascending=False)\n",
                "r.head(10).style.background_gradient(cmap='Reds')\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "vot_soft1 = VotingClassifier(estimators = Estimator, voting ='soft') \n",
                "vot_soft1.fit(x_train, x_test) \n",
                "y_pred = vot_soft1.predict(y_train)\n",
                "vot_soft1.score(y_train,y_test)\n",
                "\n",
                "modelpred3 = vot_soft1.predict(y_train)\n",
                "sub3 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "sub3['PassengerId'] = result['PassengerId']\n",
                "sub3['Survived'] = modelpred3\n",
                "sub3.to_csv('SoftVoting(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "vot_soft1.fit(x_train, x_test) \n",
                "vot_soft1.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "source": [
                "vot_hard1 = VotingClassifier(estimators = Estimator, voting ='hard') \n",
                "vot_hard1.fit(x_train, x_test) \n",
                "y_pred = vot_hard1.predict(y_train)\n",
                "vot_hard1.score(y_train,y_test)\n",
                "\n",
                "modelpred4 = vot_hard1.predict(y_train)\n",
                "sub4 = pd.DataFrame(columns = ['PassengerId','Survived'])\n",
                "sub4['PassengerId'] = result['PassengerId']\n",
                "sub4['Survived'] = modelpred4\n",
                "sub4.to_csv('HardVoting(HT).csv',index = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "vot_hard1.fit(x_train, x_test) \n",
                "vot_hard1.score(y_train,y_test)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "output = pd.concat([output1,output2],axis = 1)\n",
                "output.sort_values(by=['Accuracy after HT'], inplace=True, ascending=False)\n",
                "output.head(10)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "cbd = pd.read_csv(\"../input/crimeanalysis/crime_by_district.csv\")\n",
                "cbdr = pd.read_csv(\"../input/crimeanalysis/crime_by_district_rt.csv\")\n",
                "cbs = pd.read_csv(\"../input/crimeanalysis/crime_by_state.csv\")\n",
                "cbsr = pd.read_csv(\"../input/crimeanalysis/crime_by_state_rt.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cbd.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cbdr.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cbd.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cbdr.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "total1 = pd.concat([cbd,cbdr]).drop_duplicates(keep = False)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "total1['Total Atrocities'] = total1['Murder'] +total1['Assault on women']+total1['Kidnapping and Abduction']+total1['Dacoity']+total1['Robbery']+total1['Arson']+total1['Hurt']+total1['Prevention of atrocities (POA) Act']+total1['Protection of Civil Rights (PCR) Act']+total1['Other Crimes Against SCs']\n",
                "total1.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cbs.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cbsr.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "total = pd.concat([cbs,cbsr]).drop_duplicates(keep = False)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "total.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "total.head(20)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "total.tail(20)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "total.drop(total[total['STATE/UT'] == 'TOTAL (UTs)'].index , inplace = True) \n",
                "total.drop(total[total['STATE/UT'] == 'TOTAL (STATES)'].index , inplace = True) "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cbdr.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "cbsr.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "total.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "cbdr['Total Atrocities'] = cbdr['Murder'] +cbdr['Assault on women']+cbdr['Kidnapping and Abduction']+cbdr['Dacoity']+cbdr['Robbery']+cbdr['Arson']+cbdr['Hurt']+cbdr['Prevention of atrocities (POA) Act']+cbdr['Protection of Civil Rights (PCR) Act']+cbdr['Other Crimes Against SCs']\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "source": [
                "s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Murder'].sum().reset_index().sort_values(by='Murder',ascending=False)\n",
                "s.head(10).style.background_gradient(cmap='Reds')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Assault on women'].sum().reset_index().sort_values(by='Assault on women',ascending=False)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Robbery'].sum().reset_index().sort_values(by='Robbery',ascending=False)\n",
                "s.head(10).style.background_gradient(cmap='Oranges')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Arson'].sum().reset_index().sort_values(by='Arson',ascending=False)\n",
                "s.head(10).style.background_gradient(cmap='RdPu')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "s= cbdr.groupby(['STATE/UT','DISTRICT','Year'])['Hurt'].sum().reset_index().sort_values(by='Hurt',ascending=False)\n",
                "s.head(10).style.background_gradient(cmap='Greys')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Murder', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Assault on women', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Kidnapping and Abduction', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Dacoity', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Robbery', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Arson', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Hurt', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Prevention of atrocities (POA) Act', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Protection of Civil Rights (PCR) Act', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Other Crimes Against SCs', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Total Atrocities', data=cbdr,height = 5, aspect = 4)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "cbsr['Total Atrocities'] = cbsr['Murder'] +cbsr['Assault on women']+cbsr['Kidnapping and Abduction']+cbsr['Dacoity']+cbsr['Robbery']+cbsr['Arson']+cbsr['Hurt']+cbsr['Prevention of atrocities (POA) Act']+cbsr['Protection of Civil Rights (PCR) Act']+cbsr['Other Crimes Against SCs']\n",
                "cbsr.head()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.relplot(x ='Total Atrocities', y ='Year', col = 'STATE/UT', data = cbsr, height=3 ,col_wrap = 9)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.axes_style('white')\n",
                "sns.jointplot(x=x, y=y, kind = 'hex', color = 'green')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "f, ax = plt.subplots(figsize=(6,6))\n",
                "cmap = sns.cubehelix_palette(as_cmap = True, dark=0,light = 1,reverse=True)\n",
                "sns.kdeplot(x,y,cmap=cmap, n_levels = 60, shade= True)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "total['Total Atrocities'] = total['Murder'] +total['Assault on women']+total['Kidnapping and Abduction']+total['Dacoity']+total['Robbery']+total['Arson']+total['Hurt']+total['Prevention of atrocities (POA) Act']+total['Protection of Civil Rights (PCR) Act']+total['Other Crimes Against SCs']\n",
                "total.head(15)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "s= total.groupby(['Year'])['Murder'].sum().reset_index().sort_values(by='Murder',ascending=False)\n",
                "s.head(15).style.background_gradient(cmap='Reds')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Murder', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Assault on women', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Kidnapping and Abduction', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Dacoity', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Robbery', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Arson', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Hurt', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Prevention of atrocities (POA) Act', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Protection of Civil Rights (PCR) Act', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Other Crimes Against SCs', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y='Total Atrocities', data=total ,height = 5, aspect = 4,kind = 'bar')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "data = pd.read_csv(\"../input/vehicle-dataset-from-cardekho/car data.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data.describe"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.relplot( y = 'Selling_Price', x = 'Kms_Driven',  data = data)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Owner', y ='Selling_Price', kind = 'violin',data= data)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Transmission', y ='Selling_Price', kind = 'swarm',data= data)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Seller_Type', y ='Selling_Price', kind = 'swarm',data= data,hue = 'Fuel_Type')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.catplot(x='Year', y ='Selling_Price', kind = 'swarm',data= data)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "correlation = data.corr()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.subplots(figsize=(10,15))\n",
                "sns.heatmap(correlation, annot = True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.jointplot(x = 'Present_Price', y ='Selling_Price', data=data, color = 'Green')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.pairplot(data = data)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "dummy1 = pd.get_dummies(data.Fuel_Type)\n",
                "dummy2 = pd.get_dummies(data.Seller_Type)\n",
                "dummy3 = pd.get_dummies(data.Transmission)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "merge = pd.concat([data,dummy1,dummy2,dummy3], axis = 'columns')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "final = merge.drop(['Car_Name','Fuel_Type','Seller_Type','Transmission','CNG','Individual','Automatic','Owner','Kms_Driven'], axis = 'columns')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "final\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X = final.drop(['Selling_Price'],axis = 'columns')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "y = final['Selling_Price']"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,random_state = 20)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "from sklearn.linear_model import LinearRegression\n",
                "model = LinearRegression()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model.fit(X_train,y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "model.score(X_test,y_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "df = pd.read_csv(\"../input/graduate-admissions/Admission_Predict_Ver1.1.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "df.drop(['Serial No.'], axis = 1, inplace = True)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(25,10))\n",
                "sns.heatmap(df.corr(), annot=True, linewidth=0.5, cmap='coolwarm')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.pairplot(df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "x = df['Chance of Admit ']\n",
                "sns.distplot(x , kde= True,rug = False, bins = 30)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "x = df['GRE Score']\n",
                "sns.distplot(x , kde= True,rug = False, bins = 30)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "x = df['TOEFL Score']\n",
                "sns.distplot(x , kde= True,rug = False, bins = 30)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "x = df['CGPA']\n",
                "sns.distplot(x , kde= True,rug = False, bins = 30)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.lineplot(x = 'GRE Score', y = 'CGPA', data = df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.lineplot(x = 'TOEFL Score', y = 'CGPA', data = df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.jointplot(x = 'GRE Score', y = 'CGPA', data=df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.jointplot(x = 'TOEFL Score', y = 'CGPA', data=df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.jointplot(x = 'TOEFL Score', y = 'University Rating', data=df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.jointplot(x = 'GRE Score', y = 'University Rating', data=df)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.relplot(x ='SOP', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.relplot(x ='LOR ', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.relplot(x ='Research', y ='Chance of Admit ', col = 'University Rating', data = df, estimator = None,palette = 'ch:r = -0.8, l = 0.95')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn import metrics\n",
                "from sklearn import linear_model\n",
                "from sklearn.linear_model import Lasso\n",
                "from sklearn.linear_model import ElasticNet\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.svm import SVR\n",
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "\n",
                "from sklearn_pandas import DataFrameMapper\n",
                "from numpy import asarray\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.preprocessing import StandardScaler"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "X = df.drop(['Chance of Admit '], axis = 1)\n",
                "X"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "y = df['Chance of Admit ']\n",
                "y"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20,shuffle = False)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model1 = LinearRegression()\n",
                "model1.fit(X_train, y_train)\n",
                "\n",
                "accuracy1 = model1.score(X_test,y_test)\n",
                "print(accuracy1*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred1 = model1.predict(X_test)\n",
                "\n",
                "val = mean_squared_error(y_test, y_pred1, squared=False)\n",
                "val1 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model2 = DecisionTreeRegressor()\n",
                "model2.fit(X_train, y_train)\n",
                "\n",
                "accuracy2 = model2.score(X_test,y_test)\n",
                "print(accuracy2*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred2 = model2.predict(X_test)\n",
                "\n",
                "val = mean_squared_error(y_test, y_pred2, squared=False)\n",
                "val2 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "n_estimators = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]\n",
                "\n",
                "RF = RandomForestRegressor()\n",
                "\n",
                "parameters = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}\n",
                "\n",
                "RFR = GridSearchCV(RF, parameters,scoring='neg_mean_squared_error', cv=5)\n",
                "\n",
                "RFR.fit(X_train, y_train)\n",
                "\n",
                "RFR.best_params_\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "model3 = RandomForestRegressor(n_estimators = 190)\n",
                "model3.fit(X_train, y_train)\n",
                "\n",
                "accuracy3 = model3.score(X_test,y_test)\n",
                "print(accuracy3*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred3 = model3.predict(X_test)\n",
                "\n",
                "val = mean_squared_error(y_test, y_pred3, squared=False)\n",
                "val3 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred3)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "lasso = Lasso()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "lasso_regressor.fit(X_train, y_train)\n",
                "\n",
                "lasso_regressor.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model4 = linear_model.Lasso(alpha=.001)\n",
                "model4.fit(X_train,y_train)\n",
                "\n",
                "accuracy4 = model4.score(X_test,y_test)\n",
                "print(accuracy4*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred4 = model4.predict(X_test)\n",
                "\n",
                "val= mean_squared_error(y_test, y_pred4, squared=False)\n",
                "val4 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred4)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "alpha = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
                "\n",
                "ridge = Ridge()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "ridge_regressor = GridSearchCV(ridge, parameters,scoring='neg_mean_squared_error', cv=100)\n",
                "\n",
                "ridge_regressor.fit(X_train, y_train)\n",
                "ridge_regressor.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model5 = linear_model.Ridge(alpha=1)\n",
                "model5.fit(X_train,y_train)\n",
                "\n",
                "accuracy5 = model5.score(X_test,y_test)\n",
                "print(accuracy5*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred5 = model5.predict(X_test)\n",
                "\n",
                "val = mean_squared_error(y_test, y_pred5, squared=False)\n",
                "val5 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred5)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "Elasticnet = ElasticNet()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "en_regressor = GridSearchCV(Elasticnet, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "en_regressor.fit(X_train, y_train)\n",
                "en_regressor.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model6 = linear_model.ElasticNet(alpha=0.001)\n",
                "model6.fit(X_train,y_train)\n",
                "\n",
                "accuracy6 = model6.score(X_test,y_test)\n",
                "print(accuracy6*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred6 = model6.predict(X_test)\n",
                "\n",
                "val = mean_squared_error(y_test, y_pred6, squared=False)\n",
                "val6 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred6)))\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "x = data.drop(['Chance of Admit '], axis = 1)\n",
                "x"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "Y = data['Chance of Admit ']\n",
                "Y"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x_train, x_test, Y_train, Y_test = train_test_split(x, Y, test_size = 0.20,shuffle = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model7 = LinearRegression()\n",
                "model7.fit(x_train, Y_train)\n",
                "\n",
                "accuracy7 = model7.score(x_test,Y_test)\n",
                "print(accuracy7*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred7 = model7.predict(x_test)\n",
                "\n",
                "val = mean_squared_error(Y_test, y_pred7, squared=False)\n",
                "val7 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred7)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model8 = DecisionTreeRegressor()\n",
                "model8.fit(x_train, Y_train)\n",
                "\n",
                "accuracy8 = model8.score(x_test,Y_test)\n",
                "print(accuracy8*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred8 = model8.predict(x_test)\n",
                "\n",
                "val = mean_squared_error(Y_test, y_pred8, squared=False)\n",
                "val8 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred8)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "n_estimators = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]\n",
                "\n",
                "rf = RandomForestRegressor()\n",
                "\n",
                "parameters = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}\n",
                "\n",
                "rfr = GridSearchCV(rf, parameters,scoring='neg_mean_squared_error', cv=10)\n",
                "\n",
                "rfr.fit(x_train, Y_train)\n",
                "\n",
                "rfr.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model9 = RandomForestRegressor(n_estimators = 220)\n",
                "model9.fit(x_train, Y_train)\n",
                "\n",
                "accuracy9 = model9.score(x_test,Y_test)\n",
                "print(accuracy9*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred9 = model9.predict(x_test)\n",
                "\n",
                "val = mean_squared_error(Y_test, y_pred9, squared=False)\n",
                "val9 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred9)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "L = Lasso()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "LR = GridSearchCV(L, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "LR.fit(x_train, Y_train)\n",
                "LR.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model10 = linear_model.Lasso(alpha=.01)\n",
                "model10.fit(x_train,Y_train)\n",
                "\n",
                "accuracy10 = model10.score(x_test,Y_test)\n",
                "print(accuracy10*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred10 = model10.predict(x_test)\n",
                "\n",
                "val = mean_squared_error(Y_test, y_pred10, squared=False)\n",
                "val10 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred10)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "EN = ElasticNet()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "ENR = GridSearchCV(Elasticnet, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "ENR.fit(x_train, Y_train)\n",
                "ENR.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model11 = linear_model.Lasso(alpha=.01)\n",
                "model11.fit(x_train,Y_train)\n",
                "\n",
                "accuracy11 = model11.score(x_test,Y_test)\n",
                "print(accuracy11*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred11 = model11.predict(x_test)\n",
                "\n",
                "val = mean_squared_error(Y_test, y_pred11, squared=False)\n",
                "val11 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred11)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "SVR = SVR()\n",
                "\n",
                "parameters = {'C':[.0001 ,.001 ,0.1, 1, 10, 100, 1000],\n",
                "              'epsilon':[0.001, 0.01, 0.1, 0.5, 1, 2, 4]\n",
                "             }\n",
                "\n",
                "ENR = GridSearchCV(SVR, parameters, scoring='neg_mean_squared_error', cv = 10)\n",
                "\n",
                "ENR.fit(x_train, Y_train)\n",
                "ENR.best_params_"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "from sklearn.svm import SVR\n",
                "model12 = SVR(C=1, epsilon=0.1)\n",
                "model12.fit(x_train,Y_train)\n",
                "\n",
                "model12 = model12.score(x_test,Y_test)\n",
                "print(model12*100,'%')"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "R = Ridge()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "R = GridSearchCV(R, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "R.fit(x_train, Y_train)\n",
                "R.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model13 = linear_model.Ridge(alpha=10)\n",
                "model13.fit(x_train,Y_train)\n",
                "\n",
                "accuracy13 = model13.score(x_test,Y_test)\n",
                "print(accuracy13*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred13 = model13.predict(x_test)\n",
                "\n",
                "val = mean_squared_error(Y_test, y_pred13, squared=False)\n",
                "val13 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred13)))\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "source": [
                "from pandas import DataFrame\n",
                "trans = MinMaxScaler()\n",
                "dat = trans.fit_transform(df)\n",
                "dataset = DataFrame(dat)\n",
                "\n",
                "df.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "dataset.columns = ['GRE Score','TOEFL Score','University Rating','SOP','LOR','CGPA','Research','Chance of Admit']\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "ex = dataset.drop(['Chance of Admit'], axis = 1)\n",
                "ex"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "ey = dataset['Chance of Admit']\n",
                "ey"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x_t, x_es, Y_t, Y_es = train_test_split(ex, ey, test_size = 0.20,shuffle = False)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model14 = LinearRegression()\n",
                "model14.fit(x_t, Y_t)\n",
                "\n",
                "accuracy14 = model14.score(x_es,Y_es)\n",
                "print(accuracy14*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred14 = model14.predict(x_es)\n",
                "\n",
                "val = mean_squared_error(Y_es, y_pred14, squared=False)\n",
                "val14 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred14)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "l = Lasso()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "lr = GridSearchCV(l, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "lr.fit(x_t, Y_t)\n",
                "lr.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model15 = linear_model.Lasso(alpha=.001)\n",
                "model15.fit(x_t,Y_t)\n",
                "\n",
                "accuracy15 = model15.score(x_es,Y_es)\n",
                "print(accuracy15*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred15 = model15.predict(x_es)\n",
                "\n",
                "val = mean_squared_error(Y_es, y_pred15, squared=False)\n",
                "val15 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred15)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "r = Ridge()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "r = GridSearchCV(r, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "r.fit(x_t, Y_t)\n",
                "r.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "model16 = linear_model.Ridge(alpha=0.01)\n",
                "model16.fit(x_t,Y_t)\n",
                "\n",
                "accuracy16 = model16.score(x_es,Y_es)\n",
                "print(accuracy16*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred16 = model16.predict(x_es)\n",
                "\n",
                "val = mean_squared_error(Y_es, y_pred16, squared=False)\n",
                "val16 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred16)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "en = ElasticNet()\n",
                "\n",
                "parameters = {'alpha': [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]}\n",
                "\n",
                "enr = GridSearchCV(en, parameters, scoring='neg_mean_squared_error', cv = 100)\n",
                "\n",
                "enr.fit(x_t, Y_t)\n",
                "enr.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results",
                "evaluate_model"
            ],
            "source": [
                "model17 = linear_model.Lasso(alpha=.001)\n",
                "model17.fit(x_t,Y_t)\n",
                "\n",
                "accuracy17 = model17.score(x_es,Y_es)\n",
                "print(accuracy17*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred17 = model17.predict(x_es)\n",
                "\n",
                "val = mean_squared_error(Y_es, y_pred17, squared=False)\n",
                "val17 = str(round(val, 4))\n",
                "\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred17)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "model18 = DecisionTreeRegressor()\n",
                "model18.fit(x_t, Y_t)\n",
                "\n",
                "accuracy18 = model18.score(x_es,Y_es)\n",
                "print(accuracy18*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred18 = model18.predict(x_es)\n",
                "\n",
                "val = mean_squared_error(Y_es, y_pred8, squared=False)\n",
                "val18 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred18)))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "n_estimators = [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]\n",
                "\n",
                "Rf = RandomForestRegressor()\n",
                "\n",
                "parameters = {'n_estimators': [10, 40, 70, 100, 130, 160, 190, 220, 250, 270]}\n",
                "\n",
                "Rfr = GridSearchCV(Rf, parameters,scoring='neg_mean_squared_error', cv=10)\n",
                "\n",
                "Rfr.fit(x_t, Y_t)\n",
                "\n",
                "Rfr.best_params_"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "model19 = RandomForestRegressor(n_estimators = 100)\n",
                "model19.fit(x_t, Y_t)\n",
                "\n",
                "accuracy19 = model19.score(x_es,Y_es)\n",
                "print(accuracy19*100,'%')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "y_pred19 = model19.predict(x_es)\n",
                "\n",
                "val = mean_squared_error(Y_es, y_pred19, squared=False)\n",
                "val19 = str(round(val, 4))\n",
                "\n",
                "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_es, y_pred19)))\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "train_model"
            ],
            "source": [
                "from sklearn.svm import SVR\n",
                "\n",
                "Svr = SVR()\n",
                "\n",
                "parameters = {'C':[.0001 ,.001 ,0.1, 1, 10, 100, 1000],\n",
                "              'epsilon':[0.001, 0.01, 0.1, 0.5, 1, 2, 4]\n",
                "             }\n",
                "\n",
                "Enr = GridSearchCV(Svr, parameters, scoring='neg_mean_squared_error', cv = 10)\n",
                "\n",
                "Enr.fit(x_t, Y_t)\n",
                "Enr.best_params_"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "evaluate_model",
                "train_model"
            ],
            "source": [
                "from sklearn.svm import SVR\n",
                "model20 = SVR(C=1, epsilon=0.1)\n",
                "model20.fit(x_t,Y_t)\n",
                "\n",
                "model20 = model20.score(x_es,Y_es)\n",
                "print(model20*100,'%')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "frames = [Half1RMSE,Half2RMSE,Half3RMSE] \n",
                "FullRMSE = pd.concat(frames, axis = 1)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "FullRMSE"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import seaborn as sns\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "check_results"
            ],
            "source": [
                "train_data=pd.read_csv('/kaggle/input/ghouls-goblins-and-ghosts-boo/train.csv.zip')\n",
                "train_data.head()"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "test_data=pd.read_csv('/kaggle/input/ghouls-goblins-and-ghosts-boo/test.csv.zip')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_data.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_data.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "numerical = ['bone_length','rotting_flesh','hair_length','has_soul']\n",
                "categorical = ['color','type']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "corr = train_data.corr()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#heatmap gives a visual representation of correlation between different attributes of the data\n",
                "sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.pairplot(train_data,hue='type')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#Check if numerical attributes have normal distributions\n",
                "train_data[numerical].hist(bins=15, figsize=(15, 6), layout=(2, 4),rwidth=0.9,grid=False,color='purple');"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "#Visualize categorical variables\n",
                "sns.set()\n",
                "sns.countplot(train_data['color'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.set()\n",
                "sns.countplot(train_data['type'])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test_data.shape"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test_data.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "test_data.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train_data['color'].unique()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#Check if train and test data have the same categories\n",
                "test_data['color'].unique()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#one-hot-encoding categorrical attribute:color\n",
                "train_data=pd.concat([train_data,pd.get_dummies(train_data['color'])],axis=1)\n",
                "train_data.drop('color',axis=1,inplace=True)\n",
                "train_data.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "test_data=pd.concat([test_data,pd.get_dummies(test_data['color'])],axis=1)\n",
                "test_data.drop('color',axis=1,inplace=True)\n",
                "test_data.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "X=train_data.drop(['id','type'],axis=1)\n",
                "y=pd.get_dummies(train_data['type'])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "check_results"
            ],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
                "print(X_train.shape,y_train.shape)\n",
                "print(X_test.shape,y_test.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from keras.layers import Dense\n",
                "from keras.models import Sequential"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "model = Sequential()\n",
                "model.add(Dense(100,input_shape=(X.shape[1],)))\n",
                "model.add(Dense(100,activation='relu'))\n",
                "model.add(Dense(100,activation='relu'))\n",
                "model.add(Dense(3,activation='softmax'))\n",
                "model.summary()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "model.compile(optimizer='adam',\n",
                "              loss='categorical_crossentropy',\n",
                "              metrics=['accuracy'])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "train=model.fit(x=X_train,y=y_train,batch_size=10,epochs=10,verbose=2,validation_data=(X_test,y_test))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(5,5))\n",
                "plt.plot(train.history['accuracy'],'r',label='Training accuracy')\n",
                "plt.plot(train.history['val_accuracy'],'b',label='Validation accuracy')\n",
                "plt.legend()"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "source": [
                "pred=model.predict(test_data.drop('id',axis=1))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "pred_final=[np.argmax(i) for i in pred]"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "submission = pd.DataFrame({'id':test_data['id'], 'type':pred_final})\n",
                "submission.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "submission.to_csv('../working/submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn import preprocessing\n",
                "sample_submission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n",
                "test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train[:5]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train.describe()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#Save the 'Id' column\n",
                "train_ID = train['Id']\n",
                "test_ID = test['Id']\n",
                "\n",
                "#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
                "train.drop(\"Id\", axis = 1, inplace = True)\n",
                "test.drop(\"Id\", axis = 1, inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train = train[train.GrLivArea < 4500]\n",
                "train.reset_index(drop=True, inplace=True)\n",
                "outliers = [30, 88, 462, 631, 1322]\n",
                "train = train.drop(train.index[outliers])"
            ]
        },
        {
            "tags": [
                "check_results",
                "visualize_data"
            ],
            "source": [
                "sns.distplot(train['SalePrice'])\n",
                "print('Skewness: %f' % train['SalePrice'].skew())\n",
                "print('Kurtosis: %f' % train['SalePrice'].kurt())"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results",
                "visualize_data"
            ],
            "source": [
                "#the qq-plot before log-transfomation\n",
                "from scipy import stats\n",
                "from scipy.stats import norm\n",
                "fig = plt.figure(figsize=(15,5)) #这里要记得写等于\n",
                "plt.subplot(1,2,1) #如果需要一行多幅图 需要在这里先制定是哪一幅图\n",
                "sns.distplot(train[\"SalePrice\"], fit=norm)\n",
                "mu, sigma = norm.fit(train['SalePrice'])\n",
                "#plt.legend(['Norm dist. mu = %f, sigma = %f' %(mu,sigma)], loc='upper right')\n",
                "plt.legend(['Norm dist. mu={:.2f}, sigma={:.2f}'.format(mu,sigma)])\n",
                "plt.subplot(1,2,2)\n",
                "stats.probplot(train['SalePrice'],plot=plt)\n",
                "plt.title('Before transfomation')\n",
                "print('mu = {:.2f},\\nsigma = {:.2f}'.format(mu,sigma))\n",
                "\n",
                "#Do the transformation\n",
                "train.SalePrice = np.log1p(train.SalePrice)\n",
                "y_train = train.SalePrice.values\n",
                "y_train_orig = train.SalePrice\n",
                "#the reason why we do this is because the models like linear regression and SVM need the data to be norm distribution.\n",
                "\n",
                "#after the transformation\n",
                "fig = plt.figure(figsize=(15,5))\n",
                "plt.subplot(1,2,1)\n",
                "sns.distplot(train.SalePrice, fit=norm)\n",
                "mu, sigma = norm.fit(train.SalePrice)\n",
                "plt.legend(['Norm distribution.(mu={:.2f}, sigma={:.2f})'.format(mu, sigma)])\n",
                "plt.subplot(1,2,2)\n",
                "plt.ylabel('Frequency')\n",
                "stats.probplot(train.SalePrice, plot=plt)\n",
                "plt.title('After transformation')\n",
                "print('\\n mu={:.2f}, sigma={:.2f}'.format(mu,sigma))"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "train_X = train.drop('SalePrice',axis=1)\n",
                "data_features = pd.concat((train_X, test)).reset_index(drop=True)\n",
                "print(data_features.shape)\n",
                "data_features.columns\n",
                "#We concatenate the train set and the test set since we need to handle the data both on the train set and the test set."
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features[['MSSubClass', 'MSZoning']]"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_features_na = data_features.isnull().sum().sort_values(ascending=False)\n",
                "data_features_na = data_features_na[data_features_na>0]\n",
                "data_features_na"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "percent = (data_features.isnull().sum()/data_features.isnull().count()).sort_values(ascending=False)\n",
                "percent = percent[percent>0]\n",
                "pd.concat([data_features_na, percent],axis=1,keys=['total', 'percent'])"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "#Now the check the missing values\n",
                "missing_data = data_features.isnull().sum().sort_values(ascending = False)\n",
                "missing_data = missing_data[missing_data > 0]\n",
                "missing_data"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#Some number features stand for categories.\n",
                "str_var = [\"MSSubClass\",'MoSold','YrSold']\n",
                "for var in str_var:\n",
                "    data_features[var] = data_features[var].apply(str)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "#For variables about years,we use labelencoder method to transform. \n",
                "#You can also use one-hot encode but I don't like the dimensional disaster...\n",
                "le = preprocessing.LabelEncoder()\n",
                "#ohe = preprocessing.OneHotEncoder()\n",
                "Year_var = ['YearBuilt','YearRemodAdd','GarageYrBlt']\n",
                "for var in Year_var:\n",
                "    data_features[var] = le.fit_transform(data_features[var])\n",
                "data_features[Year_var]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_features['MasVnrScore'] = data_features['MasVnrType'] * data_features['MasVnrArea']\n",
                "data_features['BsmtScore2'] = data_features['BsmtQual'] * data_features['BsmtCond'] * data_features['BsmtExposure']\n",
                "data_features['TotalSF'] = data_features['TotalBsmtSF'] + data_features['1stFlrSF'] + data_features['2ndFlrSF']\n",
                "data_features[\"AllSF\"] = data_features[\"GrLivArea\"] + data_features[\"TotalBsmtSF\"]\n",
                "data_features['TotalBath'] = data_features['FullBath'] + 0.5*data_features['HalfBath'] + data_features['BsmtFullBath'] + 0.5*data_features['BsmtHalfBath']\n",
                "data_features['Total_porch_sf'] = data_features['OpenPorchSF'] + data_features['3SsnPorch'] + data_features['EnclosedPorch'] + data_features['ScreenPorch'] + data_features['WoodDeckSF']"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "train1 = data_features[:len(y_train)]\n",
                "train1.loc[:,'SalePrice'] = y_train\n",
                "corr1 = train1.corr()['SalePrice'].sort_values(ascending = False)\n",
                "corr15 = corr1[:15]\n",
                "corr15"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#Polynomial on the top 10 features.For the features have connection themself, only use one to do polynomail.\n",
                "corr10 = corr15.drop(['SalePrice','AllSF','1stFlrSF','GarageFinish','GarageArea'])\n",
                "corr10_var = corr10.index.tolist()\n",
                "for col in corr10_var:\n",
                "    data_features[col + '-2'] = data_features[col] **2\n",
                "    data_features[col + '-3'] = data_features[col] **3\n",
                "    data_features[col + '-sqrt'] = np.sqrt(data_features[col])\n",
                "\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "cat_features = data_features.select_dtypes(include = ['object']).columns\n",
                "num_features = data_features.select_dtypes(exclude = ['object']).columns\n",
                "print(cat_features)\n",
                "print('Categorial features :' + str(len(cat_features)) + '\\n')\n",
                "\n",
                "print(num_features)\n",
                "print('Numerical features :' + str(len(num_features)))"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "data_num = data_features[num_features]\n",
                "data_cat = data_features[cat_features]\n",
                "data_num.head()\n",
                "data_cat.head()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "skew_features = data_num.skew().sort_values(ascending = False)\n",
                "skew_features"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from scipy.special import boxcox1p\n",
                "from scipy.stats import boxcox_normmax\n",
                "from sklearn.base import BaseEstimator, TransformerMixin"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "skew_features = skew_features[abs(skew_features) > 0.5]\n",
                "print('The mean skewness of the variables is{}'.format(np.mean(data_num.skew())))\n",
                "print('There are {} features have to boxcox1p transform'.format(len(skew_features)))\n",
                "skew_features"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "len(data_features)"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "final_features = pd.get_dummies(data_features)\n",
                "X_train = final_features.iloc[:len(y_train),:]\n",
                "X_test = final_features.iloc[len(y_train):,:]\n",
                "print('The shape of train set is{},y set is{},and the shape of test set is{}'.format(X_train.shape,y_train.shape,X_test.shape))"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "type(X)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "X.isnull().sum().sort_values(ascending = False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from datetime import datetime\n",
                "from sklearn.preprocessing import RobustScaler\n",
                "from sklearn.model_selection import KFold, cross_val_score\n",
                "from sklearn.metrics import mean_squared_error , make_scorer\n",
                "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
                "from sklearn.pipeline import make_pipeline\n",
                "from sklearn.linear_model import LinearRegression\n",
                "\n",
                "from sklearn.ensemble import GradientBoostingRegressor\n",
                "from sklearn.svm import SVR\n",
                "from mlxtend.regressor import StackingCVRegressor\n",
                "from sklearn.linear_model import LinearRegression\n",
                "\n",
                "from xgboost import XGBRegressor\n",
                "from lightgbm import LGBMRegressor"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "models = {'XGBoost':XGBRegressor(n_estimators=1000,\n",
                "                                max_depth=6,\n",
                "                                objective ='reg:squarederror')}"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "models.keys()"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "source": [
                "for name in models.keys():\n",
                "    model = models[name]\n",
                "    model.fit(X_train, y_train)\n",
                "    scores = cv_rmse(model,X_train, y_train)\n",
                "    print(scores.mean)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "scores.mean()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "source": [
                "sample_submission.to_csv(\"submission.csv\", index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "check_results"
            ],
            "source": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "source": [
                "import pandas as pd\n",
                "sample_submission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n",
                "test = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "train = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "train[:10]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train_id = train['Id']\n",
                "test_id = test['Id']\n",
                "train.drop('Id',inplace = True, axis = 1)\n",
                "test.drop('Id',inplace = True, axis = 1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "train = train[train.GrLivArea < 4500]\n",
                "train.reset_index(drop = True, inplace = True)\n",
                "outliars = [30, 88, 462, 631, 1322]\n",
                "train.drop(train.index[outliars], inplace = True)\n",
                "#If you want to know why remove these values u can read kernels about EDA"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "#we don't need SalePrice so drop it\n",
                "train.drop('SalePrice', inplace=True, axis=1)\n",
                "data_features = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],test.loc[:,'MSSubClass':'SaleCondition'])).reset_index(drop=True)"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#first get the total missing values\n",
                "total = data_features.isnull().sum().sort_values(ascending=False)\n",
                "total = total[total>0]\n",
                "percent = total/len(data_features)\n",
                "percent\n",
                "missing_data = pd.concat((total,percent),axis=1,keys=['total','percent'])\n",
                "missing_data"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features.loc[(data_features['PoolQC'].notnull()),['PoolArea','PoolQC']]"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "#The code below just for a simple look\n",
                "a = data_features.loc[data_features['MiscFeature'].notnull(),'MiscFeature']\n",
                "data_features.loc[data_features['MiscFeature'].notnull(),'MiscFeature'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "a = data_features.loc[data_features['Alley'].notnull(),'Alley']\n",
                "data_features.loc[data_features['Alley'].notnull(),'Alley'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "a = data_features.loc[data_features['Fence'].notnull(),'Fence']\n",
                "data_features.loc[data_features['Fence'].notnull(),'Fence'].value_counts()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['FireplaceQu'].groupby([data_features['Fireplaces'],data_features['FireplaceQu']]).count()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features.groupby('Neighborhood')['LotFrontage'].mean()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "plt.figure(figsize=(16,9))\n",
                "plt.plot(data_features['LotArea'], data_features['LotFrontage'])"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['GarageCond'],data_features['GarageQual'])).count()\n",
                "#These two variables have positive correlation.So we can use the mode to fill the missing in GarageCond and GarageQual. "
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageQual'].notnull()), ['GarageQual']]\n",
                "#it seems like they have the same missing values.\n",
                "#the same for the two below\n",
                "#so fill them all with NONE"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageYrBlt'].notnull()), ['GarageYrBlt']]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features.loc[(data_features['GarageCond'].isnull() & data_features['GarageFinish'].notnull()), ['GarageFinish']]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_features.loc[(data_features['GarageYrBlt'].isnull() & data_features['GarageType'].notnull())\n",
                "                  ,['GarageYrBlt','GarageType','GarageCond','GarageFinish','GarageQual']]\n",
                "#so we use the value of GarageType to fill the other four variables\n",
                "garage_var = ['GarageYrBlt','GarageType','GarageCond','GarageFinish','GarageQual']\n",
                "condition1 = (data_features['GarageYrBlt'].isnull() & data_features['GarageType'].notnull())\n",
                "for col in garage_var:\n",
                "    data_features.loc[condition1,col] = data_features[(data_features['GarageType'] == 'Detchd')][col].mode()[0]\n",
                "#Note that we still have 156 missing values to fill for all 5 variables"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "#handle them in the same way as garage\n",
                "missing_data[11:]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features[(data_features['BsmtFinType1'].isnull() & data_features['BsmtCond'].notnull())]\n",
                "#We detect that all bsmt variables have 79 common missing values. Obviously these data mean there are no basements in these houses.\n",
                "#So we use the NONE to fill them like the garage variables."
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtExposure'].isnull())][\n",
                "    ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['BsmtQual'],data_features['BsmtExposure'])).count()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['BsmtCond'],data_features['BsmtExposure'])).count()\n",
                "#When Con is TA and Qual is Gd we shuold choose No to fill the missing value in 'BsmtExposure'"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "condition2 = (data_features['BsmtFinType1'].notnull() & data_features['BsmtExposure'].isnull())\n",
                "data_features.loc[condition2,'BsmtExposure'] = 'No'"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtCond'].isnull())][\n",
                "    ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]\n"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['BsmtExposure'],data_features['BsmtCond'])).count()\n",
                "#The proportion of TA is larger than other values so use TA to fill 'BsmtCond'"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "condition3 = (data_features['BsmtFinType1'].notnull() & data_features['BsmtCond'].isnull())\n",
                "data_features.loc[condition3,'BsmtCond'] = 'TA'"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features[(data_features['BsmtFinType1'].notnull() & data_features['BsmtQual'].isnull())][\n",
                "    ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['BsmtExposure'],data_features['BsmtQual'])).count()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['BsmtCond'],data_features['BsmtQual'])).count()"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "#The last one \n",
                "condition5 = (data_features['BsmtFinType2'].isnull() & data_features['BsmtFinType1'].notnull())\n",
                "data_features[condition5][['BsmtFinType1','BsmtFinType2']]\n",
                "#From the data description "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_features['MSSubClass'].groupby((data_features['BsmtFinType1'],data_features['BsmtFinType2'])).count()\n",
                "#I guess even if Type1 is good , the Type 2 is more likely to be Unf. So fill 'BsmtFinType2' by Unf\n",
                "data_features.loc[condition5, 'BsmtFinType2'] = 'Unf'"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "bsmt_var = ['BsmtCond','BsmtExposure','BsmtQual','BsmtFinType1','BsmtFinType2']\n",
                "garage_var = ['GarageType','GarageCond','GarageFinish','GarageQual']\n",
                "NONE_var = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu']\n",
                "missing_data = data_features.isnull().sum().sort_values(ascending = False)\n",
                "missing_data = missing_data[missing_data > 0]\n",
                "missing_data"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "for col in bsmt_var, garage_var,NONE_var:\n",
                "    data_features[col] = data_features[col].fillna('NONE')\n",
                "data_features['GarageYrBlt'] = data_features['GarageYrBlt'].fillna(0)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['LotFrontage']"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "missing_data = data_features.isnull().sum().sort_values(ascending = False)\n",
                "missing_data = missing_data[missing_data > 0]\n",
                "missing_data"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "condition6 = (data_features['MasVnrType'].isnull() & data_features['MasVnrArea'].notnull())\n",
                "data_features.loc[condition6,['MasVnrType','MasVnrArea']]"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MasVnrArea'].groupby(data_features['MasVnrType']).describe()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features['MasVnrArea'].groupby(data_features['MasVnrType']).median()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "sns.boxplot(data_features['MasVnrType'],data_features['MasVnrArea'])\n",
                "#Maybe fill it with 'Stone' is more reasonable.\n",
                "#Something strange here. None means no masonry but there still several values. May theuy are outliars that I should remove?"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_features.loc[condition6,'MasVnrType'] = 'Stone'\n",
                "data_features['MasVnrType'] = data_features['MasVnrType'].fillna('None')\n",
                "data_features['MasVnrArea'] = data_features['MasVnrArea'].fillna(0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_features['MSZoning'].groupby([data_features['MSSubClass'],data_features['MSZoning']]).count()\n",
                "data_features['MSZoning'] = data_features['MSZoning'].groupby(data_features['MSSubClass']).transform(lambda x:x.fillna(x.mode()[0]))"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "missing_data = data_features.isnull().sum().sort_values(ascending = False)\n",
                "missing_data = missing_data[missing_data > 0]\n",
                "missing_data"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "NA_for_0 = ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath',\n",
                "            'GarageArea', 'GarageCars','MasVnrArea']\n",
                "for col in NA_for_0:\n",
                "    data_features[col] = data_features[col].fillna(0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "common_for_NA = ['Exterior1st','Exterior2nd','SaleType','Electrical','KitchenQual']\n",
                "for col in common_for_NA:\n",
                "    data_features[col].fillna(data_features[col].mode()[0], inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "data_features['Functional'] = data_features['Functional'].fillna('Typ')\n",
                "data_features['Utilities'] = data_features['Utilities'].fillna('None')"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "missing_data = data_features.isnull().sum().sort_values(ascending = False)\n",
                "missing_data = missing_data[missing_data > 0]\n",
                "missing_data"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "data_features[(data_features['BsmtCond'].isnull() & data_features['BsmtFinType1'].notnull())][['BsmtQual','BsmtFinType1','BsmtFinType2']]"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "condition1 = (data_features['BsmtCond'].isnull() & data_features['BsmtExposure'].notnull())\n",
                "data_features[condition1]"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import os\n",
                "import re\n",
                "from glob import glob\n",
                "from tqdm import tqdm\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import ast\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "process_data",
                "ingest_data",
                "check_results"
            ],
            "source": [
                "fnames = glob('../input/train_simplified/*.csv') #<class 'list'>\n",
                "cnames = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n",
                "drawlist = []\n",
                "for f in fnames[0:6]: # num of word : 5\n",
                "    first = pd.read_csv(f, nrows=10) # make sure we get a recognized drawing\n",
                "    first = first[first.recognized==True].head(2) # top head 2 get \n",
                "    drawlist.append(first)\n",
                "draw_df = pd.DataFrame(np.concatenate(drawlist), columns=cnames) # <class 'pandas.core.frame.DataFrame'>\n",
                "draw_df"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "draw_df.drawing.values[0]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "evens = range(0,11,2)\n",
                "odds = range(1,12, 2)\n",
                "# We have drawing images, 2 per label, consecutively\n",
                "df1 = draw_df[draw_df.index.isin(evens)]\n",
                "df2 = draw_df[draw_df.index.isin(odds)]\n",
                "\n",
                "example1s = [ast.literal_eval(pts) for pts in df1.drawing.values]\n",
                "example2s = [ast.literal_eval(pts) for pts in df2.drawing.values]\n",
                "labels = df2.word.tolist()\n",
                "\n",
                "for i, example in enumerate(example1s):\n",
                "    plt.figure(figsize=(6,3))\n",
                "    \n",
                "    for x,y in example:\n",
                "        plt.subplot(1,2,1)\n",
                "        plt.plot(x, y, marker='.')\n",
                "        plt.axis('off')\n",
                "\n",
                "    for x,y, in example2s[i]:\n",
                "        plt.subplot(1,2,2)\n",
                "        plt.plot(x, y, marker='.')\n",
                "        plt.axis('off')\n",
                "        label = labels[i]\n",
                "        plt.title(label, fontsize=10)\n",
                "\n",
                "    plt.show()  "
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "\n",
                "import os\n",
                "from glob import glob\n",
                "import re\n",
                "import ast\n",
                "import numpy as np \n",
                "import pandas as pd\n",
                "from PIL import Image, ImageDraw \n",
                "from tqdm import tqdm\n",
                "from dask import bag\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras.models import Sequential\n",
                "from keras.utils import to_categorical\n",
                "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
                "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "def pyth_test (x1, x2):\n",
                "   \n",
                "    print (x1 + x2)\n",
                "\n",
                "pyth_test(1,2)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import os\n",
                "import re\n",
                "from glob import glob\n",
                "from tqdm import tqdm\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import ast\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "draw_df.drawing.values[0]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "source": [
                "evens = range(0,11,2)\n",
                "odds = range(1,12, 2)\n",
                "# We have drawing images, 2 per label, consecutively\n",
                "df1 = draw_df[draw_df.index.isin(evens)]\n",
                "df2 = draw_df[draw_df.index.isin(odds)]\n",
                "\n",
                "example1s = [ast.literal_eval(pts) for pts in df1.drawing.values]\n",
                "example2s = [ast.literal_eval(pts) for pts in df2.drawing.values]\n",
                "labels = df2.word.tolist()\n",
                "\n",
                "for i, example in enumerate(example1s):\n",
                "    plt.figure(figsize=(6,3))\n",
                "    \n",
                "    for x,y in example:\n",
                "        plt.subplot(1,2,1)\n",
                "        plt.plot(x, y, marker='.')\n",
                "        plt.axis('off')\n",
                "\n",
                "    for x,y, in example2s[i]:\n",
                "        plt.subplot(1,2,2)\n",
                "        plt.plot(x, y, marker='.')\n",
                "        plt.axis('off')\n",
                "        label = labels[i]\n",
                "        plt.title(label, fontsize=10)\n",
                "\n",
                "    plt.show()  "
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "\n",
                "import os\n",
                "from glob import glob\n",
                "import re\n",
                "import ast\n",
                "import numpy as np \n",
                "import pandas as pd\n",
                "from PIL import Image, ImageDraw \n",
                "from tqdm import tqdm\n",
                "from dask import bag\n",
                "import json\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras.models import Sequential\n",
                "from keras.models import Model\n",
                "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
                "from tensorflow.keras.metrics import top_k_categorical_accuracy\n",
                "from keras.layers import Input, Conv1D, Dense, Dropout, BatchNormalization, Flatten, MaxPool1D\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping"
            ]
        },
        {
            "tags": [
                "check_results",
                "process_data"
            ],
            "source": [
                "valfrac = 0.1 \n",
                "cutpt = int(valfrac * train_grand.shape[0])\n",
                "print(cutpt)\n",
                "\n",
                "np.random.shuffle(train_grand)\n",
                "y_train, X_train = train_grand[cutpt: , 0], train_grand[cutpt: , 1:]\n",
                "y_val, X_val = train_grand[0:cutpt, 0], train_grand[0:cutpt, 1:]\n",
                "\n",
                "del train_grand\n",
                "\n",
                "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
                "X_train = X_train.reshape(-1, sequence_length,2)\n",
                "\n",
                "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
                "X_val = X_val.reshape(-1, sequence_length,2)\n",
                "\n",
                "print(y_train.shape, \"\\n\",\n",
                "      X_train.shape, \"\\n\",\n",
                "      y_val.shape, \"\\n\",\n",
                "      X_val.shape)\n"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "def createNetwork(seq_len):\n",
                "    \n",
                "    # Function to add a convolution layer with batch normalization\n",
                "    def addConv(network, features, kernel):\n",
                "        network = BatchNormalization()(network)\n",
                "        return Conv1D(features, kernel, padding='same', activation='relu')(network)\n",
                "    \n",
                "    # Function to add a dense layer with batch normalization and dropout\n",
                "    def addDense(network, size):\n",
                "        network = BatchNormalization()(network)\n",
                "        network = Dropout(0.2)(network)\n",
                "        return Dense(size, activation='relu')(network)\n",
                "    \n",
                "    \n",
                "    # Input layer\n",
                "    input = Input(shape=(seq_len, 2))\n",
                "    network = input\n",
                "    \n",
                "    # Add 1D Convolution\n",
                "    for features in [16, 24, 32]:\n",
                "        network = addConv(network, features, 5)\n",
                "    network = MaxPool1D(pool_size=5)(network)\n",
                "    \n",
                "    # Add 1D Convolution\n",
                "    for features in [64, 96, 128]:\n",
                "        network = addConv(network, features, 5)\n",
                "    network = MaxPool1D(pool_size=5)(network)\n",
                "\n",
                "    # Add 1D Convolution\n",
                "    for features in [256, 384, 512]:\n",
                "        network = addConv(network, features, 5)\n",
                "    #network = MaxPool1D(pool_size=5)(network)\n",
                "\n",
                "    # Flatten\n",
                "    network = Flatten()(network)\n",
                "    \n",
                "    # Dense layer for combination\n",
                "    for size in [128, 128]:\n",
                "        network = addDense(network, size)\n",
                "    \n",
                "    # Output layer\n",
                "    output = Dense(len(files), activation='softmax')(network)\n",
                "\n",
                "\n",
                "    # Create and compile model\n",
                "    model = Model(inputs = input, outputs = output)\n",
                "#     model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
                "\n",
                "#     # Display model\n",
                "#     model.summary()\n",
                "    return model\n",
                "\n",
                "model = createNetwork(sequence_length)"
            ]
        },
        {
            "tags": [
                "train_model",
                "check_results"
            ],
            "source": [
                "def top_3_accuracy(x,y): \n",
                "    t3 = top_k_categorical_accuracy(x,y, 3)\n",
                "    return t3\n",
                "\n",
                "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, \n",
                "                                   verbose=1, mode='auto', min_delta=0.005, cooldown=5, min_lr=0)\n",
                "\n",
                "earlystop = EarlyStopping(monitor='val_loss', mode='auto', patience=2,verbose=0) \n",
                "\n",
                "#callbacks = [reduceLROnPlat, earlystop]\n",
                "#callbacks = earlystop\n",
                "\n",
                "model.compile(loss='categorical_crossentropy',\n",
                "              optimizer='adam',\n",
                "              metrics=['accuracy', top_3_accuracy])\n",
                "\n",
                "model.summary()\n",
                "\n",
                "# model.fit(x=X_train, y=y_train,\n",
                "#           batch_size = 1000,\n",
                "#           epochs = 100,\n",
                "#           validation_data = (X_val, y_val),\n",
                "#           callbacks = callbacks,\n",
                "#           verbose = 1)\n",
                "model.fit(x=X_train, y=y_train,\n",
                "          batch_size = 1000,\n",
                "          epochs = 25,\n",
                "          validation_data = (X_val, y_val),\n",
                "          verbose = 1)\n"
            ]
        },
        {
            "tags": [
                "check_results",
                "ingest_data",
                "process_data"
            ],
            "source": [
                "#%% get test set\n",
                "ttvlist = []\n",
                "reader = pd.read_csv('../input/test_simplified.csv', index_col=['key_id'],\n",
                "    chunksize=2048)\n",
                "\n",
                "for chunk in tqdm(reader, total=55):\n",
                "    X =[]\n",
                "    for values in chunk.drawing.values:\n",
                "        image = json.loads(values)\n",
                "        strokes = []\n",
                "        for x_axis, y_axis in image:\n",
                "            strokes.extend(list(zip(x_axis, y_axis)))\n",
                "        strokes = np.array(strokes)\n",
                "        pad = np.zeros((sequence_length, 2))\n",
                "        if sequence_length>strokes.shape[0]:\n",
                "            pad[:strokes.shape[0],:] = strokes\n",
                "        else:\n",
                "            pad = strokes[:sequence_length, :]\n",
                "        X.append(pad)\n",
                "        \n",
                "    X = np.array(X)\n",
                "    X = np.reshape(X, (-1,sequence_length, 2))\n",
                "    testpreds = model.predict(X, verbose=0)\n",
                "    ttvs = np.argsort(-testpreds)[:, 0:3]\n",
                "    ttvlist.append(ttvs)\n",
                "#     imagebag = bag.from_sequence(chunk.drawing.values).map(draw_it)\n",
                "#     testarray = np.array(imagebag.compute())\n",
                "\n",
                "#     testarray = np.reshape(testarray, (testarray.shape[0], imheight, imwidth, 1))\n",
                "#     testpreds = model.predict(testarray, verbose=0)\n",
                "#     ttvs = np.argsort(-testpreds)[:, 0:3]  # top 3\n",
                "#     ttvlist.append(ttvs)\n",
                "    \n",
                "ttvarray = np.concatenate(ttvlist)"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "check_results",
                "ingest_data",
                "process_data"
            ],
            "source": [
                "preds_df = pd.DataFrame({'first': ttvarray[:,0], 'second': ttvarray[:,1], 'third': ttvarray[:,2]})\n",
                "preds_df = preds_df.replace(numstonames)\n",
                "preds_df['words'] = preds_df['first'] + \" \" + preds_df['second'] + \" \" + preds_df['third']\n",
                "\n",
                "sub = pd.read_csv('../input/sample_submission.csv', index_col=['key_id'])\n",
                "sub['word'] = preds_df.words.values\n",
                "sub.to_csv('subcnn_small.csv')\n",
                "sub.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sb\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report\n",
                "from sklearn.metrics import confusion_matrix\n",
                "from sklearn.metrics import r2_score"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "source": [
                "zomato_orgnl=pd.read_csv(\"../input/zomato-bangalore-restaurants/zomato.csv\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato_orgnl.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato_orgnl.isnull().sum()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato_orgnl.info()"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "zomato=zomato_orgnl.drop(['url','dish_liked','phone'],axis=1)\n",
                "zomato.columns"
            ]
        },
        {
            "tags": [
                "process_data",
                "check_results"
            ],
            "source": [
                "zomato.rename({'approx_cost(for two people)': 'approx_cost_for_2_people',\n",
                "               'listed_in(type)':'listed_in_type',\n",
                "               'listed_in(city)':'listed_in_city'\n",
                "              }, axis=1, inplace=True)\n",
                "zomato.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "remove_comma = lambda x: int(x.replace(',', '')) if type(x) == np.str and x != np.nan else x \n",
                "zomato.votes = zomato.votes.astype('int')\n",
                "zomato['approx_cost_for_2_people'] = zomato['approx_cost_for_2_people'].apply(remove_comma)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato.info()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato['rate'].unique()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "zomato = zomato.loc[zomato.rate !='NEW']\n",
                "zomato = zomato.loc[zomato.rate !='-'].reset_index(drop=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "remove_slash = lambda x: x.replace('/5', '') if type(x) == np.str else x\n",
                "zomato.rate = zomato.rate.apply(remove_slash).str.strip().astype('float')"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato['rate'].head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "zomato_en['rate'] = zomato_en['rate'].fillna(zomato_en['rate'].mean())\n",
                "zomato_en['approx_cost_for_2_people'] = zomato_en['approx_cost_for_2_people'].fillna(zomato_en['approx_cost_for_2_people'].mean())"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato_en.isna().sum()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "transfer_results"
            ],
            "source": [
                "plt.figure(figsize=(15,8))\n",
                "sns.heatmap(corr, annot=True)\n",
                "plt.savefig(\"image0.png\")"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "zomato_en.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x = zomato_en.iloc[:,[2,3,5,6,7,8,9,11]]\n",
                "y = zomato_en['rate']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=353)"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "x_train.head()"
            ]
        },
        {
            "tags": [
                "check_results"
            ],
            "source": [
                "y_train.head()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "source": [
                "reg=LinearRegression()\n",
                "reg.fit(x_train,y_train)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "source": [
                "y_pred=reg.predict(x_test)\n",
                "from sklearn.metrics import r2_score\n",
                "r2_score(y_test,y_pred)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "'''reg_score=[]\n",
                "import numpy as np\n",
                "for j in range(1000):\n",
                "    x_train,x_test,y_train,y_test =train_test_split(x,y,random_state=j,test_size=0.1)\n",
                "    lr=LinearRegression().fit(x_train,y_train)\n",
                "    reg_score.append(lr.score(x_test,y_test))\n",
                "K=reg_score.index(np.max(reg_score))\n",
                "#K=353'''"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from sklearn.tree import DecisionTreeRegressor"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "source": [
                "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=105)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "DTree.fit(x_train,y_train)\n",
                "y_predict=DTree.predict(x_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "source": [
                "from sklearn.metrics import r2_score"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "source": [
                "r2_score(y_test,y_predict)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "'''from sklearn.tree import DecisionTreeRegressor\n",
                "ts_score=[]\n",
                "for j in range(1000):\n",
                "    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.1,random_state=j)\n",
                "    dc=DecisionTreeRegressor().fit(x_train,y_train)\n",
                "    ts_score.append(dc.score(x_test,y_test))\n",
                "J= ts_score.index(np.max(ts_score))\n",
                "\n",
                "J\n",
                "#J=105'''"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "source": [
                "from sklearn.ensemble import RandomForestRegressor\n",
                "RForest=RandomForestRegressor(n_estimators=5,random_state=329,min_samples_leaf=.0001)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "source": [
                "RForest.fit(x_train,y_train)\n",
                "y_predict=RForest.predict(x_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "source": [
                "from sklearn.metrics import r2_score\n",
                "r2_score(y_test,y_predict)"
            ]
        }
    ]
}