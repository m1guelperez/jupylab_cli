{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c17253e0-77b4-eb1b-f40f-98566083d37e"
   },
   "source": [
    "There are already a bunch of awesome Scripts, but I wanted to step back and work with some more rudimentary models to make sure I was doing the right data preparation.\n",
    "\n",
    "Let's start by loading our packages and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "19f5d752-089f-e8b1-ff2c-e82e076fa649"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skew, boxcox\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Load Training Data\n",
    "train = pd.read_csv('../input/train.csv', dtype={'id': np.int32})\n",
    "\n",
    "# Load Test Data\n",
    "test = pd.read_csv('../input/test.csv', dtype={'id': np.int32})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3e16f0b1-7c27-d32d-7998-897d47f304b3"
   },
   "source": [
    "Nomenclature note: The outcome variable for this competition is 'loss'. (If you read much machine learning literature, you've probably heard the term loss as in '[loss function](https://en.wikipedia.org/wiki/Loss_function)'.) That isn't exactly what we mean in this context. The 'loss' variable in this case literally refers to the amount AllState lost on the settlement. Wherever you see 'loss' in this document, assume I'm talking about the amount AllState lost, and not the output of a loss function.\n",
    "\n",
    "Now, prediction is easier on an outcome that's normally distributed. Let's check to see if this data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "2b8320ec-2d10-dcda-966f-9fd48b011b4d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAawAAAEZCAYAAADLzxFqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHS9JREFUeJzt3XuUZWV55/Hvj0t7446hGxtslKuwEpFRZMZMrGQcbirN\n5IJglJuZIVGXRk0CqAkySUZJvAUJEhwSGogijia0Wa0Qgm0yUREF1Ai0jUILNF1IGBAhGoRn/ti7\n4HD6VJ3TUKeqNnw/a51V+/K+7372qerz9Pvud++TqkKSpIVus/kOQJKkUZiwJEmdYMKSJHWCCUuS\n1AkmLElSJ5iwJEmdYMLSk0qSjyZ51yy1tWuSHyZJu/6FJCfORttte6uSvH622tuE4/5Rkh8kWT/X\nx5aeiHgflroiyS3ATsCDwEPA9cCFwLm1iX/ISW4G3lBVV25CnS8AF1bVX27Ksdq6pwG7V9Wxm1p3\nNiXZFVgD7FpV/zpg/8uBi6pq1zkPThrCHpa6pIBXVtW2wDLgfcDJwHmzfaAkm892mwvEMuCuQcmq\nh/+L1YJkwlLXBKCq7quqvwNeAxyXZF+AJH+V5H+2yzsm+WyS/5fkX5N8sd1+AfBc4LPtkN/vJFmW\n5OEkJyZZB/xDz7befyd7JLkqyb1J/ibJdm2bL09y62MCTW5O8ktJDgHeCbwmyX1Jrm33PzLEmMa7\nk9ySZEOS85Ns0+6biuPYJOuS3JnkndO+Qck2SS5oy908NUSa5L8AlwPPac97k3qK07Xb7ts9yeok\n97T7P9Gz70NJJtv37Bs9v6tFSd7fntMdSc5O8rSZfnd6ajNhqdOq6mrgNuA/D9j9DuBWYEeaocR3\ntnWOBb4PvKqqtqmq9/fU+QVgH+CQqUP0tfl64HhgCc2w5Ed6w5kmxsuA/wV8sqq2rqoXDSh2AnAs\n8HLg+cDWwFl9ZV4G7Am8AviDJHsPOl5bb2tgN2ACODbJCVX1D8BhwPr2vDf1etzAdtt9fwhcVlXb\nAbvQvi9JDgZ+Htij7RkfBUz17s4A9gB+rv25FPiDdt/A352e2kxYejJYD+wwYPuDwM7A86rqoar6\n57796Vsv4LSq+req+sk0x7qwqm6oqn8Dfh/4talJGU/Qa4EPVtW6qnoAOBU4uqd3V8B7qurfq+qb\nwDeAF/Y30pZ/DXBKVT1QVeuAD9Ak2sdthHYfBJYlWdrG+KWe7VsD+yZJVa2pqsl2338H3lZV91bV\n/TRDvMf01Jvpd6enIBOWngyWAncP2P6nwHeBy5PclOTkEdq6bcj+3mG/dcCWwLNHinJmz2nb6217\nC2Bxz7bJnuUHgK0GtPPstt73+9pa+gTjG9bu79F8nnw1ybemel5V9QWantmfA5NJzkmyVZKfAZ4J\nfD3J3UnuBj5H06OCx/e705OcCUudluQlNB/2/9S/r6p+VFW/U1W7A0cAb0/yi1O7p2ly2ISD3tlz\ny2h6AncB99N8AE/FtTnwM5vQ7vq2vf62JwcXn9Zdbb3+tm7fxHY2qd2qmqyq/1FVS4HfBM5O8vx2\n31lV9WJgX2Bv4Hfb9h4A9quqHdrXdu2w4bDfnZ6iTFjqpCRbJ3kV8AmaYbrrB5R5ZZLd29X7gJ/S\nXHeCJhE8v7/KoEP1rb8uyT5JngmcDnyqnVL/HeDpSQ5LsgXwbmBRT71JYLcZhg8/AbwtyW5JtgL+\nGLi4qh6eIbaNtOUvAf647cksA95GM/1/VEnytN7XsHaT/GqSqd7WPcDDwMNJXpzkwPY9+Tfgx8DD\n7Xv2MeDDbW+LJEvba17T/e6m3gs9RZmw1DWfTXIvzdDUqcD7gekmD+wJXJHkPuCfgT+vqn9s970X\n+P12OOrt7bZBvaDqW74QWEHTI1oEvBWgqn4IvJFmiv1tNB+yvcOLn6JJOv+a5GsD2v7Ltu1/pBkK\newB4yzRxTBfrlLe09b/XtndRVf3VDOX7Paet/wBNknmg7S3N1O5LgKuS/BD4W+AtVXULsA1NYrob\nuJmmZ/WnbZ2TgZuAryS5h2YG417tvkG/O2cKPsWN/cbhJIcCH6ZJjudV1RkDypxJM3vpfuD4qrpu\nprpJtgc+STMkcQtwVFXdm+QVNBdutwT+Hfi9dgydJAcA5wNPB1ZV1W+P65wlSbNvrD2sdmbRWTRT\nhPcDjkmyT1+Zw2ieALAncBJwzgh1TwGuqKq9gStp/qcN8AOaqcovpJl63DsM8lGaJxvsBeyV5t4Y\nSVJHjHtI8EBgbTtV90HgYmB5X5nlwAUAVXUVsG2SxUPqLqcZlqH9eWRb/xtVtaFd/jbNNYUtkywB\ntm7v2aE93pGzf7qSpHEZd8JaymOnAd/GxtNrpyszU93FU/dytAlqp/4DJ/lV4Jo22S3lsdcTBsUh\nSVrAtpjvAAZ4PDdhPuZCXJL9aC6q/9dZiUiSNO/GnbBup3lm25Rd2Ph+kNt57L0tU2UWzVB3Q5LF\nVTXZDvfdOVUoyS7AZ4DXt7OUZjrGRpL44E9Jehyqajae+jKtcQ8JXk3zsNBlSRYBRwMr+8qspHmG\nGkkOAu5ph/tmqruSZlIFwHHApW397YC/A06uqq9MHaAdNry3vR8k7fEunS7oqurs67TTTpv3GJ6K\nsRv//L+Mf35jnwtj7WFV1UNJ3kxzf8XU1PQbkpzU7K5zq2pVksOT3EQzrf2Emeq2TZ8BXJLmSdfr\naB6oCfAmYHeaB4OeRjNUeHBV3dXuO59Hp7V/fpznLkmaXWO/htUmhr37tv1F3/qbR63bbr+b5onV\n/dv/mOYJAYPa+jrwsyMHLklaUHzSxZPMxMTEfIfwuHU5djD++Wb882euYh/7ky66pvkGBN8TSdoU\nSaiOT7qQJGlWmLAkSZ1gwpIkdYIJS5LUCQvx0Uzz7g1vGDjL/hEve9mBnHjisXMUjSQJnCW4kebR\nTB+ZocQkO+98CevXr5mzmCRpoZuLWYL2sAaaqYe1huabwiVJc8lrWJKkTjBhSZI6wYQlSeoEE5Yk\nqRNMWJKkTjBhSZI6wYQlSeoEE5YkqRNMWJKkTjBhSZI6wYQlSeoEE5YkqRNMWJKkTjBhSZI6wYQl\nSeoEE5YkqRNMWJKkTjBhSZI6wYQlSeoEE5YkqRNMWJKkTjBhSZI6wYQlSeoEE5YkqRNMWJKkTjBh\nSZI6wYQlSeoEE5YkqRNMWJKkTjBhSZI6wYQlSeoEE5YkqRNMWJKkTjBhSZI6wYQlSeoEE5YkqRNM\nWJKkTjBhSZI6YewJK8mhSW5M8p0kJ09T5swka5Ncl2T/YXWTbJ/k8iRrklyWZNt2+w5JrkxyX5Iz\n+47xhbata5Nck+TZ4zpnSdLsG2vCSrIZcBZwCLAfcEySffrKHAbsXlV7AicB54xQ9xTgiqraG7gS\nOLXd/mPg3cA7pgnpmKp6UVUdUFV3zdJpSpLmwLh7WAcCa6tqXVU9CFwMLO8rsxy4AKCqrgK2TbJ4\nSN3lwIp2eQVwZFv/gar6EvCTaeJxCFSSOmrcH+BLgVt71m9rt41SZqa6i6tqEqCqNgA7jRjP+e1w\n4LtHLC9JWiC2mO8ABsjjqFMjlHltVd2R5FnAZ5K8rqouGlz0PT3LE+1LkjRl9erVrF69ek6POe6E\ndTvw3J71Xdpt/WV2HVBm0Qx1NyRZXFWTSZYAdw4LpKruaH/en+TjNEOOIyQsSVK/iYkJJiYmHlk/\n/fTTx37McQ8JXg3skWRZkkXA0cDKvjIrgWMBkhwE3NMO981UdyVwfLt8HHDpgGM/0lNLsnmSHdvl\nLYFXAf/yxE9PkjRXxtrDqqqHkrwZuJwmOZ5XVTckOanZXedW1aokhye5CbgfOGGmum3TZwCXJDkR\nWAccNXXMJDcDWwOLkiwHDga+D1yWZAtgc+AK4GPjPHdJ0uxK1SiXf546ktTMl8TWsPPOR7B+/Zo5\ni0mSFrokVNXjmYMwMqd5S5I6wYQlSeoEE5YkqRNMWJKkTjBhSZI6wYQlSeoEE5YkqRNMWJKkTjBh\nSZI6wYQlSeoEE5YkqRNMWJKkTjBhSZI6wYQlSeoEE5YkqRNMWJKkTjBhSZI6wYQlSeoEE5YkqRNM\nWJKkTjBhSZI6wYQlSeoEE5YkqRNMWJKkTjBhSZI6wYQlSeoEE5YkqRNGSlhJPpPklUlMcJKkeTFq\nAjobeC2wNsn7kuw9xpgkSdrISAmrqq6oql8HDgBuAa5I8qUkJyTZcpwBSpIEm3ANK8mOwPHAbwDX\nAn9Gk8D+fiyRSZLUY4tRCiX5G2Bv4ELg1VV1R7vrk0m+Nq7gJEmaMlLCAj5WVat6NyR5WlX9pKpe\nPIa4JEl6jFGHBP9owLYvz2YgkiTNZMYeVpIlwFLgGUleBKTdtQ3wzDHHJknSI4YNCR5CM9FiF+CD\nPdvvA945ppgkSdrIjAmrqlYAK5L8SlV9eo5ikiRpI8OGBF9XVRcBuyV5e//+qvrggGqSJM26YUOC\nz2p/bjXuQCRJmsmwIcG/aH+ePjfhSJI02LAhwTNn2l9Vb5ndcCRJGmzYkODX5yQKSZKGGGWWoCRJ\n827YkOCHq+q3k3wWqP79VXXE2CKTJKnHsCHBC9uf7x93IJIkzWTYkODX259fTLII2Iemp7Wmqv59\nDuKTJAkY/etFXgmcA3yX5nmCz0tyUlV9bpzBSZI0ZdSntX8A+MWqmqiqlwO/CHxolIpJDk1yY5Lv\nJDl5mjJnJlmb5Lok+w+rm2T7JJcnWZPksiTbttt3SHJlkvv6p+QnOSDJN9u2PjzieUuSFohRE9Z9\nVXVTz/r3aB6AO6MkmwFn0TxEdz/gmCT79JU5DNi9qvYETqLpyQ2rewpwRVXtDVwJnNpu/zHwbuAd\nA8L5KPCGqtoL2CvJIUPPWpK0YMyYsJL8cpJfBr6WZFWS45McB3wWuHqE9g8E1lbVuqp6ELgYWN5X\nZjlwAUBVXQVsm2TxkLrLgakp9yuAI9v6D1TVl4Cf9J3HEmDrqpqK+YKpOpKkbhh2DevVPcuTwMvb\n5R8Azxih/aXArT3rt9EkomFllg6pu7iqJgGqakOSnUaI47YBx5AkdcSwWYInzFUgPTK8yEY2ukfs\niXlPz/JE+5IkTVm9ejWrV6+e02OOOkvw6cAbaK4lPX1qe1WdOKTq7cBze9Z3abf1l9l1QJlFM9Td\nkGRxVU22w313jhDHoGNM4z1DmpOkp7aJiQkmJiYeWT/99PE/I33USRcXAktoJkB8keYDf+ikC5rr\nXHskWdbex3U0sLKvzErgWIAkBwH3tMN9M9VdSfNNyADHAZcOOPYjPbWq2gDcm+TAJGmPN6iOJGmB\nGqmHBexRVb+WZHlVrUjyceCfhlWqqoeSvBm4nCY5nldVNyQ5qdld51bVqiSHJ7kJuB84Yaa6bdNn\nAJckORFYBxw1dcwkNwNbA4uSLAcOrqobgTcB59P0EFdV1edHPHdJ0gKQquGXf5J8taoOTPKPwBuB\nDcBXq+r54w5wriWpmS+JrWHnnY9g/fo1cxaTJC10SaiqxzMHYWSj9rDOTbI98Ps0w3FbtcuSJM2J\nkRJWVf3vdvGLwJOuVyVJWvhGmnSRZMckH0lyTZKvJ/lwkh3HHZwkSVNGnSV4Mc3U8V8BfhW4C/jk\nuIKSJKnfqNewdq6qP+xZ/6MkrxlHQJIkDTJqD+vyJEcn2ax9HQVcNs7AJEnqNeO09iT30czxDvAs\n4OF212bAj6pqm7FHOMec1i5Jm27ep7VX1dbjPLgkSaMa9RoWSY4AfqFdXV1VfzeekCRJ2tio09rf\nB7wVuL59vTXJe8cZmCRJvUbtYR0O7F9VDwMkWQFcy6Pf9CtJ0liNOksQYLue5W1nOxBJkmYyag/r\nvcC1Sb5AM2PwF4BTxhaVJEl9hias9vuj/i9wEPCSdvPJ7XdMSZI0J4YmrKqqJKuq6mfZ+MsXJUma\nE6New7omyUuGF5MkaTxGvYb1UuB1SW6h+Vbg0HS+fm5cgUmS1GvUhHXIWKOQJGmIGRNWkqcDvwns\nAXwLOK+qfjoXgUmS1GvYNawVwItpktVhwAfGHpEkSQMMGxLct50dSJLzgK+OPyRJkjY2rIf14NSC\nQ4GSpPk0rIf1wiQ/bJcDPKNdn5ol+KT7PixJ0sI07PuwNp+rQCRJmsmmPPxWkqR5Y8KSJHWCCUuS\n1AkmLElSJ5iwJEmdYMKSJHWCCUuS1AkmLElSJ5iwJEmdYMKSJHWCCUuS1AkmLElSJ5iwJEmdYMKS\nJHWCCUuS1AkmLElSJ5iwJEmdYMKSJHWCCUuS1AkmLElSJ5iwJEmdYMKSJHWCCUuS1AljT1hJDk1y\nY5LvJDl5mjJnJlmb5Lok+w+rm2T7JJcnWZPksiTb9uw7tW3rhiQH92z/QtvWtUmuSfLscZ2zJGn2\njTVhJdkMOAs4BNgPOCbJPn1lDgN2r6o9gZOAc0aoewpwRVXtDVwJnNrW2Rc4CngBcBhwdpL0HO6Y\nqnpRVR1QVXeN45wlSeMx7h7WgcDaqlpXVQ8CFwPL+8osBy4AqKqrgG2TLB5Sdzmwol1eARzZLh8B\nXFxVP62qW4C1bTtTHAKVpI4a9wf4UuDWnvXb2m2jlJmp7uKqmgSoqg3ATtO0dXvf8c5vhwPfvemn\nIkmaT1vMdwADZHiRjdQIZV5bVXckeRbwmSSvq6qLBhd9T8/yRPuSJE1ZvXo1q1evntNjjjth3Q48\nt2d9l3Zbf5ldB5RZNEPdDUkWV9VkkiXAnUPaoqruaH/en+TjNEOFIyQsSVK/iYkJJiYmHlk//fTT\nx37McQ8JXg3skWRZkkXA0cDKvjIrgWMBkhwE3NMO981UdyVwfLt8HHBpz/ajkyxK8jxgD+CrSTZP\nsmN7jC2BVwH/MutnK0kam7H2sKrqoSRvBi6nSY7nVdUNSU5qdte5VbUqyeFJbgLuB06YqW7b9BnA\nJUlOBNbRzAykqq5PcglwPfAg8MaqqiRPAy5LsgWwOXAF8LFxnrskaXalapTLP08dSWrmS2Jr2Hnn\nI1i/fs2cxSRJC10SqurxzEEYmdO8JUmdYMKSJHWCCetxmJy8nSRDX0uW7DbfoUrSk8ZCvA9rwXv4\n4fsZ5davycmxDudK0lOKPSxJUieYsCRJnWDCkiR1gglLktQJJixJUieYsCRJnWDCkiR1gglLktQJ\nJixJUieYsCRJnWDCkiR1gglLktQJJixJUieYsCRJnWDCkiR1gglLktQJJixJUieYsCRJnWDCkiR1\ngglLktQJJixJUieYsCRJnWDCkiR1gglLktQJJixJUieYsCRJnWDCkiR1gglLktQJJixJUieYsCRJ\nnWDCkiR1gglLktQJJixJUieYsCRJnWDCkiR1gglLktQJJqyxehpJhr6WLNltvgOVpAVvi/kO4Mnt\nJ0ANLTU5mfGHIkkdZw9LktQJJixJUieYsCRJnWDCkiR1wtgTVpJDk9yY5DtJTp6mzJlJ1ia5Lsn+\nw+om2T7J5UnWJLksybY9+05t27ohycE92w9I8s22rQ+P63wlSeMx1oSVZDPgLOAQYD/gmCT79JU5\nDNi9qvYETgLOGaHuKcAVVbU3cCVwaltnX+Ao4AXAYcDZSaam4H0UeENV7QXsleSQ8Zz1/Fq9evV8\nh/C4dTl2MP75ZvzzZ65iH3cP60BgbVWtq6oHgYuB5X1llgMXAFTVVcC2SRYPqbscWNEurwCObJeP\nAC6uqp9W1S3AWuDAJEuAravq6rbcBT11FoDh92uNeq+Wf/Tzx/jnl/HPnydLwloK3Nqzflu7bZQy\nM9VdXFWTAFW1AdhpmrZu72nrtiFxzKOp+7Wmf01Orpu/8CRpAViINw4/nrtoh9+duwm22ebV0+57\n+OEf8aMfzebRRtX0wobbgtNPP31oqcWLl7Fhwy1POCpJmjNVNbYXcBDw+Z71U4CT+8qcA7ymZ/1G\nYPFMdYEbaHpZAEuAGwa1D3weeGlvmXb70cBHp4l55q6OL1++fPka+BpnPqmqsfewrgb2SLIMuIMm\nURzTV2Yl8Cbgk0kOAu6pqskkd81QdyVwPHAGcBxwac/2v07yIZohvz2Ar1ZVJbk3yYFtTMcCZw4K\nuKp8TpIkLUBjTVhV9VCSNwOX01wvO6+qbkhyUrO7zq2qVUkOT3ITcD9wwkx126bPAC5JciKwjmZm\nIFV1fZJLgOuBB4E3VtttokmK5wNPB1ZV1efHee6SpNmVRz/PJUlauHzSRWuUG5znKI5dklyZ5NtJ\nvpXkLe32WbtZOsmiJBe3db6c5LljOI/NklyTZGXX4k+ybZJPtfF8O8lLOxb/25L8S3vsv26Pt2Dj\nT3Jekskk3+zZNifxJjmuLb8mybGzGP+ftPFdl+TTSbZZiPEPir1n3zuSPJxkhwUT+7gvknXhRZO4\nbwKWAVsC1wH7zFMsS4D92+WtgDXAPjTDoL/Xbj8ZeF+7vC9wLc3w7m7teUz1nK8CXtIurwIOaZd/\nCzi7XX4Nzb1rs30ebwMuAla2652Jn2bo+IR2eQtg267EDzwH+B6wqF3/JM113gUbP/DzwP7AN3u2\njT1eYHvgu+3vd7up5VmK/xXAZu3y+4D3LsT4B8Xebt+FZtLazcAO7bYXzHfsc/6BvBBfNDMSP9ez\nvtFsxnmM7W/bP/4beezMyBsHxQp8jkdnRl7fs/2RmZHtH+JL2+XNgR/Mcsy7AH8PTPBowupE/MA2\nwHcHbO9K/M+hua67ffvBsrILfz80/1ns/cAfZ7x39pdp1z9Kz4zlJxJ/374jgQsXavyDYgc+Bfws\nj01Y8x67Q4KNUW5wnnNJdqP5389XmN2bpR+pU1UPAff0dvtnwYeA36WZ6jqlK/E/D7gryV+lGdI8\nN8kzuxJ/Va0HPgB8v43l3qq6oivx99hpjPHe28Y7XVuz7USaXsdjYuk75oKJP8kRwK1V9a2+XfMe\nuwlrgUqyFfB/gLdW1Y947Ic/A9af0OFmraHklcBkVV03pN0FGT9Nr+QA4M+r6gCamaun0J33fzua\nR5cto+ltPSvJr9OR+GfQtXibAyXvAh6sqk/MZrOz2NZjG06eAbwTOG1ch3gilU1YjduB3gvHu7Tb\n5kWSLWiS1YVVNXWP2WSaZyyS5tmId7bbbwd27ak+Fft02x9TJ8nmwDZVdfcshf8y4Igk3wM+AfxS\nkguBDR2J/zaa/11+rV3/NE0C68r7/wrge1V1d/s/2r8B/lOH4p8yF/GO9d99kuOBw4HX9mxe6PHv\nTnN96htJbm7bvCbJTjMcb+5if6Jjz0+GF83Y6tSki0U0ky5eMI/xXAB8sG/bGTz6pI9BF6EX0Qxn\n9V4I/QrNQ4RDMyRxaLv9jTx6IfRoxjDpom375Tx6DetPuhI/8EVgr3b5tPa978T73x7vWzT3G4Zm\nAsmbFnr8NB+S35rLv3cee+F/anm7WYr/UODbwI595RZc/P2x9+27Gdh+ocQ+6x9SXX21f2BraJ7w\nfso8xvEy4CGapHktcE0b2w7AFW2Ml/f+cmm+XuUmmkdWHdyz/T/QfHitBf6sZ/vTgEva7V8BdhvT\nufQmrM7ED7yQ5oko1wGfaf9RdSn+09pYvknzbQZbLuT4gY8D62meAv19mocHbD8X8dI8MWct8B3g\n2FmMfy3N5Jdr2tfZCzH+QbH37f8e7aSLhRC7Nw5LkjrBa1iSpE4wYUmSOsGEJUnqBBOWJKkTTFiS\npE4wYUmSOsGEJc2zJPfNdwxSF5iwpPnnzZDSCExY0gKUZFmSf2i/APDvk+zSbv+1NF/seW2S1e22\nfZNc1T5d/roku89r8NKY+KQLaZ4l+WFVbdO3bSVwSVVdlOQE4Iiq+m/tN8MeUlV3JNmmqn6Y5Ezg\ny1X1ifbByZtX1U/m4VSksbKHJS1M/5HmafcAF9I8YxLgn4EVSX6D5qtQAL4MvCvJ79I8q81kpScl\nE5a0MA0c+qiq3wLeRfOVDV9Psn0137X0auDHwKokE3MWpTSHTFjS/Bv0pXZfAo5pl18H/BNAkudX\n1dVVdRrNd0TtmuR5VXVzVX0EuBT4ubkIWpprXsOS5lmSn9J8xUNoelYfpPniyPOBHYEf0Hztw21J\nPg3s2Va9oqrenuRk4PXAg8AdwGur6p65PQtp/ExYkqROcEhQktQJJixJUieYsCRJnWDCkiR1gglL\nktQJJixJUieYsCRJnWDCkiR1wv8HBMi7vBVokicAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5cbd32e2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train['loss'], 30, normed=1)\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Distribution of Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b695414e-3aff-6f82-cb7a-1b19200ef67f"
   },
   "source": [
    "Wow. That isn't normally distributed at all: it's super *[skewed](https://en.wikipedia.org/wiki/Skewness)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "8b7ea3d5-856f-6d13-73bc-fc1a8a575c4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7949281496777445"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skew(train['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1c4fa9c4-f801-4ece-0dc3-1864f76abe40"
   },
   "source": [
    "Any skew greater than one should probably catch your attention. Luckily, we have a simple counterspell! Let's *log-transform* the 'loss' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1b3a0cf-f020-2c12-abbd-c304e180fc57"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEZCAYAAACTsIJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntayb5z4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5ca75db9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train['log_loss'] = np.log(train['loss'])\n",
    "\n",
    "plt.hist(train['log_loss'], 30, normed=1)\n",
    "plt.xlabel('Log(Loss)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Distribution of Log(Loss)es')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e90b49ae-5b8e-d632-0b13-56d86085f2e4"
   },
   "source": [
    "Much Better. Now, what about our input variables? Are they similarly skewed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "82ce863f-e4a1-c0b1-16dc-51182057a96e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       -0.002155\n",
       "cont1     0.516420\n",
       "cont2    -0.310939\n",
       "cont3    -0.010002\n",
       "cont4     0.416093\n",
       "cont5     0.681617\n",
       "cont6     0.461211\n",
       "cont7     0.826046\n",
       "cont8     0.676629\n",
       "cont9     1.072420\n",
       "cont10    0.354998\n",
       "cont11    0.280819\n",
       "cont12    0.291990\n",
       "cont13    0.380739\n",
       "cont14    0.248672\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_numeric = test.dtypes[test.dtypes != \"object\"].index\n",
    "features_skewed = train[features_numeric].apply(lambda x: skew(x.dropna()))\n",
    "features_skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "57bf5d17-6e97-c8a3-42a1-f56d202e1ca3"
   },
   "source": [
    "Some of them, yeah. We can fix that by taking their log-transforms as well, but log is sort of a blunt instrument. It's easily reversible, which makes it good for the outcome. But the Box-Cox transform is a better tool for modifying our inputs. Let's apply it to any features with a skew greater than, say, .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "375f0216-0940-b168-97b7-3b6daa07bcd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       -0.002155\n",
       "cont1     0.003913\n",
       "cont2    -0.310939\n",
       "cont3    -0.010002\n",
       "cont4     0.051768\n",
       "cont5     0.205930\n",
       "cont6     0.038875\n",
       "cont7     0.054278\n",
       "cont8     0.124674\n",
       "cont9    -0.012653\n",
       "cont10    0.020272\n",
       "cont11    0.023379\n",
       "cont12    0.024830\n",
       "cont13    0.093885\n",
       "cont14    0.067617\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_skewed = features_skewed[features_skewed > 0.2]\n",
    "for feat in features_skewed.index:\n",
    "    train[feat], lam = boxcox(train[feat] + 1)\n",
    "    test[feat] = boxcox(test[feat] + 1, lam)\n",
    "\n",
    "features_skewed = train[features_numeric].apply(lambda x: skew(x.dropna()))\n",
    "features_skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "378337bc-f12f-d45a-2279-403ea9acbcda"
   },
   "source": [
    "That eliminated much of the skewness. Before we move on, however, I'd like to call attention to the way we handle `lam` in the above block. We let `boxcox` figure out the optimal `lam` using our training data, and then force it to use that same `lam` on the test data, even if it isn't necessarily optimal for the test data. The alternative approach is to bind `train` and `test` together, perform these transformations on the entire set, and then split them back apart when it comes time to build models. I've opted not to for the benefit of clarity, but possibly at the cost of some small modeling advantage.\n",
    "\n",
    "Now, we have some categorical features we need to handle. The textbook approach to Linear Regression says you can leave categorical variables in, provided you do something like *[one-hot encode](https://en.wikipedia.org/wiki/One-hot)* them and leave out the smallest category. Personally, I prefer to replace the category with the arithmetic mean of its corresponding subset of outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "25edd603-a172-820c-ee84-98efd5dd2aed"
   },
   "outputs": [],
   "source": [
    "features_categorical = [feat for feat in test.columns if 'cat' in feat]\n",
    "\n",
    "for feat in features_categorical:\n",
    "    a = pd.DataFrame(train['log_loss'].groupby([train[feat]]).mean())\n",
    "    a[feat] = a.index\n",
    "    train[feat] = pd.merge(left=train, right=a, how='left', on=feat)['log_loss_y']\n",
    "    test[feat] = pd.merge(left=test, right=a, how='left', on=feat)['log_loss']\n",
    "\n",
    "features_categorical = test.dtypes[test.dtypes == \"object\"].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6e4398c0-74b4-8cdd-d3ae-fea624d587fd"
   },
   "source": [
    "There's just one more thing to check on. Linear Regression generally doesn't handle missing values very well. Let's see if we have any:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "8d73d9dd-dd87-b594-5945-18717da35d3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = train.count()\n",
    "len(counts[counts < train.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bda39254-3c42-4389-95f0-dd1b2b3850d2"
   },
   "source": [
    "Not in the training dataset. Let's check `test` now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "21e4d42c-e50b-2e05-ff60-7cd5f8a2b82a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = test.count()\n",
    "len(counts[counts < test.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "35bfb319-730b-5ca6-f603-38c0e3b3ed13"
   },
   "source": [
    "Rats. OK, Rather than design a elaborate solution, I'm just going to drop any columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "c2e245bb-9016-f41e-96e3-5ae823d8dfc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = test.dropna(1)\n",
    "counts = temp.count()\n",
    "len(counts[counts < temp.shape[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e82983e1-ae55-aa5f-ae75-cc72a3477f84"
   },
   "source": [
    "Cool. Now, we're ready to make a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "1e670242-680f-375c-4a2b-a3de1fd2f5b3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>log_loss</td>     <th>  R-squared:         </th>  <td>   0.506</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.506</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   1606.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 18 Oct 2016</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:45:49</td>     <th>  Log-Likelihood:    </th> <td>-1.6157e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>188318</td>      <th>  AIC:               </th>  <td>3.234e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>188197</td>      <th>  BIC:               </th>  <td>3.246e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>   120</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td> -129.6974</td> <td>   22.701</td> <td>   -5.713</td> <td> 0.000</td> <td> -174.191   -85.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>id</th>        <td> 3.358e-09</td> <td> 7.77e-09</td> <td>    0.432</td> <td> 0.666</td> <td>-1.19e-08  1.86e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat1</th>      <td>    0.2117</td> <td>    0.007</td> <td>   30.067</td> <td> 0.000</td> <td>    0.198     0.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat2</th>      <td>    0.8843</td> <td>    0.030</td> <td>   29.222</td> <td> 0.000</td> <td>    0.825     0.944</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat3</th>      <td>   -0.8232</td> <td>    0.454</td> <td>   -1.813</td> <td> 0.070</td> <td>   -1.713     0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat4</th>      <td>    0.0221</td> <td>    0.027</td> <td>    0.830</td> <td> 0.407</td> <td>   -0.030     0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat5</th>      <td>    0.1907</td> <td>    0.025</td> <td>    7.706</td> <td> 0.000</td> <td>    0.142     0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat6</th>      <td>   -0.3005</td> <td>    0.293</td> <td>   -1.026</td> <td> 0.305</td> <td>   -0.875     0.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat7</th>      <td>    0.1068</td> <td>    0.032</td> <td>    3.337</td> <td> 0.001</td> <td>    0.044     0.169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat8</th>      <td>    0.1646</td> <td>    0.103</td> <td>    1.592</td> <td> 0.111</td> <td>   -0.038     0.367</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat9</th>      <td>   -1.0021</td> <td>    0.048</td> <td>  -21.089</td> <td> 0.000</td> <td>   -1.095    -0.909</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat10</th>     <td>   -0.4859</td> <td>    0.028</td> <td>  -17.423</td> <td> 0.000</td> <td>   -0.541    -0.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat11</th>     <td>   -0.5264</td> <td>    0.028</td> <td>  -18.474</td> <td> 0.000</td> <td>   -0.582    -0.471</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat12</th>     <td>   -0.2022</td> <td>    0.025</td> <td>   -7.967</td> <td> 0.000</td> <td>   -0.252    -0.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat13</th>     <td>   -0.5359</td> <td>    0.029</td> <td>  -18.568</td> <td> 0.000</td> <td>   -0.592    -0.479</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat14</th>     <td>   -0.5296</td> <td>    0.047</td> <td>  -11.203</td> <td> 0.000</td> <td>   -0.622    -0.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat15</th>     <td>   -2.4210</td> <td>    0.470</td> <td>   -5.156</td> <td> 0.000</td> <td>   -3.341    -1.501</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat16</th>     <td>   -0.2379</td> <td>    0.074</td> <td>   -3.201</td> <td> 0.001</td> <td>   -0.384    -0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat17</th>     <td>   -0.2920</td> <td>    0.094</td> <td>   -3.103</td> <td> 0.002</td> <td>   -0.476    -0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat18</th>     <td>   -0.4254</td> <td>    0.135</td> <td>   -3.159</td> <td> 0.002</td> <td>   -0.689    -0.161</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat19</th>     <td>   -0.2509</td> <td>    0.202</td> <td>   -1.242</td> <td> 0.214</td> <td>   -0.647     0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat20</th>     <td>   -0.2420</td> <td>    0.095</td> <td>   -2.548</td> <td> 0.011</td> <td>   -0.428    -0.056</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat21</th>     <td>   -2.3928</td> <td>    0.544</td> <td>   -4.398</td> <td> 0.000</td> <td>   -3.459    -1.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat22</th>     <td>   -1.3080</td> <td>    0.334</td> <td>   -3.914</td> <td> 0.000</td> <td>   -1.963    -0.653</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat23</th>     <td>    0.2610</td> <td>    0.015</td> <td>   17.785</td> <td> 0.000</td> <td>    0.232     0.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat24</th>     <td>    0.0569</td> <td>    0.020</td> <td>    2.910</td> <td> 0.004</td> <td>    0.019     0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat25</th>     <td>    0.5953</td> <td>    0.019</td> <td>   31.774</td> <td> 0.000</td> <td>    0.559     0.632</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat26</th>     <td>    1.0373</td> <td>    0.027</td> <td>   38.356</td> <td> 0.000</td> <td>    0.984     1.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat27</th>     <td>    4.5747</td> <td>    0.159</td> <td>   28.820</td> <td> 0.000</td> <td>    4.264     4.886</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat28</th>     <td>    0.0926</td> <td>    0.016</td> <td>    5.971</td> <td> 0.000</td> <td>    0.062     0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat29</th>     <td>    0.2311</td> <td>    0.023</td> <td>    9.900</td> <td> 0.000</td> <td>    0.185     0.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat30</th>     <td>    0.1322</td> <td>    0.037</td> <td>    3.606</td> <td> 0.000</td> <td>    0.060     0.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat31</th>     <td>   -0.4536</td> <td>    0.151</td> <td>   -3.008</td> <td> 0.003</td> <td>   -0.749    -0.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat32</th>     <td>    0.6868</td> <td>    0.058</td> <td>   11.933</td> <td> 0.000</td> <td>    0.574     0.800</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat33</th>     <td>    0.1161</td> <td>    0.063</td> <td>    1.847</td> <td> 0.065</td> <td>   -0.007     0.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat34</th>     <td>    0.3307</td> <td>    0.067</td> <td>    4.972</td> <td> 0.000</td> <td>    0.200     0.461</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat35</th>     <td>    0.3542</td> <td>    0.074</td> <td>    4.770</td> <td> 0.000</td> <td>    0.209     0.500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat36</th>     <td>    0.2171</td> <td>    0.015</td> <td>   14.871</td> <td> 0.000</td> <td>    0.188     0.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat37</th>     <td>    2.4094</td> <td>    0.088</td> <td>   27.228</td> <td> 0.000</td> <td>    2.236     2.583</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat38</th>     <td>    0.6092</td> <td>    0.017</td> <td>   36.078</td> <td> 0.000</td> <td>    0.576     0.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat39</th>     <td>   -0.2229</td> <td>    0.098</td> <td>   -2.285</td> <td> 0.022</td> <td>   -0.414    -0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat40</th>     <td>    0.0609</td> <td>    0.016</td> <td>    3.816</td> <td> 0.000</td> <td>    0.030     0.092</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat41</th>     <td>    0.0072</td> <td>    0.021</td> <td>    0.343</td> <td> 0.731</td> <td>   -0.034     0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat42</th>     <td>    0.8393</td> <td>    0.061</td> <td>   13.733</td> <td> 0.000</td> <td>    0.719     0.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat43</th>     <td>    0.1480</td> <td>    0.036</td> <td>    4.159</td> <td> 0.000</td> <td>    0.078     0.218</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat44</th>     <td>    1.1577</td> <td>    0.025</td> <td>   45.747</td> <td> 0.000</td> <td>    1.108     1.207</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat45</th>     <td>    0.0969</td> <td>    0.025</td> <td>    3.936</td> <td> 0.000</td> <td>    0.049     0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat46</th>     <td>    0.2195</td> <td>    0.053</td> <td>    4.111</td> <td> 0.000</td> <td>    0.115     0.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat47</th>     <td>    0.2977</td> <td>    0.064</td> <td>    4.617</td> <td> 0.000</td> <td>    0.171     0.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat48</th>     <td>    0.3557</td> <td>    0.081</td> <td>    4.395</td> <td> 0.000</td> <td>    0.197     0.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat49</th>     <td>    1.0186</td> <td>    0.328</td> <td>    3.107</td> <td> 0.002</td> <td>    0.376     1.661</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat50</th>     <td>   -0.0918</td> <td>    0.118</td> <td>   -0.775</td> <td> 0.438</td> <td>   -0.324     0.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat51</th>     <td>    0.4165</td> <td>    0.118</td> <td>    3.520</td> <td> 0.000</td> <td>    0.185     0.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat52</th>     <td>    0.6384</td> <td>    0.289</td> <td>    2.208</td> <td> 0.027</td> <td>    0.072     1.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat53</th>     <td>    1.4976</td> <td>    0.174</td> <td>    8.586</td> <td> 0.000</td> <td>    1.156     1.839</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat54</th>     <td>   -0.5682</td> <td>    1.043</td> <td>   -0.545</td> <td> 0.586</td> <td>   -2.612     1.475</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat55</th>     <td>    0.1054</td> <td>    0.117</td> <td>    0.901</td> <td> 0.367</td> <td>   -0.124     0.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat56</th>     <td>    0.3130</td> <td>    0.134</td> <td>    2.338</td> <td> 0.019</td> <td>    0.051     0.575</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat57</th>     <td>    0.2879</td> <td>    0.024</td> <td>   11.822</td> <td> 0.000</td> <td>    0.240     0.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat58</th>     <td>   -0.0550</td> <td>    0.059</td> <td>   -0.939</td> <td> 0.348</td> <td>   -0.170     0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat59</th>     <td>    0.0648</td> <td>    0.055</td> <td>    1.176</td> <td> 0.240</td> <td>   -0.043     0.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat60</th>     <td>   -0.2207</td> <td>    0.136</td> <td>   -1.622</td> <td> 0.105</td> <td>   -0.487     0.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat61</th>     <td>   -0.2670</td> <td>    0.085</td> <td>   -3.155</td> <td> 0.002</td> <td>   -0.433    -0.101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat62</th>     <td>    0.0218</td> <td>    0.130</td> <td>    0.168</td> <td> 0.867</td> <td>   -0.232     0.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat63</th>     <td>   -1.0247</td> <td>    0.314</td> <td>   -3.262</td> <td> 0.001</td> <td>   -1.640    -0.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat64</th>     <td>    0.1304</td> <td>    0.104</td> <td>    1.257</td> <td> 0.209</td> <td>   -0.073     0.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat65</th>     <td>    0.0560</td> <td>    0.059</td> <td>    0.941</td> <td> 0.346</td> <td>   -0.061     0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat66</th>     <td>   -0.0370</td> <td>    0.108</td> <td>   -0.341</td> <td> 0.733</td> <td>   -0.250     0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat67</th>     <td>    0.1944</td> <td>    0.070</td> <td>    2.758</td> <td> 0.006</td> <td>    0.056     0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat68</th>     <td>   -0.3142</td> <td>    0.235</td> <td>   -1.338</td> <td> 0.181</td> <td>   -0.774     0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat69</th>     <td>   -0.0396</td> <td>    0.101</td> <td>   -0.392</td> <td> 0.695</td> <td>   -0.237     0.158</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat70</th>     <td>    1.4320</td> <td>    1.463</td> <td>    0.979</td> <td> 0.328</td> <td>   -1.435     4.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat71</th>     <td>    0.2961</td> <td>    0.035</td> <td>    8.531</td> <td> 0.000</td> <td>    0.228     0.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat72</th>     <td>    0.1881</td> <td>    0.007</td> <td>   25.287</td> <td> 0.000</td> <td>    0.173     0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat73</th>     <td>    0.1240</td> <td>    0.010</td> <td>   12.273</td> <td> 0.000</td> <td>    0.104     0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat74</th>     <td>    0.2358</td> <td>    0.066</td> <td>    3.558</td> <td> 0.000</td> <td>    0.106     0.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat75</th>     <td>    0.2967</td> <td>    0.033</td> <td>    8.928</td> <td> 0.000</td> <td>    0.232     0.362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat76</th>     <td>    0.0193</td> <td>    0.050</td> <td>    0.383</td> <td> 0.702</td> <td>   -0.080     0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat77</th>     <td>   -0.0570</td> <td>    0.062</td> <td>   -0.915</td> <td> 0.360</td> <td>   -0.179     0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat78</th>     <td>   -0.0709</td> <td>    0.032</td> <td>   -2.219</td> <td> 0.027</td> <td>   -0.134    -0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat79</th>     <td>    0.2675</td> <td>    0.005</td> <td>   51.832</td> <td> 0.000</td> <td>    0.257     0.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat80</th>     <td>    0.3771</td> <td>    0.005</td> <td>   76.928</td> <td> 0.000</td> <td>    0.368     0.387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat81</th>     <td>    0.2771</td> <td>    0.006</td> <td>   43.526</td> <td> 0.000</td> <td>    0.265     0.290</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat82</th>     <td>    0.2792</td> <td>    0.014</td> <td>   20.420</td> <td> 0.000</td> <td>    0.252     0.306</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat83</th>     <td>    0.5160</td> <td>    0.046</td> <td>   11.317</td> <td> 0.000</td> <td>    0.427     0.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat84</th>     <td>   -0.1890</td> <td>    0.034</td> <td>   -5.547</td> <td> 0.000</td> <td>   -0.256    -0.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat85</th>     <td>    0.0191</td> <td>    0.023</td> <td>    0.847</td> <td> 0.397</td> <td>   -0.025     0.063</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat86</th>     <td>   -0.1223</td> <td>    0.067</td> <td>   -1.823</td> <td> 0.068</td> <td>   -0.254     0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat87</th>     <td>    0.0992</td> <td>    0.006</td> <td>   15.630</td> <td> 0.000</td> <td>    0.087     0.112</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat88</th>     <td>    0.9344</td> <td>    0.169</td> <td>    5.517</td> <td> 0.000</td> <td>    0.602     1.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat90</th>     <td>    1.2106</td> <td>    0.547</td> <td>    2.214</td> <td> 0.027</td> <td>    0.139     2.282</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat91</th>     <td>    0.1219</td> <td>    0.012</td> <td>    9.807</td> <td> 0.000</td> <td>    0.098     0.146</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat93</th>     <td>    0.5070</td> <td>    0.044</td> <td>   11.452</td> <td> 0.000</td> <td>    0.420     0.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat94</th>     <td>    0.2877</td> <td>    0.027</td> <td>   10.818</td> <td> 0.000</td> <td>    0.236     0.340</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat95</th>     <td>    0.0700</td> <td>    0.061</td> <td>    1.140</td> <td> 0.254</td> <td>   -0.050     0.190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat97</th>     <td>    0.0295</td> <td>    0.142</td> <td>    0.207</td> <td> 0.836</td> <td>   -0.249     0.308</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat98</th>     <td>   -0.2854</td> <td>    0.077</td> <td>   -3.722</td> <td> 0.000</td> <td>   -0.436    -0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat100</th>    <td>    0.3527</td> <td>    0.006</td> <td>   59.883</td> <td> 0.000</td> <td>    0.341     0.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat101</th>    <td>    1.6064</td> <td>    0.065</td> <td>   24.601</td> <td> 0.000</td> <td>    1.478     1.734</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat102</th>    <td>   -0.0090</td> <td>    0.145</td> <td>   -0.062</td> <td> 0.951</td> <td>   -0.294     0.276</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat104</th>    <td>    0.3902</td> <td>    0.037</td> <td>   10.426</td> <td> 0.000</td> <td>    0.317     0.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat105</th>    <td>    0.4083</td> <td>    0.032</td> <td>   12.856</td> <td> 0.000</td> <td>    0.346     0.471</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat107</th>    <td>    0.2204</td> <td>    0.040</td> <td>    5.497</td> <td> 0.000</td> <td>    0.142     0.299</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat108</th>    <td>    0.6055</td> <td>    0.029</td> <td>   20.619</td> <td> 0.000</td> <td>    0.548     0.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat112</th>    <td>    0.6205</td> <td>    0.011</td> <td>   58.315</td> <td> 0.000</td> <td>    0.600     0.641</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat114</th>    <td>    0.3156</td> <td>    0.108</td> <td>    2.932</td> <td> 0.003</td> <td>    0.105     0.527</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cat115</th>    <td>    0.4888</td> <td>    0.050</td> <td>    9.860</td> <td> 0.000</td> <td>    0.392     0.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont1</th>     <td>   -0.2835</td> <td>    0.047</td> <td>   -6.036</td> <td> 0.000</td> <td>   -0.376    -0.191</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont2</th>     <td>    0.2355</td> <td>    0.010</td> <td>   24.429</td> <td> 0.000</td> <td>    0.217     0.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont3</th>     <td>    0.0463</td> <td>    0.011</td> <td>    4.045</td> <td> 0.000</td> <td>    0.024     0.069</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont4</th>     <td>   -0.2413</td> <td>    0.021</td> <td>  -11.361</td> <td> 0.000</td> <td>   -0.283    -0.200</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont5</th>     <td>    0.2051</td> <td>    0.037</td> <td>    5.531</td> <td> 0.000</td> <td>    0.132     0.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont6</th>     <td>    0.2660</td> <td>    0.073</td> <td>    3.653</td> <td> 0.000</td> <td>    0.123     0.409</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont7</th>     <td>    0.9443</td> <td>    0.079</td> <td>   12.016</td> <td> 0.000</td> <td>    0.790     1.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont8</th>     <td>    0.3370</td> <td>    0.038</td> <td>    8.937</td> <td> 0.000</td> <td>    0.263     0.411</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont9</th>     <td>    0.5186</td> <td>    0.109</td> <td>    4.768</td> <td> 0.000</td> <td>    0.305     0.732</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont10</th>    <td>    0.0202</td> <td>    0.038</td> <td>    0.531</td> <td> 0.595</td> <td>   -0.054     0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont11</th>    <td>   -0.2251</td> <td>    0.121</td> <td>   -1.864</td> <td> 0.062</td> <td>   -0.462     0.012</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont12</th>    <td>    0.1091</td> <td>    0.127</td> <td>    0.856</td> <td> 0.392</td> <td>   -0.141     0.359</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont13</th>    <td>   -0.1589</td> <td>    0.042</td> <td>   -3.797</td> <td> 0.000</td> <td>   -0.241    -0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>cont14</th>    <td>    0.2560</td> <td>    0.013</td> <td>   19.566</td> <td> 0.000</td> <td>    0.230     0.282</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>7399.572</td> <th>  Durbin-Watson:     </th> <td>   1.996</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>16979.984</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>-0.240</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 4.390</td>  <th>  Cond. No.          </th> <td>5.87e+09</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               log_loss   R-squared:                       0.506\n",
       "Model:                            OLS   Adj. R-squared:                  0.506\n",
       "Method:                 Least Squares   F-statistic:                     1606.\n",
       "Date:                Tue, 18 Oct 2016   Prob (F-statistic):               0.00\n",
       "Time:                        20:45:49   Log-Likelihood:            -1.6157e+05\n",
       "No. Observations:              188318   AIC:                         3.234e+05\n",
       "Df Residuals:                  188197   BIC:                         3.246e+05\n",
       "Df Model:                         120                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept   -129.6974     22.701     -5.713      0.000      -174.191   -85.203\n",
       "id          3.358e-09   7.77e-09      0.432      0.666     -1.19e-08  1.86e-08\n",
       "cat1           0.2117      0.007     30.067      0.000         0.198     0.225\n",
       "cat2           0.8843      0.030     29.222      0.000         0.825     0.944\n",
       "cat3          -0.8232      0.454     -1.813      0.070        -1.713     0.067\n",
       "cat4           0.0221      0.027      0.830      0.407        -0.030     0.074\n",
       "cat5           0.1907      0.025      7.706      0.000         0.142     0.239\n",
       "cat6          -0.3005      0.293     -1.026      0.305        -0.875     0.274\n",
       "cat7           0.1068      0.032      3.337      0.001         0.044     0.169\n",
       "cat8           0.1646      0.103      1.592      0.111        -0.038     0.367\n",
       "cat9          -1.0021      0.048    -21.089      0.000        -1.095    -0.909\n",
       "cat10         -0.4859      0.028    -17.423      0.000        -0.541    -0.431\n",
       "cat11         -0.5264      0.028    -18.474      0.000        -0.582    -0.471\n",
       "cat12         -0.2022      0.025     -7.967      0.000        -0.252    -0.152\n",
       "cat13         -0.5359      0.029    -18.568      0.000        -0.592    -0.479\n",
       "cat14         -0.5296      0.047    -11.203      0.000        -0.622    -0.437\n",
       "cat15         -2.4210      0.470     -5.156      0.000        -3.341    -1.501\n",
       "cat16         -0.2379      0.074     -3.201      0.001        -0.384    -0.092\n",
       "cat17         -0.2920      0.094     -3.103      0.002        -0.476    -0.108\n",
       "cat18         -0.4254      0.135     -3.159      0.002        -0.689    -0.161\n",
       "cat19         -0.2509      0.202     -1.242      0.214        -0.647     0.145\n",
       "cat20         -0.2420      0.095     -2.548      0.011        -0.428    -0.056\n",
       "cat21         -2.3928      0.544     -4.398      0.000        -3.459    -1.326\n",
       "cat22         -1.3080      0.334     -3.914      0.000        -1.963    -0.653\n",
       "cat23          0.2610      0.015     17.785      0.000         0.232     0.290\n",
       "cat24          0.0569      0.020      2.910      0.004         0.019     0.095\n",
       "cat25          0.5953      0.019     31.774      0.000         0.559     0.632\n",
       "cat26          1.0373      0.027     38.356      0.000         0.984     1.090\n",
       "cat27          4.5747      0.159     28.820      0.000         4.264     4.886\n",
       "cat28          0.0926      0.016      5.971      0.000         0.062     0.123\n",
       "cat29          0.2311      0.023      9.900      0.000         0.185     0.277\n",
       "cat30          0.1322      0.037      3.606      0.000         0.060     0.204\n",
       "cat31         -0.4536      0.151     -3.008      0.003        -0.749    -0.158\n",
       "cat32          0.6868      0.058     11.933      0.000         0.574     0.800\n",
       "cat33          0.1161      0.063      1.847      0.065        -0.007     0.239\n",
       "cat34          0.3307      0.067      4.972      0.000         0.200     0.461\n",
       "cat35          0.3542      0.074      4.770      0.000         0.209     0.500\n",
       "cat36          0.2171      0.015     14.871      0.000         0.188     0.246\n",
       "cat37          2.4094      0.088     27.228      0.000         2.236     2.583\n",
       "cat38          0.6092      0.017     36.078      0.000         0.576     0.642\n",
       "cat39         -0.2229      0.098     -2.285      0.022        -0.414    -0.032\n",
       "cat40          0.0609      0.016      3.816      0.000         0.030     0.092\n",
       "cat41          0.0072      0.021      0.343      0.731        -0.034     0.048\n",
       "cat42          0.8393      0.061     13.733      0.000         0.719     0.959\n",
       "cat43          0.1480      0.036      4.159      0.000         0.078     0.218\n",
       "cat44          1.1577      0.025     45.747      0.000         1.108     1.207\n",
       "cat45          0.0969      0.025      3.936      0.000         0.049     0.145\n",
       "cat46          0.2195      0.053      4.111      0.000         0.115     0.324\n",
       "cat47          0.2977      0.064      4.617      0.000         0.171     0.424\n",
       "cat48          0.3557      0.081      4.395      0.000         0.197     0.514\n",
       "cat49          1.0186      0.328      3.107      0.002         0.376     1.661\n",
       "cat50         -0.0918      0.118     -0.775      0.438        -0.324     0.140\n",
       "cat51          0.4165      0.118      3.520      0.000         0.185     0.648\n",
       "cat52          0.6384      0.289      2.208      0.027         0.072     1.205\n",
       "cat53          1.4976      0.174      8.586      0.000         1.156     1.839\n",
       "cat54         -0.5682      1.043     -0.545      0.586        -2.612     1.475\n",
       "cat55          0.1054      0.117      0.901      0.367        -0.124     0.335\n",
       "cat56          0.3130      0.134      2.338      0.019         0.051     0.575\n",
       "cat57          0.2879      0.024     11.822      0.000         0.240     0.336\n",
       "cat58         -0.0550      0.059     -0.939      0.348        -0.170     0.060\n",
       "cat59          0.0648      0.055      1.176      0.240        -0.043     0.173\n",
       "cat60         -0.2207      0.136     -1.622      0.105        -0.487     0.046\n",
       "cat61         -0.2670      0.085     -3.155      0.002        -0.433    -0.101\n",
       "cat62          0.0218      0.130      0.168      0.867        -0.232     0.276\n",
       "cat63         -1.0247      0.314     -3.262      0.001        -1.640    -0.409\n",
       "cat64          0.1304      0.104      1.257      0.209        -0.073     0.334\n",
       "cat65          0.0560      0.059      0.941      0.346        -0.061     0.172\n",
       "cat66         -0.0370      0.108     -0.341      0.733        -0.250     0.176\n",
       "cat67          0.1944      0.070      2.758      0.006         0.056     0.333\n",
       "cat68         -0.3142      0.235     -1.338      0.181        -0.774     0.146\n",
       "cat69         -0.0396      0.101     -0.392      0.695        -0.237     0.158\n",
       "cat70          1.4320      1.463      0.979      0.328        -1.435     4.299\n",
       "cat71          0.2961      0.035      8.531      0.000         0.228     0.364\n",
       "cat72          0.1881      0.007     25.287      0.000         0.173     0.203\n",
       "cat73          0.1240      0.010     12.273      0.000         0.104     0.144\n",
       "cat74          0.2358      0.066      3.558      0.000         0.106     0.366\n",
       "cat75          0.2967      0.033      8.928      0.000         0.232     0.362\n",
       "cat76          0.0193      0.050      0.383      0.702        -0.080     0.118\n",
       "cat77         -0.0570      0.062     -0.915      0.360        -0.179     0.065\n",
       "cat78         -0.0709      0.032     -2.219      0.027        -0.134    -0.008\n",
       "cat79          0.2675      0.005     51.832      0.000         0.257     0.278\n",
       "cat80          0.3771      0.005     76.928      0.000         0.368     0.387\n",
       "cat81          0.2771      0.006     43.526      0.000         0.265     0.290\n",
       "cat82          0.2792      0.014     20.420      0.000         0.252     0.306\n",
       "cat83          0.5160      0.046     11.317      0.000         0.427     0.605\n",
       "cat84         -0.1890      0.034     -5.547      0.000        -0.256    -0.122\n",
       "cat85          0.0191      0.023      0.847      0.397        -0.025     0.063\n",
       "cat86         -0.1223      0.067     -1.823      0.068        -0.254     0.009\n",
       "cat87          0.0992      0.006     15.630      0.000         0.087     0.112\n",
       "cat88          0.9344      0.169      5.517      0.000         0.602     1.266\n",
       "cat90          1.2106      0.547      2.214      0.027         0.139     2.282\n",
       "cat91          0.1219      0.012      9.807      0.000         0.098     0.146\n",
       "cat93          0.5070      0.044     11.452      0.000         0.420     0.594\n",
       "cat94          0.2877      0.027     10.818      0.000         0.236     0.340\n",
       "cat95          0.0700      0.061      1.140      0.254        -0.050     0.190\n",
       "cat97          0.0295      0.142      0.207      0.836        -0.249     0.308\n",
       "cat98         -0.2854      0.077     -3.722      0.000        -0.436    -0.135\n",
       "cat100         0.3527      0.006     59.883      0.000         0.341     0.364\n",
       "cat101         1.6064      0.065     24.601      0.000         1.478     1.734\n",
       "cat102        -0.0090      0.145     -0.062      0.951        -0.294     0.276\n",
       "cat104         0.3902      0.037     10.426      0.000         0.317     0.464\n",
       "cat105         0.4083      0.032     12.856      0.000         0.346     0.471\n",
       "cat107         0.2204      0.040      5.497      0.000         0.142     0.299\n",
       "cat108         0.6055      0.029     20.619      0.000         0.548     0.663\n",
       "cat112         0.6205      0.011     58.315      0.000         0.600     0.641\n",
       "cat114         0.3156      0.108      2.932      0.003         0.105     0.527\n",
       "cat115         0.4888      0.050      9.860      0.000         0.392     0.586\n",
       "cont1         -0.2835      0.047     -6.036      0.000        -0.376    -0.191\n",
       "cont2          0.2355      0.010     24.429      0.000         0.217     0.254\n",
       "cont3          0.0463      0.011      4.045      0.000         0.024     0.069\n",
       "cont4         -0.2413      0.021    -11.361      0.000        -0.283    -0.200\n",
       "cont5          0.2051      0.037      5.531      0.000         0.132     0.278\n",
       "cont6          0.2660      0.073      3.653      0.000         0.123     0.409\n",
       "cont7          0.9443      0.079     12.016      0.000         0.790     1.098\n",
       "cont8          0.3370      0.038      8.937      0.000         0.263     0.411\n",
       "cont9          0.5186      0.109      4.768      0.000         0.305     0.732\n",
       "cont10         0.0202      0.038      0.531      0.595        -0.054     0.095\n",
       "cont11        -0.2251      0.121     -1.864      0.062        -0.462     0.012\n",
       "cont12         0.1091      0.127      0.856      0.392        -0.141     0.359\n",
       "cont13        -0.1589      0.042     -3.797      0.000        -0.241    -0.077\n",
       "cont14         0.2560      0.013     19.566      0.000         0.230     0.282\n",
       "==============================================================================\n",
       "Omnibus:                     7399.572   Durbin-Watson:                   1.996\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            16979.984\n",
       "Skew:                          -0.240   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.390   Cond. No.                     5.87e+09\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.87e+09. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smf.ols('log_loss ~ ' + ' + '.join(temp.columns), data=train).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "69d2010b-9035-df6c-7480-270196d8b9a0"
   },
   "source": [
    "There's a lot of useful information here. However, since this is a prediction challenge, I'm not interested in most of it. Instead, I'm interested in how well it can predict new values. To do that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "f7bb6f3e-2dd1-1d23-a1e9-6321bcd61726"
   },
   "outputs": [],
   "source": [
    "yhat = np.exp(model.predict(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2410310a-4e40-f587-489c-ff03d67548ba"
   },
   "source": [
    "Note that we call `np.exp` on our model predictions. Remember how we log-transformed 'loss' up at the beginning of this script? Exponentiating the outcome sort of undoes that, so our predictions will be on the same scale as 'loss' instead of 'log_loss'. Forgetting this step is a really good way to get a terrible score.\n",
    "\n",
    "Now that we have some predictions, let's write them out and score them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "8a63fe18-2219-4487-9647-d2168e214764"
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'id': test['id'].values, 'loss': yhat})\n",
    "result = result.set_index('id')\n",
    "result.to_csv('simplelmprediction.csv', index=True, index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "16e05916-d708-cee5-086e-d0ba9188ebaa"
   },
   "source": [
    "If you submit that, it should give you a score something like 1245.99. That's a bit worse than the Random Forest Benchmark (which isn't surprising). Onward to greater refinements!\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "32b889da-8f69-a396-1714-64b9fc258bc9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 41,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
