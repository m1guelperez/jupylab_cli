{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The SchNet architecture was published in the paper \"SchNet: A continuous-filter convolutional neural network for modeling quantum interactions\".\n",
    "https://arxiv.org/abs/1706.08566\n",
    "\n",
    "This implementation uses the DGL (deep graph libary - which is based on pytorch) SchNet implementation as a basis. It is not part of the standard library but can be found in the folder 'examples' on the DGL github page.  \n",
    "Of course, the original implementation was not built to predict the interaction between two nodes (atoms) so it had to be extended to inlcude a regression for the interaction between a number of given atom-pairs.\n",
    "\n",
    "### Template:\n",
    "\n",
    "https://www.kaggle.com/toshik/schnet-starter-kit\n",
    "\n",
    "I the modified it for the task at hand using the awesome SchNet from this kernel: https://www.kaggle.com/toshik/schnet-starter-kit as a guide. (Which was incredibly useful and thought me a lot - many thanks to the author for sharing it!). It's implemented in chainer but if you're used to pytorch, you can more or less read the code without knowing chainer.\n",
    "\n",
    "Then, I made a couple of changes described below.\n",
    "\n",
    "### Changes compared to the chainer-implementation\n",
    "\n",
    "* **A separate model is trained for each j-coupling type: 1JHN, 1JHC, etc.**  \n",
    "  That's an easier task than to fit a model that works well for all j-coupling types although the latter would be more elegant.\n",
    "\n",
    "* **Dense-shortcuts instead of residual shortcuts in the Interaction-layers.**  \n",
    "  The original SchNet implementation uses residual shortcuts as described in the ResNet-paper (https://arxiv.org/abs/1512.03385). Toshi's chainer implementation keeps these shortcutes and adds a batch-normalization.  \n",
    "  Here, I also added a BN like in the Toshi's chainer implementation but replaced the residual shortcuts with dense shortcuts (concatenation instead of addition) as described in the DenseNet-paper (https://arxiv.org/abs/1608.06993).\n",
    "  \n",
    "* **Addition of a graph-state.**  \n",
    "  This implementation also sums up all the node hidden-states and puts them through a small MLP. This gives a bunch of features describing the molecule as a whole. These features are than added to the concatenation of the j-coupled nodes(atoms) which is then fed into the final regression. The idea is that this path could learn some attributes of molecules as a whole and improve the predictions.\n",
    "  \n",
    "* The other architectural changes are mostly a consequence of the changes described above.\n",
    "\n",
    "(None of these changes fully account for for the lower performance mentioned below.)\n",
    "  \n",
    " \n",
    "\n",
    "### Performance\n",
    "\n",
    "While the net optimizes, the performance is a good bit  below the chainer-implementation linked above and I couldn't figure out, why exactly.\n",
    "\n",
    "So no competitive score but a great learning experience building this model and experimenting :-)\n",
    "\n",
    "### GPU\n",
    "\n",
    "As the GPU-version of DGL can't be installed in a kernel (or at least I didn't manage to), this kernel uses only CPU. But you can download it and run it with the GPU-version of DGL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dgl\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/89/af06ae8fcc4529bd660017f71848a7ad14523c0568c9c87b50bbfe10dccc/dgl-0.3.1-cp36-cp36m-manylinux1_x86_64.whl (1.9MB)\r\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 5.0MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /opt/conda/lib/python3.6/site-packages (from dgl) (2.3)\r\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.6/site-packages (from dgl) (1.17.0)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from dgl) (1.2.1)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.1->dgl) (4.4.0)\r\n",
      "Installing collected packages: dgl\r\n",
      "Successfully installed dgl-0.3.1\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "try:\n",
    "    import dgl\n",
    "except:\n",
    "    !pip install dgl\n",
    "    import dgl\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because DGL could not be installed in a utility-script, the layers needed for this architecture have to be in this kernel. The next cell contains the layers.py script from the DGL-github page. No modifications have been made in this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "layers-script from dgl-schnet implementation:\n",
    "https://github.com/dmlc/dgl/blob/master/examples/pytorch/schnet/layers.py\n",
    "\"\"\"\n",
    "\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import dgl.function as fn\n",
    "from torch.nn import Softplus\n",
    "\n",
    "\n",
    "class AtomEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the atom(node) list to atom embeddings.\n",
    "    The atom with the same element share the same initial embeddding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=128, type_num=100, pre_train=None):\n",
    "        \"\"\"\n",
    "        Randomly init the element embeddings.\n",
    "        Args:\n",
    "            dim: the dim of embeddings\n",
    "            type_num: the largest atomic number of atoms in the dataset\n",
    "            pre_train: the pre_trained embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._dim = dim\n",
    "        self._type_num = type_num\n",
    "        if pre_train is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pre_train,\n",
    "                                                          padding_idx=0)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(type_num, dim, padding_idx=0)\n",
    "\n",
    "    def forward(self, g, p_name=\"node\"):\n",
    "        \"\"\"Input type is dgl graph\"\"\"\n",
    "        atom_list = g.ndata[\"node_type\"]\n",
    "        g.ndata[p_name] = self.embedding(atom_list)\n",
    "        return g.ndata[p_name]\n",
    "\n",
    "\n",
    "class EdgeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the edge to embedding.\n",
    "    The edge links same pair of atoms share the same initial embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim=128, edge_num=3000, pre_train=None):\n",
    "        \"\"\"\n",
    "        Randomly init the edge embeddings.\n",
    "        Args:\n",
    "            dim: the dim of embeddings\n",
    "            edge_num: the maximum type of edges\n",
    "            pre_train: the pre_trained embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._dim = dim\n",
    "        self._edge_num = edge_num\n",
    "        if pre_train is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(pre_train,\n",
    "                                                          padding_idx=0)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(edge_num, dim, padding_idx=0)\n",
    "\n",
    "    def generate_edge_type(self, edges):\n",
    "        \"\"\"\n",
    "        Generate the edge type based on the src&dst atom type of the edge.\n",
    "        Note that C-O and O-C are the same edge type.\n",
    "        To map a pair of nodes to one number, we use an unordered pairing function here\n",
    "        See more detail in this disscussion:\n",
    "        https://math.stackexchange.com/questions/23503/create-unique-number-from-2-numbers\n",
    "        Note that, the edge_num should larger than the square of maximum atomic number\n",
    "        in the dataset.\n",
    "        \"\"\"\n",
    "        atom_type_x = edges.src[\"node_type\"]\n",
    "        atom_type_y = edges.dst[\"node_type\"]\n",
    "\n",
    "        return {\n",
    "            \"type\":\n",
    "            atom_type_x * atom_type_y +\n",
    "            (th.abs(atom_type_x - atom_type_y) - 1)**2 / 4\n",
    "        }\n",
    "\n",
    "    def forward(self, g, p_name=\"edge_f\"):\n",
    "        g.apply_edges(self.generate_edge_type)\n",
    "        g.edata[p_name] = self.embedding(g.edata[\"type\"])\n",
    "        return g.edata[p_name]\n",
    "\n",
    "\n",
    "class ShiftSoftplus(Softplus):\n",
    "    \"\"\"\n",
    "    Shiftsoft plus activation function:\n",
    "        1/beta * (log(1 + exp**(beta * x)) - log(shift))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta=1, shift=2, threshold=20):\n",
    "        super().__init__(beta, threshold)\n",
    "        self.shift = shift\n",
    "        self.softplus = Softplus(beta, threshold)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.softplus(input) - np.log(float(self.shift))\n",
    "\n",
    "\n",
    "class RBFLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Radial basis functions Layer.\n",
    "    e(d) = exp(- gamma * ||d - mu_k||^2)\n",
    "    default settings:\n",
    "        gamma = 10\n",
    "        0 <= mu_k <= 30 for k=1~300\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, low=0, high=30, gap=0.1, dim=1):\n",
    "        super().__init__()\n",
    "        self._low = low\n",
    "        self._high = high\n",
    "        self._gap = gap\n",
    "        self._dim = dim\n",
    "\n",
    "        self._n_centers = int(np.ceil((high - low) / gap))\n",
    "        centers = np.linspace(low, high, self._n_centers)\n",
    "        self.centers = th.tensor(centers, dtype=th.float, requires_grad=False)\n",
    "        self.centers = nn.Parameter(self.centers, requires_grad=False)\n",
    "        self._fan_out = self._dim * self._n_centers\n",
    "\n",
    "        self._gap = centers[1] - centers[0]\n",
    "\n",
    "    def dis2rbf(self, edges):        \n",
    "        dist = edges.data[\"distance\"]\n",
    "        radial = dist - self.centers\n",
    "        coef = -1 / self._gap\n",
    "        rbf = th.exp(coef * (radial**2))\n",
    "        return {\"rbf\": rbf}\n",
    "\n",
    "    def forward(self, g):\n",
    "        \"\"\"Convert distance scalar to rbf vector\"\"\"\n",
    "        g.apply_edges(self.dis2rbf)\n",
    "        return g.edata[\"rbf\"]\n",
    "\n",
    "\n",
    "class CFConv(nn.Module):\n",
    "    \"\"\"\n",
    "    The continuous-filter convolution layer in SchNet.\n",
    "    One CFConv contains one rbf layer and three linear layer\n",
    "        (two of them have activation funct).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rbf_dim, dim=64, act=\"sp\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rbf_dim: the dimsion of the RBF layer\n",
    "            dim: the dimension of linear layers\n",
    "            act: activation function (default shifted softplus)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._rbf_dim = rbf_dim\n",
    "        self._dim = dim\n",
    "\n",
    "        self.linear_layer1 = nn.Linear(self._rbf_dim, self._dim)\n",
    "        self.linear_layer2 = nn.Linear(self._dim, self._dim)\n",
    "\n",
    "        if act == \"sp\":\n",
    "            self.activation = nn.Softplus(beta=0.5, threshold=14)\n",
    "        else:\n",
    "            self.activation = act\n",
    "\n",
    "    def update_edge(self, edges):\n",
    "        rbf = edges.data[\"rbf\"]\n",
    "        h = self.linear_layer1(rbf)\n",
    "        h = self.activation(h)\n",
    "        h = self.linear_layer2(h)\n",
    "        return {\"h\": h}\n",
    "\n",
    "    def forward(self, g):\n",
    "        g.apply_edges(self.update_edge)\n",
    "        g.update_all(message_func=fn.u_mul_e('new_node', 'h', 'neighbor_info'),\n",
    "                     reduce_func=fn.sum('neighbor_info', 'new_node'))\n",
    "        return g.ndata[\"new_node\"]\n",
    "\n",
    "\n",
    "class Interaction(nn.Module):\n",
    "    \"\"\"\n",
    "    The interaction layer in the SchNet model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rbf_dim, dim):\n",
    "        super().__init__()\n",
    "        self._node_dim = dim\n",
    "        self.activation = nn.Softplus(beta=0.5, threshold=14)\n",
    "        self.node_layer1 = nn.Linear(dim, dim, bias=False)\n",
    "        self.cfconv = CFConv(rbf_dim, dim, act=self.activation)\n",
    "        self.node_layer2 = nn.Linear(dim, dim)\n",
    "        self.node_layer3 = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, g):\n",
    "\n",
    "        g.ndata[\"new_node\"] = self.node_layer1(g.ndata[\"node\"])\n",
    "        cf_node = self.cfconv(g)\n",
    "        cf_node_1 = self.node_layer2(cf_node)\n",
    "        cf_node_1a = self.activation(cf_node_1)\n",
    "        new_node = self.node_layer3(cf_node_1a)\n",
    "        g.ndata[\"node\"] = g.ndata[\"node\"] + new_node\n",
    "        return g.ndata[\"node\"]\n",
    "\n",
    "\n",
    "class VEConv(nn.Module):\n",
    "    \"\"\"\n",
    "    The Vertex-Edge convolution layer in MGCN which take edge & vertex features\n",
    "    in consideratoin at the same time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rbf_dim, dim=64, update_edge=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rbf_dim: the dimension of the RBF layer\n",
    "            dim: the dimension of linear layers\n",
    "            update_edge: whether update the edge emebedding in each conv-layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._rbf_dim = rbf_dim\n",
    "        self._dim = dim\n",
    "        self._update_edge = update_edge\n",
    "\n",
    "        self.linear_layer1 = nn.Linear(self._rbf_dim, self._dim)\n",
    "        self.linear_layer2 = nn.Linear(self._dim, self._dim)\n",
    "        self.linear_layer3 = nn.Linear(self._dim, self._dim)\n",
    "\n",
    "        self.activation = nn.Softplus(beta=0.5, threshold=14)\n",
    "\n",
    "    def update_rbf(self, edges):\n",
    "        rbf = edges.data[\"rbf\"]\n",
    "        h = self.linear_layer1(rbf)\n",
    "        h = self.activation(h)\n",
    "        h = self.linear_layer2(h)\n",
    "        return {\"h\": h}\n",
    "\n",
    "    def update_edge(self, edges):\n",
    "        edge_f = edges.data[\"edge_f\"]\n",
    "        h = self.linear_layer3(edge_f)\n",
    "        return {\"edge_f\": h}\n",
    "\n",
    "    def forward(self, g):\n",
    "        g.apply_edges(self.update_rbf)\n",
    "        if self._update_edge:\n",
    "            g.apply_edges(self.update_edge)\n",
    "\n",
    "        g.update_all(\n",
    "            message_func=[\n",
    "                fn.u_mul_e(\"new_node\", \"h\", \"m_0\"),\n",
    "                fn.copy_e(\"edge_f\", \"m_1\")],\n",
    "            reduce_func=[\n",
    "                fn.sum(\"m_0\", \"new_node_0\"),\n",
    "                fn.sum(\"m_1\", \"new_node_1\")])\n",
    "        g.ndata[\"new_node\"] = g.ndata.pop(\"new_node_0\") + g.ndata.pop(\n",
    "            \"new_node_1\")\n",
    "\n",
    "        return g.ndata[\"new_node\"]\n",
    "\n",
    "\n",
    "class MultiLevelInteraction(nn.Module):\n",
    "    \"\"\"\n",
    "    The multilevel interaction in the MGCN model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rbf_dim, dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self._atom_dim = dim\n",
    "\n",
    "        self.activation = nn.Softplus(beta=0.5, threshold=14)\n",
    "\n",
    "        self.node_layer1 = nn.Linear(dim, dim, bias=True)\n",
    "        self.edge_layer1 = nn.Linear(dim, dim, bias=True)\n",
    "        self.conv_layer = VEConv(rbf_dim, dim)\n",
    "        self.node_layer2 = nn.Linear(dim, dim)\n",
    "        self.node_layer3 = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, g, level=1):\n",
    "        g.ndata[\"new_node\"] = self.node_layer1(g.ndata[\"node_%s\" %\n",
    "                                                       (level - 1)])\n",
    "        node = self.conv_layer(g)\n",
    "        g.edata[\"edge_f\"] = self.activation(self.edge_layer1(\n",
    "            g.edata[\"edge_f\"]))\n",
    "        node_1 = self.node_layer2(node)\n",
    "        node_1a = self.activation(node_1)\n",
    "        new_node = self.node_layer3(node_1a)\n",
    "\n",
    "        g.ndata[\"node_%s\" % (level)] = g.ndata[\"node_%s\" %\n",
    "                                               (level - 1)] + new_node\n",
    "\n",
    "        return g.ndata[\"node_%s\" % (level)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SchNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBFLayerTensor(RBFLayer):\n",
    "    \"\"\"\n",
    "    Same as DGL's RBFLayer only applied to just a tensor (not a DGLGraph-object with edges).\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, dist):\n",
    "        \n",
    "        radial = dist - self.centers\n",
    "        coef   = -1 / self._gap\n",
    "        rbf    = th.exp(coef * (radial**2))\n",
    "        \n",
    "        return rbf\n",
    "    \n",
    "    \n",
    "class Interaction_Dense_BN(nn.Module):\n",
    "    \"\"\"\n",
    "    Like DGL's Interaction-layer only with:\n",
    "        * added batch-normalization\n",
    "        * dense-shortcut instead of residual shortcut\n",
    "    @ rbf_dim: dimension of radial_distance_function(distance)\n",
    "    @ in_dim: dimension of input node states\n",
    "    @ k_dim: dimension of newly created node-state features\n",
    "             (equivalent to growth-rate k in DenseNet)\n",
    "    return: new node hidden states with dimension in_dim + k_dim\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rbf_dim, in_dim, k_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.activation  = nn.Softplus(beta=0.5, threshold=14)\n",
    "        self.node_layer1 = nn.Linear(in_dim, in_dim, bias=False)\n",
    "        self.cfconv      = CFConv(rbf_dim, in_dim, act=self.activation)\n",
    "        self.node_layer2 = nn.Linear(in_dim, k_dim)\n",
    "        self.node_layer3 = nn.Linear(k_dim, k_dim)\n",
    "        self.batch_norm  = nn.BatchNorm1d(k_dim)\n",
    "\n",
    "    def forward(self, g):\n",
    "\n",
    "        g.ndata[\"new_node\"] = self.node_layer1(g.ndata[\"node\"])\n",
    "        cf_node             = self.cfconv(g)\n",
    "        cf_node_1           = self.node_layer2(cf_node)\n",
    "        cf_node_1a          = self.activation(cf_node_1)\n",
    "        new_node            = self.node_layer3(cf_node_1a)\n",
    "        \n",
    "        new_features        = self.batch_norm(new_node)\n",
    "        g.ndata['node']     = torch.cat([g.ndata['node'], new_features], dim=1)\n",
    "        \n",
    "        return g.ndata[\"node\"]\n",
    "\n",
    "class J_Coupling_Regression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, intermediate_dim, output_dim=1):\n",
    "        \"\"\"\n",
    "        @ input_dim: 2 * node-state-dim + additional input\n",
    "        @ intermediate_dim: dimension of both hidden layers\n",
    "        @ output_dim:\n",
    "            * Set to 1 for predicting sc-constant\n",
    "            * Set to 4 for predicting the 4 sc-contributions:\n",
    "              The sum up to the sc-constant but may provide more detailed feedback for the model        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(inplace=True)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 2),\n",
    "            self.activation,\n",
    "            #nn.Dropout(p=0.2),\n",
    "            nn.Linear(input_dim // 2, intermediate_dim),\n",
    "            self.activation,\n",
    "            #nn.Dropout(p=0.1),\n",
    "            nn.Linear(intermediate_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is a concatenation of the hidden-states of 2 j-coupled nodes\n",
    "        and some additional input\n",
    "        \"\"\"            \n",
    "        return self.mlp(x)\n",
    "\n",
    "        \n",
    "class Atominator(nn.Module):\n",
    "    \"\"\"\n",
    "    Schnet for feature extraction and regression to predict j-coupling constant\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 num_atom_types=6,  # count starts at 1\n",
    "                 embedding_dim=128,\n",
    "                 graph_state_dim=64,\n",
    "                 output_dim=4,\n",
    "                 n_conv=3,\n",
    "                 cutoff=5,\n",
    "                 width=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_layer = AtomEmbedding(type_num=num_atom_types,\n",
    "                                             dim=embedding_dim)\n",
    "        \n",
    "        self.rbf_layer        = RBFLayer(0, cutoff, width)\n",
    "        self.tensor_rbf_layer = RBFLayerTensor(0, cutoff, width)\n",
    "        \n",
    "        self.n_conv = n_conv\n",
    "        self.conv_layers = nn.ModuleList(\n",
    "            [Interaction_Dense_BN(self.rbf_layer._fan_out, in_dim=embedding_dim,     k_dim=embedding_dim),\n",
    "             Interaction_Dense_BN(self.rbf_layer._fan_out, in_dim=embedding_dim * 2, k_dim=embedding_dim),\n",
    "             Interaction_Dense_BN(self.rbf_layer._fan_out, in_dim=embedding_dim * 3, k_dim=embedding_dim)]\n",
    "        )\n",
    "        final_node_state_dim = embedding_dim * 4\n",
    "        \n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(final_node_state_dim, final_node_state_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Dropout(p=0.2),\n",
    "            nn.Linear(final_node_state_dim // 2, graph_state_dim)\n",
    "        )\n",
    "        \n",
    "        # 2 node-hidden-states + rbf(distance) + graph_state\n",
    "        reg_input_dim = (final_node_state_dim * 2\n",
    "                         + self.tensor_rbf_layer._fan_out\n",
    "                         + graph_state_dim)\n",
    "        self.target_regression = J_Coupling_Regression(\n",
    "            input_dim=reg_input_dim,\n",
    "            intermediate_dim=128,\n",
    "            output_dim=output_dim\n",
    "        )\n",
    "            \n",
    "    def forward(self,\n",
    "                g: dgl.DGLGraph,\n",
    "                j_pairs: np.array):\n",
    "        \"\"\"\n",
    "        @ g: molecule-graph\n",
    "        @ j_pairs: (i, j, distance) where i, j are node-indices\n",
    "        \"\"\"\n",
    "                \n",
    "        self.embedding_layer(g)\n",
    "        self.rbf_layer(g)\n",
    "                \n",
    "        for idx in range(self.n_conv):\n",
    "            self.conv_layers[idx](g)\n",
    "            \n",
    "        node_state_sum = graph.ndata['node'].sum(dim=0)\n",
    "        graph_state = self.readout(node_state_sum)\n",
    "        \n",
    "        concatentations = []\n",
    "        for id_, i, j, dist in j_pairs:\n",
    "            rbf_dist  = self.tensor_rbf_layer(torch.tensor([dist]).to(DEVICE))\n",
    "            concatentations.append(torch.cat([g.ndata['node'][int(i)],\n",
    "                                              g.ndata['node'][int(j)],\n",
    "                                              rbf_dist,\n",
    "                                              graph_state]\n",
    "                                            ))\n",
    "        \n",
    "        concat_batch = torch.stack(concatentations, dim=0)\n",
    "        y = self.target_regression(concat_batch)\n",
    "        \n",
    "        return y  # estimated of j-coupling constant for all coupled atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and select j-coupling type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all global variables:\n",
    "\n",
    "DATA_DIR = '../input/champs-scalar-coupling'\n",
    "ATOM2ENUM = {\n",
    "    'H': 1,  # start at 1 just to be sure in cas 0 is a default embedding in DGL\n",
    "    'C': 2,\n",
    "    'N': 3,\n",
    "    'O': 4,\n",
    "    'F': 5\n",
    "}\n",
    "J_TYPE = '1JHN'\n",
    "DEVICE = None  # using only CPU here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (34691, 10)\n",
      "validation: (8672, 10)\n",
      "test: (24195, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>molecule_name</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "      <th>fc</th>\n",
       "      <th>sd</th>\n",
       "      <th>pso</th>\n",
       "      <th>dso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHN</td>\n",
       "      <td>32.6889</td>\n",
       "      <td>30.6116</td>\n",
       "      <td>0.059952</td>\n",
       "      <td>1.94935</td>\n",
       "      <td>0.067923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHN</td>\n",
       "      <td>32.6891</td>\n",
       "      <td>30.6119</td>\n",
       "      <td>0.059961</td>\n",
       "      <td>1.94935</td>\n",
       "      <td>0.067922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>dsgdb9nsd_000002</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHN</td>\n",
       "      <td>32.6905</td>\n",
       "      <td>30.6135</td>\n",
       "      <td>0.059939</td>\n",
       "      <td>1.94911</td>\n",
       "      <td>0.067931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>dsgdb9nsd_000012</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHN</td>\n",
       "      <td>55.5252</td>\n",
       "      <td>53.6945</td>\n",
       "      <td>0.143624</td>\n",
       "      <td>1.48139</td>\n",
       "      <td>0.205708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>101</td>\n",
       "      <td>dsgdb9nsd_000012</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHN</td>\n",
       "      <td>54.7359</td>\n",
       "      <td>52.7562</td>\n",
       "      <td>0.175517</td>\n",
       "      <td>1.60638</td>\n",
       "      <td>0.197831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id     molecule_name  atom_index_0  atom_index_1  type  \\\n",
       "10    10  dsgdb9nsd_000002             1             0  1JHN   \n",
       "13    13  dsgdb9nsd_000002             2             0  1JHN   \n",
       "15    15  dsgdb9nsd_000002             3             0  1JHN   \n",
       "97    97  dsgdb9nsd_000012             3             0  1JHN   \n",
       "101  101  dsgdb9nsd_000012             4             0  1JHN   \n",
       "\n",
       "     scalar_coupling_constant       fc        sd      pso       dso  \n",
       "10                    32.6889  30.6116  0.059952  1.94935  0.067923  \n",
       "13                    32.6891  30.6119  0.059961  1.94935  0.067922  \n",
       "15                    32.6905  30.6135  0.059939  1.94911  0.067931  \n",
       "97                    55.5252  53.6945  0.143624  1.48139  0.205708  \n",
       "101                   54.7359  52.7562  0.175517  1.60638  0.197831  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_val_split(df: pd.DataFrame, val_fraction=0.2):\n",
    "    \"\"\" Split by molecule. \"\"\"\n",
    "    molecules     = df.molecule_name.unique().tolist()\n",
    "    val_molecules = np.random.choice(molecules,\n",
    "                                     size=int(val_fraction * len(molecules)),\n",
    "                                     replace=False)\n",
    "    val_set   = df.query('molecule_name in @val_molecules')\n",
    "    train_set = df.query('molecule_name not in @val_molecules')\n",
    "    return train_set, val_set\n",
    "\n",
    "\n",
    "def load_dataset(j_type=None):\n",
    "    \n",
    "    train = pd.merge(pd.read_csv(join(DATA_DIR, 'train.csv')),\n",
    "                     pd.read_csv(join(DATA_DIR, 'scalar_coupling_contributions.csv')),\n",
    "                     on=['molecule_name', 'atom_index_0', 'atom_index_1', 'type'])\n",
    "    test  = pd.read_csv(join(DATA_DIR, 'test.csv'))\n",
    "    \n",
    "    if j_type is not None:\n",
    "        train = train.query('type == @j_type')\n",
    "        test  = test.query('type == @j_type')\n",
    "        \n",
    "    train, valid = train_val_split(train)\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "\n",
    "structures_df = pd.read_csv(join(DATA_DIR, 'structures.csv'))\n",
    "structures_df.index = structures_df.molecule_name\n",
    "structures_df = structures_df.drop('molecule_name', axis=1)\n",
    "\n",
    "train, valid, test = load_dataset(j_type=J_TYPE)\n",
    "\n",
    "print(f'train: {train.shape}')\n",
    "print(f'validation: {valid.shape}')\n",
    "print(f'test: {test.shape}')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized dataset with 22848 and 34691 j-couplings.\n",
      "initialized dataset with 5712 and 8672 j-couplings.\n",
      "CPU times: user 1min 37s, sys: 424 ms, total: 1min 37s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class Molecule_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 structures: pd.DataFrame,\n",
    "                 targets: pd.DataFrame,\n",
    "                 adj_cutoff=3,  # create a edges between atoms within this distance\n",
    "                 train=True,\n",
    "                 limit=None):\n",
    "        \"\"\"\n",
    "        For each molecule, save in a list:\n",
    "            * all information required to create a molecule graph\n",
    "              (The graph has to be created on the fly to avoid memory leakage)\n",
    "            * all information required for j-coupling regression (atom indices, distance)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.molecule_list = []\n",
    "        self.num_j_couplings = len(targets)\n",
    "        \n",
    "        self.atom_counts       = []\n",
    "        self.j_coupling_counts = []\n",
    "\n",
    "        for i, (mol_name, group_df) in enumerate(targets.groupby('molecule_name')):\n",
    "\n",
    "            struct_df = structures.loc[mol_name]\n",
    "            \n",
    "            self.atom_counts.append(len(struct_df))\n",
    "            self.j_coupling_counts.append(len(group_df))\n",
    "  \n",
    "            atom_types  = struct_df.atom.map(ATOM2ENUM).values\n",
    "            coords      = struct_df[['x', 'y', 'z']].values\n",
    "            dist_matrix = distance_matrix(coords, coords)\n",
    "            adj_matrix  = np.multiply(dist_matrix <= adj_cutoff,  dist_matrix > 0)\n",
    "            edges       = np.where(adj_matrix > 0)\n",
    "            distances   = torch.tensor(dist_matrix[edges].tolist())\n",
    "\n",
    "            graph_input = (atom_types, edges, distances)\n",
    "            \n",
    "            ids = group_df.id.values\n",
    "            a0_idx  = group_df.atom_index_0.values\n",
    "            a1_idx  = group_df.atom_index_1.values\n",
    "            j_dists = dist_matrix[a0_idx, a1_idx]\n",
    "            j_pairs = np.concatenate([np.expand_dims(ids,     axis=1),\n",
    "                                      np.expand_dims(a0_idx,  axis=1),\n",
    "                                      np.expand_dims(a1_idx,  axis=1),\n",
    "                                      np.expand_dims(j_dists, axis=1)],\n",
    "                                     axis=1)\n",
    "            \n",
    "            if train:\n",
    "                sc_contributions = ['fc', 'sd', 'pso', 'dso']  # sum up to sc-constant\n",
    "                y = group_df[sc_contributions].values\n",
    "                self.molecule_list.append( [graph_input, (j_pairs, y)] )\n",
    "            else:\n",
    "                self.molecule_list.append( [graph_input, (j_pairs, )] )\n",
    "\n",
    "            if i == limit:\n",
    "                break\n",
    "                \n",
    "        self.num_molecules = len(self.molecule_list)\n",
    "        self.batch_sizes   = len(set(zip(self.atom_counts, self.j_coupling_counts)))\n",
    "        print(f'initialized dataset with {self.num_molecules} and {self.num_j_couplings} j-couplings.')\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_graph(atom_types, edges, distances) -> dgl.DGLGraph:\n",
    "        \"\"\"\n",
    "        Create graph on the fly.\n",
    "        Delete it after passing through the net.\n",
    "        Required to prevent memory leak. Somehow DGLGraph does not release cuda-memory...\n",
    "        \"\"\"\n",
    "        g = dgl.DGLGraph()\n",
    "        g.add_nodes(len(atom_types))\n",
    "        g.ndata['node_type'] = torch.LongTensor(atom_types)\n",
    "        \n",
    "        g.add_edges(edges[0].tolist(), edges[1].tolist())\n",
    "        g.edata['distance'] = distances.view(-1, 1)\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.molecule_list)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        graph_input, target_infos = self.molecule_list[i]\n",
    "        graph = self.get_graph(*graph_input)\n",
    "        return graph, target_infos\n",
    "\n",
    "\n",
    "ds_train = Molecule_Dataset(structures_df, train)\n",
    "ds_valid = Molecule_Dataset(structures_df, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train net for a few epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Atominator()\n",
    "net.train()\n",
    "net.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.005, weight_decay=0)  #1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.97)\n",
    "\n",
    "loss_function = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\ttrain\tvalidation\tlearning-rate\n",
      "0:\t1.2589\t0.4348\t0.00485\n",
      "1:\t0.7851\t0.3335\t0.0047045\n",
      "2:\t0.6945\t0.5847\t0.004563365\n",
      "3:\t0.5965\t0.4618\t0.0044264640499999995\n",
      "4:\t0.5172\t0.4487\t0.004293670128499999\n",
      "5:\t0.4799\t0.6011\t0.004164860024644999\n",
      "6:\t0.4214\t0.3783\t0.004039914223905649\n",
      "7:\t0.3769\t0.5528\t0.003918716797188479\n",
      "8:\t0.2928\t0.3745\t0.003801155293272825\n",
      "9:\t0.2043\t0.5157\t0.0036871206344746403\n",
      "CPU times: user 10h 24min 58s, sys: 9min 31s, total: 10h 34min 29s\n",
      "Wall time: 4h 20min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_epochs=10\n",
    "print('epoch\\ttrain\\tvalidation\\tlearning-rate')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss   = []\n",
    "    running_loss = []\n",
    "    \n",
    "    random_indices = np.random.choice(range(len(ds_train)), size=len(ds_train), replace=False)\n",
    "    for i in range(len(ds_train)):\n",
    "\n",
    "        random_i = random_indices[i]\n",
    "        graph, (j_pairs, y) = ds_train[random_i]\n",
    "\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        graph.ndata['node_type'] = graph.ndata['node_type'].to(DEVICE)\n",
    "        graph.edata['distance']  = graph.edata['distance'].to(DEVICE)\n",
    "        \n",
    "        y_hat = net(graph, j_pairs=j_pairs)\n",
    "\n",
    "        y_truth = torch.tensor(y).float().to(DEVICE)\n",
    "        loss = loss_function(y_hat, target=y_truth)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # multiply times 4 to obtain error of sum of the 4 sc-contributions:\n",
    "        train_loss.append(np.log(loss.item() * 4))\n",
    "        running_loss.append(np.log(loss.item() * 4))\n",
    "        \n",
    "        # free GPU-memory:\n",
    "        del y_truth, y_hat\n",
    "        del graph\n",
    "\n",
    "        #if i and i % 2500 == 0:\n",
    "        #    print(f'{i}\\t{np.mean(running_loss):.2f}')\n",
    "        #    running_loss = []\n",
    "\n",
    "    validation_loss = []\n",
    "    for  graph, (j_pairs, y) in ds_valid:\n",
    "\n",
    "        graph.ndata['node_type'] = graph.ndata['node_type'].to(DEVICE)\n",
    "        graph.edata['distance']  = graph.edata['distance'].to(DEVICE)\n",
    "        \n",
    "        net.eval()\n",
    "        y_hat = net(graph, j_pairs=j_pairs)\n",
    "        \n",
    "        # sum up sc-contributions to obtain the sc-constant:\n",
    "        sc_truth = torch.tensor(y).float().sum(dim=1).to(DEVICE)\n",
    "        sc_pred  = y_hat.sum(dim=1)\n",
    "        \n",
    "        loss = loss_function(sc_pred, target=sc_truth)\n",
    "        validation_loss.append(np.log(loss.item()))\n",
    "        \n",
    "        # free GPU-memory:\n",
    "        del sc_truth, y_hat\n",
    "        del graph\n",
    "\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'{epoch}:\\t{np.mean(train_loss):.4f}\\t{np.mean(validation_loss):.4f}\\t{current_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized dataset with 15581 and 24195 j-couplings.\n",
      "CPU times: user 8min 28s, sys: 4.06 s, total: 8min 32s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "ds_test = Molecule_Dataset(structures_df, test, train=False)\n",
    "\n",
    "predictions = []\n",
    "id2prediction = {}\n",
    "\n",
    "for  graph, (j_pairs, ) in ds_test:\n",
    "\n",
    "    graph.ndata['node_type'] = graph.ndata['node_type'].to(DEVICE)\n",
    "    graph.edata['distance']  = graph.edata['distance'].to(DEVICE)\n",
    "        \n",
    "    net.eval()\n",
    "    y_hat = net(graph, j_pairs=j_pairs)\n",
    "        \n",
    "    # sum up sc-contributions to obtain the sc-constant:\n",
    "    sc_pred  = y_hat.sum(dim=1).detach().cpu().numpy().tolist()\n",
    "    predictions.extend(sc_pred)\n",
    "        \n",
    "    # free GPU-memory:\n",
    "    del graph\n",
    "    del y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(predictions) == len(test)\n",
    "test['scalar_coupling_constant'] = predictions\n",
    "\n",
    "test[['id', 'scalar_coupling_constant']].to_csv(f'submission_{J_TYPE}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
