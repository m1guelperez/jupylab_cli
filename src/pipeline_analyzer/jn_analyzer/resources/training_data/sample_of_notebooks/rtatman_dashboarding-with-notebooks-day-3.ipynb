{"cells":[{"metadata":{},"cell_type":"markdown","source":"____\n\n* **Day 1**: Determining what information should be monitored with a dashboard. [Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-1), [Livestream Recording](https://www.youtube.com/watch?v=QO2ihJS2QLM)\n* **Day 2**: How to create effective dashboards in notebooks, [Python Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-2-python), [R Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-2-r), [Livestream](https://www.youtube.com/watch?v=rhi_nexCUMI)\n* **Day 3**: Running notebooks with the Kaggle API, [Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-3), [Livestream](https://youtu.be/cdEUEe2scNo)\n* **Day 4**: Scheduling notebook runs using cloud services, [Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-4), [Livestream](https://youtu.be/Oujj6nT7etY)\n* **Day 5**: Testing and validation, [Python Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-5), [R Notebook](https://www.kaggle.com/rtatman/dashboarding-with-notebooks-day-5-r), [Livestream](https://www.youtube.com/watch?v=H6DcpIykT8E)\n\n____\n\n\nWelcome to the third day of Dashboarding with scheduled notebooks. Today we're going to do two things:\n\n* Get your Kaggle credentials set up in a cloud service (either GCP or PythonAnywhere)\n* Run a kernel from the shell provided by that service\n\nToday's timeline: \n\n* **5 minutes:** Read notebook & pick service to use\n* **5 minutes:** Make account & sign up\n* **5 minutes:** Get your credentials set up\n* **5 minutes:** Run kernel from cloud service\n\n\n# The Kaggle API\n\nToday weâ€™re going to be using the Kaggle API to run our notebooks. The full documentation for the API is [on GitHub](https://github.com/Kaggle/kaggle-api); today Iâ€™ll just be covering how to  download and upload notebooks.\n\n> **Whatâ€™s an API?** An API is an application program interface. It lets you interact with a program or website by using a programming language rather than a graphic interface like the front end of a website. \n\nThe most common sticking point when starting with the API is in setting up your credentials. Your credentials are what allow you to interact with Kaggle using your account. If you donâ€™t have them set up just right, the API will throw an error when you try to run commands. Since this is a common source of problems, Iâ€™ll be going over how to set them up in excruciating detail today (thatâ€™s why the notebook is so long!). \n\nWe're also going to be using Bash today to set up our credentials. If you're not familiar with Bash or it's just been a while, my colleague Alexis has put together [a helpful getting started guide](https://www.kaggle.com/alexisbcook/intro-to-unix-commands ) to help you get up to speed.\n\n____\n\n# Which cloud service should you pick?\n\nIâ€™ve picked two services to walk you through using. Hereâ€™s a quick run-down of some pros and cons of each to help you decide which one to use. Youâ€™re also free to use any other cloud service, these are just the ones Iâ€™ll be talking about.\n\n### GCP\n\n[GCP](https://cloud.google.com) is Googleâ€™s cloud platform. Itâ€™s made up of a large number of services, but for this event weâ€™ll only be using four:\n\n* [Cloud Shell](https://cloud.google.com/shell/docs/), a shell environment for managing resources hosted on Google Cloud Platform. This is the only one weâ€™ll be using today; the other three are going to come into play tomorrow.\n* [Cloud Functions](https://cloud.google.com/functions/docs/concepts/overview), which allow you to write single purpose functions that you can automatically run whenever a trigger happens. Iâ€™ll be showing you how to write functions in Python, but you can also use JavaScript if you prefer. Weâ€™re going to be using these functions to push and then pull our notebooks.\n* [Cloud Scheduler](https://cloud.google.com/scheduler/), a fairly new service that lets you schedule and run cron jobs in the cloud. (Weâ€™ll talk about cron jobs tomorrow, if youâ€™re not familiar with them.) Weâ€™ll use Cloud Scheduler to trigger our Cloud Functions.\n* [Cloud Storage](https://cloud.google.com/storage/getting-started/), which, as the name suggests, lets you store things, specifically objects or blobs. Weâ€™ll be using it to store notebooks and notebook metadata in. \n\nThere are advantages and disadvantages to using GCP for this project. The main advantage is that, once itâ€™s set up, you can easily scale up your work and integrate with other GCP products, like the Speech API for audio transcription or the Maps API for getting shape files.\n\nThe main disadvantage is that, to schedule jobs, you need to set up a billing account, which requires a credit card. For this project, you shouldnâ€™t need more resources than are available in the [free tier](https://cloud.google.com/free) unless you really go to town but it is possible you may be charged. Beyond the issue of cost, I do know that not everyone has access to a credit card, however. If you donâ€™t, Iâ€™d recommend using PythonAnywhere instead.\n\n#### [Instructions for using GCP here](#GCP-Instructions)\n\n___\n\n### PythonAnywhere\n\n[PythonAnywhere](https://www.pythonanywhere.com/) is an in-browser Python coding environment. I picked it because I find it to be fairly user friendly and because it lets you schedule scripts to run as cron jobs.\n\n> **IMPORTANT NOTE:** PythonAnywhere will have scheduled downtime for a systems upgrade on Wednesday 19th December 2018 (2018-12-19) at 07:00 AM UTC. They expect approximately 20 minutes of downtime. (I only found out about this Tuesday or I would have planned around it! ðŸ˜…)\n\nThe main advantage of PythonAnywhere is that you donâ€™t need a credit card to schedule a script to run. The main disadvantage (besides the really unfortunate timing on that downtime) is that PythonAnywhere doesnâ€™t have as many features as GCP. Itâ€™s mainly designed for running web apps or websites, so if youâ€™re not working in that domain you may end up needing to migrate to a different service that offers more features.\n\n#### [Instructions for using PythonAnywhere here](#PythonAnywhere-Instructions)\n\n____\n\n# GCP Instructions\n\nHere are step-by-step instructions on how to use the Kaggle API to run kernels from the Cloud Shell. \n\nNote that any place where Iâ€™ve written something in all caps, like â€œYOUR KAGGLE USERNAME HEREâ€, youâ€™ll need to replace that text with your actual Kaggle username. \n\n### Open Cloud Shell\n\n1. Sign into your Google Account (if you donâ€™t have a Google account, [create one first](https://support.google.com/accounts/answer/27441?hl=en)).\n2. Go to https://cloud.google.com/\n3. Click \"Go To Console\". This will take you to your GCP dashboard.\n4. Click on the square button with an arrow and dash in the top right hand corner that says \"Activate Cloud Shell\" when you hover over it.\n6. You should see a black shell open at the bottom of your screen. Click in it to begin typing.\n7. (Optional: You can check out the readme to learn more about the shell by running `cat README-cloudshell.txt`)\n\n### Install the Kaggle API\n\n1. Install Kaggle by running `sudo pip install kaggle`\n\n### Set up your credentials\n\n1. Go to your Kaggle account page at `https://www.kaggle.com/[YOUR KAGGLE USERNAME HERE]/account`.\n2. Scroll down to the API section and click \"Create New API Token\". This will download a .json file with your Kaggle credentials.\n3. Go back to the Cloud Shell in your GCP account.\n4. Upload your credentials by clicking on the three dots in the menu in the header of the shell and then clicking on the \"Upload File\" command. Follow the prompts to upload the .json file with your credentials. They should be uploaded to the directory `/home/[YOUR GCP USERNAME HERE]`, which is the directory your session starts in by default.\n5. (Optional: Run the command `kaggle`. This will throw an error because you haven't moved your credentials to the correct directory yet. The final line of the error will tell you the path to put your credentials in.)\n6. Move your credentials to the .kaggle directory by running `mv kaggle.json /home/[YOUR GCP USERNAME HERE]/.kaggle/kaggle.json'\n7. Make your credentials private by running `chmod 600 /home/[YOUR GCP USERNAME HERE]/.kaggle/kaggle.json`\n8. Check that you did everything correctly by running the command `kaggle -h`. This should bring up the API help menu.\n\n### Pull your notebook\n\n> To \"pull\" a notebook means to download a local copy of it. \n\n1. (Optional: you can search for your kernels, ranked by how recently you ran them, by running `kaggle kernels list --user [YOUR KAGGLE USERNAME HERE] --sort-by dateRun`.)\n2. Pull a copy of your kernel by running kaggle `kaggle kernels pull [AUTHOR'S KAGGLE USERNAME]/[KERNEL SLUG FROM URL] -m`. For example, if you wanted to pull a copy of [this kernel](https://www.kaggle.com/rtatman/world-bank-open-calls-dashboard), you would run `kaggle kernels pull rtatman/world-bank-open-calls-dashboard -m`.\n3. Check that you pulled it correctly by running `ls`.  You should see that the notebook file and metadata file are both in your current working directory.\n\n### Push your notebook\n\n> To \"push\" a notebook means to upload a copy to Kaggle. When you push your notebook, it is automatically committed and creates a new version. Since committing a notebook runs all the code from top to bottom, this may take a while if you have a very computation-heavy notebook.\n\n1. Check to make sure that you have both the .ipynb and `kernel-metadata.json` files in your current working directory by running `ls`. If you don't see them, either move to the directory where you downloaded them or pull a fresh copy of notebook.\n2. Push your notebook by running `kaggle kernels push`.\n\nAnd that's all you need to do to update your Kaggle Kernels from the command line on GCP!\n\n____\n\n# PythonAnywhere Instructions\n\nHere are step-by-step instructions on how to use the Kaggle API to run kernels from the PythonAnywhere Bash Console.\n\nNote that any place where Iâ€™ve written something in all caps, like â€œYOUR KAGGLE USERNAME HEREâ€, youâ€™ll need to replace that text with your actual Kaggle username. \n\n### Create a Bash Console\n\n1. Log in to your PythonAnywhere account. If you donâ€™t have one yet, you can [sign up here](https://www.pythonanywhere.com/registration/register/beginner/). \n2. Create a Bash console by going to your account home (it will be at `https://www.pythonanywhere.com/user/[YOUR PYTHONANYWHERE USERNAME]`) and clicking on the `$ Bash` button under â€œNew console:â€. You should see a black shell with a green dollar sign.  \n\n### Install the Kaggle API\n\n1. Run the command `pip install kaggle --user`\n\n### Set up your credentials\n\n1. Go to your Kaggle account page at `https://www.kaggle.com/[YOUR KAGGLE USERNAME HERE]/account`.\n2. Scroll down to the API section and click \"Create New API Token\". This will download a .json file with your Kaggle credentials.\n3. Go to your file upload page at `https://www.pythonanywhere.com/user/YOUR PYTHONANYWHERE USERNAME/files/home/YOUR PYTHONANYWHERE USERNAME`. \n4. Click on the yellow â€œUpload a fileâ€ button and follow the instructions to upload your `kaggle.json` file.\n5. Go back to your PythonAnywhere shell. \n6. Check that your file has been uploaded by running `ls`. You should see `kaggle.json` listed. \n7. Make a .kaggle directory and then move the .json files with your credentials to it by running the command `mkdir --parents /home/YOUR PYTHONANYWHERE USERNAME/.kaggle; mv kaggle.json`.\n8. Make your credentials private by running `chmod 600 /home/[YOUR GCP USERNAME HERE]/.kaggle/kaggle.json`\n9. Check that you did everything correctly by running the command `kaggle -h`. This should bring up the API help menu.\n\n### Add proxy information to your credentials\n\nThis step will allow your Python Anywhere account internet access in order to connect to Kaggle via the API. \n\n1. Go to the .kaggle directory by running `cd /home/YOUR_USERNAME/.kaggle`. \n2. Open the nano editor to edit your credentials by running `nano kaggle.json`.\n3. Add the line `\"proxy\": \"http://proxy.server:3128\"` to the end of your .json file. The final file should look something like this: \n\n```\n{\"username\":\"YOUR KAGGLE USERNAME\", \n\"key\":\"YOUR KAGGLE KEY\", \n\"proxy\": \"http://proxy.server:3128\"}\n```\n\n4. Hit CTRL + O and then enter to save your changes.\n5. Hit CTRL + X to exit the editor. \n6. (Optional: Print out your file to make sure it looks correct by running `cat kaggle.json`.)\n7. Navigate back to the root directory by running `cd ~`. \n\n### Pull your notebook\n\n> To \"pull\" a notebook means to download a local copy of it. \n\n1. (Optional: you can search for your kernels, ranked by how recently you ran them, by running `kaggle kernels list --user [YOUR KAGGLE USERNAME HERE] --sort-by dateRun`.)\n2. Pull a copy of your kernel by running kaggle `kaggle kernels pull [AUTHOR'S KAGGLE USERNAME]/[KERNEL SLUG FROM URL] -m`. For example, if you wanted to pull a copy of [this kernel](https://www.kaggle.com/rtatman/world-bank-open-calls-dashboard), you would run `kaggle kernels pull rtatman/world-bank-open-calls-dashboard -m`.\n3. Check that you pulled it correctly by running `ls`.  You should see that the notebook file and metadata file are both in your current working directory.\n\n### Push your notebook\n\n> To \"push\" a notebook means to upload a copy to Kaggle. When you push your notebook, it is automatically committed and creates a new version. Since committing a notebook runs all the code from top to bottom, this may take a while if you have a very computation-heavy notebook.\n\n1. Check to make sure that you have both the .ipynb and `kernel-metadata.json` files in your current working directory by running `ls`. If you don't see them, either move to the directory where you downloaded them or pull a fresh copy of notebook.\n2. Push your notebook by running `kaggle kernels push`.\n\nAnd thatâ€™s all you need to do to run your notebooks on PythonAnywhere!\n\n# Your turn!\n\nTodayâ€™s a fairly self-explanatory one; pick a service and follow the directions to get your credentials set up and run your notebook using the API. :)\n\n"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}