{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "TEXT_COL = 'comment_text'\n",
    "EMB_PATH = '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv', index_col='id')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv', index_col='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1804874 entries, 59848 to 6334010\n",
      "Data columns (total 44 columns):\n",
      "target                                 float64\n",
      "comment_text                           object\n",
      "severe_toxicity                        float64\n",
      "obscene                                float64\n",
      "identity_attack                        float64\n",
      "insult                                 float64\n",
      "threat                                 float64\n",
      "asian                                  float64\n",
      "atheist                                float64\n",
      "bisexual                               float64\n",
      "black                                  float64\n",
      "buddhist                               float64\n",
      "christian                              float64\n",
      "female                                 float64\n",
      "heterosexual                           float64\n",
      "hindu                                  float64\n",
      "homosexual_gay_or_lesbian              float64\n",
      "intellectual_or_learning_disability    float64\n",
      "jewish                                 float64\n",
      "latino                                 float64\n",
      "male                                   float64\n",
      "muslim                                 float64\n",
      "other_disability                       float64\n",
      "other_gender                           float64\n",
      "other_race_or_ethnicity                float64\n",
      "other_religion                         float64\n",
      "other_sexual_orientation               float64\n",
      "physical_disability                    float64\n",
      "psychiatric_or_mental_illness          float64\n",
      "transgender                            float64\n",
      "white                                  float64\n",
      "created_date                           object\n",
      "publication_id                         int64\n",
      "parent_id                              float64\n",
      "article_id                             int64\n",
      "rating                                 object\n",
      "funny                                  int64\n",
      "wow                                    int64\n",
      "sad                                    int64\n",
      "likes                                  int64\n",
      "disagree                               int64\n",
      "sexual_explicit                        float64\n",
      "identity_annotator_count               int64\n",
      "toxicity_annotator_count               int64\n",
      "dtypes: float64(32), int64(9), object(3)\n",
      "memory usage: 619.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(embed_dir=EMB_PATH):\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(embed_dir)))\n",
    "    return embedding_index\n",
    "\n",
    "def build_embedding_matrix(word_index, embeddings_index, max_features, lower = True, verbose = True):\n",
    "    embedding_matrix = np.zeros((max_features, 300))\n",
    "    for word, i in tqdm(word_index.items(),disable = not verbose):\n",
    "        if lower:\n",
    "            word = word.lower()\n",
    "        if i >= max_features: continue\n",
    "        try:\n",
    "            embedding_vector = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_vector = embeddings_index[\"unknown\"]\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "def build_matrix(word_index, embeddings_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1,300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_matrix[i] = embeddings_index[\"unknown\"]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting tokenizer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import gc\n",
    "\n",
    "maxlen = 220\n",
    "max_features = 100000\n",
    "embed_size = 300\n",
    "tokenizer = Tokenizer(num_words=max_features, lower=True) #filters = ''\n",
    "#tokenizer = text.Tokenizer(num_words=max_features)\n",
    "print('fitting tokenizer')\n",
    "tokenizer.fit_on_texts(list(train[TEXT_COL]) + list(test[TEXT_COL]))\n",
    "word_index = tokenizer.word_index\n",
    "X_train = tokenizer.texts_to_sequences(list(train[TEXT_COL]))\n",
    "y_train = train['target'].values\n",
    "X_test = tokenizer.texts_to_sequences(list(test[TEXT_COL]))\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "del tokenizer\n",
    "gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000001it [02:15, 14801.34it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = build_matrix(word_index, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del embeddings_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import keras.layers as L\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def build_model(verbose = False, compile = True):\n",
    "    sequence_input = L.Input(shape=(maxlen,), dtype='int32')\n",
    "    embedding_layer = L.Embedding(len(word_index) + 1,\n",
    "                                300,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=False)\n",
    "    x = embedding_layer(sequence_input)\n",
    "    x = L.SpatialDropout1D(0.2)(x)\n",
    "    x = L.Bidirectional(L.CuDNNLSTM(64, return_sequences=True))(x)\n",
    "\n",
    "    att = Attention(maxlen)(x)\n",
    "    avg_pool1 = L.GlobalAveragePooling1D()(x)\n",
    "    max_pool1 = L.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = L.concatenate([att,avg_pool1, max_pool1])\n",
    "\n",
    "    preds = L.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    if compile:\n",
    "        model.compile(loss='binary_crossentropy',optimizer=Adam(0.005),metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/100\n",
      "1443899/1443899 [==============================] - 139s 96us/step - loss: 0.1187 - acc: 0.9571 - val_loss: 0.0940 - val_acc: 0.9645\n",
      "Epoch 2/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0940 - acc: 0.9643 - val_loss: 0.0902 - val_acc: 0.9652\n",
      "Epoch 3/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0900 - acc: 0.9655 - val_loss: 0.0892 - val_acc: 0.9653\n",
      "Epoch 4/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0873 - acc: 0.9665 - val_loss: 0.0897 - val_acc: 0.9647\n",
      "Epoch 5/100\n",
      "1443899/1443899 [==============================] - 138s 95us/step - loss: 0.0852 - acc: 0.9671 - val_loss: 0.0878 - val_acc: 0.9667\n",
      "Epoch 6/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0834 - acc: 0.9679 - val_loss: 0.0906 - val_acc: 0.9644\n",
      "Epoch 7/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0819 - acc: 0.9684 - val_loss: 0.0879 - val_acc: 0.9661\n",
      "Epoch 8/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0806 - acc: 0.9687 - val_loss: 0.0881 - val_acc: 0.9662\n",
      "Epoch 00008: early stopping\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/100\n",
      "1443899/1443899 [==============================] - 138s 96us/step - loss: 0.1153 - acc: 0.9587 - val_loss: 0.0929 - val_acc: 0.9649\n",
      "Epoch 2/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0932 - acc: 0.9644 - val_loss: 0.0887 - val_acc: 0.9658\n",
      "Epoch 3/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0893 - acc: 0.9658 - val_loss: 0.0874 - val_acc: 0.9663\n",
      "Epoch 4/100\n",
      "1443899/1443899 [==============================] - 138s 95us/step - loss: 0.0866 - acc: 0.9667 - val_loss: 0.1034 - val_acc: 0.9577\n",
      "Epoch 5/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0848 - acc: 0.9672 - val_loss: 0.0998 - val_acc: 0.9598\n",
      "Epoch 6/100\n",
      "1443899/1443899 [==============================] - 137s 95us/step - loss: 0.0833 - acc: 0.9679 - val_loss: 0.0909 - val_acc: 0.9635\n",
      "Epoch 00006: early stopping\n",
      "Train on 1443899 samples, validate on 360975 samples\n",
      "Epoch 1/100\n",
      "1443899/1443899 [==============================] - 138s 96us/step - loss: 0.1197 - acc: 0.9565 - val_loss: 0.0895 - val_acc: 0.9664\n",
      "Epoch 2/100\n",
      "  53248/1443899 [>.............................] - ETA: 2:01 - loss: 0.0974 - acc: 0.9632"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "splits = list(KFold(n_splits=5).split(X_train,y_train))\n",
    "\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "BATCH_SIZE = 2048\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "oof_preds = np.zeros((X_train.shape[0]))\n",
    "test_preds = np.zeros((X_test.shape[0]))\n",
    "for fold in [0,1,2,3,4]:\n",
    "    K.clear_session()\n",
    "    tr_ind, val_ind = splits[fold]\n",
    "    ckpt = ModelCheckpoint(f'gru_{fold}.hdf5', save_best_only = True)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "    model = build_model()\n",
    "    model.fit(X_train[tr_ind],\n",
    "        y_train[tr_ind]>0.5,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        validation_data=(X_train[val_ind], y_train[val_ind]>0.5),\n",
    "        callbacks = [es,ckpt])\n",
    "\n",
    "    oof_preds[val_ind] += model.predict(X_train[val_ind])[:,0]\n",
    "    test_preds += model.predict(X_test)[:,0]\n",
    "test_preds /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9703013100575182"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train>0.5,oof_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7000000</td>\n",
       "      <td>0.002605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7000001</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7000002</td>\n",
       "      <td>0.003359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7000003</td>\n",
       "      <td>0.000592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7000004</td>\n",
       "      <td>0.955201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  prediction\n",
       "0  7000000    0.002605\n",
       "1  7000001    0.000110\n",
       "2  7000002    0.003359\n",
       "3  7000003    0.000592\n",
       "4  7000004    0.955201"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\n",
    "submission['prediction'] = test_preds\n",
    "submission.reset_index(drop=False, inplace=True)\n",
    "submission.head()\n",
    "#%%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
