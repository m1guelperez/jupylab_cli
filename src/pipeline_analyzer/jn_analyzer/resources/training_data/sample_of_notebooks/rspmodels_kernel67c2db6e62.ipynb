{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/Kannada-MNIST/test.csv\n",
      "/kaggle/input/Kannada-MNIST/train.csv\n",
      "/kaggle/input/Kannada-MNIST/Dig-MNIST.csv\n",
      "/kaggle/input/Kannada-MNIST/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "\n",
    "#!pip install -q keras\n",
    "#!pip install -q keract\n",
    "#!pip install -q matplotlib\n",
    "#!pip install -q sklearn\n",
    "#!pip install -q numpy\n",
    "#!pip install  imutils\n",
    "#!pip install  Image\n",
    "#!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n",
    "\n",
    "#%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, Dense,MaxPooling2D,Dropout,Flatten\n",
    "from keras.models import Sequential\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "treinamento=pd.read_csv('/kaggle/input/Kannada-MNIST/train.csv')\n",
    "teste=pd.read_csv('/kaggle/input/Kannada-MNIST/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertCSVImage(frame):\n",
    "  Y=frame['label']\n",
    "  Y=np.array(Y)\n",
    "  X=frame\n",
    "  X=X.drop('label',axis=1)\n",
    "  #del X['label']\n",
    "  X=np.array(X)\n",
    "  #print(X.shape)\n",
    "  X=np.reshape(X, (X.shape[0], 28,28)) # C-like index ordering\n",
    "  X=X/255\n",
    "  Y=keras.utils.to_categorical(Y)\n",
    "\n",
    "  return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train=ConvertCSVImage(treinamento)\n",
    "#X_test,Y_test=ConvertCSVImage(validacao)\n",
    "X_test=X_train[50000:60000,]\n",
    "Y_test=Y_train[50000:60000,]\n",
    "X_train=X_train[0:50000,]\n",
    "Y_train=Y_train[0:50000,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(28,28,1)\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 95s 2ms/step - loss: 0.1384 - accuracy: 0.9582 - val_loss: 0.0370 - val_accuracy: 0.9888\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 96s 2ms/step - loss: 0.0488 - accuracy: 0.9856 - val_loss: 0.0235 - val_accuracy: 0.9920\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 96s 2ms/step - loss: 0.0373 - accuracy: 0.9887 - val_loss: 0.0238 - val_accuracy: 0.9924\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 95s 2ms/step - loss: 0.0268 - accuracy: 0.9919 - val_loss: 0.0281 - val_accuracy: 0.9912\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 95s 2ms/step - loss: 0.0244 - accuracy: 0.9928 - val_loss: 0.0309 - val_accuracy: 0.9917\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 94s 2ms/step - loss: 0.0234 - accuracy: 0.9933 - val_loss: 0.0330 - val_accuracy: 0.9916\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 95s 2ms/step - loss: 0.0234 - accuracy: 0.9934 - val_loss: 0.0304 - val_accuracy: 0.9925\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 96s 2ms/step - loss: 0.0226 - accuracy: 0.9935 - val_loss: 0.0303 - val_accuracy: 0.9928\n",
      "Epoch 10/10\n",
      " 6784/50000 [===>..........................] - ETA: 1:19 - loss: 0.0164 - accuracy: 0.9959"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "batch_size=32\n",
    "num_classes=10\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradCAM(model,imagearray,NClasses=1):\n",
    "  #from keras.preprocessing import image\n",
    "  import cv2 \n",
    "  from PIL import Image\n",
    "  #from keras.models import load_model\n",
    "  import numpy as np\n",
    "  #model=load_model(model)\n",
    "  #img_size=img_size\n",
    "  #img_size=img_size\n",
    "  #img_size=list(img_size)\n",
    "  #for j in range(0,(len(img_size)) ):\n",
    "  #  img_size[j]=int(img_size[j])\n",
    "  #img_size=tuple(img_size)  \n",
    "  #Data = image.load_img(img_path, target_size=img_size) #(224,224)\n",
    "  #Data = image.img_to_array(Data)\n",
    "  #Data = np.expand_dims(Data, axis=0)\n",
    " # Data = preprocess_input(Data)\n",
    "\n",
    "  #x=Data\n",
    "  x=imagearray\n",
    "  #x=x/255\n",
    "\n",
    "  layer_names=[]\n",
    "  cont=0\n",
    "  Indices=[]\n",
    "  for layers in model.layers:\n",
    "    aux=type(layers)\n",
    "    aux=str(aux)\n",
    "    if \"Conv2D\" in aux:\n",
    "      layer_names.append(layers.name)\n",
    "      Indices.append(cont)\n",
    "    cont=cont+1\n",
    "  Indices=Indices[-1]\n",
    "\n",
    "  preds = model.predict(x)\n",
    "  class_idx = np.argmax(preds[0,])\n",
    "  class_output = model.output[:, class_idx]\n",
    "  last_conv_layer = model.layers[Indices]#model.get_layer(\"block5_conv3\")\n",
    "\n",
    "  import keras.backend as K\n",
    "  grads = K.gradients(class_output, last_conv_layer.output)[0]\n",
    "  pooled_grads = K.mean(grads, axis=(0, 1, 2))\n",
    "  iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n",
    "  pooled_grads_value, conv_layer_output_value = iterate([x])\n",
    "  #print(len(conv_layer_output_value[1,1,:]))\n",
    "  for i in range(len(conv_layer_output_value[1,1,:])):\n",
    "    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n",
    "  heatmap = np.mean(conv_layer_output_value, axis=-1)\n",
    "  heatmap = np.maximum(heatmap, 0)\n",
    "  heatmap /= np.max(heatmap)\n",
    "\n",
    " # from google.colab.patches import cv2_imshow\n",
    "\n",
    "  #img = cv2.imread(img_path)\n",
    "  heatmap = cv2.resize(heatmap, (28, 28))\n",
    "  heatmap = np.uint8(255 * heatmap)\n",
    "  heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "  superimposed_img=0.6*imagearray + 0.4*heatmap\n",
    "  #superimposed_img = cv2.addWeighted(imagearray, 0.6, heatmap, 0.4, 0)\n",
    "  #cv2.imshow('Original',img)\n",
    "  print(superimposed_img.shape)\n",
    "  img=np.reshape(superimposed_img,(28,28,3))\n",
    "  imagearray=np.reshape(imagearray,(28,28))\n",
    "  #img = Image.fromarray(superimposed_img, 'RGB')\n",
    " # print(\"A classe Predita foi\",class_idx)\n",
    "  return(img,imagearray)\n",
    "  #cv2_imshow(superimposed_img)\n",
    "  #cv2.waitKey(0)\n",
    "  #return(superimposed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TesteX=teste\n",
    "TesteX=TesteX.drop('id',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TesteX=np.array(TesteX)\n",
    "TesteX=np.reshape(TesteX, (TesteX.shape[0], 28,28,1)) # C-like index orderin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes=model.predict(TesteX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[]\n",
    "for i in range(0,previsoes.shape[0]):\n",
    "  labels.append(np.argmax(previsoes[i,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrevisoesFinais=pd.DataFrame([teste['id'].values,labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrevisoesFinais=PrevisoesFinais.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrevisoesFinais.columns = ['id', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrevisoesFinais.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well we then trained a model and found our predictions but we can wonder why does the neural network tough of this answer?\n",
    "\n",
    "To do so we utilize GradCam as defined above to vizualize which pixels were important in the image for top 1 classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "Indice=43\n",
    "aux=TesteX[Indice,]\n",
    "aux=np.reshape(aux,(1,aux.shape[0],aux.shape[1],aux.shape[2] ) )\n",
    "aux.shape\n",
    "Novo,Antigo=GradCAM(model,aux,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Antigo,cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Novo, interpolation='nearest')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
