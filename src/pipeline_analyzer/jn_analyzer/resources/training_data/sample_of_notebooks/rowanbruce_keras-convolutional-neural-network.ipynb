{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8dfdd6e5-35a8-4265-8033-c60b870e1030",
    "_uuid": "e06ee4dfdab5af825b5662765973a22f3330575e"
   },
   "source": [
    "### The Data\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive\n",
    "\n",
    "The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel-values of the associated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "8424354b-4105-4f9b-81a2-04e71aae6e66",
    "_uuid": "8815ce425e0f67728b85ad8484a8c3753d8e6d8f",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "36e0f072-9adb-4907-a01f-a1a6943feeb6",
    "_uuid": "516596152ae12de1b2a0d2da2bf40d49d3e681f7"
   },
   "source": [
    "## Load in test and training data/ Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "9a2c0b36-581c-4db7-85a4-526a77face61",
    "_uuid": "0f3dea3b63b6541444f616fbbaedefcaf833a7a8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train1/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b484d6d4-dcfd-4c91-86aa-df4c04f272d9",
    "_uuid": "ab77d5c9f5e82002f7a3ac510573a2137d40a2e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../input/digit-recognizer/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "7776911f-02d6-4444-911c-37732bd43857",
    "_uuid": "2aea51cd99702975d60739bcdc960c0c8afc25ae",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b4495810-3f3a-402c-ab33-3be4582a3b02",
    "_uuid": "0adbfbba906e36144a9aa611e8823c4eb2f10972",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = train.label\n",
    "train_labels = a.to_frame()\n",
    "\n",
    "#important to have pixels np arrays to use \"reshape\" \n",
    "train_pixels = train.drop('label', 1).values\n",
    "test_pixels = test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "bdd9c0eb-4e47-4ded-beef-57a9decae7ab",
    "_uuid": "3f88626b00a9483342f0d5c5573599805f48f918",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reshape to input into cnn as [w][h[d]\n",
    "train_pixels = train_pixels.reshape(train_pixels.shape[0], 28, 28,1).astype('float')\n",
    "test_pixels = test_pixels.reshape(test_pixels.shape[0], 28, 28,1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "41d07053-4652-4028-9229-631318b99ec0",
    "_uuid": "9e5c14c5a819d48233d9c0926de995d79a8e3384",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 28, 28, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pixels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "45b305e9-40f4-4766-919d-4d2e8ff4411a",
    "_uuid": "7858a31f0b68a803f5a7b02bb55f52d9d4ce69e0"
   },
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "06cf0fd2-7fb4-4928-bfc4-da4376d61b6e",
    "_uuid": "75dd13cf6cdda9f28c7519eef55cbaa374f743b6"
   },
   "source": [
    "### standardize? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "04090a11-3b5e-47ca-a5fc-cf1490f51cb6",
    "_uuid": "4cc646bf42ca3cf05ee9e2f02657bf9e6482ab7f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_pixels = train_pixels/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e47c954f-8e08-47e6-b4af-5003d5b3ba4e",
    "_uuid": "66873d11634162baa1170d6d5e9235916f2e551a"
   },
   "source": [
    "### one-hot encode labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "7b7d47f5-b823-4da6-b2d0-0c93b33e5b79",
    "_uuid": "66ab8847bc168eff2fba47fbdcc5ac61b8ef7df1",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "d2ba51f5-de07-44b4-b9cd-a7317c1d0dd1",
    "_uuid": "0b2d299ebd7e7a5a4720c31f019a9736731722ae",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = to_categorical(train_labels)\n",
    "num_classes = train_labels.shape[1]\n",
    "\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "e0203e02-e577-4631-bfed-2374ceb833e8",
    "_uuid": "2e92dd69a690ffe168505e5da271b1ac9ffff5b8",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cdef22f4-70b9-40e4-863c-8aa4b0d59ab8",
    "_uuid": "0c1b2a3092ac4f91db2442f68147d85c7e05836d",
    "collapsed": true
   },
   "source": [
    "## Design Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "87c5021f-a267-4e64-b7f6-7be64dd7bc3f",
    "_uuid": "62a94d3c92b5907d66bcf83ad8167bde6ee2042c"
   },
   "source": [
    "Karas Sequential model = linear stack of layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "d0f3c812-40cd-4637-b148-f360ad64d9a9",
    "_uuid": "71300b5e738036bd49a02914b6cedc5d7438e5e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "#add convolutional layer\n",
    "#32 kernals per conv layer, size of the kernals is 5x5\n",
    "#input_shape [width][height][depth]\n",
    "model.add(Conv2D(filters = 32,kernel_size = (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "\n",
    "#add pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#adding 2nd Conv layer!\n",
    "model.add(Conv2D(32, (3, 3),activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#3rd conv\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu'))\n",
    "\n",
    "#add dropout layer, excludes 20% of neurons to avoid overfiting\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#converts 2d matrix to vector... allows the output to be processed by standard fully connected layers.\n",
    "model.add(Flatten())\n",
    "\n",
    "#adds a fully connected layer with 256 neurons\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c0b7d93e-cded-4be3-8317-2c62a8f90da7",
    "_uuid": "23231df5163285cc770063ee7da0e56e93836151"
   },
   "source": [
    "## Compile NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "e77136bf-ba8d-446c-8eca-3fe057db708d",
    "_uuid": "49094755134a437192e2a45ad986bcd797891b4a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "d72d7c81-6edb-4071-890c-3cdc47b5b1ab",
    "_uuid": "af2f8f4a5fdb953f2f75014ed2fb69a6704ebc95",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "60f685d8-8f07-4806-9f3c-23cc80340bd5",
    "_uuid": "672930ee4cd51d0be318ef3ad8ada5f0a276f7c1"
   },
   "source": [
    "## Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "3eb98806-ae3a-41a5-a2dc-c6bec141d438",
    "_uuid": "adef5e4590781e12f76a2f252ebb3b109a48d757",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8442fcd1-6bab-44d7-9b96-ad3f4852a753",
    "_uuid": "2aa547a2119b493641c53190672e73b3525eb643"
   },
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "21dd7460-c351-4b80-9632-aa9298240cd1",
    "_uuid": "bc934ac7fe7feddcf86e152992c23a7f97e83cd4",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 47s - loss: 0.5116 - acc: 0.8303\n",
      "Epoch 2/10\n",
      " - 46s - loss: 0.1341 - acc: 0.9586\n",
      "Epoch 3/10\n",
      " - 45s - loss: 0.0983 - acc: 0.9698\n",
      "Epoch 4/10\n",
      " - 45s - loss: 0.0783 - acc: 0.9754\n",
      "Epoch 5/10\n",
      " - 46s - loss: 0.0697 - acc: 0.9785\n",
      "Epoch 6/10\n",
      " - 43s - loss: 0.0608 - acc: 0.9804\n",
      "Epoch 7/10\n",
      " - 44s - loss: 0.0584 - acc: 0.9820\n",
      "Epoch 8/10\n",
      " - 46s - loss: 0.0531 - acc: 0.9833\n",
      "Epoch 9/10\n",
      " - 45s - loss: 0.0490 - acc: 0.9850\n",
      "Epoch 10/10\n",
      " - 45s - loss: 0.0475 - acc: 0.9847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb5077e3cf8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_pixels, train_labels, epochs=10, batch_size=200, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "000c980a-32d1-4007-88a5-0317d10bdace",
    "_uuid": "f8392c0db74850f5dde2e5c46b354cc5630a31b0",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test_pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "c7c972c0-30bc-420c-866e-09c8cd1cb190",
    "_uuid": "37951603d4bbee06bc992a5398ddb54234933c4c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
    "                         \"Label\": predictions})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "8897ce78-3969-4d5f-ab42-1a01b155b1f8",
    "_uuid": "f472246317049236ba8c52639c2cbbceed6c40b1",
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      2\n",
       "1        2      0\n",
       "2        3      9\n",
       "3        4      9\n",
       "4        5      3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "24ac18a9-3e9f-4a31-ae69-1e64a0856e33",
    "_uuid": "071c05be3f87f52dec02b718731ceedcdafa0a28",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submissions.to_csv(\"result.csv\", index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4ab6333-ab14-4203-82af-f59e49ca89ad",
    "_uuid": "58b676850e29d1435c5d76cf80a80c35a8c80a7e"
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "In order to avoid overfitting problem, we need to expand artificially our handwritten digit dataset. We can make your existing dataset even larger. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\n",
    "\n",
    "Approaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Some popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "01c138c0-8c29-4caf-9f73-c12188d5b44d",
    "_uuid": "a5b608569f0e19868e7f4efcbbbf98625a8ccb80",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(train_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "0b7a5875-ae27-47ed-a379-8663849e9111",
    "_uuid": "f9ecaedc9fafc71f9804d121f5bffbcff67c96f5",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 44s - loss: 0.1895 - acc: 0.9416\n",
      "Epoch 2/10\n",
      " - 43s - loss: 0.1275 - acc: 0.9605\n",
      "Epoch 3/10\n",
      " - 43s - loss: 0.1170 - acc: 0.9647\n",
      "Epoch 4/10\n",
      " - 44s - loss: 0.1058 - acc: 0.9675\n",
      "Epoch 5/10\n",
      " - 42s - loss: 0.0983 - acc: 0.9699\n",
      "Epoch 6/10\n",
      " - 40s - loss: 0.0929 - acc: 0.9710\n",
      "Epoch 7/10\n",
      " - 41s - loss: 0.0872 - acc: 0.9735\n",
      "Epoch 8/10\n",
      " - 41s - loss: 0.0853 - acc: 0.9739\n",
      "Epoch 9/10\n",
      " - 40s - loss: 0.0848 - acc: 0.9737\n",
      "Epoch 10/10\n",
      " - 41s - loss: 0.0811 - acc: 0.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb5053b44e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(train_pixels,train_labels, batch_size=200),epochs =10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "bbcb0ac6-40ed-426e-aea3-6e67a5478a33",
    "_uuid": "9a2f1f8250851edfa33fbe72fe5f0101c95eb133",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(test_pixels)\n",
    "submissions=pd.DataFrame({\"ImageId\": list(range(1,len(predictions)+1)),\n",
    "                         \"Label\": predictions})\n",
    "submissions.to_csv(\"result.csv\", index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "232408a6-a733-4a62-aedf-abb94220c27f",
    "_uuid": "4023529b7598b347e039d556687e57ab246476cc"
   },
   "source": [
    "# NOTES\n",
    "\n",
    "-Finally got model to run. achieved 0.78 accuracy\n",
    "\n",
    "-will see how much accuracy improves after converting input to float\n",
    "achieved 0.79 acc\n",
    "\n",
    "-will see how much accuracy improves after standardizing input to 0-1 range\n",
    "jumped to 0.99 accuracy\n",
    "kaggle score of 97%\n",
    "\n",
    "-fixed standardization from 225 to 255\n",
    "only 93% accuracy?\n",
    "\n",
    "-dont think i changed anything but acc is up to 99.4%\n",
    "maybe this is because I am not using a random seed\n",
    "kaggle score of 8%???\n",
    "kaggle score of 98 now....\n",
    "\n",
    "-adding one more conv layer\n",
    "kaggle slight increase\n",
    "\n",
    "-adding dropout after each conv layer\n",
    "another slight increase\n",
    "kaggle is now at 99.1%\n",
    "\n",
    "-adding another fully connected layer(dense 256)\n",
    "kaggle score does not change\n",
    "\n",
    "\n",
    "\n",
    "-set epochs to 30?\n",
    "lowered kaggle score\n",
    "\n",
    "-add augmented data\n",
    "lowered kaggle to 96\n",
    "\n",
    "\n",
    "addint more layers really increased run time\n",
    "30sec to 1 min to 1.2 min per epoch\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
