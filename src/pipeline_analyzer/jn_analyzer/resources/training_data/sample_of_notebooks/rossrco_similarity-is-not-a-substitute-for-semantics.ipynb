{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "037b4592-b1cb-9a5d-fe44-0904afe3ab88"
   },
   "source": [
    "Hey dear Kagglers, I'm excited to share with you my very first notebook and I'll be very happy to get some advice on the many things I can improve in my investigation into the Quora dataset. Here goes...\n",
    "\n",
    "I decided to take a hybrid approach (including naive as well as tf-idf features).\n",
    "\n",
    "We start by first deriving the naive features:\n",
    "\n",
    " - Similarity: basic similarity ratio between the two question strings\n",
    " - Pruned similarity: similarity of the two question strings excluding the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "6954c41e-07f7-0a8d-af2f-1e08bd161b28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    similarity  pruned_similarity\n",
      "0     0.910569           0.900000\n",
      "1     0.618705           0.614035\n",
      "2     0.378788           0.380952\n",
      "3     0.139130           0.117647\n",
      "4     0.347826           0.247423\n",
      "5     0.556818           0.521739\n",
      "6     0.148148           0.142857\n",
      "7     0.591549           0.787879\n",
      "8     0.823529           0.590909\n",
      "9     0.293578           0.369565\n",
      "10    0.242775           0.285714\n",
      "11    0.641975           0.760000\n",
      "12    0.826667           0.960000\n",
      "13    0.913580           0.872727\n",
      "14    0.946619           0.919255\n",
      "15    0.402174           0.450704\n",
      "16    0.947368           0.871795\n",
      "17    0.329897           0.350877\n",
      "18    0.567742           0.630631\n",
      "19    0.894737           0.833333\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 250) #so that the full column of tagged sentences can be displayed\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning) #to stop the annoying deprecation warnings from sklearn\n",
    "\n",
    "#Some simple functions\n",
    "def remove_stopwords(tokenized_sent):\n",
    "    unique_stopwords = set(stopwords.words('english'))\n",
    "    return [word for word in tokenized_sent if word.lower() not in unique_stopwords]\n",
    "\n",
    "def concatenate_tokens(token_list):\n",
    "    return str(' '.join(token_list))\n",
    "\n",
    "def find_similarity(sent1, sent2):\n",
    "\treturn SequenceMatcher(lambda x: x in (' ', '?', '.', '\"\"', '!'), sent1, sent2).ratio()\n",
    "\n",
    "def return_common_tokens(sent1, sent2):\n",
    "    return \" \".join([word.lower() for word in sent1 if word in sent2])\n",
    "\n",
    "def convert_tokens_lower(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "#Reading the train file\n",
    "train_sample = pd.read_csv('../input/train.csv', encoding = 'utf-8', index_col = 0, header = 0, iterator = True).get_chunk(100000)\n",
    "\n",
    "transformed_sentences_train = pd.DataFrame(index = train_sample.index)\n",
    "naive_similarity = pd.DataFrame()\n",
    "temp_features = pd.DataFrame()\n",
    "dictionary = pd.DataFrame()\n",
    "\n",
    "#Deriving the naive features\n",
    "for i in (1, 2):\n",
    "        transformed_sentences_train['question%s_tokens' % i] = train_sample['question%s' % i].apply(nltk.word_tokenize)\n",
    "        transformed_sentences_train['question%s_lowercase_tokens' % i] = transformed_sentences_train['question%s_tokens' % i].apply(convert_tokens_lower)\n",
    "        transformed_sentences_train['question%s_lowercase' % i] = transformed_sentences_train['question%s_lowercase_tokens' % i].apply(concatenate_tokens)\n",
    "        transformed_sentences_train['question%s_words' % i] = transformed_sentences_train['question%s_tokens' % i].apply(remove_stopwords)\n",
    "        transformed_sentences_train['question%s_pruned' % i] = transformed_sentences_train['question%s_words' % i].apply(concatenate_tokens)\n",
    "naive_similarity['similarity'] = np.vectorize(find_similarity)(train_sample['question1'], train_sample['question2'])\n",
    "naive_similarity['pruned_similarity'] = np.vectorize(find_similarity)(transformed_sentences_train['question1_pruned'], transformed_sentences_train['question2_pruned'])\n",
    "temp_features['common_tokens'] = np.vectorize(return_common_tokens)(transformed_sentences_train['question1_tokens'], transformed_sentences_train['question2_tokens'])\n",
    "\n",
    "print (naive_similarity[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b93de2fe-25cd-4bf1-a5f3-fe31c1ac9d58"
   },
   "source": [
    "This is supposed to catch the most elementary non-duplicates (where the questions are obviously different), e.g. question id 3:\n",
    "\n",
    " - Why am I mentally very lonely? How can I solve it?\n",
    " - Find the remainder when [math]23^{24}[/math] is divided by 24,23?\n",
    "\n",
    "As we can see from the output, the similarity there is 14% and the pruned similarity is 11%\n",
    "\n",
    "Next, we can enrich the feature set by adding the term frequency inverse dictionary frequency measure (tf-idf). The term frequency is the count of a term in a specific question, the inverse document frequency is the log of the total number of questions divided by the number of questions containing the term. Here is the derivation using scikit-learn's library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "88979f8c-f8e8-139e-5c3e-4d6180025815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       weight_q1  weight_q2  common_weight\n",
      "index                                     \n",
      "0       2.828994   2.634564       2.694020\n",
      "1       2.490214   3.230564       1.886181\n",
      "2       3.116418   2.871033       1.503125\n",
      "3       2.694861   2.645140       0.000000\n",
      "4       3.299253   2.456702       1.810175\n",
      "5       3.149575   2.859596       2.392546\n",
      "6       1.552112   2.906378       0.000000\n",
      "7       1.782090   2.112007       1.237502\n",
      "8       2.273709   2.448624       2.273709\n",
      "9       2.559370   2.433884       1.681997\n",
      "10      2.652685   3.555135       1.000000\n",
      "11      2.550162   2.420967       1.489142\n",
      "12      2.382559   2.563881       2.067898\n",
      "13      2.476111   2.280261       2.280261\n",
      "14      4.094661   4.094851       3.908361\n",
      "15      3.502439   3.368390       1.714752\n",
      "16      1.627202   1.672566       1.348121\n",
      "17      3.133376   2.318855       1.000000\n",
      "18      3.371366   3.409532       2.152384\n",
      "19      2.558040   2.667569       2.315686\n"
     ]
    }
   ],
   "source": [
    "dictionary = pd.DataFrame()\n",
    "\n",
    "#Deriving the TF-IDF\n",
    "dictionary['concatenated_questions'] = transformed_sentences_train['question1_lowercase'] + transformed_sentences_train['question2_lowercase']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "terms_matrix = vectorizer.fit_transform(dictionary['concatenated_questions'])\n",
    "terms_matrix_1 = vectorizer.transform(transformed_sentences_train['question1_lowercase'])\n",
    "terms_matrix_2 = vectorizer.transform(transformed_sentences_train['question2_lowercase'])\n",
    "common_terms_matrx = vectorizer.transform(temp_features['common_tokens'])\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf = False)\n",
    "weights_matrix = transformer.fit_transform(terms_matrix)\n",
    "weights_matrix_1 = transformer.transform(terms_matrix_1)\n",
    "weights_matrix_2 = transformer.transform(terms_matrix_2)\n",
    "common_weights_matrix = transformer.transform(common_terms_matrx)\n",
    "\n",
    "#Converting the sparse matrices into dataframes\n",
    "transformed_matrix_1 = weights_matrix_1.tocoo(copy = False)\n",
    "transformed_matrix_2 = weights_matrix_2.tocoo(copy = False)\n",
    "transformed_common_weights_matrix = common_weights_matrix.tocoo(copy = False)\n",
    "\n",
    "weights_dataframe_1 = pd.DataFrame({'index': transformed_matrix_1.row, 'term_id': transformed_matrix_1.col, 'weight_q1': transformed_matrix_1.data})[['index', 'term_id', 'weight_q1']].sort_values(['index', 'term_id']).reset_index(drop = True)\n",
    "weights_dataframe_2 = pd.DataFrame({'index': transformed_matrix_2.row, 'term_id': transformed_matrix_2.col, 'weight_q2': transformed_matrix_2.data})[['index', 'term_id', 'weight_q2']].sort_values(['index', 'term_id']).reset_index(drop = True)\n",
    "weights_dataframe_3 = pd.DataFrame({'index': transformed_common_weights_matrix.row, 'term_id': transformed_common_weights_matrix.col, 'common_weight': transformed_common_weights_matrix.data})[['index', 'term_id', 'common_weight']].sort_values(['index', 'term_id']).reset_index(drop = True)\n",
    "\n",
    "#Summing the weights of each token in each question to get the summed weight of the question\n",
    "sum_weights_1, sum_weights_2, sum_weights_3 = weights_dataframe_1.groupby('index').sum(), weights_dataframe_2.groupby('index').sum(), weights_dataframe_3.groupby('index').sum()\n",
    "\n",
    "weights = sum_weights_1.join(sum_weights_2, how = 'outer', lsuffix = '_q1', rsuffix = '_q2').join(sum_weights_3, how = 'outer', lsuffix = '_cw', rsuffix = '_cw')\n",
    "weights = weights.fillna(0)\n",
    "del weights['term_id_q1'], weights['term_id_q2'], weights['term_id']\n",
    "\n",
    "print (weights[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "83991ad1-9869-3579-e203-9120ccf49191"
   },
   "source": [
    "This feature is designed to account for questions that are quite similar as strings but are different in meaning. The difference usually comes from a small amount of very significant terms. Example pair id 0:\n",
    "\n",
    " - What is the step by step guide to invest in share market in india?\n",
    " - What is the step by step guide to invest in share market?\n",
    "\n",
    "As is obvious from the data, these two questions have a 91% similarity and 90% pruned similarity. However, the one word that significantly differentiates them is 'india.' The way tf-idf is supposed to address this issue is by applying a larger weight to the 'india' term than to the others. This changes significantly the weight sum of the first and second questions (as is evident from the data above).\n",
    "\n",
    "In addition, we also derive the 'common weight' of the two questions, i.e. the sum of the weight of all the tokens that the two questions share. As we can see this weight is very similar to the weight of the second question which also agrees with our observations.\n",
    "\n",
    "Next, we'll join the features we derived, shuffle and scale them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "7caf8fb7-27f5-edac-8383-7af5de5727ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       similarity  pruned_similarity  weight_q1  weight_q2  common_weight\n",
      "85797    0.951923           0.927536   3.218209   3.287685       2.971282\n",
      "47354    0.680000           0.727273   2.007460   1.705371       1.504887\n",
      "6327     0.431818           0.377358   2.488874   2.356028       1.542083\n",
      "93393    0.245283           0.308725   3.430824   3.930812       1.331517\n",
      "19743    0.285714           0.318841   2.427981   2.415591       0.000000\n",
      "18119    0.918367           0.953846   2.657394   2.628523       2.129179\n",
      "42008    0.687500           0.870968   2.546116   2.732959       1.982470\n",
      "11918    0.630137           0.739130   2.137716   2.501109       1.804043\n",
      "20367    0.512821           0.451128   3.596046   2.692666       2.204984\n",
      "54164    0.919540           0.875000   2.750975   2.666664       2.421819\n",
      "57688    0.255144           0.229508   4.048673   3.243112       1.377743\n",
      "26263    0.237288           0.243902   2.431181   3.333762       1.405362\n",
      "5332     0.750000           0.781250   2.453283   2.663893       2.147433\n",
      "34074    0.598425           0.727273   2.609531   2.897341       1.826722\n",
      "30964    0.234694           0.247934   3.445161   3.546037       1.447615\n",
      "14451    0.147287           0.106952   3.914471   4.055963       1.411103\n",
      "34237    0.279412           0.310680   2.834931   2.839914       0.000000\n",
      "22404    0.675676           0.697674   2.300272   1.976149       1.554981\n",
      "66625    0.506329           0.533333   2.335982   2.435194       1.414212\n",
      "84826    0.494624           0.680000   2.455795   3.163716       1.941067\n"
     ]
    }
   ],
   "source": [
    "X = naive_similarity.join(weights, how = 'inner')\n",
    "\n",
    "#Creating a random train-test split\n",
    "y = train_sample['is_duplicate']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 42)\n",
    "\n",
    "#Scaling the features\n",
    "sc = StandardScaler()\n",
    "for frame in (X_train, X_test):\n",
    "    sc.fit(frame)\n",
    "    frame = pd.DataFrame(sc.transform(frame), index = frame.index, columns = frame.columns)\n",
    "\n",
    "print (X_train[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e989eb4a-f429-6e49-e9fb-8721a9b6cb0d"
   },
   "source": [
    "We train our algorithm (gradient boosting classifier) and print the logarithmic loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "118bc787-8ef3-709a-1175-978736110867"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The log loss is 9.62675838909\n"
     ]
    }
   ],
   "source": [
    "#Training the algorithm and making a prediction\n",
    "gbc = GradientBoostingClassifier(n_estimators = 8000, learning_rate = 0.3, max_depth = 3)\n",
    "gbc.fit(X_train, y_train.values.ravel())\n",
    "prediction = pd.DataFrame(gbc.predict(X_test), columns = ['is_duplicate'], index = X_test.index)\n",
    "\n",
    "#Inspecting our mistakes\n",
    "prediction_actual = prediction.join(y_test, how = 'inner', lsuffix = '_predicted', rsuffix = '_actual').join(train_sample[['question1', 'question2']], how = 'inner').join(X_test, how = 'inner')\n",
    "\n",
    "print ('The log loss is %s' % log_loss(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5241c582-9b55-1746-db1e-9f8b85205b22"
   },
   "source": [
    "As we can see, the log loss is abysmal for the 30 question pairs in the sample, but it actually goes down substantially if the algorithm is trained over most of the training data.\n",
    "\n",
    "Finally, we evaluate our mistakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "72baef14-d622-f8d2-7eba-3c7d686f8d1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       is_duplicate_predicted  is_duplicate_actual  \\\n",
      "75721                       1                    0   \n",
      "76434                       0                    1   \n",
      "60767                       0                    1   \n",
      "42141                       1                    0   \n",
      "1761                        0                    1   \n",
      "64883                       1                    0   \n",
      "80296                       1                    0   \n",
      "14888                       0                    1   \n",
      "94892                       1                    0   \n",
      "84936                       0                    1   \n",
      "\n",
      "                                                                                question1  \\\n",
      "75721                                                             Why do I root my phone?   \n",
      "76434               How will the ban on 500 and 1000 rupee notes effect land/house rates?   \n",
      "60767                                           How many sports bars are there in the US?   \n",
      "42141                                                    What is affective communication?   \n",
      "1761                                                   Can skipping increase your height?   \n",
      "64883                                                    Why does Quora remove questions?   \n",
      "80296                                               In C++, what does a call function do?   \n",
      "14888  How it will effect Indian economy after banning rs.500 and rs.1000 currency bills?   \n",
      "94892                                      How do you add a video to a question on Quora?   \n",
      "84936                                                           How can I flirt on Quora?   \n",
      "\n",
      "                                                                                        question2  \\\n",
      "75721                                                    What should I do after rooting my phone?   \n",
      "76434  How will abolishing Rs. 500 and Rs. 1000 notes affect the real estate businesses in India?   \n",
      "60767                                                 How many sports bars are there in the U.S.?   \n",
      "42141                                                      How can lighting affect communication?   \n",
      "1761                                    What is the best and fastest way to increase your height?   \n",
      "64883     What kind of questions on Quora aren't OK? What is Quora's policy on question deletion?   \n",
      "80296                                                            What is a calling function in C?   \n",
      "14888       What effect will the rupee 500 and 1000 currency note ban have on the Indian economy?   \n",
      "94892                                  How can I add multimedia to Quora while asking a question?   \n",
      "84936                                                 What are some good ways to flirt via Quora?   \n",
      "\n",
      "       similarity  pruned_similarity  weight_q1  weight_q2  common_weight  \n",
      "75721    0.666667           0.888889   2.018962   2.325742       1.618464  \n",
      "76434    0.540881           0.487805   3.485516   3.446076       2.427153  \n",
      "60767    0.952381           0.863636   2.595295   2.397591       2.397591  \n",
      "42141    0.714286           0.785714   1.582235   1.968738       1.000000  \n",
      "1761     0.549451           0.633333   2.056061   2.921762       1.689578  \n",
      "64883    0.235294           0.247191   2.136588   3.133079       1.410445  \n",
      "80296    0.637681           0.682927   2.129078   1.843074       1.000000  \n",
      "14888    0.431138           0.478632   3.413739   3.663620       2.342187  \n",
      "94892    0.461538           0.531250   2.734950   2.670186       2.038949  \n",
      "84936    0.500000           0.650000   1.905308   2.639840       1.341999  \n"
     ]
    }
   ],
   "source": [
    "print (prediction_actual[prediction_actual['is_duplicate_predicted'] != prediction_actual['is_duplicate_actual']][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c4aafd87-e09f-54b4-513e-70bef6cec118"
   },
   "source": [
    "As we can see, this approach needs to be supplemented by other metrics. The types of errors we are likely to encounter are:\n",
    "\n",
    " - Cases where the weights of two contextually different expressions are similar (e.g. pair 28). In this case 'ask for' and 'make' may have very similar weights due to similar counts of the term throughout the corpus, but have a fundamentally different meaning.\n",
    " - Algorithmic errors - where the features indicate difference to an observer but not to the algorithm (e.g. pair 24 where the similarity is 48% and the weight ratio is 75%). This could potentially be improved by tweaking the training parameters, adding more training data and executing more epochs.\n",
    "\n",
    "In addition, our data derivation has several shortcomings. Namely: we have done no canonization of the terms in the corpus. This means that the following terms will be considered different (and have different counts and weights according to the tf-idf):\n",
    "\n",
    " - 2016-12-01 and 1st of December 2016\n",
    " - Youtube and YouTube\n",
    " - india and India\n",
    "\n",
    "This problem can be solved through a similarity matching and some regular expressions.\n",
    "\n",
    "Another issue we haven't addressed is the semantic closeness of terms in the question pairs for cases like:\n",
    "\n",
    " - Holland and The Netherlands\n",
    " - Holland and France (both may have equal frequency in the corpus and equal weights but have different meaning)\n",
    "\n",
    "This problem can be resolved through vectorization of the terms and taking cosine of their values.\n",
    "\n",
    "Unfortunately those tasks are beyond the allocated time or hardware of my current participation (30 hours and Acer Revo One, respectively), but had time been abundantly available, I would work on the following additional features:\n",
    "\n",
    " - Regular expression parser to canonize the training and test corpus\n",
    " - Cosine of the terms of each question pair\n",
    " - N-gram derivation and comparison\n",
    "\n",
    "I'm eager to hear your constructive criticism and suggestions for improvement!"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 3,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
