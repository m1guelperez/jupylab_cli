{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Getting Started with Automated Data Pipelines series is a set of three notebooks and livestreams (recordings are available) designed to help you get started with creating data pipeline that allow you to automate the process of moving and transforming data.\n",
    "\n",
    "* Day 1: Versioning & Creating Datasets from GitHub Repos\n",
    "    * [Notebook](https://www.kaggle.com/rtatman/kerneld4769833fe/)\n",
    "    * [Livestream](https://youtu.be/Xi140XVOznM)\n",
    "* Day 2: Validation & Creating Datasets from URL's\n",
    "    * [Notebook](https://www.kaggle.com/rtatman/automating-data-pipelines-day-2)\n",
    "    * [Livestream](https://youtu.be/-wF1hSEQqIc)\n",
    "* Day 3: ETL & Creating Datasets from Kernel Output\n",
    "    * [Notebook](https://www.kaggle.com/rtatman/automating-data-pipelines-day-3)\n",
    "    * [Livestream](https://youtu.be/2pWifnSPN5E)    \n",
    "_____\n",
    "\n",
    "Welcome to the third and final day of the Getting Started with Automated Data Pipelines! Today we put it all together and create a simple pipeline that applies all things we’ve talked about the last couple days.\n",
    "\n",
    "Today we're going to cover two things: \n",
    "\n",
    "* Basic principles of Extract, Transform & Load (aka ETL) pipelines\n",
    "* Creating datasets from Kaggle Kernel outputs\n",
    "\n",
    "I’ll be going over this notebook live at 9:00 AM Pacific time on January, 31 2019. [Here’s a link to the livestream, which should also point to the recording if you miss the livestream](https://youtu.be/2pWifnSPN5E). \n",
    "\n",
    "# Extract, Transform & Load\n",
    "\n",
    "So far in this series, we’ve talked about specific pieces of data pipelines: data versioning and data validation. But how can we connect these together? How does data cleaning fit into all of this? How should you go about setting up your pipeline? \n",
    "\n",
    "One commonly used framework for setting up pipeline is “Extract, Transform and Load”, or ETL. As you might guess from the name, there are three stages to an ETL pipeline. \n",
    "\n",
    "* **Extracting**: Get the raw data you need from where it’s being stored. \n",
    "* **Transforming**: Rearrange that data to fit your needs.\n",
    "* **Loading**: Storing your transformed data in a different place so it can be used. \n",
    "\n",
    "This general framework for thinking about moving data has been around for a pretty long time; [it started getting popular in the 1970’s](https://www.sas.com/en_us/insights/data-management/what-is-etl.html). A lot of the specific best practices around ETL are designed to be more useful for people building and maintaining databases which I wouldn’t personally consider the core duty of a data scientist’s job. That said, we can still apply a lot of the ideas of ETL in a data science workflow.  \n",
    "\n",
    "## Extract\n",
    "\n",
    "The extract step is probably the one where there’s the most variation: data comes in a huge range of formats, each with their own way of interacting with them. \n",
    "\n",
    "On Kaggle, the “extract” step generally means creating a dataset, like we’ve done the last two days by adding datasets from URLs and GitHub repos. In other contexts, it’s generally more common to get data programmatically from an API or from a database, perhaps by using SQL. ([I’ve done a whole series of beginner SQL lessons](https://www.kaggle.com/rtatman/sql-scavenger-hunt-handbook/) if you’re unfamiliar with the language.)\n",
    "\n",
    "I can give you some general tips that should be applicable regardless, though:\n",
    "\n",
    "* Make sure to *save and version the scripts you use for data extraction*. Whether that’s web scraping or SQL queries, you want to make sure you know how you actually got your data. \n",
    "* If you’re extracting a small dataset from a much larger one (say, you’re using one of the public BigQuery databases on Kaggle) then, depending on how long it takes to extract the data, it might be a good idea to save a version of the smaller dataset.\n",
    "* I’d probably **do data validation during the extraction step** to make sure that I actually did get the data I thought I did.\n",
    "\n",
    "## Transform\n",
    "\n",
    "The transformation step is part of what I’d consider “data cleaning”: we modify so that it’s in the format we want. If you’re working with data you’re already familiar with, you probably have a good idea what you want it to look like and how to get it there.\n",
    "\n",
    "If you’re looking for something to practice, though, I’ve also written a couple of data cleaning tutorials you might find helpful:\n",
    "\n",
    "* [Drop or impute missing values](https://www.kaggle.com/rtatman/data-cleaning-challenge-handling-missing-values) ([R version](https://www.kaggle.com/rtatman/data-cleaning-challenge-imputing-missing-values/))\n",
    "* [Scale or normalize your data](https://www.kaggle.com/rtatman/data-cleaning-challenge-scale-and-normalize-data)\n",
    "* [Parsing dates](https://www.kaggle.com/rtatman/data-cleaning-challenge-parsing-dates/)\n",
    "* [Detect character encodings](https://www.kaggle.com/rtatman/data-cleaning-challenge-character-encodings/)\n",
    "* [Tidy up inconsistent data entry](https://www.kaggle.com/rtatman/data-cleaning-challenge-inconsistent-data-entry/)\n",
    "* [Identify outliers and figure out how to deal with them](https://www.kaggle.com/rtatman/data-cleaning-challenge-outliers/)\n",
    "* [Remove duplicate records](https://www.kaggle.com/rtatman/data-cleaning-challenge-deduplication/) \n",
    "* [Correctly parse numeric fields, like percentages or money amounts](https://www.kaggle.com/rtatman/data-cleaning-challenge-cleaning-numeric-columns/)\n",
    "\n",
    "Once your data is cleaned, however, you need to have some way to share and work with just the clean version.\n",
    "\n",
    "## Load\n",
    "\n",
    "Finally, we have loading. This means saving a fresh copy of your transformed data. (I like to think of this as your “Sunday best” data; this is the version you want to show to guests!)\n",
    "\n",
    "On Kaggle you can save your cleaned data by writing your files to disk. (For example, by using the `to_csv` function in Pandas.) Once you commit your kernel, the new version of your notebook will have a data file associated with it that you can use to create a new dataset! \n",
    "\n",
    "# What does ETL actually look like for a data scientist?\n",
    "\n",
    "Great question! Obviously there’s going to be variation based on your specific needs, but if you’re working on Kaggle, the general process will probably look like this.  \n",
    "\n",
    "* **Extract:** \n",
    "    * Create a Kaggle dataset with the data you’re interested in. For example, I created [this dataset of livestock auction data](https://www.kaggle.com/rebeccaturner/lamb-auction-data) from URL endpoints provided by the USDA. \n",
    "    * Validate your data (using either a script or notebook). For the data in my example, I wrote [a script to automate data validation](https://www.kaggle.com/rtatman/virginia-lamb-data-validation-script?scriptVersionId=10091127).\n",
    "* **Transform:** \n",
    "    * Perform any necessary data cleaning. For my example, I did both my validation (by importing and calling the functions from my script) and data cleaning in [a single kernel here](https://www.kaggle.com/rtatman/sample-extract-and-transform-for-lamb-data). \n",
    "* **Load:**\n",
    "    * Save your cleaned data to disk in your kernel and commit your kernel.\n",
    "    * Create a new dataset from the output of your kernel. I [created a dataset from my cleaned dataset here](https://www.kaggle.com/rtatman/cleaned-va-sheep-livestock-data) that I can use as the input for other projects.\n",
    "* **From there you’re ready to get your stats/machine learning on!**\n",
    "\n",
    "There may be some differences in your particular use case (for example, you may want to combine data cleaning and validation step) but this general outline should work for most projects.\n",
    "\n",
    "# Your turn!\n",
    "\n",
    "Now that we’ve talked about the theory and I’ve shown you an example, you should be ready to set up your own pipeline. :)\n",
    "\n",
    "* Find a dataset you’re interested in working with (possibly one you uploaded during day one or two).\n",
    "* Write a kernel to do some sort of data cleaning for your data. Get it to the stage where you’d be ready to start working on visualization or modelling. \n",
    "* Save your cleaned file and commit your kernel. \n",
    "* Create a new dataset from your file output. \n",
    "    *  Go to www.kaggle.com/datasets (or just click on the \"Datasets\" tab up near the search bar). \n",
    "    * Click on \"New Dataset\".\n",
    "    * In the modal that pops up, click on the “</>” symbol.\n",
    "    * Search for your kernel and add the files you’d like to be in your dataset. \n",
    "    * Hit create. \n",
    "* Nice work, you’ve made a data pipeline!\n",
    "\n",
    "I encourage you to share your own pipelines in the comments once they’re done; I’d love to see what you’ve come up with! \n",
    "\n",
    "(I also personally think little sample pipelines make really great data science portfolio pieces; they show you’re thinking about modular, professional code and have a handle on data cleaning, which can sometimes be hard to highlight.)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
