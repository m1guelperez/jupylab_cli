{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 政务文本分类\n",
    "## 零、数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入第三方工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import openpyxl  # 读取excel的工具\n",
    "import jieba  # 中文分词工具\n",
    "import sqlite3  # 数据库引擎\n",
    "import nltk  # 英文nlp工具\n",
    "import gensim  # 词向量化工具\n",
    "import sklearn  # sci-kit learn机器学习工具\n",
    "\n",
    "import matplotlib.pyplot as plt  # 绘图工具\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号\n",
    "\n",
    "path_cc = \"/kaggle/input/citizen-comment/citizen-comments-v2/\"  # 数据集路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表格读取方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def read_xl_by_line(path: str, skip_first_line=True):\n",
    "    \"\"\"\n",
    "    读取表（通用）\n",
    "    :param path: excel文件路径\n",
    "    :param skip_first_line: 是否跳过首行\n",
    "    :return: 列表，元素为n元组\n",
    "    \"\"\"\n",
    "    res_rows = []\n",
    "    wb = openpyxl.load_workbook(path, read_only=True)\n",
    "    ws = wb[wb.sheetnames[0]]\n",
    "\n",
    "    if skip_first_line:\n",
    "        row = ws[2:ws.max_row]  # 跳过第一行（表头）\n",
    "    else:\n",
    "        row = ws[1:ws.max_row]\n",
    "\n",
    "    for r in row:\n",
    "        # 将openpyxl的内置对象转化为元组\n",
    "        row_tuple = tuple(map(lambda x: x.value, r))\n",
    "        if row_tuple[0] is None:  # 到达空行时退出\n",
    "            break\n",
    "        res_rows.append(row_tuple)\n",
    "    return res_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取四个附件表格到列表对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取四个附件表\n",
    "labels = read_xl_by_line(path_cc + \"xls/e1.xlsx\")\n",
    "comments = read_xl_by_line(path_cc + \"xls/e2.xlsx\")\n",
    "comments_with_likes = read_xl_by_line(path_cc + \"xls/e3.xlsx\")\n",
    "comments_with_reply = read_xl_by_line(path_cc + \"xls/e4.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、预处理\n",
    "### 1.1 中文分词\n",
    "这里使用jieba（结巴）中文分词工具\n",
    "\n",
    "+ 附件一（标签体系）不需要分词\n",
    "+ 附件二、三中的“主题”、“详情”需要处理\n",
    "+ 附件四中的“主题”、“详情”、“答复意见”需要处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.559 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Text: \n",
       " \t 位于 书院 路 主干道 的 在水一方...>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 效果试验\n",
    "words = list(jieba.cut(comments[1][4]))  # 默认模式，即精准模式\n",
    "\n",
    "# 尝试用nltk中的FreqDist进行词典生成\n",
    "fd = nltk.FreqDist(words)\n",
    "comment_text = nltk.Text(fd)\n",
    "comment_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便留言的处理，采用面向对象的思想，将附件中的每一行抽象成为一个“留言”对象\n",
    "\n",
    "后面用到的“分词”、“去停用词”等方法也写进了类中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立“留言”对象，映射表中的每一行（留言）\n",
    "# 同时储存分词、标签等处理结果\n",
    "class Comm(object):\n",
    "    \"\"\"留言\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"构造方法\"\"\"\n",
    "        self.comm_id = None  # 留言编号\n",
    "        self.user_id = None  # 用户编号\n",
    "        self.topic = None  # 留言主题\n",
    "        self.date = None  # 留言时间\n",
    "        self.detail = None  # 留言详情\n",
    "        self.fir_lev_label = None  # 一级标签\n",
    "        self.likes = None  # 点赞数\n",
    "        self.treads = None  # 反对数\n",
    "        self.reply = None  # 答复\n",
    "        self.reply_date = None  # 答复时间\n",
    "        \n",
    "        self.seg_topic = None  # 分词后的留言主题\n",
    "        self.seg_detail = None  # 分词后的留言详情\n",
    "        self.seg_reply = None  # 分词后的留言回复classmethod\n",
    "    \n",
    "    def load_from_row(self, row):\n",
    "        \"\"\"从行元组加载实例\"\"\"\n",
    "        if len(row) == 6:\n",
    "            # 六元组是附件二的格式\n",
    "            self.comm_id, self.user_id, self.topic, \\\n",
    "                self.date, self.detail, self.fir_lev_label = row\n",
    "        if len(row) == 7:\n",
    "            # 附件三和附件四都是七元组\n",
    "            if isinstance(row[5], int):\n",
    "                # 附件三的第六列是点赞数，是整型数据\n",
    "                self.comm_id, self.user_id, self.topic, \\\n",
    "                self.date, self.detail, self.likes, self.treads = row\n",
    "            else:\n",
    "                # 附件四\n",
    "                self.comm_id, self.user_id, self.topic, \\\n",
    "                self.date, self.detail, self.reply, self.reply_date = row\n",
    "                \n",
    "    def cut(self, cut_all=False, stop_words_lt=None):\n",
    "        \"\"\"分词并去停用词\"\"\"\n",
    "        # 跳过无意义的行\n",
    "        if self.comm_id is None:\n",
    "            return\n",
    "        \n",
    "        # 分别对主题、详情、回复字段分词\n",
    "        self.seg_topic = jieba.lcut(self.topic, cut_all=cut_all)\n",
    "        self.seg_detail = jieba.lcut(self.detail, cut_all=cut_all)\n",
    "        if self.reply is not None:\n",
    "            self.seg_reply = jieba.lcut(self.reply, cut_all=cut_all)\n",
    "        \n",
    "        # 去停用词\n",
    "        if stop_words_lt is not None:\n",
    "            # 扫描主题\n",
    "            self.seg_topic = [word for word in self.seg_topic if word not in stop_words_lt]\n",
    "            # 扫描详情\n",
    "            self.seg_detail = [word for word in self.seg_detail if word not in stop_words_lt]\n",
    "            # 扫描回复\n",
    "            if self.reply is not None:\n",
    "                self.seg_reply = [word for word in self.seg_reply if word not in stop_words_lt]\n",
    "                \n",
    "    def get_vec(self, model):\n",
    "        \"\"\"计算词向量\"\"\"\n",
    "        vec = []\n",
    "        topic_vec = []\n",
    "        detail_vec = []\n",
    "        \n",
    "        for word in self.seg_topic:\n",
    "            try:\n",
    "                word_vec = model[word]\n",
    "            except KeyError:  # 跳过未登录词\n",
    "                continue\n",
    "            topic_vec.append(word_vec)\n",
    "        for word in self.seg_detail:\n",
    "            try:\n",
    "                word_vec = model[word]\n",
    "            except KeyError:  # 跳过未登录词\n",
    "                continue\n",
    "            detail_vec.append(word_vec)\n",
    "        vec.append(topic_vec)\n",
    "        vec.append(detail_vec)      \n",
    "\n",
    "        if self.seg_reply is not None:\n",
    "            reply_vec = []\n",
    "            for word in self.seg_reply:\n",
    "                try:\n",
    "                    word_vec = model[word]\n",
    "                except KeyError:  # 跳过未登录词\n",
    "                    continue\n",
    "                reply_vec.append(word_vec)\n",
    "            vec.append(reply_vec)\n",
    "        return np.array(vec, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 去停用词\n",
    "这里使用的停用词表来自哈工大停用词表, 百度停用词表, 四川大学机器智能实验室停用词库的整合，共有2312个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"', '#', '$', '&', \"'\", '(', ')', '*', '+', ',']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义加载停用词表的方法\n",
    "def load_word_list(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]\n",
    "    return stopwords \n",
    "\n",
    "# 加载停用词到列表\n",
    "stop_words_lt = load_word_list(path_cc + \"special-words/stop_words.txt\")\n",
    "\n",
    "stop_words_lt[:10]  # 展示前十个"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~考虑到本题中的地名（大写字母加数字的形式）没有意义，把他们也放到停用词~~\n",
    "\n",
    "地名（城市名、市区名等）可能与题目2的热度问题有关，暂且不去除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大写字母表\n",
    "uppercase = [chr(ch) for ch in range(ord(\"A\"), ord(\"Z\")+1)]\n",
    "# 大写字母加一位数字的形式\n",
    "city_names = [letter+num \n",
    "              for letter in uppercase\n",
    "              for num in [chr(n) for n in range(ord(\"0\"), ord(\"9\")+1)]]\n",
    "\n",
    "# 将它们放入停用词表（4.11：暂时不添加）\n",
    "# stop_words_lt.extend(uppercase + city_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从行生成Comm字典的方法\n",
    "def generate_comm_dict(row_lt, cut_all=False, stop_words_lt=None):\n",
    "    \"\"\"从数据行生成key为留言编号，value为Comm对象的字典\n",
    "    并对字典中的Comm对象执行分词、去停用词等预处理\"\"\"\n",
    "    comm_dict = {}\n",
    "    for row in row_lt:\n",
    "        c = Comm()\n",
    "        c.load_from_row(row)\n",
    "        c.cut(cut_all=cut_all, stop_words_lt=stop_words_lt)\n",
    "        comm_dict[row[0]] = c\n",
    "    return comm_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑到切分歧义，这里尝试使用jieba的全模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为附件三、四执行同样的映射和预处理\n",
    "comm_dict_2 = generate_comm_dict(comments, cut_all=True, stop_words_lt=stop_words_lt)\n",
    "comm_dict_3 = generate_comm_dict(comments_with_likes, cut_all=True, stop_words_lt=stop_words_lt)\n",
    "comm_dict_4 = generate_comm_dict(comments_with_reply, cut_all=True, stop_words_lt=stop_words_lt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看一个位于附件三的评论的主题，可以看到已经完成分词和去停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '市', '人才', 'app', '申请', '购房', '补贴', '通不过']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm_dict_3[289408].seg_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 向量化（特征工程）\n",
    "使用word2vec训练词嵌入（word embedding）模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['教育文体', '城乡建设', '商贸旅游', '劳动和社会保障', '交通运输', '环境保护', '卫生计生']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取表2中标注的七个一级标签\n",
    "labels_in_e2 = list(set([c.fir_lev_label for c in comm_dict_2.values()]))\n",
    "labels_in_e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把表2中的“主题”和“详情”文本“处理成gensim的lineSentence形式，\n",
    "# 并输出到文本文件\n",
    "text_file = open(\"./e2_line_sents.txt\", \"w\", encoding=\"utf8\")\n",
    "for c in comm_dict_2.values():\n",
    "    text_file.write(\" \".join(c.seg_topic) + \"\\n\")\n",
    "    text_file.write(\" \".join(c.seg_detail) + \"\\n\")\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型参数表\n",
    "\n",
    "+ LineSentence(inp)：格式简单：一句话=一行; 单词已经过预处理并被空格分隔。\n",
    "+ size：是每个词的向量维度； \n",
    "+ window：是词向量训练时的上下文扫描窗口大小，窗口为5就是考虑前5个词和后5个词； \n",
    "+ min-count：设置最低频率，默认是5，如果一个词语在文档中出现的次数小于5，那么就会丢弃； \n",
    "+ workers：是训练的进程数（需要更精准的解释，请指正），默认是当前运行机器的处理器核数。这些参数先记住就可以了。\n",
    "+ sg ({0, 1}, optional) – 模型的训练算法: 1: skip-gram; 0: CBOW\n",
    "+ alpha (float, optional) – 初始学习率\n",
    "+ iter (int, optional) – 迭代次数，默认为5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "# 使用表2中的语料建立模型，使用CBOW\n",
    "word2vec_model = Word2Vec(LineSentence(\"./e2_line_sents.txt\"), size=400, window=5, sg=0, min_count=5)  # 训练模型\n",
    "# word2vec_model.save(\"./word2vec_model\")\n",
    "word2vec_model.wv.save_word2vec_format(\"./word2vec_e2_model\", binary=False)  # 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('K', 0.9999104142189026),\n",
       " ('市', 0.9999015927314758),\n",
       " ('换乘', 0.9998969435691833),\n",
       " ('车站', 0.9998964667320251),\n",
       " ('城市', 0.9998923540115356),\n",
       " ('区', 0.9998903870582581),\n",
       " ('线', 0.9998894333839417),\n",
       " ('A', 0.9998855590820312),\n",
       " ('火车', 0.9998847842216492),\n",
       " ('建议', 0.9998830556869507)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试模型\n",
    "# 输出与某个词相近的10个词\n",
    "test_words = \"机场\"\n",
    "word2vec_model.wv.most_similar(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "考虑到表2、3、4中都包含留言文本，尝试使用三个表的文本（而不是只用表2）来训练词嵌入模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将三个表的语料输出到文本文件\n",
    "text_file = open(\"./line_sents.txt\", \"w\", encoding=\"utf8\")\n",
    "for d in (comm_dict_2, comm_dict_3, comm_dict_4):\n",
    "    for c in d.values():\n",
    "        text_file.write(\" \".join(c.seg_topic) + \"\\n\")\n",
    "        text_file.write(\" \".join(c.seg_detail) + \"\\n\")\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练word embedding模型\n",
    "# sg=1，使用Skip-Gram模式（小规模语料适用）\n",
    "word2vec_build_on_all_text = Word2Vec(LineSentence(\"./line_sents.txt\"), size=400, window=5, sg=1, min_count=5)  # 训练模型\n",
    "word2vec_build_on_all_text.wv.save_word2vec_format(\"./word2vec_build_on_all_text\", binary=False)  # 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('换乘', 0.9867381453514099),\n",
       " ('K1', 0.9864612817764282),\n",
       " ('地级', 0.9829998016357422),\n",
       " ('城区', 0.9810256361961365),\n",
       " ('火车站', 0.9803483486175537),\n",
       " ('楚江', 0.9796959757804871),\n",
       " ('地级市', 0.9791610836982727),\n",
       " ('火车', 0.9775784015655518),\n",
       " ('桥', 0.9766429662704468),\n",
       " ('A3', 0.9757394194602966)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试模型\n",
    "# 输出与某个词相近的10个词\n",
    "word2vec_build_on_all_text.wv.most_similar(\"机场\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、分类模型\n",
    "### 2.1 KNN模型操作练习\n",
    "使用Sklearn自带的鸢尾花分类案例练习KNN模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# 加载鸢尾花数据集\n",
    "iris = sklearn.datasets.load_iris()\n",
    "# 打印鸢尾花数据集的说明信息\n",
    "print(iris[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把数据集划分成训练集与测试集\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X是 \\[sample, feature\\] 形式的数组，y是 target 数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.7, 1.5, 0.4],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [6.4, 2.9, 4.3, 1.3]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看X数组的结构\n",
    "X_train[:10]  # 显示前10个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, 1, 2, 2, 1, 2, 0, 1, 1, 0, 1, 2, 2, 2, 1, 2, 1, 0, 0, 1,\n",
       "       0, 2, 2, 0, 1, 0, 0, 1, 1, 0, 2, 2, 1, 2, 1, 2, 1, 0, 0, 1, 1, 1,\n",
       "       2, 0, 1, 0, 1, 1, 1, 2, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 2,\n",
       "       2, 0, 0, 2, 1, 0, 2, 0, 2, 1, 0, 2, 1, 0, 1, 0, 2, 1, 0, 1, 2, 0,\n",
       "       1, 1, 0, 1, 1, 2, 2, 2, 2, 1, 0, 2, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看y数组的结构\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，在鸢尾花例中，分类目标（target）共有三种（0, 1, 2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入KNN模型\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# 实例化一个KNN模型\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 填充训练数据进行训练\n",
    "knn.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 0, 2, 1, 0, 0, 2, 2, 1, 2, 0, 0, 0, 2, 0, 2, 2, 0, 2, 2,\n",
       "       1, 1, 0, 2, 1, 0, 0, 0, 0, 1, 2, 0, 2, 1, 2, 0, 0, 0, 2, 2, 2, 2,\n",
       "       0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用模型预测\n",
    "knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Naive Bayes模型操作练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = ['alt.atheism', 'soc.religion.christian', \\\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "# 加载sklearn自带的新闻语料\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "    categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "    categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 35788)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载sklearn自带的向量化工具\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 构造向量化工具\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "# 拟合模型\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面shape字段的元组(2257, 35788)表示共有2257篇训练文章，词典的大小为35788维（有这么多个不重复的词）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365886"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建TF-TDF特征\n",
    "# 加载sklearn自带的TF-TDF构造器\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# 用训练数据进行拟合\n",
    "tf_transformer = TfidfTransformer().fit(X_train_counts)\n",
    "# 构建TF-IDF特征\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "\n",
    "len(X_train_tf.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF特征将每个词在当前文档中的重要程度给计算出来了，而且归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建朴素贝叶斯分类器\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# 使用特征集（X）和目标（target）训练/拟合一个朴素贝叶斯分类器\n",
    "clf = MultinomialNB().fit(X_train_tf, twenty_train.target)\n",
    "\n",
    "clf  # 分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用训练好的模型预测测试集\n",
    "\n",
    "先将测试集的数据也转化成TF-IDF特征的形式，且需要用刚才构造的转换器\n",
    "\n",
    "而转化TF-IDF之前，还要先将语料向量化，要使用之前构造的转换器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, ..., 2, 2, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预测测试\n",
    "# 将测试集数据向量化并转化成TF-IDF的形式\n",
    "test_tf = tf_transformer.transform(count_vect.transform(twenty_test.data))\n",
    "\n",
    "# 使用模型预测\n",
    "predicted = clf.predict(test_tf)\n",
    "\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicted集合中即为预测的结果，每个分量代表一个预测分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.97      0.60      0.74       319\n",
      "         comp.graphics       0.96      0.89      0.92       389\n",
      "               sci.med       0.97      0.81      0.88       396\n",
      "soc.religion.christian       0.65      0.99      0.78       398\n",
      "\n",
      "              accuracy                           0.83      1502\n",
      "             macro avg       0.89      0.82      0.83      1502\n",
      "          weighted avg       0.88      0.83      0.84      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用sklearn自带的评价工具来评价分类的结果\n",
    "from sklearn import metrics  # 引入评价工具\n",
    "\n",
    "print(metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评价结果分别展示了四个分类的准确率、召回率等指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 使用表2的语料训练KNN模型（TF-IDF）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造文本列表\n",
    "# 列表的格式为“单条评论词链表”的列表\n",
    "# train_text = [comm_dict_2[row[0]].seg_topic + comm_dict_2[row[0]].seg_detail for row in comments]\n",
    "train_text = [row[2] + row[4] for row in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<495x15430 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 16320 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 尝试使用TF-IDF构造特征\n",
    "count_vect = CountVectorizer()\n",
    "train_count = count_vect.fit_transform(train_text)\n",
    "train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# 构建TF-IDF转换器\n",
    "tf_transformer = TfidfTransformer().fit(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把训练集转化成tf-idf\n",
    "train_tf = tf_transformer.transform(train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['教育文体', '城乡建设', '商贸旅游', '劳动和社会保障', '交通运输', '环境保护', '卫生计生']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取训练集中的全部标签\n",
    "train_target_names = list(set(list(map(lambda x: x[5], comments))))\n",
    "train_target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将标签转换为数字序号\n",
    "train_target = [train_target_names.index(comm_dict_2[row[0]].fir_lev_label) for row in comments]\n",
    "train_target[98] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从训练集中分出一部分作为测试集\n",
    "train_tf, test_tf, train_target, test_target = sklearn.model_selection.train_test_split(train_tf, train_target, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 引入KNN模型\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "# 训练模型\n",
    "knn.fit(train_tf, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测语料的分类\n",
    "test_predicted = knn.predict(test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<149x15430 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4228 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        教育文体       0.21      0.16      0.18        31\n",
      "        城乡建设       0.20      0.17      0.18        30\n",
      "        商贸旅游       0.00      0.00      0.00        19\n",
      "     劳动和社会保障       0.55      0.17      0.26        35\n",
      "        交通运输       0.10      0.56      0.17        16\n",
      "        环境保护       0.00      0.00      0.00         6\n",
      "        卫生计生       0.00      0.00      0.00        12\n",
      "\n",
      "    accuracy                           0.17       149\n",
      "   macro avg       0.15      0.15      0.11       149\n",
      "weighted avg       0.22      0.17      0.15       149\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# 评价模型\n",
    "from sklearn import metrics \n",
    "print(metrics.classification_report(test_target, test_predicted, target_names=train_target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 从Word2Vec向量得到文档向量\n",
    "求和求平均得到文档向量：\n",
    "对每一篇文章的词向量进行求和，然后除以词数量，得到文档向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载之前训练的Word2Vec模型\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"/kaggle/working/word2vec_e2_model\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义计算文档向量的方法\n",
    "def get_doc_vec(doc: list, model):\n",
    "    \"\"\"计算文档向量\"\"\"\n",
    "    ignore = [\"\\t\", \" \", \"\\n\"]\n",
    "    words = [word for word in doc if word not in ignore]\n",
    "    # 所有词向量求和并除以词数量\n",
    "    words_num = len(words)\n",
    "    \n",
    "    print(words_num)\n",
    "    \n",
    "    vec_sum = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            vec_sum += model[word]\n",
    "        except KeyError:\n",
    "            words_num -= 1  # todo::是否应该跳过不在词典中的词？为什么会很多词不在词典中？\n",
    "            continue\n",
    "            \n",
    "    print(words_num)\n",
    "\n",
    "    return vec_sum / words_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "45\n",
      "\n",
      "79\n",
      "30\n",
      "\n",
      "501\n",
      "375\n",
      "\n",
      "100\n",
      "65\n",
      "\n",
      "81\n",
      "33\n",
      "\n",
      "115\n",
      "46\n",
      "\n",
      "123\n",
      "63\n",
      "\n",
      "82\n",
      "29\n",
      "\n",
      "112\n",
      "56\n",
      "\n",
      "1735\n",
      "1296\n",
      "\n",
      "1735\n",
      "1296\n",
      "\n",
      "83\n",
      "28\n",
      "\n",
      "85\n",
      "47\n",
      "\n",
      "154\n",
      "94\n",
      "\n",
      "90\n",
      "48\n",
      "\n",
      "63\n",
      "23\n",
      "\n",
      "86\n",
      "32\n",
      "\n",
      "125\n",
      "69\n",
      "\n",
      "68\n",
      "26\n",
      "\n",
      "100\n",
      "58\n",
      "\n",
      "74\n",
      "26\n",
      "\n",
      "143\n",
      "73\n",
      "\n",
      "160\n",
      "84\n",
      "\n",
      "106\n",
      "38\n",
      "\n",
      "182\n",
      "117\n",
      "\n",
      "50\n",
      "12\n",
      "\n",
      "95\n",
      "44\n",
      "\n",
      "192\n",
      "122\n",
      "\n",
      "56\n",
      "19\n",
      "\n",
      "318\n",
      "209\n",
      "\n",
      "501\n",
      "401\n",
      "\n",
      "109\n",
      "63\n",
      "\n",
      "227\n",
      "105\n",
      "\n",
      "546\n",
      "252\n",
      "\n",
      "92\n",
      "30\n",
      "\n",
      "75\n",
      "28\n",
      "\n",
      "107\n",
      "66\n",
      "\n",
      "796\n",
      "663\n",
      "\n",
      "184\n",
      "95\n",
      "\n",
      "78\n",
      "33\n",
      "\n",
      "340\n",
      "184\n",
      "\n",
      "524\n",
      "343\n",
      "\n",
      "299\n",
      "186\n",
      "\n",
      "407\n",
      "250\n",
      "\n",
      "77\n",
      "28\n",
      "\n",
      "353\n",
      "205\n",
      "\n",
      "241\n",
      "155\n",
      "\n",
      "450\n",
      "258\n",
      "\n",
      "589\n",
      "451\n",
      "\n",
      "114\n",
      "69\n",
      "\n",
      "77\n",
      "36\n",
      "\n",
      "171\n",
      "92\n",
      "\n",
      "635\n",
      "318\n",
      "\n",
      "261\n",
      "152\n",
      "\n",
      "88\n",
      "37\n",
      "\n",
      "144\n",
      "79\n",
      "\n",
      "120\n",
      "66\n",
      "\n",
      "579\n",
      "354\n",
      "\n",
      "144\n",
      "85\n",
      "\n",
      "133\n",
      "87\n",
      "\n",
      "87\n",
      "40\n",
      "\n",
      "348\n",
      "239\n",
      "\n",
      "550\n",
      "378\n",
      "\n",
      "533\n",
      "413\n",
      "\n",
      "236\n",
      "141\n",
      "\n",
      "91\n",
      "41\n",
      "\n",
      "193\n",
      "124\n",
      "\n",
      "465\n",
      "291\n",
      "\n",
      "1317\n",
      "1084\n",
      "\n",
      "99\n",
      "50\n",
      "\n",
      "456\n",
      "327\n",
      "\n",
      "443\n",
      "350\n",
      "\n",
      "939\n",
      "746\n",
      "\n",
      "90\n",
      "48\n",
      "\n",
      "188\n",
      "128\n",
      "\n",
      "94\n",
      "51\n",
      "\n",
      "178\n",
      "113\n",
      "\n",
      "79\n",
      "30\n",
      "\n",
      "58\n",
      "23\n",
      "\n",
      "76\n",
      "39\n",
      "\n",
      "48\n",
      "17\n",
      "\n",
      "137\n",
      "88\n",
      "\n",
      "65\n",
      "27\n",
      "\n",
      "217\n",
      "152\n",
      "\n",
      "149\n",
      "87\n",
      "\n",
      "631\n",
      "409\n",
      "\n",
      "76\n",
      "35\n",
      "\n",
      "98\n",
      "49\n",
      "\n",
      "73\n",
      "30\n",
      "\n",
      "89\n",
      "52\n",
      "\n",
      "70\n",
      "32\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279\n",
      "185\n",
      "\n",
      "354\n",
      "204\n",
      "\n",
      "92\n",
      "29\n",
      "\n",
      "85\n",
      "38\n",
      "\n",
      "171\n",
      "93\n",
      "\n",
      "188\n",
      "104\n",
      "\n",
      "417\n",
      "346\n",
      "\n",
      "60\n",
      "24\n",
      "\n",
      "156\n",
      "99\n",
      "\n",
      "891\n",
      "422\n",
      "\n",
      "353\n",
      "258\n",
      "\n",
      "66\n",
      "28\n",
      "\n",
      "71\n",
      "26\n",
      "\n",
      "92\n",
      "49\n",
      "\n",
      "163\n",
      "86\n",
      "\n",
      "123\n",
      "68\n",
      "\n",
      "139\n",
      "50\n",
      "\n",
      "304\n",
      "171\n",
      "\n",
      "133\n",
      "65\n",
      "\n",
      "254\n",
      "170\n",
      "\n",
      "112\n",
      "42\n",
      "\n",
      "348\n",
      "234\n",
      "\n",
      "711\n",
      "463\n",
      "\n",
      "1066\n",
      "747\n",
      "\n",
      "512\n",
      "349\n",
      "\n",
      "739\n",
      "472\n",
      "\n",
      "74\n",
      "27\n",
      "\n",
      "304\n",
      "166\n",
      "\n",
      "172\n",
      "88\n",
      "\n",
      "225\n",
      "104\n",
      "\n",
      "118\n",
      "76\n",
      "\n",
      "111\n",
      "53\n",
      "\n",
      "167\n",
      "89\n",
      "\n",
      "171\n",
      "103\n",
      "\n",
      "799\n",
      "622\n",
      "\n",
      "274\n",
      "161\n",
      "\n",
      "626\n",
      "355\n",
      "\n",
      "179\n",
      "105\n",
      "\n",
      "190\n",
      "113\n",
      "\n",
      "294\n",
      "187\n",
      "\n",
      "108\n",
      "69\n",
      "\n",
      "420\n",
      "320\n",
      "\n",
      "350\n",
      "225\n",
      "\n",
      "117\n",
      "64\n",
      "\n",
      "331\n",
      "166\n",
      "\n",
      "145\n",
      "77\n",
      "\n",
      "863\n",
      "535\n",
      "\n",
      "233\n",
      "143\n",
      "\n",
      "131\n",
      "73\n",
      "\n",
      "233\n",
      "127\n",
      "\n",
      "270\n",
      "174\n",
      "\n",
      "69\n",
      "32\n",
      "\n",
      "90\n",
      "48\n",
      "\n",
      "174\n",
      "98\n",
      "\n",
      "97\n",
      "56\n",
      "\n",
      "61\n",
      "26\n",
      "\n",
      "140\n",
      "63\n",
      "\n",
      "99\n",
      "41\n",
      "\n",
      "170\n",
      "107\n",
      "\n",
      "70\n",
      "24\n",
      "\n",
      "345\n",
      "199\n",
      "\n",
      "211\n",
      "115\n",
      "\n",
      "61\n",
      "13\n",
      "\n",
      "127\n",
      "61\n",
      "\n",
      "124\n",
      "76\n",
      "\n",
      "129\n",
      "71\n",
      "\n",
      "130\n",
      "69\n",
      "\n",
      "468\n",
      "271\n",
      "\n",
      "140\n",
      "83\n",
      "\n",
      "140\n",
      "82\n",
      "\n",
      "84\n",
      "36\n",
      "\n",
      "68\n",
      "23\n",
      "\n",
      "68\n",
      "26\n",
      "\n",
      "107\n",
      "43\n",
      "\n",
      "56\n",
      "13\n",
      "\n",
      "74\n",
      "32\n",
      "\n",
      "156\n",
      "95\n",
      "\n",
      "88\n",
      "41\n",
      "\n",
      "118\n",
      "60\n",
      "\n",
      "581\n",
      "387\n",
      "\n",
      "119\n",
      "51\n",
      "\n",
      "84\n",
      "34\n",
      "\n",
      "386\n",
      "145\n",
      "\n",
      "76\n",
      "37\n",
      "\n",
      "152\n",
      "89\n",
      "\n",
      "128\n",
      "70\n",
      "\n",
      "81\n",
      "39\n",
      "\n",
      "119\n",
      "64\n",
      "\n",
      "176\n",
      "87\n",
      "\n",
      "68\n",
      "28\n",
      "\n",
      "272\n",
      "213\n",
      "\n",
      "95\n",
      "47\n",
      "\n",
      "156\n",
      "86\n",
      "\n",
      "128\n",
      "61\n",
      "\n",
      "119\n",
      "53\n",
      "\n",
      "360\n",
      "234\n",
      "\n",
      "110\n",
      "55\n",
      "\n",
      "179\n",
      "102\n",
      "\n",
      "119\n",
      "60\n",
      "\n",
      "553\n",
      "358\n",
      "\n",
      "122\n",
      "67\n",
      "\n",
      "107\n",
      "48\n",
      "\n",
      "146\n",
      "90\n",
      "\n",
      "136\n",
      "76\n",
      "\n",
      "172\n",
      "99\n",
      "\n",
      "404\n",
      "279\n",
      "\n",
      "75\n",
      "38\n",
      "\n",
      "746\n",
      "437\n",
      "\n",
      "131\n",
      "73\n",
      "\n",
      "140\n",
      "82\n",
      "\n",
      "62\n",
      "26\n",
      "\n",
      "151\n",
      "95\n",
      "\n",
      "216\n",
      "147\n",
      "\n",
      "121\n",
      "80\n",
      "\n",
      "58\n",
      "22\n",
      "\n",
      "329\n",
      "250\n",
      "\n",
      "86\n",
      "45\n",
      "\n",
      "77\n",
      "35\n",
      "\n",
      "336\n",
      "238\n",
      "\n",
      "331\n",
      "199\n",
      "\n",
      "331\n",
      "150\n",
      "\n",
      "635\n",
      "427\n",
      "\n",
      "149\n",
      "89\n",
      "\n",
      "124\n",
      "69\n",
      "\n",
      "64\n",
      "24\n",
      "\n",
      "100\n",
      "53\n",
      "\n",
      "91\n",
      "46\n",
      "\n",
      "133\n",
      "73\n",
      "\n",
      "109\n",
      "57\n",
      "\n",
      "150\n",
      "81\n",
      "\n",
      "461\n",
      "292\n",
      "\n",
      "96\n",
      "58\n",
      "\n",
      "85\n",
      "40\n",
      "\n",
      "222\n",
      "129\n",
      "\n",
      "136\n",
      "89\n",
      "\n",
      "505\n",
      "411\n",
      "\n",
      "177\n",
      "109\n",
      "\n",
      "344\n",
      "218\n",
      "\n",
      "236\n",
      "157\n",
      "\n",
      "127\n",
      "76\n",
      "\n",
      "131\n",
      "72\n",
      "\n",
      "113\n",
      "68\n",
      "\n",
      "313\n",
      "169\n",
      "\n",
      "86\n",
      "38\n",
      "\n",
      "157\n",
      "92\n",
      "\n",
      "62\n",
      "20\n",
      "\n",
      "128\n",
      "61\n",
      "\n",
      "166\n",
      "100\n",
      "\n",
      "191\n",
      "125\n",
      "\n",
      "309\n",
      "222\n",
      "\n",
      "398\n",
      "289\n",
      "\n",
      "268\n",
      "155\n",
      "\n",
      "97\n",
      "48\n",
      "\n",
      "154\n",
      "85\n",
      "\n",
      "260\n",
      "165\n",
      "\n",
      "65\n",
      "23\n",
      "\n",
      "300\n",
      "187\n",
      "\n",
      "77\n",
      "29\n",
      "\n",
      "180\n",
      "96\n",
      "\n",
      "221\n",
      "127\n",
      "\n",
      "64\n",
      "31\n",
      "\n",
      "78\n",
      "33\n",
      "\n",
      "64\n",
      "24\n",
      "\n",
      "80\n",
      "40\n",
      "\n",
      "253\n",
      "120\n",
      "\n",
      "72\n",
      "31\n",
      "\n",
      "69\n",
      "29\n",
      "\n",
      "103\n",
      "60\n",
      "\n",
      "107\n",
      "50\n",
      "\n",
      "103\n",
      "45\n",
      "\n",
      "144\n",
      "56\n",
      "\n",
      "390\n",
      "228\n",
      "\n",
      "120\n",
      "51\n",
      "\n",
      "224\n",
      "111\n",
      "\n",
      "112\n",
      "66\n",
      "\n",
      "119\n",
      "53\n",
      "\n",
      "100\n",
      "36\n",
      "\n",
      "116\n",
      "47\n",
      "\n",
      "408\n",
      "270\n",
      "\n",
      "816\n",
      "647\n",
      "\n",
      "239\n",
      "96\n",
      "\n",
      "355\n",
      "203\n",
      "\n",
      "451\n",
      "266\n",
      "\n",
      "199\n",
      "137\n",
      "\n",
      "339\n",
      "196\n",
      "\n",
      "71\n",
      "17\n",
      "\n",
      "95\n",
      "52\n",
      "\n",
      "114\n",
      "41\n",
      "\n",
      "110\n",
      "48\n",
      "\n",
      "73\n",
      "26\n",
      "\n",
      "132\n",
      "60\n",
      "\n",
      "1610\n",
      "722\n",
      "\n",
      "83\n",
      "38\n",
      "\n",
      "324\n",
      "202\n",
      "\n",
      "51\n",
      "20\n",
      "\n",
      "168\n",
      "95\n",
      "\n",
      "84\n",
      "44\n",
      "\n",
      "129\n",
      "81\n",
      "\n",
      "106\n",
      "52\n",
      "\n",
      "189\n",
      "128\n",
      "\n",
      "842\n",
      "565\n",
      "\n",
      "79\n",
      "41\n",
      "\n",
      "151\n",
      "52\n",
      "\n",
      "208\n",
      "137\n",
      "\n",
      "76\n",
      "39\n",
      "\n",
      "141\n",
      "87\n",
      "\n",
      "133\n",
      "77\n",
      "\n",
      "81\n",
      "44\n",
      "\n",
      "60\n",
      "25\n",
      "\n",
      "62\n",
      "27\n",
      "\n",
      "126\n",
      "63\n",
      "\n",
      "278\n",
      "207\n",
      "\n",
      "89\n",
      "45\n",
      "\n",
      "267\n",
      "182\n",
      "\n",
      "117\n",
      "70\n",
      "\n",
      "75\n",
      "34\n",
      "\n",
      "1974\n",
      "1626\n",
      "\n",
      "111\n",
      "66\n",
      "\n",
      "541\n",
      "328\n",
      "\n",
      "60\n",
      "24\n",
      "\n",
      "150\n",
      "102\n",
      "\n",
      "161\n",
      "109\n",
      "\n",
      "234\n",
      "173\n",
      "\n",
      "74\n",
      "24\n",
      "\n",
      "70\n",
      "37\n",
      "\n",
      "54\n",
      "24\n",
      "\n",
      "77\n",
      "35\n",
      "\n",
      "125\n",
      "72\n",
      "\n",
      "450\n",
      "275\n",
      "\n",
      "69\n",
      "35\n",
      "\n",
      "236\n",
      "130\n",
      "\n",
      "58\n",
      "19\n",
      "\n",
      "144\n",
      "77\n",
      "\n",
      "101\n",
      "42\n",
      "\n",
      "105\n",
      "66\n",
      "\n",
      "214\n",
      "146\n",
      "\n",
      "56\n",
      "22\n",
      "\n",
      "136\n",
      "74\n",
      "\n",
      "61\n",
      "22\n",
      "\n",
      "221\n",
      "122\n",
      "\n",
      "92\n",
      "38\n",
      "\n",
      "55\n",
      "22\n",
      "\n",
      "259\n",
      "85\n",
      "\n",
      "75\n",
      "28\n",
      "\n",
      "52\n",
      "21\n",
      "\n",
      "258\n",
      "184\n",
      "\n",
      "412\n",
      "316\n",
      "\n",
      "110\n",
      "67\n",
      "\n",
      "104\n",
      "56\n",
      "\n",
      "152\n",
      "74\n",
      "\n",
      "84\n",
      "36\n",
      "\n",
      "82\n",
      "33\n",
      "\n",
      "342\n",
      "269\n",
      "\n",
      "85\n",
      "44\n",
      "\n",
      "1210\n",
      "812\n",
      "\n",
      "115\n",
      "44\n",
      "\n",
      "116\n",
      "54\n",
      "\n",
      "163\n",
      "92\n",
      "\n",
      "180\n",
      "118\n",
      "\n",
      "1230\n",
      "960\n",
      "\n",
      "404\n",
      "239\n",
      "\n",
      "939\n",
      "526\n",
      "\n",
      "750\n",
      "463\n",
      "\n",
      "102\n",
      "58\n",
      "\n",
      "98\n",
      "48\n",
      "\n",
      "74\n",
      "32\n",
      "\n",
      "76\n",
      "35\n",
      "\n",
      "297\n",
      "210\n",
      "\n",
      "210\n",
      "142\n",
      "\n",
      "311\n",
      "177\n",
      "\n",
      "744\n",
      "437\n",
      "\n",
      "546\n",
      "419\n",
      "\n",
      "146\n",
      "78\n",
      "\n",
      "137\n",
      "84\n",
      "\n",
      "82\n",
      "29\n",
      "\n",
      "242\n",
      "169\n",
      "\n",
      "350\n",
      "263\n",
      "\n",
      "285\n",
      "177\n",
      "\n",
      "142\n",
      "95\n",
      "\n",
      "112\n",
      "62\n",
      "\n",
      "585\n",
      "310\n",
      "\n",
      "88\n",
      "30\n",
      "\n",
      "181\n",
      "110\n",
      "\n",
      "91\n",
      "41\n",
      "\n",
      "145\n",
      "89\n",
      "\n",
      "146\n",
      "85\n",
      "\n",
      "92\n",
      "42\n",
      "\n",
      "118\n",
      "67\n",
      "\n",
      "94\n",
      "47\n",
      "\n",
      "345\n",
      "249\n",
      "\n",
      "90\n",
      "39\n",
      "\n",
      "132\n",
      "79\n",
      "\n",
      "106\n",
      "66\n",
      "\n",
      "80\n",
      "28\n",
      "\n",
      "66\n",
      "27\n",
      "\n",
      "80\n",
      "40\n",
      "\n",
      "109\n",
      "53\n",
      "\n",
      "127\n",
      "72\n",
      "\n",
      "177\n",
      "86\n",
      "\n",
      "181\n",
      "102\n",
      "\n",
      "96\n",
      "42\n",
      "\n",
      "298\n",
      "178\n",
      "\n",
      "124\n",
      "72\n",
      "\n",
      "88\n",
      "48\n",
      "\n",
      "139\n",
      "74\n",
      "\n",
      "80\n",
      "26\n",
      "\n",
      "121\n",
      "51\n",
      "\n",
      "96\n",
      "44\n",
      "\n",
      "62\n",
      "21\n",
      "\n",
      "596\n",
      "338\n",
      "\n",
      "203\n",
      "113\n",
      "\n",
      "197\n",
      "110\n",
      "\n",
      "342\n",
      "239\n",
      "\n",
      "406\n",
      "215\n",
      "\n",
      "85\n",
      "34\n",
      "\n",
      "101\n",
      "48\n",
      "\n",
      "109\n",
      "43\n",
      "\n",
      "943\n",
      "671\n",
      "\n",
      "72\n",
      "30\n",
      "\n",
      "96\n",
      "44\n",
      "\n",
      "88\n",
      "40\n",
      "\n",
      "106\n",
      "64\n",
      "\n",
      "232\n",
      "132\n",
      "\n",
      "360\n",
      "212\n",
      "\n",
      "113\n",
      "60\n",
      "\n",
      "163\n",
      "108\n",
      "\n",
      "559\n",
      "261\n",
      "\n",
      "91\n",
      "30\n",
      "\n",
      "87\n",
      "38\n",
      "\n",
      "62\n",
      "19\n",
      "\n",
      "110\n",
      "59\n",
      "\n",
      "325\n",
      "227\n",
      "\n",
      "100\n",
      "44\n",
      "\n",
      "140\n",
      "69\n",
      "\n",
      "378\n",
      "250\n",
      "\n",
      "211\n",
      "119\n",
      "\n",
      "247\n",
      "71\n",
      "\n",
      "136\n",
      "75\n",
      "\n",
      "64\n",
      "29\n",
      "\n",
      "73\n",
      "37\n",
      "\n",
      "110\n",
      "52\n",
      "\n",
      "125\n",
      "75\n",
      "\n",
      "49\n",
      "19\n",
      "\n",
      "116\n",
      "69\n",
      "\n",
      "54\n",
      "24\n",
      "\n",
      "71\n",
      "33\n",
      "\n",
      "552\n",
      "378\n",
      "\n",
      "206\n",
      "121\n",
      "\n",
      "83\n",
      "35\n",
      "\n",
      "64\n",
      "28\n",
      "\n",
      "178\n",
      "114\n",
      "\n",
      "107\n",
      "65\n",
      "\n",
      "89\n",
      "37\n",
      "\n",
      "122\n",
      "56\n",
      "\n",
      "130\n",
      "71\n",
      "\n",
      "179\n",
      "98\n",
      "\n",
      "662\n",
      "498\n",
      "\n",
      "480\n",
      "302\n",
      "\n",
      "246\n",
      "144\n",
      "\n",
      "101\n",
      "47\n",
      "\n",
      "171\n",
      "100\n",
      "\n",
      "71\n",
      "27\n",
      "\n",
      "137\n",
      "77\n",
      "\n",
      "73\n",
      "39\n",
      "\n",
      "199\n",
      "114\n",
      "\n",
      "230\n",
      "148\n",
      "\n",
      "109\n",
      "52\n",
      "\n",
      "119\n",
      "56\n",
      "\n",
      "84\n",
      "40\n",
      "\n",
      "162\n",
      "97\n",
      "\n",
      "178\n",
      "98\n",
      "\n",
      "95\n",
      "37\n",
      "\n",
      "84\n",
      "35\n",
      "\n",
      "84\n",
      "39\n",
      "\n",
      "218\n",
      "135\n",
      "\n",
      "110\n",
      "45\n",
      "\n",
      "113\n",
      "59\n",
      "\n",
      "85\n",
      "38\n",
      "\n",
      "467\n",
      "282\n",
      "\n",
      "95\n",
      "40\n",
      "\n",
      "1265\n",
      "861\n",
      "\n",
      "213\n",
      "117\n",
      "\n",
      "671\n",
      "351\n",
      "\n",
      "134\n",
      "78\n",
      "\n",
      "89\n",
      "41\n",
      "\n",
      "351\n",
      "224\n",
      "\n",
      "151\n",
      "98\n",
      "\n",
      "750\n",
      "475\n",
      "\n",
      "98\n",
      "44\n",
      "\n",
      "64\n",
      "30\n",
      "\n",
      "72\n",
      "32\n",
      "\n",
      "132\n",
      "58\n",
      "\n",
      "140\n",
      "78\n",
      "\n",
      "172\n",
      "88\n",
      "\n",
      "143\n",
      "84\n",
      "\n",
      "377\n",
      "259\n",
      "\n",
      "243\n",
      "135\n",
      "\n",
      "146\n",
      "80\n",
      "\n",
      "111\n",
      "62\n",
      "\n",
      "94\n",
      "46\n",
      "\n",
      "122\n",
      "70\n",
      "\n",
      "74\n",
      "35\n",
      "\n",
      "111\n",
      "55\n",
      "\n",
      "114\n",
      "58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 只有大概一半非空格词在词典中\n",
    "for row in comments:\n",
    "    c = comm_dict_2[row[0]]\n",
    "    doc = c.seg_topic + c.seg_detail\n",
    "    get_doc_vec(doc, word2vec_build_on_all_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原先使用的是仅从表2语料构造的词嵌入模型，换成从三个表构造的模型（word2vec_build_on_all_text）后，分类器的准确率有所提升"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "45\n",
      "79\n",
      "30\n",
      "501\n",
      "375\n",
      "100\n",
      "65\n",
      "81\n",
      "33\n",
      "115\n",
      "46\n",
      "123\n",
      "63\n",
      "82\n",
      "29\n",
      "112\n",
      "56\n",
      "1735\n",
      "1296\n",
      "1735\n",
      "1296\n",
      "83\n",
      "28\n",
      "85\n",
      "47\n",
      "154\n",
      "94\n",
      "90\n",
      "48\n",
      "63\n",
      "23\n",
      "86\n",
      "32\n",
      "125\n",
      "69\n",
      "68\n",
      "26\n",
      "100\n",
      "58\n",
      "74\n",
      "26\n",
      "143\n",
      "73\n",
      "160\n",
      "84\n",
      "106\n",
      "38\n",
      "182\n",
      "117\n",
      "50\n",
      "12\n",
      "95\n",
      "44\n",
      "192\n",
      "122\n",
      "56\n",
      "19\n",
      "318\n",
      "209\n",
      "501\n",
      "401\n",
      "109\n",
      "63\n",
      "227\n",
      "105\n",
      "546\n",
      "252\n",
      "92\n",
      "30\n",
      "75\n",
      "28\n",
      "107\n",
      "66\n",
      "796\n",
      "663\n",
      "184\n",
      "95\n",
      "78\n",
      "33\n",
      "340\n",
      "184\n",
      "524\n",
      "343\n",
      "299\n",
      "186\n",
      "407\n",
      "250\n",
      "77\n",
      "28\n",
      "353\n",
      "205\n",
      "241\n",
      "155\n",
      "450\n",
      "258\n",
      "589\n",
      "451\n",
      "114\n",
      "69\n",
      "77\n",
      "36\n",
      "171\n",
      "92\n",
      "635\n",
      "318\n",
      "261\n",
      "152\n",
      "88\n",
      "37\n",
      "144\n",
      "79\n",
      "120\n",
      "66\n",
      "579\n",
      "354\n",
      "144\n",
      "85\n",
      "133\n",
      "87\n",
      "87\n",
      "40\n",
      "348\n",
      "239\n",
      "550\n",
      "378\n",
      "533\n",
      "413\n",
      "236\n",
      "141\n",
      "91\n",
      "41\n",
      "193\n",
      "124\n",
      "465\n",
      "291\n",
      "1317\n",
      "1084\n",
      "99\n",
      "50\n",
      "456\n",
      "327\n",
      "443\n",
      "350\n",
      "939\n",
      "746\n",
      "90\n",
      "48\n",
      "188\n",
      "128\n",
      "94\n",
      "51\n",
      "178\n",
      "113\n",
      "79\n",
      "30\n",
      "58\n",
      "23\n",
      "76\n",
      "39\n",
      "48\n",
      "17\n",
      "137\n",
      "88\n",
      "65\n",
      "27\n",
      "217\n",
      "152\n",
      "149\n",
      "87\n",
      "631\n",
      "409\n",
      "76\n",
      "35\n",
      "98\n",
      "49\n",
      "73\n",
      "30\n",
      "89\n",
      "52\n",
      "70\n",
      "32\n",
      "279\n",
      "185\n",
      "354\n",
      "204\n",
      "92\n",
      "29\n",
      "85\n",
      "38\n",
      "171\n",
      "93\n",
      "188\n",
      "104\n",
      "417\n",
      "346\n",
      "60\n",
      "24\n",
      "156\n",
      "99\n",
      "891\n",
      "422\n",
      "353\n",
      "258\n",
      "66\n",
      "28\n",
      "71\n",
      "26\n",
      "92\n",
      "49\n",
      "163\n",
      "86\n",
      "123\n",
      "68\n",
      "139\n",
      "50\n",
      "304\n",
      "171\n",
      "133\n",
      "65\n",
      "254\n",
      "170\n",
      "112\n",
      "42\n",
      "348\n",
      "234\n",
      "711\n",
      "463\n",
      "1066\n",
      "747\n",
      "512\n",
      "349\n",
      "739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472\n",
      "74\n",
      "27\n",
      "304\n",
      "166\n",
      "172\n",
      "88\n",
      "225\n",
      "104\n",
      "118\n",
      "76\n",
      "111\n",
      "53\n",
      "167\n",
      "89\n",
      "171\n",
      "103\n",
      "799\n",
      "622\n",
      "274\n",
      "161\n",
      "626\n",
      "355\n",
      "179\n",
      "105\n",
      "190\n",
      "113\n",
      "294\n",
      "187\n",
      "108\n",
      "69\n",
      "420\n",
      "320\n",
      "350\n",
      "225\n",
      "117\n",
      "64\n",
      "331\n",
      "166\n",
      "145\n",
      "77\n",
      "863\n",
      "535\n",
      "233\n",
      "143\n",
      "131\n",
      "73\n",
      "233\n",
      "127\n",
      "270\n",
      "174\n",
      "69\n",
      "32\n",
      "90\n",
      "48\n",
      "174\n",
      "98\n",
      "97\n",
      "56\n",
      "61\n",
      "26\n",
      "140\n",
      "63\n",
      "99\n",
      "41\n",
      "170\n",
      "107\n",
      "70\n",
      "24\n",
      "345\n",
      "199\n",
      "211\n",
      "115\n",
      "61\n",
      "13\n",
      "127\n",
      "61\n",
      "124\n",
      "76\n",
      "129\n",
      "71\n",
      "130\n",
      "69\n",
      "468\n",
      "271\n",
      "140\n",
      "83\n",
      "140\n",
      "82\n",
      "84\n",
      "36\n",
      "68\n",
      "23\n",
      "68\n",
      "26\n",
      "107\n",
      "43\n",
      "56\n",
      "13\n",
      "74\n",
      "32\n",
      "156\n",
      "95\n",
      "88\n",
      "41\n",
      "118\n",
      "60\n",
      "581\n",
      "387\n",
      "119\n",
      "51\n",
      "84\n",
      "34\n",
      "386\n",
      "145\n",
      "76\n",
      "37\n",
      "152\n",
      "89\n",
      "128\n",
      "70\n",
      "81\n",
      "39\n",
      "119\n",
      "64\n",
      "176\n",
      "87\n",
      "68\n",
      "28\n",
      "272\n",
      "213\n",
      "95\n",
      "47\n",
      "156\n",
      "86\n",
      "128\n",
      "61\n",
      "119\n",
      "53\n",
      "360\n",
      "234\n",
      "110\n",
      "55\n",
      "179\n",
      "102\n",
      "119\n",
      "60\n",
      "553\n",
      "358\n",
      "122\n",
      "67\n",
      "107\n",
      "48\n",
      "146\n",
      "90\n",
      "136\n",
      "76\n",
      "172\n",
      "99\n",
      "404\n",
      "279\n",
      "75\n",
      "38\n",
      "746\n",
      "437\n",
      "131\n",
      "73\n",
      "140\n",
      "82\n",
      "62\n",
      "26\n",
      "151\n",
      "95\n",
      "216\n",
      "147\n",
      "121\n",
      "80\n",
      "58\n",
      "22\n",
      "329\n",
      "250\n",
      "86\n",
      "45\n",
      "77\n",
      "35\n",
      "336\n",
      "238\n",
      "331\n",
      "199\n",
      "331\n",
      "150\n",
      "635\n",
      "427\n",
      "149\n",
      "89\n",
      "124\n",
      "69\n",
      "64\n",
      "24\n",
      "100\n",
      "53\n",
      "91\n",
      "46\n",
      "133\n",
      "73\n",
      "109\n",
      "57\n",
      "150\n",
      "81\n",
      "461\n",
      "292\n",
      "96\n",
      "58\n",
      "85\n",
      "40\n",
      "222\n",
      "129\n",
      "136\n",
      "89\n",
      "505\n",
      "411\n",
      "177\n",
      "109\n",
      "344\n",
      "218\n",
      "236\n",
      "157\n",
      "127\n",
      "76\n",
      "131\n",
      "72\n",
      "113\n",
      "68\n",
      "313\n",
      "169\n",
      "86\n",
      "38\n",
      "157\n",
      "92\n",
      "62\n",
      "20\n",
      "128\n",
      "61\n",
      "166\n",
      "100\n",
      "191\n",
      "125\n",
      "309\n",
      "222\n",
      "398\n",
      "289\n",
      "268\n",
      "155\n",
      "97\n",
      "48\n",
      "154\n",
      "85\n",
      "260\n",
      "165\n",
      "65\n",
      "23\n",
      "300\n",
      "187\n",
      "77\n",
      "29\n",
      "180\n",
      "96\n",
      "221\n",
      "127\n",
      "64\n",
      "31\n",
      "78\n",
      "33\n",
      "64\n",
      "24\n",
      "80\n",
      "40\n",
      "253\n",
      "120\n",
      "72\n",
      "31\n",
      "69\n",
      "29\n",
      "103\n",
      "60\n",
      "107\n",
      "50\n",
      "103\n",
      "45\n",
      "144\n",
      "56\n",
      "390\n",
      "228\n",
      "120\n",
      "51\n",
      "224\n",
      "111\n",
      "112\n",
      "66\n",
      "119\n",
      "53\n",
      "100\n",
      "36\n",
      "116\n",
      "47\n",
      "408\n",
      "270\n",
      "816\n",
      "647\n",
      "239\n",
      "96\n",
      "355\n",
      "203\n",
      "451\n",
      "266\n",
      "199\n",
      "137\n",
      "339\n",
      "196\n",
      "71\n",
      "17\n",
      "95\n",
      "52\n",
      "114\n",
      "41\n",
      "110\n",
      "48\n",
      "73\n",
      "26\n",
      "132\n",
      "60\n",
      "1610\n",
      "722\n",
      "83\n",
      "38\n",
      "324\n",
      "202\n",
      "51\n",
      "20\n",
      "168\n",
      "95\n",
      "84\n",
      "44\n",
      "129\n",
      "81\n",
      "106\n",
      "52\n",
      "189\n",
      "128\n",
      "842\n",
      "565\n",
      "79\n",
      "41\n",
      "151\n",
      "52\n",
      "208\n",
      "137\n",
      "76\n",
      "39\n",
      "141\n",
      "87\n",
      "133\n",
      "77\n",
      "81\n",
      "44\n",
      "60\n",
      "25\n",
      "62\n",
      "27\n",
      "126\n",
      "63\n",
      "278\n",
      "207\n",
      "89\n",
      "45\n",
      "267\n",
      "182\n",
      "117\n",
      "70\n",
      "75\n",
      "34\n",
      "1974\n",
      "1626\n",
      "111\n",
      "66\n",
      "541\n",
      "328\n",
      "60\n",
      "24\n",
      "150\n",
      "102\n",
      "161\n",
      "109\n",
      "234\n",
      "173\n",
      "74\n",
      "24\n",
      "70\n",
      "37\n",
      "54\n",
      "24\n",
      "77\n",
      "35\n",
      "125\n",
      "72\n",
      "450\n",
      "275\n",
      "69\n",
      "35\n",
      "236\n",
      "130\n",
      "58\n",
      "19\n",
      "144\n",
      "77\n",
      "101\n",
      "42\n",
      "105\n",
      "66\n",
      "214\n",
      "146\n",
      "56\n",
      "22\n",
      "136\n",
      "74\n",
      "61\n",
      "22\n",
      "221\n",
      "122\n",
      "92\n",
      "38\n",
      "55\n",
      "22\n",
      "259\n",
      "85\n",
      "75\n",
      "28\n",
      "52\n",
      "21\n",
      "258\n",
      "184\n",
      "412\n",
      "316\n",
      "110\n",
      "67\n",
      "104\n",
      "56\n",
      "152\n",
      "74\n",
      "84\n",
      "36\n",
      "82\n",
      "33\n",
      "342\n",
      "269\n",
      "85\n",
      "44\n",
      "1210\n",
      "812\n",
      "115\n",
      "44\n",
      "116\n",
      "54\n",
      "163\n",
      "92\n",
      "180\n",
      "118\n",
      "1230\n",
      "960\n",
      "404\n",
      "239\n",
      "939\n",
      "526\n",
      "750\n",
      "463\n",
      "102\n",
      "58\n",
      "98\n",
      "48\n",
      "74\n",
      "32\n",
      "76\n",
      "35\n",
      "297\n",
      "210\n",
      "210\n",
      "142\n",
      "311\n",
      "177\n",
      "744\n",
      "437\n",
      "546\n",
      "419\n",
      "146\n",
      "78\n",
      "137\n",
      "84\n",
      "82\n",
      "29\n",
      "242\n",
      "169\n",
      "350\n",
      "263\n",
      "285\n",
      "177\n",
      "142\n",
      "95\n",
      "112\n",
      "62\n",
      "585\n",
      "310\n",
      "88\n",
      "30\n",
      "181\n",
      "110\n",
      "91\n",
      "41\n",
      "145\n",
      "89\n",
      "146\n",
      "85\n",
      "92\n",
      "42\n",
      "118\n",
      "67\n",
      "94\n",
      "47\n",
      "345\n",
      "249\n",
      "90\n",
      "39\n",
      "132\n",
      "79\n",
      "106\n",
      "66\n",
      "80\n",
      "28\n",
      "66\n",
      "27\n",
      "80\n",
      "40\n",
      "109\n",
      "53\n",
      "127\n",
      "72\n",
      "177\n",
      "86\n",
      "181\n",
      "102\n",
      "96\n",
      "42\n",
      "298\n",
      "178\n",
      "124\n",
      "72\n",
      "88\n",
      "48\n",
      "139\n",
      "74\n",
      "80\n",
      "26\n",
      "121\n",
      "51\n",
      "96\n",
      "44\n",
      "62\n",
      "21\n",
      "596\n",
      "338\n",
      "203\n",
      "113\n",
      "197\n",
      "110\n",
      "342\n",
      "239\n",
      "406\n",
      "215\n",
      "85\n",
      "34\n",
      "101\n",
      "48\n",
      "109\n",
      "43\n",
      "943\n",
      "671\n",
      "72\n",
      "30\n",
      "96\n",
      "44\n",
      "88\n",
      "40\n",
      "106\n",
      "64\n",
      "232\n",
      "132\n",
      "360\n",
      "212\n",
      "113\n",
      "60\n",
      "163\n",
      "108\n",
      "559\n",
      "261\n",
      "91\n",
      "30\n",
      "87\n",
      "38\n",
      "62\n",
      "19\n",
      "110\n",
      "59\n",
      "325\n",
      "227\n",
      "100\n",
      "44\n",
      "140\n",
      "69\n",
      "378\n",
      "250\n",
      "211\n",
      "119\n",
      "247\n",
      "71\n",
      "136\n",
      "75\n",
      "64\n",
      "29\n",
      "73\n",
      "37\n",
      "110\n",
      "52\n",
      "125\n",
      "75\n",
      "49\n",
      "19\n",
      "116\n",
      "69\n",
      "54\n",
      "24\n",
      "71\n",
      "33\n",
      "552\n",
      "378\n",
      "206\n",
      "121\n",
      "83\n",
      "35\n",
      "64\n",
      "28\n",
      "178\n",
      "114\n",
      "107\n",
      "65\n",
      "89\n",
      "37\n",
      "122\n",
      "56\n",
      "130\n",
      "71\n",
      "179\n",
      "98\n",
      "662\n",
      "498\n",
      "480\n",
      "302\n",
      "246\n",
      "144\n",
      "101\n",
      "47\n",
      "171\n",
      "100\n",
      "71\n",
      "27\n",
      "137\n",
      "77\n",
      "73\n",
      "39\n",
      "199\n",
      "114\n",
      "230\n",
      "148\n",
      "109\n",
      "52\n",
      "119\n",
      "56\n",
      "84\n",
      "40\n",
      "162\n",
      "97\n",
      "178\n",
      "98\n",
      "95\n",
      "37\n",
      "84\n",
      "35\n",
      "84\n",
      "39\n",
      "218\n",
      "135\n",
      "110\n",
      "45\n",
      "113\n",
      "59\n",
      "85\n",
      "38\n",
      "467\n",
      "282\n",
      "95\n",
      "40\n",
      "1265\n",
      "861\n",
      "213\n",
      "117\n",
      "671\n",
      "351\n",
      "134\n",
      "78\n",
      "89\n",
      "41\n",
      "351\n",
      "224\n",
      "151\n",
      "98\n",
      "750\n",
      "475\n",
      "98\n",
      "44\n",
      "64\n",
      "30\n",
      "72\n",
      "32\n",
      "132\n",
      "58\n",
      "140\n",
      "78\n",
      "172\n",
      "88\n",
      "143\n",
      "84\n",
      "377\n",
      "259\n",
      "243\n",
      "135\n",
      "146\n",
      "80\n",
      "111\n",
      "62\n",
      "94\n",
      "46\n",
      "122\n",
      "70\n",
      "74\n",
      "35\n",
      "111\n",
      "55\n",
      "114\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "# 计算所有评论的文档向量\n",
    "comms_vec = [get_doc_vec(comm_dict_2[row[0]].seg_topic + comm_dict_2[row[0]].seg_detail, word2vec_build_on_all_text) for row in comments]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然不知道为什么词典中少了很多词语，但总算是得到了针对每条评论的“文档向量”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['教育文体', '城乡建设', '商贸旅游', '劳动和社会保障', '交通运输', '环境保护', '卫生计生']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 表2中涉及的所有一级标签\n",
    "target_names = list(set([row[5] for row in comments]))\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将表2中标注的所有一级标签转化成数字表示（target_names中的index）\n",
    "targets = [target_names.index(row[5]) for row in comments]\n",
    "\n",
    "targets[:10]  # 展示前十个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割训练集和测试集\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(comms_vec, targets, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入KNN模型\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(leaf_size=50, n_neighbors=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=50, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练knn模型\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        教育文体       0.57      0.52      0.54        33\n",
      "        城乡建设       0.28      0.52      0.36        23\n",
      "        商贸旅游       0.31      0.33      0.32        12\n",
      "     劳动和社会保障       0.77      0.77      0.77        31\n",
      "        交通运输       0.40      0.21      0.28        19\n",
      "        环境保护       0.33      0.23      0.27        13\n",
      "        卫生计生       0.46      0.33      0.39        18\n",
      "\n",
      "    accuracy                           0.47       149\n",
      "   macro avg       0.45      0.42      0.42       149\n",
      "weighted avg       0.49      0.47      0.47       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 评价模型\n",
    "from sklearn import metrics \n",
    "print(metrics.classification_report(y_test, predicted, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 使用朴素贝叶斯模型\n",
    "贝叶斯模型参数少、训练快，在测试中表现优秀，在大数据集下应该会有更好的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造向量化工具\n",
    "count_vect = CountVectorizer()\n",
    "comments = read_xl_by_line(path_cc + \"/xls/e2.xlsx\")  # 留言文本\n",
    "stop_words = load_word_list(path_cc + \"/special-words/stop_words.txt\")  # 停用词\n",
    "comm_dict_2 = generate_comm_dict(comments, True, stop_words)\n",
    "\n",
    "line_sents = [comm_dict_2[row[0]].seg_topic + comm_dict_2[row[0]].seg_detail for row in comments]\n",
    "sents = list(map(lambda x: \" \".join(x), line_sents))\n",
    "\n",
    "# 表2中涉及的所有一级标签\n",
    "target_names = list(set([row[5] for row in comments]))\n",
    "# 将表2中标注的所有一级标签转化成数字表示（target_names中的index）\n",
    "targets = [target_names.index(row[5]) for row in comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割训练集和测试集\n",
    "x_train, x_test, y_train, y_test \\\n",
    "    = sklearn.model_selection.train_test_split(sents, targets, test_size=0.3)\n",
    "\n",
    "x_train_counts = count_vect.fit_transform(x_train)  # 拟合模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        教育文体       0.82      0.84      0.83        32\n",
      "        城乡建设       0.59      0.72      0.65        36\n",
      "        商贸旅游       0.00      0.00      0.00        11\n",
      "     劳动和社会保障       0.58      0.97      0.73        33\n",
      "        交通运输       0.86      0.46      0.60        13\n",
      "        环境保护       1.00      0.25      0.40         8\n",
      "        卫生计生       1.00      0.50      0.67        16\n",
      "\n",
      "    accuracy                           0.68       149\n",
      "   macro avg       0.69      0.54      0.55       149\n",
      "weighted avg       0.68      0.68      0.64       149\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# 构建TF-TDF特征\n",
    "tf_transformer = TfidfTransformer().fit(x_train_counts)\n",
    "# 构建TF-IDF特征\n",
    "x_train_tf = tf_transformer.transform(x_train_counts)\n",
    "\n",
    "# 使用特征集（X）和目标（target）训练/拟合一个朴素贝叶斯分类器\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_tf.toarray(), y_train)\n",
    "\n",
    "test_tf = tf_transformer.transform(count_vect.transform(x_test))\n",
    "\n",
    "predicted = clf.predict(test_tf.toarray())  # 分类器\n",
    "\n",
    "print(sklearn.metrics.classification_report(y_true=y_test, y_pred=predicted, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
