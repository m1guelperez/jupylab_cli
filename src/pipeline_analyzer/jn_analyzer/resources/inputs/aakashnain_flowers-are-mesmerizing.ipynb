{"nbformat": 4, "metadata": {"language_info": {"version": "3.6.3", "name": "python", "mimetype": "text/x-python", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "cells": [{"metadata": {"_uuid": "cbce90977f702ed483246f48bec330ad4bc89fd5", "_cell_guid": "8aed180f-612b-4bf4-9e0f-b15f44f0d617"}, "source": ["![flowers](https://media.giphy.com/media/6igrowTjYLNvi/giphy.gif)"], "cell_type": "markdown"}, {"metadata": {"_uuid": "3a84018dde0b34628526a96e52ad092d095610f7", "_cell_guid": "9068897b-4d8a-4957-b4e0-208cc3012463"}, "source": ["Hello Kagglers. Working on Kaggle kernels is fun. The purpose of this kernel is totally different. Here I am not going to do a typical EDA or typical data modelling but I would love to share some cool things. We are going to dive into following topics:\n", "* How to add pre-trained Keras models to your kernel and answer the question **Why do I need to do that at all?**\n", "* What generator should I use for my model- inbuilt or a custom one?\n", "* How to effectively use Keras ImageDataGenerator in kernels?"], "cell_type": "markdown"}, {"metadata": {"_uuid": "ec574a3d110b9a1dd77fa3617b90d6dba36066cc", "_cell_guid": "fb1e0446-7d65-445e-a5eb-0df97ea9321e"}, "source": ["## Adding Keras pre-trained models to your kernel\n", "\n", "Transfer learning (Here I am assuming that you know about it) **almost always** works. Before doing some serious modelling, people like me always starts with transfer learning to get a baseline. For this, we need pre-trained models. Keras provides a lot of SOTA pre-trained models. When you want to use a pre-trained architecture for the first time, Keras download the weights for the corresponding model *but* Kernels can't use network connection to download pretrained keras model weights. So, the big question is `If Kernels can't use network connection to download pre-trained weights, how can I use them at all?` \n", "\n", "This is a great question and for people who are beginners or just getting started on Kaggle kernels, this can be very confusing. In order to use, pre-trained Keras model weights, people have uploaded the weights to a kernel and published it. Now here is the catch. **You can add the output of any other kernel as input data source for your kernel **. Follow these simple steps:\n", "* On the top-left of your notebook, there is a `Input Files` cell. Expand it by clicking the `+` button.\n", "* You will see a list of input data files on the left along with the description of the data on the right.\n", "* Click the add `Add Data Source` button. A window will appear.\n", "* In the search bar, search like this `VGG16 pretrained` or `Keras-pretrained`.\n", "* Choose the kernel you want to add. That's it!!\n", "\n", "Now if you expand your `Input Files` cell again, you will the pre-trained model as input files along with your dataset.\n"], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "fcf60fff38568bd4b88f080d222834c5ff645a08", "_cell_guid": "afc20052-a26d-4d34-b2e9-ebecfa4ed2c9"}, "execution_count": null, "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "import os\n", "import glob\n", "import shutil\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "import matplotlib.image as mimg\n", "from os import listdir, makedirs, getcwd, remove\n", "from os.path import isfile, join, abspath, exists, isdir, expanduser\n", "from PIL import Image\n", "from pathlib import Path\n", "from keras.models import Sequential, Model\n", "from keras.applications.vgg16 import VGG16, preprocess_input\n", "from keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\n", "from keras.models import Sequential\n", "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten\n", "from keras.layers import GlobalMaxPooling2D\n", "from keras.layers.normalization import BatchNormalization\n", "from keras.layers.merge import Concatenate\n", "from keras.models import Model\n", "from keras.optimizers import Adam\n", "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n", "from keras.utils import to_categorical\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler\n", "%matplotlib inline\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."]}, {"metadata": {"_uuid": "b9df4b09af59e791d78c4211d624130c397339a8", "_cell_guid": "e46bb37f-a8e6-4a79-b00f-652ceb1cd380"}, "source": ["You can see that my kernel has two kind of input files:\n", "* flowers-recognition dataset\n", "* vgg16 pre-trained model kernel that I added to my kernel\n", "\n", "Keras requires the pre-trained weights to be present in the `.keras/models` cache directory. This is how you do it"], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "7f95eff00e62bc83ba6e9a06a77e783e429621c4", "collapsed": true, "_cell_guid": "8a0f9442-5110-4c93-bc4e-4fcde8b2868b"}, "execution_count": null, "cell_type": "code", "source": ["# Check for the directory and if it doesn't exist, make one.\n", "cache_dir = expanduser(join('~', '.keras'))\n", "if not exists(cache_dir):\n", "    makedirs(cache_dir)\n", "    \n", "# make the models sub-directory\n", "models_dir = join(cache_dir, 'models')\n", "if not exists(models_dir):\n", "    makedirs(models_dir)"]}, {"outputs": [], "metadata": {"_uuid": "079f56f612c826e546f912afcd32889159e246d3", "collapsed": true, "_cell_guid": "44b23fba-316f-47d3-843c-c41885517b82"}, "execution_count": null, "cell_type": "code", "source": ["# Copy the weights from your input files to the cache directory\n", "!cp ../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5 ~/.keras/models/"]}, {"metadata": {"_uuid": "c26f5ee1e57e052b0ab4882b0f7c1c5df226afba", "_cell_guid": "9a068efc-eefa-46c2-a824-cf9f78f6e438"}, "source": ["That's it!! Now, you can use pre-trained models for transfer learning or fine-tuning. "], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "8b4c951b162b52575e58ded45e2bcb289e305844", "collapsed": true, "_cell_guid": "bc09d8a7-cb8c-44db-a487-fc9fab5b65f9"}, "execution_count": null, "cell_type": "code", "source": ["# Define some paths\n", "input_path = Path('../input/flowers-recognition/flowers/')\n", "flowers_path = input_path / 'flowers'"]}, {"outputs": [], "metadata": {"_uuid": "ef7c262834a1b2ca8d5f395f0670a792a2874c8e", "_cell_guid": "c027698d-b8c6-4c0a-9979-7f19641444bc"}, "execution_count": null, "cell_type": "code", "source": ["# Each species of flower is contained in a separate folder . Get all the sub directories\n", "flower_types = os.listdir(flowers_path)\n", "print(\"Types of flowers found: \", len(flower_types))\n", "print(\"Categories of flowers: \", flower_types)"]}, {"outputs": [], "metadata": {"_uuid": "4efef8ee4118ecc0e253f5f317fe3a5f0332418c", "_cell_guid": "dec3df06-15a5-49f2-98f6-3ef357d2748d"}, "execution_count": null, "cell_type": "code", "source": ["# In order to keep track of my data details or in order to do some EDA, I always try to \n", "# get the information in a dataframe. After all, pandas to the rescue!!\n", "\n", "# A list that is going to contain tuples: (species of the flower, corresponding image path)\n", "flowers = []\n", "\n", "for species in flower_types:\n", "    # Get all the file names\n", "    all_flowers = os.listdir(flowers_path / species)\n", "    # Add them to the list\n", "    for flower in all_flowers:\n", "        flowers.append((species, str(flowers_path /species) + '/' + flower))\n", "\n", "# Build a dataframe        \n", "flowers = pd.DataFrame(data=flowers, columns=['category', 'image'], index=None)\n", "flowers.head()"]}, {"outputs": [], "metadata": {"_uuid": "833ee388a49affe56ecdb0b83b6e396db1c528e1", "_cell_guid": "8dd68221-7756-442a-804a-3ad99016cb51"}, "execution_count": null, "cell_type": "code", "source": ["# Let's check how many samples for each category are present\n", "print(\"Total number of flowers in the dataset: \", len(flowers))\n", "fl_count = flowers['category'].value_counts()\n", "print(\"Flowers in each category: \")\n", "print(fl_count)"]}, {"outputs": [], "metadata": {"_uuid": "bd0d0c3e8490450f2c4eaef0342eca24233b64b1", "_cell_guid": "25e2bb2d-c1e1-452e-99f0-6718898869dd"}, "execution_count": null, "cell_type": "code", "source": ["# Let's do some visualization too\n", "plt.figure(figsize=(12,8))\n", "sns.barplot(x=fl_count.index, y=fl_count.values)\n", "plt.title(\"Flowers count for each category\", fontsize=16)\n", "plt.xlabel(\"Category\", fontsize=14)\n", "plt.ylabel(\"Count\", fontsize=14)\n", "plt.show()"]}, {"outputs": [], "metadata": {"_uuid": "8a7fec44aec3c708b7f80e7a5b13a9a0917b9785", "_cell_guid": "3bab7ee4-008d-4329-a875-e8878b3a4681"}, "execution_count": null, "cell_type": "code", "source": ["# Let's visualize flowers from each category\n", "\n", "# A list for storing names of some random samples from each category\n", "random_samples = []\n", "\n", "# Get samples fom each category \n", "for category in fl_count.index:\n", "    samples = flowers['image'][flowers['category'] == category].sample(4).values\n", "    for sample in samples:\n", "        random_samples.append(sample)\n", "\n", "\n", "\n", "# Plot the samples\n", "f, ax = plt.subplots(5,4, figsize=(15,10))\n", "for i,sample in enumerate(random_samples):\n", "    ax[i//4, i%4].imshow(mimg.imread(random_samples[i]))\n", "    ax[i//4, i%4].axis('off')\n", "plt.show()    "]}, {"metadata": {"_uuid": "2eb0d1f2ecb6751bad1d551ce6595bfeecca71e7", "_cell_guid": "bebb498c-6dbf-4dac-ba65-7f8d628e5d40"}, "source": ["**What generator should I use for my model-  a custom one or the default Keras ImageDataGenerator?**\n", "\n", "This is a very interesting question. I would say that it actually depends on how your dataset is arranged or how are you going to set up your data. These are the following scenarios I can think of along with the corresponding solutions. If you think of any more, do let me know in the comments section.\n", "\n", "* **Data is arranged class-wise in separate directories with corresponding names**: This is the best way to arrange your data, if possible. Although it takes some time to arrange the data in such a way but it is the way to go if you want to use the Keras ImageDataGenerator efficiently as it requires data to be separated class wise in different folders. Once you have this, you need to arrange your data like this:\n", "```\n", "data/\n", "    train/\n", "        category1/(contains all images related to category1)  \n", "        category2/(contains all images related to category2)\n", "        ...\n", "        ...\n", "            \n", "    validation/\n", "         category1/(contains all images related to category1)  \n", "        category2/(contains all images related to category2)\n", "        ...\n", "        ...\n", "```\n", "For this kernel, later in the notebook, I will show how to make this structure within the kernel for using ImageDataGenerator\n", "\n", "* **All data is within one folder and you have meta info about the images** This is a very usual case. When we quickly crawl data, we generally store the met info about the images in a csv and allthe images are stored in a single folder. There are two ways to deal with this situatio, provided you don't want all the segregation of images as in the first step.\n", "  * Define your own simple python generator which yields batches of images and labels while reading the csv\n", "  * Use another high-level api such as `Dataset` api and let it do the work for you. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "70fa89bda45f2f850df220bc219aaf042bebab80", "_cell_guid": "7f5ff66b-d076-4392-a74d-5c1dd9ffafb5"}, "source": ["Let's look at how to get the structure defined in the  first step above. If you are not aware, jupyter is pretty powerful and you can use bash directly within the notebook."], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "46783cb0593e053dcbb85f119a7e304172144f4c", "_cell_guid": "5fd87123-b515-464f-b9da-bb7941b8694d"}, "execution_count": null, "cell_type": "code", "source": ["# Make a parent directory `data` and two sub directories `train` and `valid`\n", "%mkdir -p data/train\n", "%mkdir -p data/valid\n", "\n", "# Inside the train and validation sub=directories, make sub-directories for each catgeory\n", "%cd data\n", "%mkdir -p train/daisy\n", "%mkdir -p train/tulip\n", "%mkdir -p train/sunflower\n", "%mkdir -p train/rose\n", "%mkdir -p train/dandelion\n", "\n", "%mkdir -p valid/daisy\n", "%mkdir -p valid/tulip\n", "%mkdir -p valid/sunflower\n", "%mkdir -p valid/rose\n", "%mkdir -p valid/dandelion\n", "\n", "%cd ..\n", "\n", "# You can verify that everything went correctly using ls command"]}, {"metadata": {"_uuid": "7b3e162a7b9ee182ab7e9d47a569261153d56dd1", "_cell_guid": "b75909bd-1584-41c0-ace0-810198f522f6"}, "source": ["For each category, copy samples to the train and validation directory which we defined in the above step. The number of samples you want in your training and validation set is upto you. "], "cell_type": "markdown"}, {"outputs": [], "metadata": {"_uuid": "dcc7f0dad075fa6457f95fe3ef0b9dbbcdc262ad", "collapsed": true, "_cell_guid": "8d1b4f12-5f98-42ec-b657-d7900eea0092"}, "execution_count": null, "cell_type": "code", "source": ["for category in fl_count.index:\n", "    samples = flowers['image'][flowers['category'] == category].values\n", "    perm = np.random.permutation(samples)\n", "    # Copy first 30 samples to the validation directory and rest to the train directory\n", "    for i in range(30):\n", "        name = perm[i].split('/')[-1]\n", "        shutil.copyfile(perm[i],'./data/valid/' + str(category) + '/'+ name)\n", "    for i in range(31,len(perm)):\n", "        name = perm[i].split('/')[-1]\n", "        shutil.copyfile(perm[i],'./data/train/' + str(category) + '/' + name)"]}, {"outputs": [], "metadata": {"_uuid": "4345f5b5a9d5c09be33313ddff60d0805c72f780", "_cell_guid": "668eeeee-9f44-4ec5-8ab7-0f3e746addff"}, "execution_count": null, "cell_type": "code", "source": ["# Define the generators\n", "\n", "batch_size = 8\n", "# this is the augmentation configuration we will use for training\n", "train_datagen = ImageDataGenerator(\n", "        rescale=1./255,\n", "        shear_range=0.2,\n", "        zoom_range=0.2,\n", "        horizontal_flip=True)\n", "\n", "# this is the augmentation configuration we will use for testing:\n", "# only rescaling\n", "test_datagen = ImageDataGenerator(rescale=1./255)\n", "\n", "# this is a generator that will read pictures found in\n", "# subfolers of 'data/train', and indefinitely generate\n", "# batches of augmented image data\n", "train_generator = train_datagen.flow_from_directory(\n", "        'data/train',  # this is the target directory\n", "        target_size=(150, 150),  # all images will be resized to 150x150\n", "        batch_size=batch_size,\n", "        class_mode='categorical')  # more than two classes\n", "\n", "# this is a similar generator, for validation data\n", "validation_generator = test_datagen.flow_from_directory(\n", "        'data/valid',\n", "        target_size=(150,150),\n", "        batch_size=batch_size,\n", "        class_mode='categorical')"]}, {"outputs": [], "metadata": {"_uuid": "09dce09c85d423ad0ebca9ad5254cdb92a1c4d7c", "collapsed": true, "_cell_guid": "e310c803-5299-461b-be6c-ba8dc0c2ddc5"}, "execution_count": null, "cell_type": "code", "source": ["def get_model():\n", "    # Get base model \n", "    base_model = VGG16(include_top=False, input_shape=(150,150,3))\n", "    # Freeze the layers in base model\n", "    for layer in base_model.layers:\n", "        layer.trainable = False\n", "    # Get base model output \n", "    base_model_ouput = base_model.output\n", "    \n", "    # Add new layers\n", "    x = Flatten()(base_model.output)\n", "    x = Dense(500, activation='relu', name='fc1')(x)\n", "    x = Dropout(0.5)(x)\n", "    x = Dense(5, activation='softmax', name='fc2')(x)\n", "    \n", "    model = Model(inputs=base_model.input, outputs=x)\n", "    return model"]}, {"outputs": [], "metadata": {"_uuid": "a0a8103517cbfe72b2899623118b69dd2bbdbd03", "_cell_guid": "69e43c92-ee16-498f-bf52-2b01b0bcf471"}, "execution_count": null, "cell_type": "code", "source": ["# Get the model\n", "model = get_model()\n", "# Compile it\n", "opt = Adam(lr=1e-3, decay=1e-6)\n", "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n", "#Summary\n", "model.summary()"]}, {"outputs": [], "metadata": {"_uuid": "5f96254ffa9a242b9de7e508a7b6bca932e6f2b3", "_cell_guid": "e134441b-8497-48c4-97a7-f8adc250eaf0"}, "execution_count": null, "cell_type": "code", "source": ["# Fit the genertor \n", "model.fit_generator(\n", "        train_generator,\n", "        steps_per_epoch=4168 // batch_size,\n", "        epochs=50,\n", "        validation_data=validation_generator,\n", "        validation_steps=150 // batch_size)"]}, {"metadata": {"_uuid": "3d481ccb412779b5aed48df2cb39888fa07579da", "_cell_guid": "b27904f4-a596-4b12-a456-2ccd11998ccf"}, "source": ["That's all folks. I hope you enjoyed this. One last thing: Kaggle kernels doesn't provide you GPU, so the training time will depend on your architecture and size of your dataset. Also, if you find this kernel helpful, please upvote!!"], "cell_type": "markdown"}, {"outputs": [], "metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "source": []}], "nbformat_minor": 1}