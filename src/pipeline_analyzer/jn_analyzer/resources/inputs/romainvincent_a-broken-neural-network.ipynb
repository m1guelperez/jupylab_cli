{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Broken Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could have tried to find a solution to this challenge using a Random Forest / XGBoost approach, but I decided to use a neural network instead because I'm a complete deep learning newbie. Since I never used that technique for regression problems, I thought it could be fun to give it a try, but after a few hours of hyper parameters tweaking and loss functions testing, my network still doesn't work (at all). Is it the loss function choice that is wrong? Is it the shape of the graph? Is it a standardization problem? Is it just the wrong approach for that kind of problem? I have no idea.\n",
    "\n",
    "Do you folks know what went wrong? Any feedback is much appreciated!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geopy.distance import vincenty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new york city area\n",
    "min_lon = -74.844,\n",
    "min_lat = 40.026\n",
    "max_lon = -72.221\n",
    "max_lat = 41.372"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_len = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ruling points outside area as outliers\n",
    "train = train[train['pickup_longitude'].between(min_lon, max_lon)]\n",
    "train = train[train['pickup_latitude'].between(min_lat, max_lat)]\n",
    "train = train[train['dropoff_longitude'].between(min_lon, max_lon)]\n",
    "train = train[train['dropoff_latitude'].between(min_lat, max_lat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_len = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outliers removed\n",
    "initial_len - cleaned_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_distance(row):\n",
    "    p1 = (row['pickup_latitude'], row['pickup_longitude'])\n",
    "    p2 = (row['dropoff_latitude'], row['dropoff_longitude'])\n",
    "    return vincenty(p1, p2).meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['distance'] = train.apply(get_distance, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lon = train.loc[0]['pickup_longitude']\n",
    "lat = train.loc[0]['pickup_latitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    0\n",
       "vendor_id             0\n",
       "pickup_datetime       0\n",
       "dropoff_datetime      0\n",
       "passenger_count       0\n",
       "pickup_longitude      0\n",
       "pickup_latitude       0\n",
       "dropoff_longitude     0\n",
       "dropoff_latitude      0\n",
       "store_and_fwd_flag    0\n",
       "trip_duration         0\n",
       "distance              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for na's\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to datetime\n",
    "train['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'])\n",
    "train['dropoff_datetime'] = pd.to_datetime(train['dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get day and hour information\n",
    "train['pickup_hour'] = train['pickup_datetime'].apply(lambda x: x.hour)\n",
    "train['pickup_day'] = train['pickup_datetime'].apply(lambda x: x.weekday())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert timestamps to float\n",
    "train['pickup_datetime'] = train['pickup_datetime'].apply(lambda x: x.timestamp())\n",
    "train['dropoff_datetime'] = train['dropoff_datetime'].apply(lambda x: x.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get dummy variables for categorial data\n",
    "train['store_and_fwd_flag'] = pd.get_dummies(train['store_and_fwd_flag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance / Trip Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10287229740259092, 0.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearsonr(train['distance'], train['trip_duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, LSTM, Merge\n",
    "from keras.optimizers import SGD\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train.drop('trip_duration', axis=1)\n",
    "y = train['trip_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "y_norm = normalize(y.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1093950, 12)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1458600, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_norm.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(48, input_dim=12, activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear', kernel_initializer='normal'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimators = []\n",
    "regressor = KerasRegressor(build_fn=baseline, epochs=5, batch_size=50, verbose=1)\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', regressor))\n",
    "pipeline = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1166880/1166880 [==============================] - 61s - loss: 8.6363e-07 - acc: 0.0000e+00    \n",
      "Epoch 2/5\n",
      "1166880/1166880 [==============================] - 61s - loss: 8.0243e-07 - acc: 0.0000e+00    \n",
      "Epoch 3/5\n",
      "1166880/1166880 [==============================] - 47s - loss: 8.0432e-07 - acc: 0.0000e+00    \n",
      "Epoch 4/5\n",
      "1166880/1166880 [==============================] - 51s - loss: 8.0250e-07 - acc: 0.0000e+00    \n",
      "Epoch 5/5\n",
      "1166880/1166880 [==============================] - 48s - loss: 8.0367e-07 - acc: 0.0000e+00    \n",
      "291350/291720 [============================>.] - ETA: 0sEpoch 1/5\n",
      " 675500/1166880 [================>.............] - ETA: 20s - loss: 1.8000e-06 - acc: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5)\n",
    "results = cross_val_score(pipeline, X, y_norm.T, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.00 (0.00) MSE\n"
     ]
    }
   ],
   "source": [
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
