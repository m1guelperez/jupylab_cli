{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConditionalEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n",
    "        b = b.sum(dim=1)\n",
    "        return -1.0 * b.mean(dim=0)\n",
    "\n",
    "\n",
    "class VAT(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(VAT, self).__init__()\n",
    "        self.n_power = 1\n",
    "        self.XI = 1e-6\n",
    "        self.model = model\n",
    "        self.epsilon = 3.5\n",
    "\n",
    "    def forward(self, X, logit):\n",
    "        vat_loss = self.virtual_adversarial_loss(X, logit)\n",
    "        return vat_loss\n",
    "\n",
    "    def generate_virtual_adversarial_perturbation(self, x, logit):\n",
    "        d = torch.randn_like(x, device='cuda')\n",
    "\n",
    "        for _ in range(self.n_power):\n",
    "            d = self.XI * self.get_normalized_vector(d).requires_grad_()\n",
    "            _, logit_m = self.model(x + d)\n",
    "            dist = self.kl_divergence_with_logit(logit, logit_m)\n",
    "            grad = torch.autograd.grad(dist, [d])[0]\n",
    "            d = grad.detach()\n",
    "\n",
    "        return self.epsilon * self.get_normalized_vector(d)\n",
    "\n",
    "    def kl_divergence_with_logit(self, q_logit, p_logit):\n",
    "        q = F.softmax(q_logit, dim=1)\n",
    "        qlogq = torch.mean(torch.sum(q * F.log_softmax(q_logit, dim=1), dim=1))\n",
    "        qlogp = torch.mean(torch.sum(q * F.log_softmax(p_logit, dim=1), dim=1))\n",
    "        return qlogq - qlogp\n",
    "\n",
    "    def get_normalized_vector(self, d):\n",
    "        return F.normalize(d.view(d.size(0), -1), p=2, dim=1).reshape(d.size())\n",
    "\n",
    "    def virtual_adversarial_loss(self, x, logit):\n",
    "        r_vadv = self.generate_virtual_adversarial_perturbation(x, logit)\n",
    "        logit_p = logit.detach()\n",
    "        _, logit_m = self.model(x + r_vadv)\n",
    "        loss = self.kl_divergence_with_logit(logit_p, logit_m)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, iseval, dataratio=1.0):\n",
    "\n",
    "        self.eval = iseval\n",
    "\n",
    "        # svhn\n",
    "        data = loadmat('../input/train_32x32.mat')\n",
    "        self.datalist_target = [{\n",
    "                                'image': data['X'][..., ij],\n",
    "                                'label': int(data['y'][ij][0]) if int(data['y'][ij][0]) < 10 else 0\n",
    "        } for ij in range(data['y'].shape[0]) if np.random.rand() <= dataratio]\n",
    "\n",
    "        \n",
    "        data = loadmat('../input/train_32x32.mat')\n",
    "        self.datalist_src = [{\n",
    "                                'image': data['X'][..., ij],\n",
    "                                'label': int(data['y'][ij][0]) if int(data['y'][ij][0]) < 10 else 0\n",
    "        } for ij in range(data['y'].shape[0]) if np.random.rand() <= dataratio]\n",
    "\n",
    "        self.totensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "        self.source_larger = len(self.datalist_src) > len(self.datalist_target)\n",
    "        self.n_smallerdataset = len(self.datalist_target) if self.source_larger else len(self.datalist_src)\n",
    "\n",
    "    def __len__(self):\n",
    "        return np.maximum(len(self.datalist_src), len(self.datalist_target))\n",
    "\n",
    "    def shuffledata(self):\n",
    "        self.datalist_src = [self.datalist_src[ij] for ij in torch.randperm(len(self.datalist_src))]\n",
    "        self.datalist_target = [self.datalist_target[ij] for ij in torch.randperm(len(self.datalist_target))]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        index_src = index if self.source_larger else index % self.n_smallerdataset\n",
    "        index_target = index if not self.source_larger else index % self.n_smallerdataset\n",
    "\n",
    "        image_source = self.datalist_src[index_src]['image']\n",
    "        image_source = self.totensor(image_source)\n",
    "        image_source = self.normalize(image_source)\n",
    "\n",
    "        image_target = self.datalist_target[index_target]['image']\n",
    "        image_target = self.totensor(image_target)\n",
    "        image_target = self.normalize(image_target)\n",
    "\n",
    "        return image_source, self.datalist_src[index_src]['label'], image_target, self.datalist_target[index_target]['label']\n",
    "\n",
    "\n",
    "class Dataset_eval(data.Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        # mnist.\n",
    "        \n",
    "        data = loadmat('../input/mnist32_test.mat')\n",
    "        self.datalist_target = [{\n",
    "                                'image': data['X'][ij],\n",
    "                                'label': int(data['y'][0][ij])\n",
    "        } for ij in range(data['y'].shape[1])]\n",
    "\n",
    "        self.totensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datalist_target)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image_target = self.datalist_target[index]['image']\n",
    "        image_target = self.totensor(image_target)\n",
    "        image_target = self.normalize(image_target)\n",
    "\n",
    "        return image_target, self.datalist_target[index]['label']\n",
    "\n",
    "\n",
    "def GenerateIterator(iseval=False):\n",
    "    params = {\n",
    "        'pin_memory': True,\n",
    "        'batch_size': 16 if not iseval else 128,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4,\n",
    "        'drop_last': True,\n",
    "    }\n",
    "\n",
    "    return data.DataLoader(Dataset(iseval), **params)\n",
    "\n",
    "\n",
    "def GenerateIterator_eval():\n",
    "    params = {\n",
    "        'pin_memory': True,\n",
    "        'batch_size': 128,\n",
    "        'num_workers': 4,\n",
    "    }\n",
    "\n",
    "    return data.DataLoader(Dataset_eval(), **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply xavier initialization\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, sigma=1.0):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "        self.noise = torch.tensor(0.0).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            sampled_noise = self.noise.repeat(*x.size()).normal_(mean=0, std=self.sigma)\n",
    "            x = x + sampled_noise\n",
    "        return x\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, large=False):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        n_features = 192 if large else 64\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.InstanceNorm2d(3, momentum=1, eps=1e-3),  # L-17\n",
    "            nn.Conv2d(3, n_features, 3, 1, 1),  # L-16\n",
    "            nn.BatchNorm2d(n_features, momentum=0.99, eps=1e-3),  # L-16\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),  # L-16\n",
    "            nn.Conv2d(n_features, n_features, 3, 1, 1),  # L-15\n",
    "            nn.BatchNorm2d(n_features, momentum=0.99, eps=1e-3),  # L-15\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),  # L-15\n",
    "            nn.Conv2d(n_features, n_features, 3, 1, 1),  # L-14\n",
    "            nn.BatchNorm2d(n_features, momentum=0.99, eps=1e-3),  # L-14\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),  # L-14\n",
    "            nn.MaxPool2d(2),  # L-13\n",
    "            nn.Dropout(0.5),  # L-12\n",
    "            GaussianNoise(1.0),  # L-11\n",
    "            nn.Conv2d(n_features, n_features, 3, 1, 1),  # L-10\n",
    "            nn.BatchNorm2d(n_features, momentum=0.99, eps=1e-3),  # L-10\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),  # L-10\n",
    "            nn.Conv2d(n_features, n_features, 3, 1, 1),  # L-9\n",
    "            nn.BatchNorm2d(n_features, momentum=0.99, eps=1e-3),  # L-9\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),  # L-9\n",
    "            nn.Conv2d(n_features, n_features, 3, 1, 1),  # L-8\n",
    "            nn.BatchNorm2d(n_features, momentum=0.99, eps=1e-3),  # L-8\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),  # L-8\n",
    "            nn.MaxPool2d(2),  # L-7\n",
    "            nn.Dropout(0.5),  # L-6\n",
    "            GaussianNoise(1.0),  # L-5\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(n_features, n_features, 3, 1, 1),  # L-4\n",
    "            nn.BatchNorm2d(n_features, momentum=0.99, eps=1e-3),  # L-4\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),  # L-4\n",
    "            nn.Conv2d(n_features, n_features, 3, 1, 1),  # L-3\n",
    "            nn.BatchNorm2d(n_features, momentum=0.99, eps=1e-3),  # L-3\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),  # L-3\n",
    "            nn.Conv2d(n_features, n_features, 3, 1, 1),  # L-2\n",
    "            nn.BatchNorm2d(n_features, momentum=0.99, eps=1e-3),  # L-2\n",
    "            nn.LeakyReLU(negative_slope=0.1, inplace=True),  # L-2\n",
    "            nn.AdaptiveAvgPool2d(1),  # L-1\n",
    "            nn.Conv2d(n_features, 10, 1)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                m.track_running_stats = False\n",
    "\n",
    "    def track_bn_stats(self, track):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.track_running_stats = track\n",
    "\n",
    "    def forward(self, x, track_bn=False):\n",
    "\n",
    "        if track_bn:\n",
    "            self.track_bn_stats(True)\n",
    "\n",
    "        features = self.feature_extractor(x)\n",
    "        logits = self.classifier(features)\n",
    "\n",
    "        if track_bn:\n",
    "            self.track_bn_stats(False)\n",
    "\n",
    "        return features, logits.view(x.size(0), 10)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, large=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        n_features = 192 if large else 64\n",
    "\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(n_features * 1 * 8 * 8, 100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.disc(x).view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, decay):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "\n",
    "    def register(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "        self.params = self.shadow.keys()\n",
    "\n",
    "    def __call__(self, model):\n",
    "        if self.decay > 0:\n",
    "            for name, param in model.named_parameters():\n",
    "                if name in self.params and param.requires_grad:\n",
    "                    self.shadow[name] -= (1 - self.decay) * (self.shadow[name] - param.data)\n",
    "                    param.data = self.shadow[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(1) Target accuracy (%) 0.737\n",
      "\n",
      "\n",
      "(2) Target accuracy (%) 0.740\n",
      "\n",
      "\n",
      "(3) Target accuracy (%) 0.765\n",
      "\n",
      "\n",
      "(4) Target accuracy (%) 0.801\n",
      "\n",
      "\n",
      "(5) Target accuracy (%) 0.791\n",
      "\n",
      "\n",
      "(6) Target accuracy (%) 0.772\n",
      "\n",
      "\n",
      "(7) Target accuracy (%) 0.765\n",
      "\n",
      "\n",
      "(8) Target accuracy (%) 0.766\n",
      "\n",
      "\n",
      "(9) Target accuracy (%) 0.796\n",
      "\n",
      "\n",
      "(10) Target accuracy (%) 0.783\n",
      "\n",
      "\n",
      "(11) Target accuracy (%) 0.795\n",
      "\n",
      "\n",
      "(12) Target accuracy (%) 0.797\n",
      "\n",
      "\n",
      "(13) Target accuracy (%) 0.852\n",
      "\n",
      "\n",
      "(14) Target accuracy (%) 0.820\n",
      "\n",
      "\n",
      "(15) Target accuracy (%) 0.813\n",
      "\n",
      "\n",
      "(16) Target accuracy (%) 0.820\n",
      "\n",
      "\n",
      "(17) Target accuracy (%) 0.832\n",
      "\n",
      "\n",
      "(18) Target accuracy (%) 0.792\n",
      "\n",
      "\n",
      "(19) Target accuracy (%) 0.821\n",
      "\n",
      "\n",
      "(20) Target accuracy (%) 0.787\n",
      "\n",
      "\n",
      "(21) Target accuracy (%) 0.809\n",
      "\n",
      "\n",
      "(22) Target accuracy (%) 0.807\n",
      "\n",
      "\n",
      "(23) Target accuracy (%) 0.810\n",
      "\n",
      "\n",
      "(24) Target accuracy (%) 0.793\n",
      "\n",
      "\n",
      "(25) Target accuracy (%) 0.814\n",
      "\n",
      "\n",
      "(26) Target accuracy (%) 0.841\n",
      "\n",
      "\n",
      "(27) Target accuracy (%) 0.815\n",
      "\n",
      "\n",
      "(28) Target accuracy (%) 0.825\n",
      "\n",
      "\n",
      "(29) Target accuracy (%) 0.848\n",
      "\n",
      "\n",
      "(30) Target accuracy (%) 0.820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# discriminator network\n",
    "feature_discriminator = Discriminator(large=False).apply(init_weights).cuda()\n",
    "\n",
    "# classifier network.\n",
    "classifier = Classifier(large=False).apply(init_weights).cuda()\n",
    "\n",
    "# loss functions\n",
    "cent = ConditionalEntropyLoss().cuda()\n",
    "xent = nn.CrossEntropyLoss(reduction='mean').cuda()\n",
    "sigmoid_xent = nn.BCEWithLogitsLoss(reduction='mean').cuda()\n",
    "vat_loss = VAT(classifier).cuda()\n",
    "\n",
    "# ADAM optimizer.\n",
    "optimizer_cls = torch.optim.Adam(classifier.parameters(), lr=2e-3, betas=(0.5, 0.999))\n",
    "optimizer_disc = torch.optim.Adam(feature_discriminator.parameters(), lr=2e-3, betas=(0.5, 0.999))\n",
    "\n",
    "# datasets.\n",
    "iterator_train = GenerateIterator()\n",
    "iterator_val = GenerateIterator_eval()\n",
    "\n",
    "# loss params.\n",
    "dw = 1e-2\n",
    "cw = 1\n",
    "sw = 1\n",
    "tw = 1e-2\n",
    "bw = 1e-2\n",
    "\n",
    "\n",
    "''' Exponential moving average (simulating teacher model) '''\n",
    "ema = EMA(0.998)\n",
    "ema.register(classifier)\n",
    "\n",
    "# training..\n",
    "for epoch in range(1, 31):\n",
    "    iterator_train.dataset.shuffledata()\n",
    "    pbar = tqdm(iterator_train, disable=True,\n",
    "                bar_format=\"{percentage:.0f}%,{elapsed},{remaining},{desc}\")\n",
    "\n",
    "    loss_main_sum, n_total = 0, 0\n",
    "    loss_domain_sum, loss_src_class_sum, \\\n",
    "    loss_src_vat_sum, loss_trg_cent_sum, loss_trg_vat_sum = 0, 0, 0, 0, 0\n",
    "    loss_disc_sum = 0\n",
    "\n",
    "    for images_source, labels_source, images_target, labels_target in pbar:\n",
    "        images_source, labels_source, images_target, labels_target = images_source.cuda(), labels_source.cuda(), images_target.cuda(), labels_target.cuda()\n",
    "\n",
    "        # pass images through the classifier network.\n",
    "        feats_source, pred_source = classifier(images_source)\n",
    "        feats_target, pred_target = classifier(images_target, track_bn=True)\n",
    "\n",
    "        ' Discriminator losses setup. '\n",
    "        # discriminator loss.\n",
    "        real_logit_disc = feature_discriminator(feats_source.detach())\n",
    "        fake_logit_disc = feature_discriminator(feats_target.detach())\n",
    "\n",
    "        loss_disc = 0.5 * (\n",
    "                sigmoid_xent(real_logit_disc, torch.ones_like(real_logit_disc, device='cuda')) +\n",
    "                sigmoid_xent(fake_logit_disc, torch.zeros_like(fake_logit_disc, device='cuda'))\n",
    "        )\n",
    "\n",
    "        ' Classifier losses setup. '\n",
    "        # supervised/source classification.\n",
    "        loss_src_class = xent(pred_source, labels_source)\n",
    "\n",
    "        # conditional entropy loss.\n",
    "        loss_trg_cent = cent(pred_target)\n",
    "\n",
    "        # virtual adversarial loss.\n",
    "        loss_src_vat = vat_loss(images_source, pred_source)\n",
    "        loss_trg_vat = vat_loss(images_target, pred_target)\n",
    "\n",
    "        # domain loss.\n",
    "        real_logit = feature_discriminator(feats_source)\n",
    "        fake_logit = feature_discriminator(feats_target)\n",
    "\n",
    "        loss_domain = 0.5 * (\n",
    "                sigmoid_xent(real_logit, torch.zeros_like(real_logit, device='cuda')) +\n",
    "                sigmoid_xent(fake_logit, torch.ones_like(fake_logit, device='cuda'))\n",
    "        )\n",
    "\n",
    "        # combined loss.\n",
    "        loss_main = (\n",
    "                dw * loss_domain +\n",
    "                cw * loss_src_class +\n",
    "                sw * loss_src_vat +\n",
    "                tw * loss_trg_cent +\n",
    "                tw * loss_trg_vat\n",
    "        )\n",
    "\n",
    "        ' Update network(s) '\n",
    "\n",
    "        # Update discriminator.\n",
    "        optimizer_disc.zero_grad()\n",
    "        loss_disc.backward()\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        # Update classifier.\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss_main.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        # Polyak averaging.\n",
    "        ema(classifier)  \n",
    "\n",
    "        loss_domain_sum += loss_domain.item()\n",
    "        loss_src_class_sum += loss_src_class.item()\n",
    "        loss_src_vat_sum += loss_src_vat.item()\n",
    "        loss_trg_cent_sum += loss_trg_cent.item()\n",
    "        loss_trg_vat_sum += loss_trg_vat.item()\n",
    "        loss_main_sum += loss_main.item()\n",
    "        loss_disc_sum += loss_disc.item()\n",
    "        n_total += 1\n",
    "        \n",
    "        pbar.set_description('loss {:.3f},'\n",
    "                             ' domain {:.3f},'\n",
    "                             ' s cls {:.3f},'\n",
    "                             ' s vat {:.3f},'\n",
    "                             ' t c-ent {:.3f},'\n",
    "                             ' t vat {:.3f},'\n",
    "                             ' disc {:.3f}'.format(\n",
    "            loss_main_sum/n_total,\n",
    "            loss_domain_sum/n_total,\n",
    "            loss_src_class_sum/n_total,\n",
    "            loss_src_vat_sum/n_total,\n",
    "            loss_trg_cent_sum/n_total,\n",
    "            loss_trg_vat_sum/n_total,\n",
    "            loss_disc_sum / n_total,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # validate.\n",
    "    if epoch % 1 == 0:\n",
    "        classifier.eval()\n",
    "        feature_discriminator.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds_val, gts_val = [], []\n",
    "            val_loss = 0\n",
    "            for images_target, labels_target in iterator_val:\n",
    "                images_target, labels_target = images_target.cuda(), labels_target.cuda()\n",
    "\n",
    "                # cross entropy based classification\n",
    "                _, pred_val = classifier(images_target)\n",
    "\n",
    "                pred_val = np.argmax(pred_val.cpu().data.numpy(), 1)\n",
    "\n",
    "                preds_val.extend(pred_val)\n",
    "                gts_val.extend(labels_target)\n",
    "\n",
    "            preds_val = np.asarray(preds_val)\n",
    "            gts_val = np.asarray(gts_val)\n",
    "\n",
    "            score_cls_val = (np.mean(preds_val == gts_val)).astype(np.float)\n",
    "            print('\\n({}) Target accuracy (%) {:.3f}\\n'.format(epoch, score_cls_val))\n",
    "\n",
    "        feature_discriminator.train()\n",
    "        classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
