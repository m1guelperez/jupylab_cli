{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Getting Started with Automated Data Pipelines series is a set of three notebooks and livestreams (recordings are available) designed to help you get started with creating data pipeline that allow you to automate the process of moving and transforming data.\n",
    "\n",
    "* Day 1: Versioning & Creating Datasets from GitHub Repos\n",
    "    * [Notebook](https://www.kaggle.com/rtatman/kerneld4769833fe/)\n",
    "    * [Livestream](https://youtu.be/Xi140XVOznM)\n",
    "* Day 2: Validation & Creating Datasets from URL's\n",
    "    * [Notebook](https://www.kaggle.com/rtatman/automating-data-pipelines-day-2)\n",
    "    * [Livestream](https://youtu.be/-wF1hSEQqIc)\n",
    "* Day 3: ETL & Creating Datasets from Kernel Output\n",
    "    * [Notebook](https://www.kaggle.com/rtatman/automating-data-pipelines-day-3)\n",
    "    * [Livestream](https://youtu.be/2pWifnSPN5E)     \n",
    "_____\n",
    "\n",
    "Welcome to the first day of Getting Started with Automated Data Pipelines!\n",
    "\n",
    "Today we're going to cover two things: \n",
    "\n",
    "* Data versioning: what is it and when should you do it?\n",
    "* Creating a Kaggle dataset from GitHub\n",
    "\n",
    "Iâ€™ll be going over this notebook live at 9:00 AM Pacific time on January, 29 2019. [Hereâ€™s a link to the livestream, which should also point to the recording if you miss the livestream](https://youtu.be/Xi140XVOznM), \n",
    "\n",
    "\n",
    "# Data Versioning\n",
    "\n",
    "You might already be familiar with versioning from working with version control or source control for your code. The basic idea is that, as you work, you create static copies of your work that you can refer back to later. (This is what the \"commit\" button in kernels does.) This is particularly important when youâ€™re collaborating because if you and a collaborator are working on the same file, you can compare each of your versions and decide which changes to make. \n",
    "\n",
    "So version control for code is a good ideaâ€¦ but what about data?\n",
    "\n",
    "The idea that you should version your data is actually a somewhat  controversial. \n",
    "\n",
    "[Lots of people](https://www.datacamp.com/community/blog/version-control-data-science) [will recommend](https://datascience.stackexchange.com/questions/5178/how-to-deal-with-version-control-of-large-amounts-of-binary-data/6943#6943) [that you donâ€™t version your data](http://columbia-applied-data-science.github.io/homework/2013/04/29/homework-08-stackoverflow-questions/) at all. And there are some good reasons for this: \n",
    "\n",
    "* Most version control software is designed for files with code in them, and these files generally arenâ€™t very big. Trying to use version control software for large files (more than a couple MB) means that a lot of their useful features, like showing line differences, are no longer as useful.\n",
    "* Version control can mean storing multiple copies of files, and if you have a large dataset this can quickly get very expensive.\n",
    "* If youâ€™re routinely backing up your data and are using version control for the queries or script youâ€™re using to extract data, then it may be redundant to store specific subsets of your data separately.\n",
    "\n",
    "I agree that you donâ€™t need to save separate versions of your data for every single task you do. However, **if youâ€™re training machine learning models, I believe you should version both the exact data you used to train your model and your code**. This is because, without the code, data and environment, you wonâ€™t be able to reproduce your model. (I wrote [a whole paper about it if youâ€™re interested](https://openreview.net/forum?id=B1eYYK5QgX).)\n",
    "\n",
    "This becomes a really thorny problem if you want to run experiments and compare models to each other. Does this new model architecture outperform the one youâ€™re currently using? Without knowing what data you trained the original model on, itâ€™s hard to tell.\n",
    "\n",
    "## When is data versioning appropriate?\n",
    "\n",
    "When should you version your data?\n",
    "\n",
    "* When making schema/metadata changes, like adding or deleting columns or changing the units that information is stored in.\n",
    "* When youâ€™re training experimental machine learning models. The smallest reproducible unit for machine learning models is training data + model specification code. (I talk more about this [in this blog post](http://blog.kaggle.com/2018/09/19/help-i-cant-reproduce-a-machine-learning-project/).)\n",
    "\n",
    "When should you consider not versioning your data?\n",
    "\n",
    "* When your data isnâ€™t being used to train models. For example, itâ€™s more space efficient to just save the SQL query you used to make a chart than it is to save all the transformed data.\n",
    "* When your data is large enough that storing a versioned copy would be prohibitively expensive. In this case, Iâ€™d recommend versioning both the scripts you used to extract the data and enough descriptive statistics that you could re-generate a very similar dataset.\n",
    "* When your project lives entirely on GitHub. Versioning large datasets via GitHub can quickly become unwieldy. (GitLFS can help with this, but if youâ€™re storing very large datasets, in general GitHub probably isnâ€™t the best tool for the job. A database or blog storage hosting service specifically designed for large data will generally give you fewer headaches. Most cloud services will generally already have some form of versioning built in.) \n",
    "\n",
    "Of course, whether or not you should version data eventually comes down to a judgement call on your part. \n",
    "\n",
    "## Tools for data versioning when working locally\n",
    "\n",
    "If your data is on Kaggle, we already take care of the data versioning for you; you can scroll to the bottom of your dataset landing page to see the \"History\" tab and check out previous versions of the dataset, what changes were made between versions and any updates that have been made to the metadata. [This dataset of  bike counters in Ottawa is a good example of a dataset that's  gone through multiple iterations](https://www.kaggle.com/m7homson/ottawa-bike-counters).\n",
    "\n",
    "But some data shouldn't be put on Kaggle ([we're not HIPAA compliant right now](https://www.hhs.gov/hipaa/for-professionals/special-topics/cloud-computing/index.html), for example) and sometimes you might prefer to work locally. What tools can you use in that case?\n",
    "\n",
    "> **If you're already using cloud tools to store your data, most platforms will have versioning built in.** You should probably use these rather than setting up your own system, if for no other reason than that it will be someone elseâ€™s problem when it inevitably breaks. ðŸ˜‚\n",
    "\n",
    "The ecosystem of data versioning tools is still pretty young, but you do have some options. Right now, two of the more popular are [DVC](https://dvc.org/), which is short for Data Version Control, and [Pachyderm](https://pachyderm.io/). These aren't coding environments like Kaggle Kernels are. Rather, they're similar to Git: they let you save specific versions of your code and data along with comments on them. This allows you to revert your work back later and track what you did and to collaborate with others on the same code/data. \n",
    "\n",
    "The biggest difference between these tools and Git is that they are specifically designed to handle data files and trained models as well as code.\n",
    "\n",
    "**Similarities:**\n",
    "\n",
    "* Both are based on Git and are designed to interface well with existing Git toolchains. \n",
    "* They focus on versioning whole pipelines, versioning the data, code and trained models together.\n",
    "\n",
    "**Differences:**\n",
    "\n",
    "* DVC is only available as a command line tool. Pachyderm also has a graphical user interface (GUI).\n",
    "* DVC is open source and free. Pachyderm does have an open source version, but to get all the bells and whistles you'll need to shell out for the enterprise edition.\n",
    "* Pachyderm is fully containerized (Docker and Kubernetes). You can use containers with DVC but they're not the default.\n",
    "\n",
    "Other options for versioning data and pipelines include, in alphabetical order: [Dataiku](https://www.dataiku.com/), [Datalad](https://www.datalad.org/), [Datmo](https://github.com/datmo/datmo), [GitLFS](https://git-lfs.github.com/), [qri](https://github.com/qri-io/qri) and [Quilt](https://quiltdata.com/). These are a mix of closed and open source and a lot of them are in development, so it's a good idea to do some shopping around before you commit to a tooling system.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "This section is a little bit theoretical, so I've got some discussion questions for you.\n",
    "\n",
    "For each of these datasets consider whether it makes sense to version this data.\n",
    "\n",
    "* You have a streaming datasets of sensor data with more than five billion columns. It's updated every five seconds. Youâ€™re using it to create a dashboard to let stakeholders monitor anomalies. \n",
    "* Youâ€™ve got a .CSV with a few thousand rows of student data. Youâ€™re storing it in a computer that conforms with your country's laws around data privacy. You want to build a model to see if thereâ€™s an effect of when tests are administered on test scores.\n",
    "*  Youâ€™re working with a customer database of one million pet owners who have ordered custom dog food through your startup. You want to create a slide deck with visualizations that summarize information about your customers to help the marketing team decide where to buy newspaper ads. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your notes can go here if you like. :)\n",
    "# You can also answer/discuss on the comments of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Kaggle dataset from GitHub\n",
    "\n",
    "OK, now that weâ€™ve got some theory out of the way and you should have a better idea of whether versioning is appropriate, letâ€™s make some new datasets! \n",
    "\n",
    "Today, weâ€™re going to be making datasets from GitHub repos (short for â€œrepositoriesâ€). First, youâ€™ll need to pick a repo. \n",
    "\n",
    "> Currently, we only support creating datasets from **public** repos. If you donâ€™t have any public data of your own, you can check out [this list of public GitHub datasets](https://github.com/awesomedata/awesome-public-datasets) to see if something tickles your fancy.\n",
    "\n",
    "Once youâ€™ve picked a repo, itâ€™s fairly easy to create your dataset.\n",
    "\n",
    "## Create a Kaggle Dataset from a GitHub repo\n",
    "\n",
    "* Go to www.kaggle.com/datasets (or just click on the \"Datasets\" tab up near the search bar). \n",
    "* Click on \"New Dataset\".\n",
    "* In the modal that pops up, click on the circle with a silhouette of an Octocat (the GitHub mascot) in it. \n",
    "* Enter a dataset title and the URL of the GitHub repository you're interested in. If the URL is valid, you should see a list of all the files that will be included in your dataset. \n",
    "* Hit create. \n",
    "* Success!\n",
    "\n",
    "## Modify versioning\n",
    "\n",
    "* Go to the page for your dataset. (You can find it by clicking on the \"Datasets\" tab next to the search bar and then clicking on the \"Your Datasets\" tab.)\n",
    "* Click on the \"Settings\" tab. (The end of the URL will be /settings.)\n",
    "* **To turn off versioning**, select \"Latest version only\" in the \"Versioning\" drop down. \n",
    "* **To automatically update your dataset**, choose your preferred frequency from the \"Update\" dropdown.\n",
    "* That's all there is to it! ðŸ˜Ž"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
