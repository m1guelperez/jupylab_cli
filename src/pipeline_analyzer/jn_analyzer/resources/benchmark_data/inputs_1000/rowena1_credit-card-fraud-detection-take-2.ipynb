{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previously](http://www.kaggle.com/rowena1/my-first-kernel), through undersampling the majority class, I was able to get a Random Forest classifier that outperforms a no-skill dummy classifier -- area under precision recall curve was 0.4816 compared to 0.2538 in the no-skill case.  The aim of this exercise is to improve upon the previous classifier.\n",
    "\n",
    "Here, instead of undersampling the majority class, I used Synthetic Minority Over-sampling Technique. This raised the area under the precision-recall curve from 0.4816 to 0.5445.  Then, by tweaking the hyperparameters improved model performance further, bringing the area under the precision-recall curve to 0.8163.  Also, this was achieved with a test set that preserved the imbalance between the majority and minority classes to better represent reality.  However, since PCA was applied to the original dataset to protect customer privacy, there would have been some information leaked to the test set.\n",
    "\n",
    "Searching through various combinations of hyperparameters is time consuming.  It is more efficient to first launch an interim model using default hyperparametric settings and an edited set of predictors.  Then, search for an optimized model and release that.  Using a more parsimonious set of predictors also aids interpretability of the decision paths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['creditcard.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Get tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, auc, roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3  ...         V27       V28  Amount  Class\n",
      "0   0.0 -1.359807 -0.072781  2.536347  ...    0.133558 -0.021053  149.62      0\n",
      "1   0.0  1.191857  0.266151  0.166480  ...   -0.008983  0.014724    2.69      0\n",
      "2   1.0 -1.358354 -1.340163  1.773209  ...   -0.055353 -0.059752  378.66      0\n",
      "3   1.0 -0.966272 -0.185226  1.792993  ...    0.062723  0.061458  123.50      0\n",
      "4   2.0 -1.158233  0.877737  1.548718  ...    0.219422  0.215153   69.99      0\n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('../input/creditcard.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         V1        V4        V6  ...         V17       V18  Class\n",
      "0 -1.359807  1.378155  0.462388  ...    0.207971  0.025791      0\n",
      "1  1.191857  0.448154 -0.082361  ...   -0.114805 -0.183361      0\n",
      "2 -1.358354  0.379780  1.800499  ...    1.109969 -0.121359      0\n",
      "3 -0.966272 -0.863291  1.247203  ...   -0.684093  1.965775      0\n",
      "4 -1.158233  0.403034  0.095921  ...   -0.237033 -0.038195      0\n",
      "\n",
      "[5 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#From previous exercise, unsupporting variables and variables with collinearity were identified\n",
    "#Get rid of those for a simpler set of predictors\n",
    "df.drop(['Amount','Time'],axis=1,inplace=True)\n",
    "df.drop(['V8','V13','V23','V26','V27','V28'],axis=1,inplace=True)\n",
    "df.drop(['V2','V3','V5','V7','V9','V11','V15','V19','V20','V21','V22','V24','V25'],axis=1,inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraudulent share, train set (before SMOTE): 0.17%\n",
      "Fraudulent share, test set: 0.17%\n"
     ]
    }
   ],
   "source": [
    "#As in previous exercise, imbalanced class ratio warrants\n",
    "#(1)training set with balanced class representation; and \n",
    "#(2)an unseen test set with a similar distribution of minority to majority class (fraud ~ 0.17%)\n",
    "\n",
    "#Instead of undersampling the majority class, here I use Synthetic Minority Over-sampling Technique (SMOTE)\n",
    "\n",
    "#First, split into train and test sets to ensure no information leaked to test set\n",
    "\n",
    "X,y=df.iloc[:,:-1],df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n",
    "\n",
    "#Check majority vs. minority class distribution in train and test sets\n",
    "\n",
    "print('Fraudulent share, train set (before SMOTE): {0:.2%}'.format(sum(y_train==1)/len(y_train)))\n",
    "print('Fraudulent share, test set: {0:.2%}'.format(sum(y_test==1)/len(y_test))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraudulent share, train set (after SMOTE): 50.00%\n"
     ]
    }
   ],
   "source": [
    "#Apply SMOTE to train set\n",
    "sm=SMOTE(random_state=22)\n",
    "X_resampled, y_resampled=sm.fit_sample(X_train,y_train)\n",
    "\n",
    "#Check majority vs. minority class distribution in train set after resampling\n",
    "\n",
    "print('Fraudulent share, train set (after SMOTE): {0:.2%}'.format(sum(y_resampled==1)/len(y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                       n_jobs=None, oob_score=False, random_state=22, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit Random Forest Classifier used previously to data\n",
    "#Predict and evaluate effect of SMOTE\n",
    "\n",
    "RFC_mod=RandomForestClassifier(max_depth=4,random_state=22)\n",
    "\n",
    "RFC_mod.fit(X_resampled,y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with Random Forest Classifier model took: 0.0468 seconds\n",
      "\n",
      "Area under precision recall curve, Random Forest Classifier model: 0.5445\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Fraud       1.00      0.99      1.00     85295\n",
      "       Fraud       0.20      0.89      0.32       148\n",
      "\n",
      "    accuracy                           0.99     85443\n",
      "   macro avg       0.60      0.94      0.66     85443\n",
      "weighted avg       1.00      0.99      1.00     85443\n",
      "\n",
      "\n",
      "[[84757   538]\n",
      " [   16   132]]\n"
     ]
    }
   ],
   "source": [
    "#Predict and check AUPRC as well as time the prediction\n",
    "\n",
    "t0=time.time()\n",
    "y_pred_RFC=RFC_mod.predict(X_test)\n",
    "t1=time.time()\n",
    "\n",
    "print('Predicting with Random Forest Classifier model took: {0:.4f} seconds'.format(t1-t0))\n",
    "\n",
    "labels=['No Fraud','Fraud']\n",
    "#Calculate precision recall curve and area under curve\n",
    "precision_RFC,recall_RFC,threshold_RFC=precision_recall_curve(y_test,y_pred_RFC)\n",
    "auprc_RFC=auc(recall_RFC,precision_RFC)\n",
    "print()\n",
    "print('Area under precision recall curve, Random Forest Classifier model: {0:.4f}'.format(auprc_RFC))\n",
    "print()\n",
    "print(classification_report(y_test,y_pred_RFC,target_names=labels))\n",
    "print()\n",
    "print(confusion_matrix(y_test,y_pred_RFC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9995956187669541\n",
      "{'max_depth': None, 'n_estimators': 16}\n"
     ]
    }
   ],
   "source": [
    "#Using SMOTE instead of undersampling the majority the class improved model performance\n",
    "#Area under precision recall curve increased to 0.5445 from 0.4824\n",
    "#Prediction still took under one second - YAY!\n",
    "\n",
    "#Let's see if changing hyperparameters of Random Forest Classifier can improve model performance further\n",
    "#Here, I choose to tweak the \n",
    "#(1)max_depth (maximum depth of each tree), and \n",
    "#(2)n_estimators (number of trees).\n",
    "\n",
    "# Maximum depth of each tree\n",
    "max_depth=[None,5,10]\n",
    "\n",
    "# Number of trees\n",
    "n_estimators=[10,12,14,16]\n",
    "\n",
    "#create grid\n",
    "grid={\n",
    "    'max_depth': max_depth,\n",
    "    'n_estimators': n_estimators\n",
    "}\n",
    "\n",
    "#Random search of parameters\n",
    "RFC_search=GridSearchCV(estimator=RandomForestClassifier(random_state=22), param_grid=grid,cv=3,scoring='f1',verbose=True)\n",
    "\n",
    "#Fit model\n",
    "RFC_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "#print results\n",
    "print(RFC_search.best_score_)\n",
    "print(RFC_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with Random Forest Classifier model took: 0.1233 seconds\n",
      "\n",
      "Area under precision recall curve, Random Forest Classifier model: 0.8163\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Fraud       1.00      1.00      1.00     85295\n",
      "       Fraud       0.81      0.82      0.82       148\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.90      0.91      0.91     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "\n",
      "[[85266    29]\n",
      " [   26   122]]\n"
     ]
    }
   ],
   "source": [
    "#Wow! That took a long time for hardly an exhaustive search!\n",
    "\n",
    "#The best max_depth and n_estimators identified are:\n",
    "#'None' and '16', respectively.\n",
    "#Refit the random forest classifier with these hyperparameters and check performance\n",
    "\n",
    "RFC_mod=RandomForestClassifier(random_state=22,max_depth=None,n_estimators=16)\n",
    "RFC_mod.fit(X_resampled,y_resampled)\n",
    "\n",
    "t0=time.time()\n",
    "y_pred_RFC=RFC_mod.predict(X_test)\n",
    "t1=time.time()\n",
    "\n",
    "print('Predicting with Random Forest Classifier model took: {0:.4f} seconds'.format(t1-t0))\n",
    "\n",
    "#Calculate precision recall curve and area under curve\n",
    "precision_RFC,recall_RFC,threshold_RFC=precision_recall_curve(y_test,y_pred_RFC)\n",
    "auprc_RFC=auc(recall_RFC,precision_RFC)\n",
    "print()\n",
    "print('Area under precision recall curve, Random Forest Classifier model: {0:.4f}'.format(auprc_RFC))\n",
    "print()\n",
    "print(classification_report(y_test,y_pred_RFC,target_names=labels))\n",
    "print()\n",
    "print(confusion_matrix(y_test,y_pred_RFC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with Random Forest Classifier model took: 0.7475 seconds\n",
      "\n",
      "Area under precision recall curve, Random Forest Classifier model: 0.8042\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Fraud       1.00      1.00      1.00     85295\n",
      "       Fraud       0.77      0.84      0.80       148\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.88      0.92      0.90     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "\n",
      "[[85258    37]\n",
      " [   24   124]]\n"
     ]
    }
   ],
   "source": [
    "#Great! The area under the precision recall curve rose to 0.8163\n",
    "#and the number of false positives and false negatives limited.\n",
    "\n",
    "#Since the Random Forest Classifier is a 'bagging' algorithm, \n",
    "#wouldn't increasing the number of trees improve performance?\n",
    "#Right now, the default number of trees is 10; but this will be changed to 100 soon.\n",
    "#Let's fit this soon-to-be 'default' model to our training set and compare the performance.\n",
    "\n",
    "RFC_mod=RandomForestClassifier(random_state=22,max_depth=None,n_estimators=100)\n",
    "RFC_mod.fit(X_resampled,y_resampled)\n",
    "\n",
    "t0=time.time()\n",
    "y_pred_RFC=RFC_mod.predict(X_test)\n",
    "t1=time.time()\n",
    "\n",
    "print('Predicting with Random Forest Classifier model took: {0:.4f} seconds'.format(t1-t0))\n",
    "\n",
    "#Calculate precision recall curve and area under curve\n",
    "precision_RFC,recall_RFC,threshold_RFC=precision_recall_curve(y_test,y_pred_RFC)\n",
    "auprc_RFC=auc(recall_RFC,precision_RFC)\n",
    "print()\n",
    "print('Area under precision recall curve, Random Forest Classifier model: {0:.4f}'.format(auprc_RFC))\n",
    "print()\n",
    "print(classification_report(y_test,y_pred_RFC,target_names=labels))\n",
    "print()\n",
    "print(confusion_matrix(y_test,y_pred_RFC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as good as the classifier using n_estimators=16 but, still, comparable performance.  It seems more efficient to spend time identifying supportive / correlated features before fitting a model than to depend on an exhaustive search for the best hyperparameters.  Even though the Random Forest Classifier was developed to minimize overfitting, it is not immune to it.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
