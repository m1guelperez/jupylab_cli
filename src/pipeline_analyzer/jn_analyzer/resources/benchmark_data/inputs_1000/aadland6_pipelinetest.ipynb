{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "99a6fc0b-fb51-408e-8286-84eca2e41d8e",
    "_kg_hide-output": true,
    "_uuid": "25f7c6f106386ab466021ff2245f3273345f2dd3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "from nltk import tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag\n",
    "from pkg_resources import resource_filename as filename\n",
    "DEFAULT_STOPWORDS = set(stopwords.words(\"english\")) | set(string.ascii_letters)\n",
    "DEFAULT_LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#### Data Setup ####\n",
    "DEFAULT_STRING = \"@#$%^&*()+=,.:;'{}[]|<>`?“”\"\n",
    "EXCLUSION = {}\n",
    "for character in DEFAULT_STRING:\n",
    "    EXCLUSION[character] = \"\"\n",
    "EXCLUSION['\"'] = \"\"\n",
    "EXCLUSION[\"\\\\\"] = \" \"\n",
    "EXCLUSION[\"/\"] = \" \"\n",
    "EXCLUSIONS_TABLE = EXCLUSION\n",
    "# negations map \n",
    "NEGATIONS = [\"not \", \"no \",]\n",
    "NEGATIONS_MAP = [\"not_\", \"no_\"]\n",
    "NEGATIONS_TABLE = {}\n",
    "for negation, conversion in zip(NEGATIONS, NEGATIONS_MAP):\n",
    "    NEGATIONS_TABLE[\"\\\\b{0}\\\\b\".format(negation)] = conversion\n",
    "NEGATIONS = NEGATIONS_TABLE\n",
    "\n",
    "NEGATIONS_RE = re.compile(\"{0}\".format(\"|\").join(NEGATIONS.keys()),\n",
    "                          flags=re.IGNORECASE)\n",
    "\n",
    "#### Processing Functions ####\n",
    "def replacement_gen(document, repl_dict=NEGATIONS, repl=NEGATIONS_RE):\n",
    "    \"\"\" Replaces specific phrases with a corresponding term\n",
    "        Args:\n",
    "            document(str): pre-tokenized string\n",
    "            repl_dict(dict): dict of words and their replacements\n",
    "            repl(SRE_Pattern): precompiled regrex pattern\n",
    "        Returns:\n",
    "            document(str): orginal document with the target words replaced\n",
    "    \"\"\"\n",
    "    def replace(match):\n",
    "        \"\"\" replaces the match key\n",
    "        \"\"\"\n",
    "        match_token = \"\\\\b{0}\\\\b\".format(match.group(0).lower())\n",
    "        return repl_dict[match_token]\n",
    "    return repl.sub(replace, document)\n",
    "def token_gen(document):\n",
    "    \"\"\"Generates tokens using nltk.word_tokenize\n",
    "        Args:\n",
    "            document(str): higher level document primative\n",
    "        Returns:\n",
    "            tokens(list): list of word tokens\n",
    "    \"\"\"\n",
    "    return tokenize.word_tokenize(document)\n",
    "\n",
    "# def keep_gen(token, LETTERS=string.ascii_letters):\n",
    "#    return [token for token in tokens if set(tokens).intersection(LETTERS)]\n",
    "\n",
    "def clean_gen(tokens, exclusion_table=EXCLUSIONS_TABLE, LETTERS=set(string.ascii_letters)):\n",
    "    \"\"\"Cleans a list of tokens\n",
    "        Args:\n",
    "            tokens(list): list of word tokens\n",
    "            exclusion_table(dict) characters to remove from a token\n",
    "                defaults to \"!@#$%^&*()+=,.:;'{}[]|<>`?“”\"\n",
    "        Returns:\n",
    "            clean_tokens(list): tokens with the offending characters removed\n",
    "    \"\"\"\n",
    "    exclusion = str.maketrans(exclusion_table)\n",
    "    return [token.translate(exclusion).lower() for token in tokens if set(token).intersection(LETTERS)]\n",
    "\n",
    "\n",
    "def wordnet_get(tagged_tokens):\n",
    "    \"\"\"Helper function for normalizing wordnet labels\n",
    "    \"\"\"\n",
    "    out_tokens = []\n",
    "    for token in tagged_tokens:\n",
    "        if token[1].startswith(\"J\"):\n",
    "            out_token = (token[0], wordnet.ADJ)\n",
    "        elif token[1].startswith(\"V\"):\n",
    "            out_token = (token[0], wordnet.VERB)\n",
    "        elif token[1].startswith(\"R\"):\n",
    "            out_token = (token[0], wordnet.ADV)\n",
    "        else:\n",
    "            out_token = (token[0], wordnet.NOUN)\n",
    "        out_tokens.append(out_token)\n",
    "    return out_tokens\n",
    "\n",
    "def pos_gen(tokens):\n",
    "    \"\"\"Generates parts of speech and normalizes them to wordnet labels\n",
    "    \"\"\"\n",
    "    tagged = wordnet_get(pos_tag(tokens))\n",
    "    return tagged \n",
    "\n",
    "def lemma_gen(tokens, wnl=DEFAULT_LEMMATIZER, tag=False):\n",
    "    \"\"\"Lemmatizes words\n",
    "        Args:\n",
    "            tokens(list): list of word strings\n",
    "            wnl(WordNetLemmatizer): lemmatizer object\n",
    "            tag(bool): performs part of speech tagging if true defaults to False \n",
    "                for speed\n",
    "        Returns:\n",
    "            lemms(list):list of words that have been lemmatized\n",
    "    \"\"\"\n",
    "    if tag:\n",
    "        wnl_tokens = pos_gen(tokens)\n",
    "        lemmas = [wnl.lemmatize(token[0], pos=token[1]) for token in wnl_tokens]\n",
    "    else:\n",
    "        lemmas = [wnl.lemmatize(token) for token in tokens]\n",
    "    return lemmas\n",
    "def stopword_gen(tokens, default=DEFAULT_STOPWORDS, custom=None):\n",
    "    \"\"\"Removes the stopwords\n",
    "        Args:\n",
    "            tokens(list): list of word tokens\n",
    "            default(set): set of the stopwords in nltk's stopword corpus\n",
    "            custom(set): custom stopwords\n",
    "        returns:\n",
    "            no_stops(list): lists of words not found in either set\n",
    "    \"\"\"\n",
    "    if custom is not None:\n",
    "        module_stopwords = default | custom\n",
    "    else:\n",
    "        module_stopwords = default\n",
    "    return [word for word in tokens if word not in module_stopwords]\n",
    "def default_gen(documents):\n",
    "    \"\"\"Default pipeline for cleaning a text field\n",
    "        Args:\n",
    "            documents(list): list of strings with the top level document\n",
    "            Runs in the order of:\n",
    "                1. Tokenize\n",
    "                2. Character level cleaning (numbers, punctuation, etc.)\n",
    "                3. Part of Speech Tagging\n",
    "                4. Lemmatize the tokens\n",
    "                5. Generate phrases for negations\n",
    "                6. Remove stopwords\n",
    "        Yields:\n",
    "            finished_document(list): list of tokens with the text normalized\n",
    "    \"\"\"\n",
    "    for document in documents:\n",
    "        tokens = token_gen(document)\n",
    "        clean_tokens = clean_gen(tokens)\n",
    "        lemmas = lemma_gen(clean_tokens, tag=True)\n",
    "        # add the phrase model here\n",
    "        negations = replacement_gen(\" \".join(lemmas), NEGATIONS,\n",
    "                                    NEGATIONS_RE)\n",
    "        new_tokens = negations.split(\" \")\n",
    "        finished_document = stopword_gen(new_tokens)\n",
    "        yield finished_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "ef9477d1-6d4e-41be-9eef-2021c572da53",
    "_uuid": "ab3801c44f884e861cc143d45badecbeb09e66e7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_phrases(documents):\n",
    "    \"\"\"Generate a pipeline before the phrases are built\n",
    "    \"\"\"\n",
    "    for document in documents:\n",
    "        doc_string = \" \".join(document)\n",
    "        raw_tokens = token_gen(doc_string)\n",
    "        clean_tokens = clean_gen(raw_tokens)\n",
    "        lemmas = lemma_gen(clean_tokens, tag=False)\n",
    "        yield lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "a403063e-ff0d-4895-9cc5-edc7b48c9863",
    "_uuid": "e2c27f0b8336466c11c35537252a2e94ccb8b03e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "data = pd.read_json(\"../input/train.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "dc566919-f6c0-45d9-b240-8a66865d5a6e",
    "_uuid": "ddc66a51ec72c77c57aef18052bb14857595dd29",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Function Pipeline for the Data processing ####\n",
    "def ingredient_pipeline(recipe, bigram_model):\n",
    "    \"\"\"Operates on individual recipes\n",
    "    \"\"\"\n",
    "    new_recipe = []\n",
    "    for ingredient in recipe:\n",
    "        ingredient_tokens = token_gen(ingredient)\n",
    "        clean_tokens = clean_gen(ingredient_tokens)\n",
    "        lemmas = lemma_gen(clean_tokens)\n",
    "        bigrams = bigram_model[lemmas]\n",
    "        clean_ingredient = \" \".join(bigrams)\n",
    "        new_recipe.append(clean_ingredient)\n",
    "    return new_recipe\n",
    "\n",
    "def all_recipes(recipes, bigram_model):\n",
    "    \"\"\"Operates on all the recipes\n",
    "    \"\"\"\n",
    "    for recipe in recipes:\n",
    "        yield ingredient_pipeline(recipe, bigram_model)\n",
    "\n",
    "def subset_pairs(X_rows, y_rows, target_1, target_2):\n",
    "    \"\"\"Generates a unique classifier to distinguish between two cuisine types \n",
    "    \"\"\"\n",
    "    X_index, y_subset = [], []\n",
    "    for index, row in enumerate(y_rows):\n",
    "        if row == target_1 or row == target_2:\n",
    "            y_subset.append(row)\n",
    "            X_index.append(index)\n",
    "    X_subset = X_rows[X_index]\n",
    "    return X_subset, y_rows[y_subset]\n",
    "\n",
    "def regroup(X_rows, y_rows, target_1, target_2):\n",
    "    \"\"\"Splits the testing pairs again\n",
    "    \"\"\"\n",
    "    X_index, y_subset = [], []\n",
    "    for index, row in enumerate(y_rows.index):\n",
    "        if y_rows[row] == target_1 or y_rows[row] == target_2:\n",
    "            y_subset.append(row)\n",
    "            X_index.append(index)\n",
    "    X_subset = X_rows[X_index]\n",
    "    return X_subset, y_rows[y_subset]\n",
    "\n",
    "def pairwise_clf(X_predictions, y_predictions, target_1, target_2):\n",
    "    \"\"\"Generates a binary classifier between target_1 and target_2\n",
    "    \"\"\"\n",
    "    retrain_x, retrain_y = regroup(X_train, y_train, target_1, target_2)\n",
    "    retrain_test_x, retrain_test_y = regroup(X_test, y_predictions, target_1, target_2)\n",
    "    retrain_clf = SGDClassifier(shuffle=True).fit(retrain_x, retrain_y)\n",
    "    retrain_predictions = retrain_clf.predict(retrain_test_x)\n",
    "    retrain_predictions = pd.Series(retrain_predictions, index=retrain_test_x.index)\n",
    "    # print(classification_report(retrain_test_y, retrain_predictions))\n",
    "    return retrain_predictions\n",
    "\n",
    "def reassign_groups(X_train, y_train, first_pred, y_test, targets):\n",
    "    \"\"\"Reassigns groups through an iterator\n",
    "    \"\"\"\n",
    "    reassigned_predictions = first_pred\n",
    "    for target in targets:\n",
    "        print(\"Converting {0} and {1}\".format(target[0], target[1]))\n",
    "        prediction_next = pairwise_clf(X_train, y_train, target[0], target[1])\n",
    "        reassigned_predictions[prediction_next.index] = prediction_next\n",
    "        #print(classification_report(y_test, reassigned_predictions))\n",
    "    return reassigned_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "a672168b-0127-4445-a20c-d97211cbdeb0",
    "_uuid": "7367286f706d9409f1f7fd39fcc39284e31aabd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39774, 6714)\n"
     ]
    }
   ],
   "source": [
    "text_gen = pre_phrases(data[\"ingredients\"])\n",
    "bigram_model = Phraser(Phrases(text_gen))\n",
    "recipe_generator = all_recipes(data[\"ingredients\"], bigram_model)\n",
    "recipe_list = [x for x in recipe_generator]\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer = lambda doc: doc, lowercase=False)\n",
    "matrix = tfidf_vectorizer.fit_transform(data[\"ingredients\"])\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "960e5952-f2d7-4fdb-869c-26ce3439ab54",
    "_uuid": "89a960f1f6e2681abe2bd9a76595467ad6c2e05f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.72      0.41      0.53       102\n",
      "     british       0.63      0.32      0.42       153\n",
      "cajun_creole       0.82      0.74      0.78       312\n",
      "     chinese       0.76      0.89      0.82       547\n",
      "    filipino       0.84      0.64      0.72       157\n",
      "      french       0.65      0.56      0.60       517\n",
      "       greek       0.74      0.64      0.68       228\n",
      "      indian       0.81      0.90      0.85       586\n",
      "       irish       0.69      0.35      0.47       128\n",
      "     italian       0.75      0.92      0.83      1537\n",
      "    jamaican       0.73      0.60      0.66        90\n",
      "    japanese       0.85      0.67      0.75       309\n",
      "      korean       0.88      0.67      0.76       171\n",
      "     mexican       0.87      0.93      0.90      1292\n",
      "    moroccan       0.85      0.75      0.79       166\n",
      "     russian       0.64      0.36      0.46       101\n",
      " southern_us       0.70      0.79      0.74       873\n",
      "     spanish       0.73      0.30      0.42       192\n",
      "        thai       0.69      0.78      0.73       296\n",
      "  vietnamese       0.81      0.44      0.58       198\n",
      "\n",
      " avg / total       0.77      0.77      0.76      7955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split(matrix, data[\"cuisine\"], test_size=.20)\n",
    "sgd_clf = SGDClassifier(shuffle=True).fit(X_train, y_train)\n",
    "sgd_predictions = sgd_clf.predict(X_test)\n",
    "sgd_series = pd.Series(sgd_predictions, index=y_test.index)\n",
    "sgd_report = classification_report(y_test, sgd_predictions)\n",
    "print(sgd_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "f8d43955-0aee-4474-8807-4630f17a6bc9",
    "_uuid": "cee08fdeada54850d83a4285faf408a4ba5152c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting vietnamese and thai\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index (31817) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-cb5ef975cb6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         ('spanish', 'italian'), ('filipino', 'chinese')]\n\u001b[1;32m      7\u001b[0m new_predictions = reassign_groups(X_train=X_train, y_train=y_train, first_pred=sgd_series, y_test=y_test,\n\u001b[0;32m----> 8\u001b[0;31m                                  targets=pairs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-b348a747f6d2>\u001b[0m in \u001b[0;36mreassign_groups\u001b[0;34m(X_train, y_train, first_pred, y_test, targets)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Converting {0} and {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mprediction_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mreassigned_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m#print(classification_report(y_test, reassigned_predictions))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b348a747f6d2>\u001b[0m in \u001b[0;36mpairwise_clf\u001b[0;34m(X_predictions, y_predictions, target_1, target_2)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \"\"\"\n\u001b[1;32m     46\u001b[0m     \u001b[0mretrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mretrain_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrain_test_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mretrain_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mretrain_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrain_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrain_test_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b348a747f6d2>\u001b[0m in \u001b[0;36mregroup\u001b[0;34m(X_rows, y_rows, target_1, target_2)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0my_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mX_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mX_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_subset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;31m# [[1,2],??]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                 \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# [[1,2],j] or [[1,2],1:2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0mextracted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36mextractor\u001b[0;34m(indices, N)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mmin_indx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmin_indx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36mcheck_bounds\u001b[0;34m(indices, N)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mmax_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_indx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index (%d) out of range'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mmin_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index (31817) out of range"
     ]
    }
   ],
   "source": [
    "pairs = [(\"vietnamese\", \"thai\"), (\"brazilian\", \"mexican\"), (\"cajun_creole\", \"southern_us\"),\n",
    "        ('french', 'italian'), ('greek', 'italian'), ('british', 'southern_us'), \n",
    "        ('southern_us', 'italian'), ('southern_us', 'mexican'), ('irish', 'southern_us'),\n",
    "        ('british', 'italian'), ('italian', 'mexican'), ('russian', 'french'), \n",
    "         ('russian', 'southern_us'),\n",
    "        ('spanish', 'italian'), ('filipino', 'chinese')]\n",
    "new_predictions = reassign_groups(X_train=X_train, y_train=y_train, first_pred=sgd_series, y_test=y_test,\n",
    "                                 targets=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9944, 6714)\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_json(\"../input/test.json\")\n",
    "test_generator = all_recipes(test_data[\"ingredients\"], bigram_model)\n",
    "test_list = [x for x in test_generator]\n",
    "test_matrix = tfidf_vectorizer.transform(test_list)\n",
    "\n",
    "print(test_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting vietnamese and thai\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index (31817) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e093f415afb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m final_predictions = reassign_groups(X_train=X_train, y_train=y_train, first_pred=inital_series, y_test=y_test,\n\u001b[0;32m----> 5\u001b[0;31m                                  targets=pairs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-b348a747f6d2>\u001b[0m in \u001b[0;36mreassign_groups\u001b[0;34m(X_train, y_train, first_pred, y_test, targets)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Converting {0} and {1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mprediction_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_clf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mreassigned_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m#print(classification_report(y_test, reassigned_predictions))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b348a747f6d2>\u001b[0m in \u001b[0;36mpairwise_clf\u001b[0;34m(X_predictions, y_predictions, target_1, target_2)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \"\"\"\n\u001b[1;32m     46\u001b[0m     \u001b[0mretrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mretrain_test_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrain_test_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mretrain_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mretrain_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrain_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrain_test_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-b348a747f6d2>\u001b[0m in \u001b[0;36mregroup\u001b[0;34m(X_rows, y_rows, target_1, target_2)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0my_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mX_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mX_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_subset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;31m# [[1,2],??]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                 \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# [[1,2],j] or [[1,2],1:2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0mextracted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36mextractor\u001b[0;34m(indices, N)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mmin_indx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmin_indx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36mcheck_bounds\u001b[0;34m(indices, N)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mmax_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_indx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index (%d) out of range'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax_indx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0mmin_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index (31817) out of range"
     ]
    }
   ],
   "source": [
    "inital_test = sgd_clf.predict(test_matrix)\n",
    "inital_series = pd.Series(inital_test, index=test_data[\"ingredients\"].index)\n",
    "X_test = test_matrix\n",
    "final_predictions = reassign_groups(X_train=X_train, y_train=y_train, first_pred=inital_series, y_test=y_test,\n",
    "                                 targets=pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5a636f490a13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minital_series\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcuisine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cuisine\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcuisine\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "id = inital_series.index\n",
    "cuisine = final_predictions\n",
    "submission = pd.DataFrame({\"id\":id, \"cuisine\":cuisine})\n",
    "print(submission.head())\n",
    "print(id[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__output__.json', '__results__.html', '__temp_notebook_source__.ipynb', 'custom.css']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
