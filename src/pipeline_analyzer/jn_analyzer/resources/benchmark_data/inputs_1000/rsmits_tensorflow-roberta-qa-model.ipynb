{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "So this is yet another RoBERTa tweet sentiment extraction notebook. However with some different things and approaches used so I hope you like it. And if so then please upvote it ;-)\n",
    "\n",
    "My original work in this competition was inspired by the kernels of Chris Deotte. \n",
    "\n",
    "This kernel is based somewhat on that work yet offers a couple of different/new attempts:\n",
    "* It is using the TFRobertaForQuestionAnswering model that was recently released by Huggingface. And with custom head layers added.\n",
    "* It is using the default RobertaTokenizer\n",
    "* It is using some different preprocessing where I use more of the default tokenizer capabilities.\n",
    "* Label Smoothing.\n",
    "* Added some simple post-processing.\n",
    "\n",
    "Update: In the last version I've added support for running on TPU. Note that this only works for Training. Inference and submission should be done on GPU as TPU is not allowed in this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "# Import Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.2.0\n"
     ]
    }
   ],
   "source": [
    "# Show versions of Tensorflow\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set other Seeds\n",
    "SEED = 4262\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set strategy for tpu\n",
    "USE_TPU = False\n",
    "if USE_TPU:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.OneDeviceStrategy(device = \"/gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchSize: 32\n",
      "LearningRate: 3e-05\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "MAX_LEN = 128\n",
    "FOLDS = 5\n",
    "EPOCHS = 3\n",
    "VERBOSE = 1\n",
    "ROBERTA_BASE_PATH = '../input/tf-roberta/'\n",
    "\n",
    "BATCH_SIZE = 32 * strategy.num_replicas_in_sync\n",
    "print('BatchSize: {}'.format(BATCH_SIZE))\n",
    "\n",
    "LR = 3e-5 * strategy.num_replicas_in_sync\n",
    "print('LearningRate: {}'.format(LR))\n",
    "\n",
    "# Set the following to True after training to make a submission\n",
    "INFERENCE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Train Data (I read in a multiple of batch size 256 when training on TPU)\n",
    "#train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv', nrows = 26880).fillna('')\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "\n",
    "# Summary\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01082688c6</td>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33987a8ee5</td>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative\n",
       "3  01082688c6                                        happy bday!  positive\n",
       "4  33987a8ee5             http://twitpic.com/4w75p - I like it!!  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Test Data\n",
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "# Summary\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "For the tokenizer I'am using the default Huggingface Roberta Tokenizer and loading the vocab and merges file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = RobertaTokenizer(vocab_file = ROBERTA_BASE_PATH + 'vocab-roberta-base.json',\n",
    "                             merges_file = ROBERTA_BASE_PATH + 'merges-roberta-base.txt',\n",
    "                             add_prefix_space = True,\n",
    "                             do_lower_case = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "For processing of the training data I mostly use the proces as was shown in one of the earlier kernels from Abhishek Thakur. Main difference is I'am using the default Roberta Tokenizer. Also I just use the tokenizers functionality to generate the complete input sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Training Data\n",
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN), dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN), dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN), dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN), dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN), dtype='int32')\n",
    "\n",
    "for k in range(ct):\n",
    "    # Process Text\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    \n",
    "    # Skip rows where Text1 is empty (RobertaTokenizer crashes: https://github.com/huggingface/transformers/issues/3809)\n",
    "    if text1 != '':\n",
    "        # Encode Full input sample\n",
    "        input_encoded = tokenizer.encode_plus(text1, train.loc[k,'sentiment'], add_special_tokens = True, max_length = MAX_LEN)\n",
    "        input_ids_sample = input_encoded[\"input_ids\"]\n",
    "        attention_mask_sample = input_encoded[\"attention_mask\"]\n",
    "\n",
    "        # Attention Mask\n",
    "        attention_mask[k,:len(attention_mask_sample)] = attention_mask_sample\n",
    "\n",
    "        # Input Ids\n",
    "        input_ids[k,:len(input_ids_sample)] = input_ids_sample\n",
    "\n",
    "        # Find overlap between Full Text and Selected Text\n",
    "        idx = text1.find(text2)\n",
    "        chars = np.zeros((len(text1)))\n",
    "        chars[idx:idx + len(text2)] = 1\n",
    "        k_ids = tokenizer.encode(text1, add_special_tokens = False) \n",
    "\n",
    "        # ID_OFFSETS\n",
    "        offsets = [] \n",
    "        idx = 0\n",
    "        for t in k_ids:\n",
    "            w = tokenizer.decode([t])\n",
    "            offsets.append((idx,idx+len(w)))\n",
    "            idx += len(w)\n",
    "\n",
    "        # Get Start and End Tokens\n",
    "        toks = []\n",
    "        for i,(a,b) in enumerate(offsets):\n",
    "            sm = np.sum(chars[a:b])\n",
    "            if sm>0: \n",
    "                toks.append(i) \n",
    "        if len(toks) > 0:\n",
    "            start_tokens[k,toks[0]+1] = 1\n",
    "            end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Process Test Data\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(ct):\n",
    "    # Process Text\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    \n",
    "    # Skip rows where Text1 is empty (RobertaTokenizer crashes: https://github.com/huggingface/transformers/issues/3809)\n",
    "    if text1 != '':\n",
    "        # Encode Full input sample\n",
    "        input_encoded = tokenizer.encode_plus(text1, test.loc[k, 'sentiment'], add_special_tokens = True, max_length = MAX_LEN)\n",
    "        input_ids_sample = input_encoded[\"input_ids\"]\n",
    "        attention_mask_sample = input_encoded[\"attention_mask\"]\n",
    "\n",
    "        # Attention Mask\n",
    "        attention_mask_t[k,:len(attention_mask_sample)] = attention_mask_sample\n",
    "\n",
    "        # Input Ids\n",
    "        input_ids_t[k,:len(input_ids_sample)] = input_ids_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    \n",
    "    if (len(a)==0) & (len(b)==0): \n",
    "        return 0.5\n",
    "    \n",
    "    c = a.intersection(b)\n",
    "    \n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits = False, label_smoothing = 0.1)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "When setting the config I increase the dropout for the attention layers a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"eos_token_ids\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Config\n",
    "config = RobertaConfig.from_pretrained(ROBERTA_BASE_PATH+'config-roberta-base.json')\n",
    "#config.attention_probs_dropout_prob = 0.15\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "When building the model I use the recently added TFRobertaForQuestionAnswering model as a basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Create Model\n",
    "    with strategy.scope():      \n",
    "        ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "        att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "        tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "        roberta_model = TFRobertaForQuestionAnswering.from_pretrained(ROBERTA_BASE_PATH + 'pretrained-roberta-base.h5', config = config)\n",
    "        x = roberta_model(ids, attention_mask = att, token_type_ids = tok)\n",
    "\n",
    "        x1 = tf.keras.layers.Dropout(0.30)(x[0]) \n",
    "        x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "\n",
    "        x2 = tf.keras.layers.Dropout(0.30)(x[1]) \n",
    "        x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs = [ids, att, tok], outputs=[x1, x2])\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate = LR)\n",
    "\n",
    "        model.compile(loss = custom_loss, optimizer = optimizer)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "After training the model I apply some simple post processing. See for yourself if you want to use it. In the situation where the start position is after the end position I try to find if the 2nd or 3rd options provides a solution for that situation.\n",
    "\n",
    "With the similarity between 'text' and 'selected_text' when the sentiment is neutral I just use the complete 'text'. Risky because we don't know the data for the private board..but worth a try ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 28s 163ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 18s 165ms/step\n",
      ">>>> FOLD 1 Jaccard = 0.7106094664162447\n",
      ">>>> FOLD 1 Jaccard = 0.7106704071002036\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 28s 163ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 18s 165ms/step\n",
      ">>>> FOLD 2 Jaccard = 0.7007708686451285\n",
      ">>>> FOLD 2 Jaccard = 0.7007656922696233\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 28s 164ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 18s 166ms/step\n",
      ">>>> FOLD 3 Jaccard = 0.6990822640943382\n",
      ">>>> FOLD 3 Jaccard = 0.6991504416621909\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 28s 163ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 18s 163ms/step\n",
      ">>>> FOLD 4 Jaccard = 0.7043849417096955\n",
      ">>>> FOLD 4 Jaccard = 0.7042651487833174\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 28s 162ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 18s 164ms/step\n",
      ">>>> FOLD 5 Jaccard = 0.7056124868460568\n",
      ">>>> FOLD 5 Jaccard = 0.7054355343235481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Placeholders\n",
    "jac, jac1 = [], []\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    # Clear session and create Model\n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    # Callbacks\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint('roberta-%i.h5'%(fold), monitor = 'val_loss', verbose = 1, save_best_only = True,\n",
    "                                                          save_weights_only = True, mode = 'auto', save_freq = 'epoch')\n",
    "    \n",
    "    if not INFERENCE:\n",
    "        # Train Model \n",
    "        model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "                  epochs = EPOCHS, \n",
    "                  batch_size = BATCH_SIZE, \n",
    "                  verbose = VERBOSE, \n",
    "                  callbacks = [model_checkpoint],\n",
    "                  validation_data = ([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], [start_tokens[idxV,], end_tokens[idxV,]]),\n",
    "                  shuffle = True)\n",
    "\n",
    "        print('Loading model...')\n",
    "        model.load_weights(f'roberta-{fold}.h5')        \n",
    "    else:\n",
    "        print('Loading model...')\n",
    "        model.load_weights(f'/kaggle/input/tensorflow-roberta-qa-model/roberta-{fold}.h5')\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose = VERBOSE)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t, attention_mask_t, token_type_ids_t], verbose = VERBOSE)\n",
    "    preds_start += preds[0] / skf.n_splits\n",
    "    preds_end += preds[1] / skf.n_splits\n",
    "    \n",
    "    # Display Fold Jaccard\n",
    "    all = []\n",
    "    all1 = []\n",
    "\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        \n",
    "        # Encode Text\n",
    "        text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1, add_special_tokens = False)\n",
    "        \n",
    "        if train.loc[k,'sentiment'] == 'neutral':\n",
    "            st = \" \".join(train.loc[k,'text'].split())\n",
    "            st1 = \" \".join(train.loc[k,'text'].split())\n",
    "        else:\n",
    "            # Check if start comes after end\n",
    "            if a > b:\n",
    "                st = text1\n",
    "                st1 = text1\n",
    "\n",
    "                # Sort according to max probabilities and get the indices            \n",
    "                start_sort = np.argsort(oof_start[k,])[::-1] \n",
    "                end_sort = np.argsort(oof_end[k,])[::-1]\n",
    "\n",
    "                a1 = start_sort[1]\n",
    "                b1 = end_sort[1]\n",
    "                a2 = start_sort[2]\n",
    "                b2 = end_sort[2]\n",
    "\n",
    "                # Try the next 2 positions..if one of them has the correct order\n",
    "                if a1 < b1:\n",
    "                    st1 = tokenizer.decode(enc[a1-1:b1])\n",
    "                elif a2 < b2:\n",
    "                    st1 = tokenizer.decode(enc[a2-1:b2])   \n",
    "            else:\n",
    "                st = tokenizer.decode(enc[a-1:b])\n",
    "                st1 = tokenizer.decode(enc[a-1:b])\n",
    "\n",
    "        # Store Results\n",
    "        all.append(jaccard(st, train.loc[k,'selected_text']))\n",
    "        all1.append(jaccard(st1, train.loc[k,'selected_text']))\n",
    "\n",
    "    jac.append(np.mean(all))\n",
    "    jac1.append(np.mean(all1))\n",
    "    \n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1), np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1), np.mean(all1))\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OVERALL 5 Fold CV Jaccard                  = 0.7040920055422928\n",
      "=== OVERALL 5 Fold CV Jaccard - Post Processed = 0.7040574448277765\n"
     ]
    }
   ],
   "source": [
    "print(f'=== OVERALL 5 Fold CV Jaccard                  = {np.mean(jac)}')\n",
    "print(f'=== OVERALL 5 Fold CV Jaccard - Post Processed = {np.mean(jac1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final results for submission\n",
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    \n",
    "    # Encode Text\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1, add_special_tokens = False)\n",
    "    \n",
    "    if test.loc[k, 'sentiment'] == 'neutral':\n",
    "        st = \" \".join(test.loc[k,'text'].split())\n",
    "    else:\n",
    "        # Check if start comes after end\n",
    "        if a > b:  \n",
    "            st = text1\n",
    "\n",
    "            # Sort according to max probabilities and get the indices            \n",
    "            start_sort = np.argsort(preds_start[k,])[::-1] \n",
    "            end_sort = np.argsort(preds_end[k,])[::-1]\n",
    "\n",
    "            a1 = start_sort[1]\n",
    "            b1 = end_sort[1]\n",
    "            a2 = start_sort[2]\n",
    "            b2 = end_sort[2]\n",
    "\n",
    "            # Try the next 2 positions..if one of them has the correct order\n",
    "            if a1 < b1:\n",
    "                st = tokenizer.decode(enc[a1-1:b1])\n",
    "            elif a2 < b2:\n",
    "                st = tokenizer.decode(enc[a2-1:b2])  \n",
    "        else:\n",
    "            st = tokenizer.decode(enc[a-1:b])\n",
    "\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3320</th>\n",
       "      <td>6d68218d8b</td>\n",
       "      <td>tMeme&gt; My besties. If only  was there.  on Twitpic: http...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>tMeme&gt; My besties. If only was there. on Twitpic: http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>e8641ec31d</td>\n",
       "      <td>Phase 2 was a success. Self discovery is vital. Phase 3 ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>success.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2622</th>\n",
       "      <td>15db00c707</td>\n",
       "      <td>OmG U NasTY</td>\n",
       "      <td>negative</td>\n",
       "      <td>OmG U NasTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>2b8b0a7d10</td>\n",
       "      <td>Is twubbing again</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Is twubbing again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>410dd99aa3</td>\n",
       "      <td>man im so sad school is ending  but then again high scho...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>man im so sad school is ending but then again high schoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>b2e3ebc647</td>\n",
       "      <td>waiting for aaron to get into town, and then leave him f...</td>\n",
       "      <td>negative</td>\n",
       "      <td>booo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>0aec5ff9a0</td>\n",
       "      <td>is *ugh* what a miserable looking day. 54 degrees. Where...</td>\n",
       "      <td>negative</td>\n",
       "      <td>miserable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>97e53162ff</td>\n",
       "      <td>dear oh dear.....</td>\n",
       "      <td>negative</td>\n",
       "      <td>dear oh dear.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3380</th>\n",
       "      <td>fa625455c7</td>\n",
       "      <td>Everyone is working tonight! I`m bored</td>\n",
       "      <td>negative</td>\n",
       "      <td>bored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>196e5dec12</td>\n",
       "      <td>They wouldn`t reverse any of my overdraft fees.</td>\n",
       "      <td>negative</td>\n",
       "      <td>They wouldn`t reverse any of my overdraft fees.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>fec3cae0c2</td>\n",
       "      <td>found a swing set for sale 2 blocks from home, super che...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>found a swing set for sale 2 blocks from home, super che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>178b3ca782</td>\n",
       "      <td>People at work are stressing me out.</td>\n",
       "      <td>negative</td>\n",
       "      <td>stressing me out.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>887e90ff88</td>\n",
       "      <td>ew.  sorry zach</td>\n",
       "      <td>negative</td>\n",
       "      <td>sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2039</th>\n",
       "      <td>9dfd003f1e</td>\n",
       "      <td>i want you to text me first everyday, not me</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i want you to text me first everyday, not me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2322</th>\n",
       "      <td>cee062574a</td>\n",
       "      <td>thanks to  and i`m now on twitter!</td>\n",
       "      <td>positive</td>\n",
       "      <td>thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>aa8b44f1b2</td>\n",
       "      <td>hiya! been looking for you1</td>\n",
       "      <td>positive</td>\n",
       "      <td>hiya! been looking for you1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>d4b544abff</td>\n",
       "      <td>left the bases loaded...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>left the bases loaded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>d33ed264b3</td>\n",
       "      <td>almost got a kitty yesterday...but it didn`t work out</td>\n",
       "      <td>neutral</td>\n",
       "      <td>almost got a kitty yesterday...but it didn`t work out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>778c5660f8</td>\n",
       "      <td>Justin Timberlake and Andy Samberg do it again.. Mother ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>136b1926c9</td>\n",
       "      <td>Cool. That`d be fantastic!</td>\n",
       "      <td>positive</td>\n",
       "      <td>fantastic!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>926f9f5895</td>\n",
       "      <td>watching old skool fall out boy vids</td>\n",
       "      <td>neutral</td>\n",
       "      <td>watching old skool fall out boy vids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2861</th>\n",
       "      <td>4454e4d53a</td>\n",
       "      <td>I`m not sure... But, 4719 fans in just a week is a wow ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>cf1a0bb3c2</td>\n",
       "      <td>Nope, kicked that habit more then a year ago</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Nope, kicked that habit more then a year ago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3214</th>\n",
       "      <td>2280788f99</td>\n",
       "      <td>crash in Qmbol</td>\n",
       "      <td>negative</td>\n",
       "      <td>crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>f3afc03b4c</td>\n",
       "      <td>I`m trying to get myself moving this morning!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>I`m trying to get myself moving this morning!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "3320  6d68218d8b  tMeme> My besties. If only  was there.  on Twitpic: http...   \n",
       "2412  e8641ec31d  Phase 2 was a success. Self discovery is vital. Phase 3 ...   \n",
       "2622  15db00c707                                                  OmG U NasTY   \n",
       "1369  2b8b0a7d10                                            Is twubbing again   \n",
       "125   410dd99aa3  man im so sad school is ending  but then again high scho...   \n",
       "347   b2e3ebc647  waiting for aaron to get into town, and then leave him f...   \n",
       "2787  0aec5ff9a0  is *ugh* what a miserable looking day. 54 degrees. Where...   \n",
       "401   97e53162ff                                            dear oh dear.....   \n",
       "3380  fa625455c7                       Everyone is working tonight! I`m bored   \n",
       "1053  196e5dec12              They wouldn`t reverse any of my overdraft fees.   \n",
       "481   fec3cae0c2  found a swing set for sale 2 blocks from home, super che...   \n",
       "682   178b3ca782                         People at work are stressing me out.   \n",
       "1616  887e90ff88                                              ew.  sorry zach   \n",
       "2039  9dfd003f1e                 i want you to text me first everyday, not me   \n",
       "2322  cee062574a                           thanks to  and i`m now on twitter!   \n",
       "581   aa8b44f1b2                                  hiya! been looking for you1   \n",
       "1020  d4b544abff                                     left the bases loaded...   \n",
       "57    d33ed264b3        almost got a kitty yesterday...but it didn`t work out   \n",
       "2038  778c5660f8  Justin Timberlake and Andy Samberg do it again.. Mother ...   \n",
       "860   136b1926c9                                   Cool. That`d be fantastic!   \n",
       "2999  926f9f5895                         watching old skool fall out boy vids   \n",
       "2861  4454e4d53a   I`m not sure... But, 4719 fans in just a week is a wow ...   \n",
       "1431  cf1a0bb3c2                 Nope, kicked that habit more then a year ago   \n",
       "3214  2280788f99                                               crash in Qmbol   \n",
       "2715  f3afc03b4c                I`m trying to get myself moving this morning!   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "3320   neutral  tMeme> My besties. If only was there. on Twitpic: http:/...  \n",
       "2412  positive                                                     success.  \n",
       "2622  negative                                                  OmG U NasTY  \n",
       "1369   neutral                                            Is twubbing again  \n",
       "125    neutral  man im so sad school is ending but then again high schoo...  \n",
       "347   negative                                                         booo  \n",
       "2787  negative                                                    miserable  \n",
       "401   negative                                            dear oh dear.....  \n",
       "3380  negative                                                        bored  \n",
       "1053  negative              They wouldn`t reverse any of my overdraft fees.  \n",
       "481    neutral  found a swing set for sale 2 blocks from home, super che...  \n",
       "682   negative                                            stressing me out.  \n",
       "1616  negative                                                        sorry  \n",
       "2039   neutral                 i want you to text me first everyday, not me  \n",
       "2322  positive                                                       thanks  \n",
       "581   positive                                  hiya! been looking for you1  \n",
       "1020   neutral                                     left the bases loaded...  \n",
       "57     neutral        almost got a kitty yesterday...but it didn`t work out  \n",
       "2038  positive                                                        great  \n",
       "860   positive                                                   fantastic!  \n",
       "2999   neutral                         watching old skool fall out boy vids  \n",
       "2861  positive                                                          wow  \n",
       "1431   neutral                 Nope, kicked that habit more then a year ago  \n",
       "3214  negative                                                        crash  \n",
       "2715   neutral                I`m trying to get myself moving this morning!  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store Submission and show some results\n",
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
