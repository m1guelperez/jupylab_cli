{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic/train.csv\n",
      "/kaggle/input/titanic/gender_submission.csv\n",
      "/kaggle/input/titanic/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Pclass                                          Name     Sex  \\\n",
       "0          892       3                              Kelly, Mr. James    male   \n",
       "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
       "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
       "3          895       3                              Wirz, Mr. Albert    male   \n",
       "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
       "\n",
       "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
       "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
       "1  47.0      1      0   363272   7.0000   NaN        S  \n",
       "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
       "3  27.0      0      0   315154   8.6625   NaN        S  \n",
       "4  22.0      1      1  3101298  12.2875   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of women who survived: 0.7420382165605095\n"
     ]
    }
   ],
   "source": [
    "women = train_data[train_data.Sex == 'female'][\"Survived\"]\n",
    "rate_women = sum(women)/len(women)\n",
    "\n",
    "print(\"% of women who survived:\", rate_women)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of men who survived: 0.18890814558058924\n"
     ]
    }
   ],
   "source": [
    "men = train_data[train_data.Sex == 'male'][\"Survived\"]\n",
    "rate_men = sum(men)/len(men)\n",
    "\n",
    "print(\"% of men who survived:\", rate_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of upper class who survived: 0.6296296296296297\n"
     ]
    }
   ],
   "source": [
    "upper_class = train_data[train_data.Pclass == 1][\"Survived\"]\n",
    "rate_upper_class = sum(upper_class)/len(upper_class)\n",
    "\n",
    "print(\"% of upper class who survived:\", rate_upper_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of middle class who survived: 0.47282608695652173\n"
     ]
    }
   ],
   "source": [
    "middle_class = train_data[train_data.Pclass == 2][\"Survived\"]\n",
    "rate_middle_class = sum(middle_class)/len(middle_class)\n",
    "\n",
    "print(\"% of middle class who survived:\", rate_middle_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of lower class who survived: 0.24236252545824846\n"
     ]
    }
   ],
   "source": [
    "lower_class = train_data[train_data.Pclass == 3][\"Survived\"]\n",
    "rate_lower_class = sum(lower_class)/len(lower_class)\n",
    "\n",
    "print(\"% of lower class who survived:\", rate_lower_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substrings_in_string(big_string, substrings):\n",
    "    for substring in substrings:\n",
    "        if substring in big_string:\n",
    "            return substring\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n",
    "            'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n",
    "            'Don', 'Jonkheer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_titles(x):\n",
    "    title=x['Title']\n",
    "    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n",
    "        return 'Mr'\n",
    "    elif title in ['Countess', 'Mme']:\n",
    "        return 'Mrs'\n",
    "    elif title in ['Mlle', 'Ms']:\n",
    "        return 'Miss'\n",
    "    elif title =='Dr':\n",
    "        if x['Sex']=='Male':\n",
    "            return 'Mr'\n",
    "        else:\n",
    "            return 'Mrs'\n",
    "    else:\n",
    "        return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.concat([train_data.drop('Survived', axis=1), test_data])\n",
    "alldata = alldata.drop('PassengerId', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata['Title']=alldata['Name'].map(lambda x: substrings_in_string(x, title_list))\n",
    "alldata['Title']=alldata.apply(replace_titles, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8de177d898>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alldata['Title'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata[\"Age\"].fillna(alldata.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata[\"Cabin_Level\"] = alldata[\"Cabin\"].apply(lambda cabin: cabin[0] if pd.notnull(cabin) else cabin)\n",
    "alldata[\"Cabin_Level\"].fillna(alldata.groupby(\"Pclass\")[\"Cabin_Level\"].transform(\"first\"), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata[\"Embarked\"].fillna('S', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata[\"Fare\"].fillna(alldata.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = alldata.drop(['Name', 'Ticket', 'Cabin'], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1309 entries, 0 to 417\n",
      "Data columns (total 9 columns):\n",
      "Pclass         1309 non-null int64\n",
      "Sex            1309 non-null object\n",
      "Age            1309 non-null float64\n",
      "SibSp          1309 non-null int64\n",
      "Parch          1309 non-null int64\n",
      "Fare           1309 non-null float64\n",
      "Embarked       1309 non-null object\n",
      "Title          1309 non-null object\n",
      "Cabin_Level    1309 non-null object\n",
      "dtypes: float64(2), int64(3), object(4)\n",
      "memory usage: 102.3+ KB\n"
     ]
    }
   ],
   "source": [
    "alldata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>Cabin_Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Q</td>\n",
       "      <td>Mr</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>S</td>\n",
       "      <td>Master</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>S</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>C</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass     Sex   Age  SibSp  Parch     Fare Embarked   Title Cabin_Level\n",
       "0       3    male  22.0      1      0   7.2500        S      Mr           G\n",
       "1       1  female  38.0      1      0  71.2833        C     Mrs           C\n",
       "2       3  female  26.0      0      0   7.9250        S    Miss           G\n",
       "3       1  female  35.0      1      0  53.1000        S     Mrs           C\n",
       "4       3    male  35.0      0      0   8.0500        S      Mr           G\n",
       "5       3    male  30.0      0      0   8.4583        Q      Mr           G\n",
       "6       1    male  54.0      0      0  51.8625        S      Mr           E\n",
       "7       3    male   2.0      3      1  21.0750        S  Master           G\n",
       "8       3  female  27.0      0      2  11.1333        S     Mrs           G\n",
       "9       2  female  14.0      1      0  30.0708        C     Mrs           D"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.get_dummies(alldata, dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Sex_nan</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>...</th>\n",
       "      <th>Title_nan</th>\n",
       "      <th>Cabin_Level_A</th>\n",
       "      <th>Cabin_Level_B</th>\n",
       "      <th>Cabin_Level_C</th>\n",
       "      <th>Cabin_Level_D</th>\n",
       "      <th>Cabin_Level_E</th>\n",
       "      <th>Cabin_Level_F</th>\n",
       "      <th>Cabin_Level_G</th>\n",
       "      <th>Cabin_Level_T</th>\n",
       "      <th>Cabin_Level_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.273456</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473882</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.323563</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.436302</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.436302</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass       Age  SibSp  Parch      Fare  Sex_female  Sex_male  Sex_nan  \\\n",
       "0     1.0  0.273456  0.125    0.0  0.014151         0.0       1.0      0.0   \n",
       "1     0.0  0.473882  0.125    0.0  0.139136         1.0       0.0      0.0   \n",
       "2     1.0  0.323563  0.000    0.0  0.015469         1.0       0.0      0.0   \n",
       "3     0.0  0.436302  0.125    0.0  0.103644         1.0       0.0      0.0   \n",
       "4     1.0  0.436302  0.000    0.0  0.015713         0.0       1.0      0.0   \n",
       "\n",
       "   Embarked_C  Embarked_Q  ...  Title_nan  Cabin_Level_A  Cabin_Level_B  \\\n",
       "0         0.0         0.0  ...        0.0            0.0            0.0   \n",
       "1         1.0         0.0  ...        0.0            0.0            0.0   \n",
       "2         0.0         0.0  ...        0.0            0.0            0.0   \n",
       "3         0.0         0.0  ...        0.0            0.0            0.0   \n",
       "4         0.0         0.0  ...        0.0            0.0            0.0   \n",
       "\n",
       "   Cabin_Level_C  Cabin_Level_D  Cabin_Level_E  Cabin_Level_F  Cabin_Level_G  \\\n",
       "0            0.0            0.0            0.0            0.0            1.0   \n",
       "1            1.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            1.0   \n",
       "3            1.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            1.0   \n",
       "\n",
       "   Cabin_Level_T  Cabin_Level_nan  \n",
       "0            0.0              0.0  \n",
       "1            0.0              0.0  \n",
       "2            0.0              0.0  \n",
       "3            0.0              0.0  \n",
       "4            0.0              0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "alldata = pd.DataFrame(scaler.fit_transform(alldata), columns = alldata.columns)\n",
    "\n",
    "alldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# y = train_data[\"Survived\"]\n",
    "\n",
    "# features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Title\", \"Age\"]\n",
    "# X = pd.get_dummies(train_data[features])\n",
    "# X_test = pd.get_dummies(test_data[features])\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n",
    "# model.fit(X, y)\n",
    "# predictions = model.predict(X_test)\n",
    "\n",
    "# output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "# output.to_csv('my_submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data[\"Survived\"]\n",
    "\n",
    "X = alldata[:len(train_data)]\n",
    "X_test = alldata[len(train_data):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 891 to 1308\n",
      "Data columns (total 26 columns):\n",
      "Pclass             418 non-null float64\n",
      "Age                418 non-null float64\n",
      "SibSp              418 non-null float64\n",
      "Parch              418 non-null float64\n",
      "Fare               418 non-null float64\n",
      "Sex_female         418 non-null float64\n",
      "Sex_male           418 non-null float64\n",
      "Sex_nan            418 non-null float64\n",
      "Embarked_C         418 non-null float64\n",
      "Embarked_Q         418 non-null float64\n",
      "Embarked_S         418 non-null float64\n",
      "Embarked_nan       418 non-null float64\n",
      "Title_Master       418 non-null float64\n",
      "Title_Miss         418 non-null float64\n",
      "Title_Mr           418 non-null float64\n",
      "Title_Mrs          418 non-null float64\n",
      "Title_nan          418 non-null float64\n",
      "Cabin_Level_A      418 non-null float64\n",
      "Cabin_Level_B      418 non-null float64\n",
      "Cabin_Level_C      418 non-null float64\n",
      "Cabin_Level_D      418 non-null float64\n",
      "Cabin_Level_E      418 non-null float64\n",
      "Cabin_Level_F      418 non-null float64\n",
      "Cabin_Level_G      418 non-null float64\n",
      "Cabin_Level_T      418 non-null float64\n",
      "Cabin_Level_nan    418 non-null float64\n",
      "dtypes: float64(26)\n",
      "memory usage: 85.0 KB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 26 columns):\n",
      "Pclass             891 non-null float64\n",
      "Age                891 non-null float64\n",
      "SibSp              891 non-null float64\n",
      "Parch              891 non-null float64\n",
      "Fare               891 non-null float64\n",
      "Sex_female         891 non-null float64\n",
      "Sex_male           891 non-null float64\n",
      "Sex_nan            891 non-null float64\n",
      "Embarked_C         891 non-null float64\n",
      "Embarked_Q         891 non-null float64\n",
      "Embarked_S         891 non-null float64\n",
      "Embarked_nan       891 non-null float64\n",
      "Title_Master       891 non-null float64\n",
      "Title_Miss         891 non-null float64\n",
      "Title_Mr           891 non-null float64\n",
      "Title_Mrs          891 non-null float64\n",
      "Title_nan          891 non-null float64\n",
      "Cabin_Level_A      891 non-null float64\n",
      "Cabin_Level_B      891 non-null float64\n",
      "Cabin_Level_C      891 non-null float64\n",
      "Cabin_Level_D      891 non-null float64\n",
      "Cabin_Level_E      891 non-null float64\n",
      "Cabin_Level_F      891 non-null float64\n",
      "Cabin_Level_G      891 non-null float64\n",
      "Cabin_Level_T      891 non-null float64\n",
      "Cabin_Level_nan    891 non-null float64\n",
      "dtypes: float64(26)\n",
      "memory usage: 181.1 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Sex_nan</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>...</th>\n",
       "      <th>Title_nan</th>\n",
       "      <th>Cabin_Level_A</th>\n",
       "      <th>Cabin_Level_B</th>\n",
       "      <th>Cabin_Level_C</th>\n",
       "      <th>Cabin_Level_D</th>\n",
       "      <th>Cabin_Level_E</th>\n",
       "      <th>Cabin_Level_F</th>\n",
       "      <th>Cabin_Level_G</th>\n",
       "      <th>Cabin_Level_T</th>\n",
       "      <th>Cabin_Level_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.273456</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473882</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.323563</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.436302</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.436302</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass       Age  SibSp  Parch      Fare  Sex_female  Sex_male  Sex_nan  \\\n",
       "0     1.0  0.273456  0.125    0.0  0.014151         0.0       1.0      0.0   \n",
       "1     0.0  0.473882  0.125    0.0  0.139136         1.0       0.0      0.0   \n",
       "2     1.0  0.323563  0.000    0.0  0.015469         1.0       0.0      0.0   \n",
       "3     0.0  0.436302  0.125    0.0  0.103644         1.0       0.0      0.0   \n",
       "4     1.0  0.436302  0.000    0.0  0.015713         0.0       1.0      0.0   \n",
       "\n",
       "   Embarked_C  Embarked_Q  ...  Title_nan  Cabin_Level_A  Cabin_Level_B  \\\n",
       "0         0.0         0.0  ...        0.0            0.0            0.0   \n",
       "1         1.0         0.0  ...        0.0            0.0            0.0   \n",
       "2         0.0         0.0  ...        0.0            0.0            0.0   \n",
       "3         0.0         0.0  ...        0.0            0.0            0.0   \n",
       "4         0.0         0.0  ...        0.0            0.0            0.0   \n",
       "\n",
       "   Cabin_Level_C  Cabin_Level_D  Cabin_Level_E  Cabin_Level_F  Cabin_Level_G  \\\n",
       "0            0.0            0.0            0.0            0.0            1.0   \n",
       "1            1.0            0.0            0.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            1.0   \n",
       "3            1.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            1.0   \n",
       "\n",
       "   Cabin_Level_T  Cabin_Level_nan  \n",
       "0            0.0              0.0  \n",
       "1            0.0              0.0  \n",
       "2            0.0              0.0  \n",
       "3            0.0              0.0  \n",
       "4            0.0              0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = len(X) // 50\n",
    "\n",
    "# Architecture\n",
    "num_features = len(X.columns.values)\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X.values)\n",
    "y = torch.tensor(y.values)\n",
    "\n",
    "train_loader = DataLoader(dataset=list(zip(X, y)),\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftmaxRegression(\n",
       "  (linear): Linear(in_features=26, out_features=100, bias=True)\n",
       "  (linear2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (linear3): Linear(in_features=50, out_features=2, bias=True)\n",
       "  (bn): BatchNorm1d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "class SoftmaxRegression(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(SoftmaxRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(num_features, 100)\n",
    "        self.linear2 = torch.nn.Linear(100, 50)\n",
    "        self.linear3 = torch.nn.Linear(50, num_classes)\n",
    "        \n",
    "        self.bn = torch.nn.BatchNorm1d(num_features)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(100)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(50)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        logits = self.linear(x)\n",
    "        logits = F.relu(logits)\n",
    "                \n",
    "        logits = self.bn2(logits)\n",
    "        logits = self.linear2(logits)\n",
    "        logits = F.relu(logits)\n",
    "        \n",
    "        logits = self.bn3(logits)\n",
    "        logits = self.linear3(logits)\n",
    "        logits = F.relu(logits)\n",
    "        \n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas\n",
    "\n",
    "model = SoftmaxRegression(num_features=num_features,\n",
    "                          num_classes=num_classes)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### COST AND OPTIMIZER\n",
    "##########################\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/1000 | Batch 000/052 | Cost: 0.7891\n",
      "Epoch: 001/1000 | Batch 010/052 | Cost: 0.7007\n",
      "Epoch: 001/1000 | Batch 020/052 | Cost: 0.4175\n",
      "Epoch: 001/1000 | Batch 030/052 | Cost: 0.5034\n",
      "Epoch: 001/1000 | Batch 040/052 | Cost: 0.3748\n",
      "Epoch: 001/1000 | Batch 050/052 | Cost: 0.4979\n",
      "Epoch: 001/1000 training accuracy: 76.32%\n",
      "Epoch: 002/1000 | Batch 000/052 | Cost: 0.4859\n",
      "Epoch: 002/1000 | Batch 010/052 | Cost: 0.4413\n",
      "Epoch: 002/1000 | Batch 020/052 | Cost: 0.4795\n",
      "Epoch: 002/1000 | Batch 030/052 | Cost: 0.2848\n",
      "Epoch: 002/1000 | Batch 040/052 | Cost: 0.4661\n",
      "Epoch: 002/1000 | Batch 050/052 | Cost: 0.3653\n",
      "Epoch: 002/1000 training accuracy: 79.01%\n",
      "Epoch: 003/1000 | Batch 000/052 | Cost: 0.4411\n",
      "Epoch: 003/1000 | Batch 010/052 | Cost: 0.4854\n",
      "Epoch: 003/1000 | Batch 020/052 | Cost: 0.3839\n",
      "Epoch: 003/1000 | Batch 030/052 | Cost: 0.3855\n",
      "Epoch: 003/1000 | Batch 040/052 | Cost: 0.2925\n",
      "Epoch: 003/1000 | Batch 050/052 | Cost: 0.8493\n",
      "Epoch: 003/1000 training accuracy: 79.35%\n",
      "Epoch: 004/1000 | Batch 000/052 | Cost: 0.4704\n",
      "Epoch: 004/1000 | Batch 010/052 | Cost: 0.5369\n",
      "Epoch: 004/1000 | Batch 020/052 | Cost: 0.4420\n",
      "Epoch: 004/1000 | Batch 030/052 | Cost: 0.2879\n",
      "Epoch: 004/1000 | Batch 040/052 | Cost: 0.6953\n",
      "Epoch: 004/1000 | Batch 050/052 | Cost: 0.1987\n",
      "Epoch: 004/1000 training accuracy: 78.56%\n",
      "Epoch: 005/1000 | Batch 000/052 | Cost: 0.3103\n",
      "Epoch: 005/1000 | Batch 010/052 | Cost: 0.4058\n",
      "Epoch: 005/1000 | Batch 020/052 | Cost: 0.2841\n",
      "Epoch: 005/1000 | Batch 030/052 | Cost: 0.6983\n",
      "Epoch: 005/1000 | Batch 040/052 | Cost: 0.5866\n",
      "Epoch: 005/1000 | Batch 050/052 | Cost: 0.6729\n",
      "Epoch: 005/1000 training accuracy: 80.47%\n",
      "Epoch: 006/1000 | Batch 000/052 | Cost: 0.3130\n",
      "Epoch: 006/1000 | Batch 010/052 | Cost: 0.4157\n",
      "Epoch: 006/1000 | Batch 020/052 | Cost: 0.3008\n",
      "Epoch: 006/1000 | Batch 030/052 | Cost: 0.7655\n",
      "Epoch: 006/1000 | Batch 040/052 | Cost: 0.3013\n",
      "Epoch: 006/1000 | Batch 050/052 | Cost: 0.7735\n",
      "Epoch: 006/1000 training accuracy: 80.70%\n",
      "Epoch: 007/1000 | Batch 000/052 | Cost: 0.4052\n",
      "Epoch: 007/1000 | Batch 010/052 | Cost: 0.5143\n",
      "Epoch: 007/1000 | Batch 020/052 | Cost: 0.3045\n",
      "Epoch: 007/1000 | Batch 030/052 | Cost: 0.4554\n",
      "Epoch: 007/1000 | Batch 040/052 | Cost: 0.4529\n",
      "Epoch: 007/1000 | Batch 050/052 | Cost: 0.4792\n",
      "Epoch: 007/1000 training accuracy: 79.69%\n",
      "Epoch: 008/1000 | Batch 000/052 | Cost: 0.4516\n",
      "Epoch: 008/1000 | Batch 010/052 | Cost: 0.2981\n",
      "Epoch: 008/1000 | Batch 020/052 | Cost: 0.2338\n",
      "Epoch: 008/1000 | Batch 030/052 | Cost: 0.5976\n",
      "Epoch: 008/1000 | Batch 040/052 | Cost: 0.5379\n",
      "Epoch: 008/1000 | Batch 050/052 | Cost: 0.5209\n",
      "Epoch: 008/1000 training accuracy: 80.02%\n",
      "Epoch: 009/1000 | Batch 000/052 | Cost: 0.3440\n",
      "Epoch: 009/1000 | Batch 010/052 | Cost: 0.5685\n",
      "Epoch: 009/1000 | Batch 020/052 | Cost: 0.2270\n",
      "Epoch: 009/1000 | Batch 030/052 | Cost: 0.3267\n",
      "Epoch: 009/1000 | Batch 040/052 | Cost: 0.4843\n",
      "Epoch: 009/1000 | Batch 050/052 | Cost: 0.2949\n",
      "Epoch: 009/1000 training accuracy: 80.36%\n",
      "Epoch: 010/1000 | Batch 000/052 | Cost: 0.2676\n",
      "Epoch: 010/1000 | Batch 010/052 | Cost: 0.3534\n",
      "Epoch: 010/1000 | Batch 020/052 | Cost: 0.5127\n",
      "Epoch: 010/1000 | Batch 030/052 | Cost: 0.4273\n",
      "Epoch: 010/1000 | Batch 040/052 | Cost: 0.3200\n",
      "Epoch: 010/1000 | Batch 050/052 | Cost: 0.5294\n",
      "Epoch: 010/1000 training accuracy: 83.16%\n",
      "Epoch: 011/1000 | Batch 000/052 | Cost: 0.4077\n",
      "Epoch: 011/1000 | Batch 010/052 | Cost: 0.3594\n",
      "Epoch: 011/1000 | Batch 020/052 | Cost: 0.4056\n",
      "Epoch: 011/1000 | Batch 030/052 | Cost: 0.3445\n",
      "Epoch: 011/1000 | Batch 040/052 | Cost: 0.6222\n",
      "Epoch: 011/1000 | Batch 050/052 | Cost: 0.4794\n",
      "Epoch: 011/1000 training accuracy: 83.39%\n",
      "Epoch: 012/1000 | Batch 000/052 | Cost: 0.3755\n",
      "Epoch: 012/1000 | Batch 010/052 | Cost: 0.5423\n",
      "Epoch: 012/1000 | Batch 020/052 | Cost: 0.2238\n",
      "Epoch: 012/1000 | Batch 030/052 | Cost: 0.2820\n",
      "Epoch: 012/1000 | Batch 040/052 | Cost: 0.2584\n",
      "Epoch: 012/1000 | Batch 050/052 | Cost: 0.5608\n",
      "Epoch: 012/1000 training accuracy: 82.15%\n",
      "Epoch: 013/1000 | Batch 000/052 | Cost: 0.2525\n",
      "Epoch: 013/1000 | Batch 010/052 | Cost: 0.3355\n",
      "Epoch: 013/1000 | Batch 020/052 | Cost: 0.3151\n",
      "Epoch: 013/1000 | Batch 030/052 | Cost: 0.3431\n",
      "Epoch: 013/1000 | Batch 040/052 | Cost: 0.3641\n",
      "Epoch: 013/1000 | Batch 050/052 | Cost: 0.2840\n",
      "Epoch: 013/1000 training accuracy: 82.15%\n",
      "Epoch: 014/1000 | Batch 000/052 | Cost: 0.4004\n",
      "Epoch: 014/1000 | Batch 010/052 | Cost: 0.7249\n",
      "Epoch: 014/1000 | Batch 020/052 | Cost: 0.2873\n",
      "Epoch: 014/1000 | Batch 030/052 | Cost: 0.2805\n",
      "Epoch: 014/1000 | Batch 040/052 | Cost: 0.2099\n",
      "Epoch: 014/1000 | Batch 050/052 | Cost: 0.4040\n",
      "Epoch: 014/1000 training accuracy: 84.29%\n",
      "Epoch: 015/1000 | Batch 000/052 | Cost: 0.3843\n",
      "Epoch: 015/1000 | Batch 010/052 | Cost: 0.6108\n",
      "Epoch: 015/1000 | Batch 020/052 | Cost: 0.3691\n",
      "Epoch: 015/1000 | Batch 030/052 | Cost: 0.1501\n",
      "Epoch: 015/1000 | Batch 040/052 | Cost: 0.2850\n",
      "Epoch: 015/1000 | Batch 050/052 | Cost: 0.3243\n",
      "Epoch: 015/1000 training accuracy: 83.61%\n",
      "Epoch: 016/1000 | Batch 000/052 | Cost: 0.3193\n",
      "Epoch: 016/1000 | Batch 010/052 | Cost: 0.1507\n",
      "Epoch: 016/1000 | Batch 020/052 | Cost: 0.5129\n",
      "Epoch: 016/1000 | Batch 030/052 | Cost: 0.4976\n",
      "Epoch: 016/1000 | Batch 040/052 | Cost: 0.6189\n",
      "Epoch: 016/1000 | Batch 050/052 | Cost: 0.4241\n",
      "Epoch: 016/1000 training accuracy: 85.19%\n",
      "Epoch: 017/1000 | Batch 000/052 | Cost: 0.6584\n",
      "Epoch: 017/1000 | Batch 010/052 | Cost: 0.2482\n",
      "Epoch: 017/1000 | Batch 020/052 | Cost: 0.3848\n",
      "Epoch: 017/1000 | Batch 030/052 | Cost: 0.1935\n",
      "Epoch: 017/1000 | Batch 040/052 | Cost: 0.4453\n",
      "Epoch: 017/1000 | Batch 050/052 | Cost: 0.3340\n",
      "Epoch: 017/1000 training accuracy: 83.61%\n",
      "Epoch: 018/1000 | Batch 000/052 | Cost: 0.5245\n",
      "Epoch: 018/1000 | Batch 010/052 | Cost: 0.4342\n",
      "Epoch: 018/1000 | Batch 020/052 | Cost: 0.2520\n",
      "Epoch: 018/1000 | Batch 030/052 | Cost: 0.2221\n",
      "Epoch: 018/1000 | Batch 040/052 | Cost: 0.4317\n",
      "Epoch: 018/1000 | Batch 050/052 | Cost: 0.1912\n",
      "Epoch: 018/1000 training accuracy: 83.50%\n",
      "Epoch: 019/1000 | Batch 000/052 | Cost: 0.4042\n",
      "Epoch: 019/1000 | Batch 010/052 | Cost: 0.4188\n",
      "Epoch: 019/1000 | Batch 020/052 | Cost: 0.3773\n",
      "Epoch: 019/1000 | Batch 030/052 | Cost: 0.4167\n",
      "Epoch: 019/1000 | Batch 040/052 | Cost: 0.4543\n",
      "Epoch: 019/1000 | Batch 050/052 | Cost: 0.2998\n",
      "Epoch: 019/1000 training accuracy: 83.84%\n",
      "Epoch: 020/1000 | Batch 000/052 | Cost: 0.3494\n",
      "Epoch: 020/1000 | Batch 010/052 | Cost: 0.2621\n",
      "Epoch: 020/1000 | Batch 020/052 | Cost: 0.4630\n",
      "Epoch: 020/1000 | Batch 030/052 | Cost: 0.1997\n",
      "Epoch: 020/1000 | Batch 040/052 | Cost: 0.4616\n",
      "Epoch: 020/1000 | Batch 050/052 | Cost: 0.4877\n",
      "Epoch: 020/1000 training accuracy: 83.05%\n",
      "Epoch: 021/1000 | Batch 000/052 | Cost: 0.5118\n",
      "Epoch: 021/1000 | Batch 010/052 | Cost: 0.2334\n",
      "Epoch: 021/1000 | Batch 020/052 | Cost: 0.4422\n",
      "Epoch: 021/1000 | Batch 030/052 | Cost: 0.4290\n",
      "Epoch: 021/1000 | Batch 040/052 | Cost: 0.7073\n",
      "Epoch: 021/1000 | Batch 050/052 | Cost: 0.2648\n",
      "Epoch: 021/1000 training accuracy: 83.73%\n",
      "Epoch: 022/1000 | Batch 000/052 | Cost: 0.2326\n",
      "Epoch: 022/1000 | Batch 010/052 | Cost: 0.3659\n",
      "Epoch: 022/1000 | Batch 020/052 | Cost: 0.1648\n",
      "Epoch: 022/1000 | Batch 030/052 | Cost: 0.3629\n",
      "Epoch: 022/1000 | Batch 040/052 | Cost: 0.5083\n",
      "Epoch: 022/1000 | Batch 050/052 | Cost: 0.5752\n",
      "Epoch: 022/1000 training accuracy: 83.39%\n",
      "Epoch: 023/1000 | Batch 000/052 | Cost: 0.2928\n",
      "Epoch: 023/1000 | Batch 010/052 | Cost: 0.3644\n",
      "Epoch: 023/1000 | Batch 020/052 | Cost: 0.2042\n",
      "Epoch: 023/1000 | Batch 030/052 | Cost: 0.3756\n",
      "Epoch: 023/1000 | Batch 040/052 | Cost: 0.6207\n",
      "Epoch: 023/1000 | Batch 050/052 | Cost: 0.5097\n",
      "Epoch: 023/1000 training accuracy: 84.40%\n",
      "Epoch: 024/1000 | Batch 000/052 | Cost: 0.3474\n",
      "Epoch: 024/1000 | Batch 010/052 | Cost: 0.3310\n",
      "Epoch: 024/1000 | Batch 020/052 | Cost: 0.4146\n",
      "Epoch: 024/1000 | Batch 030/052 | Cost: 0.5495\n",
      "Epoch: 024/1000 | Batch 040/052 | Cost: 0.5845\n",
      "Epoch: 024/1000 | Batch 050/052 | Cost: 0.2787\n",
      "Epoch: 024/1000 training accuracy: 83.50%\n",
      "Epoch: 025/1000 | Batch 000/052 | Cost: 0.2981\n",
      "Epoch: 025/1000 | Batch 010/052 | Cost: 0.2832\n",
      "Epoch: 025/1000 | Batch 020/052 | Cost: 0.5402\n",
      "Epoch: 025/1000 | Batch 030/052 | Cost: 0.3387\n",
      "Epoch: 025/1000 | Batch 040/052 | Cost: 0.4204\n",
      "Epoch: 025/1000 | Batch 050/052 | Cost: 0.5139\n",
      "Epoch: 025/1000 training accuracy: 83.50%\n",
      "Epoch: 026/1000 | Batch 000/052 | Cost: 0.4309\n",
      "Epoch: 026/1000 | Batch 010/052 | Cost: 0.3285\n",
      "Epoch: 026/1000 | Batch 020/052 | Cost: 0.3401\n",
      "Epoch: 026/1000 | Batch 030/052 | Cost: 0.6865\n",
      "Epoch: 026/1000 | Batch 040/052 | Cost: 0.5683\n",
      "Epoch: 026/1000 | Batch 050/052 | Cost: 0.4079\n",
      "Epoch: 026/1000 training accuracy: 83.73%\n",
      "Epoch: 027/1000 | Batch 000/052 | Cost: 0.2077\n",
      "Epoch: 027/1000 | Batch 010/052 | Cost: 0.3943\n",
      "Epoch: 027/1000 | Batch 020/052 | Cost: 0.4604\n",
      "Epoch: 027/1000 | Batch 030/052 | Cost: 0.2316\n",
      "Epoch: 027/1000 | Batch 040/052 | Cost: 0.4540\n",
      "Epoch: 027/1000 | Batch 050/052 | Cost: 0.4989\n",
      "Epoch: 027/1000 training accuracy: 84.96%\n",
      "Epoch: 028/1000 | Batch 000/052 | Cost: 0.1419\n",
      "Epoch: 028/1000 | Batch 010/052 | Cost: 0.4245\n",
      "Epoch: 028/1000 | Batch 020/052 | Cost: 0.2102\n",
      "Epoch: 028/1000 | Batch 030/052 | Cost: 0.3294\n",
      "Epoch: 028/1000 | Batch 040/052 | Cost: 0.3283\n",
      "Epoch: 028/1000 | Batch 050/052 | Cost: 0.6334\n",
      "Epoch: 028/1000 training accuracy: 83.73%\n",
      "Epoch: 029/1000 | Batch 000/052 | Cost: 0.1880\n",
      "Epoch: 029/1000 | Batch 010/052 | Cost: 0.2764\n",
      "Epoch: 029/1000 | Batch 020/052 | Cost: 0.3583\n",
      "Epoch: 029/1000 | Batch 030/052 | Cost: 0.2770\n",
      "Epoch: 029/1000 | Batch 040/052 | Cost: 0.7321\n",
      "Epoch: 029/1000 | Batch 050/052 | Cost: 0.2215\n",
      "Epoch: 029/1000 training accuracy: 84.74%\n",
      "Epoch: 030/1000 | Batch 000/052 | Cost: 0.4258\n",
      "Epoch: 030/1000 | Batch 010/052 | Cost: 0.2318\n",
      "Epoch: 030/1000 | Batch 020/052 | Cost: 0.2676\n",
      "Epoch: 030/1000 | Batch 030/052 | Cost: 0.4201\n",
      "Epoch: 030/1000 | Batch 040/052 | Cost: 0.3461\n",
      "Epoch: 030/1000 | Batch 050/052 | Cost: 0.4031\n",
      "Epoch: 030/1000 training accuracy: 84.51%\n",
      "Epoch: 031/1000 | Batch 000/052 | Cost: 0.1801\n",
      "Epoch: 031/1000 | Batch 010/052 | Cost: 0.2951\n",
      "Epoch: 031/1000 | Batch 020/052 | Cost: 0.2614\n",
      "Epoch: 031/1000 | Batch 030/052 | Cost: 0.5857\n",
      "Epoch: 031/1000 | Batch 040/052 | Cost: 0.6747\n",
      "Epoch: 031/1000 | Batch 050/052 | Cost: 0.5244\n",
      "Epoch: 031/1000 training accuracy: 83.95%\n",
      "Epoch: 032/1000 | Batch 000/052 | Cost: 0.6095\n",
      "Epoch: 032/1000 | Batch 010/052 | Cost: 0.5933\n",
      "Epoch: 032/1000 | Batch 020/052 | Cost: 0.3860\n",
      "Epoch: 032/1000 | Batch 030/052 | Cost: 0.3695\n",
      "Epoch: 032/1000 | Batch 040/052 | Cost: 0.2341\n",
      "Epoch: 032/1000 | Batch 050/052 | Cost: 0.4637\n",
      "Epoch: 032/1000 training accuracy: 84.40%\n",
      "Epoch: 033/1000 | Batch 000/052 | Cost: 0.4714\n",
      "Epoch: 033/1000 | Batch 010/052 | Cost: 0.3921\n",
      "Epoch: 033/1000 | Batch 020/052 | Cost: 0.3327\n",
      "Epoch: 033/1000 | Batch 030/052 | Cost: 0.3904\n",
      "Epoch: 033/1000 | Batch 040/052 | Cost: 0.3570\n",
      "Epoch: 033/1000 | Batch 050/052 | Cost: 0.4181\n",
      "Epoch: 033/1000 training accuracy: 84.96%\n",
      "Epoch: 034/1000 | Batch 000/052 | Cost: 0.2436\n",
      "Epoch: 034/1000 | Batch 010/052 | Cost: 0.5383\n",
      "Epoch: 034/1000 | Batch 020/052 | Cost: 0.3096\n",
      "Epoch: 034/1000 | Batch 030/052 | Cost: 0.3107\n",
      "Epoch: 034/1000 | Batch 040/052 | Cost: 0.2197\n",
      "Epoch: 034/1000 | Batch 050/052 | Cost: 0.3472\n",
      "Epoch: 034/1000 training accuracy: 83.50%\n",
      "Epoch: 035/1000 | Batch 000/052 | Cost: 0.5439\n",
      "Epoch: 035/1000 | Batch 010/052 | Cost: 0.6180\n",
      "Epoch: 035/1000 | Batch 020/052 | Cost: 0.4147\n",
      "Epoch: 035/1000 | Batch 030/052 | Cost: 0.4590\n",
      "Epoch: 035/1000 | Batch 040/052 | Cost: 0.2130\n",
      "Epoch: 035/1000 | Batch 050/052 | Cost: 0.5322\n",
      "Epoch: 035/1000 training accuracy: 84.18%\n",
      "Epoch: 036/1000 | Batch 000/052 | Cost: 0.2593\n",
      "Epoch: 036/1000 | Batch 010/052 | Cost: 0.2948\n",
      "Epoch: 036/1000 | Batch 020/052 | Cost: 0.3841\n",
      "Epoch: 036/1000 | Batch 030/052 | Cost: 0.3691\n",
      "Epoch: 036/1000 | Batch 040/052 | Cost: 0.4878\n",
      "Epoch: 036/1000 | Batch 050/052 | Cost: 0.6222\n",
      "Epoch: 036/1000 training accuracy: 83.84%\n",
      "Epoch: 037/1000 | Batch 000/052 | Cost: 0.2347\n",
      "Epoch: 037/1000 | Batch 010/052 | Cost: 0.2728\n",
      "Epoch: 037/1000 | Batch 020/052 | Cost: 0.1311\n",
      "Epoch: 037/1000 | Batch 030/052 | Cost: 0.4145\n",
      "Epoch: 037/1000 | Batch 040/052 | Cost: 0.7322\n",
      "Epoch: 037/1000 | Batch 050/052 | Cost: 0.4672\n",
      "Epoch: 037/1000 training accuracy: 85.30%\n",
      "Epoch: 038/1000 | Batch 000/052 | Cost: 0.3967\n",
      "Epoch: 038/1000 | Batch 010/052 | Cost: 0.3250\n",
      "Epoch: 038/1000 | Batch 020/052 | Cost: 0.2372\n",
      "Epoch: 038/1000 | Batch 030/052 | Cost: 0.4805\n",
      "Epoch: 038/1000 | Batch 040/052 | Cost: 0.3101\n",
      "Epoch: 038/1000 | Batch 050/052 | Cost: 0.2580\n",
      "Epoch: 038/1000 training accuracy: 84.74%\n",
      "Epoch: 039/1000 | Batch 000/052 | Cost: 0.7434\n",
      "Epoch: 039/1000 | Batch 010/052 | Cost: 0.3196\n",
      "Epoch: 039/1000 | Batch 020/052 | Cost: 0.3838\n",
      "Epoch: 039/1000 | Batch 030/052 | Cost: 0.5083\n",
      "Epoch: 039/1000 | Batch 040/052 | Cost: 0.3489\n",
      "Epoch: 039/1000 | Batch 050/052 | Cost: 0.2530\n",
      "Epoch: 039/1000 training accuracy: 83.61%\n",
      "Epoch: 040/1000 | Batch 000/052 | Cost: 0.4976\n",
      "Epoch: 040/1000 | Batch 010/052 | Cost: 0.4016\n",
      "Epoch: 040/1000 | Batch 020/052 | Cost: 0.2155\n",
      "Epoch: 040/1000 | Batch 030/052 | Cost: 0.2576\n",
      "Epoch: 040/1000 | Batch 040/052 | Cost: 0.3696\n",
      "Epoch: 040/1000 | Batch 050/052 | Cost: 0.2995\n",
      "Epoch: 040/1000 training accuracy: 85.86%\n",
      "Epoch: 041/1000 | Batch 000/052 | Cost: 0.5153\n",
      "Epoch: 041/1000 | Batch 010/052 | Cost: 0.4448\n",
      "Epoch: 041/1000 | Batch 020/052 | Cost: 0.2559\n",
      "Epoch: 041/1000 | Batch 030/052 | Cost: 0.3183\n",
      "Epoch: 041/1000 | Batch 040/052 | Cost: 0.5976\n",
      "Epoch: 041/1000 | Batch 050/052 | Cost: 0.3213\n",
      "Epoch: 041/1000 training accuracy: 84.74%\n",
      "Epoch: 042/1000 | Batch 000/052 | Cost: 0.3024\n",
      "Epoch: 042/1000 | Batch 010/052 | Cost: 0.2646\n",
      "Epoch: 042/1000 | Batch 020/052 | Cost: 0.3171\n",
      "Epoch: 042/1000 | Batch 030/052 | Cost: 0.2985\n",
      "Epoch: 042/1000 | Batch 040/052 | Cost: 0.3291\n",
      "Epoch: 042/1000 | Batch 050/052 | Cost: 0.3598\n",
      "Epoch: 042/1000 training accuracy: 86.08%\n",
      "Epoch: 043/1000 | Batch 000/052 | Cost: 0.5105\n",
      "Epoch: 043/1000 | Batch 010/052 | Cost: 0.2925\n",
      "Epoch: 043/1000 | Batch 020/052 | Cost: 0.4426\n",
      "Epoch: 043/1000 | Batch 030/052 | Cost: 0.2098\n",
      "Epoch: 043/1000 | Batch 040/052 | Cost: 0.4744\n",
      "Epoch: 043/1000 | Batch 050/052 | Cost: 0.6225\n",
      "Epoch: 043/1000 training accuracy: 84.85%\n",
      "Epoch: 044/1000 | Batch 000/052 | Cost: 0.2527\n",
      "Epoch: 044/1000 | Batch 010/052 | Cost: 0.3271\n",
      "Epoch: 044/1000 | Batch 020/052 | Cost: 0.3943\n",
      "Epoch: 044/1000 | Batch 030/052 | Cost: 0.1417\n",
      "Epoch: 044/1000 | Batch 040/052 | Cost: 0.3309\n",
      "Epoch: 044/1000 | Batch 050/052 | Cost: 0.4372\n",
      "Epoch: 044/1000 training accuracy: 86.31%\n",
      "Epoch: 045/1000 | Batch 000/052 | Cost: 0.1918\n",
      "Epoch: 045/1000 | Batch 010/052 | Cost: 0.5029\n",
      "Epoch: 045/1000 | Batch 020/052 | Cost: 0.8106\n",
      "Epoch: 045/1000 | Batch 030/052 | Cost: 0.2681\n",
      "Epoch: 045/1000 | Batch 040/052 | Cost: 0.2973\n",
      "Epoch: 045/1000 | Batch 050/052 | Cost: 0.3296\n",
      "Epoch: 045/1000 training accuracy: 85.86%\n",
      "Epoch: 046/1000 | Batch 000/052 | Cost: 0.1552\n",
      "Epoch: 046/1000 | Batch 010/052 | Cost: 0.3692\n",
      "Epoch: 046/1000 | Batch 020/052 | Cost: 0.1695\n",
      "Epoch: 046/1000 | Batch 030/052 | Cost: 0.4753\n",
      "Epoch: 046/1000 | Batch 040/052 | Cost: 0.3614\n",
      "Epoch: 046/1000 | Batch 050/052 | Cost: 0.2629\n",
      "Epoch: 046/1000 training accuracy: 85.86%\n",
      "Epoch: 047/1000 | Batch 000/052 | Cost: 0.2329\n",
      "Epoch: 047/1000 | Batch 010/052 | Cost: 0.4806\n",
      "Epoch: 047/1000 | Batch 020/052 | Cost: 0.4132\n",
      "Epoch: 047/1000 | Batch 030/052 | Cost: 0.2999\n",
      "Epoch: 047/1000 | Batch 040/052 | Cost: 0.3034\n",
      "Epoch: 047/1000 | Batch 050/052 | Cost: 0.5082\n",
      "Epoch: 047/1000 training accuracy: 84.62%\n",
      "Epoch: 048/1000 | Batch 000/052 | Cost: 0.5025\n",
      "Epoch: 048/1000 | Batch 010/052 | Cost: 0.3368\n",
      "Epoch: 048/1000 | Batch 020/052 | Cost: 0.2695\n",
      "Epoch: 048/1000 | Batch 030/052 | Cost: 0.2624\n",
      "Epoch: 048/1000 | Batch 040/052 | Cost: 0.2916\n",
      "Epoch: 048/1000 | Batch 050/052 | Cost: 0.4197\n",
      "Epoch: 048/1000 training accuracy: 85.07%\n",
      "Epoch: 049/1000 | Batch 000/052 | Cost: 0.1244\n",
      "Epoch: 049/1000 | Batch 010/052 | Cost: 0.6445\n",
      "Epoch: 049/1000 | Batch 020/052 | Cost: 0.4137\n",
      "Epoch: 049/1000 | Batch 030/052 | Cost: 0.3866\n",
      "Epoch: 049/1000 | Batch 040/052 | Cost: 0.1624\n",
      "Epoch: 049/1000 | Batch 050/052 | Cost: 0.2200\n",
      "Epoch: 049/1000 training accuracy: 85.07%\n",
      "Epoch: 050/1000 | Batch 000/052 | Cost: 0.4605\n",
      "Epoch: 050/1000 | Batch 010/052 | Cost: 0.1956\n",
      "Epoch: 050/1000 | Batch 020/052 | Cost: 0.5790\n",
      "Epoch: 050/1000 | Batch 030/052 | Cost: 0.2173\n",
      "Epoch: 050/1000 | Batch 040/052 | Cost: 0.3141\n",
      "Epoch: 050/1000 | Batch 050/052 | Cost: 0.3703\n",
      "Epoch: 050/1000 training accuracy: 85.86%\n",
      "Epoch: 051/1000 | Batch 000/052 | Cost: 0.1302\n",
      "Epoch: 051/1000 | Batch 010/052 | Cost: 0.4759\n",
      "Epoch: 051/1000 | Batch 020/052 | Cost: 0.4413\n",
      "Epoch: 051/1000 | Batch 030/052 | Cost: 0.3104\n",
      "Epoch: 051/1000 | Batch 040/052 | Cost: 0.3326\n",
      "Epoch: 051/1000 | Batch 050/052 | Cost: 0.2364\n",
      "Epoch: 051/1000 training accuracy: 84.74%\n",
      "Epoch: 052/1000 | Batch 000/052 | Cost: 0.3908\n",
      "Epoch: 052/1000 | Batch 010/052 | Cost: 0.3188\n",
      "Epoch: 052/1000 | Batch 020/052 | Cost: 0.1674\n",
      "Epoch: 052/1000 | Batch 030/052 | Cost: 0.5961\n",
      "Epoch: 052/1000 | Batch 040/052 | Cost: 0.4478\n",
      "Epoch: 052/1000 | Batch 050/052 | Cost: 0.4565\n",
      "Epoch: 052/1000 training accuracy: 85.41%\n",
      "Epoch: 053/1000 | Batch 000/052 | Cost: 0.3416\n",
      "Epoch: 053/1000 | Batch 010/052 | Cost: 0.2977\n",
      "Epoch: 053/1000 | Batch 020/052 | Cost: 0.2365\n",
      "Epoch: 053/1000 | Batch 030/052 | Cost: 0.2633\n",
      "Epoch: 053/1000 | Batch 040/052 | Cost: 0.2375\n",
      "Epoch: 053/1000 | Batch 050/052 | Cost: 0.1828\n",
      "Epoch: 053/1000 training accuracy: 85.07%\n",
      "Epoch: 054/1000 | Batch 000/052 | Cost: 0.2619\n",
      "Epoch: 054/1000 | Batch 010/052 | Cost: 0.1877\n",
      "Epoch: 054/1000 | Batch 020/052 | Cost: 0.2008\n",
      "Epoch: 054/1000 | Batch 030/052 | Cost: 0.2843\n",
      "Epoch: 054/1000 | Batch 040/052 | Cost: 0.6605\n",
      "Epoch: 054/1000 | Batch 050/052 | Cost: 0.6915\n",
      "Epoch: 054/1000 training accuracy: 85.07%\n",
      "Epoch: 055/1000 | Batch 000/052 | Cost: 0.2098\n",
      "Epoch: 055/1000 | Batch 010/052 | Cost: 0.4578\n",
      "Epoch: 055/1000 | Batch 020/052 | Cost: 0.1454\n",
      "Epoch: 055/1000 | Batch 030/052 | Cost: 0.3515\n",
      "Epoch: 055/1000 | Batch 040/052 | Cost: 0.3361\n",
      "Epoch: 055/1000 | Batch 050/052 | Cost: 0.5582\n",
      "Epoch: 055/1000 training accuracy: 86.08%\n",
      "Epoch: 056/1000 | Batch 000/052 | Cost: 0.3636\n",
      "Epoch: 056/1000 | Batch 010/052 | Cost: 0.2765\n",
      "Epoch: 056/1000 | Batch 020/052 | Cost: 0.5557\n",
      "Epoch: 056/1000 | Batch 030/052 | Cost: 0.2612\n",
      "Epoch: 056/1000 | Batch 040/052 | Cost: 0.1836\n",
      "Epoch: 056/1000 | Batch 050/052 | Cost: 0.2354\n",
      "Epoch: 056/1000 training accuracy: 84.62%\n",
      "Epoch: 057/1000 | Batch 000/052 | Cost: 0.5179\n",
      "Epoch: 057/1000 | Batch 010/052 | Cost: 0.3867\n",
      "Epoch: 057/1000 | Batch 020/052 | Cost: 0.1954\n",
      "Epoch: 057/1000 | Batch 030/052 | Cost: 0.1810\n",
      "Epoch: 057/1000 | Batch 040/052 | Cost: 0.1885\n",
      "Epoch: 057/1000 | Batch 050/052 | Cost: 0.4027\n",
      "Epoch: 057/1000 training accuracy: 84.96%\n",
      "Epoch: 058/1000 | Batch 000/052 | Cost: 0.3385\n",
      "Epoch: 058/1000 | Batch 010/052 | Cost: 0.4393\n",
      "Epoch: 058/1000 | Batch 020/052 | Cost: 0.1978\n",
      "Epoch: 058/1000 | Batch 030/052 | Cost: 0.3408\n",
      "Epoch: 058/1000 | Batch 040/052 | Cost: 0.5311\n",
      "Epoch: 058/1000 | Batch 050/052 | Cost: 0.3265\n",
      "Epoch: 058/1000 training accuracy: 85.75%\n",
      "Epoch: 059/1000 | Batch 000/052 | Cost: 0.3170\n",
      "Epoch: 059/1000 | Batch 010/052 | Cost: 0.3247\n",
      "Epoch: 059/1000 | Batch 020/052 | Cost: 0.1624\n",
      "Epoch: 059/1000 | Batch 030/052 | Cost: 0.2630\n",
      "Epoch: 059/1000 | Batch 040/052 | Cost: 0.3580\n",
      "Epoch: 059/1000 | Batch 050/052 | Cost: 0.2694\n",
      "Epoch: 059/1000 training accuracy: 84.85%\n",
      "Epoch: 060/1000 | Batch 000/052 | Cost: 0.3202\n",
      "Epoch: 060/1000 | Batch 010/052 | Cost: 0.3730\n",
      "Epoch: 060/1000 | Batch 020/052 | Cost: 0.3913\n",
      "Epoch: 060/1000 | Batch 030/052 | Cost: 0.3260\n",
      "Epoch: 060/1000 | Batch 040/052 | Cost: 0.1832\n",
      "Epoch: 060/1000 | Batch 050/052 | Cost: 0.1319\n",
      "Epoch: 060/1000 training accuracy: 85.63%\n",
      "Epoch: 061/1000 | Batch 000/052 | Cost: 0.2207\n",
      "Epoch: 061/1000 | Batch 010/052 | Cost: 0.2907\n",
      "Epoch: 061/1000 | Batch 020/052 | Cost: 0.4598\n",
      "Epoch: 061/1000 | Batch 030/052 | Cost: 0.2224\n",
      "Epoch: 061/1000 | Batch 040/052 | Cost: 0.4075\n",
      "Epoch: 061/1000 | Batch 050/052 | Cost: 0.4508\n",
      "Epoch: 061/1000 training accuracy: 86.08%\n",
      "Epoch: 062/1000 | Batch 000/052 | Cost: 0.1671\n",
      "Epoch: 062/1000 | Batch 010/052 | Cost: 0.2452\n",
      "Epoch: 062/1000 | Batch 020/052 | Cost: 0.2611\n",
      "Epoch: 062/1000 | Batch 030/052 | Cost: 0.2537\n",
      "Epoch: 062/1000 | Batch 040/052 | Cost: 0.5889\n",
      "Epoch: 062/1000 | Batch 050/052 | Cost: 0.4645\n",
      "Epoch: 062/1000 training accuracy: 85.75%\n",
      "Epoch: 063/1000 | Batch 000/052 | Cost: 0.1254\n",
      "Epoch: 063/1000 | Batch 010/052 | Cost: 0.2055\n",
      "Epoch: 063/1000 | Batch 020/052 | Cost: 0.1493\n",
      "Epoch: 063/1000 | Batch 030/052 | Cost: 0.2697\n",
      "Epoch: 063/1000 | Batch 040/052 | Cost: 0.1355\n",
      "Epoch: 063/1000 | Batch 050/052 | Cost: 0.2216\n",
      "Epoch: 063/1000 training accuracy: 85.63%\n",
      "Epoch: 064/1000 | Batch 000/052 | Cost: 0.1942\n",
      "Epoch: 064/1000 | Batch 010/052 | Cost: 0.2704\n",
      "Epoch: 064/1000 | Batch 020/052 | Cost: 0.5232\n",
      "Epoch: 064/1000 | Batch 030/052 | Cost: 0.2285\n",
      "Epoch: 064/1000 | Batch 040/052 | Cost: 0.3986\n",
      "Epoch: 064/1000 | Batch 050/052 | Cost: 0.4477\n",
      "Epoch: 064/1000 training accuracy: 86.64%\n",
      "Epoch: 065/1000 | Batch 000/052 | Cost: 0.4162\n",
      "Epoch: 065/1000 | Batch 010/052 | Cost: 0.2683\n",
      "Epoch: 065/1000 | Batch 020/052 | Cost: 0.3669\n",
      "Epoch: 065/1000 | Batch 030/052 | Cost: 0.2601\n",
      "Epoch: 065/1000 | Batch 040/052 | Cost: 0.3800\n",
      "Epoch: 065/1000 | Batch 050/052 | Cost: 0.3340\n",
      "Epoch: 065/1000 training accuracy: 86.53%\n",
      "Epoch: 066/1000 | Batch 000/052 | Cost: 0.2693\n",
      "Epoch: 066/1000 | Batch 010/052 | Cost: 0.1995\n",
      "Epoch: 066/1000 | Batch 020/052 | Cost: 0.2920\n",
      "Epoch: 066/1000 | Batch 030/052 | Cost: 0.3403\n",
      "Epoch: 066/1000 | Batch 040/052 | Cost: 0.1846\n",
      "Epoch: 066/1000 | Batch 050/052 | Cost: 0.5438\n",
      "Epoch: 066/1000 training accuracy: 86.20%\n",
      "Epoch: 067/1000 | Batch 000/052 | Cost: 0.3612\n",
      "Epoch: 067/1000 | Batch 010/052 | Cost: 0.1799\n",
      "Epoch: 067/1000 | Batch 020/052 | Cost: 0.2781\n",
      "Epoch: 067/1000 | Batch 030/052 | Cost: 0.3918\n",
      "Epoch: 067/1000 | Batch 040/052 | Cost: 0.4184\n",
      "Epoch: 067/1000 | Batch 050/052 | Cost: 0.2192\n",
      "Epoch: 067/1000 training accuracy: 85.97%\n",
      "Epoch: 068/1000 | Batch 000/052 | Cost: 0.1924\n",
      "Epoch: 068/1000 | Batch 010/052 | Cost: 0.1845\n",
      "Epoch: 068/1000 | Batch 020/052 | Cost: 0.2389\n",
      "Epoch: 068/1000 | Batch 030/052 | Cost: 0.3979\n",
      "Epoch: 068/1000 | Batch 040/052 | Cost: 0.4890\n",
      "Epoch: 068/1000 | Batch 050/052 | Cost: 0.5181\n",
      "Epoch: 068/1000 training accuracy: 85.63%\n",
      "Epoch: 069/1000 | Batch 000/052 | Cost: 0.2800\n",
      "Epoch: 069/1000 | Batch 010/052 | Cost: 0.2452\n",
      "Epoch: 069/1000 | Batch 020/052 | Cost: 0.1500\n",
      "Epoch: 069/1000 | Batch 030/052 | Cost: 0.5221\n",
      "Epoch: 069/1000 | Batch 040/052 | Cost: 0.2420\n",
      "Epoch: 069/1000 | Batch 050/052 | Cost: 0.2509\n",
      "Epoch: 069/1000 training accuracy: 86.98%\n",
      "Epoch: 070/1000 | Batch 000/052 | Cost: 0.3834\n",
      "Epoch: 070/1000 | Batch 010/052 | Cost: 0.1776\n",
      "Epoch: 070/1000 | Batch 020/052 | Cost: 0.2844\n",
      "Epoch: 070/1000 | Batch 030/052 | Cost: 0.4367\n",
      "Epoch: 070/1000 | Batch 040/052 | Cost: 0.3007\n",
      "Epoch: 070/1000 | Batch 050/052 | Cost: 0.4814\n",
      "Epoch: 070/1000 training accuracy: 86.20%\n",
      "Epoch: 071/1000 | Batch 000/052 | Cost: 0.4942\n",
      "Epoch: 071/1000 | Batch 010/052 | Cost: 0.4141\n",
      "Epoch: 071/1000 | Batch 020/052 | Cost: 0.4074\n",
      "Epoch: 071/1000 | Batch 030/052 | Cost: 0.3107\n",
      "Epoch: 071/1000 | Batch 040/052 | Cost: 0.3009\n",
      "Epoch: 071/1000 | Batch 050/052 | Cost: 0.2456\n",
      "Epoch: 071/1000 training accuracy: 85.19%\n",
      "Epoch: 072/1000 | Batch 000/052 | Cost: 0.2112\n",
      "Epoch: 072/1000 | Batch 010/052 | Cost: 0.5120\n",
      "Epoch: 072/1000 | Batch 020/052 | Cost: 0.4517\n",
      "Epoch: 072/1000 | Batch 030/052 | Cost: 0.3117\n",
      "Epoch: 072/1000 | Batch 040/052 | Cost: 0.3332\n",
      "Epoch: 072/1000 | Batch 050/052 | Cost: 0.2715\n",
      "Epoch: 072/1000 training accuracy: 85.97%\n",
      "Epoch: 073/1000 | Batch 000/052 | Cost: 0.5736\n",
      "Epoch: 073/1000 | Batch 010/052 | Cost: 0.4728\n",
      "Epoch: 073/1000 | Batch 020/052 | Cost: 0.1699\n",
      "Epoch: 073/1000 | Batch 030/052 | Cost: 0.2667\n",
      "Epoch: 073/1000 | Batch 040/052 | Cost: 0.3934\n",
      "Epoch: 073/1000 | Batch 050/052 | Cost: 0.3191\n",
      "Epoch: 073/1000 training accuracy: 85.19%\n",
      "Epoch: 074/1000 | Batch 000/052 | Cost: 0.2245\n",
      "Epoch: 074/1000 | Batch 010/052 | Cost: 0.2371\n",
      "Epoch: 074/1000 | Batch 020/052 | Cost: 0.3326\n",
      "Epoch: 074/1000 | Batch 030/052 | Cost: 0.2452\n",
      "Epoch: 074/1000 | Batch 040/052 | Cost: 0.5177\n",
      "Epoch: 074/1000 | Batch 050/052 | Cost: 0.2932\n",
      "Epoch: 074/1000 training accuracy: 86.42%\n",
      "Epoch: 075/1000 | Batch 000/052 | Cost: 0.4257\n",
      "Epoch: 075/1000 | Batch 010/052 | Cost: 0.1151\n",
      "Epoch: 075/1000 | Batch 020/052 | Cost: 0.3929\n",
      "Epoch: 075/1000 | Batch 030/052 | Cost: 0.1894\n",
      "Epoch: 075/1000 | Batch 040/052 | Cost: 0.2040\n",
      "Epoch: 075/1000 | Batch 050/052 | Cost: 0.2409\n",
      "Epoch: 075/1000 training accuracy: 85.52%\n",
      "Epoch: 076/1000 | Batch 000/052 | Cost: 0.4362\n",
      "Epoch: 076/1000 | Batch 010/052 | Cost: 0.3454\n",
      "Epoch: 076/1000 | Batch 020/052 | Cost: 0.3704\n",
      "Epoch: 076/1000 | Batch 030/052 | Cost: 0.3283\n",
      "Epoch: 076/1000 | Batch 040/052 | Cost: 0.1380\n",
      "Epoch: 076/1000 | Batch 050/052 | Cost: 0.1188\n",
      "Epoch: 076/1000 training accuracy: 86.42%\n",
      "Epoch: 077/1000 | Batch 000/052 | Cost: 0.2879\n",
      "Epoch: 077/1000 | Batch 010/052 | Cost: 0.4145\n",
      "Epoch: 077/1000 | Batch 020/052 | Cost: 0.4498\n",
      "Epoch: 077/1000 | Batch 030/052 | Cost: 0.3731\n",
      "Epoch: 077/1000 | Batch 040/052 | Cost: 0.5597\n",
      "Epoch: 077/1000 | Batch 050/052 | Cost: 0.3698\n",
      "Epoch: 077/1000 training accuracy: 87.43%\n",
      "Epoch: 078/1000 | Batch 000/052 | Cost: 0.4595\n",
      "Epoch: 078/1000 | Batch 010/052 | Cost: 0.3640\n",
      "Epoch: 078/1000 | Batch 020/052 | Cost: 0.3938\n",
      "Epoch: 078/1000 | Batch 030/052 | Cost: 0.3118\n",
      "Epoch: 078/1000 | Batch 040/052 | Cost: 0.4871\n",
      "Epoch: 078/1000 | Batch 050/052 | Cost: 0.3183\n",
      "Epoch: 078/1000 training accuracy: 87.54%\n",
      "Epoch: 079/1000 | Batch 000/052 | Cost: 0.1687\n",
      "Epoch: 079/1000 | Batch 010/052 | Cost: 0.1442\n",
      "Epoch: 079/1000 | Batch 020/052 | Cost: 0.1903\n",
      "Epoch: 079/1000 | Batch 030/052 | Cost: 0.6919\n",
      "Epoch: 079/1000 | Batch 040/052 | Cost: 0.3504\n",
      "Epoch: 079/1000 | Batch 050/052 | Cost: 0.4271\n",
      "Epoch: 079/1000 training accuracy: 87.43%\n",
      "Epoch: 080/1000 | Batch 000/052 | Cost: 0.2073\n",
      "Epoch: 080/1000 | Batch 010/052 | Cost: 0.2200\n",
      "Epoch: 080/1000 | Batch 020/052 | Cost: 0.2681\n",
      "Epoch: 080/1000 | Batch 030/052 | Cost: 0.1763\n",
      "Epoch: 080/1000 | Batch 040/052 | Cost: 0.5477\n",
      "Epoch: 080/1000 | Batch 050/052 | Cost: 0.2477\n",
      "Epoch: 080/1000 training accuracy: 86.42%\n",
      "Epoch: 081/1000 | Batch 000/052 | Cost: 0.6342\n",
      "Epoch: 081/1000 | Batch 010/052 | Cost: 0.3274\n",
      "Epoch: 081/1000 | Batch 020/052 | Cost: 0.5780\n",
      "Epoch: 081/1000 | Batch 030/052 | Cost: 0.2232\n",
      "Epoch: 081/1000 | Batch 040/052 | Cost: 0.2538\n",
      "Epoch: 081/1000 | Batch 050/052 | Cost: 0.3293\n",
      "Epoch: 081/1000 training accuracy: 86.87%\n",
      "Epoch: 082/1000 | Batch 000/052 | Cost: 0.3467\n",
      "Epoch: 082/1000 | Batch 010/052 | Cost: 0.4219\n",
      "Epoch: 082/1000 | Batch 020/052 | Cost: 0.4996\n",
      "Epoch: 082/1000 | Batch 030/052 | Cost: 0.2299\n",
      "Epoch: 082/1000 | Batch 040/052 | Cost: 0.1406\n",
      "Epoch: 082/1000 | Batch 050/052 | Cost: 0.4870\n",
      "Epoch: 082/1000 training accuracy: 87.43%\n",
      "Epoch: 083/1000 | Batch 000/052 | Cost: 0.2354\n",
      "Epoch: 083/1000 | Batch 010/052 | Cost: 0.5931\n",
      "Epoch: 083/1000 | Batch 020/052 | Cost: 0.2159\n",
      "Epoch: 083/1000 | Batch 030/052 | Cost: 0.1905\n",
      "Epoch: 083/1000 | Batch 040/052 | Cost: 0.3232\n",
      "Epoch: 083/1000 | Batch 050/052 | Cost: 0.1265\n",
      "Epoch: 083/1000 training accuracy: 87.43%\n",
      "Epoch: 084/1000 | Batch 000/052 | Cost: 0.2725\n",
      "Epoch: 084/1000 | Batch 010/052 | Cost: 0.5799\n",
      "Epoch: 084/1000 | Batch 020/052 | Cost: 0.5249\n",
      "Epoch: 084/1000 | Batch 030/052 | Cost: 0.3725\n",
      "Epoch: 084/1000 | Batch 040/052 | Cost: 0.6166\n",
      "Epoch: 084/1000 | Batch 050/052 | Cost: 0.2355\n",
      "Epoch: 084/1000 training accuracy: 87.65%\n",
      "Epoch: 085/1000 | Batch 000/052 | Cost: 0.2141\n",
      "Epoch: 085/1000 | Batch 010/052 | Cost: 0.3455\n",
      "Epoch: 085/1000 | Batch 020/052 | Cost: 0.2152\n",
      "Epoch: 085/1000 | Batch 030/052 | Cost: 0.2586\n",
      "Epoch: 085/1000 | Batch 040/052 | Cost: 0.3772\n",
      "Epoch: 085/1000 | Batch 050/052 | Cost: 0.3267\n",
      "Epoch: 085/1000 training accuracy: 86.31%\n",
      "Epoch: 086/1000 | Batch 000/052 | Cost: 0.2209\n",
      "Epoch: 086/1000 | Batch 010/052 | Cost: 0.3357\n",
      "Epoch: 086/1000 | Batch 020/052 | Cost: 0.2948\n",
      "Epoch: 086/1000 | Batch 030/052 | Cost: 0.2066\n",
      "Epoch: 086/1000 | Batch 040/052 | Cost: 0.3778\n",
      "Epoch: 086/1000 | Batch 050/052 | Cost: 0.3411\n",
      "Epoch: 086/1000 training accuracy: 86.76%\n",
      "Epoch: 087/1000 | Batch 000/052 | Cost: 0.4773\n",
      "Epoch: 087/1000 | Batch 010/052 | Cost: 0.1902\n",
      "Epoch: 087/1000 | Batch 020/052 | Cost: 0.1638\n",
      "Epoch: 087/1000 | Batch 030/052 | Cost: 0.3633\n",
      "Epoch: 087/1000 | Batch 040/052 | Cost: 0.2866\n",
      "Epoch: 087/1000 | Batch 050/052 | Cost: 0.5585\n",
      "Epoch: 087/1000 training accuracy: 87.32%\n",
      "Epoch: 088/1000 | Batch 000/052 | Cost: 0.4496\n",
      "Epoch: 088/1000 | Batch 010/052 | Cost: 0.3515\n",
      "Epoch: 088/1000 | Batch 020/052 | Cost: 0.6380\n",
      "Epoch: 088/1000 | Batch 030/052 | Cost: 0.2207\n",
      "Epoch: 088/1000 | Batch 040/052 | Cost: 0.1798\n",
      "Epoch: 088/1000 | Batch 050/052 | Cost: 0.1763\n",
      "Epoch: 088/1000 training accuracy: 86.76%\n",
      "Epoch: 089/1000 | Batch 000/052 | Cost: 0.2831\n",
      "Epoch: 089/1000 | Batch 010/052 | Cost: 0.2662\n",
      "Epoch: 089/1000 | Batch 020/052 | Cost: 0.2600\n",
      "Epoch: 089/1000 | Batch 030/052 | Cost: 0.1366\n",
      "Epoch: 089/1000 | Batch 040/052 | Cost: 0.2122\n",
      "Epoch: 089/1000 | Batch 050/052 | Cost: 0.4353\n",
      "Epoch: 089/1000 training accuracy: 87.32%\n",
      "Epoch: 090/1000 | Batch 000/052 | Cost: 0.1923\n",
      "Epoch: 090/1000 | Batch 010/052 | Cost: 0.3964\n",
      "Epoch: 090/1000 | Batch 020/052 | Cost: 0.1621\n",
      "Epoch: 090/1000 | Batch 030/052 | Cost: 0.1643\n",
      "Epoch: 090/1000 | Batch 040/052 | Cost: 0.3593\n",
      "Epoch: 090/1000 | Batch 050/052 | Cost: 0.2606\n",
      "Epoch: 090/1000 training accuracy: 86.64%\n",
      "Epoch: 091/1000 | Batch 000/052 | Cost: 0.1564\n",
      "Epoch: 091/1000 | Batch 010/052 | Cost: 0.1937\n",
      "Epoch: 091/1000 | Batch 020/052 | Cost: 0.3034\n",
      "Epoch: 091/1000 | Batch 030/052 | Cost: 0.3225\n",
      "Epoch: 091/1000 | Batch 040/052 | Cost: 0.4222\n",
      "Epoch: 091/1000 | Batch 050/052 | Cost: 0.5110\n",
      "Epoch: 091/1000 training accuracy: 86.42%\n",
      "Epoch: 092/1000 | Batch 000/052 | Cost: 0.1194\n",
      "Epoch: 092/1000 | Batch 010/052 | Cost: 0.3620\n",
      "Epoch: 092/1000 | Batch 020/052 | Cost: 0.2074\n",
      "Epoch: 092/1000 | Batch 030/052 | Cost: 0.3386\n",
      "Epoch: 092/1000 | Batch 040/052 | Cost: 0.3339\n",
      "Epoch: 092/1000 | Batch 050/052 | Cost: 0.5248\n",
      "Epoch: 092/1000 training accuracy: 87.21%\n",
      "Epoch: 093/1000 | Batch 000/052 | Cost: 0.3320\n",
      "Epoch: 093/1000 | Batch 010/052 | Cost: 0.1905\n",
      "Epoch: 093/1000 | Batch 020/052 | Cost: 0.1870\n",
      "Epoch: 093/1000 | Batch 030/052 | Cost: 0.2707\n",
      "Epoch: 093/1000 | Batch 040/052 | Cost: 0.3566\n",
      "Epoch: 093/1000 | Batch 050/052 | Cost: 0.3910\n",
      "Epoch: 093/1000 training accuracy: 87.77%\n",
      "Epoch: 094/1000 | Batch 000/052 | Cost: 0.2294\n",
      "Epoch: 094/1000 | Batch 010/052 | Cost: 0.2043\n",
      "Epoch: 094/1000 | Batch 020/052 | Cost: 0.2264\n",
      "Epoch: 094/1000 | Batch 030/052 | Cost: 0.3673\n",
      "Epoch: 094/1000 | Batch 040/052 | Cost: 0.1662\n",
      "Epoch: 094/1000 | Batch 050/052 | Cost: 0.2510\n",
      "Epoch: 094/1000 training accuracy: 87.09%\n",
      "Epoch: 095/1000 | Batch 000/052 | Cost: 0.1255\n",
      "Epoch: 095/1000 | Batch 010/052 | Cost: 0.7122\n",
      "Epoch: 095/1000 | Batch 020/052 | Cost: 0.6098\n",
      "Epoch: 095/1000 | Batch 030/052 | Cost: 0.3074\n",
      "Epoch: 095/1000 | Batch 040/052 | Cost: 0.3153\n",
      "Epoch: 095/1000 | Batch 050/052 | Cost: 0.3068\n",
      "Epoch: 095/1000 training accuracy: 87.32%\n",
      "Epoch: 096/1000 | Batch 000/052 | Cost: 0.4553\n",
      "Epoch: 096/1000 | Batch 010/052 | Cost: 0.1782\n",
      "Epoch: 096/1000 | Batch 020/052 | Cost: 0.5411\n",
      "Epoch: 096/1000 | Batch 030/052 | Cost: 0.3870\n",
      "Epoch: 096/1000 | Batch 040/052 | Cost: 0.3119\n",
      "Epoch: 096/1000 | Batch 050/052 | Cost: 0.3490\n",
      "Epoch: 096/1000 training accuracy: 86.76%\n",
      "Epoch: 097/1000 | Batch 000/052 | Cost: 0.0876\n",
      "Epoch: 097/1000 | Batch 010/052 | Cost: 0.3457\n",
      "Epoch: 097/1000 | Batch 020/052 | Cost: 0.3716\n",
      "Epoch: 097/1000 | Batch 030/052 | Cost: 0.3112\n",
      "Epoch: 097/1000 | Batch 040/052 | Cost: 0.1492\n",
      "Epoch: 097/1000 | Batch 050/052 | Cost: 0.4157\n",
      "Epoch: 097/1000 training accuracy: 87.43%\n",
      "Epoch: 098/1000 | Batch 000/052 | Cost: 0.6960\n",
      "Epoch: 098/1000 | Batch 010/052 | Cost: 0.2683\n",
      "Epoch: 098/1000 | Batch 020/052 | Cost: 0.3233\n",
      "Epoch: 098/1000 | Batch 030/052 | Cost: 0.3632\n",
      "Epoch: 098/1000 | Batch 040/052 | Cost: 0.1739\n",
      "Epoch: 098/1000 | Batch 050/052 | Cost: 0.2254\n",
      "Epoch: 098/1000 training accuracy: 87.32%\n",
      "Epoch: 099/1000 | Batch 000/052 | Cost: 0.1284\n",
      "Epoch: 099/1000 | Batch 010/052 | Cost: 0.2007\n",
      "Epoch: 099/1000 | Batch 020/052 | Cost: 0.5319\n",
      "Epoch: 099/1000 | Batch 030/052 | Cost: 0.6466\n",
      "Epoch: 099/1000 | Batch 040/052 | Cost: 0.4333\n",
      "Epoch: 099/1000 | Batch 050/052 | Cost: 0.5132\n",
      "Epoch: 099/1000 training accuracy: 87.54%\n",
      "Epoch: 100/1000 | Batch 000/052 | Cost: 0.2742\n",
      "Epoch: 100/1000 | Batch 010/052 | Cost: 0.2593\n",
      "Epoch: 100/1000 | Batch 020/052 | Cost: 0.6022\n",
      "Epoch: 100/1000 | Batch 030/052 | Cost: 0.3407\n",
      "Epoch: 100/1000 | Batch 040/052 | Cost: 0.1313\n",
      "Epoch: 100/1000 | Batch 050/052 | Cost: 0.2145\n",
      "Epoch: 100/1000 training accuracy: 87.99%\n",
      "Epoch: 101/1000 | Batch 000/052 | Cost: 0.1545\n",
      "Epoch: 101/1000 | Batch 010/052 | Cost: 0.2047\n",
      "Epoch: 101/1000 | Batch 020/052 | Cost: 0.2566\n",
      "Epoch: 101/1000 | Batch 030/052 | Cost: 0.2698\n",
      "Epoch: 101/1000 | Batch 040/052 | Cost: 0.6428\n",
      "Epoch: 101/1000 | Batch 050/052 | Cost: 0.1929\n",
      "Epoch: 101/1000 training accuracy: 87.54%\n",
      "Epoch: 102/1000 | Batch 000/052 | Cost: 0.2063\n",
      "Epoch: 102/1000 | Batch 010/052 | Cost: 0.2945\n",
      "Epoch: 102/1000 | Batch 020/052 | Cost: 0.2367\n",
      "Epoch: 102/1000 | Batch 030/052 | Cost: 0.2621\n",
      "Epoch: 102/1000 | Batch 040/052 | Cost: 0.2724\n",
      "Epoch: 102/1000 | Batch 050/052 | Cost: 0.3581\n",
      "Epoch: 102/1000 training accuracy: 86.64%\n",
      "Epoch: 103/1000 | Batch 000/052 | Cost: 0.4111\n",
      "Epoch: 103/1000 | Batch 010/052 | Cost: 0.3008\n",
      "Epoch: 103/1000 | Batch 020/052 | Cost: 0.4791\n",
      "Epoch: 103/1000 | Batch 030/052 | Cost: 0.4613\n",
      "Epoch: 103/1000 | Batch 040/052 | Cost: 0.1901\n",
      "Epoch: 103/1000 | Batch 050/052 | Cost: 0.3695\n",
      "Epoch: 103/1000 training accuracy: 86.64%\n",
      "Epoch: 104/1000 | Batch 000/052 | Cost: 0.2323\n",
      "Epoch: 104/1000 | Batch 010/052 | Cost: 0.4501\n",
      "Epoch: 104/1000 | Batch 020/052 | Cost: 0.3475\n",
      "Epoch: 104/1000 | Batch 030/052 | Cost: 0.5404\n",
      "Epoch: 104/1000 | Batch 040/052 | Cost: 0.3144\n",
      "Epoch: 104/1000 | Batch 050/052 | Cost: 0.3261\n",
      "Epoch: 104/1000 training accuracy: 86.98%\n",
      "Epoch: 105/1000 | Batch 000/052 | Cost: 0.1690\n",
      "Epoch: 105/1000 | Batch 010/052 | Cost: 0.1604\n",
      "Epoch: 105/1000 | Batch 020/052 | Cost: 0.3167\n",
      "Epoch: 105/1000 | Batch 030/052 | Cost: 0.2456\n",
      "Epoch: 105/1000 | Batch 040/052 | Cost: 0.1541\n",
      "Epoch: 105/1000 | Batch 050/052 | Cost: 0.3609\n",
      "Epoch: 105/1000 training accuracy: 87.77%\n",
      "Epoch: 106/1000 | Batch 000/052 | Cost: 0.1802\n",
      "Epoch: 106/1000 | Batch 010/052 | Cost: 0.5586\n",
      "Epoch: 106/1000 | Batch 020/052 | Cost: 0.4999\n",
      "Epoch: 106/1000 | Batch 030/052 | Cost: 0.4066\n",
      "Epoch: 106/1000 | Batch 040/052 | Cost: 0.4405\n",
      "Epoch: 106/1000 | Batch 050/052 | Cost: 0.2089\n",
      "Epoch: 106/1000 training accuracy: 86.42%\n",
      "Epoch: 107/1000 | Batch 000/052 | Cost: 0.2540\n",
      "Epoch: 107/1000 | Batch 010/052 | Cost: 0.2482\n",
      "Epoch: 107/1000 | Batch 020/052 | Cost: 0.3318\n",
      "Epoch: 107/1000 | Batch 030/052 | Cost: 0.1866\n",
      "Epoch: 107/1000 | Batch 040/052 | Cost: 0.5979\n",
      "Epoch: 107/1000 | Batch 050/052 | Cost: 0.1015\n",
      "Epoch: 107/1000 training accuracy: 87.32%\n",
      "Epoch: 108/1000 | Batch 000/052 | Cost: 0.3605\n",
      "Epoch: 108/1000 | Batch 010/052 | Cost: 0.2870\n",
      "Epoch: 108/1000 | Batch 020/052 | Cost: 0.2318\n",
      "Epoch: 108/1000 | Batch 030/052 | Cost: 0.3167\n",
      "Epoch: 108/1000 | Batch 040/052 | Cost: 0.2649\n",
      "Epoch: 108/1000 | Batch 050/052 | Cost: 0.3462\n",
      "Epoch: 108/1000 training accuracy: 87.09%\n",
      "Epoch: 109/1000 | Batch 000/052 | Cost: 0.1220\n",
      "Epoch: 109/1000 | Batch 010/052 | Cost: 0.4625\n",
      "Epoch: 109/1000 | Batch 020/052 | Cost: 0.3362\n",
      "Epoch: 109/1000 | Batch 030/052 | Cost: 0.2917\n",
      "Epoch: 109/1000 | Batch 040/052 | Cost: 0.4267\n",
      "Epoch: 109/1000 | Batch 050/052 | Cost: 0.3260\n",
      "Epoch: 109/1000 training accuracy: 86.98%\n",
      "Epoch: 110/1000 | Batch 000/052 | Cost: 0.2673\n",
      "Epoch: 110/1000 | Batch 010/052 | Cost: 0.2001\n",
      "Epoch: 110/1000 | Batch 020/052 | Cost: 0.3199\n",
      "Epoch: 110/1000 | Batch 030/052 | Cost: 0.4582\n",
      "Epoch: 110/1000 | Batch 040/052 | Cost: 0.4258\n",
      "Epoch: 110/1000 | Batch 050/052 | Cost: 0.2501\n",
      "Epoch: 110/1000 training accuracy: 87.65%\n",
      "Epoch: 111/1000 | Batch 000/052 | Cost: 0.2865\n",
      "Epoch: 111/1000 | Batch 010/052 | Cost: 0.5728\n",
      "Epoch: 111/1000 | Batch 020/052 | Cost: 0.3357\n",
      "Epoch: 111/1000 | Batch 030/052 | Cost: 0.2973\n",
      "Epoch: 111/1000 | Batch 040/052 | Cost: 0.2287\n",
      "Epoch: 111/1000 | Batch 050/052 | Cost: 0.2490\n",
      "Epoch: 111/1000 training accuracy: 87.21%\n",
      "Epoch: 112/1000 | Batch 000/052 | Cost: 0.1626\n",
      "Epoch: 112/1000 | Batch 010/052 | Cost: 0.3145\n",
      "Epoch: 112/1000 | Batch 020/052 | Cost: 0.6078\n",
      "Epoch: 112/1000 | Batch 030/052 | Cost: 0.4579\n",
      "Epoch: 112/1000 | Batch 040/052 | Cost: 0.1548\n",
      "Epoch: 112/1000 | Batch 050/052 | Cost: 0.2685\n",
      "Epoch: 112/1000 training accuracy: 87.54%\n",
      "Epoch: 113/1000 | Batch 000/052 | Cost: 0.3630\n",
      "Epoch: 113/1000 | Batch 010/052 | Cost: 0.2706\n",
      "Epoch: 113/1000 | Batch 020/052 | Cost: 0.3145\n",
      "Epoch: 113/1000 | Batch 030/052 | Cost: 0.2611\n",
      "Epoch: 113/1000 | Batch 040/052 | Cost: 0.2023\n",
      "Epoch: 113/1000 | Batch 050/052 | Cost: 0.1947\n",
      "Epoch: 113/1000 training accuracy: 87.32%\n",
      "Epoch: 114/1000 | Batch 000/052 | Cost: 0.1805\n",
      "Epoch: 114/1000 | Batch 010/052 | Cost: 0.2197\n",
      "Epoch: 114/1000 | Batch 020/052 | Cost: 0.3958\n",
      "Epoch: 114/1000 | Batch 030/052 | Cost: 0.3931\n",
      "Epoch: 114/1000 | Batch 040/052 | Cost: 0.3985\n",
      "Epoch: 114/1000 | Batch 050/052 | Cost: 0.4411\n",
      "Epoch: 114/1000 training accuracy: 87.65%\n",
      "Epoch: 115/1000 | Batch 000/052 | Cost: 0.4171\n",
      "Epoch: 115/1000 | Batch 010/052 | Cost: 0.4463\n",
      "Epoch: 115/1000 | Batch 020/052 | Cost: 0.3550\n",
      "Epoch: 115/1000 | Batch 030/052 | Cost: 0.7056\n",
      "Epoch: 115/1000 | Batch 040/052 | Cost: 0.0503\n",
      "Epoch: 115/1000 | Batch 050/052 | Cost: 0.4191\n",
      "Epoch: 115/1000 training accuracy: 87.21%\n",
      "Epoch: 116/1000 | Batch 000/052 | Cost: 0.3800\n",
      "Epoch: 116/1000 | Batch 010/052 | Cost: 0.4235\n",
      "Epoch: 116/1000 | Batch 020/052 | Cost: 0.1579\n",
      "Epoch: 116/1000 | Batch 030/052 | Cost: 0.3055\n",
      "Epoch: 116/1000 | Batch 040/052 | Cost: 0.1661\n",
      "Epoch: 116/1000 | Batch 050/052 | Cost: 0.1662\n",
      "Epoch: 116/1000 training accuracy: 87.43%\n",
      "Epoch: 117/1000 | Batch 000/052 | Cost: 0.4430\n",
      "Epoch: 117/1000 | Batch 010/052 | Cost: 0.1249\n",
      "Epoch: 117/1000 | Batch 020/052 | Cost: 0.2416\n",
      "Epoch: 117/1000 | Batch 030/052 | Cost: 0.5050\n",
      "Epoch: 117/1000 | Batch 040/052 | Cost: 0.3357\n",
      "Epoch: 117/1000 | Batch 050/052 | Cost: 0.3440\n",
      "Epoch: 117/1000 training accuracy: 88.55%\n",
      "Epoch: 118/1000 | Batch 000/052 | Cost: 0.0572\n",
      "Epoch: 118/1000 | Batch 010/052 | Cost: 0.3853\n",
      "Epoch: 118/1000 | Batch 020/052 | Cost: 0.3245\n",
      "Epoch: 118/1000 | Batch 030/052 | Cost: 0.2727\n",
      "Epoch: 118/1000 | Batch 040/052 | Cost: 0.2812\n",
      "Epoch: 118/1000 | Batch 050/052 | Cost: 0.5402\n",
      "Epoch: 118/1000 training accuracy: 87.77%\n",
      "Epoch: 119/1000 | Batch 000/052 | Cost: 0.3067\n",
      "Epoch: 119/1000 | Batch 010/052 | Cost: 0.3452\n",
      "Epoch: 119/1000 | Batch 020/052 | Cost: 0.3253\n",
      "Epoch: 119/1000 | Batch 030/052 | Cost: 0.1224\n",
      "Epoch: 119/1000 | Batch 040/052 | Cost: 0.2729\n",
      "Epoch: 119/1000 | Batch 050/052 | Cost: 0.4401\n",
      "Epoch: 119/1000 training accuracy: 88.33%\n",
      "Epoch: 120/1000 | Batch 000/052 | Cost: 0.2769\n",
      "Epoch: 120/1000 | Batch 010/052 | Cost: 0.2713\n",
      "Epoch: 120/1000 | Batch 020/052 | Cost: 0.6981\n",
      "Epoch: 120/1000 | Batch 030/052 | Cost: 0.3498\n",
      "Epoch: 120/1000 | Batch 040/052 | Cost: 0.4367\n",
      "Epoch: 120/1000 | Batch 050/052 | Cost: 0.4526\n",
      "Epoch: 120/1000 training accuracy: 87.09%\n",
      "Epoch: 121/1000 | Batch 000/052 | Cost: 0.2239\n",
      "Epoch: 121/1000 | Batch 010/052 | Cost: 0.2177\n",
      "Epoch: 121/1000 | Batch 020/052 | Cost: 0.1277\n",
      "Epoch: 121/1000 | Batch 030/052 | Cost: 0.2326\n",
      "Epoch: 121/1000 | Batch 040/052 | Cost: 0.3641\n",
      "Epoch: 121/1000 | Batch 050/052 | Cost: 0.3091\n",
      "Epoch: 121/1000 training accuracy: 86.64%\n",
      "Epoch: 122/1000 | Batch 000/052 | Cost: 0.1532\n",
      "Epoch: 122/1000 | Batch 010/052 | Cost: 0.3084\n",
      "Epoch: 122/1000 | Batch 020/052 | Cost: 0.4795\n",
      "Epoch: 122/1000 | Batch 030/052 | Cost: 0.3333\n",
      "Epoch: 122/1000 | Batch 040/052 | Cost: 0.1703\n",
      "Epoch: 122/1000 | Batch 050/052 | Cost: 0.2807\n",
      "Epoch: 122/1000 training accuracy: 87.43%\n",
      "Epoch: 123/1000 | Batch 000/052 | Cost: 0.1724\n",
      "Epoch: 123/1000 | Batch 010/052 | Cost: 0.1345\n",
      "Epoch: 123/1000 | Batch 020/052 | Cost: 0.2100\n",
      "Epoch: 123/1000 | Batch 030/052 | Cost: 0.3827\n",
      "Epoch: 123/1000 | Batch 040/052 | Cost: 0.2031\n",
      "Epoch: 123/1000 | Batch 050/052 | Cost: 0.4468\n",
      "Epoch: 123/1000 training accuracy: 87.77%\n",
      "Epoch: 124/1000 | Batch 000/052 | Cost: 0.1505\n",
      "Epoch: 124/1000 | Batch 010/052 | Cost: 0.1130\n",
      "Epoch: 124/1000 | Batch 020/052 | Cost: 0.1559\n",
      "Epoch: 124/1000 | Batch 030/052 | Cost: 0.2249\n",
      "Epoch: 124/1000 | Batch 040/052 | Cost: 0.1861\n",
      "Epoch: 124/1000 | Batch 050/052 | Cost: 0.3636\n",
      "Epoch: 124/1000 training accuracy: 88.10%\n",
      "Epoch: 125/1000 | Batch 000/052 | Cost: 0.1437\n",
      "Epoch: 125/1000 | Batch 010/052 | Cost: 0.4066\n",
      "Epoch: 125/1000 | Batch 020/052 | Cost: 0.4148\n",
      "Epoch: 125/1000 | Batch 030/052 | Cost: 0.5308\n",
      "Epoch: 125/1000 | Batch 040/052 | Cost: 0.5336\n",
      "Epoch: 125/1000 | Batch 050/052 | Cost: 0.1193\n",
      "Epoch: 125/1000 training accuracy: 87.43%\n",
      "Epoch: 126/1000 | Batch 000/052 | Cost: 0.1373\n",
      "Epoch: 126/1000 | Batch 010/052 | Cost: 0.2258\n",
      "Epoch: 126/1000 | Batch 020/052 | Cost: 0.1927\n",
      "Epoch: 126/1000 | Batch 030/052 | Cost: 0.2905\n",
      "Epoch: 126/1000 | Batch 040/052 | Cost: 0.1292\n",
      "Epoch: 126/1000 | Batch 050/052 | Cost: 0.2951\n",
      "Epoch: 126/1000 training accuracy: 87.77%\n",
      "Epoch: 127/1000 | Batch 000/052 | Cost: 0.3460\n",
      "Epoch: 127/1000 | Batch 010/052 | Cost: 0.1674\n",
      "Epoch: 127/1000 | Batch 020/052 | Cost: 0.2666\n",
      "Epoch: 127/1000 | Batch 030/052 | Cost: 0.2020\n",
      "Epoch: 127/1000 | Batch 040/052 | Cost: 0.1264\n",
      "Epoch: 127/1000 | Batch 050/052 | Cost: 0.2842\n",
      "Epoch: 127/1000 training accuracy: 87.32%\n",
      "Epoch: 128/1000 | Batch 000/052 | Cost: 0.3435\n",
      "Epoch: 128/1000 | Batch 010/052 | Cost: 0.4593\n",
      "Epoch: 128/1000 | Batch 020/052 | Cost: 0.0984\n",
      "Epoch: 128/1000 | Batch 030/052 | Cost: 0.2159\n",
      "Epoch: 128/1000 | Batch 040/052 | Cost: 0.1196\n",
      "Epoch: 128/1000 | Batch 050/052 | Cost: 0.1742\n",
      "Epoch: 128/1000 training accuracy: 87.54%\n",
      "Epoch: 129/1000 | Batch 000/052 | Cost: 0.3634\n",
      "Epoch: 129/1000 | Batch 010/052 | Cost: 0.1834\n",
      "Epoch: 129/1000 | Batch 020/052 | Cost: 0.1986\n",
      "Epoch: 129/1000 | Batch 030/052 | Cost: 0.1122\n",
      "Epoch: 129/1000 | Batch 040/052 | Cost: 0.1471\n",
      "Epoch: 129/1000 | Batch 050/052 | Cost: 0.2539\n",
      "Epoch: 129/1000 training accuracy: 87.65%\n",
      "Epoch: 130/1000 | Batch 000/052 | Cost: 0.2512\n",
      "Epoch: 130/1000 | Batch 010/052 | Cost: 0.2722\n",
      "Epoch: 130/1000 | Batch 020/052 | Cost: 0.2016\n",
      "Epoch: 130/1000 | Batch 030/052 | Cost: 0.3195\n",
      "Epoch: 130/1000 | Batch 040/052 | Cost: 0.4269\n",
      "Epoch: 130/1000 | Batch 050/052 | Cost: 0.1458\n",
      "Epoch: 130/1000 training accuracy: 86.64%\n",
      "Epoch: 131/1000 | Batch 000/052 | Cost: 0.1824\n",
      "Epoch: 131/1000 | Batch 010/052 | Cost: 0.1083\n",
      "Epoch: 131/1000 | Batch 020/052 | Cost: 0.1621\n",
      "Epoch: 131/1000 | Batch 030/052 | Cost: 0.4966\n",
      "Epoch: 131/1000 | Batch 040/052 | Cost: 0.0910\n",
      "Epoch: 131/1000 | Batch 050/052 | Cost: 0.6143\n",
      "Epoch: 131/1000 training accuracy: 87.88%\n",
      "Epoch: 132/1000 | Batch 000/052 | Cost: 0.4651\n",
      "Epoch: 132/1000 | Batch 010/052 | Cost: 0.5614\n",
      "Epoch: 132/1000 | Batch 020/052 | Cost: 0.3048\n",
      "Epoch: 132/1000 | Batch 030/052 | Cost: 0.2070\n",
      "Epoch: 132/1000 | Batch 040/052 | Cost: 0.2939\n",
      "Epoch: 132/1000 | Batch 050/052 | Cost: 0.3271\n",
      "Epoch: 132/1000 training accuracy: 87.88%\n",
      "Epoch: 133/1000 | Batch 000/052 | Cost: 0.3090\n",
      "Epoch: 133/1000 | Batch 010/052 | Cost: 0.2787\n",
      "Epoch: 133/1000 | Batch 020/052 | Cost: 0.2813\n",
      "Epoch: 133/1000 | Batch 030/052 | Cost: 0.2431\n",
      "Epoch: 133/1000 | Batch 040/052 | Cost: 0.3342\n",
      "Epoch: 133/1000 | Batch 050/052 | Cost: 0.3487\n",
      "Epoch: 133/1000 training accuracy: 87.21%\n",
      "Epoch: 134/1000 | Batch 000/052 | Cost: 0.3423\n",
      "Epoch: 134/1000 | Batch 010/052 | Cost: 0.2622\n",
      "Epoch: 134/1000 | Batch 020/052 | Cost: 0.3404\n",
      "Epoch: 134/1000 | Batch 030/052 | Cost: 0.3711\n",
      "Epoch: 134/1000 | Batch 040/052 | Cost: 0.2902\n",
      "Epoch: 134/1000 | Batch 050/052 | Cost: 0.4270\n",
      "Epoch: 134/1000 training accuracy: 87.43%\n",
      "Epoch: 135/1000 | Batch 000/052 | Cost: 0.1381\n",
      "Epoch: 135/1000 | Batch 010/052 | Cost: 0.1425\n",
      "Epoch: 135/1000 | Batch 020/052 | Cost: 0.2467\n",
      "Epoch: 135/1000 | Batch 030/052 | Cost: 0.3429\n",
      "Epoch: 135/1000 | Batch 040/052 | Cost: 0.2739\n",
      "Epoch: 135/1000 | Batch 050/052 | Cost: 0.4453\n",
      "Epoch: 135/1000 training accuracy: 87.43%\n",
      "Epoch: 136/1000 | Batch 000/052 | Cost: 0.2748\n",
      "Epoch: 136/1000 | Batch 010/052 | Cost: 0.2262\n",
      "Epoch: 136/1000 | Batch 020/052 | Cost: 0.4613\n",
      "Epoch: 136/1000 | Batch 030/052 | Cost: 0.1868\n",
      "Epoch: 136/1000 | Batch 040/052 | Cost: 0.2971\n",
      "Epoch: 136/1000 | Batch 050/052 | Cost: 0.3809\n",
      "Epoch: 136/1000 training accuracy: 87.43%\n",
      "Epoch: 137/1000 | Batch 000/052 | Cost: 0.3374\n",
      "Epoch: 137/1000 | Batch 010/052 | Cost: 0.3739\n",
      "Epoch: 137/1000 | Batch 020/052 | Cost: 0.2749\n",
      "Epoch: 137/1000 | Batch 030/052 | Cost: 0.2345\n",
      "Epoch: 137/1000 | Batch 040/052 | Cost: 0.1797\n",
      "Epoch: 137/1000 | Batch 050/052 | Cost: 0.1911\n",
      "Epoch: 137/1000 training accuracy: 87.43%\n",
      "Epoch: 138/1000 | Batch 000/052 | Cost: 0.2887\n",
      "Epoch: 138/1000 | Batch 010/052 | Cost: 0.4513\n",
      "Epoch: 138/1000 | Batch 020/052 | Cost: 0.3662\n",
      "Epoch: 138/1000 | Batch 030/052 | Cost: 0.1858\n",
      "Epoch: 138/1000 | Batch 040/052 | Cost: 0.4843\n",
      "Epoch: 138/1000 | Batch 050/052 | Cost: 0.5438\n",
      "Epoch: 138/1000 training accuracy: 86.87%\n",
      "Epoch: 139/1000 | Batch 000/052 | Cost: 0.2172\n",
      "Epoch: 139/1000 | Batch 010/052 | Cost: 0.2519\n",
      "Epoch: 139/1000 | Batch 020/052 | Cost: 0.7069\n",
      "Epoch: 139/1000 | Batch 030/052 | Cost: 0.3023\n",
      "Epoch: 139/1000 | Batch 040/052 | Cost: 0.3105\n",
      "Epoch: 139/1000 | Batch 050/052 | Cost: 0.1585\n",
      "Epoch: 139/1000 training accuracy: 88.10%\n",
      "Epoch: 140/1000 | Batch 000/052 | Cost: 0.4057\n",
      "Epoch: 140/1000 | Batch 010/052 | Cost: 0.3231\n",
      "Epoch: 140/1000 | Batch 020/052 | Cost: 0.4964\n",
      "Epoch: 140/1000 | Batch 030/052 | Cost: 0.1636\n",
      "Epoch: 140/1000 | Batch 040/052 | Cost: 0.2693\n",
      "Epoch: 140/1000 | Batch 050/052 | Cost: 0.0842\n",
      "Epoch: 140/1000 training accuracy: 86.42%\n",
      "Epoch: 141/1000 | Batch 000/052 | Cost: 0.4016\n",
      "Epoch: 141/1000 | Batch 010/052 | Cost: 0.1221\n",
      "Epoch: 141/1000 | Batch 020/052 | Cost: 0.5583\n",
      "Epoch: 141/1000 | Batch 030/052 | Cost: 0.2060\n",
      "Epoch: 141/1000 | Batch 040/052 | Cost: 0.4839\n",
      "Epoch: 141/1000 | Batch 050/052 | Cost: 0.4740\n",
      "Epoch: 141/1000 training accuracy: 87.09%\n",
      "Epoch: 142/1000 | Batch 000/052 | Cost: 0.1681\n",
      "Epoch: 142/1000 | Batch 010/052 | Cost: 0.6404\n",
      "Epoch: 142/1000 | Batch 020/052 | Cost: 0.4009\n",
      "Epoch: 142/1000 | Batch 030/052 | Cost: 0.2387\n",
      "Epoch: 142/1000 | Batch 040/052 | Cost: 0.2993\n",
      "Epoch: 142/1000 | Batch 050/052 | Cost: 0.3904\n",
      "Epoch: 142/1000 training accuracy: 87.54%\n",
      "Epoch: 143/1000 | Batch 000/052 | Cost: 0.1978\n",
      "Epoch: 143/1000 | Batch 010/052 | Cost: 0.2732\n",
      "Epoch: 143/1000 | Batch 020/052 | Cost: 0.1927\n",
      "Epoch: 143/1000 | Batch 030/052 | Cost: 0.0946\n",
      "Epoch: 143/1000 | Batch 040/052 | Cost: 0.3118\n",
      "Epoch: 143/1000 | Batch 050/052 | Cost: 0.2128\n",
      "Epoch: 143/1000 training accuracy: 87.54%\n",
      "Epoch: 144/1000 | Batch 000/052 | Cost: 0.1478\n",
      "Epoch: 144/1000 | Batch 010/052 | Cost: 0.1515\n",
      "Epoch: 144/1000 | Batch 020/052 | Cost: 0.4918\n",
      "Epoch: 144/1000 | Batch 030/052 | Cost: 0.3676\n",
      "Epoch: 144/1000 | Batch 040/052 | Cost: 0.4513\n",
      "Epoch: 144/1000 | Batch 050/052 | Cost: 0.5308\n",
      "Epoch: 144/1000 training accuracy: 87.32%\n",
      "Epoch: 145/1000 | Batch 000/052 | Cost: 0.2298\n",
      "Epoch: 145/1000 | Batch 010/052 | Cost: 0.1093\n",
      "Epoch: 145/1000 | Batch 020/052 | Cost: 0.3783\n",
      "Epoch: 145/1000 | Batch 030/052 | Cost: 0.1816\n",
      "Epoch: 145/1000 | Batch 040/052 | Cost: 0.2245\n",
      "Epoch: 145/1000 | Batch 050/052 | Cost: 0.2556\n",
      "Epoch: 145/1000 training accuracy: 86.87%\n",
      "Epoch: 146/1000 | Batch 000/052 | Cost: 0.1436\n",
      "Epoch: 146/1000 | Batch 010/052 | Cost: 0.2483\n",
      "Epoch: 146/1000 | Batch 020/052 | Cost: 0.5770\n",
      "Epoch: 146/1000 | Batch 030/052 | Cost: 0.2947\n",
      "Epoch: 146/1000 | Batch 040/052 | Cost: 0.4691\n",
      "Epoch: 146/1000 | Batch 050/052 | Cost: 0.4979\n",
      "Epoch: 146/1000 training accuracy: 88.33%\n",
      "Epoch: 147/1000 | Batch 000/052 | Cost: 0.1233\n",
      "Epoch: 147/1000 | Batch 010/052 | Cost: 0.1417\n",
      "Epoch: 147/1000 | Batch 020/052 | Cost: 0.5007\n",
      "Epoch: 147/1000 | Batch 030/052 | Cost: 0.3588\n",
      "Epoch: 147/1000 | Batch 040/052 | Cost: 0.1531\n",
      "Epoch: 147/1000 | Batch 050/052 | Cost: 0.4880\n",
      "Epoch: 147/1000 training accuracy: 87.54%\n",
      "Epoch: 148/1000 | Batch 000/052 | Cost: 0.3045\n",
      "Epoch: 148/1000 | Batch 010/052 | Cost: 0.2411\n",
      "Epoch: 148/1000 | Batch 020/052 | Cost: 0.3354\n",
      "Epoch: 148/1000 | Batch 030/052 | Cost: 0.4482\n",
      "Epoch: 148/1000 | Batch 040/052 | Cost: 0.2909\n",
      "Epoch: 148/1000 | Batch 050/052 | Cost: 0.2805\n",
      "Epoch: 148/1000 training accuracy: 87.21%\n",
      "Epoch: 149/1000 | Batch 000/052 | Cost: 0.1267\n",
      "Epoch: 149/1000 | Batch 010/052 | Cost: 0.3816\n",
      "Epoch: 149/1000 | Batch 020/052 | Cost: 0.4417\n",
      "Epoch: 149/1000 | Batch 030/052 | Cost: 0.5314\n",
      "Epoch: 149/1000 | Batch 040/052 | Cost: 0.3443\n",
      "Epoch: 149/1000 | Batch 050/052 | Cost: 0.2120\n",
      "Epoch: 149/1000 training accuracy: 87.88%\n",
      "Epoch: 150/1000 | Batch 000/052 | Cost: 0.2285\n",
      "Epoch: 150/1000 | Batch 010/052 | Cost: 0.1740\n",
      "Epoch: 150/1000 | Batch 020/052 | Cost: 0.3119\n",
      "Epoch: 150/1000 | Batch 030/052 | Cost: 0.3244\n",
      "Epoch: 150/1000 | Batch 040/052 | Cost: 0.2490\n",
      "Epoch: 150/1000 | Batch 050/052 | Cost: 0.1533\n",
      "Epoch: 150/1000 training accuracy: 88.22%\n",
      "Epoch: 151/1000 | Batch 000/052 | Cost: 0.2573\n",
      "Epoch: 151/1000 | Batch 010/052 | Cost: 0.5252\n",
      "Epoch: 151/1000 | Batch 020/052 | Cost: 0.5631\n",
      "Epoch: 151/1000 | Batch 030/052 | Cost: 0.3131\n",
      "Epoch: 151/1000 | Batch 040/052 | Cost: 0.2095\n",
      "Epoch: 151/1000 | Batch 050/052 | Cost: 0.2701\n",
      "Epoch: 151/1000 training accuracy: 88.78%\n",
      "Epoch: 152/1000 | Batch 000/052 | Cost: 0.2809\n",
      "Epoch: 152/1000 | Batch 010/052 | Cost: 0.1619\n",
      "Epoch: 152/1000 | Batch 020/052 | Cost: 0.4667\n",
      "Epoch: 152/1000 | Batch 030/052 | Cost: 0.4022\n",
      "Epoch: 152/1000 | Batch 040/052 | Cost: 0.4108\n",
      "Epoch: 152/1000 | Batch 050/052 | Cost: 0.1467\n",
      "Epoch: 152/1000 training accuracy: 87.77%\n",
      "Epoch: 153/1000 | Batch 000/052 | Cost: 0.3149\n",
      "Epoch: 153/1000 | Batch 010/052 | Cost: 0.2192\n",
      "Epoch: 153/1000 | Batch 020/052 | Cost: 0.4292\n",
      "Epoch: 153/1000 | Batch 030/052 | Cost: 0.2528\n",
      "Epoch: 153/1000 | Batch 040/052 | Cost: 0.1518\n",
      "Epoch: 153/1000 | Batch 050/052 | Cost: 0.1397\n",
      "Epoch: 153/1000 training accuracy: 87.65%\n",
      "Epoch: 154/1000 | Batch 000/052 | Cost: 0.2287\n",
      "Epoch: 154/1000 | Batch 010/052 | Cost: 0.6029\n",
      "Epoch: 154/1000 | Batch 020/052 | Cost: 0.2717\n",
      "Epoch: 154/1000 | Batch 030/052 | Cost: 0.4207\n",
      "Epoch: 154/1000 | Batch 040/052 | Cost: 0.5071\n",
      "Epoch: 154/1000 | Batch 050/052 | Cost: 0.5166\n",
      "Epoch: 154/1000 training accuracy: 87.65%\n",
      "Epoch: 155/1000 | Batch 000/052 | Cost: 0.2992\n",
      "Epoch: 155/1000 | Batch 010/052 | Cost: 0.3101\n",
      "Epoch: 155/1000 | Batch 020/052 | Cost: 0.1383\n",
      "Epoch: 155/1000 | Batch 030/052 | Cost: 0.3368\n",
      "Epoch: 155/1000 | Batch 040/052 | Cost: 0.1007\n",
      "Epoch: 155/1000 | Batch 050/052 | Cost: 0.3201\n",
      "Epoch: 155/1000 training accuracy: 87.43%\n",
      "Epoch: 156/1000 | Batch 000/052 | Cost: 0.2114\n",
      "Epoch: 156/1000 | Batch 010/052 | Cost: 0.2105\n",
      "Epoch: 156/1000 | Batch 020/052 | Cost: 0.3143\n",
      "Epoch: 156/1000 | Batch 030/052 | Cost: 0.3744\n",
      "Epoch: 156/1000 | Batch 040/052 | Cost: 0.5439\n",
      "Epoch: 156/1000 | Batch 050/052 | Cost: 0.3986\n",
      "Epoch: 156/1000 training accuracy: 87.65%\n",
      "Epoch: 157/1000 | Batch 000/052 | Cost: 0.2366\n",
      "Epoch: 157/1000 | Batch 010/052 | Cost: 0.4121\n",
      "Epoch: 157/1000 | Batch 020/052 | Cost: 0.4927\n",
      "Epoch: 157/1000 | Batch 030/052 | Cost: 0.2911\n",
      "Epoch: 157/1000 | Batch 040/052 | Cost: 0.1527\n",
      "Epoch: 157/1000 | Batch 050/052 | Cost: 0.3251\n",
      "Epoch: 157/1000 training accuracy: 88.33%\n",
      "Epoch: 158/1000 | Batch 000/052 | Cost: 0.3443\n",
      "Epoch: 158/1000 | Batch 010/052 | Cost: 0.6597\n",
      "Epoch: 158/1000 | Batch 020/052 | Cost: 0.1701\n",
      "Epoch: 158/1000 | Batch 030/052 | Cost: 0.2351\n",
      "Epoch: 158/1000 | Batch 040/052 | Cost: 0.2674\n",
      "Epoch: 158/1000 | Batch 050/052 | Cost: 0.0774\n",
      "Epoch: 158/1000 training accuracy: 87.88%\n",
      "Epoch: 159/1000 | Batch 000/052 | Cost: 0.1452\n",
      "Epoch: 159/1000 | Batch 010/052 | Cost: 0.1557\n",
      "Epoch: 159/1000 | Batch 020/052 | Cost: 0.0753\n",
      "Epoch: 159/1000 | Batch 030/052 | Cost: 0.2867\n",
      "Epoch: 159/1000 | Batch 040/052 | Cost: 0.1724\n",
      "Epoch: 159/1000 | Batch 050/052 | Cost: 0.6031\n",
      "Epoch: 159/1000 training accuracy: 88.66%\n",
      "Epoch: 160/1000 | Batch 000/052 | Cost: 0.3016\n",
      "Epoch: 160/1000 | Batch 010/052 | Cost: 0.1161\n",
      "Epoch: 160/1000 | Batch 020/052 | Cost: 0.1776\n",
      "Epoch: 160/1000 | Batch 030/052 | Cost: 0.2146\n",
      "Epoch: 160/1000 | Batch 040/052 | Cost: 0.3583\n",
      "Epoch: 160/1000 | Batch 050/052 | Cost: 0.3581\n",
      "Epoch: 160/1000 training accuracy: 88.78%\n",
      "Epoch: 161/1000 | Batch 000/052 | Cost: 0.2408\n",
      "Epoch: 161/1000 | Batch 010/052 | Cost: 0.2016\n",
      "Epoch: 161/1000 | Batch 020/052 | Cost: 0.3863\n",
      "Epoch: 161/1000 | Batch 030/052 | Cost: 0.2910\n",
      "Epoch: 161/1000 | Batch 040/052 | Cost: 0.1189\n",
      "Epoch: 161/1000 | Batch 050/052 | Cost: 0.3341\n",
      "Epoch: 161/1000 training accuracy: 87.32%\n",
      "Epoch: 162/1000 | Batch 000/052 | Cost: 0.2590\n",
      "Epoch: 162/1000 | Batch 010/052 | Cost: 0.5964\n",
      "Epoch: 162/1000 | Batch 020/052 | Cost: 0.5026\n",
      "Epoch: 162/1000 | Batch 030/052 | Cost: 0.2095\n",
      "Epoch: 162/1000 | Batch 040/052 | Cost: 0.1531\n",
      "Epoch: 162/1000 | Batch 050/052 | Cost: 0.1355\n",
      "Epoch: 162/1000 training accuracy: 86.98%\n",
      "Epoch: 163/1000 | Batch 000/052 | Cost: 0.3032\n",
      "Epoch: 163/1000 | Batch 010/052 | Cost: 0.0936\n",
      "Epoch: 163/1000 | Batch 020/052 | Cost: 0.5616\n",
      "Epoch: 163/1000 | Batch 030/052 | Cost: 0.2975\n",
      "Epoch: 163/1000 | Batch 040/052 | Cost: 0.2240\n",
      "Epoch: 163/1000 | Batch 050/052 | Cost: 0.3144\n",
      "Epoch: 163/1000 training accuracy: 87.32%\n",
      "Epoch: 164/1000 | Batch 000/052 | Cost: 0.2139\n",
      "Epoch: 164/1000 | Batch 010/052 | Cost: 0.3880\n",
      "Epoch: 164/1000 | Batch 020/052 | Cost: 0.0992\n",
      "Epoch: 164/1000 | Batch 030/052 | Cost: 0.4608\n",
      "Epoch: 164/1000 | Batch 040/052 | Cost: 0.7417\n",
      "Epoch: 164/1000 | Batch 050/052 | Cost: 0.1436\n",
      "Epoch: 164/1000 training accuracy: 88.66%\n",
      "Epoch: 165/1000 | Batch 000/052 | Cost: 0.1528\n",
      "Epoch: 165/1000 | Batch 010/052 | Cost: 0.3428\n",
      "Epoch: 165/1000 | Batch 020/052 | Cost: 0.2326\n",
      "Epoch: 165/1000 | Batch 030/052 | Cost: 0.2367\n",
      "Epoch: 165/1000 | Batch 040/052 | Cost: 0.1293\n",
      "Epoch: 165/1000 | Batch 050/052 | Cost: 0.1509\n",
      "Epoch: 165/1000 training accuracy: 86.64%\n",
      "Epoch: 166/1000 | Batch 000/052 | Cost: 0.3828\n",
      "Epoch: 166/1000 | Batch 010/052 | Cost: 0.2665\n",
      "Epoch: 166/1000 | Batch 020/052 | Cost: 0.3763\n",
      "Epoch: 166/1000 | Batch 030/052 | Cost: 0.2010\n",
      "Epoch: 166/1000 | Batch 040/052 | Cost: 0.1670\n",
      "Epoch: 166/1000 | Batch 050/052 | Cost: 0.1468\n",
      "Epoch: 166/1000 training accuracy: 88.10%\n",
      "Epoch: 167/1000 | Batch 000/052 | Cost: 0.3974\n",
      "Epoch: 167/1000 | Batch 010/052 | Cost: 0.1374\n",
      "Epoch: 167/1000 | Batch 020/052 | Cost: 0.6282\n",
      "Epoch: 167/1000 | Batch 030/052 | Cost: 0.1937\n",
      "Epoch: 167/1000 | Batch 040/052 | Cost: 0.2229\n",
      "Epoch: 167/1000 | Batch 050/052 | Cost: 0.2897\n",
      "Epoch: 167/1000 training accuracy: 87.21%\n",
      "Epoch: 168/1000 | Batch 000/052 | Cost: 0.2462\n",
      "Epoch: 168/1000 | Batch 010/052 | Cost: 0.5764\n",
      "Epoch: 168/1000 | Batch 020/052 | Cost: 0.2863\n",
      "Epoch: 168/1000 | Batch 030/052 | Cost: 0.2283\n",
      "Epoch: 168/1000 | Batch 040/052 | Cost: 0.1037\n",
      "Epoch: 168/1000 | Batch 050/052 | Cost: 0.2367\n",
      "Epoch: 168/1000 training accuracy: 87.65%\n",
      "Epoch: 169/1000 | Batch 000/052 | Cost: 0.4567\n",
      "Epoch: 169/1000 | Batch 010/052 | Cost: 0.0888\n",
      "Epoch: 169/1000 | Batch 020/052 | Cost: 0.2936\n",
      "Epoch: 169/1000 | Batch 030/052 | Cost: 0.4876\n",
      "Epoch: 169/1000 | Batch 040/052 | Cost: 0.4177\n",
      "Epoch: 169/1000 | Batch 050/052 | Cost: 0.1827\n",
      "Epoch: 169/1000 training accuracy: 88.22%\n",
      "Epoch: 170/1000 | Batch 000/052 | Cost: 0.4229\n",
      "Epoch: 170/1000 | Batch 010/052 | Cost: 0.1946\n",
      "Epoch: 170/1000 | Batch 020/052 | Cost: 0.2139\n",
      "Epoch: 170/1000 | Batch 030/052 | Cost: 0.3660\n",
      "Epoch: 170/1000 | Batch 040/052 | Cost: 0.0711\n",
      "Epoch: 170/1000 | Batch 050/052 | Cost: 0.1391\n",
      "Epoch: 170/1000 training accuracy: 87.77%\n",
      "Epoch: 171/1000 | Batch 000/052 | Cost: 0.2215\n",
      "Epoch: 171/1000 | Batch 010/052 | Cost: 0.3292\n",
      "Epoch: 171/1000 | Batch 020/052 | Cost: 0.2775\n",
      "Epoch: 171/1000 | Batch 030/052 | Cost: 0.0628\n",
      "Epoch: 171/1000 | Batch 040/052 | Cost: 0.2325\n",
      "Epoch: 171/1000 | Batch 050/052 | Cost: 0.1691\n",
      "Epoch: 171/1000 training accuracy: 88.66%\n",
      "Epoch: 172/1000 | Batch 000/052 | Cost: 0.2694\n",
      "Epoch: 172/1000 | Batch 010/052 | Cost: 0.2763\n",
      "Epoch: 172/1000 | Batch 020/052 | Cost: 0.3070\n",
      "Epoch: 172/1000 | Batch 030/052 | Cost: 0.2844\n",
      "Epoch: 172/1000 | Batch 040/052 | Cost: 0.3246\n",
      "Epoch: 172/1000 | Batch 050/052 | Cost: 0.4468\n",
      "Epoch: 172/1000 training accuracy: 86.98%\n",
      "Epoch: 173/1000 | Batch 000/052 | Cost: 0.2018\n",
      "Epoch: 173/1000 | Batch 010/052 | Cost: 0.6699\n",
      "Epoch: 173/1000 | Batch 020/052 | Cost: 0.2327\n",
      "Epoch: 173/1000 | Batch 030/052 | Cost: 0.1680\n",
      "Epoch: 173/1000 | Batch 040/052 | Cost: 0.3497\n",
      "Epoch: 173/1000 | Batch 050/052 | Cost: 0.4327\n",
      "Epoch: 173/1000 training accuracy: 87.99%\n",
      "Epoch: 174/1000 | Batch 000/052 | Cost: 0.1774\n",
      "Epoch: 174/1000 | Batch 010/052 | Cost: 0.7045\n",
      "Epoch: 174/1000 | Batch 020/052 | Cost: 0.1045\n",
      "Epoch: 174/1000 | Batch 030/052 | Cost: 0.2989\n",
      "Epoch: 174/1000 | Batch 040/052 | Cost: 0.3330\n",
      "Epoch: 174/1000 | Batch 050/052 | Cost: 0.3371\n",
      "Epoch: 174/1000 training accuracy: 87.99%\n",
      "Epoch: 175/1000 | Batch 000/052 | Cost: 0.2866\n",
      "Epoch: 175/1000 | Batch 010/052 | Cost: 0.3621\n",
      "Epoch: 175/1000 | Batch 020/052 | Cost: 0.2134\n",
      "Epoch: 175/1000 | Batch 030/052 | Cost: 0.3064\n",
      "Epoch: 175/1000 | Batch 040/052 | Cost: 0.2567\n",
      "Epoch: 175/1000 | Batch 050/052 | Cost: 0.2959\n",
      "Epoch: 175/1000 training accuracy: 87.77%\n",
      "Epoch: 176/1000 | Batch 000/052 | Cost: 0.2185\n",
      "Epoch: 176/1000 | Batch 010/052 | Cost: 0.1922\n",
      "Epoch: 176/1000 | Batch 020/052 | Cost: 0.3515\n",
      "Epoch: 176/1000 | Batch 030/052 | Cost: 0.5529\n",
      "Epoch: 176/1000 | Batch 040/052 | Cost: 0.3275\n",
      "Epoch: 176/1000 | Batch 050/052 | Cost: 0.3693\n",
      "Epoch: 176/1000 training accuracy: 87.65%\n",
      "Epoch: 177/1000 | Batch 000/052 | Cost: 0.3550\n",
      "Epoch: 177/1000 | Batch 010/052 | Cost: 0.1761\n",
      "Epoch: 177/1000 | Batch 020/052 | Cost: 0.3079\n",
      "Epoch: 177/1000 | Batch 030/052 | Cost: 0.2522\n",
      "Epoch: 177/1000 | Batch 040/052 | Cost: 0.5186\n",
      "Epoch: 177/1000 | Batch 050/052 | Cost: 0.3674\n",
      "Epoch: 177/1000 training accuracy: 86.53%\n",
      "Epoch: 178/1000 | Batch 000/052 | Cost: 0.1982\n",
      "Epoch: 178/1000 | Batch 010/052 | Cost: 0.2029\n",
      "Epoch: 178/1000 | Batch 020/052 | Cost: 0.3087\n",
      "Epoch: 178/1000 | Batch 030/052 | Cost: 0.1450\n",
      "Epoch: 178/1000 | Batch 040/052 | Cost: 0.3155\n",
      "Epoch: 178/1000 | Batch 050/052 | Cost: 0.4871\n",
      "Epoch: 178/1000 training accuracy: 87.88%\n",
      "Epoch: 179/1000 | Batch 000/052 | Cost: 0.4079\n",
      "Epoch: 179/1000 | Batch 010/052 | Cost: 0.2213\n",
      "Epoch: 179/1000 | Batch 020/052 | Cost: 0.1664\n",
      "Epoch: 179/1000 | Batch 030/052 | Cost: 0.1053\n",
      "Epoch: 179/1000 | Batch 040/052 | Cost: 0.3239\n",
      "Epoch: 179/1000 | Batch 050/052 | Cost: 0.2270\n",
      "Epoch: 179/1000 training accuracy: 87.54%\n",
      "Epoch: 180/1000 | Batch 000/052 | Cost: 0.4143\n",
      "Epoch: 180/1000 | Batch 010/052 | Cost: 0.1298\n",
      "Epoch: 180/1000 | Batch 020/052 | Cost: 0.1298\n",
      "Epoch: 180/1000 | Batch 030/052 | Cost: 0.0592\n",
      "Epoch: 180/1000 | Batch 040/052 | Cost: 0.2306\n",
      "Epoch: 180/1000 | Batch 050/052 | Cost: 0.1747\n",
      "Epoch: 180/1000 training accuracy: 87.43%\n",
      "Epoch: 181/1000 | Batch 000/052 | Cost: 0.3781\n",
      "Epoch: 181/1000 | Batch 010/052 | Cost: 0.3917\n",
      "Epoch: 181/1000 | Batch 020/052 | Cost: 0.4593\n",
      "Epoch: 181/1000 | Batch 030/052 | Cost: 0.1218\n",
      "Epoch: 181/1000 | Batch 040/052 | Cost: 0.3195\n",
      "Epoch: 181/1000 | Batch 050/052 | Cost: 0.2921\n",
      "Epoch: 181/1000 training accuracy: 87.77%\n",
      "Epoch: 182/1000 | Batch 000/052 | Cost: 0.1798\n",
      "Epoch: 182/1000 | Batch 010/052 | Cost: 0.2396\n",
      "Epoch: 182/1000 | Batch 020/052 | Cost: 0.3529\n",
      "Epoch: 182/1000 | Batch 030/052 | Cost: 0.2743\n",
      "Epoch: 182/1000 | Batch 040/052 | Cost: 0.2179\n",
      "Epoch: 182/1000 | Batch 050/052 | Cost: 0.3478\n",
      "Epoch: 182/1000 training accuracy: 86.98%\n",
      "Epoch: 183/1000 | Batch 000/052 | Cost: 0.1834\n",
      "Epoch: 183/1000 | Batch 010/052 | Cost: 0.3918\n",
      "Epoch: 183/1000 | Batch 020/052 | Cost: 0.3605\n",
      "Epoch: 183/1000 | Batch 030/052 | Cost: 0.6202\n",
      "Epoch: 183/1000 | Batch 040/052 | Cost: 0.4627\n",
      "Epoch: 183/1000 | Batch 050/052 | Cost: 0.2828\n",
      "Epoch: 183/1000 training accuracy: 87.54%\n",
      "Epoch: 184/1000 | Batch 000/052 | Cost: 0.3485\n",
      "Epoch: 184/1000 | Batch 010/052 | Cost: 0.4247\n",
      "Epoch: 184/1000 | Batch 020/052 | Cost: 0.3039\n",
      "Epoch: 184/1000 | Batch 030/052 | Cost: 0.3409\n",
      "Epoch: 184/1000 | Batch 040/052 | Cost: 0.2461\n",
      "Epoch: 184/1000 | Batch 050/052 | Cost: 0.1759\n",
      "Epoch: 184/1000 training accuracy: 87.99%\n",
      "Epoch: 185/1000 | Batch 000/052 | Cost: 0.1240\n",
      "Epoch: 185/1000 | Batch 010/052 | Cost: 0.1411\n",
      "Epoch: 185/1000 | Batch 020/052 | Cost: 0.2325\n",
      "Epoch: 185/1000 | Batch 030/052 | Cost: 0.2380\n",
      "Epoch: 185/1000 | Batch 040/052 | Cost: 0.2509\n",
      "Epoch: 185/1000 | Batch 050/052 | Cost: 0.3205\n",
      "Epoch: 185/1000 training accuracy: 87.88%\n",
      "Epoch: 186/1000 | Batch 000/052 | Cost: 0.4416\n",
      "Epoch: 186/1000 | Batch 010/052 | Cost: 0.3158\n",
      "Epoch: 186/1000 | Batch 020/052 | Cost: 0.2837\n",
      "Epoch: 186/1000 | Batch 030/052 | Cost: 0.1508\n",
      "Epoch: 186/1000 | Batch 040/052 | Cost: 0.2227\n",
      "Epoch: 186/1000 | Batch 050/052 | Cost: 0.2577\n",
      "Epoch: 186/1000 training accuracy: 88.33%\n",
      "Epoch: 187/1000 | Batch 000/052 | Cost: 0.2167\n",
      "Epoch: 187/1000 | Batch 010/052 | Cost: 0.3094\n",
      "Epoch: 187/1000 | Batch 020/052 | Cost: 0.1120\n",
      "Epoch: 187/1000 | Batch 030/052 | Cost: 0.1464\n",
      "Epoch: 187/1000 | Batch 040/052 | Cost: 0.2282\n",
      "Epoch: 187/1000 | Batch 050/052 | Cost: 0.3394\n",
      "Epoch: 187/1000 training accuracy: 87.88%\n",
      "Epoch: 188/1000 | Batch 000/052 | Cost: 0.1460\n",
      "Epoch: 188/1000 | Batch 010/052 | Cost: 0.0943\n",
      "Epoch: 188/1000 | Batch 020/052 | Cost: 0.1401\n",
      "Epoch: 188/1000 | Batch 030/052 | Cost: 0.2491\n",
      "Epoch: 188/1000 | Batch 040/052 | Cost: 0.1248\n",
      "Epoch: 188/1000 | Batch 050/052 | Cost: 0.6165\n",
      "Epoch: 188/1000 training accuracy: 87.54%\n",
      "Epoch: 189/1000 | Batch 000/052 | Cost: 0.3724\n",
      "Epoch: 189/1000 | Batch 010/052 | Cost: 0.1154\n",
      "Epoch: 189/1000 | Batch 020/052 | Cost: 0.4223\n",
      "Epoch: 189/1000 | Batch 030/052 | Cost: 0.2905\n",
      "Epoch: 189/1000 | Batch 040/052 | Cost: 0.3618\n",
      "Epoch: 189/1000 | Batch 050/052 | Cost: 0.1400\n",
      "Epoch: 189/1000 training accuracy: 88.22%\n",
      "Epoch: 190/1000 | Batch 000/052 | Cost: 0.0917\n",
      "Epoch: 190/1000 | Batch 010/052 | Cost: 0.4034\n",
      "Epoch: 190/1000 | Batch 020/052 | Cost: 0.0899\n",
      "Epoch: 190/1000 | Batch 030/052 | Cost: 0.2882\n",
      "Epoch: 190/1000 | Batch 040/052 | Cost: 0.3676\n",
      "Epoch: 190/1000 | Batch 050/052 | Cost: 0.2940\n",
      "Epoch: 190/1000 training accuracy: 88.10%\n",
      "Epoch: 191/1000 | Batch 000/052 | Cost: 0.2727\n",
      "Epoch: 191/1000 | Batch 010/052 | Cost: 0.3835\n",
      "Epoch: 191/1000 | Batch 020/052 | Cost: 0.3203\n",
      "Epoch: 191/1000 | Batch 030/052 | Cost: 0.2951\n",
      "Epoch: 191/1000 | Batch 040/052 | Cost: 0.3690\n",
      "Epoch: 191/1000 | Batch 050/052 | Cost: 0.1577\n",
      "Epoch: 191/1000 training accuracy: 88.33%\n",
      "Epoch: 192/1000 | Batch 000/052 | Cost: 0.2479\n",
      "Epoch: 192/1000 | Batch 010/052 | Cost: 0.1729\n",
      "Epoch: 192/1000 | Batch 020/052 | Cost: 0.4799\n",
      "Epoch: 192/1000 | Batch 030/052 | Cost: 0.4139\n",
      "Epoch: 192/1000 | Batch 040/052 | Cost: 0.2821\n",
      "Epoch: 192/1000 | Batch 050/052 | Cost: 0.4186\n",
      "Epoch: 192/1000 training accuracy: 87.54%\n",
      "Epoch: 193/1000 | Batch 000/052 | Cost: 0.2303\n",
      "Epoch: 193/1000 | Batch 010/052 | Cost: 0.1206\n",
      "Epoch: 193/1000 | Batch 020/052 | Cost: 0.2225\n",
      "Epoch: 193/1000 | Batch 030/052 | Cost: 0.4296\n",
      "Epoch: 193/1000 | Batch 040/052 | Cost: 0.4706\n",
      "Epoch: 193/1000 | Batch 050/052 | Cost: 0.2048\n",
      "Epoch: 193/1000 training accuracy: 88.10%\n",
      "Epoch: 194/1000 | Batch 000/052 | Cost: 0.2787\n",
      "Epoch: 194/1000 | Batch 010/052 | Cost: 0.4895\n",
      "Epoch: 194/1000 | Batch 020/052 | Cost: 0.3154\n",
      "Epoch: 194/1000 | Batch 030/052 | Cost: 0.2996\n",
      "Epoch: 194/1000 | Batch 040/052 | Cost: 0.2045\n",
      "Epoch: 194/1000 | Batch 050/052 | Cost: 0.1313\n",
      "Epoch: 194/1000 training accuracy: 88.44%\n",
      "Epoch: 195/1000 | Batch 000/052 | Cost: 0.6113\n",
      "Epoch: 195/1000 | Batch 010/052 | Cost: 0.3986\n",
      "Epoch: 195/1000 | Batch 020/052 | Cost: 0.2072\n",
      "Epoch: 195/1000 | Batch 030/052 | Cost: 0.3495\n",
      "Epoch: 195/1000 | Batch 040/052 | Cost: 0.3450\n",
      "Epoch: 195/1000 | Batch 050/052 | Cost: 0.2430\n",
      "Epoch: 195/1000 training accuracy: 87.09%\n",
      "Epoch: 196/1000 | Batch 000/052 | Cost: 0.1037\n",
      "Epoch: 196/1000 | Batch 010/052 | Cost: 0.5112\n",
      "Epoch: 196/1000 | Batch 020/052 | Cost: 0.3674\n",
      "Epoch: 196/1000 | Batch 030/052 | Cost: 0.0905\n",
      "Epoch: 196/1000 | Batch 040/052 | Cost: 0.2952\n",
      "Epoch: 196/1000 | Batch 050/052 | Cost: 0.4434\n",
      "Epoch: 196/1000 training accuracy: 87.43%\n",
      "Epoch: 197/1000 | Batch 000/052 | Cost: 0.2878\n",
      "Epoch: 197/1000 | Batch 010/052 | Cost: 0.1812\n",
      "Epoch: 197/1000 | Batch 020/052 | Cost: 0.3320\n",
      "Epoch: 197/1000 | Batch 030/052 | Cost: 0.4977\n",
      "Epoch: 197/1000 | Batch 040/052 | Cost: 0.3532\n",
      "Epoch: 197/1000 | Batch 050/052 | Cost: 0.4530\n",
      "Epoch: 197/1000 training accuracy: 86.87%\n",
      "Epoch: 198/1000 | Batch 000/052 | Cost: 0.3654\n",
      "Epoch: 198/1000 | Batch 010/052 | Cost: 0.3042\n",
      "Epoch: 198/1000 | Batch 020/052 | Cost: 0.2350\n",
      "Epoch: 198/1000 | Batch 030/052 | Cost: 0.5362\n",
      "Epoch: 198/1000 | Batch 040/052 | Cost: 0.2658\n",
      "Epoch: 198/1000 | Batch 050/052 | Cost: 0.1559\n",
      "Epoch: 198/1000 training accuracy: 87.32%\n",
      "Epoch: 199/1000 | Batch 000/052 | Cost: 0.2401\n",
      "Epoch: 199/1000 | Batch 010/052 | Cost: 0.0865\n",
      "Epoch: 199/1000 | Batch 020/052 | Cost: 0.3377\n",
      "Epoch: 199/1000 | Batch 030/052 | Cost: 0.0933\n",
      "Epoch: 199/1000 | Batch 040/052 | Cost: 0.3114\n",
      "Epoch: 199/1000 | Batch 050/052 | Cost: 0.3884\n",
      "Epoch: 199/1000 training accuracy: 87.77%\n",
      "Epoch: 200/1000 | Batch 000/052 | Cost: 0.5120\n",
      "Epoch: 200/1000 | Batch 010/052 | Cost: 0.3692\n",
      "Epoch: 200/1000 | Batch 020/052 | Cost: 0.2773\n",
      "Epoch: 200/1000 | Batch 030/052 | Cost: 0.2771\n",
      "Epoch: 200/1000 | Batch 040/052 | Cost: 0.1347\n",
      "Epoch: 200/1000 | Batch 050/052 | Cost: 0.1751\n",
      "Epoch: 200/1000 training accuracy: 88.33%\n",
      "Epoch: 201/1000 | Batch 000/052 | Cost: 0.2003\n",
      "Epoch: 201/1000 | Batch 010/052 | Cost: 0.3002\n",
      "Epoch: 201/1000 | Batch 020/052 | Cost: 0.1472\n",
      "Epoch: 201/1000 | Batch 030/052 | Cost: 0.2399\n",
      "Epoch: 201/1000 | Batch 040/052 | Cost: 0.2000\n",
      "Epoch: 201/1000 | Batch 050/052 | Cost: 0.4084\n",
      "Epoch: 201/1000 training accuracy: 87.88%\n",
      "Epoch: 202/1000 | Batch 000/052 | Cost: 0.1094\n",
      "Epoch: 202/1000 | Batch 010/052 | Cost: 0.2072\n",
      "Epoch: 202/1000 | Batch 020/052 | Cost: 0.4923\n",
      "Epoch: 202/1000 | Batch 030/052 | Cost: 0.3718\n",
      "Epoch: 202/1000 | Batch 040/052 | Cost: 0.3121\n",
      "Epoch: 202/1000 | Batch 050/052 | Cost: 0.2831\n",
      "Epoch: 202/1000 training accuracy: 88.22%\n",
      "Epoch: 203/1000 | Batch 000/052 | Cost: 0.3414\n",
      "Epoch: 203/1000 | Batch 010/052 | Cost: 0.2861\n",
      "Epoch: 203/1000 | Batch 020/052 | Cost: 0.2894\n",
      "Epoch: 203/1000 | Batch 030/052 | Cost: 0.5217\n",
      "Epoch: 203/1000 | Batch 040/052 | Cost: 0.7222\n",
      "Epoch: 203/1000 | Batch 050/052 | Cost: 0.1000\n",
      "Epoch: 203/1000 training accuracy: 88.10%\n",
      "Epoch: 204/1000 | Batch 000/052 | Cost: 0.1503\n",
      "Epoch: 204/1000 | Batch 010/052 | Cost: 0.2454\n",
      "Epoch: 204/1000 | Batch 020/052 | Cost: 0.1251\n",
      "Epoch: 204/1000 | Batch 030/052 | Cost: 0.3503\n",
      "Epoch: 204/1000 | Batch 040/052 | Cost: 0.4394\n",
      "Epoch: 204/1000 | Batch 050/052 | Cost: 0.1554\n",
      "Epoch: 204/1000 training accuracy: 87.77%\n",
      "Epoch: 205/1000 | Batch 000/052 | Cost: 0.3045\n",
      "Epoch: 205/1000 | Batch 010/052 | Cost: 0.1584\n",
      "Epoch: 205/1000 | Batch 020/052 | Cost: 0.3521\n",
      "Epoch: 205/1000 | Batch 030/052 | Cost: 0.2487\n",
      "Epoch: 205/1000 | Batch 040/052 | Cost: 0.2460\n",
      "Epoch: 205/1000 | Batch 050/052 | Cost: 0.4984\n",
      "Epoch: 205/1000 training accuracy: 88.66%\n",
      "Epoch: 206/1000 | Batch 000/052 | Cost: 0.1063\n",
      "Epoch: 206/1000 | Batch 010/052 | Cost: 0.1177\n",
      "Epoch: 206/1000 | Batch 020/052 | Cost: 0.1884\n",
      "Epoch: 206/1000 | Batch 030/052 | Cost: 0.3334\n",
      "Epoch: 206/1000 | Batch 040/052 | Cost: 0.3324\n",
      "Epoch: 206/1000 | Batch 050/052 | Cost: 0.3228\n",
      "Epoch: 206/1000 training accuracy: 88.22%\n",
      "Epoch: 207/1000 | Batch 000/052 | Cost: 0.1631\n",
      "Epoch: 207/1000 | Batch 010/052 | Cost: 0.1761\n",
      "Epoch: 207/1000 | Batch 020/052 | Cost: 0.4473\n",
      "Epoch: 207/1000 | Batch 030/052 | Cost: 0.6128\n",
      "Epoch: 207/1000 | Batch 040/052 | Cost: 0.4880\n",
      "Epoch: 207/1000 | Batch 050/052 | Cost: 0.4417\n",
      "Epoch: 207/1000 training accuracy: 88.44%\n",
      "Epoch: 208/1000 | Batch 000/052 | Cost: 0.1530\n",
      "Epoch: 208/1000 | Batch 010/052 | Cost: 0.4587\n",
      "Epoch: 208/1000 | Batch 020/052 | Cost: 0.1434\n",
      "Epoch: 208/1000 | Batch 030/052 | Cost: 0.4735\n",
      "Epoch: 208/1000 | Batch 040/052 | Cost: 0.6180\n",
      "Epoch: 208/1000 | Batch 050/052 | Cost: 0.3121\n",
      "Epoch: 208/1000 training accuracy: 87.65%\n",
      "Epoch: 209/1000 | Batch 000/052 | Cost: 0.3656\n",
      "Epoch: 209/1000 | Batch 010/052 | Cost: 0.2105\n",
      "Epoch: 209/1000 | Batch 020/052 | Cost: 0.2836\n",
      "Epoch: 209/1000 | Batch 030/052 | Cost: 0.1927\n",
      "Epoch: 209/1000 | Batch 040/052 | Cost: 0.2629\n",
      "Epoch: 209/1000 | Batch 050/052 | Cost: 0.3190\n",
      "Epoch: 209/1000 training accuracy: 87.54%\n",
      "Epoch: 210/1000 | Batch 000/052 | Cost: 0.1458\n",
      "Epoch: 210/1000 | Batch 010/052 | Cost: 0.2374\n",
      "Epoch: 210/1000 | Batch 020/052 | Cost: 0.7444\n",
      "Epoch: 210/1000 | Batch 030/052 | Cost: 0.2733\n",
      "Epoch: 210/1000 | Batch 040/052 | Cost: 0.3815\n",
      "Epoch: 210/1000 | Batch 050/052 | Cost: 0.2002\n",
      "Epoch: 210/1000 training accuracy: 87.65%\n",
      "Epoch: 211/1000 | Batch 000/052 | Cost: 0.2829\n",
      "Epoch: 211/1000 | Batch 010/052 | Cost: 0.2682\n",
      "Epoch: 211/1000 | Batch 020/052 | Cost: 0.1005\n",
      "Epoch: 211/1000 | Batch 030/052 | Cost: 0.2767\n",
      "Epoch: 211/1000 | Batch 040/052 | Cost: 0.5301\n",
      "Epoch: 211/1000 | Batch 050/052 | Cost: 0.3201\n",
      "Epoch: 211/1000 training accuracy: 87.88%\n",
      "Epoch: 212/1000 | Batch 000/052 | Cost: 0.5060\n",
      "Epoch: 212/1000 | Batch 010/052 | Cost: 0.1629\n",
      "Epoch: 212/1000 | Batch 020/052 | Cost: 0.2211\n",
      "Epoch: 212/1000 | Batch 030/052 | Cost: 0.1709\n",
      "Epoch: 212/1000 | Batch 040/052 | Cost: 0.2681\n",
      "Epoch: 212/1000 | Batch 050/052 | Cost: 0.4702\n",
      "Epoch: 212/1000 training accuracy: 87.65%\n",
      "Epoch: 213/1000 | Batch 000/052 | Cost: 0.0962\n",
      "Epoch: 213/1000 | Batch 010/052 | Cost: 0.4280\n",
      "Epoch: 213/1000 | Batch 020/052 | Cost: 0.4343\n",
      "Epoch: 213/1000 | Batch 030/052 | Cost: 0.5964\n",
      "Epoch: 213/1000 | Batch 040/052 | Cost: 0.4499\n",
      "Epoch: 213/1000 | Batch 050/052 | Cost: 0.1163\n",
      "Epoch: 213/1000 training accuracy: 87.65%\n",
      "Epoch: 214/1000 | Batch 000/052 | Cost: 0.2336\n",
      "Epoch: 214/1000 | Batch 010/052 | Cost: 0.3299\n",
      "Epoch: 214/1000 | Batch 020/052 | Cost: 0.5012\n",
      "Epoch: 214/1000 | Batch 030/052 | Cost: 0.3488\n",
      "Epoch: 214/1000 | Batch 040/052 | Cost: 0.4130\n",
      "Epoch: 214/1000 | Batch 050/052 | Cost: 0.6214\n",
      "Epoch: 214/1000 training accuracy: 86.98%\n",
      "Epoch: 215/1000 | Batch 000/052 | Cost: 0.1867\n",
      "Epoch: 215/1000 | Batch 010/052 | Cost: 0.2972\n",
      "Epoch: 215/1000 | Batch 020/052 | Cost: 0.1502\n",
      "Epoch: 215/1000 | Batch 030/052 | Cost: 0.5499\n",
      "Epoch: 215/1000 | Batch 040/052 | Cost: 0.3171\n",
      "Epoch: 215/1000 | Batch 050/052 | Cost: 0.2480\n",
      "Epoch: 215/1000 training accuracy: 87.65%\n",
      "Epoch: 216/1000 | Batch 000/052 | Cost: 0.1953\n",
      "Epoch: 216/1000 | Batch 010/052 | Cost: 0.4419\n",
      "Epoch: 216/1000 | Batch 020/052 | Cost: 0.1397\n",
      "Epoch: 216/1000 | Batch 030/052 | Cost: 0.3445\n",
      "Epoch: 216/1000 | Batch 040/052 | Cost: 0.1361\n",
      "Epoch: 216/1000 | Batch 050/052 | Cost: 0.1826\n",
      "Epoch: 216/1000 training accuracy: 88.44%\n",
      "Epoch: 217/1000 | Batch 000/052 | Cost: 0.2537\n",
      "Epoch: 217/1000 | Batch 010/052 | Cost: 0.1630\n",
      "Epoch: 217/1000 | Batch 020/052 | Cost: 0.3559\n",
      "Epoch: 217/1000 | Batch 030/052 | Cost: 0.3923\n",
      "Epoch: 217/1000 | Batch 040/052 | Cost: 0.0898\n",
      "Epoch: 217/1000 | Batch 050/052 | Cost: 0.1788\n",
      "Epoch: 217/1000 training accuracy: 86.64%\n",
      "Epoch: 218/1000 | Batch 000/052 | Cost: 0.2213\n",
      "Epoch: 218/1000 | Batch 010/052 | Cost: 0.4442\n",
      "Epoch: 218/1000 | Batch 020/052 | Cost: 0.5746\n",
      "Epoch: 218/1000 | Batch 030/052 | Cost: 0.2559\n",
      "Epoch: 218/1000 | Batch 040/052 | Cost: 0.3339\n",
      "Epoch: 218/1000 | Batch 050/052 | Cost: 0.3684\n",
      "Epoch: 218/1000 training accuracy: 86.64%\n",
      "Epoch: 219/1000 | Batch 000/052 | Cost: 0.1797\n",
      "Epoch: 219/1000 | Batch 010/052 | Cost: 0.3538\n",
      "Epoch: 219/1000 | Batch 020/052 | Cost: 0.6205\n",
      "Epoch: 219/1000 | Batch 030/052 | Cost: 0.4138\n",
      "Epoch: 219/1000 | Batch 040/052 | Cost: 0.2783\n",
      "Epoch: 219/1000 | Batch 050/052 | Cost: 0.3449\n",
      "Epoch: 219/1000 training accuracy: 87.32%\n",
      "Epoch: 220/1000 | Batch 000/052 | Cost: 0.2762\n",
      "Epoch: 220/1000 | Batch 010/052 | Cost: 0.2913\n",
      "Epoch: 220/1000 | Batch 020/052 | Cost: 0.2936\n",
      "Epoch: 220/1000 | Batch 030/052 | Cost: 1.2970\n",
      "Epoch: 220/1000 | Batch 040/052 | Cost: 0.4648\n",
      "Epoch: 220/1000 | Batch 050/052 | Cost: 0.2191\n",
      "Epoch: 220/1000 training accuracy: 88.10%\n",
      "Epoch: 221/1000 | Batch 000/052 | Cost: 0.4813\n",
      "Epoch: 221/1000 | Batch 010/052 | Cost: 0.2466\n",
      "Epoch: 221/1000 | Batch 020/052 | Cost: 0.2765\n",
      "Epoch: 221/1000 | Batch 030/052 | Cost: 0.4463\n",
      "Epoch: 221/1000 | Batch 040/052 | Cost: 0.1273\n",
      "Epoch: 221/1000 | Batch 050/052 | Cost: 0.3259\n",
      "Epoch: 221/1000 training accuracy: 87.65%\n",
      "Epoch: 222/1000 | Batch 000/052 | Cost: 0.5176\n",
      "Epoch: 222/1000 | Batch 010/052 | Cost: 0.4377\n",
      "Epoch: 222/1000 | Batch 020/052 | Cost: 0.1136\n",
      "Epoch: 222/1000 | Batch 030/052 | Cost: 0.3084\n",
      "Epoch: 222/1000 | Batch 040/052 | Cost: 0.5726\n",
      "Epoch: 222/1000 | Batch 050/052 | Cost: 0.3538\n",
      "Epoch: 222/1000 training accuracy: 87.54%\n",
      "Epoch: 223/1000 | Batch 000/052 | Cost: 0.2377\n",
      "Epoch: 223/1000 | Batch 010/052 | Cost: 0.4689\n",
      "Epoch: 223/1000 | Batch 020/052 | Cost: 0.2808\n",
      "Epoch: 223/1000 | Batch 030/052 | Cost: 0.2279\n",
      "Epoch: 223/1000 | Batch 040/052 | Cost: 0.3119\n",
      "Epoch: 223/1000 | Batch 050/052 | Cost: 0.3905\n",
      "Epoch: 223/1000 training accuracy: 87.09%\n",
      "Epoch: 224/1000 | Batch 000/052 | Cost: 0.3053\n",
      "Epoch: 224/1000 | Batch 010/052 | Cost: 0.2659\n",
      "Epoch: 224/1000 | Batch 020/052 | Cost: 0.2680\n",
      "Epoch: 224/1000 | Batch 030/052 | Cost: 0.5408\n",
      "Epoch: 224/1000 | Batch 040/052 | Cost: 0.1364\n",
      "Epoch: 224/1000 | Batch 050/052 | Cost: 0.3063\n",
      "Epoch: 224/1000 training accuracy: 86.76%\n",
      "Epoch: 225/1000 | Batch 000/052 | Cost: 0.3310\n",
      "Epoch: 225/1000 | Batch 010/052 | Cost: 0.8403\n",
      "Epoch: 225/1000 | Batch 020/052 | Cost: 0.0937\n",
      "Epoch: 225/1000 | Batch 030/052 | Cost: 0.1908\n",
      "Epoch: 225/1000 | Batch 040/052 | Cost: 0.1260\n",
      "Epoch: 225/1000 | Batch 050/052 | Cost: 0.1624\n",
      "Epoch: 225/1000 training accuracy: 87.77%\n",
      "Epoch: 226/1000 | Batch 000/052 | Cost: 0.2583\n",
      "Epoch: 226/1000 | Batch 010/052 | Cost: 0.5127\n",
      "Epoch: 226/1000 | Batch 020/052 | Cost: 0.2512\n",
      "Epoch: 226/1000 | Batch 030/052 | Cost: 0.2484\n",
      "Epoch: 226/1000 | Batch 040/052 | Cost: 0.2053\n",
      "Epoch: 226/1000 | Batch 050/052 | Cost: 0.7166\n",
      "Epoch: 226/1000 training accuracy: 87.21%\n",
      "Epoch: 227/1000 | Batch 000/052 | Cost: 0.2902\n",
      "Epoch: 227/1000 | Batch 010/052 | Cost: 0.1131\n",
      "Epoch: 227/1000 | Batch 020/052 | Cost: 0.2128\n",
      "Epoch: 227/1000 | Batch 030/052 | Cost: 0.1952\n",
      "Epoch: 227/1000 | Batch 040/052 | Cost: 0.3364\n",
      "Epoch: 227/1000 | Batch 050/052 | Cost: 0.2056\n",
      "Epoch: 227/1000 training accuracy: 88.10%\n",
      "Epoch: 228/1000 | Batch 000/052 | Cost: 0.4449\n",
      "Epoch: 228/1000 | Batch 010/052 | Cost: 0.1988\n",
      "Epoch: 228/1000 | Batch 020/052 | Cost: 0.1544\n",
      "Epoch: 228/1000 | Batch 030/052 | Cost: 0.5883\n",
      "Epoch: 228/1000 | Batch 040/052 | Cost: 0.1066\n",
      "Epoch: 228/1000 | Batch 050/052 | Cost: 0.5529\n",
      "Epoch: 228/1000 training accuracy: 86.98%\n",
      "Epoch: 229/1000 | Batch 000/052 | Cost: 0.0996\n",
      "Epoch: 229/1000 | Batch 010/052 | Cost: 0.0981\n",
      "Epoch: 229/1000 | Batch 020/052 | Cost: 0.4258\n",
      "Epoch: 229/1000 | Batch 030/052 | Cost: 0.2718\n",
      "Epoch: 229/1000 | Batch 040/052 | Cost: 0.2439\n",
      "Epoch: 229/1000 | Batch 050/052 | Cost: 0.4024\n",
      "Epoch: 229/1000 training accuracy: 86.98%\n",
      "Epoch: 230/1000 | Batch 000/052 | Cost: 0.3306\n",
      "Epoch: 230/1000 | Batch 010/052 | Cost: 0.0854\n",
      "Epoch: 230/1000 | Batch 020/052 | Cost: 0.3161\n",
      "Epoch: 230/1000 | Batch 030/052 | Cost: 0.3307\n",
      "Epoch: 230/1000 | Batch 040/052 | Cost: 0.2080\n",
      "Epoch: 230/1000 | Batch 050/052 | Cost: 0.3439\n",
      "Epoch: 230/1000 training accuracy: 87.32%\n",
      "Epoch: 231/1000 | Batch 000/052 | Cost: 0.2771\n",
      "Epoch: 231/1000 | Batch 010/052 | Cost: 0.2230\n",
      "Epoch: 231/1000 | Batch 020/052 | Cost: 0.3084\n",
      "Epoch: 231/1000 | Batch 030/052 | Cost: 0.3068\n",
      "Epoch: 231/1000 | Batch 040/052 | Cost: 0.1608\n",
      "Epoch: 231/1000 | Batch 050/052 | Cost: 0.5044\n",
      "Epoch: 231/1000 training accuracy: 87.99%\n",
      "Epoch: 232/1000 | Batch 000/052 | Cost: 0.1501\n",
      "Epoch: 232/1000 | Batch 010/052 | Cost: 0.3159\n",
      "Epoch: 232/1000 | Batch 020/052 | Cost: 0.3042\n",
      "Epoch: 232/1000 | Batch 030/052 | Cost: 0.5076\n",
      "Epoch: 232/1000 | Batch 040/052 | Cost: 0.2649\n",
      "Epoch: 232/1000 | Batch 050/052 | Cost: 0.3841\n",
      "Epoch: 232/1000 training accuracy: 86.76%\n",
      "Epoch: 233/1000 | Batch 000/052 | Cost: 0.2546\n",
      "Epoch: 233/1000 | Batch 010/052 | Cost: 0.4547\n",
      "Epoch: 233/1000 | Batch 020/052 | Cost: 0.4436\n",
      "Epoch: 233/1000 | Batch 030/052 | Cost: 0.2874\n",
      "Epoch: 233/1000 | Batch 040/052 | Cost: 0.1457\n",
      "Epoch: 233/1000 | Batch 050/052 | Cost: 0.2430\n",
      "Epoch: 233/1000 training accuracy: 87.43%\n",
      "Epoch: 234/1000 | Batch 000/052 | Cost: 0.1679\n",
      "Epoch: 234/1000 | Batch 010/052 | Cost: 0.2412\n",
      "Epoch: 234/1000 | Batch 020/052 | Cost: 0.2323\n",
      "Epoch: 234/1000 | Batch 030/052 | Cost: 0.3301\n",
      "Epoch: 234/1000 | Batch 040/052 | Cost: 0.1823\n",
      "Epoch: 234/1000 | Batch 050/052 | Cost: 0.3138\n",
      "Epoch: 234/1000 training accuracy: 87.32%\n",
      "Epoch: 235/1000 | Batch 000/052 | Cost: 0.4588\n",
      "Epoch: 235/1000 | Batch 010/052 | Cost: 0.3228\n",
      "Epoch: 235/1000 | Batch 020/052 | Cost: 0.3415\n",
      "Epoch: 235/1000 | Batch 030/052 | Cost: 0.1082\n",
      "Epoch: 235/1000 | Batch 040/052 | Cost: 0.2855\n",
      "Epoch: 235/1000 | Batch 050/052 | Cost: 0.5561\n",
      "Epoch: 235/1000 training accuracy: 88.10%\n",
      "Epoch: 236/1000 | Batch 000/052 | Cost: 0.2805\n",
      "Epoch: 236/1000 | Batch 010/052 | Cost: 0.2968\n",
      "Epoch: 236/1000 | Batch 020/052 | Cost: 0.4272\n",
      "Epoch: 236/1000 | Batch 030/052 | Cost: 0.3585\n",
      "Epoch: 236/1000 | Batch 040/052 | Cost: 0.3996\n",
      "Epoch: 236/1000 | Batch 050/052 | Cost: 0.5505\n",
      "Epoch: 236/1000 training accuracy: 88.66%\n",
      "Epoch: 237/1000 | Batch 000/052 | Cost: 0.5781\n",
      "Epoch: 237/1000 | Batch 010/052 | Cost: 0.1482\n",
      "Epoch: 237/1000 | Batch 020/052 | Cost: 0.3199\n",
      "Epoch: 237/1000 | Batch 030/052 | Cost: 0.4448\n",
      "Epoch: 237/1000 | Batch 040/052 | Cost: 0.3097\n",
      "Epoch: 237/1000 | Batch 050/052 | Cost: 0.4917\n",
      "Epoch: 237/1000 training accuracy: 87.54%\n",
      "Epoch: 238/1000 | Batch 000/052 | Cost: 0.1883\n",
      "Epoch: 238/1000 | Batch 010/052 | Cost: 0.3003\n",
      "Epoch: 238/1000 | Batch 020/052 | Cost: 0.1968\n",
      "Epoch: 238/1000 | Batch 030/052 | Cost: 0.4390\n",
      "Epoch: 238/1000 | Batch 040/052 | Cost: 0.3413\n",
      "Epoch: 238/1000 | Batch 050/052 | Cost: 0.2287\n",
      "Epoch: 238/1000 training accuracy: 86.20%\n",
      "Epoch: 239/1000 | Batch 000/052 | Cost: 0.2138\n",
      "Epoch: 239/1000 | Batch 010/052 | Cost: 0.4375\n",
      "Epoch: 239/1000 | Batch 020/052 | Cost: 0.1787\n",
      "Epoch: 239/1000 | Batch 030/052 | Cost: 0.1967\n",
      "Epoch: 239/1000 | Batch 040/052 | Cost: 0.3690\n",
      "Epoch: 239/1000 | Batch 050/052 | Cost: 0.6930\n",
      "Epoch: 239/1000 training accuracy: 87.21%\n",
      "Epoch: 240/1000 | Batch 000/052 | Cost: 0.3792\n",
      "Epoch: 240/1000 | Batch 010/052 | Cost: 0.1977\n",
      "Epoch: 240/1000 | Batch 020/052 | Cost: 0.1291\n",
      "Epoch: 240/1000 | Batch 030/052 | Cost: 0.1490\n",
      "Epoch: 240/1000 | Batch 040/052 | Cost: 0.4448\n",
      "Epoch: 240/1000 | Batch 050/052 | Cost: 0.3763\n",
      "Epoch: 240/1000 training accuracy: 86.87%\n",
      "Epoch: 241/1000 | Batch 000/052 | Cost: 0.3236\n",
      "Epoch: 241/1000 | Batch 010/052 | Cost: 0.1481\n",
      "Epoch: 241/1000 | Batch 020/052 | Cost: 0.2688\n",
      "Epoch: 241/1000 | Batch 030/052 | Cost: 0.1440\n",
      "Epoch: 241/1000 | Batch 040/052 | Cost: 0.3001\n",
      "Epoch: 241/1000 | Batch 050/052 | Cost: 0.1114\n",
      "Epoch: 241/1000 training accuracy: 87.99%\n",
      "Epoch: 242/1000 | Batch 000/052 | Cost: 0.0753\n",
      "Epoch: 242/1000 | Batch 010/052 | Cost: 0.1337\n",
      "Epoch: 242/1000 | Batch 020/052 | Cost: 0.4571\n",
      "Epoch: 242/1000 | Batch 030/052 | Cost: 0.4110\n",
      "Epoch: 242/1000 | Batch 040/052 | Cost: 0.6326\n",
      "Epoch: 242/1000 | Batch 050/052 | Cost: 0.3043\n",
      "Epoch: 242/1000 training accuracy: 86.87%\n",
      "Epoch: 243/1000 | Batch 000/052 | Cost: 0.0898\n",
      "Epoch: 243/1000 | Batch 010/052 | Cost: 0.2190\n",
      "Epoch: 243/1000 | Batch 020/052 | Cost: 0.2220\n",
      "Epoch: 243/1000 | Batch 030/052 | Cost: 0.2065\n",
      "Epoch: 243/1000 | Batch 040/052 | Cost: 0.4271\n",
      "Epoch: 243/1000 | Batch 050/052 | Cost: 0.1599\n",
      "Epoch: 243/1000 training accuracy: 87.99%\n",
      "Epoch: 244/1000 | Batch 000/052 | Cost: 0.3241\n",
      "Epoch: 244/1000 | Batch 010/052 | Cost: 0.5301\n",
      "Epoch: 244/1000 | Batch 020/052 | Cost: 0.2776\n",
      "Epoch: 244/1000 | Batch 030/052 | Cost: 0.1625\n",
      "Epoch: 244/1000 | Batch 040/052 | Cost: 0.5471\n",
      "Epoch: 244/1000 | Batch 050/052 | Cost: 0.5374\n",
      "Epoch: 244/1000 training accuracy: 87.65%\n",
      "Epoch: 245/1000 | Batch 000/052 | Cost: 0.1940\n",
      "Epoch: 245/1000 | Batch 010/052 | Cost: 0.2092\n",
      "Epoch: 245/1000 | Batch 020/052 | Cost: 0.2373\n",
      "Epoch: 245/1000 | Batch 030/052 | Cost: 0.3530\n",
      "Epoch: 245/1000 | Batch 040/052 | Cost: 0.4017\n",
      "Epoch: 245/1000 | Batch 050/052 | Cost: 0.3288\n",
      "Epoch: 245/1000 training accuracy: 87.54%\n",
      "Epoch: 246/1000 | Batch 000/052 | Cost: 0.4800\n",
      "Epoch: 246/1000 | Batch 010/052 | Cost: 0.2261\n",
      "Epoch: 246/1000 | Batch 020/052 | Cost: 0.3539\n",
      "Epoch: 246/1000 | Batch 030/052 | Cost: 0.1766\n",
      "Epoch: 246/1000 | Batch 040/052 | Cost: 0.0934\n",
      "Epoch: 246/1000 | Batch 050/052 | Cost: 0.7346\n",
      "Epoch: 246/1000 training accuracy: 88.22%\n",
      "Epoch: 247/1000 | Batch 000/052 | Cost: 0.1900\n",
      "Epoch: 247/1000 | Batch 010/052 | Cost: 0.1714\n",
      "Epoch: 247/1000 | Batch 020/052 | Cost: 0.2651\n",
      "Epoch: 247/1000 | Batch 030/052 | Cost: 0.1594\n",
      "Epoch: 247/1000 | Batch 040/052 | Cost: 0.4180\n",
      "Epoch: 247/1000 | Batch 050/052 | Cost: 0.3360\n",
      "Epoch: 247/1000 training accuracy: 87.32%\n",
      "Epoch: 248/1000 | Batch 000/052 | Cost: 0.1824\n",
      "Epoch: 248/1000 | Batch 010/052 | Cost: 0.1704\n",
      "Epoch: 248/1000 | Batch 020/052 | Cost: 0.1243\n",
      "Epoch: 248/1000 | Batch 030/052 | Cost: 0.1239\n",
      "Epoch: 248/1000 | Batch 040/052 | Cost: 0.2594\n",
      "Epoch: 248/1000 | Batch 050/052 | Cost: 0.3111\n",
      "Epoch: 248/1000 training accuracy: 87.77%\n",
      "Epoch: 249/1000 | Batch 000/052 | Cost: 0.3072\n",
      "Epoch: 249/1000 | Batch 010/052 | Cost: 0.4068\n",
      "Epoch: 249/1000 | Batch 020/052 | Cost: 0.4667\n",
      "Epoch: 249/1000 | Batch 030/052 | Cost: 0.1997\n",
      "Epoch: 249/1000 | Batch 040/052 | Cost: 0.1514\n",
      "Epoch: 249/1000 | Batch 050/052 | Cost: 0.1640\n",
      "Epoch: 249/1000 training accuracy: 87.77%\n",
      "Epoch: 250/1000 | Batch 000/052 | Cost: 0.5020\n",
      "Epoch: 250/1000 | Batch 010/052 | Cost: 0.2295\n",
      "Epoch: 250/1000 | Batch 020/052 | Cost: 0.2478\n",
      "Epoch: 250/1000 | Batch 030/052 | Cost: 0.1283\n",
      "Epoch: 250/1000 | Batch 040/052 | Cost: 0.2775\n",
      "Epoch: 250/1000 | Batch 050/052 | Cost: 0.1079\n",
      "Epoch: 250/1000 training accuracy: 88.33%\n",
      "Epoch: 251/1000 | Batch 000/052 | Cost: 0.4887\n",
      "Epoch: 251/1000 | Batch 010/052 | Cost: 0.2560\n",
      "Epoch: 251/1000 | Batch 020/052 | Cost: 0.1606\n",
      "Epoch: 251/1000 | Batch 030/052 | Cost: 0.3595\n",
      "Epoch: 251/1000 | Batch 040/052 | Cost: 0.1556\n",
      "Epoch: 251/1000 | Batch 050/052 | Cost: 0.1842\n",
      "Epoch: 251/1000 training accuracy: 87.21%\n",
      "Epoch: 252/1000 | Batch 000/052 | Cost: 0.2011\n",
      "Epoch: 252/1000 | Batch 010/052 | Cost: 0.2255\n",
      "Epoch: 252/1000 | Batch 020/052 | Cost: 0.2929\n",
      "Epoch: 252/1000 | Batch 030/052 | Cost: 0.4615\n",
      "Epoch: 252/1000 | Batch 040/052 | Cost: 0.1376\n",
      "Epoch: 252/1000 | Batch 050/052 | Cost: 0.7088\n",
      "Epoch: 252/1000 training accuracy: 87.65%\n",
      "Epoch: 253/1000 | Batch 000/052 | Cost: 0.4245\n",
      "Epoch: 253/1000 | Batch 010/052 | Cost: 0.2054\n",
      "Epoch: 253/1000 | Batch 020/052 | Cost: 0.4506\n",
      "Epoch: 253/1000 | Batch 030/052 | Cost: 0.7446\n",
      "Epoch: 253/1000 | Batch 040/052 | Cost: 0.1641\n",
      "Epoch: 253/1000 | Batch 050/052 | Cost: 0.2040\n",
      "Epoch: 253/1000 training accuracy: 86.87%\n",
      "Epoch: 254/1000 | Batch 000/052 | Cost: 0.1216\n",
      "Epoch: 254/1000 | Batch 010/052 | Cost: 0.1320\n",
      "Epoch: 254/1000 | Batch 020/052 | Cost: 0.1264\n",
      "Epoch: 254/1000 | Batch 030/052 | Cost: 0.4547\n",
      "Epoch: 254/1000 | Batch 040/052 | Cost: 0.3001\n",
      "Epoch: 254/1000 | Batch 050/052 | Cost: 0.2570\n",
      "Epoch: 254/1000 training accuracy: 87.88%\n",
      "Epoch: 255/1000 | Batch 000/052 | Cost: 0.1537\n",
      "Epoch: 255/1000 | Batch 010/052 | Cost: 0.2168\n",
      "Epoch: 255/1000 | Batch 020/052 | Cost: 0.4494\n",
      "Epoch: 255/1000 | Batch 030/052 | Cost: 0.1328\n",
      "Epoch: 255/1000 | Batch 040/052 | Cost: 0.5492\n",
      "Epoch: 255/1000 | Batch 050/052 | Cost: 0.4160\n",
      "Epoch: 255/1000 training accuracy: 87.09%\n",
      "Epoch: 256/1000 | Batch 000/052 | Cost: 0.3116\n",
      "Epoch: 256/1000 | Batch 010/052 | Cost: 0.4713\n",
      "Epoch: 256/1000 | Batch 020/052 | Cost: 0.3539\n",
      "Epoch: 256/1000 | Batch 030/052 | Cost: 0.2615\n",
      "Epoch: 256/1000 | Batch 040/052 | Cost: 0.2563\n",
      "Epoch: 256/1000 | Batch 050/052 | Cost: 0.1150\n",
      "Epoch: 256/1000 training accuracy: 87.88%\n",
      "Epoch: 257/1000 | Batch 000/052 | Cost: 0.3256\n",
      "Epoch: 257/1000 | Batch 010/052 | Cost: 0.3634\n",
      "Epoch: 257/1000 | Batch 020/052 | Cost: 0.3022\n",
      "Epoch: 257/1000 | Batch 030/052 | Cost: 0.3231\n",
      "Epoch: 257/1000 | Batch 040/052 | Cost: 0.3320\n",
      "Epoch: 257/1000 | Batch 050/052 | Cost: 0.7854\n",
      "Epoch: 257/1000 training accuracy: 87.54%\n",
      "Epoch: 258/1000 | Batch 000/052 | Cost: 0.1819\n",
      "Epoch: 258/1000 | Batch 010/052 | Cost: 0.2892\n",
      "Epoch: 258/1000 | Batch 020/052 | Cost: 0.3068\n",
      "Epoch: 258/1000 | Batch 030/052 | Cost: 0.1879\n",
      "Epoch: 258/1000 | Batch 040/052 | Cost: 0.3413\n",
      "Epoch: 258/1000 | Batch 050/052 | Cost: 0.3490\n",
      "Epoch: 258/1000 training accuracy: 87.43%\n",
      "Epoch: 259/1000 | Batch 000/052 | Cost: 0.4225\n",
      "Epoch: 259/1000 | Batch 010/052 | Cost: 0.3085\n",
      "Epoch: 259/1000 | Batch 020/052 | Cost: 0.1980\n",
      "Epoch: 259/1000 | Batch 030/052 | Cost: 0.1417\n",
      "Epoch: 259/1000 | Batch 040/052 | Cost: 0.3192\n",
      "Epoch: 259/1000 | Batch 050/052 | Cost: 0.2547\n",
      "Epoch: 259/1000 training accuracy: 87.32%\n",
      "Epoch: 260/1000 | Batch 000/052 | Cost: 0.4780\n",
      "Epoch: 260/1000 | Batch 010/052 | Cost: 0.3305\n",
      "Epoch: 260/1000 | Batch 020/052 | Cost: 0.1295\n",
      "Epoch: 260/1000 | Batch 030/052 | Cost: 0.4932\n",
      "Epoch: 260/1000 | Batch 040/052 | Cost: 0.2084\n",
      "Epoch: 260/1000 | Batch 050/052 | Cost: 0.2956\n",
      "Epoch: 260/1000 training accuracy: 87.43%\n",
      "Epoch: 261/1000 | Batch 000/052 | Cost: 0.0989\n",
      "Epoch: 261/1000 | Batch 010/052 | Cost: 0.7197\n",
      "Epoch: 261/1000 | Batch 020/052 | Cost: 0.2527\n",
      "Epoch: 261/1000 | Batch 030/052 | Cost: 0.2660\n",
      "Epoch: 261/1000 | Batch 040/052 | Cost: 0.5376\n",
      "Epoch: 261/1000 | Batch 050/052 | Cost: 0.2885\n",
      "Epoch: 261/1000 training accuracy: 87.54%\n",
      "Epoch: 262/1000 | Batch 000/052 | Cost: 0.3761\n",
      "Epoch: 262/1000 | Batch 010/052 | Cost: 0.1640\n",
      "Epoch: 262/1000 | Batch 020/052 | Cost: 0.2531\n",
      "Epoch: 262/1000 | Batch 030/052 | Cost: 0.4717\n",
      "Epoch: 262/1000 | Batch 040/052 | Cost: 0.2598\n",
      "Epoch: 262/1000 | Batch 050/052 | Cost: 0.5076\n",
      "Epoch: 262/1000 training accuracy: 87.88%\n",
      "Epoch: 263/1000 | Batch 000/052 | Cost: 0.4260\n",
      "Epoch: 263/1000 | Batch 010/052 | Cost: 0.4462\n",
      "Epoch: 263/1000 | Batch 020/052 | Cost: 0.2179\n",
      "Epoch: 263/1000 | Batch 030/052 | Cost: 0.4142\n",
      "Epoch: 263/1000 | Batch 040/052 | Cost: 0.4268\n",
      "Epoch: 263/1000 | Batch 050/052 | Cost: 0.2371\n",
      "Epoch: 263/1000 training accuracy: 87.88%\n",
      "Epoch: 264/1000 | Batch 000/052 | Cost: 0.0920\n",
      "Epoch: 264/1000 | Batch 010/052 | Cost: 0.1660\n",
      "Epoch: 264/1000 | Batch 020/052 | Cost: 0.1915\n",
      "Epoch: 264/1000 | Batch 030/052 | Cost: 0.2342\n",
      "Epoch: 264/1000 | Batch 040/052 | Cost: 0.4163\n",
      "Epoch: 264/1000 | Batch 050/052 | Cost: 0.5660\n",
      "Epoch: 264/1000 training accuracy: 88.78%\n",
      "Epoch: 265/1000 | Batch 000/052 | Cost: 0.3013\n",
      "Epoch: 265/1000 | Batch 010/052 | Cost: 0.2849\n",
      "Epoch: 265/1000 | Batch 020/052 | Cost: 0.5344\n",
      "Epoch: 265/1000 | Batch 030/052 | Cost: 0.3342\n",
      "Epoch: 265/1000 | Batch 040/052 | Cost: 0.3421\n",
      "Epoch: 265/1000 | Batch 050/052 | Cost: 0.3297\n",
      "Epoch: 265/1000 training accuracy: 87.21%\n",
      "Epoch: 266/1000 | Batch 000/052 | Cost: 0.2946\n",
      "Epoch: 266/1000 | Batch 010/052 | Cost: 0.3926\n",
      "Epoch: 266/1000 | Batch 020/052 | Cost: 0.2452\n",
      "Epoch: 266/1000 | Batch 030/052 | Cost: 0.1288\n",
      "Epoch: 266/1000 | Batch 040/052 | Cost: 0.2475\n",
      "Epoch: 266/1000 | Batch 050/052 | Cost: 0.2125\n",
      "Epoch: 266/1000 training accuracy: 87.65%\n",
      "Epoch: 267/1000 | Batch 000/052 | Cost: 0.3206\n",
      "Epoch: 267/1000 | Batch 010/052 | Cost: 0.3539\n",
      "Epoch: 267/1000 | Batch 020/052 | Cost: 0.1543\n",
      "Epoch: 267/1000 | Batch 030/052 | Cost: 0.4765\n",
      "Epoch: 267/1000 | Batch 040/052 | Cost: 0.0964\n",
      "Epoch: 267/1000 | Batch 050/052 | Cost: 0.1520\n",
      "Epoch: 267/1000 training accuracy: 87.21%\n",
      "Epoch: 268/1000 | Batch 000/052 | Cost: 0.2542\n",
      "Epoch: 268/1000 | Batch 010/052 | Cost: 0.1834\n",
      "Epoch: 268/1000 | Batch 020/052 | Cost: 0.2722\n",
      "Epoch: 268/1000 | Batch 030/052 | Cost: 0.3311\n",
      "Epoch: 268/1000 | Batch 040/052 | Cost: 0.1778\n",
      "Epoch: 268/1000 | Batch 050/052 | Cost: 0.2955\n",
      "Epoch: 268/1000 training accuracy: 87.65%\n",
      "Epoch: 269/1000 | Batch 000/052 | Cost: 0.1631\n",
      "Epoch: 269/1000 | Batch 010/052 | Cost: 0.2451\n",
      "Epoch: 269/1000 | Batch 020/052 | Cost: 0.3692\n",
      "Epoch: 269/1000 | Batch 030/052 | Cost: 0.1733\n",
      "Epoch: 269/1000 | Batch 040/052 | Cost: 0.6442\n",
      "Epoch: 269/1000 | Batch 050/052 | Cost: 0.3705\n",
      "Epoch: 269/1000 training accuracy: 86.53%\n",
      "Epoch: 270/1000 | Batch 000/052 | Cost: 0.6008\n",
      "Epoch: 270/1000 | Batch 010/052 | Cost: 0.4070\n",
      "Epoch: 270/1000 | Batch 020/052 | Cost: 0.5086\n",
      "Epoch: 270/1000 | Batch 030/052 | Cost: 0.1685\n",
      "Epoch: 270/1000 | Batch 040/052 | Cost: 0.2384\n",
      "Epoch: 270/1000 | Batch 050/052 | Cost: 0.4369\n",
      "Epoch: 270/1000 training accuracy: 88.33%\n",
      "Epoch: 271/1000 | Batch 000/052 | Cost: 0.1761\n",
      "Epoch: 271/1000 | Batch 010/052 | Cost: 0.2531\n",
      "Epoch: 271/1000 | Batch 020/052 | Cost: 0.1069\n",
      "Epoch: 271/1000 | Batch 030/052 | Cost: 0.1746\n",
      "Epoch: 271/1000 | Batch 040/052 | Cost: 0.5153\n",
      "Epoch: 271/1000 | Batch 050/052 | Cost: 0.3332\n",
      "Epoch: 271/1000 training accuracy: 87.21%\n",
      "Epoch: 272/1000 | Batch 000/052 | Cost: 0.1479\n",
      "Epoch: 272/1000 | Batch 010/052 | Cost: 0.2033\n",
      "Epoch: 272/1000 | Batch 020/052 | Cost: 0.1731\n",
      "Epoch: 272/1000 | Batch 030/052 | Cost: 0.2353\n",
      "Epoch: 272/1000 | Batch 040/052 | Cost: 0.1115\n",
      "Epoch: 272/1000 | Batch 050/052 | Cost: 0.2633\n",
      "Epoch: 272/1000 training accuracy: 87.77%\n",
      "Epoch: 273/1000 | Batch 000/052 | Cost: 0.2061\n",
      "Epoch: 273/1000 | Batch 010/052 | Cost: 0.2605\n",
      "Epoch: 273/1000 | Batch 020/052 | Cost: 0.2513\n",
      "Epoch: 273/1000 | Batch 030/052 | Cost: 0.1857\n",
      "Epoch: 273/1000 | Batch 040/052 | Cost: 0.4603\n",
      "Epoch: 273/1000 | Batch 050/052 | Cost: 0.5449\n",
      "Epoch: 273/1000 training accuracy: 87.21%\n",
      "Epoch: 274/1000 | Batch 000/052 | Cost: 0.2032\n",
      "Epoch: 274/1000 | Batch 010/052 | Cost: 0.3464\n",
      "Epoch: 274/1000 | Batch 020/052 | Cost: 0.2126\n",
      "Epoch: 274/1000 | Batch 030/052 | Cost: 0.3715\n",
      "Epoch: 274/1000 | Batch 040/052 | Cost: 0.6785\n",
      "Epoch: 274/1000 | Batch 050/052 | Cost: 0.3324\n",
      "Epoch: 274/1000 training accuracy: 88.10%\n",
      "Epoch: 275/1000 | Batch 000/052 | Cost: 0.4675\n",
      "Epoch: 275/1000 | Batch 010/052 | Cost: 0.2836\n",
      "Epoch: 275/1000 | Batch 020/052 | Cost: 0.2780\n",
      "Epoch: 275/1000 | Batch 030/052 | Cost: 0.3834\n",
      "Epoch: 275/1000 | Batch 040/052 | Cost: 0.3651\n",
      "Epoch: 275/1000 | Batch 050/052 | Cost: 0.3543\n",
      "Epoch: 275/1000 training accuracy: 87.54%\n",
      "Epoch: 276/1000 | Batch 000/052 | Cost: 0.4885\n",
      "Epoch: 276/1000 | Batch 010/052 | Cost: 0.1412\n",
      "Epoch: 276/1000 | Batch 020/052 | Cost: 0.3332\n",
      "Epoch: 276/1000 | Batch 030/052 | Cost: 0.1435\n",
      "Epoch: 276/1000 | Batch 040/052 | Cost: 0.3858\n",
      "Epoch: 276/1000 | Batch 050/052 | Cost: 0.2818\n",
      "Epoch: 276/1000 training accuracy: 87.77%\n",
      "Epoch: 277/1000 | Batch 000/052 | Cost: 0.5192\n",
      "Epoch: 277/1000 | Batch 010/052 | Cost: 0.2457\n",
      "Epoch: 277/1000 | Batch 020/052 | Cost: 0.4633\n",
      "Epoch: 277/1000 | Batch 030/052 | Cost: 0.3714\n",
      "Epoch: 277/1000 | Batch 040/052 | Cost: 0.4411\n",
      "Epoch: 277/1000 | Batch 050/052 | Cost: 0.2906\n",
      "Epoch: 277/1000 training accuracy: 87.54%\n",
      "Epoch: 278/1000 | Batch 000/052 | Cost: 0.1011\n",
      "Epoch: 278/1000 | Batch 010/052 | Cost: 0.1251\n",
      "Epoch: 278/1000 | Batch 020/052 | Cost: 0.3657\n",
      "Epoch: 278/1000 | Batch 030/052 | Cost: 0.2395\n",
      "Epoch: 278/1000 | Batch 040/052 | Cost: 0.2508\n",
      "Epoch: 278/1000 | Batch 050/052 | Cost: 0.3032\n",
      "Epoch: 278/1000 training accuracy: 88.22%\n",
      "Epoch: 279/1000 | Batch 000/052 | Cost: 0.1704\n",
      "Epoch: 279/1000 | Batch 010/052 | Cost: 0.2779\n",
      "Epoch: 279/1000 | Batch 020/052 | Cost: 0.3423\n",
      "Epoch: 279/1000 | Batch 030/052 | Cost: 0.2897\n",
      "Epoch: 279/1000 | Batch 040/052 | Cost: 0.0909\n",
      "Epoch: 279/1000 | Batch 050/052 | Cost: 0.2661\n",
      "Epoch: 279/1000 training accuracy: 87.32%\n",
      "Epoch: 280/1000 | Batch 000/052 | Cost: 0.1577\n",
      "Epoch: 280/1000 | Batch 010/052 | Cost: 0.3653\n",
      "Epoch: 280/1000 | Batch 020/052 | Cost: 0.3551\n",
      "Epoch: 280/1000 | Batch 030/052 | Cost: 0.2234\n",
      "Epoch: 280/1000 | Batch 040/052 | Cost: 0.2747\n",
      "Epoch: 280/1000 | Batch 050/052 | Cost: 0.1555\n",
      "Epoch: 280/1000 training accuracy: 88.10%\n",
      "Epoch: 281/1000 | Batch 000/052 | Cost: 0.3333\n",
      "Epoch: 281/1000 | Batch 010/052 | Cost: 0.1141\n",
      "Epoch: 281/1000 | Batch 020/052 | Cost: 0.4449\n",
      "Epoch: 281/1000 | Batch 030/052 | Cost: 0.0579\n",
      "Epoch: 281/1000 | Batch 040/052 | Cost: 0.4191\n",
      "Epoch: 281/1000 | Batch 050/052 | Cost: 0.2413\n",
      "Epoch: 281/1000 training accuracy: 87.99%\n",
      "Epoch: 282/1000 | Batch 000/052 | Cost: 0.1739\n",
      "Epoch: 282/1000 | Batch 010/052 | Cost: 0.1755\n",
      "Epoch: 282/1000 | Batch 020/052 | Cost: 0.1316\n",
      "Epoch: 282/1000 | Batch 030/052 | Cost: 0.1489\n",
      "Epoch: 282/1000 | Batch 040/052 | Cost: 0.1179\n",
      "Epoch: 282/1000 | Batch 050/052 | Cost: 0.5567\n",
      "Epoch: 282/1000 training accuracy: 86.87%\n",
      "Epoch: 283/1000 | Batch 000/052 | Cost: 0.6693\n",
      "Epoch: 283/1000 | Batch 010/052 | Cost: 0.4616\n",
      "Epoch: 283/1000 | Batch 020/052 | Cost: 0.2477\n",
      "Epoch: 283/1000 | Batch 030/052 | Cost: 0.3185\n",
      "Epoch: 283/1000 | Batch 040/052 | Cost: 0.5979\n",
      "Epoch: 283/1000 | Batch 050/052 | Cost: 0.2491\n",
      "Epoch: 283/1000 training accuracy: 88.55%\n",
      "Epoch: 284/1000 | Batch 000/052 | Cost: 0.3038\n",
      "Epoch: 284/1000 | Batch 010/052 | Cost: 0.2692\n",
      "Epoch: 284/1000 | Batch 020/052 | Cost: 0.1532\n",
      "Epoch: 284/1000 | Batch 030/052 | Cost: 0.2163\n",
      "Epoch: 284/1000 | Batch 040/052 | Cost: 0.3720\n",
      "Epoch: 284/1000 | Batch 050/052 | Cost: 0.5165\n",
      "Epoch: 284/1000 training accuracy: 89.23%\n",
      "Epoch: 285/1000 | Batch 000/052 | Cost: 0.2588\n",
      "Epoch: 285/1000 | Batch 010/052 | Cost: 0.1831\n",
      "Epoch: 285/1000 | Batch 020/052 | Cost: 0.2043\n",
      "Epoch: 285/1000 | Batch 030/052 | Cost: 1.1874\n",
      "Epoch: 285/1000 | Batch 040/052 | Cost: 0.6781\n",
      "Epoch: 285/1000 | Batch 050/052 | Cost: 0.2897\n",
      "Epoch: 285/1000 training accuracy: 87.32%\n",
      "Epoch: 286/1000 | Batch 000/052 | Cost: 0.1500\n",
      "Epoch: 286/1000 | Batch 010/052 | Cost: 0.2302\n",
      "Epoch: 286/1000 | Batch 020/052 | Cost: 0.4196\n",
      "Epoch: 286/1000 | Batch 030/052 | Cost: 0.3060\n",
      "Epoch: 286/1000 | Batch 040/052 | Cost: 0.3530\n",
      "Epoch: 286/1000 | Batch 050/052 | Cost: 0.3433\n",
      "Epoch: 286/1000 training accuracy: 87.21%\n",
      "Epoch: 287/1000 | Batch 000/052 | Cost: 0.2095\n",
      "Epoch: 287/1000 | Batch 010/052 | Cost: 0.4268\n",
      "Epoch: 287/1000 | Batch 020/052 | Cost: 0.1747\n",
      "Epoch: 287/1000 | Batch 030/052 | Cost: 0.4648\n",
      "Epoch: 287/1000 | Batch 040/052 | Cost: 0.2839\n",
      "Epoch: 287/1000 | Batch 050/052 | Cost: 0.1509\n",
      "Epoch: 287/1000 training accuracy: 87.77%\n",
      "Epoch: 288/1000 | Batch 000/052 | Cost: 0.3128\n",
      "Epoch: 288/1000 | Batch 010/052 | Cost: 0.1258\n",
      "Epoch: 288/1000 | Batch 020/052 | Cost: 0.2540\n",
      "Epoch: 288/1000 | Batch 030/052 | Cost: 0.2078\n",
      "Epoch: 288/1000 | Batch 040/052 | Cost: 0.3926\n",
      "Epoch: 288/1000 | Batch 050/052 | Cost: 0.1133\n",
      "Epoch: 288/1000 training accuracy: 87.54%\n",
      "Epoch: 289/1000 | Batch 000/052 | Cost: 0.1617\n",
      "Epoch: 289/1000 | Batch 010/052 | Cost: 0.3069\n",
      "Epoch: 289/1000 | Batch 020/052 | Cost: 0.2197\n",
      "Epoch: 289/1000 | Batch 030/052 | Cost: 0.1959\n",
      "Epoch: 289/1000 | Batch 040/052 | Cost: 0.1818\n",
      "Epoch: 289/1000 | Batch 050/052 | Cost: 0.4129\n",
      "Epoch: 289/1000 training accuracy: 87.32%\n",
      "Epoch: 290/1000 | Batch 000/052 | Cost: 0.3034\n",
      "Epoch: 290/1000 | Batch 010/052 | Cost: 0.0999\n",
      "Epoch: 290/1000 | Batch 020/052 | Cost: 0.4692\n",
      "Epoch: 290/1000 | Batch 030/052 | Cost: 0.7635\n",
      "Epoch: 290/1000 | Batch 040/052 | Cost: 0.5249\n",
      "Epoch: 290/1000 | Batch 050/052 | Cost: 0.3210\n",
      "Epoch: 290/1000 training accuracy: 87.65%\n",
      "Epoch: 291/1000 | Batch 000/052 | Cost: 0.1158\n",
      "Epoch: 291/1000 | Batch 010/052 | Cost: 0.4253\n",
      "Epoch: 291/1000 | Batch 020/052 | Cost: 0.6076\n",
      "Epoch: 291/1000 | Batch 030/052 | Cost: 0.2729\n",
      "Epoch: 291/1000 | Batch 040/052 | Cost: 0.1765\n",
      "Epoch: 291/1000 | Batch 050/052 | Cost: 0.2694\n",
      "Epoch: 291/1000 training accuracy: 87.32%\n",
      "Epoch: 292/1000 | Batch 000/052 | Cost: 0.2153\n",
      "Epoch: 292/1000 | Batch 010/052 | Cost: 0.1221\n",
      "Epoch: 292/1000 | Batch 020/052 | Cost: 0.3866\n",
      "Epoch: 292/1000 | Batch 030/052 | Cost: 0.1855\n",
      "Epoch: 292/1000 | Batch 040/052 | Cost: 0.2011\n",
      "Epoch: 292/1000 | Batch 050/052 | Cost: 0.2761\n",
      "Epoch: 292/1000 training accuracy: 87.65%\n",
      "Epoch: 293/1000 | Batch 000/052 | Cost: 0.0870\n",
      "Epoch: 293/1000 | Batch 010/052 | Cost: 0.1336\n",
      "Epoch: 293/1000 | Batch 020/052 | Cost: 0.2638\n",
      "Epoch: 293/1000 | Batch 030/052 | Cost: 0.1852\n",
      "Epoch: 293/1000 | Batch 040/052 | Cost: 0.1397\n",
      "Epoch: 293/1000 | Batch 050/052 | Cost: 0.2109\n",
      "Epoch: 293/1000 training accuracy: 87.09%\n",
      "Epoch: 294/1000 | Batch 000/052 | Cost: 0.0914\n",
      "Epoch: 294/1000 | Batch 010/052 | Cost: 0.1942\n",
      "Epoch: 294/1000 | Batch 020/052 | Cost: 0.2018\n",
      "Epoch: 294/1000 | Batch 030/052 | Cost: 0.1355\n",
      "Epoch: 294/1000 | Batch 040/052 | Cost: 0.2337\n",
      "Epoch: 294/1000 | Batch 050/052 | Cost: 0.4332\n",
      "Epoch: 294/1000 training accuracy: 87.43%\n",
      "Epoch: 295/1000 | Batch 000/052 | Cost: 0.4023\n",
      "Epoch: 295/1000 | Batch 010/052 | Cost: 0.1854\n",
      "Epoch: 295/1000 | Batch 020/052 | Cost: 0.3732\n",
      "Epoch: 295/1000 | Batch 030/052 | Cost: 0.1969\n",
      "Epoch: 295/1000 | Batch 040/052 | Cost: 0.2935\n",
      "Epoch: 295/1000 | Batch 050/052 | Cost: 0.2842\n",
      "Epoch: 295/1000 training accuracy: 86.31%\n",
      "Epoch: 296/1000 | Batch 000/052 | Cost: 0.2603\n",
      "Epoch: 296/1000 | Batch 010/052 | Cost: 0.3616\n",
      "Epoch: 296/1000 | Batch 020/052 | Cost: 0.2186\n",
      "Epoch: 296/1000 | Batch 030/052 | Cost: 0.3157\n",
      "Epoch: 296/1000 | Batch 040/052 | Cost: 0.1619\n",
      "Epoch: 296/1000 | Batch 050/052 | Cost: 0.4240\n",
      "Epoch: 296/1000 training accuracy: 88.44%\n",
      "Epoch: 297/1000 | Batch 000/052 | Cost: 0.5805\n",
      "Epoch: 297/1000 | Batch 010/052 | Cost: 0.0721\n",
      "Epoch: 297/1000 | Batch 020/052 | Cost: 0.1770\n",
      "Epoch: 297/1000 | Batch 030/052 | Cost: 0.3667\n",
      "Epoch: 297/1000 | Batch 040/052 | Cost: 0.2143\n",
      "Epoch: 297/1000 | Batch 050/052 | Cost: 0.3721\n",
      "Epoch: 297/1000 training accuracy: 88.10%\n",
      "Epoch: 298/1000 | Batch 000/052 | Cost: 0.3529\n",
      "Epoch: 298/1000 | Batch 010/052 | Cost: 0.2485\n",
      "Epoch: 298/1000 | Batch 020/052 | Cost: 0.2977\n",
      "Epoch: 298/1000 | Batch 030/052 | Cost: 0.5535\n",
      "Epoch: 298/1000 | Batch 040/052 | Cost: 0.2717\n",
      "Epoch: 298/1000 | Batch 050/052 | Cost: 0.2288\n",
      "Epoch: 298/1000 training accuracy: 88.78%\n",
      "Epoch: 299/1000 | Batch 000/052 | Cost: 0.4880\n",
      "Epoch: 299/1000 | Batch 010/052 | Cost: 0.0853\n",
      "Epoch: 299/1000 | Batch 020/052 | Cost: 0.2751\n",
      "Epoch: 299/1000 | Batch 030/052 | Cost: 0.3882\n",
      "Epoch: 299/1000 | Batch 040/052 | Cost: 0.2887\n",
      "Epoch: 299/1000 | Batch 050/052 | Cost: 0.3133\n",
      "Epoch: 299/1000 training accuracy: 87.54%\n",
      "Epoch: 300/1000 | Batch 000/052 | Cost: 0.1543\n",
      "Epoch: 300/1000 | Batch 010/052 | Cost: 0.4758\n",
      "Epoch: 300/1000 | Batch 020/052 | Cost: 0.5915\n",
      "Epoch: 300/1000 | Batch 030/052 | Cost: 0.3159\n",
      "Epoch: 300/1000 | Batch 040/052 | Cost: 0.2516\n",
      "Epoch: 300/1000 | Batch 050/052 | Cost: 0.2922\n",
      "Epoch: 300/1000 training accuracy: 88.10%\n",
      "Epoch: 301/1000 | Batch 000/052 | Cost: 0.3617\n",
      "Epoch: 301/1000 | Batch 010/052 | Cost: 0.1122\n",
      "Epoch: 301/1000 | Batch 020/052 | Cost: 0.2914\n",
      "Epoch: 301/1000 | Batch 030/052 | Cost: 0.4442\n",
      "Epoch: 301/1000 | Batch 040/052 | Cost: 0.3374\n",
      "Epoch: 301/1000 | Batch 050/052 | Cost: 0.1391\n",
      "Epoch: 301/1000 training accuracy: 88.78%\n",
      "Epoch: 302/1000 | Batch 000/052 | Cost: 0.2508\n",
      "Epoch: 302/1000 | Batch 010/052 | Cost: 0.0939\n",
      "Epoch: 302/1000 | Batch 020/052 | Cost: 0.2840\n",
      "Epoch: 302/1000 | Batch 030/052 | Cost: 0.3834\n",
      "Epoch: 302/1000 | Batch 040/052 | Cost: 0.2925\n",
      "Epoch: 302/1000 | Batch 050/052 | Cost: 0.4162\n",
      "Epoch: 302/1000 training accuracy: 87.54%\n",
      "Epoch: 303/1000 | Batch 000/052 | Cost: 0.6342\n",
      "Epoch: 303/1000 | Batch 010/052 | Cost: 0.5353\n",
      "Epoch: 303/1000 | Batch 020/052 | Cost: 0.5475\n",
      "Epoch: 303/1000 | Batch 030/052 | Cost: 0.1102\n",
      "Epoch: 303/1000 | Batch 040/052 | Cost: 0.2687\n",
      "Epoch: 303/1000 | Batch 050/052 | Cost: 0.2855\n",
      "Epoch: 303/1000 training accuracy: 88.10%\n",
      "Epoch: 304/1000 | Batch 000/052 | Cost: 0.4640\n",
      "Epoch: 304/1000 | Batch 010/052 | Cost: 0.3811\n",
      "Epoch: 304/1000 | Batch 020/052 | Cost: 0.1420\n",
      "Epoch: 304/1000 | Batch 030/052 | Cost: 0.0937\n",
      "Epoch: 304/1000 | Batch 040/052 | Cost: 0.3645\n",
      "Epoch: 304/1000 | Batch 050/052 | Cost: 0.4720\n",
      "Epoch: 304/1000 training accuracy: 87.77%\n",
      "Epoch: 305/1000 | Batch 000/052 | Cost: 0.1430\n",
      "Epoch: 305/1000 | Batch 010/052 | Cost: 0.1441\n",
      "Epoch: 305/1000 | Batch 020/052 | Cost: 0.1294\n",
      "Epoch: 305/1000 | Batch 030/052 | Cost: 0.5131\n",
      "Epoch: 305/1000 | Batch 040/052 | Cost: 0.5055\n",
      "Epoch: 305/1000 | Batch 050/052 | Cost: 0.4148\n",
      "Epoch: 305/1000 training accuracy: 87.43%\n",
      "Epoch: 306/1000 | Batch 000/052 | Cost: 0.4189\n",
      "Epoch: 306/1000 | Batch 010/052 | Cost: 0.2674\n",
      "Epoch: 306/1000 | Batch 020/052 | Cost: 0.4908\n",
      "Epoch: 306/1000 | Batch 030/052 | Cost: 0.3759\n",
      "Epoch: 306/1000 | Batch 040/052 | Cost: 0.0711\n",
      "Epoch: 306/1000 | Batch 050/052 | Cost: 0.2441\n",
      "Epoch: 306/1000 training accuracy: 86.87%\n",
      "Epoch: 307/1000 | Batch 000/052 | Cost: 0.1497\n",
      "Epoch: 307/1000 | Batch 010/052 | Cost: 0.1365\n",
      "Epoch: 307/1000 | Batch 020/052 | Cost: 0.1953\n",
      "Epoch: 307/1000 | Batch 030/052 | Cost: 0.2284\n",
      "Epoch: 307/1000 | Batch 040/052 | Cost: 0.1471\n",
      "Epoch: 307/1000 | Batch 050/052 | Cost: 0.3568\n",
      "Epoch: 307/1000 training accuracy: 87.77%\n",
      "Epoch: 308/1000 | Batch 000/052 | Cost: 0.4375\n",
      "Epoch: 308/1000 | Batch 010/052 | Cost: 0.3714\n",
      "Epoch: 308/1000 | Batch 020/052 | Cost: 0.1493\n",
      "Epoch: 308/1000 | Batch 030/052 | Cost: 0.4003\n",
      "Epoch: 308/1000 | Batch 040/052 | Cost: 0.2233\n",
      "Epoch: 308/1000 | Batch 050/052 | Cost: 0.0651\n",
      "Epoch: 308/1000 training accuracy: 87.32%\n",
      "Epoch: 309/1000 | Batch 000/052 | Cost: 0.4329\n",
      "Epoch: 309/1000 | Batch 010/052 | Cost: 0.2870\n",
      "Epoch: 309/1000 | Batch 020/052 | Cost: 0.3179\n",
      "Epoch: 309/1000 | Batch 030/052 | Cost: 0.1618\n",
      "Epoch: 309/1000 | Batch 040/052 | Cost: 0.4038\n",
      "Epoch: 309/1000 | Batch 050/052 | Cost: 0.3428\n",
      "Epoch: 309/1000 training accuracy: 87.43%\n",
      "Epoch: 310/1000 | Batch 000/052 | Cost: 0.0924\n",
      "Epoch: 310/1000 | Batch 010/052 | Cost: 0.2368\n",
      "Epoch: 310/1000 | Batch 020/052 | Cost: 0.1751\n",
      "Epoch: 310/1000 | Batch 030/052 | Cost: 0.1808\n",
      "Epoch: 310/1000 | Batch 040/052 | Cost: 0.3107\n",
      "Epoch: 310/1000 | Batch 050/052 | Cost: 0.3338\n",
      "Epoch: 310/1000 training accuracy: 87.65%\n",
      "Epoch: 311/1000 | Batch 000/052 | Cost: 0.4449\n",
      "Epoch: 311/1000 | Batch 010/052 | Cost: 0.4440\n",
      "Epoch: 311/1000 | Batch 020/052 | Cost: 0.3617\n",
      "Epoch: 311/1000 | Batch 030/052 | Cost: 0.2612\n",
      "Epoch: 311/1000 | Batch 040/052 | Cost: 0.5726\n",
      "Epoch: 311/1000 | Batch 050/052 | Cost: 0.1976\n",
      "Epoch: 311/1000 training accuracy: 87.54%\n",
      "Epoch: 312/1000 | Batch 000/052 | Cost: 0.3170\n",
      "Epoch: 312/1000 | Batch 010/052 | Cost: 0.1707\n",
      "Epoch: 312/1000 | Batch 020/052 | Cost: 0.2328\n",
      "Epoch: 312/1000 | Batch 030/052 | Cost: 0.2516\n",
      "Epoch: 312/1000 | Batch 040/052 | Cost: 0.5020\n",
      "Epoch: 312/1000 | Batch 050/052 | Cost: 0.1796\n",
      "Epoch: 312/1000 training accuracy: 87.88%\n",
      "Epoch: 313/1000 | Batch 000/052 | Cost: 0.4991\n",
      "Epoch: 313/1000 | Batch 010/052 | Cost: 0.1440\n",
      "Epoch: 313/1000 | Batch 020/052 | Cost: 0.2663\n",
      "Epoch: 313/1000 | Batch 030/052 | Cost: 0.2794\n",
      "Epoch: 313/1000 | Batch 040/052 | Cost: 0.3606\n",
      "Epoch: 313/1000 | Batch 050/052 | Cost: 0.3159\n",
      "Epoch: 313/1000 training accuracy: 87.43%\n",
      "Epoch: 314/1000 | Batch 000/052 | Cost: 0.4083\n",
      "Epoch: 314/1000 | Batch 010/052 | Cost: 0.2973\n",
      "Epoch: 314/1000 | Batch 020/052 | Cost: 0.1859\n",
      "Epoch: 314/1000 | Batch 030/052 | Cost: 0.1462\n",
      "Epoch: 314/1000 | Batch 040/052 | Cost: 0.2932\n",
      "Epoch: 314/1000 | Batch 050/052 | Cost: 0.1656\n",
      "Epoch: 314/1000 training accuracy: 87.88%\n",
      "Epoch: 315/1000 | Batch 000/052 | Cost: 0.4043\n",
      "Epoch: 315/1000 | Batch 010/052 | Cost: 0.1617\n",
      "Epoch: 315/1000 | Batch 020/052 | Cost: 0.2410\n",
      "Epoch: 315/1000 | Batch 030/052 | Cost: 0.1651\n",
      "Epoch: 315/1000 | Batch 040/052 | Cost: 0.4566\n",
      "Epoch: 315/1000 | Batch 050/052 | Cost: 0.4129\n",
      "Epoch: 315/1000 training accuracy: 88.66%\n",
      "Epoch: 316/1000 | Batch 000/052 | Cost: 0.3469\n",
      "Epoch: 316/1000 | Batch 010/052 | Cost: 0.2990\n",
      "Epoch: 316/1000 | Batch 020/052 | Cost: 0.4427\n",
      "Epoch: 316/1000 | Batch 030/052 | Cost: 0.4210\n",
      "Epoch: 316/1000 | Batch 040/052 | Cost: 0.2820\n",
      "Epoch: 316/1000 | Batch 050/052 | Cost: 0.2531\n",
      "Epoch: 316/1000 training accuracy: 87.54%\n",
      "Epoch: 317/1000 | Batch 000/052 | Cost: 0.1403\n",
      "Epoch: 317/1000 | Batch 010/052 | Cost: 0.4197\n",
      "Epoch: 317/1000 | Batch 020/052 | Cost: 0.3872\n",
      "Epoch: 317/1000 | Batch 030/052 | Cost: 0.2436\n",
      "Epoch: 317/1000 | Batch 040/052 | Cost: 0.2984\n",
      "Epoch: 317/1000 | Batch 050/052 | Cost: 0.2619\n",
      "Epoch: 317/1000 training accuracy: 87.54%\n",
      "Epoch: 318/1000 | Batch 000/052 | Cost: 0.2861\n",
      "Epoch: 318/1000 | Batch 010/052 | Cost: 0.1401\n",
      "Epoch: 318/1000 | Batch 020/052 | Cost: 0.0918\n",
      "Epoch: 318/1000 | Batch 030/052 | Cost: 0.5860\n",
      "Epoch: 318/1000 | Batch 040/052 | Cost: 0.1910\n",
      "Epoch: 318/1000 | Batch 050/052 | Cost: 0.2029\n",
      "Epoch: 318/1000 training accuracy: 87.88%\n",
      "Epoch: 319/1000 | Batch 000/052 | Cost: 0.3360\n",
      "Epoch: 319/1000 | Batch 010/052 | Cost: 0.2352\n",
      "Epoch: 319/1000 | Batch 020/052 | Cost: 0.2977\n",
      "Epoch: 319/1000 | Batch 030/052 | Cost: 0.4503\n",
      "Epoch: 319/1000 | Batch 040/052 | Cost: 0.3607\n",
      "Epoch: 319/1000 | Batch 050/052 | Cost: 0.2100\n",
      "Epoch: 319/1000 training accuracy: 87.32%\n",
      "Epoch: 320/1000 | Batch 000/052 | Cost: 0.3413\n",
      "Epoch: 320/1000 | Batch 010/052 | Cost: 0.1243\n",
      "Epoch: 320/1000 | Batch 020/052 | Cost: 0.1938\n",
      "Epoch: 320/1000 | Batch 030/052 | Cost: 0.2570\n",
      "Epoch: 320/1000 | Batch 040/052 | Cost: 0.1691\n",
      "Epoch: 320/1000 | Batch 050/052 | Cost: 0.4157\n",
      "Epoch: 320/1000 training accuracy: 87.21%\n",
      "Epoch: 321/1000 | Batch 000/052 | Cost: 0.7268\n",
      "Epoch: 321/1000 | Batch 010/052 | Cost: 0.1260\n",
      "Epoch: 321/1000 | Batch 020/052 | Cost: 0.4501\n",
      "Epoch: 321/1000 | Batch 030/052 | Cost: 0.2790\n",
      "Epoch: 321/1000 | Batch 040/052 | Cost: 0.2587\n",
      "Epoch: 321/1000 | Batch 050/052 | Cost: 0.4515\n",
      "Epoch: 321/1000 training accuracy: 87.65%\n",
      "Epoch: 322/1000 | Batch 000/052 | Cost: 0.1783\n",
      "Epoch: 322/1000 | Batch 010/052 | Cost: 0.1539\n",
      "Epoch: 322/1000 | Batch 020/052 | Cost: 0.5800\n",
      "Epoch: 322/1000 | Batch 030/052 | Cost: 0.1636\n",
      "Epoch: 322/1000 | Batch 040/052 | Cost: 0.4839\n",
      "Epoch: 322/1000 | Batch 050/052 | Cost: 0.1603\n",
      "Epoch: 322/1000 training accuracy: 87.77%\n",
      "Epoch: 323/1000 | Batch 000/052 | Cost: 0.2115\n",
      "Epoch: 323/1000 | Batch 010/052 | Cost: 0.3772\n",
      "Epoch: 323/1000 | Batch 020/052 | Cost: 0.1170\n",
      "Epoch: 323/1000 | Batch 030/052 | Cost: 0.1558\n",
      "Epoch: 323/1000 | Batch 040/052 | Cost: 0.2362\n",
      "Epoch: 323/1000 | Batch 050/052 | Cost: 0.1968\n",
      "Epoch: 323/1000 training accuracy: 87.88%\n",
      "Epoch: 324/1000 | Batch 000/052 | Cost: 0.2170\n",
      "Epoch: 324/1000 | Batch 010/052 | Cost: 0.3920\n",
      "Epoch: 324/1000 | Batch 020/052 | Cost: 0.2429\n",
      "Epoch: 324/1000 | Batch 030/052 | Cost: 0.1948\n",
      "Epoch: 324/1000 | Batch 040/052 | Cost: 0.2910\n",
      "Epoch: 324/1000 | Batch 050/052 | Cost: 0.1987\n",
      "Epoch: 324/1000 training accuracy: 87.99%\n",
      "Epoch: 325/1000 | Batch 000/052 | Cost: 0.1590\n",
      "Epoch: 325/1000 | Batch 010/052 | Cost: 0.6515\n",
      "Epoch: 325/1000 | Batch 020/052 | Cost: 0.2483\n",
      "Epoch: 325/1000 | Batch 030/052 | Cost: 0.2838\n",
      "Epoch: 325/1000 | Batch 040/052 | Cost: 0.1277\n",
      "Epoch: 325/1000 | Batch 050/052 | Cost: 0.1415\n",
      "Epoch: 325/1000 training accuracy: 86.98%\n",
      "Epoch: 326/1000 | Batch 000/052 | Cost: 0.1454\n",
      "Epoch: 326/1000 | Batch 010/052 | Cost: 0.1785\n",
      "Epoch: 326/1000 | Batch 020/052 | Cost: 0.1970\n",
      "Epoch: 326/1000 | Batch 030/052 | Cost: 0.1216\n",
      "Epoch: 326/1000 | Batch 040/052 | Cost: 0.3172\n",
      "Epoch: 326/1000 | Batch 050/052 | Cost: 0.1213\n",
      "Epoch: 326/1000 training accuracy: 87.99%\n",
      "Epoch: 327/1000 | Batch 000/052 | Cost: 0.3677\n",
      "Epoch: 327/1000 | Batch 010/052 | Cost: 0.1746\n",
      "Epoch: 327/1000 | Batch 020/052 | Cost: 0.3584\n",
      "Epoch: 327/1000 | Batch 030/052 | Cost: 0.2491\n",
      "Epoch: 327/1000 | Batch 040/052 | Cost: 0.4344\n",
      "Epoch: 327/1000 | Batch 050/052 | Cost: 0.3899\n",
      "Epoch: 327/1000 training accuracy: 88.22%\n",
      "Epoch: 328/1000 | Batch 000/052 | Cost: 0.3306\n",
      "Epoch: 328/1000 | Batch 010/052 | Cost: 0.1076\n",
      "Epoch: 328/1000 | Batch 020/052 | Cost: 0.1577\n",
      "Epoch: 328/1000 | Batch 030/052 | Cost: 0.3213\n",
      "Epoch: 328/1000 | Batch 040/052 | Cost: 0.6158\n",
      "Epoch: 328/1000 | Batch 050/052 | Cost: 0.2004\n",
      "Epoch: 328/1000 training accuracy: 87.43%\n",
      "Epoch: 329/1000 | Batch 000/052 | Cost: 0.3347\n",
      "Epoch: 329/1000 | Batch 010/052 | Cost: 0.1862\n",
      "Epoch: 329/1000 | Batch 020/052 | Cost: 0.1749\n",
      "Epoch: 329/1000 | Batch 030/052 | Cost: 0.3799\n",
      "Epoch: 329/1000 | Batch 040/052 | Cost: 0.3707\n",
      "Epoch: 329/1000 | Batch 050/052 | Cost: 0.3212\n",
      "Epoch: 329/1000 training accuracy: 87.88%\n",
      "Epoch: 330/1000 | Batch 000/052 | Cost: 0.4907\n",
      "Epoch: 330/1000 | Batch 010/052 | Cost: 0.2578\n",
      "Epoch: 330/1000 | Batch 020/052 | Cost: 0.2820\n",
      "Epoch: 330/1000 | Batch 030/052 | Cost: 0.3946\n",
      "Epoch: 330/1000 | Batch 040/052 | Cost: 0.0858\n",
      "Epoch: 330/1000 | Batch 050/052 | Cost: 0.3685\n",
      "Epoch: 330/1000 training accuracy: 88.66%\n",
      "Epoch: 331/1000 | Batch 000/052 | Cost: 0.2263\n",
      "Epoch: 331/1000 | Batch 010/052 | Cost: 0.1778\n",
      "Epoch: 331/1000 | Batch 020/052 | Cost: 0.3413\n",
      "Epoch: 331/1000 | Batch 030/052 | Cost: 0.3100\n",
      "Epoch: 331/1000 | Batch 040/052 | Cost: 0.2584\n",
      "Epoch: 331/1000 | Batch 050/052 | Cost: 0.2093\n",
      "Epoch: 331/1000 training accuracy: 88.66%\n",
      "Epoch: 332/1000 | Batch 000/052 | Cost: 0.2283\n",
      "Epoch: 332/1000 | Batch 010/052 | Cost: 0.4151\n",
      "Epoch: 332/1000 | Batch 020/052 | Cost: 0.2918\n",
      "Epoch: 332/1000 | Batch 030/052 | Cost: 0.5132\n",
      "Epoch: 332/1000 | Batch 040/052 | Cost: 0.3827\n",
      "Epoch: 332/1000 | Batch 050/052 | Cost: 0.1794\n",
      "Epoch: 332/1000 training accuracy: 87.99%\n",
      "Epoch: 333/1000 | Batch 000/052 | Cost: 0.2364\n",
      "Epoch: 333/1000 | Batch 010/052 | Cost: 0.1491\n",
      "Epoch: 333/1000 | Batch 020/052 | Cost: 0.0907\n",
      "Epoch: 333/1000 | Batch 030/052 | Cost: 0.2562\n",
      "Epoch: 333/1000 | Batch 040/052 | Cost: 0.4988\n",
      "Epoch: 333/1000 | Batch 050/052 | Cost: 0.0587\n",
      "Epoch: 333/1000 training accuracy: 87.99%\n",
      "Epoch: 334/1000 | Batch 000/052 | Cost: 0.1702\n",
      "Epoch: 334/1000 | Batch 010/052 | Cost: 0.1120\n",
      "Epoch: 334/1000 | Batch 020/052 | Cost: 0.2942\n",
      "Epoch: 334/1000 | Batch 030/052 | Cost: 0.2115\n",
      "Epoch: 334/1000 | Batch 040/052 | Cost: 0.3875\n",
      "Epoch: 334/1000 | Batch 050/052 | Cost: 0.4133\n",
      "Epoch: 334/1000 training accuracy: 87.09%\n",
      "Epoch: 335/1000 | Batch 000/052 | Cost: 0.4305\n",
      "Epoch: 335/1000 | Batch 010/052 | Cost: 0.1294\n",
      "Epoch: 335/1000 | Batch 020/052 | Cost: 0.2593\n",
      "Epoch: 335/1000 | Batch 030/052 | Cost: 0.2902\n",
      "Epoch: 335/1000 | Batch 040/052 | Cost: 0.3048\n",
      "Epoch: 335/1000 | Batch 050/052 | Cost: 0.5235\n",
      "Epoch: 335/1000 training accuracy: 88.10%\n",
      "Epoch: 336/1000 | Batch 000/052 | Cost: 0.7656\n",
      "Epoch: 336/1000 | Batch 010/052 | Cost: 0.2539\n",
      "Epoch: 336/1000 | Batch 020/052 | Cost: 0.0967\n",
      "Epoch: 336/1000 | Batch 030/052 | Cost: 0.1639\n",
      "Epoch: 336/1000 | Batch 040/052 | Cost: 0.2741\n",
      "Epoch: 336/1000 | Batch 050/052 | Cost: 0.1074\n",
      "Epoch: 336/1000 training accuracy: 88.10%\n",
      "Epoch: 337/1000 | Batch 000/052 | Cost: 0.2482\n",
      "Epoch: 337/1000 | Batch 010/052 | Cost: 0.2065\n",
      "Epoch: 337/1000 | Batch 020/052 | Cost: 0.3510\n",
      "Epoch: 337/1000 | Batch 030/052 | Cost: 0.1118\n",
      "Epoch: 337/1000 | Batch 040/052 | Cost: 0.2509\n",
      "Epoch: 337/1000 | Batch 050/052 | Cost: 0.3415\n",
      "Epoch: 337/1000 training accuracy: 87.99%\n",
      "Epoch: 338/1000 | Batch 000/052 | Cost: 0.3057\n",
      "Epoch: 338/1000 | Batch 010/052 | Cost: 0.5475\n",
      "Epoch: 338/1000 | Batch 020/052 | Cost: 0.1396\n",
      "Epoch: 338/1000 | Batch 030/052 | Cost: 0.1396\n",
      "Epoch: 338/1000 | Batch 040/052 | Cost: 0.3881\n",
      "Epoch: 338/1000 | Batch 050/052 | Cost: 0.3072\n",
      "Epoch: 338/1000 training accuracy: 88.55%\n",
      "Epoch: 339/1000 | Batch 000/052 | Cost: 0.1154\n",
      "Epoch: 339/1000 | Batch 010/052 | Cost: 0.2680\n",
      "Epoch: 339/1000 | Batch 020/052 | Cost: 0.2349\n",
      "Epoch: 339/1000 | Batch 030/052 | Cost: 0.2359\n",
      "Epoch: 339/1000 | Batch 040/052 | Cost: 0.1634\n",
      "Epoch: 339/1000 | Batch 050/052 | Cost: 0.2480\n",
      "Epoch: 339/1000 training accuracy: 87.43%\n",
      "Epoch: 340/1000 | Batch 000/052 | Cost: 0.1004\n",
      "Epoch: 340/1000 | Batch 010/052 | Cost: 0.1449\n",
      "Epoch: 340/1000 | Batch 020/052 | Cost: 0.2148\n",
      "Epoch: 340/1000 | Batch 030/052 | Cost: 0.1647\n",
      "Epoch: 340/1000 | Batch 040/052 | Cost: 0.1161\n",
      "Epoch: 340/1000 | Batch 050/052 | Cost: 0.6442\n",
      "Epoch: 340/1000 training accuracy: 87.77%\n",
      "Epoch: 341/1000 | Batch 000/052 | Cost: 0.4114\n",
      "Epoch: 341/1000 | Batch 010/052 | Cost: 0.3469\n",
      "Epoch: 341/1000 | Batch 020/052 | Cost: 0.3577\n",
      "Epoch: 341/1000 | Batch 030/052 | Cost: 0.4037\n",
      "Epoch: 341/1000 | Batch 040/052 | Cost: 0.3504\n",
      "Epoch: 341/1000 | Batch 050/052 | Cost: 0.1389\n",
      "Epoch: 341/1000 training accuracy: 87.43%\n",
      "Epoch: 342/1000 | Batch 000/052 | Cost: 0.3557\n",
      "Epoch: 342/1000 | Batch 010/052 | Cost: 0.3070\n",
      "Epoch: 342/1000 | Batch 020/052 | Cost: 0.1579\n",
      "Epoch: 342/1000 | Batch 030/052 | Cost: 0.0942\n",
      "Epoch: 342/1000 | Batch 040/052 | Cost: 0.3176\n",
      "Epoch: 342/1000 | Batch 050/052 | Cost: 0.1172\n",
      "Epoch: 342/1000 training accuracy: 88.22%\n",
      "Epoch: 343/1000 | Batch 000/052 | Cost: 0.2272\n",
      "Epoch: 343/1000 | Batch 010/052 | Cost: 0.1543\n",
      "Epoch: 343/1000 | Batch 020/052 | Cost: 0.1165\n",
      "Epoch: 343/1000 | Batch 030/052 | Cost: 0.1861\n",
      "Epoch: 343/1000 | Batch 040/052 | Cost: 0.0910\n",
      "Epoch: 343/1000 | Batch 050/052 | Cost: 0.6641\n",
      "Epoch: 343/1000 training accuracy: 89.00%\n",
      "Epoch: 344/1000 | Batch 000/052 | Cost: 0.1930\n",
      "Epoch: 344/1000 | Batch 010/052 | Cost: 0.2021\n",
      "Epoch: 344/1000 | Batch 020/052 | Cost: 0.3158\n",
      "Epoch: 344/1000 | Batch 030/052 | Cost: 0.5818\n",
      "Epoch: 344/1000 | Batch 040/052 | Cost: 0.3994\n",
      "Epoch: 344/1000 | Batch 050/052 | Cost: 0.3681\n",
      "Epoch: 344/1000 training accuracy: 88.44%\n",
      "Epoch: 345/1000 | Batch 000/052 | Cost: 0.3226\n",
      "Epoch: 345/1000 | Batch 010/052 | Cost: 0.2741\n",
      "Epoch: 345/1000 | Batch 020/052 | Cost: 0.6221\n",
      "Epoch: 345/1000 | Batch 030/052 | Cost: 0.3107\n",
      "Epoch: 345/1000 | Batch 040/052 | Cost: 0.3230\n",
      "Epoch: 345/1000 | Batch 050/052 | Cost: 0.3740\n",
      "Epoch: 345/1000 training accuracy: 87.99%\n",
      "Epoch: 346/1000 | Batch 000/052 | Cost: 0.3324\n",
      "Epoch: 346/1000 | Batch 010/052 | Cost: 0.0855\n",
      "Epoch: 346/1000 | Batch 020/052 | Cost: 0.1537\n",
      "Epoch: 346/1000 | Batch 030/052 | Cost: 0.1667\n",
      "Epoch: 346/1000 | Batch 040/052 | Cost: 0.1555\n",
      "Epoch: 346/1000 | Batch 050/052 | Cost: 0.4844\n",
      "Epoch: 346/1000 training accuracy: 88.22%\n",
      "Epoch: 347/1000 | Batch 000/052 | Cost: 0.4847\n",
      "Epoch: 347/1000 | Batch 010/052 | Cost: 0.2992\n",
      "Epoch: 347/1000 | Batch 020/052 | Cost: 0.1790\n",
      "Epoch: 347/1000 | Batch 030/052 | Cost: 0.3827\n",
      "Epoch: 347/1000 | Batch 040/052 | Cost: 0.1222\n",
      "Epoch: 347/1000 | Batch 050/052 | Cost: 0.1581\n",
      "Epoch: 347/1000 training accuracy: 88.33%\n",
      "Epoch: 348/1000 | Batch 000/052 | Cost: 0.0567\n",
      "Epoch: 348/1000 | Batch 010/052 | Cost: 0.3074\n",
      "Epoch: 348/1000 | Batch 020/052 | Cost: 0.2715\n",
      "Epoch: 348/1000 | Batch 030/052 | Cost: 0.4298\n",
      "Epoch: 348/1000 | Batch 040/052 | Cost: 0.4260\n",
      "Epoch: 348/1000 | Batch 050/052 | Cost: 0.3696\n",
      "Epoch: 348/1000 training accuracy: 88.33%\n",
      "Epoch: 349/1000 | Batch 000/052 | Cost: 0.3045\n",
      "Epoch: 349/1000 | Batch 010/052 | Cost: 0.1590\n",
      "Epoch: 349/1000 | Batch 020/052 | Cost: 0.3381\n",
      "Epoch: 349/1000 | Batch 030/052 | Cost: 0.2272\n",
      "Epoch: 349/1000 | Batch 040/052 | Cost: 0.0906\n",
      "Epoch: 349/1000 | Batch 050/052 | Cost: 0.2260\n",
      "Epoch: 349/1000 training accuracy: 88.33%\n",
      "Epoch: 350/1000 | Batch 000/052 | Cost: 0.4808\n",
      "Epoch: 350/1000 | Batch 010/052 | Cost: 0.4998\n",
      "Epoch: 350/1000 | Batch 020/052 | Cost: 0.5072\n",
      "Epoch: 350/1000 | Batch 030/052 | Cost: 0.1739\n",
      "Epoch: 350/1000 | Batch 040/052 | Cost: 0.7284\n",
      "Epoch: 350/1000 | Batch 050/052 | Cost: 0.4240\n",
      "Epoch: 350/1000 training accuracy: 88.33%\n",
      "Epoch: 351/1000 | Batch 000/052 | Cost: 0.2399\n",
      "Epoch: 351/1000 | Batch 010/052 | Cost: 0.1473\n",
      "Epoch: 351/1000 | Batch 020/052 | Cost: 0.2576\n",
      "Epoch: 351/1000 | Batch 030/052 | Cost: 0.2735\n",
      "Epoch: 351/1000 | Batch 040/052 | Cost: 0.1490\n",
      "Epoch: 351/1000 | Batch 050/052 | Cost: 0.3590\n",
      "Epoch: 351/1000 training accuracy: 87.88%\n",
      "Epoch: 352/1000 | Batch 000/052 | Cost: 0.2685\n",
      "Epoch: 352/1000 | Batch 010/052 | Cost: 0.2705\n",
      "Epoch: 352/1000 | Batch 020/052 | Cost: 0.4398\n",
      "Epoch: 352/1000 | Batch 030/052 | Cost: 0.1548\n",
      "Epoch: 352/1000 | Batch 040/052 | Cost: 0.3781\n",
      "Epoch: 352/1000 | Batch 050/052 | Cost: 0.5321\n",
      "Epoch: 352/1000 training accuracy: 88.55%\n",
      "Epoch: 353/1000 | Batch 000/052 | Cost: 0.4268\n",
      "Epoch: 353/1000 | Batch 010/052 | Cost: 0.2410\n",
      "Epoch: 353/1000 | Batch 020/052 | Cost: 0.3484\n",
      "Epoch: 353/1000 | Batch 030/052 | Cost: 0.2437\n",
      "Epoch: 353/1000 | Batch 040/052 | Cost: 0.2791\n",
      "Epoch: 353/1000 | Batch 050/052 | Cost: 0.1647\n",
      "Epoch: 353/1000 training accuracy: 87.65%\n",
      "Epoch: 354/1000 | Batch 000/052 | Cost: 0.4032\n",
      "Epoch: 354/1000 | Batch 010/052 | Cost: 0.1634\n",
      "Epoch: 354/1000 | Batch 020/052 | Cost: 0.4454\n",
      "Epoch: 354/1000 | Batch 030/052 | Cost: 0.0906\n",
      "Epoch: 354/1000 | Batch 040/052 | Cost: 0.7209\n",
      "Epoch: 354/1000 | Batch 050/052 | Cost: 0.2946\n",
      "Epoch: 354/1000 training accuracy: 88.22%\n",
      "Epoch: 355/1000 | Batch 000/052 | Cost: 0.1150\n",
      "Epoch: 355/1000 | Batch 010/052 | Cost: 0.2117\n",
      "Epoch: 355/1000 | Batch 020/052 | Cost: 0.3882\n",
      "Epoch: 355/1000 | Batch 030/052 | Cost: 0.4722\n",
      "Epoch: 355/1000 | Batch 040/052 | Cost: 0.1179\n",
      "Epoch: 355/1000 | Batch 050/052 | Cost: 0.4364\n",
      "Epoch: 355/1000 training accuracy: 87.09%\n",
      "Epoch: 356/1000 | Batch 000/052 | Cost: 0.1453\n",
      "Epoch: 356/1000 | Batch 010/052 | Cost: 0.2623\n",
      "Epoch: 356/1000 | Batch 020/052 | Cost: 0.4382\n",
      "Epoch: 356/1000 | Batch 030/052 | Cost: 0.3133\n",
      "Epoch: 356/1000 | Batch 040/052 | Cost: 0.3878\n",
      "Epoch: 356/1000 | Batch 050/052 | Cost: 0.1649\n",
      "Epoch: 356/1000 training accuracy: 87.99%\n",
      "Epoch: 357/1000 | Batch 000/052 | Cost: 0.3597\n",
      "Epoch: 357/1000 | Batch 010/052 | Cost: 0.4609\n",
      "Epoch: 357/1000 | Batch 020/052 | Cost: 0.3729\n",
      "Epoch: 357/1000 | Batch 030/052 | Cost: 0.0661\n",
      "Epoch: 357/1000 | Batch 040/052 | Cost: 0.5114\n",
      "Epoch: 357/1000 | Batch 050/052 | Cost: 0.3406\n",
      "Epoch: 357/1000 training accuracy: 88.55%\n",
      "Epoch: 358/1000 | Batch 000/052 | Cost: 0.2209\n",
      "Epoch: 358/1000 | Batch 010/052 | Cost: 0.3247\n",
      "Epoch: 358/1000 | Batch 020/052 | Cost: 0.1861\n",
      "Epoch: 358/1000 | Batch 030/052 | Cost: 0.3042\n",
      "Epoch: 358/1000 | Batch 040/052 | Cost: 0.4808\n",
      "Epoch: 358/1000 | Batch 050/052 | Cost: 0.5918\n",
      "Epoch: 358/1000 training accuracy: 88.89%\n",
      "Epoch: 359/1000 | Batch 000/052 | Cost: 0.3113\n",
      "Epoch: 359/1000 | Batch 010/052 | Cost: 0.3204\n",
      "Epoch: 359/1000 | Batch 020/052 | Cost: 0.1478\n",
      "Epoch: 359/1000 | Batch 030/052 | Cost: 0.2837\n",
      "Epoch: 359/1000 | Batch 040/052 | Cost: 0.1398\n",
      "Epoch: 359/1000 | Batch 050/052 | Cost: 0.0822\n",
      "Epoch: 359/1000 training accuracy: 87.09%\n",
      "Epoch: 360/1000 | Batch 000/052 | Cost: 0.1967\n",
      "Epoch: 360/1000 | Batch 010/052 | Cost: 0.2166\n",
      "Epoch: 360/1000 | Batch 020/052 | Cost: 0.1775\n",
      "Epoch: 360/1000 | Batch 030/052 | Cost: 0.7105\n",
      "Epoch: 360/1000 | Batch 040/052 | Cost: 0.4130\n",
      "Epoch: 360/1000 | Batch 050/052 | Cost: 0.1925\n",
      "Epoch: 360/1000 training accuracy: 88.22%\n",
      "Epoch: 361/1000 | Batch 000/052 | Cost: 0.6774\n",
      "Epoch: 361/1000 | Batch 010/052 | Cost: 0.3214\n",
      "Epoch: 361/1000 | Batch 020/052 | Cost: 0.1178\n",
      "Epoch: 361/1000 | Batch 030/052 | Cost: 0.2620\n",
      "Epoch: 361/1000 | Batch 040/052 | Cost: 0.3028\n",
      "Epoch: 361/1000 | Batch 050/052 | Cost: 0.5144\n",
      "Epoch: 361/1000 training accuracy: 88.33%\n",
      "Epoch: 362/1000 | Batch 000/052 | Cost: 0.4921\n",
      "Epoch: 362/1000 | Batch 010/052 | Cost: 0.3874\n",
      "Epoch: 362/1000 | Batch 020/052 | Cost: 0.2052\n",
      "Epoch: 362/1000 | Batch 030/052 | Cost: 0.1784\n",
      "Epoch: 362/1000 | Batch 040/052 | Cost: 0.1717\n",
      "Epoch: 362/1000 | Batch 050/052 | Cost: 0.1557\n",
      "Epoch: 362/1000 training accuracy: 88.22%\n",
      "Epoch: 363/1000 | Batch 000/052 | Cost: 0.2282\n",
      "Epoch: 363/1000 | Batch 010/052 | Cost: 0.0932\n",
      "Epoch: 363/1000 | Batch 020/052 | Cost: 0.2228\n",
      "Epoch: 363/1000 | Batch 030/052 | Cost: 0.1149\n",
      "Epoch: 363/1000 | Batch 040/052 | Cost: 0.2075\n",
      "Epoch: 363/1000 | Batch 050/052 | Cost: 0.3271\n",
      "Epoch: 363/1000 training accuracy: 88.44%\n",
      "Epoch: 364/1000 | Batch 000/052 | Cost: 0.0924\n",
      "Epoch: 364/1000 | Batch 010/052 | Cost: 0.1337\n",
      "Epoch: 364/1000 | Batch 020/052 | Cost: 0.1464\n",
      "Epoch: 364/1000 | Batch 030/052 | Cost: 0.5269\n",
      "Epoch: 364/1000 | Batch 040/052 | Cost: 0.1913\n",
      "Epoch: 364/1000 | Batch 050/052 | Cost: 0.1022\n",
      "Epoch: 364/1000 training accuracy: 88.44%\n",
      "Epoch: 365/1000 | Batch 000/052 | Cost: 0.3535\n",
      "Epoch: 365/1000 | Batch 010/052 | Cost: 0.4339\n",
      "Epoch: 365/1000 | Batch 020/052 | Cost: 0.2863\n",
      "Epoch: 365/1000 | Batch 030/052 | Cost: 0.1816\n",
      "Epoch: 365/1000 | Batch 040/052 | Cost: 0.2278\n",
      "Epoch: 365/1000 | Batch 050/052 | Cost: 0.0731\n",
      "Epoch: 365/1000 training accuracy: 87.77%\n",
      "Epoch: 366/1000 | Batch 000/052 | Cost: 0.1194\n",
      "Epoch: 366/1000 | Batch 010/052 | Cost: 0.1542\n",
      "Epoch: 366/1000 | Batch 020/052 | Cost: 0.2844\n",
      "Epoch: 366/1000 | Batch 030/052 | Cost: 0.3850\n",
      "Epoch: 366/1000 | Batch 040/052 | Cost: 0.2798\n",
      "Epoch: 366/1000 | Batch 050/052 | Cost: 0.2793\n",
      "Epoch: 366/1000 training accuracy: 87.88%\n",
      "Epoch: 367/1000 | Batch 000/052 | Cost: 0.4071\n",
      "Epoch: 367/1000 | Batch 010/052 | Cost: 0.2100\n",
      "Epoch: 367/1000 | Batch 020/052 | Cost: 0.4431\n",
      "Epoch: 367/1000 | Batch 030/052 | Cost: 0.2072\n",
      "Epoch: 367/1000 | Batch 040/052 | Cost: 0.2499\n",
      "Epoch: 367/1000 | Batch 050/052 | Cost: 0.2390\n",
      "Epoch: 367/1000 training accuracy: 87.32%\n",
      "Epoch: 368/1000 | Batch 000/052 | Cost: 0.3077\n",
      "Epoch: 368/1000 | Batch 010/052 | Cost: 0.3610\n",
      "Epoch: 368/1000 | Batch 020/052 | Cost: 0.1665\n",
      "Epoch: 368/1000 | Batch 030/052 | Cost: 0.2897\n",
      "Epoch: 368/1000 | Batch 040/052 | Cost: 0.1688\n",
      "Epoch: 368/1000 | Batch 050/052 | Cost: 0.2483\n",
      "Epoch: 368/1000 training accuracy: 87.43%\n",
      "Epoch: 369/1000 | Batch 000/052 | Cost: 0.2275\n",
      "Epoch: 369/1000 | Batch 010/052 | Cost: 0.1078\n",
      "Epoch: 369/1000 | Batch 020/052 | Cost: 0.2997\n",
      "Epoch: 369/1000 | Batch 030/052 | Cost: 0.3738\n",
      "Epoch: 369/1000 | Batch 040/052 | Cost: 0.5256\n",
      "Epoch: 369/1000 | Batch 050/052 | Cost: 0.3258\n",
      "Epoch: 369/1000 training accuracy: 87.88%\n",
      "Epoch: 370/1000 | Batch 000/052 | Cost: 0.3348\n",
      "Epoch: 370/1000 | Batch 010/052 | Cost: 0.2842\n",
      "Epoch: 370/1000 | Batch 020/052 | Cost: 0.3138\n",
      "Epoch: 370/1000 | Batch 030/052 | Cost: 0.1583\n",
      "Epoch: 370/1000 | Batch 040/052 | Cost: 0.3308\n",
      "Epoch: 370/1000 | Batch 050/052 | Cost: 0.1677\n",
      "Epoch: 370/1000 training accuracy: 87.43%\n",
      "Epoch: 371/1000 | Batch 000/052 | Cost: 0.0911\n",
      "Epoch: 371/1000 | Batch 010/052 | Cost: 0.4786\n",
      "Epoch: 371/1000 | Batch 020/052 | Cost: 0.3318\n",
      "Epoch: 371/1000 | Batch 030/052 | Cost: 0.6351\n",
      "Epoch: 371/1000 | Batch 040/052 | Cost: 0.2792\n",
      "Epoch: 371/1000 | Batch 050/052 | Cost: 0.1450\n",
      "Epoch: 371/1000 training accuracy: 87.77%\n",
      "Epoch: 372/1000 | Batch 000/052 | Cost: 0.1705\n",
      "Epoch: 372/1000 | Batch 010/052 | Cost: 0.3651\n",
      "Epoch: 372/1000 | Batch 020/052 | Cost: 0.2921\n",
      "Epoch: 372/1000 | Batch 030/052 | Cost: 0.1635\n",
      "Epoch: 372/1000 | Batch 040/052 | Cost: 0.2760\n",
      "Epoch: 372/1000 | Batch 050/052 | Cost: 0.2350\n",
      "Epoch: 372/1000 training accuracy: 87.77%\n",
      "Epoch: 373/1000 | Batch 000/052 | Cost: 0.2751\n",
      "Epoch: 373/1000 | Batch 010/052 | Cost: 0.2263\n",
      "Epoch: 373/1000 | Batch 020/052 | Cost: 0.2926\n",
      "Epoch: 373/1000 | Batch 030/052 | Cost: 0.5030\n",
      "Epoch: 373/1000 | Batch 040/052 | Cost: 0.5361\n",
      "Epoch: 373/1000 | Batch 050/052 | Cost: 0.4978\n",
      "Epoch: 373/1000 training accuracy: 87.54%\n",
      "Epoch: 374/1000 | Batch 000/052 | Cost: 0.6860\n",
      "Epoch: 374/1000 | Batch 010/052 | Cost: 0.2479\n",
      "Epoch: 374/1000 | Batch 020/052 | Cost: 0.2218\n",
      "Epoch: 374/1000 | Batch 030/052 | Cost: 0.2505\n",
      "Epoch: 374/1000 | Batch 040/052 | Cost: 0.1697\n",
      "Epoch: 374/1000 | Batch 050/052 | Cost: 0.1917\n",
      "Epoch: 374/1000 training accuracy: 87.88%\n",
      "Epoch: 375/1000 | Batch 000/052 | Cost: 0.2880\n",
      "Epoch: 375/1000 | Batch 010/052 | Cost: 0.2773\n",
      "Epoch: 375/1000 | Batch 020/052 | Cost: 0.2473\n",
      "Epoch: 375/1000 | Batch 030/052 | Cost: 0.1392\n",
      "Epoch: 375/1000 | Batch 040/052 | Cost: 0.2447\n",
      "Epoch: 375/1000 | Batch 050/052 | Cost: 0.3060\n",
      "Epoch: 375/1000 training accuracy: 87.88%\n",
      "Epoch: 376/1000 | Batch 000/052 | Cost: 0.5761\n",
      "Epoch: 376/1000 | Batch 010/052 | Cost: 0.2385\n",
      "Epoch: 376/1000 | Batch 020/052 | Cost: 0.2511\n",
      "Epoch: 376/1000 | Batch 030/052 | Cost: 0.2685\n",
      "Epoch: 376/1000 | Batch 040/052 | Cost: 0.5484\n",
      "Epoch: 376/1000 | Batch 050/052 | Cost: 0.3787\n",
      "Epoch: 376/1000 training accuracy: 87.88%\n",
      "Epoch: 377/1000 | Batch 000/052 | Cost: 0.3209\n",
      "Epoch: 377/1000 | Batch 010/052 | Cost: 0.4007\n",
      "Epoch: 377/1000 | Batch 020/052 | Cost: 0.3488\n",
      "Epoch: 377/1000 | Batch 030/052 | Cost: 0.3225\n",
      "Epoch: 377/1000 | Batch 040/052 | Cost: 0.0540\n",
      "Epoch: 377/1000 | Batch 050/052 | Cost: 0.2896\n",
      "Epoch: 377/1000 training accuracy: 87.99%\n",
      "Epoch: 378/1000 | Batch 000/052 | Cost: 0.2420\n",
      "Epoch: 378/1000 | Batch 010/052 | Cost: 0.2680\n",
      "Epoch: 378/1000 | Batch 020/052 | Cost: 0.4846\n",
      "Epoch: 378/1000 | Batch 030/052 | Cost: 0.3575\n",
      "Epoch: 378/1000 | Batch 040/052 | Cost: 0.2293\n",
      "Epoch: 378/1000 | Batch 050/052 | Cost: 0.1821\n",
      "Epoch: 378/1000 training accuracy: 88.44%\n",
      "Epoch: 379/1000 | Batch 000/052 | Cost: 0.2075\n",
      "Epoch: 379/1000 | Batch 010/052 | Cost: 0.4334\n",
      "Epoch: 379/1000 | Batch 020/052 | Cost: 0.2839\n",
      "Epoch: 379/1000 | Batch 030/052 | Cost: 0.2368\n",
      "Epoch: 379/1000 | Batch 040/052 | Cost: 0.1799\n",
      "Epoch: 379/1000 | Batch 050/052 | Cost: 0.3458\n",
      "Epoch: 379/1000 training accuracy: 87.32%\n",
      "Epoch: 380/1000 | Batch 000/052 | Cost: 0.2528\n",
      "Epoch: 380/1000 | Batch 010/052 | Cost: 0.3935\n",
      "Epoch: 380/1000 | Batch 020/052 | Cost: 0.2117\n",
      "Epoch: 380/1000 | Batch 030/052 | Cost: 0.3288\n",
      "Epoch: 380/1000 | Batch 040/052 | Cost: 0.1774\n",
      "Epoch: 380/1000 | Batch 050/052 | Cost: 0.2712\n",
      "Epoch: 380/1000 training accuracy: 88.22%\n",
      "Epoch: 381/1000 | Batch 000/052 | Cost: 0.2654\n",
      "Epoch: 381/1000 | Batch 010/052 | Cost: 0.4766\n",
      "Epoch: 381/1000 | Batch 020/052 | Cost: 0.4271\n",
      "Epoch: 381/1000 | Batch 030/052 | Cost: 0.4178\n",
      "Epoch: 381/1000 | Batch 040/052 | Cost: 0.2700\n",
      "Epoch: 381/1000 | Batch 050/052 | Cost: 0.1286\n",
      "Epoch: 381/1000 training accuracy: 87.88%\n",
      "Epoch: 382/1000 | Batch 000/052 | Cost: 0.2341\n",
      "Epoch: 382/1000 | Batch 010/052 | Cost: 0.1823\n",
      "Epoch: 382/1000 | Batch 020/052 | Cost: 0.4493\n",
      "Epoch: 382/1000 | Batch 030/052 | Cost: 0.2697\n",
      "Epoch: 382/1000 | Batch 040/052 | Cost: 0.3342\n",
      "Epoch: 382/1000 | Batch 050/052 | Cost: 0.5541\n",
      "Epoch: 382/1000 training accuracy: 88.10%\n",
      "Epoch: 383/1000 | Batch 000/052 | Cost: 0.1701\n",
      "Epoch: 383/1000 | Batch 010/052 | Cost: 0.1884\n",
      "Epoch: 383/1000 | Batch 020/052 | Cost: 0.1092\n",
      "Epoch: 383/1000 | Batch 030/052 | Cost: 0.3002\n",
      "Epoch: 383/1000 | Batch 040/052 | Cost: 0.3945\n",
      "Epoch: 383/1000 | Batch 050/052 | Cost: 0.3125\n",
      "Epoch: 383/1000 training accuracy: 87.65%\n",
      "Epoch: 384/1000 | Batch 000/052 | Cost: 0.3709\n",
      "Epoch: 384/1000 | Batch 010/052 | Cost: 0.3083\n",
      "Epoch: 384/1000 | Batch 020/052 | Cost: 0.1750\n",
      "Epoch: 384/1000 | Batch 030/052 | Cost: 0.2961\n",
      "Epoch: 384/1000 | Batch 040/052 | Cost: 0.1730\n",
      "Epoch: 384/1000 | Batch 050/052 | Cost: 0.3350\n",
      "Epoch: 384/1000 training accuracy: 88.10%\n",
      "Epoch: 385/1000 | Batch 000/052 | Cost: 0.2028\n",
      "Epoch: 385/1000 | Batch 010/052 | Cost: 0.3403\n",
      "Epoch: 385/1000 | Batch 020/052 | Cost: 0.1858\n",
      "Epoch: 385/1000 | Batch 030/052 | Cost: 0.3725\n",
      "Epoch: 385/1000 | Batch 040/052 | Cost: 0.2670\n",
      "Epoch: 385/1000 | Batch 050/052 | Cost: 0.1438\n",
      "Epoch: 385/1000 training accuracy: 87.99%\n",
      "Epoch: 386/1000 | Batch 000/052 | Cost: 0.2585\n",
      "Epoch: 386/1000 | Batch 010/052 | Cost: 0.5589\n",
      "Epoch: 386/1000 | Batch 020/052 | Cost: 0.2272\n",
      "Epoch: 386/1000 | Batch 030/052 | Cost: 0.2229\n",
      "Epoch: 386/1000 | Batch 040/052 | Cost: 0.3877\n",
      "Epoch: 386/1000 | Batch 050/052 | Cost: 0.1467\n",
      "Epoch: 386/1000 training accuracy: 87.54%\n",
      "Epoch: 387/1000 | Batch 000/052 | Cost: 0.3044\n",
      "Epoch: 387/1000 | Batch 010/052 | Cost: 0.3621\n",
      "Epoch: 387/1000 | Batch 020/052 | Cost: 0.4692\n",
      "Epoch: 387/1000 | Batch 030/052 | Cost: 0.6502\n",
      "Epoch: 387/1000 | Batch 040/052 | Cost: 0.1408\n",
      "Epoch: 387/1000 | Batch 050/052 | Cost: 0.3262\n",
      "Epoch: 387/1000 training accuracy: 86.64%\n",
      "Epoch: 388/1000 | Batch 000/052 | Cost: 0.3693\n",
      "Epoch: 388/1000 | Batch 010/052 | Cost: 0.2605\n",
      "Epoch: 388/1000 | Batch 020/052 | Cost: 0.2733\n",
      "Epoch: 388/1000 | Batch 030/052 | Cost: 0.6498\n",
      "Epoch: 388/1000 | Batch 040/052 | Cost: 0.4591\n",
      "Epoch: 388/1000 | Batch 050/052 | Cost: 0.2535\n",
      "Epoch: 388/1000 training accuracy: 87.88%\n",
      "Epoch: 389/1000 | Batch 000/052 | Cost: 0.2390\n",
      "Epoch: 389/1000 | Batch 010/052 | Cost: 0.3848\n",
      "Epoch: 389/1000 | Batch 020/052 | Cost: 0.0713\n",
      "Epoch: 389/1000 | Batch 030/052 | Cost: 0.2072\n",
      "Epoch: 389/1000 | Batch 040/052 | Cost: 0.3263\n",
      "Epoch: 389/1000 | Batch 050/052 | Cost: 0.1549\n",
      "Epoch: 389/1000 training accuracy: 87.99%\n",
      "Epoch: 390/1000 | Batch 000/052 | Cost: 0.3038\n",
      "Epoch: 390/1000 | Batch 010/052 | Cost: 0.5382\n",
      "Epoch: 390/1000 | Batch 020/052 | Cost: 0.2010\n",
      "Epoch: 390/1000 | Batch 030/052 | Cost: 0.5804\n",
      "Epoch: 390/1000 | Batch 040/052 | Cost: 0.3009\n",
      "Epoch: 390/1000 | Batch 050/052 | Cost: 0.5143\n",
      "Epoch: 390/1000 training accuracy: 87.65%\n",
      "Epoch: 391/1000 | Batch 000/052 | Cost: 0.1725\n",
      "Epoch: 391/1000 | Batch 010/052 | Cost: 0.1980\n",
      "Epoch: 391/1000 | Batch 020/052 | Cost: 0.5323\n",
      "Epoch: 391/1000 | Batch 030/052 | Cost: 0.2732\n",
      "Epoch: 391/1000 | Batch 040/052 | Cost: 0.1814\n",
      "Epoch: 391/1000 | Batch 050/052 | Cost: 0.1531\n",
      "Epoch: 391/1000 training accuracy: 88.22%\n",
      "Epoch: 392/1000 | Batch 000/052 | Cost: 0.3421\n",
      "Epoch: 392/1000 | Batch 010/052 | Cost: 0.4496\n",
      "Epoch: 392/1000 | Batch 020/052 | Cost: 0.2735\n",
      "Epoch: 392/1000 | Batch 030/052 | Cost: 0.2347\n",
      "Epoch: 392/1000 | Batch 040/052 | Cost: 0.4623\n",
      "Epoch: 392/1000 | Batch 050/052 | Cost: 0.4334\n",
      "Epoch: 392/1000 training accuracy: 88.33%\n",
      "Epoch: 393/1000 | Batch 000/052 | Cost: 0.4186\n",
      "Epoch: 393/1000 | Batch 010/052 | Cost: 0.3583\n",
      "Epoch: 393/1000 | Batch 020/052 | Cost: 0.3413\n",
      "Epoch: 393/1000 | Batch 030/052 | Cost: 0.3880\n",
      "Epoch: 393/1000 | Batch 040/052 | Cost: 0.2368\n",
      "Epoch: 393/1000 | Batch 050/052 | Cost: 0.5084\n",
      "Epoch: 393/1000 training accuracy: 87.43%\n",
      "Epoch: 394/1000 | Batch 000/052 | Cost: 0.4494\n",
      "Epoch: 394/1000 | Batch 010/052 | Cost: 0.5889\n",
      "Epoch: 394/1000 | Batch 020/052 | Cost: 0.2281\n",
      "Epoch: 394/1000 | Batch 030/052 | Cost: 0.2177\n",
      "Epoch: 394/1000 | Batch 040/052 | Cost: 0.3306\n",
      "Epoch: 394/1000 | Batch 050/052 | Cost: 0.1506\n",
      "Epoch: 394/1000 training accuracy: 87.99%\n",
      "Epoch: 395/1000 | Batch 000/052 | Cost: 0.1547\n",
      "Epoch: 395/1000 | Batch 010/052 | Cost: 0.5229\n",
      "Epoch: 395/1000 | Batch 020/052 | Cost: 0.3900\n",
      "Epoch: 395/1000 | Batch 030/052 | Cost: 0.1053\n",
      "Epoch: 395/1000 | Batch 040/052 | Cost: 0.2888\n",
      "Epoch: 395/1000 | Batch 050/052 | Cost: 0.1064\n",
      "Epoch: 395/1000 training accuracy: 87.09%\n",
      "Epoch: 396/1000 | Batch 000/052 | Cost: 0.3094\n",
      "Epoch: 396/1000 | Batch 010/052 | Cost: 0.2078\n",
      "Epoch: 396/1000 | Batch 020/052 | Cost: 0.3266\n",
      "Epoch: 396/1000 | Batch 030/052 | Cost: 0.0923\n",
      "Epoch: 396/1000 | Batch 040/052 | Cost: 0.2872\n",
      "Epoch: 396/1000 | Batch 050/052 | Cost: 0.1539\n",
      "Epoch: 396/1000 training accuracy: 87.54%\n",
      "Epoch: 397/1000 | Batch 000/052 | Cost: 0.3794\n",
      "Epoch: 397/1000 | Batch 010/052 | Cost: 0.2715\n",
      "Epoch: 397/1000 | Batch 020/052 | Cost: 0.2593\n",
      "Epoch: 397/1000 | Batch 030/052 | Cost: 0.1358\n",
      "Epoch: 397/1000 | Batch 040/052 | Cost: 0.4518\n",
      "Epoch: 397/1000 | Batch 050/052 | Cost: 0.4808\n",
      "Epoch: 397/1000 training accuracy: 88.22%\n",
      "Epoch: 398/1000 | Batch 000/052 | Cost: 0.1827\n",
      "Epoch: 398/1000 | Batch 010/052 | Cost: 0.3373\n",
      "Epoch: 398/1000 | Batch 020/052 | Cost: 0.2800\n",
      "Epoch: 398/1000 | Batch 030/052 | Cost: 0.2515\n",
      "Epoch: 398/1000 | Batch 040/052 | Cost: 0.1141\n",
      "Epoch: 398/1000 | Batch 050/052 | Cost: 0.3239\n",
      "Epoch: 398/1000 training accuracy: 88.89%\n",
      "Epoch: 399/1000 | Batch 000/052 | Cost: 0.3578\n",
      "Epoch: 399/1000 | Batch 010/052 | Cost: 0.2286\n",
      "Epoch: 399/1000 | Batch 020/052 | Cost: 0.0783\n",
      "Epoch: 399/1000 | Batch 030/052 | Cost: 0.2142\n",
      "Epoch: 399/1000 | Batch 040/052 | Cost: 0.4796\n",
      "Epoch: 399/1000 | Batch 050/052 | Cost: 0.4389\n",
      "Epoch: 399/1000 training accuracy: 87.99%\n",
      "Epoch: 400/1000 | Batch 000/052 | Cost: 0.2404\n",
      "Epoch: 400/1000 | Batch 010/052 | Cost: 0.2934\n",
      "Epoch: 400/1000 | Batch 020/052 | Cost: 0.1171\n",
      "Epoch: 400/1000 | Batch 030/052 | Cost: 0.1633\n",
      "Epoch: 400/1000 | Batch 040/052 | Cost: 0.2261\n",
      "Epoch: 400/1000 | Batch 050/052 | Cost: 0.2337\n",
      "Epoch: 400/1000 training accuracy: 86.64%\n",
      "Epoch: 401/1000 | Batch 000/052 | Cost: 0.2108\n",
      "Epoch: 401/1000 | Batch 010/052 | Cost: 0.4743\n",
      "Epoch: 401/1000 | Batch 020/052 | Cost: 0.0983\n",
      "Epoch: 401/1000 | Batch 030/052 | Cost: 0.8226\n",
      "Epoch: 401/1000 | Batch 040/052 | Cost: 0.1152\n",
      "Epoch: 401/1000 | Batch 050/052 | Cost: 0.1031\n",
      "Epoch: 401/1000 training accuracy: 87.77%\n",
      "Epoch: 402/1000 | Batch 000/052 | Cost: 0.6570\n",
      "Epoch: 402/1000 | Batch 010/052 | Cost: 0.3536\n",
      "Epoch: 402/1000 | Batch 020/052 | Cost: 0.2687\n",
      "Epoch: 402/1000 | Batch 030/052 | Cost: 0.0741\n",
      "Epoch: 402/1000 | Batch 040/052 | Cost: 0.2765\n",
      "Epoch: 402/1000 | Batch 050/052 | Cost: 0.1270\n",
      "Epoch: 402/1000 training accuracy: 88.22%\n",
      "Epoch: 403/1000 | Batch 000/052 | Cost: 0.3368\n",
      "Epoch: 403/1000 | Batch 010/052 | Cost: 0.2409\n",
      "Epoch: 403/1000 | Batch 020/052 | Cost: 0.1877\n",
      "Epoch: 403/1000 | Batch 030/052 | Cost: 0.1399\n",
      "Epoch: 403/1000 | Batch 040/052 | Cost: 0.2911\n",
      "Epoch: 403/1000 | Batch 050/052 | Cost: 0.2584\n",
      "Epoch: 403/1000 training accuracy: 87.65%\n",
      "Epoch: 404/1000 | Batch 000/052 | Cost: 0.0652\n",
      "Epoch: 404/1000 | Batch 010/052 | Cost: 0.0468\n",
      "Epoch: 404/1000 | Batch 020/052 | Cost: 0.2851\n",
      "Epoch: 404/1000 | Batch 030/052 | Cost: 0.4841\n",
      "Epoch: 404/1000 | Batch 040/052 | Cost: 0.1850\n",
      "Epoch: 404/1000 | Batch 050/052 | Cost: 0.6154\n",
      "Epoch: 404/1000 training accuracy: 87.99%\n",
      "Epoch: 405/1000 | Batch 000/052 | Cost: 0.3920\n",
      "Epoch: 405/1000 | Batch 010/052 | Cost: 0.2084\n",
      "Epoch: 405/1000 | Batch 020/052 | Cost: 0.4180\n",
      "Epoch: 405/1000 | Batch 030/052 | Cost: 0.4280\n",
      "Epoch: 405/1000 | Batch 040/052 | Cost: 0.1189\n",
      "Epoch: 405/1000 | Batch 050/052 | Cost: 0.1621\n",
      "Epoch: 405/1000 training accuracy: 87.99%\n",
      "Epoch: 406/1000 | Batch 000/052 | Cost: 0.3305\n",
      "Epoch: 406/1000 | Batch 010/052 | Cost: 0.2503\n",
      "Epoch: 406/1000 | Batch 020/052 | Cost: 0.1731\n",
      "Epoch: 406/1000 | Batch 030/052 | Cost: 0.3360\n",
      "Epoch: 406/1000 | Batch 040/052 | Cost: 0.1810\n",
      "Epoch: 406/1000 | Batch 050/052 | Cost: 0.1809\n",
      "Epoch: 406/1000 training accuracy: 87.43%\n",
      "Epoch: 407/1000 | Batch 000/052 | Cost: 0.7375\n",
      "Epoch: 407/1000 | Batch 010/052 | Cost: 0.2643\n",
      "Epoch: 407/1000 | Batch 020/052 | Cost: 0.1402\n",
      "Epoch: 407/1000 | Batch 030/052 | Cost: 0.2997\n",
      "Epoch: 407/1000 | Batch 040/052 | Cost: 0.3950\n",
      "Epoch: 407/1000 | Batch 050/052 | Cost: 0.3891\n",
      "Epoch: 407/1000 training accuracy: 88.10%\n",
      "Epoch: 408/1000 | Batch 000/052 | Cost: 0.4963\n",
      "Epoch: 408/1000 | Batch 010/052 | Cost: 0.3123\n",
      "Epoch: 408/1000 | Batch 020/052 | Cost: 0.5141\n",
      "Epoch: 408/1000 | Batch 030/052 | Cost: 0.4628\n",
      "Epoch: 408/1000 | Batch 040/052 | Cost: 0.3098\n",
      "Epoch: 408/1000 | Batch 050/052 | Cost: 0.2467\n",
      "Epoch: 408/1000 training accuracy: 88.66%\n",
      "Epoch: 409/1000 | Batch 000/052 | Cost: 0.1068\n",
      "Epoch: 409/1000 | Batch 010/052 | Cost: 0.4154\n",
      "Epoch: 409/1000 | Batch 020/052 | Cost: 0.2416\n",
      "Epoch: 409/1000 | Batch 030/052 | Cost: 0.1933\n",
      "Epoch: 409/1000 | Batch 040/052 | Cost: 0.2813\n",
      "Epoch: 409/1000 | Batch 050/052 | Cost: 0.2153\n",
      "Epoch: 409/1000 training accuracy: 87.99%\n",
      "Epoch: 410/1000 | Batch 000/052 | Cost: 0.1604\n",
      "Epoch: 410/1000 | Batch 010/052 | Cost: 0.1820\n",
      "Epoch: 410/1000 | Batch 020/052 | Cost: 0.5118\n",
      "Epoch: 410/1000 | Batch 030/052 | Cost: 0.3070\n",
      "Epoch: 410/1000 | Batch 040/052 | Cost: 0.3895\n",
      "Epoch: 410/1000 | Batch 050/052 | Cost: 0.2267\n",
      "Epoch: 410/1000 training accuracy: 88.66%\n",
      "Epoch: 411/1000 | Batch 000/052 | Cost: 0.3388\n",
      "Epoch: 411/1000 | Batch 010/052 | Cost: 0.3068\n",
      "Epoch: 411/1000 | Batch 020/052 | Cost: 0.2414\n",
      "Epoch: 411/1000 | Batch 030/052 | Cost: 0.1692\n",
      "Epoch: 411/1000 | Batch 040/052 | Cost: 0.1775\n",
      "Epoch: 411/1000 | Batch 050/052 | Cost: 0.4298\n",
      "Epoch: 411/1000 training accuracy: 87.77%\n",
      "Epoch: 412/1000 | Batch 000/052 | Cost: 0.5137\n",
      "Epoch: 412/1000 | Batch 010/052 | Cost: 0.1441\n",
      "Epoch: 412/1000 | Batch 020/052 | Cost: 0.2599\n",
      "Epoch: 412/1000 | Batch 030/052 | Cost: 0.3610\n",
      "Epoch: 412/1000 | Batch 040/052 | Cost: 0.4695\n",
      "Epoch: 412/1000 | Batch 050/052 | Cost: 0.1670\n",
      "Epoch: 412/1000 training accuracy: 87.88%\n",
      "Epoch: 413/1000 | Batch 000/052 | Cost: 0.1940\n",
      "Epoch: 413/1000 | Batch 010/052 | Cost: 0.2953\n",
      "Epoch: 413/1000 | Batch 020/052 | Cost: 0.1931\n",
      "Epoch: 413/1000 | Batch 030/052 | Cost: 0.1208\n",
      "Epoch: 413/1000 | Batch 040/052 | Cost: 0.5475\n",
      "Epoch: 413/1000 | Batch 050/052 | Cost: 0.3857\n",
      "Epoch: 413/1000 training accuracy: 87.88%\n",
      "Epoch: 414/1000 | Batch 000/052 | Cost: 0.3568\n",
      "Epoch: 414/1000 | Batch 010/052 | Cost: 0.3980\n",
      "Epoch: 414/1000 | Batch 020/052 | Cost: 0.0945\n",
      "Epoch: 414/1000 | Batch 030/052 | Cost: 0.3185\n",
      "Epoch: 414/1000 | Batch 040/052 | Cost: 0.4614\n",
      "Epoch: 414/1000 | Batch 050/052 | Cost: 0.2654\n",
      "Epoch: 414/1000 training accuracy: 88.66%\n",
      "Epoch: 415/1000 | Batch 000/052 | Cost: 0.4283\n",
      "Epoch: 415/1000 | Batch 010/052 | Cost: 0.3868\n",
      "Epoch: 415/1000 | Batch 020/052 | Cost: 0.0949\n",
      "Epoch: 415/1000 | Batch 030/052 | Cost: 0.2861\n",
      "Epoch: 415/1000 | Batch 040/052 | Cost: 0.2349\n",
      "Epoch: 415/1000 | Batch 050/052 | Cost: 0.2614\n",
      "Epoch: 415/1000 training accuracy: 88.10%\n",
      "Epoch: 416/1000 | Batch 000/052 | Cost: 0.3219\n",
      "Epoch: 416/1000 | Batch 010/052 | Cost: 0.2862\n",
      "Epoch: 416/1000 | Batch 020/052 | Cost: 0.3380\n",
      "Epoch: 416/1000 | Batch 030/052 | Cost: 0.3330\n",
      "Epoch: 416/1000 | Batch 040/052 | Cost: 0.0946\n",
      "Epoch: 416/1000 | Batch 050/052 | Cost: 0.2693\n",
      "Epoch: 416/1000 training accuracy: 88.33%\n",
      "Epoch: 417/1000 | Batch 000/052 | Cost: 0.2947\n",
      "Epoch: 417/1000 | Batch 010/052 | Cost: 0.2494\n",
      "Epoch: 417/1000 | Batch 020/052 | Cost: 0.5635\n",
      "Epoch: 417/1000 | Batch 030/052 | Cost: 0.0568\n",
      "Epoch: 417/1000 | Batch 040/052 | Cost: 0.3980\n",
      "Epoch: 417/1000 | Batch 050/052 | Cost: 0.1236\n",
      "Epoch: 417/1000 training accuracy: 88.33%\n",
      "Epoch: 418/1000 | Batch 000/052 | Cost: 0.2289\n",
      "Epoch: 418/1000 | Batch 010/052 | Cost: 0.4107\n",
      "Epoch: 418/1000 | Batch 020/052 | Cost: 0.2815\n",
      "Epoch: 418/1000 | Batch 030/052 | Cost: 0.3790\n",
      "Epoch: 418/1000 | Batch 040/052 | Cost: 0.0655\n",
      "Epoch: 418/1000 | Batch 050/052 | Cost: 0.3558\n",
      "Epoch: 418/1000 training accuracy: 87.09%\n",
      "Epoch: 419/1000 | Batch 000/052 | Cost: 0.0852\n",
      "Epoch: 419/1000 | Batch 010/052 | Cost: 0.2555\n",
      "Epoch: 419/1000 | Batch 020/052 | Cost: 0.6663\n",
      "Epoch: 419/1000 | Batch 030/052 | Cost: 0.1715\n",
      "Epoch: 419/1000 | Batch 040/052 | Cost: 0.2579\n",
      "Epoch: 419/1000 | Batch 050/052 | Cost: 0.6307\n",
      "Epoch: 419/1000 training accuracy: 88.22%\n",
      "Epoch: 420/1000 | Batch 000/052 | Cost: 0.3184\n",
      "Epoch: 420/1000 | Batch 010/052 | Cost: 0.4429\n",
      "Epoch: 420/1000 | Batch 020/052 | Cost: 0.1242\n",
      "Epoch: 420/1000 | Batch 030/052 | Cost: 0.2144\n",
      "Epoch: 420/1000 | Batch 040/052 | Cost: 0.2617\n",
      "Epoch: 420/1000 | Batch 050/052 | Cost: 0.2538\n",
      "Epoch: 420/1000 training accuracy: 87.54%\n",
      "Epoch: 421/1000 | Batch 000/052 | Cost: 0.0850\n",
      "Epoch: 421/1000 | Batch 010/052 | Cost: 0.2112\n",
      "Epoch: 421/1000 | Batch 020/052 | Cost: 0.2066\n",
      "Epoch: 421/1000 | Batch 030/052 | Cost: 0.1457\n",
      "Epoch: 421/1000 | Batch 040/052 | Cost: 0.2710\n",
      "Epoch: 421/1000 | Batch 050/052 | Cost: 0.2406\n",
      "Epoch: 421/1000 training accuracy: 88.10%\n",
      "Epoch: 422/1000 | Batch 000/052 | Cost: 0.2154\n",
      "Epoch: 422/1000 | Batch 010/052 | Cost: 0.1941\n",
      "Epoch: 422/1000 | Batch 020/052 | Cost: 0.3282\n",
      "Epoch: 422/1000 | Batch 030/052 | Cost: 0.2084\n",
      "Epoch: 422/1000 | Batch 040/052 | Cost: 0.2778\n",
      "Epoch: 422/1000 | Batch 050/052 | Cost: 0.1639\n",
      "Epoch: 422/1000 training accuracy: 87.77%\n",
      "Epoch: 423/1000 | Batch 000/052 | Cost: 0.3036\n",
      "Epoch: 423/1000 | Batch 010/052 | Cost: 0.2137\n",
      "Epoch: 423/1000 | Batch 020/052 | Cost: 0.1940\n",
      "Epoch: 423/1000 | Batch 030/052 | Cost: 0.4353\n",
      "Epoch: 423/1000 | Batch 040/052 | Cost: 0.1785\n",
      "Epoch: 423/1000 | Batch 050/052 | Cost: 0.1324\n",
      "Epoch: 423/1000 training accuracy: 88.55%\n",
      "Epoch: 424/1000 | Batch 000/052 | Cost: 0.2268\n",
      "Epoch: 424/1000 | Batch 010/052 | Cost: 0.2130\n",
      "Epoch: 424/1000 | Batch 020/052 | Cost: 0.6021\n",
      "Epoch: 424/1000 | Batch 030/052 | Cost: 0.1928\n",
      "Epoch: 424/1000 | Batch 040/052 | Cost: 0.2268\n",
      "Epoch: 424/1000 | Batch 050/052 | Cost: 0.3676\n",
      "Epoch: 424/1000 training accuracy: 86.98%\n",
      "Epoch: 425/1000 | Batch 000/052 | Cost: 0.1188\n",
      "Epoch: 425/1000 | Batch 010/052 | Cost: 0.4237\n",
      "Epoch: 425/1000 | Batch 020/052 | Cost: 0.2915\n",
      "Epoch: 425/1000 | Batch 030/052 | Cost: 0.5059\n",
      "Epoch: 425/1000 | Batch 040/052 | Cost: 0.5338\n",
      "Epoch: 425/1000 | Batch 050/052 | Cost: 0.3788\n",
      "Epoch: 425/1000 training accuracy: 88.33%\n",
      "Epoch: 426/1000 | Batch 000/052 | Cost: 0.1630\n",
      "Epoch: 426/1000 | Batch 010/052 | Cost: 0.1702\n",
      "Epoch: 426/1000 | Batch 020/052 | Cost: 0.2361\n",
      "Epoch: 426/1000 | Batch 030/052 | Cost: 0.3725\n",
      "Epoch: 426/1000 | Batch 040/052 | Cost: 0.4142\n",
      "Epoch: 426/1000 | Batch 050/052 | Cost: 0.1356\n",
      "Epoch: 426/1000 training accuracy: 86.98%\n",
      "Epoch: 427/1000 | Batch 000/052 | Cost: 0.1544\n",
      "Epoch: 427/1000 | Batch 010/052 | Cost: 0.3813\n",
      "Epoch: 427/1000 | Batch 020/052 | Cost: 0.1866\n",
      "Epoch: 427/1000 | Batch 030/052 | Cost: 0.2363\n",
      "Epoch: 427/1000 | Batch 040/052 | Cost: 0.2593\n",
      "Epoch: 427/1000 | Batch 050/052 | Cost: 0.1110\n",
      "Epoch: 427/1000 training accuracy: 87.99%\n",
      "Epoch: 428/1000 | Batch 000/052 | Cost: 0.1433\n",
      "Epoch: 428/1000 | Batch 010/052 | Cost: 0.4383\n",
      "Epoch: 428/1000 | Batch 020/052 | Cost: 0.2465\n",
      "Epoch: 428/1000 | Batch 030/052 | Cost: 0.3444\n",
      "Epoch: 428/1000 | Batch 040/052 | Cost: 0.3702\n",
      "Epoch: 428/1000 | Batch 050/052 | Cost: 0.1484\n",
      "Epoch: 428/1000 training accuracy: 87.99%\n",
      "Epoch: 429/1000 | Batch 000/052 | Cost: 0.1601\n",
      "Epoch: 429/1000 | Batch 010/052 | Cost: 0.3353\n",
      "Epoch: 429/1000 | Batch 020/052 | Cost: 0.1741\n",
      "Epoch: 429/1000 | Batch 030/052 | Cost: 0.7945\n",
      "Epoch: 429/1000 | Batch 040/052 | Cost: 0.2699\n",
      "Epoch: 429/1000 | Batch 050/052 | Cost: 0.2415\n",
      "Epoch: 429/1000 training accuracy: 87.77%\n",
      "Epoch: 430/1000 | Batch 000/052 | Cost: 0.3776\n",
      "Epoch: 430/1000 | Batch 010/052 | Cost: 0.3092\n",
      "Epoch: 430/1000 | Batch 020/052 | Cost: 0.5753\n",
      "Epoch: 430/1000 | Batch 030/052 | Cost: 0.1433\n",
      "Epoch: 430/1000 | Batch 040/052 | Cost: 0.3039\n",
      "Epoch: 430/1000 | Batch 050/052 | Cost: 0.3414\n",
      "Epoch: 430/1000 training accuracy: 87.99%\n",
      "Epoch: 431/1000 | Batch 000/052 | Cost: 0.2496\n",
      "Epoch: 431/1000 | Batch 010/052 | Cost: 0.2190\n",
      "Epoch: 431/1000 | Batch 020/052 | Cost: 0.6173\n",
      "Epoch: 431/1000 | Batch 030/052 | Cost: 0.4849\n",
      "Epoch: 431/1000 | Batch 040/052 | Cost: 0.5028\n",
      "Epoch: 431/1000 | Batch 050/052 | Cost: 0.1915\n",
      "Epoch: 431/1000 training accuracy: 87.65%\n",
      "Epoch: 432/1000 | Batch 000/052 | Cost: 0.1340\n",
      "Epoch: 432/1000 | Batch 010/052 | Cost: 0.3295\n",
      "Epoch: 432/1000 | Batch 020/052 | Cost: 0.3486\n",
      "Epoch: 432/1000 | Batch 030/052 | Cost: 0.3173\n",
      "Epoch: 432/1000 | Batch 040/052 | Cost: 0.1126\n",
      "Epoch: 432/1000 | Batch 050/052 | Cost: 0.3660\n",
      "Epoch: 432/1000 training accuracy: 88.78%\n",
      "Epoch: 433/1000 | Batch 000/052 | Cost: 0.4437\n",
      "Epoch: 433/1000 | Batch 010/052 | Cost: 0.2191\n",
      "Epoch: 433/1000 | Batch 020/052 | Cost: 0.1380\n",
      "Epoch: 433/1000 | Batch 030/052 | Cost: 0.3550\n",
      "Epoch: 433/1000 | Batch 040/052 | Cost: 0.1835\n",
      "Epoch: 433/1000 | Batch 050/052 | Cost: 0.1851\n",
      "Epoch: 433/1000 training accuracy: 88.22%\n",
      "Epoch: 434/1000 | Batch 000/052 | Cost: 0.2746\n",
      "Epoch: 434/1000 | Batch 010/052 | Cost: 0.1241\n",
      "Epoch: 434/1000 | Batch 020/052 | Cost: 0.4765\n",
      "Epoch: 434/1000 | Batch 030/052 | Cost: 0.3021\n",
      "Epoch: 434/1000 | Batch 040/052 | Cost: 0.2994\n",
      "Epoch: 434/1000 | Batch 050/052 | Cost: 0.1863\n",
      "Epoch: 434/1000 training accuracy: 88.22%\n",
      "Epoch: 435/1000 | Batch 000/052 | Cost: 0.1799\n",
      "Epoch: 435/1000 | Batch 010/052 | Cost: 0.6093\n",
      "Epoch: 435/1000 | Batch 020/052 | Cost: 0.1834\n",
      "Epoch: 435/1000 | Batch 030/052 | Cost: 0.2523\n",
      "Epoch: 435/1000 | Batch 040/052 | Cost: 0.1374\n",
      "Epoch: 435/1000 | Batch 050/052 | Cost: 0.2795\n",
      "Epoch: 435/1000 training accuracy: 88.66%\n",
      "Epoch: 436/1000 | Batch 000/052 | Cost: 0.5518\n",
      "Epoch: 436/1000 | Batch 010/052 | Cost: 0.2118\n",
      "Epoch: 436/1000 | Batch 020/052 | Cost: 0.3663\n",
      "Epoch: 436/1000 | Batch 030/052 | Cost: 0.3248\n",
      "Epoch: 436/1000 | Batch 040/052 | Cost: 0.1646\n",
      "Epoch: 436/1000 | Batch 050/052 | Cost: 0.2213\n",
      "Epoch: 436/1000 training accuracy: 88.10%\n",
      "Epoch: 437/1000 | Batch 000/052 | Cost: 0.2698\n",
      "Epoch: 437/1000 | Batch 010/052 | Cost: 0.1944\n",
      "Epoch: 437/1000 | Batch 020/052 | Cost: 0.3250\n",
      "Epoch: 437/1000 | Batch 030/052 | Cost: 0.1271\n",
      "Epoch: 437/1000 | Batch 040/052 | Cost: 0.2795\n",
      "Epoch: 437/1000 | Batch 050/052 | Cost: 0.3334\n",
      "Epoch: 437/1000 training accuracy: 87.99%\n",
      "Epoch: 438/1000 | Batch 000/052 | Cost: 0.5692\n",
      "Epoch: 438/1000 | Batch 010/052 | Cost: 0.2733\n",
      "Epoch: 438/1000 | Batch 020/052 | Cost: 0.6218\n",
      "Epoch: 438/1000 | Batch 030/052 | Cost: 0.3424\n",
      "Epoch: 438/1000 | Batch 040/052 | Cost: 0.3316\n",
      "Epoch: 438/1000 | Batch 050/052 | Cost: 0.3098\n",
      "Epoch: 438/1000 training accuracy: 88.66%\n",
      "Epoch: 439/1000 | Batch 000/052 | Cost: 0.3243\n",
      "Epoch: 439/1000 | Batch 010/052 | Cost: 0.1536\n",
      "Epoch: 439/1000 | Batch 020/052 | Cost: 0.2370\n",
      "Epoch: 439/1000 | Batch 030/052 | Cost: 0.2744\n",
      "Epoch: 439/1000 | Batch 040/052 | Cost: 0.1360\n",
      "Epoch: 439/1000 | Batch 050/052 | Cost: 0.2511\n",
      "Epoch: 439/1000 training accuracy: 88.78%\n",
      "Epoch: 440/1000 | Batch 000/052 | Cost: 0.3549\n",
      "Epoch: 440/1000 | Batch 010/052 | Cost: 0.1779\n",
      "Epoch: 440/1000 | Batch 020/052 | Cost: 0.4711\n",
      "Epoch: 440/1000 | Batch 030/052 | Cost: 0.2514\n",
      "Epoch: 440/1000 | Batch 040/052 | Cost: 0.5638\n",
      "Epoch: 440/1000 | Batch 050/052 | Cost: 0.2815\n",
      "Epoch: 440/1000 training accuracy: 87.54%\n",
      "Epoch: 441/1000 | Batch 000/052 | Cost: 0.1022\n",
      "Epoch: 441/1000 | Batch 010/052 | Cost: 0.4326\n",
      "Epoch: 441/1000 | Batch 020/052 | Cost: 0.2751\n",
      "Epoch: 441/1000 | Batch 030/052 | Cost: 0.2407\n",
      "Epoch: 441/1000 | Batch 040/052 | Cost: 0.3944\n",
      "Epoch: 441/1000 | Batch 050/052 | Cost: 0.2437\n",
      "Epoch: 441/1000 training accuracy: 88.22%\n",
      "Epoch: 442/1000 | Batch 000/052 | Cost: 0.1351\n",
      "Epoch: 442/1000 | Batch 010/052 | Cost: 0.2988\n",
      "Epoch: 442/1000 | Batch 020/052 | Cost: 0.6910\n",
      "Epoch: 442/1000 | Batch 030/052 | Cost: 0.3421\n",
      "Epoch: 442/1000 | Batch 040/052 | Cost: 0.1071\n",
      "Epoch: 442/1000 | Batch 050/052 | Cost: 0.2780\n",
      "Epoch: 442/1000 training accuracy: 88.22%\n",
      "Epoch: 443/1000 | Batch 000/052 | Cost: 0.2591\n",
      "Epoch: 443/1000 | Batch 010/052 | Cost: 0.3261\n",
      "Epoch: 443/1000 | Batch 020/052 | Cost: 0.2381\n",
      "Epoch: 443/1000 | Batch 030/052 | Cost: 0.1229\n",
      "Epoch: 443/1000 | Batch 040/052 | Cost: 0.3578\n",
      "Epoch: 443/1000 | Batch 050/052 | Cost: 0.3144\n",
      "Epoch: 443/1000 training accuracy: 88.10%\n",
      "Epoch: 444/1000 | Batch 000/052 | Cost: 0.2916\n",
      "Epoch: 444/1000 | Batch 010/052 | Cost: 0.1608\n",
      "Epoch: 444/1000 | Batch 020/052 | Cost: 0.2942\n",
      "Epoch: 444/1000 | Batch 030/052 | Cost: 0.6972\n",
      "Epoch: 444/1000 | Batch 040/052 | Cost: 0.1598\n",
      "Epoch: 444/1000 | Batch 050/052 | Cost: 0.1742\n",
      "Epoch: 444/1000 training accuracy: 88.55%\n",
      "Epoch: 445/1000 | Batch 000/052 | Cost: 0.6048\n",
      "Epoch: 445/1000 | Batch 010/052 | Cost: 0.2722\n",
      "Epoch: 445/1000 | Batch 020/052 | Cost: 0.2811\n",
      "Epoch: 445/1000 | Batch 030/052 | Cost: 0.3416\n",
      "Epoch: 445/1000 | Batch 040/052 | Cost: 0.3979\n",
      "Epoch: 445/1000 | Batch 050/052 | Cost: 0.1589\n",
      "Epoch: 445/1000 training accuracy: 88.66%\n",
      "Epoch: 446/1000 | Batch 000/052 | Cost: 0.1799\n",
      "Epoch: 446/1000 | Batch 010/052 | Cost: 0.3110\n",
      "Epoch: 446/1000 | Batch 020/052 | Cost: 0.2439\n",
      "Epoch: 446/1000 | Batch 030/052 | Cost: 0.2201\n",
      "Epoch: 446/1000 | Batch 040/052 | Cost: 0.3303\n",
      "Epoch: 446/1000 | Batch 050/052 | Cost: 0.3046\n",
      "Epoch: 446/1000 training accuracy: 88.44%\n",
      "Epoch: 447/1000 | Batch 000/052 | Cost: 0.2597\n",
      "Epoch: 447/1000 | Batch 010/052 | Cost: 0.2588\n",
      "Epoch: 447/1000 | Batch 020/052 | Cost: 0.4595\n",
      "Epoch: 447/1000 | Batch 030/052 | Cost: 0.2633\n",
      "Epoch: 447/1000 | Batch 040/052 | Cost: 0.2020\n",
      "Epoch: 447/1000 | Batch 050/052 | Cost: 0.0736\n",
      "Epoch: 447/1000 training accuracy: 88.10%\n",
      "Epoch: 448/1000 | Batch 000/052 | Cost: 0.3018\n",
      "Epoch: 448/1000 | Batch 010/052 | Cost: 0.1705\n",
      "Epoch: 448/1000 | Batch 020/052 | Cost: 0.1714\n",
      "Epoch: 448/1000 | Batch 030/052 | Cost: 0.6504\n",
      "Epoch: 448/1000 | Batch 040/052 | Cost: 0.3971\n",
      "Epoch: 448/1000 | Batch 050/052 | Cost: 0.2799\n",
      "Epoch: 448/1000 training accuracy: 87.88%\n",
      "Epoch: 449/1000 | Batch 000/052 | Cost: 0.1129\n",
      "Epoch: 449/1000 | Batch 010/052 | Cost: 0.2359\n",
      "Epoch: 449/1000 | Batch 020/052 | Cost: 0.1263\n",
      "Epoch: 449/1000 | Batch 030/052 | Cost: 0.2035\n",
      "Epoch: 449/1000 | Batch 040/052 | Cost: 0.6900\n",
      "Epoch: 449/1000 | Batch 050/052 | Cost: 0.3434\n",
      "Epoch: 449/1000 training accuracy: 88.44%\n",
      "Epoch: 450/1000 | Batch 000/052 | Cost: 0.2286\n",
      "Epoch: 450/1000 | Batch 010/052 | Cost: 0.4433\n",
      "Epoch: 450/1000 | Batch 020/052 | Cost: 0.2829\n",
      "Epoch: 450/1000 | Batch 030/052 | Cost: 0.4611\n",
      "Epoch: 450/1000 | Batch 040/052 | Cost: 0.2467\n",
      "Epoch: 450/1000 | Batch 050/052 | Cost: 0.2679\n",
      "Epoch: 450/1000 training accuracy: 86.98%\n",
      "Epoch: 451/1000 | Batch 000/052 | Cost: 0.4152\n",
      "Epoch: 451/1000 | Batch 010/052 | Cost: 0.3304\n",
      "Epoch: 451/1000 | Batch 020/052 | Cost: 0.2634\n",
      "Epoch: 451/1000 | Batch 030/052 | Cost: 0.2610\n",
      "Epoch: 451/1000 | Batch 040/052 | Cost: 0.2235\n",
      "Epoch: 451/1000 | Batch 050/052 | Cost: 0.1266\n",
      "Epoch: 451/1000 training accuracy: 87.99%\n",
      "Epoch: 452/1000 | Batch 000/052 | Cost: 0.2035\n",
      "Epoch: 452/1000 | Batch 010/052 | Cost: 0.4310\n",
      "Epoch: 452/1000 | Batch 020/052 | Cost: 0.3816\n",
      "Epoch: 452/1000 | Batch 030/052 | Cost: 0.2890\n",
      "Epoch: 452/1000 | Batch 040/052 | Cost: 0.2767\n",
      "Epoch: 452/1000 | Batch 050/052 | Cost: 0.3144\n",
      "Epoch: 452/1000 training accuracy: 87.99%\n",
      "Epoch: 453/1000 | Batch 000/052 | Cost: 0.2384\n",
      "Epoch: 453/1000 | Batch 010/052 | Cost: 0.1656\n",
      "Epoch: 453/1000 | Batch 020/052 | Cost: 0.3453\n",
      "Epoch: 453/1000 | Batch 030/052 | Cost: 0.3272\n",
      "Epoch: 453/1000 | Batch 040/052 | Cost: 0.3179\n",
      "Epoch: 453/1000 | Batch 050/052 | Cost: 0.3814\n",
      "Epoch: 453/1000 training accuracy: 89.00%\n",
      "Epoch: 454/1000 | Batch 000/052 | Cost: 0.2144\n",
      "Epoch: 454/1000 | Batch 010/052 | Cost: 0.1977\n",
      "Epoch: 454/1000 | Batch 020/052 | Cost: 0.1674\n",
      "Epoch: 454/1000 | Batch 030/052 | Cost: 0.4707\n",
      "Epoch: 454/1000 | Batch 040/052 | Cost: 0.1698\n",
      "Epoch: 454/1000 | Batch 050/052 | Cost: 0.2579\n",
      "Epoch: 454/1000 training accuracy: 87.77%\n",
      "Epoch: 455/1000 | Batch 000/052 | Cost: 0.1699\n",
      "Epoch: 455/1000 | Batch 010/052 | Cost: 0.6503\n",
      "Epoch: 455/1000 | Batch 020/052 | Cost: 0.0855\n",
      "Epoch: 455/1000 | Batch 030/052 | Cost: 0.4202\n",
      "Epoch: 455/1000 | Batch 040/052 | Cost: 0.3066\n",
      "Epoch: 455/1000 | Batch 050/052 | Cost: 0.4126\n",
      "Epoch: 455/1000 training accuracy: 88.10%\n",
      "Epoch: 456/1000 | Batch 000/052 | Cost: 0.3183\n",
      "Epoch: 456/1000 | Batch 010/052 | Cost: 0.1749\n",
      "Epoch: 456/1000 | Batch 020/052 | Cost: 0.5684\n",
      "Epoch: 456/1000 | Batch 030/052 | Cost: 0.0788\n",
      "Epoch: 456/1000 | Batch 040/052 | Cost: 0.2626\n",
      "Epoch: 456/1000 | Batch 050/052 | Cost: 0.4918\n",
      "Epoch: 456/1000 training accuracy: 87.32%\n",
      "Epoch: 457/1000 | Batch 000/052 | Cost: 0.1712\n",
      "Epoch: 457/1000 | Batch 010/052 | Cost: 0.3798\n",
      "Epoch: 457/1000 | Batch 020/052 | Cost: 0.2237\n",
      "Epoch: 457/1000 | Batch 030/052 | Cost: 0.1747\n",
      "Epoch: 457/1000 | Batch 040/052 | Cost: 0.4153\n",
      "Epoch: 457/1000 | Batch 050/052 | Cost: 0.0875\n",
      "Epoch: 457/1000 training accuracy: 88.33%\n",
      "Epoch: 458/1000 | Batch 000/052 | Cost: 0.5262\n",
      "Epoch: 458/1000 | Batch 010/052 | Cost: 0.1726\n",
      "Epoch: 458/1000 | Batch 020/052 | Cost: 0.2394\n",
      "Epoch: 458/1000 | Batch 030/052 | Cost: 0.2287\n",
      "Epoch: 458/1000 | Batch 040/052 | Cost: 0.0860\n",
      "Epoch: 458/1000 | Batch 050/052 | Cost: 0.3674\n",
      "Epoch: 458/1000 training accuracy: 87.99%\n",
      "Epoch: 459/1000 | Batch 000/052 | Cost: 0.1937\n",
      "Epoch: 459/1000 | Batch 010/052 | Cost: 0.5118\n",
      "Epoch: 459/1000 | Batch 020/052 | Cost: 0.7413\n",
      "Epoch: 459/1000 | Batch 030/052 | Cost: 0.3300\n",
      "Epoch: 459/1000 | Batch 040/052 | Cost: 0.2546\n",
      "Epoch: 459/1000 | Batch 050/052 | Cost: 0.1085\n",
      "Epoch: 459/1000 training accuracy: 87.88%\n",
      "Epoch: 460/1000 | Batch 000/052 | Cost: 0.2362\n",
      "Epoch: 460/1000 | Batch 010/052 | Cost: 0.1520\n",
      "Epoch: 460/1000 | Batch 020/052 | Cost: 0.3291\n",
      "Epoch: 460/1000 | Batch 030/052 | Cost: 0.2404\n",
      "Epoch: 460/1000 | Batch 040/052 | Cost: 0.1189\n",
      "Epoch: 460/1000 | Batch 050/052 | Cost: 0.5449\n",
      "Epoch: 460/1000 training accuracy: 88.89%\n",
      "Epoch: 461/1000 | Batch 000/052 | Cost: 0.4326\n",
      "Epoch: 461/1000 | Batch 010/052 | Cost: 0.1236\n",
      "Epoch: 461/1000 | Batch 020/052 | Cost: 0.3070\n",
      "Epoch: 461/1000 | Batch 030/052 | Cost: 0.3596\n",
      "Epoch: 461/1000 | Batch 040/052 | Cost: 0.3345\n",
      "Epoch: 461/1000 | Batch 050/052 | Cost: 0.5747\n",
      "Epoch: 461/1000 training accuracy: 86.64%\n",
      "Epoch: 462/1000 | Batch 000/052 | Cost: 0.1729\n",
      "Epoch: 462/1000 | Batch 010/052 | Cost: 0.5094\n",
      "Epoch: 462/1000 | Batch 020/052 | Cost: 0.2563\n",
      "Epoch: 462/1000 | Batch 030/052 | Cost: 0.4451\n",
      "Epoch: 462/1000 | Batch 040/052 | Cost: 0.2649\n",
      "Epoch: 462/1000 | Batch 050/052 | Cost: 0.1917\n",
      "Epoch: 462/1000 training accuracy: 87.99%\n",
      "Epoch: 463/1000 | Batch 000/052 | Cost: 0.3158\n",
      "Epoch: 463/1000 | Batch 010/052 | Cost: 0.3069\n",
      "Epoch: 463/1000 | Batch 020/052 | Cost: 0.2960\n",
      "Epoch: 463/1000 | Batch 030/052 | Cost: 0.1509\n",
      "Epoch: 463/1000 | Batch 040/052 | Cost: 0.3497\n",
      "Epoch: 463/1000 | Batch 050/052 | Cost: 0.3560\n",
      "Epoch: 463/1000 training accuracy: 87.99%\n",
      "Epoch: 464/1000 | Batch 000/052 | Cost: 0.5684\n",
      "Epoch: 464/1000 | Batch 010/052 | Cost: 0.1684\n",
      "Epoch: 464/1000 | Batch 020/052 | Cost: 0.1781\n",
      "Epoch: 464/1000 | Batch 030/052 | Cost: 0.2716\n",
      "Epoch: 464/1000 | Batch 040/052 | Cost: 0.2582\n",
      "Epoch: 464/1000 | Batch 050/052 | Cost: 0.1259\n",
      "Epoch: 464/1000 training accuracy: 87.54%\n",
      "Epoch: 465/1000 | Batch 000/052 | Cost: 0.1622\n",
      "Epoch: 465/1000 | Batch 010/052 | Cost: 0.2001\n",
      "Epoch: 465/1000 | Batch 020/052 | Cost: 0.4643\n",
      "Epoch: 465/1000 | Batch 030/052 | Cost: 0.3058\n",
      "Epoch: 465/1000 | Batch 040/052 | Cost: 0.3559\n",
      "Epoch: 465/1000 | Batch 050/052 | Cost: 0.3116\n",
      "Epoch: 465/1000 training accuracy: 87.21%\n",
      "Epoch: 466/1000 | Batch 000/052 | Cost: 0.1719\n",
      "Epoch: 466/1000 | Batch 010/052 | Cost: 0.1710\n",
      "Epoch: 466/1000 | Batch 020/052 | Cost: 0.1474\n",
      "Epoch: 466/1000 | Batch 030/052 | Cost: 0.3163\n",
      "Epoch: 466/1000 | Batch 040/052 | Cost: 0.4676\n",
      "Epoch: 466/1000 | Batch 050/052 | Cost: 0.1988\n",
      "Epoch: 466/1000 training accuracy: 87.65%\n",
      "Epoch: 467/1000 | Batch 000/052 | Cost: 0.5655\n",
      "Epoch: 467/1000 | Batch 010/052 | Cost: 0.1310\n",
      "Epoch: 467/1000 | Batch 020/052 | Cost: 0.4226\n",
      "Epoch: 467/1000 | Batch 030/052 | Cost: 0.2008\n",
      "Epoch: 467/1000 | Batch 040/052 | Cost: 0.3879\n",
      "Epoch: 467/1000 | Batch 050/052 | Cost: 0.1681\n",
      "Epoch: 467/1000 training accuracy: 86.98%\n",
      "Epoch: 468/1000 | Batch 000/052 | Cost: 0.1680\n",
      "Epoch: 468/1000 | Batch 010/052 | Cost: 0.4006\n",
      "Epoch: 468/1000 | Batch 020/052 | Cost: 0.4780\n",
      "Epoch: 468/1000 | Batch 030/052 | Cost: 0.3405\n",
      "Epoch: 468/1000 | Batch 040/052 | Cost: 0.4607\n",
      "Epoch: 468/1000 | Batch 050/052 | Cost: 0.0654\n",
      "Epoch: 468/1000 training accuracy: 88.22%\n",
      "Epoch: 469/1000 | Batch 000/052 | Cost: 0.1610\n",
      "Epoch: 469/1000 | Batch 010/052 | Cost: 0.1620\n",
      "Epoch: 469/1000 | Batch 020/052 | Cost: 0.4082\n",
      "Epoch: 469/1000 | Batch 030/052 | Cost: 0.1901\n",
      "Epoch: 469/1000 | Batch 040/052 | Cost: 0.0955\n",
      "Epoch: 469/1000 | Batch 050/052 | Cost: 0.3751\n",
      "Epoch: 469/1000 training accuracy: 87.32%\n",
      "Epoch: 470/1000 | Batch 000/052 | Cost: 0.1552\n",
      "Epoch: 470/1000 | Batch 010/052 | Cost: 0.2347\n",
      "Epoch: 470/1000 | Batch 020/052 | Cost: 0.2668\n",
      "Epoch: 470/1000 | Batch 030/052 | Cost: 0.3421\n",
      "Epoch: 470/1000 | Batch 040/052 | Cost: 0.2082\n",
      "Epoch: 470/1000 | Batch 050/052 | Cost: 0.2124\n",
      "Epoch: 470/1000 training accuracy: 88.89%\n",
      "Epoch: 471/1000 | Batch 000/052 | Cost: 0.3279\n",
      "Epoch: 471/1000 | Batch 010/052 | Cost: 0.1792\n",
      "Epoch: 471/1000 | Batch 020/052 | Cost: 0.1005\n",
      "Epoch: 471/1000 | Batch 030/052 | Cost: 0.1301\n",
      "Epoch: 471/1000 | Batch 040/052 | Cost: 0.0991\n",
      "Epoch: 471/1000 | Batch 050/052 | Cost: 0.2297\n",
      "Epoch: 471/1000 training accuracy: 87.77%\n",
      "Epoch: 472/1000 | Batch 000/052 | Cost: 0.5586\n",
      "Epoch: 472/1000 | Batch 010/052 | Cost: 0.1733\n",
      "Epoch: 472/1000 | Batch 020/052 | Cost: 0.4976\n",
      "Epoch: 472/1000 | Batch 030/052 | Cost: 0.3019\n",
      "Epoch: 472/1000 | Batch 040/052 | Cost: 0.2573\n",
      "Epoch: 472/1000 | Batch 050/052 | Cost: 0.1618\n",
      "Epoch: 472/1000 training accuracy: 88.78%\n",
      "Epoch: 473/1000 | Batch 000/052 | Cost: 0.1664\n",
      "Epoch: 473/1000 | Batch 010/052 | Cost: 0.3832\n",
      "Epoch: 473/1000 | Batch 020/052 | Cost: 0.1552\n",
      "Epoch: 473/1000 | Batch 030/052 | Cost: 0.4031\n",
      "Epoch: 473/1000 | Batch 040/052 | Cost: 0.2873\n",
      "Epoch: 473/1000 | Batch 050/052 | Cost: 0.5541\n",
      "Epoch: 473/1000 training accuracy: 86.76%\n",
      "Epoch: 474/1000 | Batch 000/052 | Cost: 0.2708\n",
      "Epoch: 474/1000 | Batch 010/052 | Cost: 0.2240\n",
      "Epoch: 474/1000 | Batch 020/052 | Cost: 0.1715\n",
      "Epoch: 474/1000 | Batch 030/052 | Cost: 0.1875\n",
      "Epoch: 474/1000 | Batch 040/052 | Cost: 0.2838\n",
      "Epoch: 474/1000 | Batch 050/052 | Cost: 0.2373\n",
      "Epoch: 474/1000 training accuracy: 87.99%\n",
      "Epoch: 475/1000 | Batch 000/052 | Cost: 0.1713\n",
      "Epoch: 475/1000 | Batch 010/052 | Cost: 0.4247\n",
      "Epoch: 475/1000 | Batch 020/052 | Cost: 0.3117\n",
      "Epoch: 475/1000 | Batch 030/052 | Cost: 0.4459\n",
      "Epoch: 475/1000 | Batch 040/052 | Cost: 0.3784\n",
      "Epoch: 475/1000 | Batch 050/052 | Cost: 0.1848\n",
      "Epoch: 475/1000 training accuracy: 87.43%\n",
      "Epoch: 476/1000 | Batch 000/052 | Cost: 0.4617\n",
      "Epoch: 476/1000 | Batch 010/052 | Cost: 0.0944\n",
      "Epoch: 476/1000 | Batch 020/052 | Cost: 0.2205\n",
      "Epoch: 476/1000 | Batch 030/052 | Cost: 0.2529\n",
      "Epoch: 476/1000 | Batch 040/052 | Cost: 0.3193\n",
      "Epoch: 476/1000 | Batch 050/052 | Cost: 0.2328\n",
      "Epoch: 476/1000 training accuracy: 88.10%\n",
      "Epoch: 477/1000 | Batch 000/052 | Cost: 0.1709\n",
      "Epoch: 477/1000 | Batch 010/052 | Cost: 0.3223\n",
      "Epoch: 477/1000 | Batch 020/052 | Cost: 0.1503\n",
      "Epoch: 477/1000 | Batch 030/052 | Cost: 0.2774\n",
      "Epoch: 477/1000 | Batch 040/052 | Cost: 0.4681\n",
      "Epoch: 477/1000 | Batch 050/052 | Cost: 0.1538\n",
      "Epoch: 477/1000 training accuracy: 87.88%\n",
      "Epoch: 478/1000 | Batch 000/052 | Cost: 0.1674\n",
      "Epoch: 478/1000 | Batch 010/052 | Cost: 0.2574\n",
      "Epoch: 478/1000 | Batch 020/052 | Cost: 0.1778\n",
      "Epoch: 478/1000 | Batch 030/052 | Cost: 0.1765\n",
      "Epoch: 478/1000 | Batch 040/052 | Cost: 0.4734\n",
      "Epoch: 478/1000 | Batch 050/052 | Cost: 0.2323\n",
      "Epoch: 478/1000 training accuracy: 87.99%\n",
      "Epoch: 479/1000 | Batch 000/052 | Cost: 0.3013\n",
      "Epoch: 479/1000 | Batch 010/052 | Cost: 0.3080\n",
      "Epoch: 479/1000 | Batch 020/052 | Cost: 0.1003\n",
      "Epoch: 479/1000 | Batch 030/052 | Cost: 0.2370\n",
      "Epoch: 479/1000 | Batch 040/052 | Cost: 0.1612\n",
      "Epoch: 479/1000 | Batch 050/052 | Cost: 0.2384\n",
      "Epoch: 479/1000 training accuracy: 88.22%\n",
      "Epoch: 480/1000 | Batch 000/052 | Cost: 0.2360\n",
      "Epoch: 480/1000 | Batch 010/052 | Cost: 0.3267\n",
      "Epoch: 480/1000 | Batch 020/052 | Cost: 0.2375\n",
      "Epoch: 480/1000 | Batch 030/052 | Cost: 0.4535\n",
      "Epoch: 480/1000 | Batch 040/052 | Cost: 0.3322\n",
      "Epoch: 480/1000 | Batch 050/052 | Cost: 0.1270\n",
      "Epoch: 480/1000 training accuracy: 87.88%\n",
      "Epoch: 481/1000 | Batch 000/052 | Cost: 0.1117\n",
      "Epoch: 481/1000 | Batch 010/052 | Cost: 0.2716\n",
      "Epoch: 481/1000 | Batch 020/052 | Cost: 0.3678\n",
      "Epoch: 481/1000 | Batch 030/052 | Cost: 0.2730\n",
      "Epoch: 481/1000 | Batch 040/052 | Cost: 0.3292\n",
      "Epoch: 481/1000 | Batch 050/052 | Cost: 0.4828\n",
      "Epoch: 481/1000 training accuracy: 87.32%\n",
      "Epoch: 482/1000 | Batch 000/052 | Cost: 0.3673\n",
      "Epoch: 482/1000 | Batch 010/052 | Cost: 0.4634\n",
      "Epoch: 482/1000 | Batch 020/052 | Cost: 0.1577\n",
      "Epoch: 482/1000 | Batch 030/052 | Cost: 0.2695\n",
      "Epoch: 482/1000 | Batch 040/052 | Cost: 0.1286\n",
      "Epoch: 482/1000 | Batch 050/052 | Cost: 0.4790\n",
      "Epoch: 482/1000 training accuracy: 87.65%\n",
      "Epoch: 483/1000 | Batch 000/052 | Cost: 0.7187\n",
      "Epoch: 483/1000 | Batch 010/052 | Cost: 0.1484\n",
      "Epoch: 483/1000 | Batch 020/052 | Cost: 0.1634\n",
      "Epoch: 483/1000 | Batch 030/052 | Cost: 0.1858\n",
      "Epoch: 483/1000 | Batch 040/052 | Cost: 0.1689\n",
      "Epoch: 483/1000 | Batch 050/052 | Cost: 0.3059\n",
      "Epoch: 483/1000 training accuracy: 88.44%\n",
      "Epoch: 484/1000 | Batch 000/052 | Cost: 0.2904\n",
      "Epoch: 484/1000 | Batch 010/052 | Cost: 0.1415\n",
      "Epoch: 484/1000 | Batch 020/052 | Cost: 0.2289\n",
      "Epoch: 484/1000 | Batch 030/052 | Cost: 0.2096\n",
      "Epoch: 484/1000 | Batch 040/052 | Cost: 0.2890\n",
      "Epoch: 484/1000 | Batch 050/052 | Cost: 0.2163\n",
      "Epoch: 484/1000 training accuracy: 88.22%\n",
      "Epoch: 485/1000 | Batch 000/052 | Cost: 0.1536\n",
      "Epoch: 485/1000 | Batch 010/052 | Cost: 0.1080\n",
      "Epoch: 485/1000 | Batch 020/052 | Cost: 0.1301\n",
      "Epoch: 485/1000 | Batch 030/052 | Cost: 0.3575\n",
      "Epoch: 485/1000 | Batch 040/052 | Cost: 0.2905\n",
      "Epoch: 485/1000 | Batch 050/052 | Cost: 0.2989\n",
      "Epoch: 485/1000 training accuracy: 88.22%\n",
      "Epoch: 486/1000 | Batch 000/052 | Cost: 0.4216\n",
      "Epoch: 486/1000 | Batch 010/052 | Cost: 0.2472\n",
      "Epoch: 486/1000 | Batch 020/052 | Cost: 0.1069\n",
      "Epoch: 486/1000 | Batch 030/052 | Cost: 0.1510\n",
      "Epoch: 486/1000 | Batch 040/052 | Cost: 0.3882\n",
      "Epoch: 486/1000 | Batch 050/052 | Cost: 0.2932\n",
      "Epoch: 486/1000 training accuracy: 88.33%\n",
      "Epoch: 487/1000 | Batch 000/052 | Cost: 0.2045\n",
      "Epoch: 487/1000 | Batch 010/052 | Cost: 0.2919\n",
      "Epoch: 487/1000 | Batch 020/052 | Cost: 0.1694\n",
      "Epoch: 487/1000 | Batch 030/052 | Cost: 0.2931\n",
      "Epoch: 487/1000 | Batch 040/052 | Cost: 0.3802\n",
      "Epoch: 487/1000 | Batch 050/052 | Cost: 0.4392\n",
      "Epoch: 487/1000 training accuracy: 88.44%\n",
      "Epoch: 488/1000 | Batch 000/052 | Cost: 0.2079\n",
      "Epoch: 488/1000 | Batch 010/052 | Cost: 0.4481\n",
      "Epoch: 488/1000 | Batch 020/052 | Cost: 0.3198\n",
      "Epoch: 488/1000 | Batch 030/052 | Cost: 0.1539\n",
      "Epoch: 488/1000 | Batch 040/052 | Cost: 0.3451\n",
      "Epoch: 488/1000 | Batch 050/052 | Cost: 0.1567\n",
      "Epoch: 488/1000 training accuracy: 88.55%\n",
      "Epoch: 489/1000 | Batch 000/052 | Cost: 0.1261\n",
      "Epoch: 489/1000 | Batch 010/052 | Cost: 0.1303\n",
      "Epoch: 489/1000 | Batch 020/052 | Cost: 0.1263\n",
      "Epoch: 489/1000 | Batch 030/052 | Cost: 0.1386\n",
      "Epoch: 489/1000 | Batch 040/052 | Cost: 0.2375\n",
      "Epoch: 489/1000 | Batch 050/052 | Cost: 0.1792\n",
      "Epoch: 489/1000 training accuracy: 87.77%\n",
      "Epoch: 490/1000 | Batch 000/052 | Cost: 0.6786\n",
      "Epoch: 490/1000 | Batch 010/052 | Cost: 0.1789\n",
      "Epoch: 490/1000 | Batch 020/052 | Cost: 0.2167\n",
      "Epoch: 490/1000 | Batch 030/052 | Cost: 0.2517\n",
      "Epoch: 490/1000 | Batch 040/052 | Cost: 0.2836\n",
      "Epoch: 490/1000 | Batch 050/052 | Cost: 0.3291\n",
      "Epoch: 490/1000 training accuracy: 88.44%\n",
      "Epoch: 491/1000 | Batch 000/052 | Cost: 0.5546\n",
      "Epoch: 491/1000 | Batch 010/052 | Cost: 0.2613\n",
      "Epoch: 491/1000 | Batch 020/052 | Cost: 0.4157\n",
      "Epoch: 491/1000 | Batch 030/052 | Cost: 0.1693\n",
      "Epoch: 491/1000 | Batch 040/052 | Cost: 0.4601\n",
      "Epoch: 491/1000 | Batch 050/052 | Cost: 0.2266\n",
      "Epoch: 491/1000 training accuracy: 87.54%\n",
      "Epoch: 492/1000 | Batch 000/052 | Cost: 0.3661\n",
      "Epoch: 492/1000 | Batch 010/052 | Cost: 0.1424\n",
      "Epoch: 492/1000 | Batch 020/052 | Cost: 0.1256\n",
      "Epoch: 492/1000 | Batch 030/052 | Cost: 0.2890\n",
      "Epoch: 492/1000 | Batch 040/052 | Cost: 0.1559\n",
      "Epoch: 492/1000 | Batch 050/052 | Cost: 0.2495\n",
      "Epoch: 492/1000 training accuracy: 88.44%\n",
      "Epoch: 493/1000 | Batch 000/052 | Cost: 0.2278\n",
      "Epoch: 493/1000 | Batch 010/052 | Cost: 0.1473\n",
      "Epoch: 493/1000 | Batch 020/052 | Cost: 0.2062\n",
      "Epoch: 493/1000 | Batch 030/052 | Cost: 0.2491\n",
      "Epoch: 493/1000 | Batch 040/052 | Cost: 0.2869\n",
      "Epoch: 493/1000 | Batch 050/052 | Cost: 0.3273\n",
      "Epoch: 493/1000 training accuracy: 87.77%\n",
      "Epoch: 494/1000 | Batch 000/052 | Cost: 0.2257\n",
      "Epoch: 494/1000 | Batch 010/052 | Cost: 0.2298\n",
      "Epoch: 494/1000 | Batch 020/052 | Cost: 0.1645\n",
      "Epoch: 494/1000 | Batch 030/052 | Cost: 0.0915\n",
      "Epoch: 494/1000 | Batch 040/052 | Cost: 0.1635\n",
      "Epoch: 494/1000 | Batch 050/052 | Cost: 0.1315\n",
      "Epoch: 494/1000 training accuracy: 87.21%\n",
      "Epoch: 495/1000 | Batch 000/052 | Cost: 0.2971\n",
      "Epoch: 495/1000 | Batch 010/052 | Cost: 0.6847\n",
      "Epoch: 495/1000 | Batch 020/052 | Cost: 0.2997\n",
      "Epoch: 495/1000 | Batch 030/052 | Cost: 0.1898\n",
      "Epoch: 495/1000 | Batch 040/052 | Cost: 0.1318\n",
      "Epoch: 495/1000 | Batch 050/052 | Cost: 0.1166\n",
      "Epoch: 495/1000 training accuracy: 87.99%\n",
      "Epoch: 496/1000 | Batch 000/052 | Cost: 0.2227\n",
      "Epoch: 496/1000 | Batch 010/052 | Cost: 0.1737\n",
      "Epoch: 496/1000 | Batch 020/052 | Cost: 0.1156\n",
      "Epoch: 496/1000 | Batch 030/052 | Cost: 0.1944\n",
      "Epoch: 496/1000 | Batch 040/052 | Cost: 0.2385\n",
      "Epoch: 496/1000 | Batch 050/052 | Cost: 0.4541\n",
      "Epoch: 496/1000 training accuracy: 87.65%\n",
      "Epoch: 497/1000 | Batch 000/052 | Cost: 0.3174\n",
      "Epoch: 497/1000 | Batch 010/052 | Cost: 0.4403\n",
      "Epoch: 497/1000 | Batch 020/052 | Cost: 0.2535\n",
      "Epoch: 497/1000 | Batch 030/052 | Cost: 0.4773\n",
      "Epoch: 497/1000 | Batch 040/052 | Cost: 0.3926\n",
      "Epoch: 497/1000 | Batch 050/052 | Cost: 0.5070\n",
      "Epoch: 497/1000 training accuracy: 87.88%\n",
      "Epoch: 498/1000 | Batch 000/052 | Cost: 0.2876\n",
      "Epoch: 498/1000 | Batch 010/052 | Cost: 0.3300\n",
      "Epoch: 498/1000 | Batch 020/052 | Cost: 0.3826\n",
      "Epoch: 498/1000 | Batch 030/052 | Cost: 0.2029\n",
      "Epoch: 498/1000 | Batch 040/052 | Cost: 0.2666\n",
      "Epoch: 498/1000 | Batch 050/052 | Cost: 0.3751\n",
      "Epoch: 498/1000 training accuracy: 88.33%\n",
      "Epoch: 499/1000 | Batch 000/052 | Cost: 0.0960\n",
      "Epoch: 499/1000 | Batch 010/052 | Cost: 0.6351\n",
      "Epoch: 499/1000 | Batch 020/052 | Cost: 0.1392\n",
      "Epoch: 499/1000 | Batch 030/052 | Cost: 0.4082\n",
      "Epoch: 499/1000 | Batch 040/052 | Cost: 0.2127\n",
      "Epoch: 499/1000 | Batch 050/052 | Cost: 0.2438\n",
      "Epoch: 499/1000 training accuracy: 87.65%\n",
      "Epoch: 500/1000 | Batch 000/052 | Cost: 0.2065\n",
      "Epoch: 500/1000 | Batch 010/052 | Cost: 0.4194\n",
      "Epoch: 500/1000 | Batch 020/052 | Cost: 0.0992\n",
      "Epoch: 500/1000 | Batch 030/052 | Cost: 0.3583\n",
      "Epoch: 500/1000 | Batch 040/052 | Cost: 0.3177\n",
      "Epoch: 500/1000 | Batch 050/052 | Cost: 0.3468\n",
      "Epoch: 500/1000 training accuracy: 87.43%\n",
      "Epoch: 501/1000 | Batch 000/052 | Cost: 0.6903\n",
      "Epoch: 501/1000 | Batch 010/052 | Cost: 0.2177\n",
      "Epoch: 501/1000 | Batch 020/052 | Cost: 0.2754\n",
      "Epoch: 501/1000 | Batch 030/052 | Cost: 0.2978\n",
      "Epoch: 501/1000 | Batch 040/052 | Cost: 0.1075\n",
      "Epoch: 501/1000 | Batch 050/052 | Cost: 0.2876\n",
      "Epoch: 501/1000 training accuracy: 89.00%\n",
      "Epoch: 502/1000 | Batch 000/052 | Cost: 0.2795\n",
      "Epoch: 502/1000 | Batch 010/052 | Cost: 0.4943\n",
      "Epoch: 502/1000 | Batch 020/052 | Cost: 0.4736\n",
      "Epoch: 502/1000 | Batch 030/052 | Cost: 0.4299\n",
      "Epoch: 502/1000 | Batch 040/052 | Cost: 0.0668\n",
      "Epoch: 502/1000 | Batch 050/052 | Cost: 0.3411\n",
      "Epoch: 502/1000 training accuracy: 87.32%\n",
      "Epoch: 503/1000 | Batch 000/052 | Cost: 0.2227\n",
      "Epoch: 503/1000 | Batch 010/052 | Cost: 0.2399\n",
      "Epoch: 503/1000 | Batch 020/052 | Cost: 0.2532\n",
      "Epoch: 503/1000 | Batch 030/052 | Cost: 0.1278\n",
      "Epoch: 503/1000 | Batch 040/052 | Cost: 0.2567\n",
      "Epoch: 503/1000 | Batch 050/052 | Cost: 0.3022\n",
      "Epoch: 503/1000 training accuracy: 87.77%\n",
      "Epoch: 504/1000 | Batch 000/052 | Cost: 0.1244\n",
      "Epoch: 504/1000 | Batch 010/052 | Cost: 0.2807\n",
      "Epoch: 504/1000 | Batch 020/052 | Cost: 0.2777\n",
      "Epoch: 504/1000 | Batch 030/052 | Cost: 0.4221\n",
      "Epoch: 504/1000 | Batch 040/052 | Cost: 0.1757\n",
      "Epoch: 504/1000 | Batch 050/052 | Cost: 0.1325\n",
      "Epoch: 504/1000 training accuracy: 87.54%\n",
      "Epoch: 505/1000 | Batch 000/052 | Cost: 0.3142\n",
      "Epoch: 505/1000 | Batch 010/052 | Cost: 0.4184\n",
      "Epoch: 505/1000 | Batch 020/052 | Cost: 0.2280\n",
      "Epoch: 505/1000 | Batch 030/052 | Cost: 0.3679\n",
      "Epoch: 505/1000 | Batch 040/052 | Cost: 0.2942\n",
      "Epoch: 505/1000 | Batch 050/052 | Cost: 0.4066\n",
      "Epoch: 505/1000 training accuracy: 87.32%\n",
      "Epoch: 506/1000 | Batch 000/052 | Cost: 0.2763\n",
      "Epoch: 506/1000 | Batch 010/052 | Cost: 0.4275\n",
      "Epoch: 506/1000 | Batch 020/052 | Cost: 0.3430\n",
      "Epoch: 506/1000 | Batch 030/052 | Cost: 0.1455\n",
      "Epoch: 506/1000 | Batch 040/052 | Cost: 0.1745\n",
      "Epoch: 506/1000 | Batch 050/052 | Cost: 0.2198\n",
      "Epoch: 506/1000 training accuracy: 87.77%\n",
      "Epoch: 507/1000 | Batch 000/052 | Cost: 0.3321\n",
      "Epoch: 507/1000 | Batch 010/052 | Cost: 0.1854\n",
      "Epoch: 507/1000 | Batch 020/052 | Cost: 0.1964\n",
      "Epoch: 507/1000 | Batch 030/052 | Cost: 0.3027\n",
      "Epoch: 507/1000 | Batch 040/052 | Cost: 0.1680\n",
      "Epoch: 507/1000 | Batch 050/052 | Cost: 0.2587\n",
      "Epoch: 507/1000 training accuracy: 87.77%\n",
      "Epoch: 508/1000 | Batch 000/052 | Cost: 0.3438\n",
      "Epoch: 508/1000 | Batch 010/052 | Cost: 0.4659\n",
      "Epoch: 508/1000 | Batch 020/052 | Cost: 0.1908\n",
      "Epoch: 508/1000 | Batch 030/052 | Cost: 0.4523\n",
      "Epoch: 508/1000 | Batch 040/052 | Cost: 0.3256\n",
      "Epoch: 508/1000 | Batch 050/052 | Cost: 0.5025\n",
      "Epoch: 508/1000 training accuracy: 87.32%\n",
      "Epoch: 509/1000 | Batch 000/052 | Cost: 0.2580\n",
      "Epoch: 509/1000 | Batch 010/052 | Cost: 0.2200\n",
      "Epoch: 509/1000 | Batch 020/052 | Cost: 0.1584\n",
      "Epoch: 509/1000 | Batch 030/052 | Cost: 0.2408\n",
      "Epoch: 509/1000 | Batch 040/052 | Cost: 0.2406\n",
      "Epoch: 509/1000 | Batch 050/052 | Cost: 0.3860\n",
      "Epoch: 509/1000 training accuracy: 87.77%\n",
      "Epoch: 510/1000 | Batch 000/052 | Cost: 0.0555\n",
      "Epoch: 510/1000 | Batch 010/052 | Cost: 0.2271\n",
      "Epoch: 510/1000 | Batch 020/052 | Cost: 0.2706\n",
      "Epoch: 510/1000 | Batch 030/052 | Cost: 0.2767\n",
      "Epoch: 510/1000 | Batch 040/052 | Cost: 0.4343\n",
      "Epoch: 510/1000 | Batch 050/052 | Cost: 0.2700\n",
      "Epoch: 510/1000 training accuracy: 87.99%\n",
      "Epoch: 511/1000 | Batch 000/052 | Cost: 0.4874\n",
      "Epoch: 511/1000 | Batch 010/052 | Cost: 0.1287\n",
      "Epoch: 511/1000 | Batch 020/052 | Cost: 0.4724\n",
      "Epoch: 511/1000 | Batch 030/052 | Cost: 0.1169\n",
      "Epoch: 511/1000 | Batch 040/052 | Cost: 0.2527\n",
      "Epoch: 511/1000 | Batch 050/052 | Cost: 0.5079\n",
      "Epoch: 511/1000 training accuracy: 88.55%\n",
      "Epoch: 512/1000 | Batch 000/052 | Cost: 0.5163\n",
      "Epoch: 512/1000 | Batch 010/052 | Cost: 0.4033\n",
      "Epoch: 512/1000 | Batch 020/052 | Cost: 0.2758\n",
      "Epoch: 512/1000 | Batch 030/052 | Cost: 0.2234\n",
      "Epoch: 512/1000 | Batch 040/052 | Cost: 0.1809\n",
      "Epoch: 512/1000 | Batch 050/052 | Cost: 0.1935\n",
      "Epoch: 512/1000 training accuracy: 87.77%\n",
      "Epoch: 513/1000 | Batch 000/052 | Cost: 0.0797\n",
      "Epoch: 513/1000 | Batch 010/052 | Cost: 0.4277\n",
      "Epoch: 513/1000 | Batch 020/052 | Cost: 0.2630\n",
      "Epoch: 513/1000 | Batch 030/052 | Cost: 0.3470\n",
      "Epoch: 513/1000 | Batch 040/052 | Cost: 0.2576\n",
      "Epoch: 513/1000 | Batch 050/052 | Cost: 0.4056\n",
      "Epoch: 513/1000 training accuracy: 88.44%\n",
      "Epoch: 514/1000 | Batch 000/052 | Cost: 0.2168\n",
      "Epoch: 514/1000 | Batch 010/052 | Cost: 0.1401\n",
      "Epoch: 514/1000 | Batch 020/052 | Cost: 0.1320\n",
      "Epoch: 514/1000 | Batch 030/052 | Cost: 0.0877\n",
      "Epoch: 514/1000 | Batch 040/052 | Cost: 0.4225\n",
      "Epoch: 514/1000 | Batch 050/052 | Cost: 0.3124\n",
      "Epoch: 514/1000 training accuracy: 88.33%\n",
      "Epoch: 515/1000 | Batch 000/052 | Cost: 0.1467\n",
      "Epoch: 515/1000 | Batch 010/052 | Cost: 0.2325\n",
      "Epoch: 515/1000 | Batch 020/052 | Cost: 0.3450\n",
      "Epoch: 515/1000 | Batch 030/052 | Cost: 0.4548\n",
      "Epoch: 515/1000 | Batch 040/052 | Cost: 0.2755\n",
      "Epoch: 515/1000 | Batch 050/052 | Cost: 0.1195\n",
      "Epoch: 515/1000 training accuracy: 87.65%\n",
      "Epoch: 516/1000 | Batch 000/052 | Cost: 0.3100\n",
      "Epoch: 516/1000 | Batch 010/052 | Cost: 0.4898\n",
      "Epoch: 516/1000 | Batch 020/052 | Cost: 0.2334\n",
      "Epoch: 516/1000 | Batch 030/052 | Cost: 0.2502\n",
      "Epoch: 516/1000 | Batch 040/052 | Cost: 0.3803\n",
      "Epoch: 516/1000 | Batch 050/052 | Cost: 0.6078\n",
      "Epoch: 516/1000 training accuracy: 87.99%\n",
      "Epoch: 517/1000 | Batch 000/052 | Cost: 0.6229\n",
      "Epoch: 517/1000 | Batch 010/052 | Cost: 0.3138\n",
      "Epoch: 517/1000 | Batch 020/052 | Cost: 0.3921\n",
      "Epoch: 517/1000 | Batch 030/052 | Cost: 0.2549\n",
      "Epoch: 517/1000 | Batch 040/052 | Cost: 0.3959\n",
      "Epoch: 517/1000 | Batch 050/052 | Cost: 0.5485\n",
      "Epoch: 517/1000 training accuracy: 88.22%\n",
      "Epoch: 518/1000 | Batch 000/052 | Cost: 0.2136\n",
      "Epoch: 518/1000 | Batch 010/052 | Cost: 0.1314\n",
      "Epoch: 518/1000 | Batch 020/052 | Cost: 0.1943\n",
      "Epoch: 518/1000 | Batch 030/052 | Cost: 0.1381\n",
      "Epoch: 518/1000 | Batch 040/052 | Cost: 0.4304\n",
      "Epoch: 518/1000 | Batch 050/052 | Cost: 0.2844\n",
      "Epoch: 518/1000 training accuracy: 87.99%\n",
      "Epoch: 519/1000 | Batch 000/052 | Cost: 0.2025\n",
      "Epoch: 519/1000 | Batch 010/052 | Cost: 0.2417\n",
      "Epoch: 519/1000 | Batch 020/052 | Cost: 0.2363\n",
      "Epoch: 519/1000 | Batch 030/052 | Cost: 0.3677\n",
      "Epoch: 519/1000 | Batch 040/052 | Cost: 0.0948\n",
      "Epoch: 519/1000 | Batch 050/052 | Cost: 0.3060\n",
      "Epoch: 519/1000 training accuracy: 87.88%\n",
      "Epoch: 520/1000 | Batch 000/052 | Cost: 0.3003\n",
      "Epoch: 520/1000 | Batch 010/052 | Cost: 0.2669\n",
      "Epoch: 520/1000 | Batch 020/052 | Cost: 0.4103\n",
      "Epoch: 520/1000 | Batch 030/052 | Cost: 0.1954\n",
      "Epoch: 520/1000 | Batch 040/052 | Cost: 0.4649\n",
      "Epoch: 520/1000 | Batch 050/052 | Cost: 0.1962\n",
      "Epoch: 520/1000 training accuracy: 87.77%\n",
      "Epoch: 521/1000 | Batch 000/052 | Cost: 0.2656\n",
      "Epoch: 521/1000 | Batch 010/052 | Cost: 0.1822\n",
      "Epoch: 521/1000 | Batch 020/052 | Cost: 0.3942\n",
      "Epoch: 521/1000 | Batch 030/052 | Cost: 0.2082\n",
      "Epoch: 521/1000 | Batch 040/052 | Cost: 0.1441\n",
      "Epoch: 521/1000 | Batch 050/052 | Cost: 0.1373\n",
      "Epoch: 521/1000 training accuracy: 88.10%\n",
      "Epoch: 522/1000 | Batch 000/052 | Cost: 0.2551\n",
      "Epoch: 522/1000 | Batch 010/052 | Cost: 0.4631\n",
      "Epoch: 522/1000 | Batch 020/052 | Cost: 0.0960\n",
      "Epoch: 522/1000 | Batch 030/052 | Cost: 0.4051\n",
      "Epoch: 522/1000 | Batch 040/052 | Cost: 0.5243\n",
      "Epoch: 522/1000 | Batch 050/052 | Cost: 0.1548\n",
      "Epoch: 522/1000 training accuracy: 88.55%\n",
      "Epoch: 523/1000 | Batch 000/052 | Cost: 0.1670\n",
      "Epoch: 523/1000 | Batch 010/052 | Cost: 0.0799\n",
      "Epoch: 523/1000 | Batch 020/052 | Cost: 0.5085\n",
      "Epoch: 523/1000 | Batch 030/052 | Cost: 0.2128\n",
      "Epoch: 523/1000 | Batch 040/052 | Cost: 0.1455\n",
      "Epoch: 523/1000 | Batch 050/052 | Cost: 0.1708\n",
      "Epoch: 523/1000 training accuracy: 88.22%\n",
      "Epoch: 524/1000 | Batch 000/052 | Cost: 0.1492\n",
      "Epoch: 524/1000 | Batch 010/052 | Cost: 0.4840\n",
      "Epoch: 524/1000 | Batch 020/052 | Cost: 0.2719\n",
      "Epoch: 524/1000 | Batch 030/052 | Cost: 0.5529\n",
      "Epoch: 524/1000 | Batch 040/052 | Cost: 0.1603\n",
      "Epoch: 524/1000 | Batch 050/052 | Cost: 0.2163\n",
      "Epoch: 524/1000 training accuracy: 88.33%\n",
      "Epoch: 525/1000 | Batch 000/052 | Cost: 0.4501\n",
      "Epoch: 525/1000 | Batch 010/052 | Cost: 0.1904\n",
      "Epoch: 525/1000 | Batch 020/052 | Cost: 0.1942\n",
      "Epoch: 525/1000 | Batch 030/052 | Cost: 0.0990\n",
      "Epoch: 525/1000 | Batch 040/052 | Cost: 0.1142\n",
      "Epoch: 525/1000 | Batch 050/052 | Cost: 0.1701\n",
      "Epoch: 525/1000 training accuracy: 87.99%\n",
      "Epoch: 526/1000 | Batch 000/052 | Cost: 0.3368\n",
      "Epoch: 526/1000 | Batch 010/052 | Cost: 0.3571\n",
      "Epoch: 526/1000 | Batch 020/052 | Cost: 0.5055\n",
      "Epoch: 526/1000 | Batch 030/052 | Cost: 0.2884\n",
      "Epoch: 526/1000 | Batch 040/052 | Cost: 0.2505\n",
      "Epoch: 526/1000 | Batch 050/052 | Cost: 0.4190\n",
      "Epoch: 526/1000 training accuracy: 88.66%\n",
      "Epoch: 527/1000 | Batch 000/052 | Cost: 0.4328\n",
      "Epoch: 527/1000 | Batch 010/052 | Cost: 0.2212\n",
      "Epoch: 527/1000 | Batch 020/052 | Cost: 0.2566\n",
      "Epoch: 527/1000 | Batch 030/052 | Cost: 0.3890\n",
      "Epoch: 527/1000 | Batch 040/052 | Cost: 0.5578\n",
      "Epoch: 527/1000 | Batch 050/052 | Cost: 0.2101\n",
      "Epoch: 527/1000 training accuracy: 87.88%\n",
      "Epoch: 528/1000 | Batch 000/052 | Cost: 0.2565\n",
      "Epoch: 528/1000 | Batch 010/052 | Cost: 0.3007\n",
      "Epoch: 528/1000 | Batch 020/052 | Cost: 0.2675\n",
      "Epoch: 528/1000 | Batch 030/052 | Cost: 0.2502\n",
      "Epoch: 528/1000 | Batch 040/052 | Cost: 0.5083\n",
      "Epoch: 528/1000 | Batch 050/052 | Cost: 0.2754\n",
      "Epoch: 528/1000 training accuracy: 88.33%\n",
      "Epoch: 529/1000 | Batch 000/052 | Cost: 0.2836\n",
      "Epoch: 529/1000 | Batch 010/052 | Cost: 0.2231\n",
      "Epoch: 529/1000 | Batch 020/052 | Cost: 0.2837\n",
      "Epoch: 529/1000 | Batch 030/052 | Cost: 0.2276\n",
      "Epoch: 529/1000 | Batch 040/052 | Cost: 0.2295\n",
      "Epoch: 529/1000 | Batch 050/052 | Cost: 0.4575\n",
      "Epoch: 529/1000 training accuracy: 88.10%\n",
      "Epoch: 530/1000 | Batch 000/052 | Cost: 0.1985\n",
      "Epoch: 530/1000 | Batch 010/052 | Cost: 0.2427\n",
      "Epoch: 530/1000 | Batch 020/052 | Cost: 0.2396\n",
      "Epoch: 530/1000 | Batch 030/052 | Cost: 0.2185\n",
      "Epoch: 530/1000 | Batch 040/052 | Cost: 0.2622\n",
      "Epoch: 530/1000 | Batch 050/052 | Cost: 0.1606\n",
      "Epoch: 530/1000 training accuracy: 87.99%\n",
      "Epoch: 531/1000 | Batch 000/052 | Cost: 0.1463\n",
      "Epoch: 531/1000 | Batch 010/052 | Cost: 0.5428\n",
      "Epoch: 531/1000 | Batch 020/052 | Cost: 0.3342\n",
      "Epoch: 531/1000 | Batch 030/052 | Cost: 0.2959\n",
      "Epoch: 531/1000 | Batch 040/052 | Cost: 0.3900\n",
      "Epoch: 531/1000 | Batch 050/052 | Cost: 0.2592\n",
      "Epoch: 531/1000 training accuracy: 88.44%\n",
      "Epoch: 532/1000 | Batch 000/052 | Cost: 0.1230\n",
      "Epoch: 532/1000 | Batch 010/052 | Cost: 0.2824\n",
      "Epoch: 532/1000 | Batch 020/052 | Cost: 0.2490\n",
      "Epoch: 532/1000 | Batch 030/052 | Cost: 0.3131\n",
      "Epoch: 532/1000 | Batch 040/052 | Cost: 0.3432\n",
      "Epoch: 532/1000 | Batch 050/052 | Cost: 0.2551\n",
      "Epoch: 532/1000 training accuracy: 88.44%\n",
      "Epoch: 533/1000 | Batch 000/052 | Cost: 0.2636\n",
      "Epoch: 533/1000 | Batch 010/052 | Cost: 0.1784\n",
      "Epoch: 533/1000 | Batch 020/052 | Cost: 0.5552\n",
      "Epoch: 533/1000 | Batch 030/052 | Cost: 0.2740\n",
      "Epoch: 533/1000 | Batch 040/052 | Cost: 0.2450\n",
      "Epoch: 533/1000 | Batch 050/052 | Cost: 0.1743\n",
      "Epoch: 533/1000 training accuracy: 87.32%\n",
      "Epoch: 534/1000 | Batch 000/052 | Cost: 0.0647\n",
      "Epoch: 534/1000 | Batch 010/052 | Cost: 0.1875\n",
      "Epoch: 534/1000 | Batch 020/052 | Cost: 0.3386\n",
      "Epoch: 534/1000 | Batch 030/052 | Cost: 0.2216\n",
      "Epoch: 534/1000 | Batch 040/052 | Cost: 0.3055\n",
      "Epoch: 534/1000 | Batch 050/052 | Cost: 0.2182\n",
      "Epoch: 534/1000 training accuracy: 88.22%\n",
      "Epoch: 535/1000 | Batch 000/052 | Cost: 0.2753\n",
      "Epoch: 535/1000 | Batch 010/052 | Cost: 0.1869\n",
      "Epoch: 535/1000 | Batch 020/052 | Cost: 0.2038\n",
      "Epoch: 535/1000 | Batch 030/052 | Cost: 0.2922\n",
      "Epoch: 535/1000 | Batch 040/052 | Cost: 0.3708\n",
      "Epoch: 535/1000 | Batch 050/052 | Cost: 0.1345\n",
      "Epoch: 535/1000 training accuracy: 87.77%\n",
      "Epoch: 536/1000 | Batch 000/052 | Cost: 0.2221\n",
      "Epoch: 536/1000 | Batch 010/052 | Cost: 0.2417\n",
      "Epoch: 536/1000 | Batch 020/052 | Cost: 0.1875\n",
      "Epoch: 536/1000 | Batch 030/052 | Cost: 0.2864\n",
      "Epoch: 536/1000 | Batch 040/052 | Cost: 0.1550\n",
      "Epoch: 536/1000 | Batch 050/052 | Cost: 0.5602\n",
      "Epoch: 536/1000 training accuracy: 87.65%\n",
      "Epoch: 537/1000 | Batch 000/052 | Cost: 0.4020\n",
      "Epoch: 537/1000 | Batch 010/052 | Cost: 0.3754\n",
      "Epoch: 537/1000 | Batch 020/052 | Cost: 0.1155\n",
      "Epoch: 537/1000 | Batch 030/052 | Cost: 0.2451\n",
      "Epoch: 537/1000 | Batch 040/052 | Cost: 0.2406\n",
      "Epoch: 537/1000 | Batch 050/052 | Cost: 0.1785\n",
      "Epoch: 537/1000 training accuracy: 88.33%\n",
      "Epoch: 538/1000 | Batch 000/052 | Cost: 0.2297\n",
      "Epoch: 538/1000 | Batch 010/052 | Cost: 0.2278\n",
      "Epoch: 538/1000 | Batch 020/052 | Cost: 0.3764\n",
      "Epoch: 538/1000 | Batch 030/052 | Cost: 0.3423\n",
      "Epoch: 538/1000 | Batch 040/052 | Cost: 0.1407\n",
      "Epoch: 538/1000 | Batch 050/052 | Cost: 0.1558\n",
      "Epoch: 538/1000 training accuracy: 88.33%\n",
      "Epoch: 539/1000 | Batch 000/052 | Cost: 0.2312\n",
      "Epoch: 539/1000 | Batch 010/052 | Cost: 0.1414\n",
      "Epoch: 539/1000 | Batch 020/052 | Cost: 0.1475\n",
      "Epoch: 539/1000 | Batch 030/052 | Cost: 0.3312\n",
      "Epoch: 539/1000 | Batch 040/052 | Cost: 0.3713\n",
      "Epoch: 539/1000 | Batch 050/052 | Cost: 0.1289\n",
      "Epoch: 539/1000 training accuracy: 88.44%\n",
      "Epoch: 540/1000 | Batch 000/052 | Cost: 0.1064\n",
      "Epoch: 540/1000 | Batch 010/052 | Cost: 0.1080\n",
      "Epoch: 540/1000 | Batch 020/052 | Cost: 0.1732\n",
      "Epoch: 540/1000 | Batch 030/052 | Cost: 0.2318\n",
      "Epoch: 540/1000 | Batch 040/052 | Cost: 0.4524\n",
      "Epoch: 540/1000 | Batch 050/052 | Cost: 0.1896\n",
      "Epoch: 540/1000 training accuracy: 87.65%\n",
      "Epoch: 541/1000 | Batch 000/052 | Cost: 0.2062\n",
      "Epoch: 541/1000 | Batch 010/052 | Cost: 0.2557\n",
      "Epoch: 541/1000 | Batch 020/052 | Cost: 0.1940\n",
      "Epoch: 541/1000 | Batch 030/052 | Cost: 0.8324\n",
      "Epoch: 541/1000 | Batch 040/052 | Cost: 0.2643\n",
      "Epoch: 541/1000 | Batch 050/052 | Cost: 0.2116\n",
      "Epoch: 541/1000 training accuracy: 88.33%\n",
      "Epoch: 542/1000 | Batch 000/052 | Cost: 0.0774\n",
      "Epoch: 542/1000 | Batch 010/052 | Cost: 0.2263\n",
      "Epoch: 542/1000 | Batch 020/052 | Cost: 0.4396\n",
      "Epoch: 542/1000 | Batch 030/052 | Cost: 0.3823\n",
      "Epoch: 542/1000 | Batch 040/052 | Cost: 0.1775\n",
      "Epoch: 542/1000 | Batch 050/052 | Cost: 0.2054\n",
      "Epoch: 542/1000 training accuracy: 88.44%\n",
      "Epoch: 543/1000 | Batch 000/052 | Cost: 0.2895\n",
      "Epoch: 543/1000 | Batch 010/052 | Cost: 0.3191\n",
      "Epoch: 543/1000 | Batch 020/052 | Cost: 0.1721\n",
      "Epoch: 543/1000 | Batch 030/052 | Cost: 0.4250\n",
      "Epoch: 543/1000 | Batch 040/052 | Cost: 0.3874\n",
      "Epoch: 543/1000 | Batch 050/052 | Cost: 0.4680\n",
      "Epoch: 543/1000 training accuracy: 88.33%\n",
      "Epoch: 544/1000 | Batch 000/052 | Cost: 0.2749\n",
      "Epoch: 544/1000 | Batch 010/052 | Cost: 0.4155\n",
      "Epoch: 544/1000 | Batch 020/052 | Cost: 0.3009\n",
      "Epoch: 544/1000 | Batch 030/052 | Cost: 0.3537\n",
      "Epoch: 544/1000 | Batch 040/052 | Cost: 0.3770\n",
      "Epoch: 544/1000 | Batch 050/052 | Cost: 0.4847\n",
      "Epoch: 544/1000 training accuracy: 87.99%\n",
      "Epoch: 545/1000 | Batch 000/052 | Cost: 0.1161\n",
      "Epoch: 545/1000 | Batch 010/052 | Cost: 0.3321\n",
      "Epoch: 545/1000 | Batch 020/052 | Cost: 0.1309\n",
      "Epoch: 545/1000 | Batch 030/052 | Cost: 0.3145\n",
      "Epoch: 545/1000 | Batch 040/052 | Cost: 0.1625\n",
      "Epoch: 545/1000 | Batch 050/052 | Cost: 0.2419\n",
      "Epoch: 545/1000 training accuracy: 87.77%\n",
      "Epoch: 546/1000 | Batch 000/052 | Cost: 0.1927\n",
      "Epoch: 546/1000 | Batch 010/052 | Cost: 0.0821\n",
      "Epoch: 546/1000 | Batch 020/052 | Cost: 0.3987\n",
      "Epoch: 546/1000 | Batch 030/052 | Cost: 0.1521\n",
      "Epoch: 546/1000 | Batch 040/052 | Cost: 0.3596\n",
      "Epoch: 546/1000 | Batch 050/052 | Cost: 0.3082\n",
      "Epoch: 546/1000 training accuracy: 87.77%\n",
      "Epoch: 547/1000 | Batch 000/052 | Cost: 0.2405\n",
      "Epoch: 547/1000 | Batch 010/052 | Cost: 0.3412\n",
      "Epoch: 547/1000 | Batch 020/052 | Cost: 0.2320\n",
      "Epoch: 547/1000 | Batch 030/052 | Cost: 0.2164\n",
      "Epoch: 547/1000 | Batch 040/052 | Cost: 0.0956\n",
      "Epoch: 547/1000 | Batch 050/052 | Cost: 0.3291\n",
      "Epoch: 547/1000 training accuracy: 88.44%\n",
      "Epoch: 548/1000 | Batch 000/052 | Cost: 0.1750\n",
      "Epoch: 548/1000 | Batch 010/052 | Cost: 0.1520\n",
      "Epoch: 548/1000 | Batch 020/052 | Cost: 0.2410\n",
      "Epoch: 548/1000 | Batch 030/052 | Cost: 0.1304\n",
      "Epoch: 548/1000 | Batch 040/052 | Cost: 0.2583\n",
      "Epoch: 548/1000 | Batch 050/052 | Cost: 0.2228\n",
      "Epoch: 548/1000 training accuracy: 88.33%\n",
      "Epoch: 549/1000 | Batch 000/052 | Cost: 0.4016\n",
      "Epoch: 549/1000 | Batch 010/052 | Cost: 0.3916\n",
      "Epoch: 549/1000 | Batch 020/052 | Cost: 0.2517\n",
      "Epoch: 549/1000 | Batch 030/052 | Cost: 0.3151\n",
      "Epoch: 549/1000 | Batch 040/052 | Cost: 0.2330\n",
      "Epoch: 549/1000 | Batch 050/052 | Cost: 0.2750\n",
      "Epoch: 549/1000 training accuracy: 88.44%\n",
      "Epoch: 550/1000 | Batch 000/052 | Cost: 0.3445\n",
      "Epoch: 550/1000 | Batch 010/052 | Cost: 0.1610\n",
      "Epoch: 550/1000 | Batch 020/052 | Cost: 0.1345\n",
      "Epoch: 550/1000 | Batch 030/052 | Cost: 0.1593\n",
      "Epoch: 550/1000 | Batch 040/052 | Cost: 0.2211\n",
      "Epoch: 550/1000 | Batch 050/052 | Cost: 0.4781\n",
      "Epoch: 550/1000 training accuracy: 87.21%\n",
      "Epoch: 551/1000 | Batch 000/052 | Cost: 0.2492\n",
      "Epoch: 551/1000 | Batch 010/052 | Cost: 0.1404\n",
      "Epoch: 551/1000 | Batch 020/052 | Cost: 0.3496\n",
      "Epoch: 551/1000 | Batch 030/052 | Cost: 0.1733\n",
      "Epoch: 551/1000 | Batch 040/052 | Cost: 0.1962\n",
      "Epoch: 551/1000 | Batch 050/052 | Cost: 0.3318\n",
      "Epoch: 551/1000 training accuracy: 87.88%\n",
      "Epoch: 552/1000 | Batch 000/052 | Cost: 0.3474\n",
      "Epoch: 552/1000 | Batch 010/052 | Cost: 0.1379\n",
      "Epoch: 552/1000 | Batch 020/052 | Cost: 0.3148\n",
      "Epoch: 552/1000 | Batch 030/052 | Cost: 0.1734\n",
      "Epoch: 552/1000 | Batch 040/052 | Cost: 0.2268\n",
      "Epoch: 552/1000 | Batch 050/052 | Cost: 0.3950\n",
      "Epoch: 552/1000 training accuracy: 87.77%\n",
      "Epoch: 553/1000 | Batch 000/052 | Cost: 0.2173\n",
      "Epoch: 553/1000 | Batch 010/052 | Cost: 0.3819\n",
      "Epoch: 553/1000 | Batch 020/052 | Cost: 0.2332\n",
      "Epoch: 553/1000 | Batch 030/052 | Cost: 0.4883\n",
      "Epoch: 553/1000 | Batch 040/052 | Cost: 0.2256\n",
      "Epoch: 553/1000 | Batch 050/052 | Cost: 0.1041\n",
      "Epoch: 553/1000 training accuracy: 88.89%\n",
      "Epoch: 554/1000 | Batch 000/052 | Cost: 0.3423\n",
      "Epoch: 554/1000 | Batch 010/052 | Cost: 0.1173\n",
      "Epoch: 554/1000 | Batch 020/052 | Cost: 0.2176\n",
      "Epoch: 554/1000 | Batch 030/052 | Cost: 0.6091\n",
      "Epoch: 554/1000 | Batch 040/052 | Cost: 0.4239\n",
      "Epoch: 554/1000 | Batch 050/052 | Cost: 0.5976\n",
      "Epoch: 554/1000 training accuracy: 87.65%\n",
      "Epoch: 555/1000 | Batch 000/052 | Cost: 0.2962\n",
      "Epoch: 555/1000 | Batch 010/052 | Cost: 0.2298\n",
      "Epoch: 555/1000 | Batch 020/052 | Cost: 0.1868\n",
      "Epoch: 555/1000 | Batch 030/052 | Cost: 0.3374\n",
      "Epoch: 555/1000 | Batch 040/052 | Cost: 0.4054\n",
      "Epoch: 555/1000 | Batch 050/052 | Cost: 0.4494\n",
      "Epoch: 555/1000 training accuracy: 87.43%\n",
      "Epoch: 556/1000 | Batch 000/052 | Cost: 0.3194\n",
      "Epoch: 556/1000 | Batch 010/052 | Cost: 0.4122\n",
      "Epoch: 556/1000 | Batch 020/052 | Cost: 0.3198\n",
      "Epoch: 556/1000 | Batch 030/052 | Cost: 0.3030\n",
      "Epoch: 556/1000 | Batch 040/052 | Cost: 0.7179\n",
      "Epoch: 556/1000 | Batch 050/052 | Cost: 0.2896\n",
      "Epoch: 556/1000 training accuracy: 87.32%\n",
      "Epoch: 557/1000 | Batch 000/052 | Cost: 0.1534\n",
      "Epoch: 557/1000 | Batch 010/052 | Cost: 0.3670\n",
      "Epoch: 557/1000 | Batch 020/052 | Cost: 0.5124\n",
      "Epoch: 557/1000 | Batch 030/052 | Cost: 0.1940\n",
      "Epoch: 557/1000 | Batch 040/052 | Cost: 0.6517\n",
      "Epoch: 557/1000 | Batch 050/052 | Cost: 0.3808\n",
      "Epoch: 557/1000 training accuracy: 88.44%\n",
      "Epoch: 558/1000 | Batch 000/052 | Cost: 0.0800\n",
      "Epoch: 558/1000 | Batch 010/052 | Cost: 0.1557\n",
      "Epoch: 558/1000 | Batch 020/052 | Cost: 0.1684\n",
      "Epoch: 558/1000 | Batch 030/052 | Cost: 0.3809\n",
      "Epoch: 558/1000 | Batch 040/052 | Cost: 0.4803\n",
      "Epoch: 558/1000 | Batch 050/052 | Cost: 0.1338\n",
      "Epoch: 558/1000 training accuracy: 88.10%\n",
      "Epoch: 559/1000 | Batch 000/052 | Cost: 0.2619\n",
      "Epoch: 559/1000 | Batch 010/052 | Cost: 0.5596\n",
      "Epoch: 559/1000 | Batch 020/052 | Cost: 0.2164\n",
      "Epoch: 559/1000 | Batch 030/052 | Cost: 0.2442\n",
      "Epoch: 559/1000 | Batch 040/052 | Cost: 0.1061\n",
      "Epoch: 559/1000 | Batch 050/052 | Cost: 0.1779\n",
      "Epoch: 559/1000 training accuracy: 88.33%\n",
      "Epoch: 560/1000 | Batch 000/052 | Cost: 0.3997\n",
      "Epoch: 560/1000 | Batch 010/052 | Cost: 0.1107\n",
      "Epoch: 560/1000 | Batch 020/052 | Cost: 0.4027\n",
      "Epoch: 560/1000 | Batch 030/052 | Cost: 0.3772\n",
      "Epoch: 560/1000 | Batch 040/052 | Cost: 0.2423\n",
      "Epoch: 560/1000 | Batch 050/052 | Cost: 0.4280\n",
      "Epoch: 560/1000 training accuracy: 88.78%\n",
      "Epoch: 561/1000 | Batch 000/052 | Cost: 0.1816\n",
      "Epoch: 561/1000 | Batch 010/052 | Cost: 0.1110\n",
      "Epoch: 561/1000 | Batch 020/052 | Cost: 0.2058\n",
      "Epoch: 561/1000 | Batch 030/052 | Cost: 0.3685\n",
      "Epoch: 561/1000 | Batch 040/052 | Cost: 0.1873\n",
      "Epoch: 561/1000 | Batch 050/052 | Cost: 0.0907\n",
      "Epoch: 561/1000 training accuracy: 88.22%\n",
      "Epoch: 562/1000 | Batch 000/052 | Cost: 0.1948\n",
      "Epoch: 562/1000 | Batch 010/052 | Cost: 0.3025\n",
      "Epoch: 562/1000 | Batch 020/052 | Cost: 0.3221\n",
      "Epoch: 562/1000 | Batch 030/052 | Cost: 0.6459\n",
      "Epoch: 562/1000 | Batch 040/052 | Cost: 0.2029\n",
      "Epoch: 562/1000 | Batch 050/052 | Cost: 0.2530\n",
      "Epoch: 562/1000 training accuracy: 89.11%\n",
      "Epoch: 563/1000 | Batch 000/052 | Cost: 0.2644\n",
      "Epoch: 563/1000 | Batch 010/052 | Cost: 0.1403\n",
      "Epoch: 563/1000 | Batch 020/052 | Cost: 0.2363\n",
      "Epoch: 563/1000 | Batch 030/052 | Cost: 0.1581\n",
      "Epoch: 563/1000 | Batch 040/052 | Cost: 0.3009\n",
      "Epoch: 563/1000 | Batch 050/052 | Cost: 0.2826\n",
      "Epoch: 563/1000 training accuracy: 87.77%\n",
      "Epoch: 564/1000 | Batch 000/052 | Cost: 0.1207\n",
      "Epoch: 564/1000 | Batch 010/052 | Cost: 0.4089\n",
      "Epoch: 564/1000 | Batch 020/052 | Cost: 0.2247\n",
      "Epoch: 564/1000 | Batch 030/052 | Cost: 0.2775\n",
      "Epoch: 564/1000 | Batch 040/052 | Cost: 0.5405\n",
      "Epoch: 564/1000 | Batch 050/052 | Cost: 0.3748\n",
      "Epoch: 564/1000 training accuracy: 89.11%\n",
      "Epoch: 565/1000 | Batch 000/052 | Cost: 0.1895\n",
      "Epoch: 565/1000 | Batch 010/052 | Cost: 0.1830\n",
      "Epoch: 565/1000 | Batch 020/052 | Cost: 0.1300\n",
      "Epoch: 565/1000 | Batch 030/052 | Cost: 0.3367\n",
      "Epoch: 565/1000 | Batch 040/052 | Cost: 0.4084\n",
      "Epoch: 565/1000 | Batch 050/052 | Cost: 0.1313\n",
      "Epoch: 565/1000 training accuracy: 88.55%\n",
      "Epoch: 566/1000 | Batch 000/052 | Cost: 0.1523\n",
      "Epoch: 566/1000 | Batch 010/052 | Cost: 0.5564\n",
      "Epoch: 566/1000 | Batch 020/052 | Cost: 0.1915\n",
      "Epoch: 566/1000 | Batch 030/052 | Cost: 0.3143\n",
      "Epoch: 566/1000 | Batch 040/052 | Cost: 0.5736\n",
      "Epoch: 566/1000 | Batch 050/052 | Cost: 0.2820\n",
      "Epoch: 566/1000 training accuracy: 88.44%\n",
      "Epoch: 567/1000 | Batch 000/052 | Cost: 0.3735\n",
      "Epoch: 567/1000 | Batch 010/052 | Cost: 0.3774\n",
      "Epoch: 567/1000 | Batch 020/052 | Cost: 0.3022\n",
      "Epoch: 567/1000 | Batch 030/052 | Cost: 0.3057\n",
      "Epoch: 567/1000 | Batch 040/052 | Cost: 0.1863\n",
      "Epoch: 567/1000 | Batch 050/052 | Cost: 0.4941\n",
      "Epoch: 567/1000 training accuracy: 87.99%\n",
      "Epoch: 568/1000 | Batch 000/052 | Cost: 0.3160\n",
      "Epoch: 568/1000 | Batch 010/052 | Cost: 0.1090\n",
      "Epoch: 568/1000 | Batch 020/052 | Cost: 0.2892\n",
      "Epoch: 568/1000 | Batch 030/052 | Cost: 0.1499\n",
      "Epoch: 568/1000 | Batch 040/052 | Cost: 0.1361\n",
      "Epoch: 568/1000 | Batch 050/052 | Cost: 0.2554\n",
      "Epoch: 568/1000 training accuracy: 87.77%\n",
      "Epoch: 569/1000 | Batch 000/052 | Cost: 0.4741\n",
      "Epoch: 569/1000 | Batch 010/052 | Cost: 0.3854\n",
      "Epoch: 569/1000 | Batch 020/052 | Cost: 0.1590\n",
      "Epoch: 569/1000 | Batch 030/052 | Cost: 0.1166\n",
      "Epoch: 569/1000 | Batch 040/052 | Cost: 0.2753\n",
      "Epoch: 569/1000 | Batch 050/052 | Cost: 0.3336\n",
      "Epoch: 569/1000 training accuracy: 87.99%\n",
      "Epoch: 570/1000 | Batch 000/052 | Cost: 0.3499\n",
      "Epoch: 570/1000 | Batch 010/052 | Cost: 0.2557\n",
      "Epoch: 570/1000 | Batch 020/052 | Cost: 0.2862\n",
      "Epoch: 570/1000 | Batch 030/052 | Cost: 0.1949\n",
      "Epoch: 570/1000 | Batch 040/052 | Cost: 0.2827\n",
      "Epoch: 570/1000 | Batch 050/052 | Cost: 0.2078\n",
      "Epoch: 570/1000 training accuracy: 87.32%\n",
      "Epoch: 571/1000 | Batch 000/052 | Cost: 0.1012\n",
      "Epoch: 571/1000 | Batch 010/052 | Cost: 0.3273\n",
      "Epoch: 571/1000 | Batch 020/052 | Cost: 0.5823\n",
      "Epoch: 571/1000 | Batch 030/052 | Cost: 0.1864\n",
      "Epoch: 571/1000 | Batch 040/052 | Cost: 0.0895\n",
      "Epoch: 571/1000 | Batch 050/052 | Cost: 0.2278\n",
      "Epoch: 571/1000 training accuracy: 87.43%\n",
      "Epoch: 572/1000 | Batch 000/052 | Cost: 0.1406\n",
      "Epoch: 572/1000 | Batch 010/052 | Cost: 0.1725\n",
      "Epoch: 572/1000 | Batch 020/052 | Cost: 0.2482\n",
      "Epoch: 572/1000 | Batch 030/052 | Cost: 0.3986\n",
      "Epoch: 572/1000 | Batch 040/052 | Cost: 0.1635\n",
      "Epoch: 572/1000 | Batch 050/052 | Cost: 0.4452\n",
      "Epoch: 572/1000 training accuracy: 87.32%\n",
      "Epoch: 573/1000 | Batch 000/052 | Cost: 0.2860\n",
      "Epoch: 573/1000 | Batch 010/052 | Cost: 0.3437\n",
      "Epoch: 573/1000 | Batch 020/052 | Cost: 0.1782\n",
      "Epoch: 573/1000 | Batch 030/052 | Cost: 0.1032\n",
      "Epoch: 573/1000 | Batch 040/052 | Cost: 0.1250\n",
      "Epoch: 573/1000 | Batch 050/052 | Cost: 0.2598\n",
      "Epoch: 573/1000 training accuracy: 88.66%\n",
      "Epoch: 574/1000 | Batch 000/052 | Cost: 0.3636\n",
      "Epoch: 574/1000 | Batch 010/052 | Cost: 0.3867\n",
      "Epoch: 574/1000 | Batch 020/052 | Cost: 0.2826\n",
      "Epoch: 574/1000 | Batch 030/052 | Cost: 0.3276\n",
      "Epoch: 574/1000 | Batch 040/052 | Cost: 0.1849\n",
      "Epoch: 574/1000 | Batch 050/052 | Cost: 0.1742\n",
      "Epoch: 574/1000 training accuracy: 87.88%\n",
      "Epoch: 575/1000 | Batch 000/052 | Cost: 0.2677\n",
      "Epoch: 575/1000 | Batch 010/052 | Cost: 0.2315\n",
      "Epoch: 575/1000 | Batch 020/052 | Cost: 0.2699\n",
      "Epoch: 575/1000 | Batch 030/052 | Cost: 0.3114\n",
      "Epoch: 575/1000 | Batch 040/052 | Cost: 0.6198\n",
      "Epoch: 575/1000 | Batch 050/052 | Cost: 0.3417\n",
      "Epoch: 575/1000 training accuracy: 88.78%\n",
      "Epoch: 576/1000 | Batch 000/052 | Cost: 0.3379\n",
      "Epoch: 576/1000 | Batch 010/052 | Cost: 0.2740\n",
      "Epoch: 576/1000 | Batch 020/052 | Cost: 0.3006\n",
      "Epoch: 576/1000 | Batch 030/052 | Cost: 0.1262\n",
      "Epoch: 576/1000 | Batch 040/052 | Cost: 0.1421\n",
      "Epoch: 576/1000 | Batch 050/052 | Cost: 0.2429\n",
      "Epoch: 576/1000 training accuracy: 87.99%\n",
      "Epoch: 577/1000 | Batch 000/052 | Cost: 0.2734\n",
      "Epoch: 577/1000 | Batch 010/052 | Cost: 0.1606\n",
      "Epoch: 577/1000 | Batch 020/052 | Cost: 0.3045\n",
      "Epoch: 577/1000 | Batch 030/052 | Cost: 0.1307\n",
      "Epoch: 577/1000 | Batch 040/052 | Cost: 0.1594\n",
      "Epoch: 577/1000 | Batch 050/052 | Cost: 0.6107\n",
      "Epoch: 577/1000 training accuracy: 88.22%\n",
      "Epoch: 578/1000 | Batch 000/052 | Cost: 0.6209\n",
      "Epoch: 578/1000 | Batch 010/052 | Cost: 0.2154\n",
      "Epoch: 578/1000 | Batch 020/052 | Cost: 0.3709\n",
      "Epoch: 578/1000 | Batch 030/052 | Cost: 0.2871\n",
      "Epoch: 578/1000 | Batch 040/052 | Cost: 0.1437\n",
      "Epoch: 578/1000 | Batch 050/052 | Cost: 0.3611\n",
      "Epoch: 578/1000 training accuracy: 87.54%\n",
      "Epoch: 579/1000 | Batch 000/052 | Cost: 0.1604\n",
      "Epoch: 579/1000 | Batch 010/052 | Cost: 0.1752\n",
      "Epoch: 579/1000 | Batch 020/052 | Cost: 0.3626\n",
      "Epoch: 579/1000 | Batch 030/052 | Cost: 0.1485\n",
      "Epoch: 579/1000 | Batch 040/052 | Cost: 0.3223\n",
      "Epoch: 579/1000 | Batch 050/052 | Cost: 0.2316\n",
      "Epoch: 579/1000 training accuracy: 88.10%\n",
      "Epoch: 580/1000 | Batch 000/052 | Cost: 0.3435\n",
      "Epoch: 580/1000 | Batch 010/052 | Cost: 0.0579\n",
      "Epoch: 580/1000 | Batch 020/052 | Cost: 0.2693\n",
      "Epoch: 580/1000 | Batch 030/052 | Cost: 0.2626\n",
      "Epoch: 580/1000 | Batch 040/052 | Cost: 0.1392\n",
      "Epoch: 580/1000 | Batch 050/052 | Cost: 0.0876\n",
      "Epoch: 580/1000 training accuracy: 87.88%\n",
      "Epoch: 581/1000 | Batch 000/052 | Cost: 0.3025\n",
      "Epoch: 581/1000 | Batch 010/052 | Cost: 0.1440\n",
      "Epoch: 581/1000 | Batch 020/052 | Cost: 0.1665\n",
      "Epoch: 581/1000 | Batch 030/052 | Cost: 0.3733\n",
      "Epoch: 581/1000 | Batch 040/052 | Cost: 0.3661\n",
      "Epoch: 581/1000 | Batch 050/052 | Cost: 0.3343\n",
      "Epoch: 581/1000 training accuracy: 87.88%\n",
      "Epoch: 582/1000 | Batch 000/052 | Cost: 0.4068\n",
      "Epoch: 582/1000 | Batch 010/052 | Cost: 0.2774\n",
      "Epoch: 582/1000 | Batch 020/052 | Cost: 0.3042\n",
      "Epoch: 582/1000 | Batch 030/052 | Cost: 0.2261\n",
      "Epoch: 582/1000 | Batch 040/052 | Cost: 0.3666\n",
      "Epoch: 582/1000 | Batch 050/052 | Cost: 0.1289\n",
      "Epoch: 582/1000 training accuracy: 87.99%\n",
      "Epoch: 583/1000 | Batch 000/052 | Cost: 0.3727\n",
      "Epoch: 583/1000 | Batch 010/052 | Cost: 0.1758\n",
      "Epoch: 583/1000 | Batch 020/052 | Cost: 0.2114\n",
      "Epoch: 583/1000 | Batch 030/052 | Cost: 0.3331\n",
      "Epoch: 583/1000 | Batch 040/052 | Cost: 0.2555\n",
      "Epoch: 583/1000 | Batch 050/052 | Cost: 0.3795\n",
      "Epoch: 583/1000 training accuracy: 88.33%\n",
      "Epoch: 584/1000 | Batch 000/052 | Cost: 0.3091\n",
      "Epoch: 584/1000 | Batch 010/052 | Cost: 0.2397\n",
      "Epoch: 584/1000 | Batch 020/052 | Cost: 0.2917\n",
      "Epoch: 584/1000 | Batch 030/052 | Cost: 0.2941\n",
      "Epoch: 584/1000 | Batch 040/052 | Cost: 0.5249\n",
      "Epoch: 584/1000 | Batch 050/052 | Cost: 0.3391\n",
      "Epoch: 584/1000 training accuracy: 87.88%\n",
      "Epoch: 585/1000 | Batch 000/052 | Cost: 0.2429\n",
      "Epoch: 585/1000 | Batch 010/052 | Cost: 0.1868\n",
      "Epoch: 585/1000 | Batch 020/052 | Cost: 0.3571\n",
      "Epoch: 585/1000 | Batch 030/052 | Cost: 0.4184\n",
      "Epoch: 585/1000 | Batch 040/052 | Cost: 0.3058\n",
      "Epoch: 585/1000 | Batch 050/052 | Cost: 0.2815\n",
      "Epoch: 585/1000 training accuracy: 88.78%\n",
      "Epoch: 586/1000 | Batch 000/052 | Cost: 0.2736\n",
      "Epoch: 586/1000 | Batch 010/052 | Cost: 0.1552\n",
      "Epoch: 586/1000 | Batch 020/052 | Cost: 0.2897\n",
      "Epoch: 586/1000 | Batch 030/052 | Cost: 0.2681\n",
      "Epoch: 586/1000 | Batch 040/052 | Cost: 0.1925\n",
      "Epoch: 586/1000 | Batch 050/052 | Cost: 0.3071\n",
      "Epoch: 586/1000 training accuracy: 87.99%\n",
      "Epoch: 587/1000 | Batch 000/052 | Cost: 0.2894\n",
      "Epoch: 587/1000 | Batch 010/052 | Cost: 0.4262\n",
      "Epoch: 587/1000 | Batch 020/052 | Cost: 0.1403\n",
      "Epoch: 587/1000 | Batch 030/052 | Cost: 0.5409\n",
      "Epoch: 587/1000 | Batch 040/052 | Cost: 0.7680\n",
      "Epoch: 587/1000 | Batch 050/052 | Cost: 0.3006\n",
      "Epoch: 587/1000 training accuracy: 88.55%\n",
      "Epoch: 588/1000 | Batch 000/052 | Cost: 0.5441\n",
      "Epoch: 588/1000 | Batch 010/052 | Cost: 0.1906\n",
      "Epoch: 588/1000 | Batch 020/052 | Cost: 0.2225\n",
      "Epoch: 588/1000 | Batch 030/052 | Cost: 0.3769\n",
      "Epoch: 588/1000 | Batch 040/052 | Cost: 0.0555\n",
      "Epoch: 588/1000 | Batch 050/052 | Cost: 0.4235\n",
      "Epoch: 588/1000 training accuracy: 87.99%\n",
      "Epoch: 589/1000 | Batch 000/052 | Cost: 0.1611\n",
      "Epoch: 589/1000 | Batch 010/052 | Cost: 0.3233\n",
      "Epoch: 589/1000 | Batch 020/052 | Cost: 0.1068\n",
      "Epoch: 589/1000 | Batch 030/052 | Cost: 0.2178\n",
      "Epoch: 589/1000 | Batch 040/052 | Cost: 0.3831\n",
      "Epoch: 589/1000 | Batch 050/052 | Cost: 0.4199\n",
      "Epoch: 589/1000 training accuracy: 88.10%\n",
      "Epoch: 590/1000 | Batch 000/052 | Cost: 0.2402\n",
      "Epoch: 590/1000 | Batch 010/052 | Cost: 0.1726\n",
      "Epoch: 590/1000 | Batch 020/052 | Cost: 0.2588\n",
      "Epoch: 590/1000 | Batch 030/052 | Cost: 0.3348\n",
      "Epoch: 590/1000 | Batch 040/052 | Cost: 0.1681\n",
      "Epoch: 590/1000 | Batch 050/052 | Cost: 0.3900\n",
      "Epoch: 590/1000 training accuracy: 88.55%\n",
      "Epoch: 591/1000 | Batch 000/052 | Cost: 0.6879\n",
      "Epoch: 591/1000 | Batch 010/052 | Cost: 0.3206\n",
      "Epoch: 591/1000 | Batch 020/052 | Cost: 0.3804\n",
      "Epoch: 591/1000 | Batch 030/052 | Cost: 0.1818\n",
      "Epoch: 591/1000 | Batch 040/052 | Cost: 0.1747\n",
      "Epoch: 591/1000 | Batch 050/052 | Cost: 0.3668\n",
      "Epoch: 591/1000 training accuracy: 88.22%\n",
      "Epoch: 592/1000 | Batch 000/052 | Cost: 0.2924\n",
      "Epoch: 592/1000 | Batch 010/052 | Cost: 0.1834\n",
      "Epoch: 592/1000 | Batch 020/052 | Cost: 0.0906\n",
      "Epoch: 592/1000 | Batch 030/052 | Cost: 0.3577\n",
      "Epoch: 592/1000 | Batch 040/052 | Cost: 0.1837\n",
      "Epoch: 592/1000 | Batch 050/052 | Cost: 0.1636\n",
      "Epoch: 592/1000 training accuracy: 88.89%\n",
      "Epoch: 593/1000 | Batch 000/052 | Cost: 0.4500\n",
      "Epoch: 593/1000 | Batch 010/052 | Cost: 0.4040\n",
      "Epoch: 593/1000 | Batch 020/052 | Cost: 0.4489\n",
      "Epoch: 593/1000 | Batch 030/052 | Cost: 0.2287\n",
      "Epoch: 593/1000 | Batch 040/052 | Cost: 0.1026\n",
      "Epoch: 593/1000 | Batch 050/052 | Cost: 0.1950\n",
      "Epoch: 593/1000 training accuracy: 88.89%\n",
      "Epoch: 594/1000 | Batch 000/052 | Cost: 0.0775\n",
      "Epoch: 594/1000 | Batch 010/052 | Cost: 0.2496\n",
      "Epoch: 594/1000 | Batch 020/052 | Cost: 0.1465\n",
      "Epoch: 594/1000 | Batch 030/052 | Cost: 0.2546\n",
      "Epoch: 594/1000 | Batch 040/052 | Cost: 0.3188\n",
      "Epoch: 594/1000 | Batch 050/052 | Cost: 0.1696\n",
      "Epoch: 594/1000 training accuracy: 88.10%\n",
      "Epoch: 595/1000 | Batch 000/052 | Cost: 0.5736\n",
      "Epoch: 595/1000 | Batch 010/052 | Cost: 0.4261\n",
      "Epoch: 595/1000 | Batch 020/052 | Cost: 0.1988\n",
      "Epoch: 595/1000 | Batch 030/052 | Cost: 0.3969\n",
      "Epoch: 595/1000 | Batch 040/052 | Cost: 0.1909\n",
      "Epoch: 595/1000 | Batch 050/052 | Cost: 0.1252\n",
      "Epoch: 595/1000 training accuracy: 88.10%\n",
      "Epoch: 596/1000 | Batch 000/052 | Cost: 0.0657\n",
      "Epoch: 596/1000 | Batch 010/052 | Cost: 0.4649\n",
      "Epoch: 596/1000 | Batch 020/052 | Cost: 0.2482\n",
      "Epoch: 596/1000 | Batch 030/052 | Cost: 0.2462\n",
      "Epoch: 596/1000 | Batch 040/052 | Cost: 0.3527\n",
      "Epoch: 596/1000 | Batch 050/052 | Cost: 0.1864\n",
      "Epoch: 596/1000 training accuracy: 89.00%\n",
      "Epoch: 597/1000 | Batch 000/052 | Cost: 0.2968\n",
      "Epoch: 597/1000 | Batch 010/052 | Cost: 0.2757\n",
      "Epoch: 597/1000 | Batch 020/052 | Cost: 0.3172\n",
      "Epoch: 597/1000 | Batch 030/052 | Cost: 0.1822\n",
      "Epoch: 597/1000 | Batch 040/052 | Cost: 0.1439\n",
      "Epoch: 597/1000 | Batch 050/052 | Cost: 0.1587\n",
      "Epoch: 597/1000 training accuracy: 88.55%\n",
      "Epoch: 598/1000 | Batch 000/052 | Cost: 0.2200\n",
      "Epoch: 598/1000 | Batch 010/052 | Cost: 0.1355\n",
      "Epoch: 598/1000 | Batch 020/052 | Cost: 0.4702\n",
      "Epoch: 598/1000 | Batch 030/052 | Cost: 0.4731\n",
      "Epoch: 598/1000 | Batch 040/052 | Cost: 0.4170\n",
      "Epoch: 598/1000 | Batch 050/052 | Cost: 0.1180\n",
      "Epoch: 598/1000 training accuracy: 88.78%\n",
      "Epoch: 599/1000 | Batch 000/052 | Cost: 0.1244\n",
      "Epoch: 599/1000 | Batch 010/052 | Cost: 0.3164\n",
      "Epoch: 599/1000 | Batch 020/052 | Cost: 0.0927\n",
      "Epoch: 599/1000 | Batch 030/052 | Cost: 0.2885\n",
      "Epoch: 599/1000 | Batch 040/052 | Cost: 0.1665\n",
      "Epoch: 599/1000 | Batch 050/052 | Cost: 0.1879\n",
      "Epoch: 599/1000 training accuracy: 88.66%\n",
      "Epoch: 600/1000 | Batch 000/052 | Cost: 0.3392\n",
      "Epoch: 600/1000 | Batch 010/052 | Cost: 0.1477\n",
      "Epoch: 600/1000 | Batch 020/052 | Cost: 0.3620\n",
      "Epoch: 600/1000 | Batch 030/052 | Cost: 0.2689\n",
      "Epoch: 600/1000 | Batch 040/052 | Cost: 0.5777\n",
      "Epoch: 600/1000 | Batch 050/052 | Cost: 0.1626\n",
      "Epoch: 600/1000 training accuracy: 88.33%\n",
      "Epoch: 601/1000 | Batch 000/052 | Cost: 0.1790\n",
      "Epoch: 601/1000 | Batch 010/052 | Cost: 0.2681\n",
      "Epoch: 601/1000 | Batch 020/052 | Cost: 0.4310\n",
      "Epoch: 601/1000 | Batch 030/052 | Cost: 0.2022\n",
      "Epoch: 601/1000 | Batch 040/052 | Cost: 0.2084\n",
      "Epoch: 601/1000 | Batch 050/052 | Cost: 0.1274\n",
      "Epoch: 601/1000 training accuracy: 87.43%\n",
      "Epoch: 602/1000 | Batch 000/052 | Cost: 0.4216\n",
      "Epoch: 602/1000 | Batch 010/052 | Cost: 0.1892\n",
      "Epoch: 602/1000 | Batch 020/052 | Cost: 0.2364\n",
      "Epoch: 602/1000 | Batch 030/052 | Cost: 0.3101\n",
      "Epoch: 602/1000 | Batch 040/052 | Cost: 0.2073\n",
      "Epoch: 602/1000 | Batch 050/052 | Cost: 0.2020\n",
      "Epoch: 602/1000 training accuracy: 88.66%\n",
      "Epoch: 603/1000 | Batch 000/052 | Cost: 0.5035\n",
      "Epoch: 603/1000 | Batch 010/052 | Cost: 0.2032\n",
      "Epoch: 603/1000 | Batch 020/052 | Cost: 0.2630\n",
      "Epoch: 603/1000 | Batch 030/052 | Cost: 0.3207\n",
      "Epoch: 603/1000 | Batch 040/052 | Cost: 0.2184\n",
      "Epoch: 603/1000 | Batch 050/052 | Cost: 0.2132\n",
      "Epoch: 603/1000 training accuracy: 88.66%\n",
      "Epoch: 604/1000 | Batch 000/052 | Cost: 0.0734\n",
      "Epoch: 604/1000 | Batch 010/052 | Cost: 0.3163\n",
      "Epoch: 604/1000 | Batch 020/052 | Cost: 0.5004\n",
      "Epoch: 604/1000 | Batch 030/052 | Cost: 0.2199\n",
      "Epoch: 604/1000 | Batch 040/052 | Cost: 0.1184\n",
      "Epoch: 604/1000 | Batch 050/052 | Cost: 0.2787\n",
      "Epoch: 604/1000 training accuracy: 88.55%\n",
      "Epoch: 605/1000 | Batch 000/052 | Cost: 0.1284\n",
      "Epoch: 605/1000 | Batch 010/052 | Cost: 0.0901\n",
      "Epoch: 605/1000 | Batch 020/052 | Cost: 0.2312\n",
      "Epoch: 605/1000 | Batch 030/052 | Cost: 0.2055\n",
      "Epoch: 605/1000 | Batch 040/052 | Cost: 0.1900\n",
      "Epoch: 605/1000 | Batch 050/052 | Cost: 0.5445\n",
      "Epoch: 605/1000 training accuracy: 89.11%\n",
      "Epoch: 606/1000 | Batch 000/052 | Cost: 0.2258\n",
      "Epoch: 606/1000 | Batch 010/052 | Cost: 0.2016\n",
      "Epoch: 606/1000 | Batch 020/052 | Cost: 0.1803\n",
      "Epoch: 606/1000 | Batch 030/052 | Cost: 0.4421\n",
      "Epoch: 606/1000 | Batch 040/052 | Cost: 0.3937\n",
      "Epoch: 606/1000 | Batch 050/052 | Cost: 0.1047\n",
      "Epoch: 606/1000 training accuracy: 88.22%\n",
      "Epoch: 607/1000 | Batch 000/052 | Cost: 0.2333\n",
      "Epoch: 607/1000 | Batch 010/052 | Cost: 0.3419\n",
      "Epoch: 607/1000 | Batch 020/052 | Cost: 0.3659\n",
      "Epoch: 607/1000 | Batch 030/052 | Cost: 0.4342\n",
      "Epoch: 607/1000 | Batch 040/052 | Cost: 0.3407\n",
      "Epoch: 607/1000 | Batch 050/052 | Cost: 0.4950\n",
      "Epoch: 607/1000 training accuracy: 88.22%\n",
      "Epoch: 608/1000 | Batch 000/052 | Cost: 0.5520\n",
      "Epoch: 608/1000 | Batch 010/052 | Cost: 0.2458\n",
      "Epoch: 608/1000 | Batch 020/052 | Cost: 0.1291\n",
      "Epoch: 608/1000 | Batch 030/052 | Cost: 0.1908\n",
      "Epoch: 608/1000 | Batch 040/052 | Cost: 0.0858\n",
      "Epoch: 608/1000 | Batch 050/052 | Cost: 0.3585\n",
      "Epoch: 608/1000 training accuracy: 88.22%\n",
      "Epoch: 609/1000 | Batch 000/052 | Cost: 0.1651\n",
      "Epoch: 609/1000 | Batch 010/052 | Cost: 0.3897\n",
      "Epoch: 609/1000 | Batch 020/052 | Cost: 0.5099\n",
      "Epoch: 609/1000 | Batch 030/052 | Cost: 0.3579\n",
      "Epoch: 609/1000 | Batch 040/052 | Cost: 0.2659\n",
      "Epoch: 609/1000 | Batch 050/052 | Cost: 0.3176\n",
      "Epoch: 609/1000 training accuracy: 88.89%\n",
      "Epoch: 610/1000 | Batch 000/052 | Cost: 0.1989\n",
      "Epoch: 610/1000 | Batch 010/052 | Cost: 0.4344\n",
      "Epoch: 610/1000 | Batch 020/052 | Cost: 0.3369\n",
      "Epoch: 610/1000 | Batch 030/052 | Cost: 0.3790\n",
      "Epoch: 610/1000 | Batch 040/052 | Cost: 0.5503\n",
      "Epoch: 610/1000 | Batch 050/052 | Cost: 0.3406\n",
      "Epoch: 610/1000 training accuracy: 88.22%\n",
      "Epoch: 611/1000 | Batch 000/052 | Cost: 0.2237\n",
      "Epoch: 611/1000 | Batch 010/052 | Cost: 0.5356\n",
      "Epoch: 611/1000 | Batch 020/052 | Cost: 0.1446\n",
      "Epoch: 611/1000 | Batch 030/052 | Cost: 0.1546\n",
      "Epoch: 611/1000 | Batch 040/052 | Cost: 0.3468\n",
      "Epoch: 611/1000 | Batch 050/052 | Cost: 0.2074\n",
      "Epoch: 611/1000 training accuracy: 87.99%\n",
      "Epoch: 612/1000 | Batch 000/052 | Cost: 0.2700\n",
      "Epoch: 612/1000 | Batch 010/052 | Cost: 0.2195\n",
      "Epoch: 612/1000 | Batch 020/052 | Cost: 0.3170\n",
      "Epoch: 612/1000 | Batch 030/052 | Cost: 0.4944\n",
      "Epoch: 612/1000 | Batch 040/052 | Cost: 0.1598\n",
      "Epoch: 612/1000 | Batch 050/052 | Cost: 0.2211\n",
      "Epoch: 612/1000 training accuracy: 88.55%\n",
      "Epoch: 613/1000 | Batch 000/052 | Cost: 0.5286\n",
      "Epoch: 613/1000 | Batch 010/052 | Cost: 0.1268\n",
      "Epoch: 613/1000 | Batch 020/052 | Cost: 0.3939\n",
      "Epoch: 613/1000 | Batch 030/052 | Cost: 0.6026\n",
      "Epoch: 613/1000 | Batch 040/052 | Cost: 0.1358\n",
      "Epoch: 613/1000 | Batch 050/052 | Cost: 0.4218\n",
      "Epoch: 613/1000 training accuracy: 88.44%\n",
      "Epoch: 614/1000 | Batch 000/052 | Cost: 0.0692\n",
      "Epoch: 614/1000 | Batch 010/052 | Cost: 0.3310\n",
      "Epoch: 614/1000 | Batch 020/052 | Cost: 0.2540\n",
      "Epoch: 614/1000 | Batch 030/052 | Cost: 0.6114\n",
      "Epoch: 614/1000 | Batch 040/052 | Cost: 0.2705\n",
      "Epoch: 614/1000 | Batch 050/052 | Cost: 0.1669\n",
      "Epoch: 614/1000 training accuracy: 88.10%\n",
      "Epoch: 615/1000 | Batch 000/052 | Cost: 0.2447\n",
      "Epoch: 615/1000 | Batch 010/052 | Cost: 0.0630\n",
      "Epoch: 615/1000 | Batch 020/052 | Cost: 0.2617\n",
      "Epoch: 615/1000 | Batch 030/052 | Cost: 0.1209\n",
      "Epoch: 615/1000 | Batch 040/052 | Cost: 0.2595\n",
      "Epoch: 615/1000 | Batch 050/052 | Cost: 0.3483\n",
      "Epoch: 615/1000 training accuracy: 87.77%\n",
      "Epoch: 616/1000 | Batch 000/052 | Cost: 0.3646\n",
      "Epoch: 616/1000 | Batch 010/052 | Cost: 0.2017\n",
      "Epoch: 616/1000 | Batch 020/052 | Cost: 0.1415\n",
      "Epoch: 616/1000 | Batch 030/052 | Cost: 0.2071\n",
      "Epoch: 616/1000 | Batch 040/052 | Cost: 0.4128\n",
      "Epoch: 616/1000 | Batch 050/052 | Cost: 0.2900\n",
      "Epoch: 616/1000 training accuracy: 87.77%\n",
      "Epoch: 617/1000 | Batch 000/052 | Cost: 0.2674\n",
      "Epoch: 617/1000 | Batch 010/052 | Cost: 0.2342\n",
      "Epoch: 617/1000 | Batch 020/052 | Cost: 0.1516\n",
      "Epoch: 617/1000 | Batch 030/052 | Cost: 0.3892\n",
      "Epoch: 617/1000 | Batch 040/052 | Cost: 0.5306\n",
      "Epoch: 617/1000 | Batch 050/052 | Cost: 0.1760\n",
      "Epoch: 617/1000 training accuracy: 89.00%\n",
      "Epoch: 618/1000 | Batch 000/052 | Cost: 0.1796\n",
      "Epoch: 618/1000 | Batch 010/052 | Cost: 0.3011\n",
      "Epoch: 618/1000 | Batch 020/052 | Cost: 0.3514\n",
      "Epoch: 618/1000 | Batch 030/052 | Cost: 0.3174\n",
      "Epoch: 618/1000 | Batch 040/052 | Cost: 0.1311\n",
      "Epoch: 618/1000 | Batch 050/052 | Cost: 0.4049\n",
      "Epoch: 618/1000 training accuracy: 88.22%\n",
      "Epoch: 619/1000 | Batch 000/052 | Cost: 0.3344\n",
      "Epoch: 619/1000 | Batch 010/052 | Cost: 0.1383\n",
      "Epoch: 619/1000 | Batch 020/052 | Cost: 0.4219\n",
      "Epoch: 619/1000 | Batch 030/052 | Cost: 0.1365\n",
      "Epoch: 619/1000 | Batch 040/052 | Cost: 0.1298\n",
      "Epoch: 619/1000 | Batch 050/052 | Cost: 0.3484\n",
      "Epoch: 619/1000 training accuracy: 88.66%\n",
      "Epoch: 620/1000 | Batch 000/052 | Cost: 0.1715\n",
      "Epoch: 620/1000 | Batch 010/052 | Cost: 0.6310\n",
      "Epoch: 620/1000 | Batch 020/052 | Cost: 0.4385\n",
      "Epoch: 620/1000 | Batch 030/052 | Cost: 0.4524\n",
      "Epoch: 620/1000 | Batch 040/052 | Cost: 0.3100\n",
      "Epoch: 620/1000 | Batch 050/052 | Cost: 0.2094\n",
      "Epoch: 620/1000 training accuracy: 87.65%\n",
      "Epoch: 621/1000 | Batch 000/052 | Cost: 0.3061\n",
      "Epoch: 621/1000 | Batch 010/052 | Cost: 0.3336\n",
      "Epoch: 621/1000 | Batch 020/052 | Cost: 0.1400\n",
      "Epoch: 621/1000 | Batch 030/052 | Cost: 0.3677\n",
      "Epoch: 621/1000 | Batch 040/052 | Cost: 0.4354\n",
      "Epoch: 621/1000 | Batch 050/052 | Cost: 0.3873\n",
      "Epoch: 621/1000 training accuracy: 87.43%\n",
      "Epoch: 622/1000 | Batch 000/052 | Cost: 0.1245\n",
      "Epoch: 622/1000 | Batch 010/052 | Cost: 0.1425\n",
      "Epoch: 622/1000 | Batch 020/052 | Cost: 0.2452\n",
      "Epoch: 622/1000 | Batch 030/052 | Cost: 0.0813\n",
      "Epoch: 622/1000 | Batch 040/052 | Cost: 0.2967\n",
      "Epoch: 622/1000 | Batch 050/052 | Cost: 0.3078\n",
      "Epoch: 622/1000 training accuracy: 88.78%\n",
      "Epoch: 623/1000 | Batch 000/052 | Cost: 0.2105\n",
      "Epoch: 623/1000 | Batch 010/052 | Cost: 0.2661\n",
      "Epoch: 623/1000 | Batch 020/052 | Cost: 0.2645\n",
      "Epoch: 623/1000 | Batch 030/052 | Cost: 0.1774\n",
      "Epoch: 623/1000 | Batch 040/052 | Cost: 0.3752\n",
      "Epoch: 623/1000 | Batch 050/052 | Cost: 0.1775\n",
      "Epoch: 623/1000 training accuracy: 88.78%\n",
      "Epoch: 624/1000 | Batch 000/052 | Cost: 0.2525\n",
      "Epoch: 624/1000 | Batch 010/052 | Cost: 0.3387\n",
      "Epoch: 624/1000 | Batch 020/052 | Cost: 0.3702\n",
      "Epoch: 624/1000 | Batch 030/052 | Cost: 0.2703\n",
      "Epoch: 624/1000 | Batch 040/052 | Cost: 0.3164\n",
      "Epoch: 624/1000 | Batch 050/052 | Cost: 0.2848\n",
      "Epoch: 624/1000 training accuracy: 88.33%\n",
      "Epoch: 625/1000 | Batch 000/052 | Cost: 0.1155\n",
      "Epoch: 625/1000 | Batch 010/052 | Cost: 0.7122\n",
      "Epoch: 625/1000 | Batch 020/052 | Cost: 0.2333\n",
      "Epoch: 625/1000 | Batch 030/052 | Cost: 0.2562\n",
      "Epoch: 625/1000 | Batch 040/052 | Cost: 0.4150\n",
      "Epoch: 625/1000 | Batch 050/052 | Cost: 0.1251\n",
      "Epoch: 625/1000 training accuracy: 88.22%\n",
      "Epoch: 626/1000 | Batch 000/052 | Cost: 0.3109\n",
      "Epoch: 626/1000 | Batch 010/052 | Cost: 0.5040\n",
      "Epoch: 626/1000 | Batch 020/052 | Cost: 0.3815\n",
      "Epoch: 626/1000 | Batch 030/052 | Cost: 0.4631\n",
      "Epoch: 626/1000 | Batch 040/052 | Cost: 0.1344\n",
      "Epoch: 626/1000 | Batch 050/052 | Cost: 0.7450\n",
      "Epoch: 626/1000 training accuracy: 87.99%\n",
      "Epoch: 627/1000 | Batch 000/052 | Cost: 0.4802\n",
      "Epoch: 627/1000 | Batch 010/052 | Cost: 0.2743\n",
      "Epoch: 627/1000 | Batch 020/052 | Cost: 0.3961\n",
      "Epoch: 627/1000 | Batch 030/052 | Cost: 0.1179\n",
      "Epoch: 627/1000 | Batch 040/052 | Cost: 0.3221\n",
      "Epoch: 627/1000 | Batch 050/052 | Cost: 0.4558\n",
      "Epoch: 627/1000 training accuracy: 88.89%\n",
      "Epoch: 628/1000 | Batch 000/052 | Cost: 0.2282\n",
      "Epoch: 628/1000 | Batch 010/052 | Cost: 0.2212\n",
      "Epoch: 628/1000 | Batch 020/052 | Cost: 0.1951\n",
      "Epoch: 628/1000 | Batch 030/052 | Cost: 0.3590\n",
      "Epoch: 628/1000 | Batch 040/052 | Cost: 0.3267\n",
      "Epoch: 628/1000 | Batch 050/052 | Cost: 0.3552\n",
      "Epoch: 628/1000 training accuracy: 88.55%\n",
      "Epoch: 629/1000 | Batch 000/052 | Cost: 0.2088\n",
      "Epoch: 629/1000 | Batch 010/052 | Cost: 0.2595\n",
      "Epoch: 629/1000 | Batch 020/052 | Cost: 0.1910\n",
      "Epoch: 629/1000 | Batch 030/052 | Cost: 0.1859\n",
      "Epoch: 629/1000 | Batch 040/052 | Cost: 0.1840\n",
      "Epoch: 629/1000 | Batch 050/052 | Cost: 0.2808\n",
      "Epoch: 629/1000 training accuracy: 88.22%\n",
      "Epoch: 630/1000 | Batch 000/052 | Cost: 0.4821\n",
      "Epoch: 630/1000 | Batch 010/052 | Cost: 0.2148\n",
      "Epoch: 630/1000 | Batch 020/052 | Cost: 0.3475\n",
      "Epoch: 630/1000 | Batch 030/052 | Cost: 0.1323\n",
      "Epoch: 630/1000 | Batch 040/052 | Cost: 0.2755\n",
      "Epoch: 630/1000 | Batch 050/052 | Cost: 0.6036\n",
      "Epoch: 630/1000 training accuracy: 88.66%\n",
      "Epoch: 631/1000 | Batch 000/052 | Cost: 0.2034\n",
      "Epoch: 631/1000 | Batch 010/052 | Cost: 0.2010\n",
      "Epoch: 631/1000 | Batch 020/052 | Cost: 0.3047\n",
      "Epoch: 631/1000 | Batch 030/052 | Cost: 0.4324\n",
      "Epoch: 631/1000 | Batch 040/052 | Cost: 0.2575\n",
      "Epoch: 631/1000 | Batch 050/052 | Cost: 0.3212\n",
      "Epoch: 631/1000 training accuracy: 87.99%\n",
      "Epoch: 632/1000 | Batch 000/052 | Cost: 0.3969\n",
      "Epoch: 632/1000 | Batch 010/052 | Cost: 0.2660\n",
      "Epoch: 632/1000 | Batch 020/052 | Cost: 0.3146\n",
      "Epoch: 632/1000 | Batch 030/052 | Cost: 0.1723\n",
      "Epoch: 632/1000 | Batch 040/052 | Cost: 0.1164\n",
      "Epoch: 632/1000 | Batch 050/052 | Cost: 0.1719\n",
      "Epoch: 632/1000 training accuracy: 87.65%\n",
      "Epoch: 633/1000 | Batch 000/052 | Cost: 0.1743\n",
      "Epoch: 633/1000 | Batch 010/052 | Cost: 0.1572\n",
      "Epoch: 633/1000 | Batch 020/052 | Cost: 0.3508\n",
      "Epoch: 633/1000 | Batch 030/052 | Cost: 0.0894\n",
      "Epoch: 633/1000 | Batch 040/052 | Cost: 0.1817\n",
      "Epoch: 633/1000 | Batch 050/052 | Cost: 0.1000\n",
      "Epoch: 633/1000 training accuracy: 87.65%\n",
      "Epoch: 634/1000 | Batch 000/052 | Cost: 0.3015\n",
      "Epoch: 634/1000 | Batch 010/052 | Cost: 0.2940\n",
      "Epoch: 634/1000 | Batch 020/052 | Cost: 0.5156\n",
      "Epoch: 634/1000 | Batch 030/052 | Cost: 0.3115\n",
      "Epoch: 634/1000 | Batch 040/052 | Cost: 0.5252\n",
      "Epoch: 634/1000 | Batch 050/052 | Cost: 0.2077\n",
      "Epoch: 634/1000 training accuracy: 87.54%\n",
      "Epoch: 635/1000 | Batch 000/052 | Cost: 0.3446\n",
      "Epoch: 635/1000 | Batch 010/052 | Cost: 0.1591\n",
      "Epoch: 635/1000 | Batch 020/052 | Cost: 0.3259\n",
      "Epoch: 635/1000 | Batch 030/052 | Cost: 0.5237\n",
      "Epoch: 635/1000 | Batch 040/052 | Cost: 0.3688\n",
      "Epoch: 635/1000 | Batch 050/052 | Cost: 0.1863\n",
      "Epoch: 635/1000 training accuracy: 87.65%\n",
      "Epoch: 636/1000 | Batch 000/052 | Cost: 0.0747\n",
      "Epoch: 636/1000 | Batch 010/052 | Cost: 0.3150\n",
      "Epoch: 636/1000 | Batch 020/052 | Cost: 0.1337\n",
      "Epoch: 636/1000 | Batch 030/052 | Cost: 0.5298\n",
      "Epoch: 636/1000 | Batch 040/052 | Cost: 0.3422\n",
      "Epoch: 636/1000 | Batch 050/052 | Cost: 0.2792\n",
      "Epoch: 636/1000 training accuracy: 88.10%\n",
      "Epoch: 637/1000 | Batch 000/052 | Cost: 0.3358\n",
      "Epoch: 637/1000 | Batch 010/052 | Cost: 0.1810\n",
      "Epoch: 637/1000 | Batch 020/052 | Cost: 0.2754\n",
      "Epoch: 637/1000 | Batch 030/052 | Cost: 0.1181\n",
      "Epoch: 637/1000 | Batch 040/052 | Cost: 0.1912\n",
      "Epoch: 637/1000 | Batch 050/052 | Cost: 0.1895\n",
      "Epoch: 637/1000 training accuracy: 88.44%\n",
      "Epoch: 638/1000 | Batch 000/052 | Cost: 0.3675\n",
      "Epoch: 638/1000 | Batch 010/052 | Cost: 0.2782\n",
      "Epoch: 638/1000 | Batch 020/052 | Cost: 0.2477\n",
      "Epoch: 638/1000 | Batch 030/052 | Cost: 0.0966\n",
      "Epoch: 638/1000 | Batch 040/052 | Cost: 0.1247\n",
      "Epoch: 638/1000 | Batch 050/052 | Cost: 0.1455\n",
      "Epoch: 638/1000 training accuracy: 88.22%\n",
      "Epoch: 639/1000 | Batch 000/052 | Cost: 0.1099\n",
      "Epoch: 639/1000 | Batch 010/052 | Cost: 0.2836\n",
      "Epoch: 639/1000 | Batch 020/052 | Cost: 0.6190\n",
      "Epoch: 639/1000 | Batch 030/052 | Cost: 0.4721\n",
      "Epoch: 639/1000 | Batch 040/052 | Cost: 0.1154\n",
      "Epoch: 639/1000 | Batch 050/052 | Cost: 0.4818\n",
      "Epoch: 639/1000 training accuracy: 88.78%\n",
      "Epoch: 640/1000 | Batch 000/052 | Cost: 0.2932\n",
      "Epoch: 640/1000 | Batch 010/052 | Cost: 0.3690\n",
      "Epoch: 640/1000 | Batch 020/052 | Cost: 0.0767\n",
      "Epoch: 640/1000 | Batch 030/052 | Cost: 0.2585\n",
      "Epoch: 640/1000 | Batch 040/052 | Cost: 0.3900\n",
      "Epoch: 640/1000 | Batch 050/052 | Cost: 0.1390\n",
      "Epoch: 640/1000 training accuracy: 88.44%\n",
      "Epoch: 641/1000 | Batch 000/052 | Cost: 0.2818\n",
      "Epoch: 641/1000 | Batch 010/052 | Cost: 0.2518\n",
      "Epoch: 641/1000 | Batch 020/052 | Cost: 0.2850\n",
      "Epoch: 641/1000 | Batch 030/052 | Cost: 0.2968\n",
      "Epoch: 641/1000 | Batch 040/052 | Cost: 0.2333\n",
      "Epoch: 641/1000 | Batch 050/052 | Cost: 0.2377\n",
      "Epoch: 641/1000 training accuracy: 87.54%\n",
      "Epoch: 642/1000 | Batch 000/052 | Cost: 0.2611\n",
      "Epoch: 642/1000 | Batch 010/052 | Cost: 0.1525\n",
      "Epoch: 642/1000 | Batch 020/052 | Cost: 0.2795\n",
      "Epoch: 642/1000 | Batch 030/052 | Cost: 0.3573\n",
      "Epoch: 642/1000 | Batch 040/052 | Cost: 0.4050\n",
      "Epoch: 642/1000 | Batch 050/052 | Cost: 0.4135\n",
      "Epoch: 642/1000 training accuracy: 89.00%\n",
      "Epoch: 643/1000 | Batch 000/052 | Cost: 0.2075\n",
      "Epoch: 643/1000 | Batch 010/052 | Cost: 0.5736\n",
      "Epoch: 643/1000 | Batch 020/052 | Cost: 0.1729\n",
      "Epoch: 643/1000 | Batch 030/052 | Cost: 0.2835\n",
      "Epoch: 643/1000 | Batch 040/052 | Cost: 0.3590\n",
      "Epoch: 643/1000 | Batch 050/052 | Cost: 0.2300\n",
      "Epoch: 643/1000 training accuracy: 87.77%\n",
      "Epoch: 644/1000 | Batch 000/052 | Cost: 0.1764\n",
      "Epoch: 644/1000 | Batch 010/052 | Cost: 0.5203\n",
      "Epoch: 644/1000 | Batch 020/052 | Cost: 0.1385\n",
      "Epoch: 644/1000 | Batch 030/052 | Cost: 0.1544\n",
      "Epoch: 644/1000 | Batch 040/052 | Cost: 0.4824\n",
      "Epoch: 644/1000 | Batch 050/052 | Cost: 0.1774\n",
      "Epoch: 644/1000 training accuracy: 88.44%\n",
      "Epoch: 645/1000 | Batch 000/052 | Cost: 0.1059\n",
      "Epoch: 645/1000 | Batch 010/052 | Cost: 0.2572\n",
      "Epoch: 645/1000 | Batch 020/052 | Cost: 0.3281\n",
      "Epoch: 645/1000 | Batch 030/052 | Cost: 0.5234\n",
      "Epoch: 645/1000 | Batch 040/052 | Cost: 0.1512\n",
      "Epoch: 645/1000 | Batch 050/052 | Cost: 0.3012\n",
      "Epoch: 645/1000 training accuracy: 89.56%\n",
      "Epoch: 646/1000 | Batch 000/052 | Cost: 0.4096\n",
      "Epoch: 646/1000 | Batch 010/052 | Cost: 0.4532\n",
      "Epoch: 646/1000 | Batch 020/052 | Cost: 0.1530\n",
      "Epoch: 646/1000 | Batch 030/052 | Cost: 0.1635\n",
      "Epoch: 646/1000 | Batch 040/052 | Cost: 0.1221\n",
      "Epoch: 646/1000 | Batch 050/052 | Cost: 0.1465\n",
      "Epoch: 646/1000 training accuracy: 88.33%\n",
      "Epoch: 647/1000 | Batch 000/052 | Cost: 0.2270\n",
      "Epoch: 647/1000 | Batch 010/052 | Cost: 0.3103\n",
      "Epoch: 647/1000 | Batch 020/052 | Cost: 0.5145\n",
      "Epoch: 647/1000 | Batch 030/052 | Cost: 0.3693\n",
      "Epoch: 647/1000 | Batch 040/052 | Cost: 0.2014\n",
      "Epoch: 647/1000 | Batch 050/052 | Cost: 0.1316\n",
      "Epoch: 647/1000 training accuracy: 88.66%\n",
      "Epoch: 648/1000 | Batch 000/052 | Cost: 0.0990\n",
      "Epoch: 648/1000 | Batch 010/052 | Cost: 0.2329\n",
      "Epoch: 648/1000 | Batch 020/052 | Cost: 0.2414\n",
      "Epoch: 648/1000 | Batch 030/052 | Cost: 0.3006\n",
      "Epoch: 648/1000 | Batch 040/052 | Cost: 0.1060\n",
      "Epoch: 648/1000 | Batch 050/052 | Cost: 0.4043\n",
      "Epoch: 648/1000 training accuracy: 87.99%\n",
      "Epoch: 649/1000 | Batch 000/052 | Cost: 0.2843\n",
      "Epoch: 649/1000 | Batch 010/052 | Cost: 0.1556\n",
      "Epoch: 649/1000 | Batch 020/052 | Cost: 0.4858\n",
      "Epoch: 649/1000 | Batch 030/052 | Cost: 0.6188\n",
      "Epoch: 649/1000 | Batch 040/052 | Cost: 0.2422\n",
      "Epoch: 649/1000 | Batch 050/052 | Cost: 0.1036\n",
      "Epoch: 649/1000 training accuracy: 87.88%\n",
      "Epoch: 650/1000 | Batch 000/052 | Cost: 0.1932\n",
      "Epoch: 650/1000 | Batch 010/052 | Cost: 0.1326\n",
      "Epoch: 650/1000 | Batch 020/052 | Cost: 0.3366\n",
      "Epoch: 650/1000 | Batch 030/052 | Cost: 0.2535\n",
      "Epoch: 650/1000 | Batch 040/052 | Cost: 0.1050\n",
      "Epoch: 650/1000 | Batch 050/052 | Cost: 0.1156\n",
      "Epoch: 650/1000 training accuracy: 88.89%\n",
      "Epoch: 651/1000 | Batch 000/052 | Cost: 0.2377\n",
      "Epoch: 651/1000 | Batch 010/052 | Cost: 0.3092\n",
      "Epoch: 651/1000 | Batch 020/052 | Cost: 0.3280\n",
      "Epoch: 651/1000 | Batch 030/052 | Cost: 0.2268\n",
      "Epoch: 651/1000 | Batch 040/052 | Cost: 0.2365\n",
      "Epoch: 651/1000 | Batch 050/052 | Cost: 0.2409\n",
      "Epoch: 651/1000 training accuracy: 88.22%\n",
      "Epoch: 652/1000 | Batch 000/052 | Cost: 0.2688\n",
      "Epoch: 652/1000 | Batch 010/052 | Cost: 0.1098\n",
      "Epoch: 652/1000 | Batch 020/052 | Cost: 0.4756\n",
      "Epoch: 652/1000 | Batch 030/052 | Cost: 0.2447\n",
      "Epoch: 652/1000 | Batch 040/052 | Cost: 0.2166\n",
      "Epoch: 652/1000 | Batch 050/052 | Cost: 0.1389\n",
      "Epoch: 652/1000 training accuracy: 88.33%\n",
      "Epoch: 653/1000 | Batch 000/052 | Cost: 0.2723\n",
      "Epoch: 653/1000 | Batch 010/052 | Cost: 0.2766\n",
      "Epoch: 653/1000 | Batch 020/052 | Cost: 0.2378\n",
      "Epoch: 653/1000 | Batch 030/052 | Cost: 0.1329\n",
      "Epoch: 653/1000 | Batch 040/052 | Cost: 0.1653\n",
      "Epoch: 653/1000 | Batch 050/052 | Cost: 0.2516\n",
      "Epoch: 653/1000 training accuracy: 87.43%\n",
      "Epoch: 654/1000 | Batch 000/052 | Cost: 0.2737\n",
      "Epoch: 654/1000 | Batch 010/052 | Cost: 0.6718\n",
      "Epoch: 654/1000 | Batch 020/052 | Cost: 0.1193\n",
      "Epoch: 654/1000 | Batch 030/052 | Cost: 0.2430\n",
      "Epoch: 654/1000 | Batch 040/052 | Cost: 0.2567\n",
      "Epoch: 654/1000 | Batch 050/052 | Cost: 0.2176\n",
      "Epoch: 654/1000 training accuracy: 87.88%\n",
      "Epoch: 655/1000 | Batch 000/052 | Cost: 0.2666\n",
      "Epoch: 655/1000 | Batch 010/052 | Cost: 0.1311\n",
      "Epoch: 655/1000 | Batch 020/052 | Cost: 0.1579\n",
      "Epoch: 655/1000 | Batch 030/052 | Cost: 0.1559\n",
      "Epoch: 655/1000 | Batch 040/052 | Cost: 0.2463\n",
      "Epoch: 655/1000 | Batch 050/052 | Cost: 0.2291\n",
      "Epoch: 655/1000 training accuracy: 88.44%\n",
      "Epoch: 656/1000 | Batch 000/052 | Cost: 0.3968\n",
      "Epoch: 656/1000 | Batch 010/052 | Cost: 0.2234\n",
      "Epoch: 656/1000 | Batch 020/052 | Cost: 0.2751\n",
      "Epoch: 656/1000 | Batch 030/052 | Cost: 0.5733\n",
      "Epoch: 656/1000 | Batch 040/052 | Cost: 0.1450\n",
      "Epoch: 656/1000 | Batch 050/052 | Cost: 0.2635\n",
      "Epoch: 656/1000 training accuracy: 88.55%\n",
      "Epoch: 657/1000 | Batch 000/052 | Cost: 0.1194\n",
      "Epoch: 657/1000 | Batch 010/052 | Cost: 0.3833\n",
      "Epoch: 657/1000 | Batch 020/052 | Cost: 0.2094\n",
      "Epoch: 657/1000 | Batch 030/052 | Cost: 0.4784\n",
      "Epoch: 657/1000 | Batch 040/052 | Cost: 0.2784\n",
      "Epoch: 657/1000 | Batch 050/052 | Cost: 0.1722\n",
      "Epoch: 657/1000 training accuracy: 88.33%\n",
      "Epoch: 658/1000 | Batch 000/052 | Cost: 0.1444\n",
      "Epoch: 658/1000 | Batch 010/052 | Cost: 0.2150\n",
      "Epoch: 658/1000 | Batch 020/052 | Cost: 0.3933\n",
      "Epoch: 658/1000 | Batch 030/052 | Cost: 0.2558\n",
      "Epoch: 658/1000 | Batch 040/052 | Cost: 0.1233\n",
      "Epoch: 658/1000 | Batch 050/052 | Cost: 0.1418\n",
      "Epoch: 658/1000 training accuracy: 88.10%\n",
      "Epoch: 659/1000 | Batch 000/052 | Cost: 0.2848\n",
      "Epoch: 659/1000 | Batch 010/052 | Cost: 0.3415\n",
      "Epoch: 659/1000 | Batch 020/052 | Cost: 0.1509\n",
      "Epoch: 659/1000 | Batch 030/052 | Cost: 0.1704\n",
      "Epoch: 659/1000 | Batch 040/052 | Cost: 0.4896\n",
      "Epoch: 659/1000 | Batch 050/052 | Cost: 0.3680\n",
      "Epoch: 659/1000 training accuracy: 88.78%\n",
      "Epoch: 660/1000 | Batch 000/052 | Cost: 0.7867\n",
      "Epoch: 660/1000 | Batch 010/052 | Cost: 0.1920\n",
      "Epoch: 660/1000 | Batch 020/052 | Cost: 0.2846\n",
      "Epoch: 660/1000 | Batch 030/052 | Cost: 0.1606\n",
      "Epoch: 660/1000 | Batch 040/052 | Cost: 0.2271\n",
      "Epoch: 660/1000 | Batch 050/052 | Cost: 0.4057\n",
      "Epoch: 660/1000 training accuracy: 88.55%\n",
      "Epoch: 661/1000 | Batch 000/052 | Cost: 0.3855\n",
      "Epoch: 661/1000 | Batch 010/052 | Cost: 0.2829\n",
      "Epoch: 661/1000 | Batch 020/052 | Cost: 0.2470\n",
      "Epoch: 661/1000 | Batch 030/052 | Cost: 0.1258\n",
      "Epoch: 661/1000 | Batch 040/052 | Cost: 0.1640\n",
      "Epoch: 661/1000 | Batch 050/052 | Cost: 0.2624\n",
      "Epoch: 661/1000 training accuracy: 88.55%\n",
      "Epoch: 662/1000 | Batch 000/052 | Cost: 0.2423\n",
      "Epoch: 662/1000 | Batch 010/052 | Cost: 0.2436\n",
      "Epoch: 662/1000 | Batch 020/052 | Cost: 0.3719\n",
      "Epoch: 662/1000 | Batch 030/052 | Cost: 0.3084\n",
      "Epoch: 662/1000 | Batch 040/052 | Cost: 0.1357\n",
      "Epoch: 662/1000 | Batch 050/052 | Cost: 0.2972\n",
      "Epoch: 662/1000 training accuracy: 87.99%\n",
      "Epoch: 663/1000 | Batch 000/052 | Cost: 0.2750\n",
      "Epoch: 663/1000 | Batch 010/052 | Cost: 0.5012\n",
      "Epoch: 663/1000 | Batch 020/052 | Cost: 0.1956\n",
      "Epoch: 663/1000 | Batch 030/052 | Cost: 0.2360\n",
      "Epoch: 663/1000 | Batch 040/052 | Cost: 0.1402\n",
      "Epoch: 663/1000 | Batch 050/052 | Cost: 0.2325\n",
      "Epoch: 663/1000 training accuracy: 87.88%\n",
      "Epoch: 664/1000 | Batch 000/052 | Cost: 0.1732\n",
      "Epoch: 664/1000 | Batch 010/052 | Cost: 0.1594\n",
      "Epoch: 664/1000 | Batch 020/052 | Cost: 0.1838\n",
      "Epoch: 664/1000 | Batch 030/052 | Cost: 0.1740\n",
      "Epoch: 664/1000 | Batch 040/052 | Cost: 0.4016\n",
      "Epoch: 664/1000 | Batch 050/052 | Cost: 0.1791\n",
      "Epoch: 664/1000 training accuracy: 88.44%\n",
      "Epoch: 665/1000 | Batch 000/052 | Cost: 0.2442\n",
      "Epoch: 665/1000 | Batch 010/052 | Cost: 0.1348\n",
      "Epoch: 665/1000 | Batch 020/052 | Cost: 0.3048\n",
      "Epoch: 665/1000 | Batch 030/052 | Cost: 0.1757\n",
      "Epoch: 665/1000 | Batch 040/052 | Cost: 0.2506\n",
      "Epoch: 665/1000 | Batch 050/052 | Cost: 0.1203\n",
      "Epoch: 665/1000 training accuracy: 88.44%\n",
      "Epoch: 666/1000 | Batch 000/052 | Cost: 0.4186\n",
      "Epoch: 666/1000 | Batch 010/052 | Cost: 0.2936\n",
      "Epoch: 666/1000 | Batch 020/052 | Cost: 0.0832\n",
      "Epoch: 666/1000 | Batch 030/052 | Cost: 0.1449\n",
      "Epoch: 666/1000 | Batch 040/052 | Cost: 0.2316\n",
      "Epoch: 666/1000 | Batch 050/052 | Cost: 0.4727\n",
      "Epoch: 666/1000 training accuracy: 88.66%\n",
      "Epoch: 667/1000 | Batch 000/052 | Cost: 0.3474\n",
      "Epoch: 667/1000 | Batch 010/052 | Cost: 0.2268\n",
      "Epoch: 667/1000 | Batch 020/052 | Cost: 0.4781\n",
      "Epoch: 667/1000 | Batch 030/052 | Cost: 0.1908\n",
      "Epoch: 667/1000 | Batch 040/052 | Cost: 0.2635\n",
      "Epoch: 667/1000 | Batch 050/052 | Cost: 0.2971\n",
      "Epoch: 667/1000 training accuracy: 88.89%\n",
      "Epoch: 668/1000 | Batch 000/052 | Cost: 0.2232\n",
      "Epoch: 668/1000 | Batch 010/052 | Cost: 0.0702\n",
      "Epoch: 668/1000 | Batch 020/052 | Cost: 0.3416\n",
      "Epoch: 668/1000 | Batch 030/052 | Cost: 0.3439\n",
      "Epoch: 668/1000 | Batch 040/052 | Cost: 0.4400\n",
      "Epoch: 668/1000 | Batch 050/052 | Cost: 0.1689\n",
      "Epoch: 668/1000 training accuracy: 88.66%\n",
      "Epoch: 669/1000 | Batch 000/052 | Cost: 0.1107\n",
      "Epoch: 669/1000 | Batch 010/052 | Cost: 0.2606\n",
      "Epoch: 669/1000 | Batch 020/052 | Cost: 0.2257\n",
      "Epoch: 669/1000 | Batch 030/052 | Cost: 0.3251\n",
      "Epoch: 669/1000 | Batch 040/052 | Cost: 0.2342\n",
      "Epoch: 669/1000 | Batch 050/052 | Cost: 0.5501\n",
      "Epoch: 669/1000 training accuracy: 89.23%\n",
      "Epoch: 670/1000 | Batch 000/052 | Cost: 0.2546\n",
      "Epoch: 670/1000 | Batch 010/052 | Cost: 0.1837\n",
      "Epoch: 670/1000 | Batch 020/052 | Cost: 0.1801\n",
      "Epoch: 670/1000 | Batch 030/052 | Cost: 0.2101\n",
      "Epoch: 670/1000 | Batch 040/052 | Cost: 0.1068\n",
      "Epoch: 670/1000 | Batch 050/052 | Cost: 0.2668\n",
      "Epoch: 670/1000 training accuracy: 88.22%\n",
      "Epoch: 671/1000 | Batch 000/052 | Cost: 0.4742\n",
      "Epoch: 671/1000 | Batch 010/052 | Cost: 0.1137\n",
      "Epoch: 671/1000 | Batch 020/052 | Cost: 0.4582\n",
      "Epoch: 671/1000 | Batch 030/052 | Cost: 0.0604\n",
      "Epoch: 671/1000 | Batch 040/052 | Cost: 0.3079\n",
      "Epoch: 671/1000 | Batch 050/052 | Cost: 0.5419\n",
      "Epoch: 671/1000 training accuracy: 88.78%\n",
      "Epoch: 672/1000 | Batch 000/052 | Cost: 0.5520\n",
      "Epoch: 672/1000 | Batch 010/052 | Cost: 0.1688\n",
      "Epoch: 672/1000 | Batch 020/052 | Cost: 0.1832\n",
      "Epoch: 672/1000 | Batch 030/052 | Cost: 0.2025\n",
      "Epoch: 672/1000 | Batch 040/052 | Cost: 0.2749\n",
      "Epoch: 672/1000 | Batch 050/052 | Cost: 0.2860\n",
      "Epoch: 672/1000 training accuracy: 89.45%\n",
      "Epoch: 673/1000 | Batch 000/052 | Cost: 0.2773\n",
      "Epoch: 673/1000 | Batch 010/052 | Cost: 0.1254\n",
      "Epoch: 673/1000 | Batch 020/052 | Cost: 0.4752\n",
      "Epoch: 673/1000 | Batch 030/052 | Cost: 0.1990\n",
      "Epoch: 673/1000 | Batch 040/052 | Cost: 0.3664\n",
      "Epoch: 673/1000 | Batch 050/052 | Cost: 0.1792\n",
      "Epoch: 673/1000 training accuracy: 87.88%\n",
      "Epoch: 674/1000 | Batch 000/052 | Cost: 0.2748\n",
      "Epoch: 674/1000 | Batch 010/052 | Cost: 0.3233\n",
      "Epoch: 674/1000 | Batch 020/052 | Cost: 0.2697\n",
      "Epoch: 674/1000 | Batch 030/052 | Cost: 0.2490\n",
      "Epoch: 674/1000 | Batch 040/052 | Cost: 0.3702\n",
      "Epoch: 674/1000 | Batch 050/052 | Cost: 0.1071\n",
      "Epoch: 674/1000 training accuracy: 87.99%\n",
      "Epoch: 675/1000 | Batch 000/052 | Cost: 0.2564\n",
      "Epoch: 675/1000 | Batch 010/052 | Cost: 0.0610\n",
      "Epoch: 675/1000 | Batch 020/052 | Cost: 0.2339\n",
      "Epoch: 675/1000 | Batch 030/052 | Cost: 0.3055\n",
      "Epoch: 675/1000 | Batch 040/052 | Cost: 0.5070\n",
      "Epoch: 675/1000 | Batch 050/052 | Cost: 0.4261\n",
      "Epoch: 675/1000 training accuracy: 88.44%\n",
      "Epoch: 676/1000 | Batch 000/052 | Cost: 0.1373\n",
      "Epoch: 676/1000 | Batch 010/052 | Cost: 0.3437\n",
      "Epoch: 676/1000 | Batch 020/052 | Cost: 0.2912\n",
      "Epoch: 676/1000 | Batch 030/052 | Cost: 0.2540\n",
      "Epoch: 676/1000 | Batch 040/052 | Cost: 0.4276\n",
      "Epoch: 676/1000 | Batch 050/052 | Cost: 0.0769\n",
      "Epoch: 676/1000 training accuracy: 87.88%\n",
      "Epoch: 677/1000 | Batch 000/052 | Cost: 0.3996\n",
      "Epoch: 677/1000 | Batch 010/052 | Cost: 0.3835\n",
      "Epoch: 677/1000 | Batch 020/052 | Cost: 0.1529\n",
      "Epoch: 677/1000 | Batch 030/052 | Cost: 0.1911\n",
      "Epoch: 677/1000 | Batch 040/052 | Cost: 0.3033\n",
      "Epoch: 677/1000 | Batch 050/052 | Cost: 0.4129\n",
      "Epoch: 677/1000 training accuracy: 89.00%\n",
      "Epoch: 678/1000 | Batch 000/052 | Cost: 0.2197\n",
      "Epoch: 678/1000 | Batch 010/052 | Cost: 0.2673\n",
      "Epoch: 678/1000 | Batch 020/052 | Cost: 0.1065\n",
      "Epoch: 678/1000 | Batch 030/052 | Cost: 0.2250\n",
      "Epoch: 678/1000 | Batch 040/052 | Cost: 0.4082\n",
      "Epoch: 678/1000 | Batch 050/052 | Cost: 0.1542\n",
      "Epoch: 678/1000 training accuracy: 88.89%\n",
      "Epoch: 679/1000 | Batch 000/052 | Cost: 0.4531\n",
      "Epoch: 679/1000 | Batch 010/052 | Cost: 0.4330\n",
      "Epoch: 679/1000 | Batch 020/052 | Cost: 0.2002\n",
      "Epoch: 679/1000 | Batch 030/052 | Cost: 0.4883\n",
      "Epoch: 679/1000 | Batch 040/052 | Cost: 0.2760\n",
      "Epoch: 679/1000 | Batch 050/052 | Cost: 0.0773\n",
      "Epoch: 679/1000 training accuracy: 89.00%\n",
      "Epoch: 680/1000 | Batch 000/052 | Cost: 0.1199\n",
      "Epoch: 680/1000 | Batch 010/052 | Cost: 0.3422\n",
      "Epoch: 680/1000 | Batch 020/052 | Cost: 0.3677\n",
      "Epoch: 680/1000 | Batch 030/052 | Cost: 0.1573\n",
      "Epoch: 680/1000 | Batch 040/052 | Cost: 0.1249\n",
      "Epoch: 680/1000 | Batch 050/052 | Cost: 0.6057\n",
      "Epoch: 680/1000 training accuracy: 89.45%\n",
      "Epoch: 681/1000 | Batch 000/052 | Cost: 0.2045\n",
      "Epoch: 681/1000 | Batch 010/052 | Cost: 0.2645\n",
      "Epoch: 681/1000 | Batch 020/052 | Cost: 0.1769\n",
      "Epoch: 681/1000 | Batch 030/052 | Cost: 0.4842\n",
      "Epoch: 681/1000 | Batch 040/052 | Cost: 0.2179\n",
      "Epoch: 681/1000 | Batch 050/052 | Cost: 0.2523\n",
      "Epoch: 681/1000 training accuracy: 88.89%\n",
      "Epoch: 682/1000 | Batch 000/052 | Cost: 0.2111\n",
      "Epoch: 682/1000 | Batch 010/052 | Cost: 0.2289\n",
      "Epoch: 682/1000 | Batch 020/052 | Cost: 0.4231\n",
      "Epoch: 682/1000 | Batch 030/052 | Cost: 0.0537\n",
      "Epoch: 682/1000 | Batch 040/052 | Cost: 0.3621\n",
      "Epoch: 682/1000 | Batch 050/052 | Cost: 0.2382\n",
      "Epoch: 682/1000 training accuracy: 88.55%\n",
      "Epoch: 683/1000 | Batch 000/052 | Cost: 0.2403\n",
      "Epoch: 683/1000 | Batch 010/052 | Cost: 0.1988\n",
      "Epoch: 683/1000 | Batch 020/052 | Cost: 0.2114\n",
      "Epoch: 683/1000 | Batch 030/052 | Cost: 0.4283\n",
      "Epoch: 683/1000 | Batch 040/052 | Cost: 0.3558\n",
      "Epoch: 683/1000 | Batch 050/052 | Cost: 0.3074\n",
      "Epoch: 683/1000 training accuracy: 88.10%\n",
      "Epoch: 684/1000 | Batch 000/052 | Cost: 0.5881\n",
      "Epoch: 684/1000 | Batch 010/052 | Cost: 0.3365\n",
      "Epoch: 684/1000 | Batch 020/052 | Cost: 0.3212\n",
      "Epoch: 684/1000 | Batch 030/052 | Cost: 0.6352\n",
      "Epoch: 684/1000 | Batch 040/052 | Cost: 0.4913\n",
      "Epoch: 684/1000 | Batch 050/052 | Cost: 0.2031\n",
      "Epoch: 684/1000 training accuracy: 88.78%\n",
      "Epoch: 685/1000 | Batch 000/052 | Cost: 0.3207\n",
      "Epoch: 685/1000 | Batch 010/052 | Cost: 0.1871\n",
      "Epoch: 685/1000 | Batch 020/052 | Cost: 0.2716\n",
      "Epoch: 685/1000 | Batch 030/052 | Cost: 0.1938\n",
      "Epoch: 685/1000 | Batch 040/052 | Cost: 0.6551\n",
      "Epoch: 685/1000 | Batch 050/052 | Cost: 0.2043\n",
      "Epoch: 685/1000 training accuracy: 89.67%\n",
      "Epoch: 686/1000 | Batch 000/052 | Cost: 0.4315\n",
      "Epoch: 686/1000 | Batch 010/052 | Cost: 0.2393\n",
      "Epoch: 686/1000 | Batch 020/052 | Cost: 0.1999\n",
      "Epoch: 686/1000 | Batch 030/052 | Cost: 0.1367\n",
      "Epoch: 686/1000 | Batch 040/052 | Cost: 0.2166\n",
      "Epoch: 686/1000 | Batch 050/052 | Cost: 0.2215\n",
      "Epoch: 686/1000 training accuracy: 89.00%\n",
      "Epoch: 687/1000 | Batch 000/052 | Cost: 0.1746\n",
      "Epoch: 687/1000 | Batch 010/052 | Cost: 0.2528\n",
      "Epoch: 687/1000 | Batch 020/052 | Cost: 0.0639\n",
      "Epoch: 687/1000 | Batch 030/052 | Cost: 0.3179\n",
      "Epoch: 687/1000 | Batch 040/052 | Cost: 0.1614\n",
      "Epoch: 687/1000 | Batch 050/052 | Cost: 0.1499\n",
      "Epoch: 687/1000 training accuracy: 89.23%\n",
      "Epoch: 688/1000 | Batch 000/052 | Cost: 0.2781\n",
      "Epoch: 688/1000 | Batch 010/052 | Cost: 0.3454\n",
      "Epoch: 688/1000 | Batch 020/052 | Cost: 0.1769\n",
      "Epoch: 688/1000 | Batch 030/052 | Cost: 0.0624\n",
      "Epoch: 688/1000 | Batch 040/052 | Cost: 0.2660\n",
      "Epoch: 688/1000 | Batch 050/052 | Cost: 0.3558\n",
      "Epoch: 688/1000 training accuracy: 88.89%\n",
      "Epoch: 689/1000 | Batch 000/052 | Cost: 0.3482\n",
      "Epoch: 689/1000 | Batch 010/052 | Cost: 0.6918\n",
      "Epoch: 689/1000 | Batch 020/052 | Cost: 0.3116\n",
      "Epoch: 689/1000 | Batch 030/052 | Cost: 0.3083\n",
      "Epoch: 689/1000 | Batch 040/052 | Cost: 0.1834\n",
      "Epoch: 689/1000 | Batch 050/052 | Cost: 0.1021\n",
      "Epoch: 689/1000 training accuracy: 87.88%\n",
      "Epoch: 690/1000 | Batch 000/052 | Cost: 0.1480\n",
      "Epoch: 690/1000 | Batch 010/052 | Cost: 0.1395\n",
      "Epoch: 690/1000 | Batch 020/052 | Cost: 0.1691\n",
      "Epoch: 690/1000 | Batch 030/052 | Cost: 0.1301\n",
      "Epoch: 690/1000 | Batch 040/052 | Cost: 0.5092\n",
      "Epoch: 690/1000 | Batch 050/052 | Cost: 0.2754\n",
      "Epoch: 690/1000 training accuracy: 87.99%\n",
      "Epoch: 691/1000 | Batch 000/052 | Cost: 0.2491\n",
      "Epoch: 691/1000 | Batch 010/052 | Cost: 0.1271\n",
      "Epoch: 691/1000 | Batch 020/052 | Cost: 0.3190\n",
      "Epoch: 691/1000 | Batch 030/052 | Cost: 0.5739\n",
      "Epoch: 691/1000 | Batch 040/052 | Cost: 0.4426\n",
      "Epoch: 691/1000 | Batch 050/052 | Cost: 0.1535\n",
      "Epoch: 691/1000 training accuracy: 89.00%\n",
      "Epoch: 692/1000 | Batch 000/052 | Cost: 0.2388\n",
      "Epoch: 692/1000 | Batch 010/052 | Cost: 0.4321\n",
      "Epoch: 692/1000 | Batch 020/052 | Cost: 0.0758\n",
      "Epoch: 692/1000 | Batch 030/052 | Cost: 0.3779\n",
      "Epoch: 692/1000 | Batch 040/052 | Cost: 0.2404\n",
      "Epoch: 692/1000 | Batch 050/052 | Cost: 0.4622\n",
      "Epoch: 692/1000 training accuracy: 87.99%\n",
      "Epoch: 693/1000 | Batch 000/052 | Cost: 0.2101\n",
      "Epoch: 693/1000 | Batch 010/052 | Cost: 0.2432\n",
      "Epoch: 693/1000 | Batch 020/052 | Cost: 0.1379\n",
      "Epoch: 693/1000 | Batch 030/052 | Cost: 0.3914\n",
      "Epoch: 693/1000 | Batch 040/052 | Cost: 0.3034\n",
      "Epoch: 693/1000 | Batch 050/052 | Cost: 0.5641\n",
      "Epoch: 693/1000 training accuracy: 88.10%\n",
      "Epoch: 694/1000 | Batch 000/052 | Cost: 0.4898\n",
      "Epoch: 694/1000 | Batch 010/052 | Cost: 0.4924\n",
      "Epoch: 694/1000 | Batch 020/052 | Cost: 0.2884\n",
      "Epoch: 694/1000 | Batch 030/052 | Cost: 0.0919\n",
      "Epoch: 694/1000 | Batch 040/052 | Cost: 0.1481\n",
      "Epoch: 694/1000 | Batch 050/052 | Cost: 0.3131\n",
      "Epoch: 694/1000 training accuracy: 88.89%\n",
      "Epoch: 695/1000 | Batch 000/052 | Cost: 0.2027\n",
      "Epoch: 695/1000 | Batch 010/052 | Cost: 0.2951\n",
      "Epoch: 695/1000 | Batch 020/052 | Cost: 0.1644\n",
      "Epoch: 695/1000 | Batch 030/052 | Cost: 0.1847\n",
      "Epoch: 695/1000 | Batch 040/052 | Cost: 0.2887\n",
      "Epoch: 695/1000 | Batch 050/052 | Cost: 0.4212\n",
      "Epoch: 695/1000 training accuracy: 87.54%\n",
      "Epoch: 696/1000 | Batch 000/052 | Cost: 0.1945\n",
      "Epoch: 696/1000 | Batch 010/052 | Cost: 0.0638\n",
      "Epoch: 696/1000 | Batch 020/052 | Cost: 0.2392\n",
      "Epoch: 696/1000 | Batch 030/052 | Cost: 0.1285\n",
      "Epoch: 696/1000 | Batch 040/052 | Cost: 0.6270\n",
      "Epoch: 696/1000 | Batch 050/052 | Cost: 0.2430\n",
      "Epoch: 696/1000 training accuracy: 88.44%\n",
      "Epoch: 697/1000 | Batch 000/052 | Cost: 0.1317\n",
      "Epoch: 697/1000 | Batch 010/052 | Cost: 0.6288\n",
      "Epoch: 697/1000 | Batch 020/052 | Cost: 0.2292\n",
      "Epoch: 697/1000 | Batch 030/052 | Cost: 0.1750\n",
      "Epoch: 697/1000 | Batch 040/052 | Cost: 0.5275\n",
      "Epoch: 697/1000 | Batch 050/052 | Cost: 0.5257\n",
      "Epoch: 697/1000 training accuracy: 88.66%\n",
      "Epoch: 698/1000 | Batch 000/052 | Cost: 0.2464\n",
      "Epoch: 698/1000 | Batch 010/052 | Cost: 0.2279\n",
      "Epoch: 698/1000 | Batch 020/052 | Cost: 0.3163\n",
      "Epoch: 698/1000 | Batch 030/052 | Cost: 0.5670\n",
      "Epoch: 698/1000 | Batch 040/052 | Cost: 0.1738\n",
      "Epoch: 698/1000 | Batch 050/052 | Cost: 0.6329\n",
      "Epoch: 698/1000 training accuracy: 88.55%\n",
      "Epoch: 699/1000 | Batch 000/052 | Cost: 0.2813\n",
      "Epoch: 699/1000 | Batch 010/052 | Cost: 0.3399\n",
      "Epoch: 699/1000 | Batch 020/052 | Cost: 0.1173\n",
      "Epoch: 699/1000 | Batch 030/052 | Cost: 0.3658\n",
      "Epoch: 699/1000 | Batch 040/052 | Cost: 0.3914\n",
      "Epoch: 699/1000 | Batch 050/052 | Cost: 0.1327\n",
      "Epoch: 699/1000 training accuracy: 88.10%\n",
      "Epoch: 700/1000 | Batch 000/052 | Cost: 0.3940\n",
      "Epoch: 700/1000 | Batch 010/052 | Cost: 0.0785\n",
      "Epoch: 700/1000 | Batch 020/052 | Cost: 0.3048\n",
      "Epoch: 700/1000 | Batch 030/052 | Cost: 0.2546\n",
      "Epoch: 700/1000 | Batch 040/052 | Cost: 0.3548\n",
      "Epoch: 700/1000 | Batch 050/052 | Cost: 0.4700\n",
      "Epoch: 700/1000 training accuracy: 87.65%\n",
      "Epoch: 701/1000 | Batch 000/052 | Cost: 0.3093\n",
      "Epoch: 701/1000 | Batch 010/052 | Cost: 0.1894\n",
      "Epoch: 701/1000 | Batch 020/052 | Cost: 0.2320\n",
      "Epoch: 701/1000 | Batch 030/052 | Cost: 0.3211\n",
      "Epoch: 701/1000 | Batch 040/052 | Cost: 0.2626\n",
      "Epoch: 701/1000 | Batch 050/052 | Cost: 0.3975\n",
      "Epoch: 701/1000 training accuracy: 87.88%\n",
      "Epoch: 702/1000 | Batch 000/052 | Cost: 0.1879\n",
      "Epoch: 702/1000 | Batch 010/052 | Cost: 0.2938\n",
      "Epoch: 702/1000 | Batch 020/052 | Cost: 0.1375\n",
      "Epoch: 702/1000 | Batch 030/052 | Cost: 0.6180\n",
      "Epoch: 702/1000 | Batch 040/052 | Cost: 0.2753\n",
      "Epoch: 702/1000 | Batch 050/052 | Cost: 0.3304\n",
      "Epoch: 702/1000 training accuracy: 88.55%\n",
      "Epoch: 703/1000 | Batch 000/052 | Cost: 0.4010\n",
      "Epoch: 703/1000 | Batch 010/052 | Cost: 0.1100\n",
      "Epoch: 703/1000 | Batch 020/052 | Cost: 0.2114\n",
      "Epoch: 703/1000 | Batch 030/052 | Cost: 0.2322\n",
      "Epoch: 703/1000 | Batch 040/052 | Cost: 0.1926\n",
      "Epoch: 703/1000 | Batch 050/052 | Cost: 0.6088\n",
      "Epoch: 703/1000 training accuracy: 88.89%\n",
      "Epoch: 704/1000 | Batch 000/052 | Cost: 0.2251\n",
      "Epoch: 704/1000 | Batch 010/052 | Cost: 0.1394\n",
      "Epoch: 704/1000 | Batch 020/052 | Cost: 0.3341\n",
      "Epoch: 704/1000 | Batch 030/052 | Cost: 0.2600\n",
      "Epoch: 704/1000 | Batch 040/052 | Cost: 0.5227\n",
      "Epoch: 704/1000 | Batch 050/052 | Cost: 0.1491\n",
      "Epoch: 704/1000 training accuracy: 87.65%\n",
      "Epoch: 705/1000 | Batch 000/052 | Cost: 0.3808\n",
      "Epoch: 705/1000 | Batch 010/052 | Cost: 0.1302\n",
      "Epoch: 705/1000 | Batch 020/052 | Cost: 0.3034\n",
      "Epoch: 705/1000 | Batch 030/052 | Cost: 0.2610\n",
      "Epoch: 705/1000 | Batch 040/052 | Cost: 0.2562\n",
      "Epoch: 705/1000 | Batch 050/052 | Cost: 0.4287\n",
      "Epoch: 705/1000 training accuracy: 87.88%\n",
      "Epoch: 706/1000 | Batch 000/052 | Cost: 0.1499\n",
      "Epoch: 706/1000 | Batch 010/052 | Cost: 0.5564\n",
      "Epoch: 706/1000 | Batch 020/052 | Cost: 0.4394\n",
      "Epoch: 706/1000 | Batch 030/052 | Cost: 0.8199\n",
      "Epoch: 706/1000 | Batch 040/052 | Cost: 0.1476\n",
      "Epoch: 706/1000 | Batch 050/052 | Cost: 0.2188\n",
      "Epoch: 706/1000 training accuracy: 87.99%\n",
      "Epoch: 707/1000 | Batch 000/052 | Cost: 0.1287\n",
      "Epoch: 707/1000 | Batch 010/052 | Cost: 0.2103\n",
      "Epoch: 707/1000 | Batch 020/052 | Cost: 0.2647\n",
      "Epoch: 707/1000 | Batch 030/052 | Cost: 0.1598\n",
      "Epoch: 707/1000 | Batch 040/052 | Cost: 0.2969\n",
      "Epoch: 707/1000 | Batch 050/052 | Cost: 0.2089\n",
      "Epoch: 707/1000 training accuracy: 88.10%\n",
      "Epoch: 708/1000 | Batch 000/052 | Cost: 0.2576\n",
      "Epoch: 708/1000 | Batch 010/052 | Cost: 0.2888\n",
      "Epoch: 708/1000 | Batch 020/052 | Cost: 0.1034\n",
      "Epoch: 708/1000 | Batch 030/052 | Cost: 0.4855\n",
      "Epoch: 708/1000 | Batch 040/052 | Cost: 0.3333\n",
      "Epoch: 708/1000 | Batch 050/052 | Cost: 0.2316\n",
      "Epoch: 708/1000 training accuracy: 87.32%\n",
      "Epoch: 709/1000 | Batch 000/052 | Cost: 0.3464\n",
      "Epoch: 709/1000 | Batch 010/052 | Cost: 0.0502\n",
      "Epoch: 709/1000 | Batch 020/052 | Cost: 0.2158\n",
      "Epoch: 709/1000 | Batch 030/052 | Cost: 0.2827\n",
      "Epoch: 709/1000 | Batch 040/052 | Cost: 0.5296\n",
      "Epoch: 709/1000 | Batch 050/052 | Cost: 0.4627\n",
      "Epoch: 709/1000 training accuracy: 88.22%\n",
      "Epoch: 710/1000 | Batch 000/052 | Cost: 0.1179\n",
      "Epoch: 710/1000 | Batch 010/052 | Cost: 0.2108\n",
      "Epoch: 710/1000 | Batch 020/052 | Cost: 0.1599\n",
      "Epoch: 710/1000 | Batch 030/052 | Cost: 0.1274\n",
      "Epoch: 710/1000 | Batch 040/052 | Cost: 0.3386\n",
      "Epoch: 710/1000 | Batch 050/052 | Cost: 0.1297\n",
      "Epoch: 710/1000 training accuracy: 88.44%\n",
      "Epoch: 711/1000 | Batch 000/052 | Cost: 0.1842\n",
      "Epoch: 711/1000 | Batch 010/052 | Cost: 0.1981\n",
      "Epoch: 711/1000 | Batch 020/052 | Cost: 0.0969\n",
      "Epoch: 711/1000 | Batch 030/052 | Cost: 0.3013\n",
      "Epoch: 711/1000 | Batch 040/052 | Cost: 0.4258\n",
      "Epoch: 711/1000 | Batch 050/052 | Cost: 0.5335\n",
      "Epoch: 711/1000 training accuracy: 88.10%\n",
      "Epoch: 712/1000 | Batch 000/052 | Cost: 0.3641\n",
      "Epoch: 712/1000 | Batch 010/052 | Cost: 0.4272\n",
      "Epoch: 712/1000 | Batch 020/052 | Cost: 0.3750\n",
      "Epoch: 712/1000 | Batch 030/052 | Cost: 0.4285\n",
      "Epoch: 712/1000 | Batch 040/052 | Cost: 0.1590\n",
      "Epoch: 712/1000 | Batch 050/052 | Cost: 0.2853\n",
      "Epoch: 712/1000 training accuracy: 88.78%\n",
      "Epoch: 713/1000 | Batch 000/052 | Cost: 0.1040\n",
      "Epoch: 713/1000 | Batch 010/052 | Cost: 0.1856\n",
      "Epoch: 713/1000 | Batch 020/052 | Cost: 0.2150\n",
      "Epoch: 713/1000 | Batch 030/052 | Cost: 0.1710\n",
      "Epoch: 713/1000 | Batch 040/052 | Cost: 0.2714\n",
      "Epoch: 713/1000 | Batch 050/052 | Cost: 0.2194\n",
      "Epoch: 713/1000 training accuracy: 87.43%\n",
      "Epoch: 714/1000 | Batch 000/052 | Cost: 0.1742\n",
      "Epoch: 714/1000 | Batch 010/052 | Cost: 0.2571\n",
      "Epoch: 714/1000 | Batch 020/052 | Cost: 0.3531\n",
      "Epoch: 714/1000 | Batch 030/052 | Cost: 0.5681\n",
      "Epoch: 714/1000 | Batch 040/052 | Cost: 0.3174\n",
      "Epoch: 714/1000 | Batch 050/052 | Cost: 0.2294\n",
      "Epoch: 714/1000 training accuracy: 87.88%\n",
      "Epoch: 715/1000 | Batch 000/052 | Cost: 0.2221\n",
      "Epoch: 715/1000 | Batch 010/052 | Cost: 0.1941\n",
      "Epoch: 715/1000 | Batch 020/052 | Cost: 0.1624\n",
      "Epoch: 715/1000 | Batch 030/052 | Cost: 0.2682\n",
      "Epoch: 715/1000 | Batch 040/052 | Cost: 0.2000\n",
      "Epoch: 715/1000 | Batch 050/052 | Cost: 0.2483\n",
      "Epoch: 715/1000 training accuracy: 88.55%\n",
      "Epoch: 716/1000 | Batch 000/052 | Cost: 0.2094\n",
      "Epoch: 716/1000 | Batch 010/052 | Cost: 0.3522\n",
      "Epoch: 716/1000 | Batch 020/052 | Cost: 0.1816\n",
      "Epoch: 716/1000 | Batch 030/052 | Cost: 0.1751\n",
      "Epoch: 716/1000 | Batch 040/052 | Cost: 0.2205\n",
      "Epoch: 716/1000 | Batch 050/052 | Cost: 0.2141\n",
      "Epoch: 716/1000 training accuracy: 87.88%\n",
      "Epoch: 717/1000 | Batch 000/052 | Cost: 0.3679\n",
      "Epoch: 717/1000 | Batch 010/052 | Cost: 0.4052\n",
      "Epoch: 717/1000 | Batch 020/052 | Cost: 0.3478\n",
      "Epoch: 717/1000 | Batch 030/052 | Cost: 0.0707\n",
      "Epoch: 717/1000 | Batch 040/052 | Cost: 0.2230\n",
      "Epoch: 717/1000 | Batch 050/052 | Cost: 0.6666\n",
      "Epoch: 717/1000 training accuracy: 87.54%\n",
      "Epoch: 718/1000 | Batch 000/052 | Cost: 0.2168\n",
      "Epoch: 718/1000 | Batch 010/052 | Cost: 0.2990\n",
      "Epoch: 718/1000 | Batch 020/052 | Cost: 0.0878\n",
      "Epoch: 718/1000 | Batch 030/052 | Cost: 0.4556\n",
      "Epoch: 718/1000 | Batch 040/052 | Cost: 0.0797\n",
      "Epoch: 718/1000 | Batch 050/052 | Cost: 0.4117\n",
      "Epoch: 718/1000 training accuracy: 88.66%\n",
      "Epoch: 719/1000 | Batch 000/052 | Cost: 0.1309\n",
      "Epoch: 719/1000 | Batch 010/052 | Cost: 0.0714\n",
      "Epoch: 719/1000 | Batch 020/052 | Cost: 0.3072\n",
      "Epoch: 719/1000 | Batch 030/052 | Cost: 0.1780\n",
      "Epoch: 719/1000 | Batch 040/052 | Cost: 0.1151\n",
      "Epoch: 719/1000 | Batch 050/052 | Cost: 0.1933\n",
      "Epoch: 719/1000 training accuracy: 88.22%\n",
      "Epoch: 720/1000 | Batch 000/052 | Cost: 0.1923\n",
      "Epoch: 720/1000 | Batch 010/052 | Cost: 0.2820\n",
      "Epoch: 720/1000 | Batch 020/052 | Cost: 0.1760\n",
      "Epoch: 720/1000 | Batch 030/052 | Cost: 0.1507\n",
      "Epoch: 720/1000 | Batch 040/052 | Cost: 0.2855\n",
      "Epoch: 720/1000 | Batch 050/052 | Cost: 0.0791\n",
      "Epoch: 720/1000 training accuracy: 89.00%\n",
      "Epoch: 721/1000 | Batch 000/052 | Cost: 0.0987\n",
      "Epoch: 721/1000 | Batch 010/052 | Cost: 0.4880\n",
      "Epoch: 721/1000 | Batch 020/052 | Cost: 0.3178\n",
      "Epoch: 721/1000 | Batch 030/052 | Cost: 0.4233\n",
      "Epoch: 721/1000 | Batch 040/052 | Cost: 0.0898\n",
      "Epoch: 721/1000 | Batch 050/052 | Cost: 0.2667\n",
      "Epoch: 721/1000 training accuracy: 88.22%\n",
      "Epoch: 722/1000 | Batch 000/052 | Cost: 0.3229\n",
      "Epoch: 722/1000 | Batch 010/052 | Cost: 0.3098\n",
      "Epoch: 722/1000 | Batch 020/052 | Cost: 0.2310\n",
      "Epoch: 722/1000 | Batch 030/052 | Cost: 0.1249\n",
      "Epoch: 722/1000 | Batch 040/052 | Cost: 0.2847\n",
      "Epoch: 722/1000 | Batch 050/052 | Cost: 0.2838\n",
      "Epoch: 722/1000 training accuracy: 88.22%\n",
      "Epoch: 723/1000 | Batch 000/052 | Cost: 0.2475\n",
      "Epoch: 723/1000 | Batch 010/052 | Cost: 0.3924\n",
      "Epoch: 723/1000 | Batch 020/052 | Cost: 0.2788\n",
      "Epoch: 723/1000 | Batch 030/052 | Cost: 0.2490\n",
      "Epoch: 723/1000 | Batch 040/052 | Cost: 0.1028\n",
      "Epoch: 723/1000 | Batch 050/052 | Cost: 0.4151\n",
      "Epoch: 723/1000 training accuracy: 88.89%\n",
      "Epoch: 724/1000 | Batch 000/052 | Cost: 0.3343\n",
      "Epoch: 724/1000 | Batch 010/052 | Cost: 0.3529\n",
      "Epoch: 724/1000 | Batch 020/052 | Cost: 0.0605\n",
      "Epoch: 724/1000 | Batch 030/052 | Cost: 0.4100\n",
      "Epoch: 724/1000 | Batch 040/052 | Cost: 0.2235\n",
      "Epoch: 724/1000 | Batch 050/052 | Cost: 0.2536\n",
      "Epoch: 724/1000 training accuracy: 88.44%\n",
      "Epoch: 725/1000 | Batch 000/052 | Cost: 0.2692\n",
      "Epoch: 725/1000 | Batch 010/052 | Cost: 0.1637\n",
      "Epoch: 725/1000 | Batch 020/052 | Cost: 0.3989\n",
      "Epoch: 725/1000 | Batch 030/052 | Cost: 0.2014\n",
      "Epoch: 725/1000 | Batch 040/052 | Cost: 0.1677\n",
      "Epoch: 725/1000 | Batch 050/052 | Cost: 0.3949\n",
      "Epoch: 725/1000 training accuracy: 87.99%\n",
      "Epoch: 726/1000 | Batch 000/052 | Cost: 0.5434\n",
      "Epoch: 726/1000 | Batch 010/052 | Cost: 0.2380\n",
      "Epoch: 726/1000 | Batch 020/052 | Cost: 0.1809\n",
      "Epoch: 726/1000 | Batch 030/052 | Cost: 0.5078\n",
      "Epoch: 726/1000 | Batch 040/052 | Cost: 0.3695\n",
      "Epoch: 726/1000 | Batch 050/052 | Cost: 0.2067\n",
      "Epoch: 726/1000 training accuracy: 87.77%\n",
      "Epoch: 727/1000 | Batch 000/052 | Cost: 0.3447\n",
      "Epoch: 727/1000 | Batch 010/052 | Cost: 0.2550\n",
      "Epoch: 727/1000 | Batch 020/052 | Cost: 0.2468\n",
      "Epoch: 727/1000 | Batch 030/052 | Cost: 0.2244\n",
      "Epoch: 727/1000 | Batch 040/052 | Cost: 0.3313\n",
      "Epoch: 727/1000 | Batch 050/052 | Cost: 0.1884\n",
      "Epoch: 727/1000 training accuracy: 88.66%\n",
      "Epoch: 728/1000 | Batch 000/052 | Cost: 0.2177\n",
      "Epoch: 728/1000 | Batch 010/052 | Cost: 0.1636\n",
      "Epoch: 728/1000 | Batch 020/052 | Cost: 0.2820\n",
      "Epoch: 728/1000 | Batch 030/052 | Cost: 0.1641\n",
      "Epoch: 728/1000 | Batch 040/052 | Cost: 0.3133\n",
      "Epoch: 728/1000 | Batch 050/052 | Cost: 0.2391\n",
      "Epoch: 728/1000 training accuracy: 88.44%\n",
      "Epoch: 729/1000 | Batch 000/052 | Cost: 0.5761\n",
      "Epoch: 729/1000 | Batch 010/052 | Cost: 0.4041\n",
      "Epoch: 729/1000 | Batch 020/052 | Cost: 0.2722\n",
      "Epoch: 729/1000 | Batch 030/052 | Cost: 0.1996\n",
      "Epoch: 729/1000 | Batch 040/052 | Cost: 0.3241\n",
      "Epoch: 729/1000 | Batch 050/052 | Cost: 0.2720\n",
      "Epoch: 729/1000 training accuracy: 87.09%\n",
      "Epoch: 730/1000 | Batch 000/052 | Cost: 0.0943\n",
      "Epoch: 730/1000 | Batch 010/052 | Cost: 0.0791\n",
      "Epoch: 730/1000 | Batch 020/052 | Cost: 0.2223\n",
      "Epoch: 730/1000 | Batch 030/052 | Cost: 0.2841\n",
      "Epoch: 730/1000 | Batch 040/052 | Cost: 0.4004\n",
      "Epoch: 730/1000 | Batch 050/052 | Cost: 0.0854\n",
      "Epoch: 730/1000 training accuracy: 88.66%\n",
      "Epoch: 731/1000 | Batch 000/052 | Cost: 0.2232\n",
      "Epoch: 731/1000 | Batch 010/052 | Cost: 0.3788\n",
      "Epoch: 731/1000 | Batch 020/052 | Cost: 0.1135\n",
      "Epoch: 731/1000 | Batch 030/052 | Cost: 0.0710\n",
      "Epoch: 731/1000 | Batch 040/052 | Cost: 0.3934\n",
      "Epoch: 731/1000 | Batch 050/052 | Cost: 0.3099\n",
      "Epoch: 731/1000 training accuracy: 88.78%\n",
      "Epoch: 732/1000 | Batch 000/052 | Cost: 0.2130\n",
      "Epoch: 732/1000 | Batch 010/052 | Cost: 0.4582\n",
      "Epoch: 732/1000 | Batch 020/052 | Cost: 0.5870\n",
      "Epoch: 732/1000 | Batch 030/052 | Cost: 0.1967\n",
      "Epoch: 732/1000 | Batch 040/052 | Cost: 0.3328\n",
      "Epoch: 732/1000 | Batch 050/052 | Cost: 0.0811\n",
      "Epoch: 732/1000 training accuracy: 88.44%\n",
      "Epoch: 733/1000 | Batch 000/052 | Cost: 0.4646\n",
      "Epoch: 733/1000 | Batch 010/052 | Cost: 0.1403\n",
      "Epoch: 733/1000 | Batch 020/052 | Cost: 0.2301\n",
      "Epoch: 733/1000 | Batch 030/052 | Cost: 0.1980\n",
      "Epoch: 733/1000 | Batch 040/052 | Cost: 0.1482\n",
      "Epoch: 733/1000 | Batch 050/052 | Cost: 0.1489\n",
      "Epoch: 733/1000 training accuracy: 87.54%\n",
      "Epoch: 734/1000 | Batch 000/052 | Cost: 0.3476\n",
      "Epoch: 734/1000 | Batch 010/052 | Cost: 0.1579\n",
      "Epoch: 734/1000 | Batch 020/052 | Cost: 0.2226\n",
      "Epoch: 734/1000 | Batch 030/052 | Cost: 0.3779\n",
      "Epoch: 734/1000 | Batch 040/052 | Cost: 0.3243\n",
      "Epoch: 734/1000 | Batch 050/052 | Cost: 0.2793\n",
      "Epoch: 734/1000 training accuracy: 88.44%\n",
      "Epoch: 735/1000 | Batch 000/052 | Cost: 0.2909\n",
      "Epoch: 735/1000 | Batch 010/052 | Cost: 0.2303\n",
      "Epoch: 735/1000 | Batch 020/052 | Cost: 0.1758\n",
      "Epoch: 735/1000 | Batch 030/052 | Cost: 0.2804\n",
      "Epoch: 735/1000 | Batch 040/052 | Cost: 0.2124\n",
      "Epoch: 735/1000 | Batch 050/052 | Cost: 0.4700\n",
      "Epoch: 735/1000 training accuracy: 88.33%\n",
      "Epoch: 736/1000 | Batch 000/052 | Cost: 0.3477\n",
      "Epoch: 736/1000 | Batch 010/052 | Cost: 0.5803\n",
      "Epoch: 736/1000 | Batch 020/052 | Cost: 0.0911\n",
      "Epoch: 736/1000 | Batch 030/052 | Cost: 0.2264\n",
      "Epoch: 736/1000 | Batch 040/052 | Cost: 0.0729\n",
      "Epoch: 736/1000 | Batch 050/052 | Cost: 0.1004\n",
      "Epoch: 736/1000 training accuracy: 87.65%\n",
      "Epoch: 737/1000 | Batch 000/052 | Cost: 0.2907\n",
      "Epoch: 737/1000 | Batch 010/052 | Cost: 0.2613\n",
      "Epoch: 737/1000 | Batch 020/052 | Cost: 0.5117\n",
      "Epoch: 737/1000 | Batch 030/052 | Cost: 0.2176\n",
      "Epoch: 737/1000 | Batch 040/052 | Cost: 0.1027\n",
      "Epoch: 737/1000 | Batch 050/052 | Cost: 0.1232\n",
      "Epoch: 737/1000 training accuracy: 88.44%\n",
      "Epoch: 738/1000 | Batch 000/052 | Cost: 0.6473\n",
      "Epoch: 738/1000 | Batch 010/052 | Cost: 0.0868\n",
      "Epoch: 738/1000 | Batch 020/052 | Cost: 0.2988\n",
      "Epoch: 738/1000 | Batch 030/052 | Cost: 0.1353\n",
      "Epoch: 738/1000 | Batch 040/052 | Cost: 0.2576\n",
      "Epoch: 738/1000 | Batch 050/052 | Cost: 0.4100\n",
      "Epoch: 738/1000 training accuracy: 88.66%\n",
      "Epoch: 739/1000 | Batch 000/052 | Cost: 0.3461\n",
      "Epoch: 739/1000 | Batch 010/052 | Cost: 0.4212\n",
      "Epoch: 739/1000 | Batch 020/052 | Cost: 0.2785\n",
      "Epoch: 739/1000 | Batch 030/052 | Cost: 0.3155\n",
      "Epoch: 739/1000 | Batch 040/052 | Cost: 0.2677\n",
      "Epoch: 739/1000 | Batch 050/052 | Cost: 0.2923\n",
      "Epoch: 739/1000 training accuracy: 87.21%\n",
      "Epoch: 740/1000 | Batch 000/052 | Cost: 0.2167\n",
      "Epoch: 740/1000 | Batch 010/052 | Cost: 0.1510\n",
      "Epoch: 740/1000 | Batch 020/052 | Cost: 0.2660\n",
      "Epoch: 740/1000 | Batch 030/052 | Cost: 0.3141\n",
      "Epoch: 740/1000 | Batch 040/052 | Cost: 0.3091\n",
      "Epoch: 740/1000 | Batch 050/052 | Cost: 0.4170\n",
      "Epoch: 740/1000 training accuracy: 87.32%\n",
      "Epoch: 741/1000 | Batch 000/052 | Cost: 0.1230\n",
      "Epoch: 741/1000 | Batch 010/052 | Cost: 0.2377\n",
      "Epoch: 741/1000 | Batch 020/052 | Cost: 0.2637\n",
      "Epoch: 741/1000 | Batch 030/052 | Cost: 0.2768\n",
      "Epoch: 741/1000 | Batch 040/052 | Cost: 0.5222\n",
      "Epoch: 741/1000 | Batch 050/052 | Cost: 0.1414\n",
      "Epoch: 741/1000 training accuracy: 88.44%\n",
      "Epoch: 742/1000 | Batch 000/052 | Cost: 0.3554\n",
      "Epoch: 742/1000 | Batch 010/052 | Cost: 0.2231\n",
      "Epoch: 742/1000 | Batch 020/052 | Cost: 0.2981\n",
      "Epoch: 742/1000 | Batch 030/052 | Cost: 0.4960\n",
      "Epoch: 742/1000 | Batch 040/052 | Cost: 0.1697\n",
      "Epoch: 742/1000 | Batch 050/052 | Cost: 0.3389\n",
      "Epoch: 742/1000 training accuracy: 89.23%\n",
      "Epoch: 743/1000 | Batch 000/052 | Cost: 0.1692\n",
      "Epoch: 743/1000 | Batch 010/052 | Cost: 0.1612\n",
      "Epoch: 743/1000 | Batch 020/052 | Cost: 0.2975\n",
      "Epoch: 743/1000 | Batch 030/052 | Cost: 0.1381\n",
      "Epoch: 743/1000 | Batch 040/052 | Cost: 0.2618\n",
      "Epoch: 743/1000 | Batch 050/052 | Cost: 0.3118\n",
      "Epoch: 743/1000 training accuracy: 88.10%\n",
      "Epoch: 744/1000 | Batch 000/052 | Cost: 0.3367\n",
      "Epoch: 744/1000 | Batch 010/052 | Cost: 0.1342\n",
      "Epoch: 744/1000 | Batch 020/052 | Cost: 0.2954\n",
      "Epoch: 744/1000 | Batch 030/052 | Cost: 0.0876\n",
      "Epoch: 744/1000 | Batch 040/052 | Cost: 0.1075\n",
      "Epoch: 744/1000 | Batch 050/052 | Cost: 0.4097\n",
      "Epoch: 744/1000 training accuracy: 88.66%\n",
      "Epoch: 745/1000 | Batch 000/052 | Cost: 0.0734\n",
      "Epoch: 745/1000 | Batch 010/052 | Cost: 0.1604\n",
      "Epoch: 745/1000 | Batch 020/052 | Cost: 0.4488\n",
      "Epoch: 745/1000 | Batch 030/052 | Cost: 0.2914\n",
      "Epoch: 745/1000 | Batch 040/052 | Cost: 0.2414\n",
      "Epoch: 745/1000 | Batch 050/052 | Cost: 0.7260\n",
      "Epoch: 745/1000 training accuracy: 89.23%\n",
      "Epoch: 746/1000 | Batch 000/052 | Cost: 0.3395\n",
      "Epoch: 746/1000 | Batch 010/052 | Cost: 0.2283\n",
      "Epoch: 746/1000 | Batch 020/052 | Cost: 0.2688\n",
      "Epoch: 746/1000 | Batch 030/052 | Cost: 0.2014\n",
      "Epoch: 746/1000 | Batch 040/052 | Cost: 0.2381\n",
      "Epoch: 746/1000 | Batch 050/052 | Cost: 0.4328\n",
      "Epoch: 746/1000 training accuracy: 88.55%\n",
      "Epoch: 747/1000 | Batch 000/052 | Cost: 0.5189\n",
      "Epoch: 747/1000 | Batch 010/052 | Cost: 0.3045\n",
      "Epoch: 747/1000 | Batch 020/052 | Cost: 0.1001\n",
      "Epoch: 747/1000 | Batch 030/052 | Cost: 0.2933\n",
      "Epoch: 747/1000 | Batch 040/052 | Cost: 0.3339\n",
      "Epoch: 747/1000 | Batch 050/052 | Cost: 0.5979\n",
      "Epoch: 747/1000 training accuracy: 87.43%\n",
      "Epoch: 748/1000 | Batch 000/052 | Cost: 0.3107\n",
      "Epoch: 748/1000 | Batch 010/052 | Cost: 0.1305\n",
      "Epoch: 748/1000 | Batch 020/052 | Cost: 0.3425\n",
      "Epoch: 748/1000 | Batch 030/052 | Cost: 0.4000\n",
      "Epoch: 748/1000 | Batch 040/052 | Cost: 0.4078\n",
      "Epoch: 748/1000 | Batch 050/052 | Cost: 0.3009\n",
      "Epoch: 748/1000 training accuracy: 87.88%\n",
      "Epoch: 749/1000 | Batch 000/052 | Cost: 0.1080\n",
      "Epoch: 749/1000 | Batch 010/052 | Cost: 0.4263\n",
      "Epoch: 749/1000 | Batch 020/052 | Cost: 0.3765\n",
      "Epoch: 749/1000 | Batch 030/052 | Cost: 0.3042\n",
      "Epoch: 749/1000 | Batch 040/052 | Cost: 0.1693\n",
      "Epoch: 749/1000 | Batch 050/052 | Cost: 0.3235\n",
      "Epoch: 749/1000 training accuracy: 88.55%\n",
      "Epoch: 750/1000 | Batch 000/052 | Cost: 0.3081\n",
      "Epoch: 750/1000 | Batch 010/052 | Cost: 0.5001\n",
      "Epoch: 750/1000 | Batch 020/052 | Cost: 0.2158\n",
      "Epoch: 750/1000 | Batch 030/052 | Cost: 0.1286\n",
      "Epoch: 750/1000 | Batch 040/052 | Cost: 0.3023\n",
      "Epoch: 750/1000 | Batch 050/052 | Cost: 0.5533\n",
      "Epoch: 750/1000 training accuracy: 88.78%\n",
      "Epoch: 751/1000 | Batch 000/052 | Cost: 0.2716\n",
      "Epoch: 751/1000 | Batch 010/052 | Cost: 0.2783\n",
      "Epoch: 751/1000 | Batch 020/052 | Cost: 0.0880\n",
      "Epoch: 751/1000 | Batch 030/052 | Cost: 0.2056\n",
      "Epoch: 751/1000 | Batch 040/052 | Cost: 0.4329\n",
      "Epoch: 751/1000 | Batch 050/052 | Cost: 0.6770\n",
      "Epoch: 751/1000 training accuracy: 87.65%\n",
      "Epoch: 752/1000 | Batch 000/052 | Cost: 0.4763\n",
      "Epoch: 752/1000 | Batch 010/052 | Cost: 0.1689\n",
      "Epoch: 752/1000 | Batch 020/052 | Cost: 0.4702\n",
      "Epoch: 752/1000 | Batch 030/052 | Cost: 0.1234\n",
      "Epoch: 752/1000 | Batch 040/052 | Cost: 0.1202\n",
      "Epoch: 752/1000 | Batch 050/052 | Cost: 0.1312\n",
      "Epoch: 752/1000 training accuracy: 87.99%\n",
      "Epoch: 753/1000 | Batch 000/052 | Cost: 0.3868\n",
      "Epoch: 753/1000 | Batch 010/052 | Cost: 0.2580\n",
      "Epoch: 753/1000 | Batch 020/052 | Cost: 0.2108\n",
      "Epoch: 753/1000 | Batch 030/052 | Cost: 0.3325\n",
      "Epoch: 753/1000 | Batch 040/052 | Cost: 0.1090\n",
      "Epoch: 753/1000 | Batch 050/052 | Cost: 0.3165\n",
      "Epoch: 753/1000 training accuracy: 88.66%\n",
      "Epoch: 754/1000 | Batch 000/052 | Cost: 0.7903\n",
      "Epoch: 754/1000 | Batch 010/052 | Cost: 0.1961\n",
      "Epoch: 754/1000 | Batch 020/052 | Cost: 0.1663\n",
      "Epoch: 754/1000 | Batch 030/052 | Cost: 0.2799\n",
      "Epoch: 754/1000 | Batch 040/052 | Cost: 0.4220\n",
      "Epoch: 754/1000 | Batch 050/052 | Cost: 0.2148\n",
      "Epoch: 754/1000 training accuracy: 88.89%\n",
      "Epoch: 755/1000 | Batch 000/052 | Cost: 0.0765\n",
      "Epoch: 755/1000 | Batch 010/052 | Cost: 0.3120\n",
      "Epoch: 755/1000 | Batch 020/052 | Cost: 0.2205\n",
      "Epoch: 755/1000 | Batch 030/052 | Cost: 0.3439\n",
      "Epoch: 755/1000 | Batch 040/052 | Cost: 0.2186\n",
      "Epoch: 755/1000 | Batch 050/052 | Cost: 0.1381\n",
      "Epoch: 755/1000 training accuracy: 88.89%\n",
      "Epoch: 756/1000 | Batch 000/052 | Cost: 0.2958\n",
      "Epoch: 756/1000 | Batch 010/052 | Cost: 0.2818\n",
      "Epoch: 756/1000 | Batch 020/052 | Cost: 0.5750\n",
      "Epoch: 756/1000 | Batch 030/052 | Cost: 0.0522\n",
      "Epoch: 756/1000 | Batch 040/052 | Cost: 0.3676\n",
      "Epoch: 756/1000 | Batch 050/052 | Cost: 0.4885\n",
      "Epoch: 756/1000 training accuracy: 88.89%\n",
      "Epoch: 757/1000 | Batch 000/052 | Cost: 0.1599\n",
      "Epoch: 757/1000 | Batch 010/052 | Cost: 0.2599\n",
      "Epoch: 757/1000 | Batch 020/052 | Cost: 0.1173\n",
      "Epoch: 757/1000 | Batch 030/052 | Cost: 0.2975\n",
      "Epoch: 757/1000 | Batch 040/052 | Cost: 0.2691\n",
      "Epoch: 757/1000 | Batch 050/052 | Cost: 0.0873\n",
      "Epoch: 757/1000 training accuracy: 87.43%\n",
      "Epoch: 758/1000 | Batch 000/052 | Cost: 0.3662\n",
      "Epoch: 758/1000 | Batch 010/052 | Cost: 0.1512\n",
      "Epoch: 758/1000 | Batch 020/052 | Cost: 0.2041\n",
      "Epoch: 758/1000 | Batch 030/052 | Cost: 0.1900\n",
      "Epoch: 758/1000 | Batch 040/052 | Cost: 0.1571\n",
      "Epoch: 758/1000 | Batch 050/052 | Cost: 0.2300\n",
      "Epoch: 758/1000 training accuracy: 88.22%\n",
      "Epoch: 759/1000 | Batch 000/052 | Cost: 0.1383\n",
      "Epoch: 759/1000 | Batch 010/052 | Cost: 0.3542\n",
      "Epoch: 759/1000 | Batch 020/052 | Cost: 0.4265\n",
      "Epoch: 759/1000 | Batch 030/052 | Cost: 0.2573\n",
      "Epoch: 759/1000 | Batch 040/052 | Cost: 0.1406\n",
      "Epoch: 759/1000 | Batch 050/052 | Cost: 0.2114\n",
      "Epoch: 759/1000 training accuracy: 87.88%\n",
      "Epoch: 760/1000 | Batch 000/052 | Cost: 0.2845\n",
      "Epoch: 760/1000 | Batch 010/052 | Cost: 0.0957\n",
      "Epoch: 760/1000 | Batch 020/052 | Cost: 0.1684\n",
      "Epoch: 760/1000 | Batch 030/052 | Cost: 0.2062\n",
      "Epoch: 760/1000 | Batch 040/052 | Cost: 0.1498\n",
      "Epoch: 760/1000 | Batch 050/052 | Cost: 0.1517\n",
      "Epoch: 760/1000 training accuracy: 88.44%\n",
      "Epoch: 761/1000 | Batch 000/052 | Cost: 0.4095\n",
      "Epoch: 761/1000 | Batch 010/052 | Cost: 0.2210\n",
      "Epoch: 761/1000 | Batch 020/052 | Cost: 0.3426\n",
      "Epoch: 761/1000 | Batch 030/052 | Cost: 0.3281\n",
      "Epoch: 761/1000 | Batch 040/052 | Cost: 0.2091\n",
      "Epoch: 761/1000 | Batch 050/052 | Cost: 0.5255\n",
      "Epoch: 761/1000 training accuracy: 88.44%\n",
      "Epoch: 762/1000 | Batch 000/052 | Cost: 0.2164\n",
      "Epoch: 762/1000 | Batch 010/052 | Cost: 0.0931\n",
      "Epoch: 762/1000 | Batch 020/052 | Cost: 0.2278\n",
      "Epoch: 762/1000 | Batch 030/052 | Cost: 0.1568\n",
      "Epoch: 762/1000 | Batch 040/052 | Cost: 0.1188\n",
      "Epoch: 762/1000 | Batch 050/052 | Cost: 0.3484\n",
      "Epoch: 762/1000 training accuracy: 88.44%\n",
      "Epoch: 763/1000 | Batch 000/052 | Cost: 0.3630\n",
      "Epoch: 763/1000 | Batch 010/052 | Cost: 0.0928\n",
      "Epoch: 763/1000 | Batch 020/052 | Cost: 0.6906\n",
      "Epoch: 763/1000 | Batch 030/052 | Cost: 0.3064\n",
      "Epoch: 763/1000 | Batch 040/052 | Cost: 0.2535\n",
      "Epoch: 763/1000 | Batch 050/052 | Cost: 0.4609\n",
      "Epoch: 763/1000 training accuracy: 88.33%\n",
      "Epoch: 764/1000 | Batch 000/052 | Cost: 0.1768\n",
      "Epoch: 764/1000 | Batch 010/052 | Cost: 0.1953\n",
      "Epoch: 764/1000 | Batch 020/052 | Cost: 0.4472\n",
      "Epoch: 764/1000 | Batch 030/052 | Cost: 0.3048\n",
      "Epoch: 764/1000 | Batch 040/052 | Cost: 0.1926\n",
      "Epoch: 764/1000 | Batch 050/052 | Cost: 0.1737\n",
      "Epoch: 764/1000 training accuracy: 88.44%\n",
      "Epoch: 765/1000 | Batch 000/052 | Cost: 0.3236\n",
      "Epoch: 765/1000 | Batch 010/052 | Cost: 0.0944\n",
      "Epoch: 765/1000 | Batch 020/052 | Cost: 0.3093\n",
      "Epoch: 765/1000 | Batch 030/052 | Cost: 0.0923\n",
      "Epoch: 765/1000 | Batch 040/052 | Cost: 0.1544\n",
      "Epoch: 765/1000 | Batch 050/052 | Cost: 0.1293\n",
      "Epoch: 765/1000 training accuracy: 88.33%\n",
      "Epoch: 766/1000 | Batch 000/052 | Cost: 0.1399\n",
      "Epoch: 766/1000 | Batch 010/052 | Cost: 0.0609\n",
      "Epoch: 766/1000 | Batch 020/052 | Cost: 0.2567\n",
      "Epoch: 766/1000 | Batch 030/052 | Cost: 0.3738\n",
      "Epoch: 766/1000 | Batch 040/052 | Cost: 0.1812\n",
      "Epoch: 766/1000 | Batch 050/052 | Cost: 0.2707\n",
      "Epoch: 766/1000 training accuracy: 87.77%\n",
      "Epoch: 767/1000 | Batch 000/052 | Cost: 0.6244\n",
      "Epoch: 767/1000 | Batch 010/052 | Cost: 0.2693\n",
      "Epoch: 767/1000 | Batch 020/052 | Cost: 0.1536\n",
      "Epoch: 767/1000 | Batch 030/052 | Cost: 0.5569\n",
      "Epoch: 767/1000 | Batch 040/052 | Cost: 0.3390\n",
      "Epoch: 767/1000 | Batch 050/052 | Cost: 0.2942\n",
      "Epoch: 767/1000 training accuracy: 88.33%\n",
      "Epoch: 768/1000 | Batch 000/052 | Cost: 0.1629\n",
      "Epoch: 768/1000 | Batch 010/052 | Cost: 0.1295\n",
      "Epoch: 768/1000 | Batch 020/052 | Cost: 0.1420\n",
      "Epoch: 768/1000 | Batch 030/052 | Cost: 0.1564\n",
      "Epoch: 768/1000 | Batch 040/052 | Cost: 0.1637\n",
      "Epoch: 768/1000 | Batch 050/052 | Cost: 0.4955\n",
      "Epoch: 768/1000 training accuracy: 88.33%\n",
      "Epoch: 769/1000 | Batch 000/052 | Cost: 0.4289\n",
      "Epoch: 769/1000 | Batch 010/052 | Cost: 0.2317\n",
      "Epoch: 769/1000 | Batch 020/052 | Cost: 0.5554\n",
      "Epoch: 769/1000 | Batch 030/052 | Cost: 0.5242\n",
      "Epoch: 769/1000 | Batch 040/052 | Cost: 0.1623\n",
      "Epoch: 769/1000 | Batch 050/052 | Cost: 0.2466\n",
      "Epoch: 769/1000 training accuracy: 88.22%\n",
      "Epoch: 770/1000 | Batch 000/052 | Cost: 0.0660\n",
      "Epoch: 770/1000 | Batch 010/052 | Cost: 0.4068\n",
      "Epoch: 770/1000 | Batch 020/052 | Cost: 0.2200\n",
      "Epoch: 770/1000 | Batch 030/052 | Cost: 0.2998\n",
      "Epoch: 770/1000 | Batch 040/052 | Cost: 0.2780\n",
      "Epoch: 770/1000 | Batch 050/052 | Cost: 0.2017\n",
      "Epoch: 770/1000 training accuracy: 88.33%\n",
      "Epoch: 771/1000 | Batch 000/052 | Cost: 0.1757\n",
      "Epoch: 771/1000 | Batch 010/052 | Cost: 0.4102\n",
      "Epoch: 771/1000 | Batch 020/052 | Cost: 0.4590\n",
      "Epoch: 771/1000 | Batch 030/052 | Cost: 0.2294\n",
      "Epoch: 771/1000 | Batch 040/052 | Cost: 0.2704\n",
      "Epoch: 771/1000 | Batch 050/052 | Cost: 0.0631\n",
      "Epoch: 771/1000 training accuracy: 89.00%\n",
      "Epoch: 772/1000 | Batch 000/052 | Cost: 0.2324\n",
      "Epoch: 772/1000 | Batch 010/052 | Cost: 0.1731\n",
      "Epoch: 772/1000 | Batch 020/052 | Cost: 0.5645\n",
      "Epoch: 772/1000 | Batch 030/052 | Cost: 0.1494\n",
      "Epoch: 772/1000 | Batch 040/052 | Cost: 0.2736\n",
      "Epoch: 772/1000 | Batch 050/052 | Cost: 0.1949\n",
      "Epoch: 772/1000 training accuracy: 88.22%\n",
      "Epoch: 773/1000 | Batch 000/052 | Cost: 0.2272\n",
      "Epoch: 773/1000 | Batch 010/052 | Cost: 0.0980\n",
      "Epoch: 773/1000 | Batch 020/052 | Cost: 0.2001\n",
      "Epoch: 773/1000 | Batch 030/052 | Cost: 0.1010\n",
      "Epoch: 773/1000 | Batch 040/052 | Cost: 0.4102\n",
      "Epoch: 773/1000 | Batch 050/052 | Cost: 0.3650\n",
      "Epoch: 773/1000 training accuracy: 87.32%\n",
      "Epoch: 774/1000 | Batch 000/052 | Cost: 0.1251\n",
      "Epoch: 774/1000 | Batch 010/052 | Cost: 0.2583\n",
      "Epoch: 774/1000 | Batch 020/052 | Cost: 0.5673\n",
      "Epoch: 774/1000 | Batch 030/052 | Cost: 0.2831\n",
      "Epoch: 774/1000 | Batch 040/052 | Cost: 0.5356\n",
      "Epoch: 774/1000 | Batch 050/052 | Cost: 0.1401\n",
      "Epoch: 774/1000 training accuracy: 87.99%\n",
      "Epoch: 775/1000 | Batch 000/052 | Cost: 0.5275\n",
      "Epoch: 775/1000 | Batch 010/052 | Cost: 0.2676\n",
      "Epoch: 775/1000 | Batch 020/052 | Cost: 0.1079\n",
      "Epoch: 775/1000 | Batch 030/052 | Cost: 0.2248\n",
      "Epoch: 775/1000 | Batch 040/052 | Cost: 0.2187\n",
      "Epoch: 775/1000 | Batch 050/052 | Cost: 0.2181\n",
      "Epoch: 775/1000 training accuracy: 88.66%\n",
      "Epoch: 776/1000 | Batch 000/052 | Cost: 0.1500\n",
      "Epoch: 776/1000 | Batch 010/052 | Cost: 0.1423\n",
      "Epoch: 776/1000 | Batch 020/052 | Cost: 0.0953\n",
      "Epoch: 776/1000 | Batch 030/052 | Cost: 0.2170\n",
      "Epoch: 776/1000 | Batch 040/052 | Cost: 0.3834\n",
      "Epoch: 776/1000 | Batch 050/052 | Cost: 0.2164\n",
      "Epoch: 776/1000 training accuracy: 88.66%\n",
      "Epoch: 777/1000 | Batch 000/052 | Cost: 0.0812\n",
      "Epoch: 777/1000 | Batch 010/052 | Cost: 0.0785\n",
      "Epoch: 777/1000 | Batch 020/052 | Cost: 0.1279\n",
      "Epoch: 777/1000 | Batch 030/052 | Cost: 0.3376\n",
      "Epoch: 777/1000 | Batch 040/052 | Cost: 0.1163\n",
      "Epoch: 777/1000 | Batch 050/052 | Cost: 0.1312\n",
      "Epoch: 777/1000 training accuracy: 87.43%\n",
      "Epoch: 778/1000 | Batch 000/052 | Cost: 0.2094\n",
      "Epoch: 778/1000 | Batch 010/052 | Cost: 0.3847\n",
      "Epoch: 778/1000 | Batch 020/052 | Cost: 0.3171\n",
      "Epoch: 778/1000 | Batch 030/052 | Cost: 0.1367\n",
      "Epoch: 778/1000 | Batch 040/052 | Cost: 0.6197\n",
      "Epoch: 778/1000 | Batch 050/052 | Cost: 0.1933\n",
      "Epoch: 778/1000 training accuracy: 87.88%\n",
      "Epoch: 779/1000 | Batch 000/052 | Cost: 0.1661\n",
      "Epoch: 779/1000 | Batch 010/052 | Cost: 0.3115\n",
      "Epoch: 779/1000 | Batch 020/052 | Cost: 0.2408\n",
      "Epoch: 779/1000 | Batch 030/052 | Cost: 0.1862\n",
      "Epoch: 779/1000 | Batch 040/052 | Cost: 0.3511\n",
      "Epoch: 779/1000 | Batch 050/052 | Cost: 0.1748\n",
      "Epoch: 779/1000 training accuracy: 87.99%\n",
      "Epoch: 780/1000 | Batch 000/052 | Cost: 0.1996\n",
      "Epoch: 780/1000 | Batch 010/052 | Cost: 0.2702\n",
      "Epoch: 780/1000 | Batch 020/052 | Cost: 0.2908\n",
      "Epoch: 780/1000 | Batch 030/052 | Cost: 0.3106\n",
      "Epoch: 780/1000 | Batch 040/052 | Cost: 0.2390\n",
      "Epoch: 780/1000 | Batch 050/052 | Cost: 0.2318\n",
      "Epoch: 780/1000 training accuracy: 88.78%\n",
      "Epoch: 781/1000 | Batch 000/052 | Cost: 0.1144\n",
      "Epoch: 781/1000 | Batch 010/052 | Cost: 0.4285\n",
      "Epoch: 781/1000 | Batch 020/052 | Cost: 0.1036\n",
      "Epoch: 781/1000 | Batch 030/052 | Cost: 0.4297\n",
      "Epoch: 781/1000 | Batch 040/052 | Cost: 0.3365\n",
      "Epoch: 781/1000 | Batch 050/052 | Cost: 0.2684\n",
      "Epoch: 781/1000 training accuracy: 88.33%\n",
      "Epoch: 782/1000 | Batch 000/052 | Cost: 0.5028\n",
      "Epoch: 782/1000 | Batch 010/052 | Cost: 0.2624\n",
      "Epoch: 782/1000 | Batch 020/052 | Cost: 0.2258\n",
      "Epoch: 782/1000 | Batch 030/052 | Cost: 0.1486\n",
      "Epoch: 782/1000 | Batch 040/052 | Cost: 0.2519\n",
      "Epoch: 782/1000 | Batch 050/052 | Cost: 0.5727\n",
      "Epoch: 782/1000 training accuracy: 87.65%\n",
      "Epoch: 783/1000 | Batch 000/052 | Cost: 0.2702\n",
      "Epoch: 783/1000 | Batch 010/052 | Cost: 0.0957\n",
      "Epoch: 783/1000 | Batch 020/052 | Cost: 0.2089\n",
      "Epoch: 783/1000 | Batch 030/052 | Cost: 0.3556\n",
      "Epoch: 783/1000 | Batch 040/052 | Cost: 0.3877\n",
      "Epoch: 783/1000 | Batch 050/052 | Cost: 0.2643\n",
      "Epoch: 783/1000 training accuracy: 89.11%\n",
      "Epoch: 784/1000 | Batch 000/052 | Cost: 0.4213\n",
      "Epoch: 784/1000 | Batch 010/052 | Cost: 0.3008\n",
      "Epoch: 784/1000 | Batch 020/052 | Cost: 0.2349\n",
      "Epoch: 784/1000 | Batch 030/052 | Cost: 0.1569\n",
      "Epoch: 784/1000 | Batch 040/052 | Cost: 0.3114\n",
      "Epoch: 784/1000 | Batch 050/052 | Cost: 0.3429\n",
      "Epoch: 784/1000 training accuracy: 88.10%\n",
      "Epoch: 785/1000 | Batch 000/052 | Cost: 0.1103\n",
      "Epoch: 785/1000 | Batch 010/052 | Cost: 0.4159\n",
      "Epoch: 785/1000 | Batch 020/052 | Cost: 0.1329\n",
      "Epoch: 785/1000 | Batch 030/052 | Cost: 0.2878\n",
      "Epoch: 785/1000 | Batch 040/052 | Cost: 0.3198\n",
      "Epoch: 785/1000 | Batch 050/052 | Cost: 0.3250\n",
      "Epoch: 785/1000 training accuracy: 87.88%\n",
      "Epoch: 786/1000 | Batch 000/052 | Cost: 0.1792\n",
      "Epoch: 786/1000 | Batch 010/052 | Cost: 0.3287\n",
      "Epoch: 786/1000 | Batch 020/052 | Cost: 0.4129\n",
      "Epoch: 786/1000 | Batch 030/052 | Cost: 0.0788\n",
      "Epoch: 786/1000 | Batch 040/052 | Cost: 0.2821\n",
      "Epoch: 786/1000 | Batch 050/052 | Cost: 0.3369\n",
      "Epoch: 786/1000 training accuracy: 87.77%\n",
      "Epoch: 787/1000 | Batch 000/052 | Cost: 0.3136\n",
      "Epoch: 787/1000 | Batch 010/052 | Cost: 0.2110\n",
      "Epoch: 787/1000 | Batch 020/052 | Cost: 0.5105\n",
      "Epoch: 787/1000 | Batch 030/052 | Cost: 0.2168\n",
      "Epoch: 787/1000 | Batch 040/052 | Cost: 0.4235\n",
      "Epoch: 787/1000 | Batch 050/052 | Cost: 0.2595\n",
      "Epoch: 787/1000 training accuracy: 87.43%\n",
      "Epoch: 788/1000 | Batch 000/052 | Cost: 0.1258\n",
      "Epoch: 788/1000 | Batch 010/052 | Cost: 0.1735\n",
      "Epoch: 788/1000 | Batch 020/052 | Cost: 0.2283\n",
      "Epoch: 788/1000 | Batch 030/052 | Cost: 0.4080\n",
      "Epoch: 788/1000 | Batch 040/052 | Cost: 0.4527\n",
      "Epoch: 788/1000 | Batch 050/052 | Cost: 0.2561\n",
      "Epoch: 788/1000 training accuracy: 88.66%\n",
      "Epoch: 789/1000 | Batch 000/052 | Cost: 0.0698\n",
      "Epoch: 789/1000 | Batch 010/052 | Cost: 0.4267\n",
      "Epoch: 789/1000 | Batch 020/052 | Cost: 0.2400\n",
      "Epoch: 789/1000 | Batch 030/052 | Cost: 0.3736\n",
      "Epoch: 789/1000 | Batch 040/052 | Cost: 0.0886\n",
      "Epoch: 789/1000 | Batch 050/052 | Cost: 0.5601\n",
      "Epoch: 789/1000 training accuracy: 87.77%\n",
      "Epoch: 790/1000 | Batch 000/052 | Cost: 0.1639\n",
      "Epoch: 790/1000 | Batch 010/052 | Cost: 0.4216\n",
      "Epoch: 790/1000 | Batch 020/052 | Cost: 0.3647\n",
      "Epoch: 790/1000 | Batch 030/052 | Cost: 0.1343\n",
      "Epoch: 790/1000 | Batch 040/052 | Cost: 0.1193\n",
      "Epoch: 790/1000 | Batch 050/052 | Cost: 0.0797\n",
      "Epoch: 790/1000 training accuracy: 88.10%\n",
      "Epoch: 791/1000 | Batch 000/052 | Cost: 0.4232\n",
      "Epoch: 791/1000 | Batch 010/052 | Cost: 0.1851\n",
      "Epoch: 791/1000 | Batch 020/052 | Cost: 0.7428\n",
      "Epoch: 791/1000 | Batch 030/052 | Cost: 0.3825\n",
      "Epoch: 791/1000 | Batch 040/052 | Cost: 0.1915\n",
      "Epoch: 791/1000 | Batch 050/052 | Cost: 0.3226\n",
      "Epoch: 791/1000 training accuracy: 87.99%\n",
      "Epoch: 792/1000 | Batch 000/052 | Cost: 0.2107\n",
      "Epoch: 792/1000 | Batch 010/052 | Cost: 0.3391\n",
      "Epoch: 792/1000 | Batch 020/052 | Cost: 0.3441\n",
      "Epoch: 792/1000 | Batch 030/052 | Cost: 0.1931\n",
      "Epoch: 792/1000 | Batch 040/052 | Cost: 0.5025\n",
      "Epoch: 792/1000 | Batch 050/052 | Cost: 0.2660\n",
      "Epoch: 792/1000 training accuracy: 87.65%\n",
      "Epoch: 793/1000 | Batch 000/052 | Cost: 0.2334\n",
      "Epoch: 793/1000 | Batch 010/052 | Cost: 0.3344\n",
      "Epoch: 793/1000 | Batch 020/052 | Cost: 0.2539\n",
      "Epoch: 793/1000 | Batch 030/052 | Cost: 0.1737\n",
      "Epoch: 793/1000 | Batch 040/052 | Cost: 0.2949\n",
      "Epoch: 793/1000 | Batch 050/052 | Cost: 0.1125\n",
      "Epoch: 793/1000 training accuracy: 87.54%\n",
      "Epoch: 794/1000 | Batch 000/052 | Cost: 0.3631\n",
      "Epoch: 794/1000 | Batch 010/052 | Cost: 0.4727\n",
      "Epoch: 794/1000 | Batch 020/052 | Cost: 0.2998\n",
      "Epoch: 794/1000 | Batch 030/052 | Cost: 0.1865\n",
      "Epoch: 794/1000 | Batch 040/052 | Cost: 0.2840\n",
      "Epoch: 794/1000 | Batch 050/052 | Cost: 0.1725\n",
      "Epoch: 794/1000 training accuracy: 89.00%\n",
      "Epoch: 795/1000 | Batch 000/052 | Cost: 0.1172\n",
      "Epoch: 795/1000 | Batch 010/052 | Cost: 0.1974\n",
      "Epoch: 795/1000 | Batch 020/052 | Cost: 0.4701\n",
      "Epoch: 795/1000 | Batch 030/052 | Cost: 0.0844\n",
      "Epoch: 795/1000 | Batch 040/052 | Cost: 0.1138\n",
      "Epoch: 795/1000 | Batch 050/052 | Cost: 0.4996\n",
      "Epoch: 795/1000 training accuracy: 88.55%\n",
      "Epoch: 796/1000 | Batch 000/052 | Cost: 0.3135\n",
      "Epoch: 796/1000 | Batch 010/052 | Cost: 0.5082\n",
      "Epoch: 796/1000 | Batch 020/052 | Cost: 0.4593\n",
      "Epoch: 796/1000 | Batch 030/052 | Cost: 0.3102\n",
      "Epoch: 796/1000 | Batch 040/052 | Cost: 0.1211\n",
      "Epoch: 796/1000 | Batch 050/052 | Cost: 0.1469\n",
      "Epoch: 796/1000 training accuracy: 87.54%\n",
      "Epoch: 797/1000 | Batch 000/052 | Cost: 0.1714\n",
      "Epoch: 797/1000 | Batch 010/052 | Cost: 0.5212\n",
      "Epoch: 797/1000 | Batch 020/052 | Cost: 0.2002\n",
      "Epoch: 797/1000 | Batch 030/052 | Cost: 0.2320\n",
      "Epoch: 797/1000 | Batch 040/052 | Cost: 0.4035\n",
      "Epoch: 797/1000 | Batch 050/052 | Cost: 0.2742\n",
      "Epoch: 797/1000 training accuracy: 86.87%\n",
      "Epoch: 798/1000 | Batch 000/052 | Cost: 0.1469\n",
      "Epoch: 798/1000 | Batch 010/052 | Cost: 0.1521\n",
      "Epoch: 798/1000 | Batch 020/052 | Cost: 0.1964\n",
      "Epoch: 798/1000 | Batch 030/052 | Cost: 0.2640\n",
      "Epoch: 798/1000 | Batch 040/052 | Cost: 0.3248\n",
      "Epoch: 798/1000 | Batch 050/052 | Cost: 0.1094\n",
      "Epoch: 798/1000 training accuracy: 89.67%\n",
      "Epoch: 799/1000 | Batch 000/052 | Cost: 0.4849\n",
      "Epoch: 799/1000 | Batch 010/052 | Cost: 0.1979\n",
      "Epoch: 799/1000 | Batch 020/052 | Cost: 0.3437\n",
      "Epoch: 799/1000 | Batch 030/052 | Cost: 0.3071\n",
      "Epoch: 799/1000 | Batch 040/052 | Cost: 0.2924\n",
      "Epoch: 799/1000 | Batch 050/052 | Cost: 0.0910\n",
      "Epoch: 799/1000 training accuracy: 89.23%\n",
      "Epoch: 800/1000 | Batch 000/052 | Cost: 0.3185\n",
      "Epoch: 800/1000 | Batch 010/052 | Cost: 0.1492\n",
      "Epoch: 800/1000 | Batch 020/052 | Cost: 0.2039\n",
      "Epoch: 800/1000 | Batch 030/052 | Cost: 0.3575\n",
      "Epoch: 800/1000 | Batch 040/052 | Cost: 0.3389\n",
      "Epoch: 800/1000 | Batch 050/052 | Cost: 0.3444\n",
      "Epoch: 800/1000 training accuracy: 87.54%\n",
      "Epoch: 801/1000 | Batch 000/052 | Cost: 0.3382\n",
      "Epoch: 801/1000 | Batch 010/052 | Cost: 0.2150\n",
      "Epoch: 801/1000 | Batch 020/052 | Cost: 0.1810\n",
      "Epoch: 801/1000 | Batch 030/052 | Cost: 0.0939\n",
      "Epoch: 801/1000 | Batch 040/052 | Cost: 0.3721\n",
      "Epoch: 801/1000 | Batch 050/052 | Cost: 0.2811\n",
      "Epoch: 801/1000 training accuracy: 87.88%\n",
      "Epoch: 802/1000 | Batch 000/052 | Cost: 0.3833\n",
      "Epoch: 802/1000 | Batch 010/052 | Cost: 0.1968\n",
      "Epoch: 802/1000 | Batch 020/052 | Cost: 0.5020\n",
      "Epoch: 802/1000 | Batch 030/052 | Cost: 0.2690\n",
      "Epoch: 802/1000 | Batch 040/052 | Cost: 0.0880\n",
      "Epoch: 802/1000 | Batch 050/052 | Cost: 0.3610\n",
      "Epoch: 802/1000 training accuracy: 88.10%\n",
      "Epoch: 803/1000 | Batch 000/052 | Cost: 0.4839\n",
      "Epoch: 803/1000 | Batch 010/052 | Cost: 0.2612\n",
      "Epoch: 803/1000 | Batch 020/052 | Cost: 0.1650\n",
      "Epoch: 803/1000 | Batch 030/052 | Cost: 0.1297\n",
      "Epoch: 803/1000 | Batch 040/052 | Cost: 0.2003\n",
      "Epoch: 803/1000 | Batch 050/052 | Cost: 0.2878\n",
      "Epoch: 803/1000 training accuracy: 88.10%\n",
      "Epoch: 804/1000 | Batch 000/052 | Cost: 0.1589\n",
      "Epoch: 804/1000 | Batch 010/052 | Cost: 0.3420\n",
      "Epoch: 804/1000 | Batch 020/052 | Cost: 0.2757\n",
      "Epoch: 804/1000 | Batch 030/052 | Cost: 0.4344\n",
      "Epoch: 804/1000 | Batch 040/052 | Cost: 0.2888\n",
      "Epoch: 804/1000 | Batch 050/052 | Cost: 0.1988\n",
      "Epoch: 804/1000 training accuracy: 88.55%\n",
      "Epoch: 805/1000 | Batch 000/052 | Cost: 0.2398\n",
      "Epoch: 805/1000 | Batch 010/052 | Cost: 0.1529\n",
      "Epoch: 805/1000 | Batch 020/052 | Cost: 0.2414\n",
      "Epoch: 805/1000 | Batch 030/052 | Cost: 0.2903\n",
      "Epoch: 805/1000 | Batch 040/052 | Cost: 0.2411\n",
      "Epoch: 805/1000 | Batch 050/052 | Cost: 0.2465\n",
      "Epoch: 805/1000 training accuracy: 87.88%\n",
      "Epoch: 806/1000 | Batch 000/052 | Cost: 0.4298\n",
      "Epoch: 806/1000 | Batch 010/052 | Cost: 0.2467\n",
      "Epoch: 806/1000 | Batch 020/052 | Cost: 0.3234\n",
      "Epoch: 806/1000 | Batch 030/052 | Cost: 0.0924\n",
      "Epoch: 806/1000 | Batch 040/052 | Cost: 0.3326\n",
      "Epoch: 806/1000 | Batch 050/052 | Cost: 0.2943\n",
      "Epoch: 806/1000 training accuracy: 88.10%\n",
      "Epoch: 807/1000 | Batch 000/052 | Cost: 0.1512\n",
      "Epoch: 807/1000 | Batch 010/052 | Cost: 0.0844\n",
      "Epoch: 807/1000 | Batch 020/052 | Cost: 0.4827\n",
      "Epoch: 807/1000 | Batch 030/052 | Cost: 0.1327\n",
      "Epoch: 807/1000 | Batch 040/052 | Cost: 0.6390\n",
      "Epoch: 807/1000 | Batch 050/052 | Cost: 0.1305\n",
      "Epoch: 807/1000 training accuracy: 86.87%\n",
      "Epoch: 808/1000 | Batch 000/052 | Cost: 0.1173\n",
      "Epoch: 808/1000 | Batch 010/052 | Cost: 0.4008\n",
      "Epoch: 808/1000 | Batch 020/052 | Cost: 0.1946\n",
      "Epoch: 808/1000 | Batch 030/052 | Cost: 0.3004\n",
      "Epoch: 808/1000 | Batch 040/052 | Cost: 0.2449\n",
      "Epoch: 808/1000 | Batch 050/052 | Cost: 0.1662\n",
      "Epoch: 808/1000 training accuracy: 88.10%\n",
      "Epoch: 809/1000 | Batch 000/052 | Cost: 0.2232\n",
      "Epoch: 809/1000 | Batch 010/052 | Cost: 0.1545\n",
      "Epoch: 809/1000 | Batch 020/052 | Cost: 0.1996\n",
      "Epoch: 809/1000 | Batch 030/052 | Cost: 0.4014\n",
      "Epoch: 809/1000 | Batch 040/052 | Cost: 0.4427\n",
      "Epoch: 809/1000 | Batch 050/052 | Cost: 0.3047\n",
      "Epoch: 809/1000 training accuracy: 87.65%\n",
      "Epoch: 810/1000 | Batch 000/052 | Cost: 0.2445\n",
      "Epoch: 810/1000 | Batch 010/052 | Cost: 0.2733\n",
      "Epoch: 810/1000 | Batch 020/052 | Cost: 0.2258\n",
      "Epoch: 810/1000 | Batch 030/052 | Cost: 0.2816\n",
      "Epoch: 810/1000 | Batch 040/052 | Cost: 0.3902\n",
      "Epoch: 810/1000 | Batch 050/052 | Cost: 0.4500\n",
      "Epoch: 810/1000 training accuracy: 88.22%\n",
      "Epoch: 811/1000 | Batch 000/052 | Cost: 0.1034\n",
      "Epoch: 811/1000 | Batch 010/052 | Cost: 0.4370\n",
      "Epoch: 811/1000 | Batch 020/052 | Cost: 0.3975\n",
      "Epoch: 811/1000 | Batch 030/052 | Cost: 0.2482\n",
      "Epoch: 811/1000 | Batch 040/052 | Cost: 0.3955\n",
      "Epoch: 811/1000 | Batch 050/052 | Cost: 0.5349\n",
      "Epoch: 811/1000 training accuracy: 87.21%\n",
      "Epoch: 812/1000 | Batch 000/052 | Cost: 0.1354\n",
      "Epoch: 812/1000 | Batch 010/052 | Cost: 0.5834\n",
      "Epoch: 812/1000 | Batch 020/052 | Cost: 0.0753\n",
      "Epoch: 812/1000 | Batch 030/052 | Cost: 0.5783\n",
      "Epoch: 812/1000 | Batch 040/052 | Cost: 0.2450\n",
      "Epoch: 812/1000 | Batch 050/052 | Cost: 0.4332\n",
      "Epoch: 812/1000 training accuracy: 88.55%\n",
      "Epoch: 813/1000 | Batch 000/052 | Cost: 0.3625\n",
      "Epoch: 813/1000 | Batch 010/052 | Cost: 0.6260\n",
      "Epoch: 813/1000 | Batch 020/052 | Cost: 0.1827\n",
      "Epoch: 813/1000 | Batch 030/052 | Cost: 0.1814\n",
      "Epoch: 813/1000 | Batch 040/052 | Cost: 0.3340\n",
      "Epoch: 813/1000 | Batch 050/052 | Cost: 0.0849\n",
      "Epoch: 813/1000 training accuracy: 89.00%\n",
      "Epoch: 814/1000 | Batch 000/052 | Cost: 0.4030\n",
      "Epoch: 814/1000 | Batch 010/052 | Cost: 0.1176\n",
      "Epoch: 814/1000 | Batch 020/052 | Cost: 0.4385\n",
      "Epoch: 814/1000 | Batch 030/052 | Cost: 0.4056\n",
      "Epoch: 814/1000 | Batch 040/052 | Cost: 0.2507\n",
      "Epoch: 814/1000 | Batch 050/052 | Cost: 0.2360\n",
      "Epoch: 814/1000 training accuracy: 88.10%\n",
      "Epoch: 815/1000 | Batch 000/052 | Cost: 0.1318\n",
      "Epoch: 815/1000 | Batch 010/052 | Cost: 0.2232\n",
      "Epoch: 815/1000 | Batch 020/052 | Cost: 0.5675\n",
      "Epoch: 815/1000 | Batch 030/052 | Cost: 0.2974\n",
      "Epoch: 815/1000 | Batch 040/052 | Cost: 0.4901\n",
      "Epoch: 815/1000 | Batch 050/052 | Cost: 0.1926\n",
      "Epoch: 815/1000 training accuracy: 89.00%\n",
      "Epoch: 816/1000 | Batch 000/052 | Cost: 0.1328\n",
      "Epoch: 816/1000 | Batch 010/052 | Cost: 0.2906\n",
      "Epoch: 816/1000 | Batch 020/052 | Cost: 0.4322\n",
      "Epoch: 816/1000 | Batch 030/052 | Cost: 0.3586\n",
      "Epoch: 816/1000 | Batch 040/052 | Cost: 0.1890\n",
      "Epoch: 816/1000 | Batch 050/052 | Cost: 0.0949\n",
      "Epoch: 816/1000 training accuracy: 88.22%\n",
      "Epoch: 817/1000 | Batch 000/052 | Cost: 0.2515\n",
      "Epoch: 817/1000 | Batch 010/052 | Cost: 0.1154\n",
      "Epoch: 817/1000 | Batch 020/052 | Cost: 0.1674\n",
      "Epoch: 817/1000 | Batch 030/052 | Cost: 0.3994\n",
      "Epoch: 817/1000 | Batch 040/052 | Cost: 0.2956\n",
      "Epoch: 817/1000 | Batch 050/052 | Cost: 0.2149\n",
      "Epoch: 817/1000 training accuracy: 88.89%\n",
      "Epoch: 818/1000 | Batch 000/052 | Cost: 0.3766\n",
      "Epoch: 818/1000 | Batch 010/052 | Cost: 0.2411\n",
      "Epoch: 818/1000 | Batch 020/052 | Cost: 0.1265\n",
      "Epoch: 818/1000 | Batch 030/052 | Cost: 0.1638\n",
      "Epoch: 818/1000 | Batch 040/052 | Cost: 0.1867\n",
      "Epoch: 818/1000 | Batch 050/052 | Cost: 0.3324\n",
      "Epoch: 818/1000 training accuracy: 88.44%\n",
      "Epoch: 819/1000 | Batch 000/052 | Cost: 0.2273\n",
      "Epoch: 819/1000 | Batch 010/052 | Cost: 0.2330\n",
      "Epoch: 819/1000 | Batch 020/052 | Cost: 0.2706\n",
      "Epoch: 819/1000 | Batch 030/052 | Cost: 0.3576\n",
      "Epoch: 819/1000 | Batch 040/052 | Cost: 0.3935\n",
      "Epoch: 819/1000 | Batch 050/052 | Cost: 0.1104\n",
      "Epoch: 819/1000 training accuracy: 87.99%\n",
      "Epoch: 820/1000 | Batch 000/052 | Cost: 0.1110\n",
      "Epoch: 820/1000 | Batch 010/052 | Cost: 0.2747\n",
      "Epoch: 820/1000 | Batch 020/052 | Cost: 0.1711\n",
      "Epoch: 820/1000 | Batch 030/052 | Cost: 0.2735\n",
      "Epoch: 820/1000 | Batch 040/052 | Cost: 0.1736\n",
      "Epoch: 820/1000 | Batch 050/052 | Cost: 0.2351\n",
      "Epoch: 820/1000 training accuracy: 87.88%\n",
      "Epoch: 821/1000 | Batch 000/052 | Cost: 0.5485\n",
      "Epoch: 821/1000 | Batch 010/052 | Cost: 0.2854\n",
      "Epoch: 821/1000 | Batch 020/052 | Cost: 0.2238\n",
      "Epoch: 821/1000 | Batch 030/052 | Cost: 0.2209\n",
      "Epoch: 821/1000 | Batch 040/052 | Cost: 0.3110\n",
      "Epoch: 821/1000 | Batch 050/052 | Cost: 0.3462\n",
      "Epoch: 821/1000 training accuracy: 88.55%\n",
      "Epoch: 822/1000 | Batch 000/052 | Cost: 0.6543\n",
      "Epoch: 822/1000 | Batch 010/052 | Cost: 0.2315\n",
      "Epoch: 822/1000 | Batch 020/052 | Cost: 0.2200\n",
      "Epoch: 822/1000 | Batch 030/052 | Cost: 0.2834\n",
      "Epoch: 822/1000 | Batch 040/052 | Cost: 0.2280\n",
      "Epoch: 822/1000 | Batch 050/052 | Cost: 0.3612\n",
      "Epoch: 822/1000 training accuracy: 88.44%\n",
      "Epoch: 823/1000 | Batch 000/052 | Cost: 0.2143\n",
      "Epoch: 823/1000 | Batch 010/052 | Cost: 0.4285\n",
      "Epoch: 823/1000 | Batch 020/052 | Cost: 0.2859\n",
      "Epoch: 823/1000 | Batch 030/052 | Cost: 0.4471\n",
      "Epoch: 823/1000 | Batch 040/052 | Cost: 0.2175\n",
      "Epoch: 823/1000 | Batch 050/052 | Cost: 0.1026\n",
      "Epoch: 823/1000 training accuracy: 88.22%\n",
      "Epoch: 824/1000 | Batch 000/052 | Cost: 0.4383\n",
      "Epoch: 824/1000 | Batch 010/052 | Cost: 0.1859\n",
      "Epoch: 824/1000 | Batch 020/052 | Cost: 0.5268\n",
      "Epoch: 824/1000 | Batch 030/052 | Cost: 0.1572\n",
      "Epoch: 824/1000 | Batch 040/052 | Cost: 0.1847\n",
      "Epoch: 824/1000 | Batch 050/052 | Cost: 0.2261\n",
      "Epoch: 824/1000 training accuracy: 88.55%\n",
      "Epoch: 825/1000 | Batch 000/052 | Cost: 0.1794\n",
      "Epoch: 825/1000 | Batch 010/052 | Cost: 0.3529\n",
      "Epoch: 825/1000 | Batch 020/052 | Cost: 0.2540\n",
      "Epoch: 825/1000 | Batch 030/052 | Cost: 0.2715\n",
      "Epoch: 825/1000 | Batch 040/052 | Cost: 0.2234\n",
      "Epoch: 825/1000 | Batch 050/052 | Cost: 0.2727\n",
      "Epoch: 825/1000 training accuracy: 88.22%\n",
      "Epoch: 826/1000 | Batch 000/052 | Cost: 0.3308\n",
      "Epoch: 826/1000 | Batch 010/052 | Cost: 0.1308\n",
      "Epoch: 826/1000 | Batch 020/052 | Cost: 0.4414\n",
      "Epoch: 826/1000 | Batch 030/052 | Cost: 0.3400\n",
      "Epoch: 826/1000 | Batch 040/052 | Cost: 0.3334\n",
      "Epoch: 826/1000 | Batch 050/052 | Cost: 0.3433\n",
      "Epoch: 826/1000 training accuracy: 87.99%\n",
      "Epoch: 827/1000 | Batch 000/052 | Cost: 0.2538\n",
      "Epoch: 827/1000 | Batch 010/052 | Cost: 0.2606\n",
      "Epoch: 827/1000 | Batch 020/052 | Cost: 0.1123\n",
      "Epoch: 827/1000 | Batch 030/052 | Cost: 0.3449\n",
      "Epoch: 827/1000 | Batch 040/052 | Cost: 0.2173\n",
      "Epoch: 827/1000 | Batch 050/052 | Cost: 0.3917\n",
      "Epoch: 827/1000 training accuracy: 88.66%\n",
      "Epoch: 828/1000 | Batch 000/052 | Cost: 0.3228\n",
      "Epoch: 828/1000 | Batch 010/052 | Cost: 0.1559\n",
      "Epoch: 828/1000 | Batch 020/052 | Cost: 0.2203\n",
      "Epoch: 828/1000 | Batch 030/052 | Cost: 0.4054\n",
      "Epoch: 828/1000 | Batch 040/052 | Cost: 0.2995\n",
      "Epoch: 828/1000 | Batch 050/052 | Cost: 0.2574\n",
      "Epoch: 828/1000 training accuracy: 88.10%\n",
      "Epoch: 829/1000 | Batch 000/052 | Cost: 0.1881\n",
      "Epoch: 829/1000 | Batch 010/052 | Cost: 0.1318\n",
      "Epoch: 829/1000 | Batch 020/052 | Cost: 0.1264\n",
      "Epoch: 829/1000 | Batch 030/052 | Cost: 0.4592\n",
      "Epoch: 829/1000 | Batch 040/052 | Cost: 0.3753\n",
      "Epoch: 829/1000 | Batch 050/052 | Cost: 0.1241\n",
      "Epoch: 829/1000 training accuracy: 88.55%\n",
      "Epoch: 830/1000 | Batch 000/052 | Cost: 0.1492\n",
      "Epoch: 830/1000 | Batch 010/052 | Cost: 0.4637\n",
      "Epoch: 830/1000 | Batch 020/052 | Cost: 0.2392\n",
      "Epoch: 830/1000 | Batch 030/052 | Cost: 0.2604\n",
      "Epoch: 830/1000 | Batch 040/052 | Cost: 0.4245\n",
      "Epoch: 830/1000 | Batch 050/052 | Cost: 0.5055\n",
      "Epoch: 830/1000 training accuracy: 87.88%\n",
      "Epoch: 831/1000 | Batch 000/052 | Cost: 0.0730\n",
      "Epoch: 831/1000 | Batch 010/052 | Cost: 0.0907\n",
      "Epoch: 831/1000 | Batch 020/052 | Cost: 0.3575\n",
      "Epoch: 831/1000 | Batch 030/052 | Cost: 0.2918\n",
      "Epoch: 831/1000 | Batch 040/052 | Cost: 0.3290\n",
      "Epoch: 831/1000 | Batch 050/052 | Cost: 0.3892\n",
      "Epoch: 831/1000 training accuracy: 88.33%\n",
      "Epoch: 832/1000 | Batch 000/052 | Cost: 0.2493\n",
      "Epoch: 832/1000 | Batch 010/052 | Cost: 0.1929\n",
      "Epoch: 832/1000 | Batch 020/052 | Cost: 0.3742\n",
      "Epoch: 832/1000 | Batch 030/052 | Cost: 0.2023\n",
      "Epoch: 832/1000 | Batch 040/052 | Cost: 0.2507\n",
      "Epoch: 832/1000 | Batch 050/052 | Cost: 0.0968\n",
      "Epoch: 832/1000 training accuracy: 88.55%\n",
      "Epoch: 833/1000 | Batch 000/052 | Cost: 0.2573\n",
      "Epoch: 833/1000 | Batch 010/052 | Cost: 0.4039\n",
      "Epoch: 833/1000 | Batch 020/052 | Cost: 0.0706\n",
      "Epoch: 833/1000 | Batch 030/052 | Cost: 0.4237\n",
      "Epoch: 833/1000 | Batch 040/052 | Cost: 0.7972\n",
      "Epoch: 833/1000 | Batch 050/052 | Cost: 0.1802\n",
      "Epoch: 833/1000 training accuracy: 87.65%\n",
      "Epoch: 834/1000 | Batch 000/052 | Cost: 0.5138\n",
      "Epoch: 834/1000 | Batch 010/052 | Cost: 0.2337\n",
      "Epoch: 834/1000 | Batch 020/052 | Cost: 0.3181\n",
      "Epoch: 834/1000 | Batch 030/052 | Cost: 0.1315\n",
      "Epoch: 834/1000 | Batch 040/052 | Cost: 0.1781\n",
      "Epoch: 834/1000 | Batch 050/052 | Cost: 0.2751\n",
      "Epoch: 834/1000 training accuracy: 89.23%\n",
      "Epoch: 835/1000 | Batch 000/052 | Cost: 0.3111\n",
      "Epoch: 835/1000 | Batch 010/052 | Cost: 0.2241\n",
      "Epoch: 835/1000 | Batch 020/052 | Cost: 0.4767\n",
      "Epoch: 835/1000 | Batch 030/052 | Cost: 0.2249\n",
      "Epoch: 835/1000 | Batch 040/052 | Cost: 0.2443\n",
      "Epoch: 835/1000 | Batch 050/052 | Cost: 0.2961\n",
      "Epoch: 835/1000 training accuracy: 87.88%\n",
      "Epoch: 836/1000 | Batch 000/052 | Cost: 0.1583\n",
      "Epoch: 836/1000 | Batch 010/052 | Cost: 0.2631\n",
      "Epoch: 836/1000 | Batch 020/052 | Cost: 0.4043\n",
      "Epoch: 836/1000 | Batch 030/052 | Cost: 0.3325\n",
      "Epoch: 836/1000 | Batch 040/052 | Cost: 0.3166\n",
      "Epoch: 836/1000 | Batch 050/052 | Cost: 0.2582\n",
      "Epoch: 836/1000 training accuracy: 88.78%\n",
      "Epoch: 837/1000 | Batch 000/052 | Cost: 0.2153\n",
      "Epoch: 837/1000 | Batch 010/052 | Cost: 0.3406\n",
      "Epoch: 837/1000 | Batch 020/052 | Cost: 0.2502\n",
      "Epoch: 837/1000 | Batch 030/052 | Cost: 0.2358\n",
      "Epoch: 837/1000 | Batch 040/052 | Cost: 0.1619\n",
      "Epoch: 837/1000 | Batch 050/052 | Cost: 0.2715\n",
      "Epoch: 837/1000 training accuracy: 88.22%\n",
      "Epoch: 838/1000 | Batch 000/052 | Cost: 0.2585\n",
      "Epoch: 838/1000 | Batch 010/052 | Cost: 0.1973\n",
      "Epoch: 838/1000 | Batch 020/052 | Cost: 0.2208\n",
      "Epoch: 838/1000 | Batch 030/052 | Cost: 0.3183\n",
      "Epoch: 838/1000 | Batch 040/052 | Cost: 0.4197\n",
      "Epoch: 838/1000 | Batch 050/052 | Cost: 0.1457\n",
      "Epoch: 838/1000 training accuracy: 88.66%\n",
      "Epoch: 839/1000 | Batch 000/052 | Cost: 0.2387\n",
      "Epoch: 839/1000 | Batch 010/052 | Cost: 0.1277\n",
      "Epoch: 839/1000 | Batch 020/052 | Cost: 0.2384\n",
      "Epoch: 839/1000 | Batch 030/052 | Cost: 0.2092\n",
      "Epoch: 839/1000 | Batch 040/052 | Cost: 0.3684\n",
      "Epoch: 839/1000 | Batch 050/052 | Cost: 0.2399\n",
      "Epoch: 839/1000 training accuracy: 87.99%\n",
      "Epoch: 840/1000 | Batch 000/052 | Cost: 0.4116\n",
      "Epoch: 840/1000 | Batch 010/052 | Cost: 0.2522\n",
      "Epoch: 840/1000 | Batch 020/052 | Cost: 0.1999\n",
      "Epoch: 840/1000 | Batch 030/052 | Cost: 0.1926\n",
      "Epoch: 840/1000 | Batch 040/052 | Cost: 0.3418\n",
      "Epoch: 840/1000 | Batch 050/052 | Cost: 0.2660\n",
      "Epoch: 840/1000 training accuracy: 89.90%\n",
      "Epoch: 841/1000 | Batch 000/052 | Cost: 0.4713\n",
      "Epoch: 841/1000 | Batch 010/052 | Cost: 0.3807\n",
      "Epoch: 841/1000 | Batch 020/052 | Cost: 0.3363\n",
      "Epoch: 841/1000 | Batch 030/052 | Cost: 0.1485\n",
      "Epoch: 841/1000 | Batch 040/052 | Cost: 0.5689\n",
      "Epoch: 841/1000 | Batch 050/052 | Cost: 0.3907\n",
      "Epoch: 841/1000 training accuracy: 88.55%\n",
      "Epoch: 842/1000 | Batch 000/052 | Cost: 0.1447\n",
      "Epoch: 842/1000 | Batch 010/052 | Cost: 0.6230\n",
      "Epoch: 842/1000 | Batch 020/052 | Cost: 0.3299\n",
      "Epoch: 842/1000 | Batch 030/052 | Cost: 0.1412\n",
      "Epoch: 842/1000 | Batch 040/052 | Cost: 0.1651\n",
      "Epoch: 842/1000 | Batch 050/052 | Cost: 0.1441\n",
      "Epoch: 842/1000 training accuracy: 89.00%\n",
      "Epoch: 843/1000 | Batch 000/052 | Cost: 0.1215\n",
      "Epoch: 843/1000 | Batch 010/052 | Cost: 0.2828\n",
      "Epoch: 843/1000 | Batch 020/052 | Cost: 0.5438\n",
      "Epoch: 843/1000 | Batch 030/052 | Cost: 0.5013\n",
      "Epoch: 843/1000 | Batch 040/052 | Cost: 0.2884\n",
      "Epoch: 843/1000 | Batch 050/052 | Cost: 0.3071\n",
      "Epoch: 843/1000 training accuracy: 88.78%\n",
      "Epoch: 844/1000 | Batch 000/052 | Cost: 0.2225\n",
      "Epoch: 844/1000 | Batch 010/052 | Cost: 0.5452\n",
      "Epoch: 844/1000 | Batch 020/052 | Cost: 0.3380\n",
      "Epoch: 844/1000 | Batch 030/052 | Cost: 0.1452\n",
      "Epoch: 844/1000 | Batch 040/052 | Cost: 0.3067\n",
      "Epoch: 844/1000 | Batch 050/052 | Cost: 0.3592\n",
      "Epoch: 844/1000 training accuracy: 87.43%\n",
      "Epoch: 845/1000 | Batch 000/052 | Cost: 0.1985\n",
      "Epoch: 845/1000 | Batch 010/052 | Cost: 0.3267\n",
      "Epoch: 845/1000 | Batch 020/052 | Cost: 0.0521\n",
      "Epoch: 845/1000 | Batch 030/052 | Cost: 0.4589\n",
      "Epoch: 845/1000 | Batch 040/052 | Cost: 0.1576\n",
      "Epoch: 845/1000 | Batch 050/052 | Cost: 0.3859\n",
      "Epoch: 845/1000 training accuracy: 88.66%\n",
      "Epoch: 846/1000 | Batch 000/052 | Cost: 0.3317\n",
      "Epoch: 846/1000 | Batch 010/052 | Cost: 0.2732\n",
      "Epoch: 846/1000 | Batch 020/052 | Cost: 0.3855\n",
      "Epoch: 846/1000 | Batch 030/052 | Cost: 0.2389\n",
      "Epoch: 846/1000 | Batch 040/052 | Cost: 0.1259\n",
      "Epoch: 846/1000 | Batch 050/052 | Cost: 0.3930\n",
      "Epoch: 846/1000 training accuracy: 88.66%\n",
      "Epoch: 847/1000 | Batch 000/052 | Cost: 0.3517\n",
      "Epoch: 847/1000 | Batch 010/052 | Cost: 0.4316\n",
      "Epoch: 847/1000 | Batch 020/052 | Cost: 0.2372\n",
      "Epoch: 847/1000 | Batch 030/052 | Cost: 0.1563\n",
      "Epoch: 847/1000 | Batch 040/052 | Cost: 0.1882\n",
      "Epoch: 847/1000 | Batch 050/052 | Cost: 0.3161\n",
      "Epoch: 847/1000 training accuracy: 88.10%\n",
      "Epoch: 848/1000 | Batch 000/052 | Cost: 0.1607\n",
      "Epoch: 848/1000 | Batch 010/052 | Cost: 0.1245\n",
      "Epoch: 848/1000 | Batch 020/052 | Cost: 0.1838\n",
      "Epoch: 848/1000 | Batch 030/052 | Cost: 0.3234\n",
      "Epoch: 848/1000 | Batch 040/052 | Cost: 0.2576\n",
      "Epoch: 848/1000 | Batch 050/052 | Cost: 0.4074\n",
      "Epoch: 848/1000 training accuracy: 89.00%\n",
      "Epoch: 849/1000 | Batch 000/052 | Cost: 0.3577\n",
      "Epoch: 849/1000 | Batch 010/052 | Cost: 0.3580\n",
      "Epoch: 849/1000 | Batch 020/052 | Cost: 0.5097\n",
      "Epoch: 849/1000 | Batch 030/052 | Cost: 0.1962\n",
      "Epoch: 849/1000 | Batch 040/052 | Cost: 0.2150\n",
      "Epoch: 849/1000 | Batch 050/052 | Cost: 0.2067\n",
      "Epoch: 849/1000 training accuracy: 89.34%\n",
      "Epoch: 850/1000 | Batch 000/052 | Cost: 0.3591\n",
      "Epoch: 850/1000 | Batch 010/052 | Cost: 0.1875\n",
      "Epoch: 850/1000 | Batch 020/052 | Cost: 0.2868\n",
      "Epoch: 850/1000 | Batch 030/052 | Cost: 0.3332\n",
      "Epoch: 850/1000 | Batch 040/052 | Cost: 0.2493\n",
      "Epoch: 850/1000 | Batch 050/052 | Cost: 0.1613\n",
      "Epoch: 850/1000 training accuracy: 88.33%\n",
      "Epoch: 851/1000 | Batch 000/052 | Cost: 0.1702\n",
      "Epoch: 851/1000 | Batch 010/052 | Cost: 0.2262\n",
      "Epoch: 851/1000 | Batch 020/052 | Cost: 0.1265\n",
      "Epoch: 851/1000 | Batch 030/052 | Cost: 0.3225\n",
      "Epoch: 851/1000 | Batch 040/052 | Cost: 0.1165\n",
      "Epoch: 851/1000 | Batch 050/052 | Cost: 0.0861\n",
      "Epoch: 851/1000 training accuracy: 88.89%\n",
      "Epoch: 852/1000 | Batch 000/052 | Cost: 0.0628\n",
      "Epoch: 852/1000 | Batch 010/052 | Cost: 0.3500\n",
      "Epoch: 852/1000 | Batch 020/052 | Cost: 0.1380\n",
      "Epoch: 852/1000 | Batch 030/052 | Cost: 0.2269\n",
      "Epoch: 852/1000 | Batch 040/052 | Cost: 0.0983\n",
      "Epoch: 852/1000 | Batch 050/052 | Cost: 0.2669\n",
      "Epoch: 852/1000 training accuracy: 88.22%\n",
      "Epoch: 853/1000 | Batch 000/052 | Cost: 0.2063\n",
      "Epoch: 853/1000 | Batch 010/052 | Cost: 0.4109\n",
      "Epoch: 853/1000 | Batch 020/052 | Cost: 0.3622\n",
      "Epoch: 853/1000 | Batch 030/052 | Cost: 0.3036\n",
      "Epoch: 853/1000 | Batch 040/052 | Cost: 0.2118\n",
      "Epoch: 853/1000 | Batch 050/052 | Cost: 0.2280\n",
      "Epoch: 853/1000 training accuracy: 88.66%\n",
      "Epoch: 854/1000 | Batch 000/052 | Cost: 0.2505\n",
      "Epoch: 854/1000 | Batch 010/052 | Cost: 0.3646\n",
      "Epoch: 854/1000 | Batch 020/052 | Cost: 0.2379\n",
      "Epoch: 854/1000 | Batch 030/052 | Cost: 0.2486\n",
      "Epoch: 854/1000 | Batch 040/052 | Cost: 0.2494\n",
      "Epoch: 854/1000 | Batch 050/052 | Cost: 0.3351\n",
      "Epoch: 854/1000 training accuracy: 88.89%\n",
      "Epoch: 855/1000 | Batch 000/052 | Cost: 0.3383\n",
      "Epoch: 855/1000 | Batch 010/052 | Cost: 0.2875\n",
      "Epoch: 855/1000 | Batch 020/052 | Cost: 0.2218\n",
      "Epoch: 855/1000 | Batch 030/052 | Cost: 0.4416\n",
      "Epoch: 855/1000 | Batch 040/052 | Cost: 0.5015\n",
      "Epoch: 855/1000 | Batch 050/052 | Cost: 0.3777\n",
      "Epoch: 855/1000 training accuracy: 88.78%\n",
      "Epoch: 856/1000 | Batch 000/052 | Cost: 0.1909\n",
      "Epoch: 856/1000 | Batch 010/052 | Cost: 0.4465\n",
      "Epoch: 856/1000 | Batch 020/052 | Cost: 0.3078\n",
      "Epoch: 856/1000 | Batch 030/052 | Cost: 0.3439\n",
      "Epoch: 856/1000 | Batch 040/052 | Cost: 0.3162\n",
      "Epoch: 856/1000 | Batch 050/052 | Cost: 0.3514\n",
      "Epoch: 856/1000 training accuracy: 88.10%\n",
      "Epoch: 857/1000 | Batch 000/052 | Cost: 0.1518\n",
      "Epoch: 857/1000 | Batch 010/052 | Cost: 0.1704\n",
      "Epoch: 857/1000 | Batch 020/052 | Cost: 0.4162\n",
      "Epoch: 857/1000 | Batch 030/052 | Cost: 0.1170\n",
      "Epoch: 857/1000 | Batch 040/052 | Cost: 0.3377\n",
      "Epoch: 857/1000 | Batch 050/052 | Cost: 0.1445\n",
      "Epoch: 857/1000 training accuracy: 89.34%\n",
      "Epoch: 858/1000 | Batch 000/052 | Cost: 0.4642\n",
      "Epoch: 858/1000 | Batch 010/052 | Cost: 0.8335\n",
      "Epoch: 858/1000 | Batch 020/052 | Cost: 0.2043\n",
      "Epoch: 858/1000 | Batch 030/052 | Cost: 0.2016\n",
      "Epoch: 858/1000 | Batch 040/052 | Cost: 0.5385\n",
      "Epoch: 858/1000 | Batch 050/052 | Cost: 0.3025\n",
      "Epoch: 858/1000 training accuracy: 88.78%\n",
      "Epoch: 859/1000 | Batch 000/052 | Cost: 0.1665\n",
      "Epoch: 859/1000 | Batch 010/052 | Cost: 0.2783\n",
      "Epoch: 859/1000 | Batch 020/052 | Cost: 0.3158\n",
      "Epoch: 859/1000 | Batch 030/052 | Cost: 0.2271\n",
      "Epoch: 859/1000 | Batch 040/052 | Cost: 0.5028\n",
      "Epoch: 859/1000 | Batch 050/052 | Cost: 0.1989\n",
      "Epoch: 859/1000 training accuracy: 88.10%\n",
      "Epoch: 860/1000 | Batch 000/052 | Cost: 0.1505\n",
      "Epoch: 860/1000 | Batch 010/052 | Cost: 0.4162\n",
      "Epoch: 860/1000 | Batch 020/052 | Cost: 0.3145\n",
      "Epoch: 860/1000 | Batch 030/052 | Cost: 0.1783\n",
      "Epoch: 860/1000 | Batch 040/052 | Cost: 0.3165\n",
      "Epoch: 860/1000 | Batch 050/052 | Cost: 0.2189\n",
      "Epoch: 860/1000 training accuracy: 87.54%\n",
      "Epoch: 861/1000 | Batch 000/052 | Cost: 0.3907\n",
      "Epoch: 861/1000 | Batch 010/052 | Cost: 0.0998\n",
      "Epoch: 861/1000 | Batch 020/052 | Cost: 0.1950\n",
      "Epoch: 861/1000 | Batch 030/052 | Cost: 0.1818\n",
      "Epoch: 861/1000 | Batch 040/052 | Cost: 0.3806\n",
      "Epoch: 861/1000 | Batch 050/052 | Cost: 0.5668\n",
      "Epoch: 861/1000 training accuracy: 89.00%\n",
      "Epoch: 862/1000 | Batch 000/052 | Cost: 0.1935\n",
      "Epoch: 862/1000 | Batch 010/052 | Cost: 0.3758\n",
      "Epoch: 862/1000 | Batch 020/052 | Cost: 0.3194\n",
      "Epoch: 862/1000 | Batch 030/052 | Cost: 0.3076\n",
      "Epoch: 862/1000 | Batch 040/052 | Cost: 0.1302\n",
      "Epoch: 862/1000 | Batch 050/052 | Cost: 0.0727\n",
      "Epoch: 862/1000 training accuracy: 88.22%\n",
      "Epoch: 863/1000 | Batch 000/052 | Cost: 0.1892\n",
      "Epoch: 863/1000 | Batch 010/052 | Cost: 0.0780\n",
      "Epoch: 863/1000 | Batch 020/052 | Cost: 0.2895\n",
      "Epoch: 863/1000 | Batch 030/052 | Cost: 0.1429\n",
      "Epoch: 863/1000 | Batch 040/052 | Cost: 0.0772\n",
      "Epoch: 863/1000 | Batch 050/052 | Cost: 0.2054\n",
      "Epoch: 863/1000 training accuracy: 88.66%\n",
      "Epoch: 864/1000 | Batch 000/052 | Cost: 0.0762\n",
      "Epoch: 864/1000 | Batch 010/052 | Cost: 0.2105\n",
      "Epoch: 864/1000 | Batch 020/052 | Cost: 0.1835\n",
      "Epoch: 864/1000 | Batch 030/052 | Cost: 0.2449\n",
      "Epoch: 864/1000 | Batch 040/052 | Cost: 0.3757\n",
      "Epoch: 864/1000 | Batch 050/052 | Cost: 0.3698\n",
      "Epoch: 864/1000 training accuracy: 88.22%\n",
      "Epoch: 865/1000 | Batch 000/052 | Cost: 0.3260\n",
      "Epoch: 865/1000 | Batch 010/052 | Cost: 0.1647\n",
      "Epoch: 865/1000 | Batch 020/052 | Cost: 0.1087\n",
      "Epoch: 865/1000 | Batch 030/052 | Cost: 0.5324\n",
      "Epoch: 865/1000 | Batch 040/052 | Cost: 0.4708\n",
      "Epoch: 865/1000 | Batch 050/052 | Cost: 0.3516\n",
      "Epoch: 865/1000 training accuracy: 88.22%\n",
      "Epoch: 866/1000 | Batch 000/052 | Cost: 0.2727\n",
      "Epoch: 866/1000 | Batch 010/052 | Cost: 0.2651\n",
      "Epoch: 866/1000 | Batch 020/052 | Cost: 0.1389\n",
      "Epoch: 866/1000 | Batch 030/052 | Cost: 0.1618\n",
      "Epoch: 866/1000 | Batch 040/052 | Cost: 0.2131\n",
      "Epoch: 866/1000 | Batch 050/052 | Cost: 0.4989\n",
      "Epoch: 866/1000 training accuracy: 89.34%\n",
      "Epoch: 867/1000 | Batch 000/052 | Cost: 0.1856\n",
      "Epoch: 867/1000 | Batch 010/052 | Cost: 0.1519\n",
      "Epoch: 867/1000 | Batch 020/052 | Cost: 0.0819\n",
      "Epoch: 867/1000 | Batch 030/052 | Cost: 0.4494\n",
      "Epoch: 867/1000 | Batch 040/052 | Cost: 0.3617\n",
      "Epoch: 867/1000 | Batch 050/052 | Cost: 0.1794\n",
      "Epoch: 867/1000 training accuracy: 87.65%\n",
      "Epoch: 868/1000 | Batch 000/052 | Cost: 0.3934\n",
      "Epoch: 868/1000 | Batch 010/052 | Cost: 0.2791\n",
      "Epoch: 868/1000 | Batch 020/052 | Cost: 0.4626\n",
      "Epoch: 868/1000 | Batch 030/052 | Cost: 0.1818\n",
      "Epoch: 868/1000 | Batch 040/052 | Cost: 0.3495\n",
      "Epoch: 868/1000 | Batch 050/052 | Cost: 0.0925\n",
      "Epoch: 868/1000 training accuracy: 89.00%\n",
      "Epoch: 869/1000 | Batch 000/052 | Cost: 0.2023\n",
      "Epoch: 869/1000 | Batch 010/052 | Cost: 0.4662\n",
      "Epoch: 869/1000 | Batch 020/052 | Cost: 0.2483\n",
      "Epoch: 869/1000 | Batch 030/052 | Cost: 0.1816\n",
      "Epoch: 869/1000 | Batch 040/052 | Cost: 0.4386\n",
      "Epoch: 869/1000 | Batch 050/052 | Cost: 0.2400\n",
      "Epoch: 869/1000 training accuracy: 88.22%\n",
      "Epoch: 870/1000 | Batch 000/052 | Cost: 0.0993\n",
      "Epoch: 870/1000 | Batch 010/052 | Cost: 0.1628\n",
      "Epoch: 870/1000 | Batch 020/052 | Cost: 0.0925\n",
      "Epoch: 870/1000 | Batch 030/052 | Cost: 0.1705\n",
      "Epoch: 870/1000 | Batch 040/052 | Cost: 0.2692\n",
      "Epoch: 870/1000 | Batch 050/052 | Cost: 0.1228\n",
      "Epoch: 870/1000 training accuracy: 87.88%\n",
      "Epoch: 871/1000 | Batch 000/052 | Cost: 0.3595\n",
      "Epoch: 871/1000 | Batch 010/052 | Cost: 0.1197\n",
      "Epoch: 871/1000 | Batch 020/052 | Cost: 0.1520\n",
      "Epoch: 871/1000 | Batch 030/052 | Cost: 0.4112\n",
      "Epoch: 871/1000 | Batch 040/052 | Cost: 0.5089\n",
      "Epoch: 871/1000 | Batch 050/052 | Cost: 0.3145\n",
      "Epoch: 871/1000 training accuracy: 89.67%\n",
      "Epoch: 872/1000 | Batch 000/052 | Cost: 0.1996\n",
      "Epoch: 872/1000 | Batch 010/052 | Cost: 0.2598\n",
      "Epoch: 872/1000 | Batch 020/052 | Cost: 0.2326\n",
      "Epoch: 872/1000 | Batch 030/052 | Cost: 0.2781\n",
      "Epoch: 872/1000 | Batch 040/052 | Cost: 0.2015\n",
      "Epoch: 872/1000 | Batch 050/052 | Cost: 0.0980\n",
      "Epoch: 872/1000 training accuracy: 88.66%\n",
      "Epoch: 873/1000 | Batch 000/052 | Cost: 0.2070\n",
      "Epoch: 873/1000 | Batch 010/052 | Cost: 0.2854\n",
      "Epoch: 873/1000 | Batch 020/052 | Cost: 0.2959\n",
      "Epoch: 873/1000 | Batch 030/052 | Cost: 0.4298\n",
      "Epoch: 873/1000 | Batch 040/052 | Cost: 0.2738\n",
      "Epoch: 873/1000 | Batch 050/052 | Cost: 0.2766\n",
      "Epoch: 873/1000 training accuracy: 88.66%\n",
      "Epoch: 874/1000 | Batch 000/052 | Cost: 0.1135\n",
      "Epoch: 874/1000 | Batch 010/052 | Cost: 0.3739\n",
      "Epoch: 874/1000 | Batch 020/052 | Cost: 0.1496\n",
      "Epoch: 874/1000 | Batch 030/052 | Cost: 0.1713\n",
      "Epoch: 874/1000 | Batch 040/052 | Cost: 0.2186\n",
      "Epoch: 874/1000 | Batch 050/052 | Cost: 0.3675\n",
      "Epoch: 874/1000 training accuracy: 88.66%\n",
      "Epoch: 875/1000 | Batch 000/052 | Cost: 0.2471\n",
      "Epoch: 875/1000 | Batch 010/052 | Cost: 0.3054\n",
      "Epoch: 875/1000 | Batch 020/052 | Cost: 0.1400\n",
      "Epoch: 875/1000 | Batch 030/052 | Cost: 0.2102\n",
      "Epoch: 875/1000 | Batch 040/052 | Cost: 0.2873\n",
      "Epoch: 875/1000 | Batch 050/052 | Cost: 0.2929\n",
      "Epoch: 875/1000 training accuracy: 88.89%\n",
      "Epoch: 876/1000 | Batch 000/052 | Cost: 0.2788\n",
      "Epoch: 876/1000 | Batch 010/052 | Cost: 0.3434\n",
      "Epoch: 876/1000 | Batch 020/052 | Cost: 0.0994\n",
      "Epoch: 876/1000 | Batch 030/052 | Cost: 0.4012\n",
      "Epoch: 876/1000 | Batch 040/052 | Cost: 0.1794\n",
      "Epoch: 876/1000 | Batch 050/052 | Cost: 0.2621\n",
      "Epoch: 876/1000 training accuracy: 88.55%\n",
      "Epoch: 877/1000 | Batch 000/052 | Cost: 0.3406\n",
      "Epoch: 877/1000 | Batch 010/052 | Cost: 0.2847\n",
      "Epoch: 877/1000 | Batch 020/052 | Cost: 0.7738\n",
      "Epoch: 877/1000 | Batch 030/052 | Cost: 0.1801\n",
      "Epoch: 877/1000 | Batch 040/052 | Cost: 0.3416\n",
      "Epoch: 877/1000 | Batch 050/052 | Cost: 0.0896\n",
      "Epoch: 877/1000 training accuracy: 88.22%\n",
      "Epoch: 878/1000 | Batch 000/052 | Cost: 0.1291\n",
      "Epoch: 878/1000 | Batch 010/052 | Cost: 0.1586\n",
      "Epoch: 878/1000 | Batch 020/052 | Cost: 0.2197\n",
      "Epoch: 878/1000 | Batch 030/052 | Cost: 0.2716\n",
      "Epoch: 878/1000 | Batch 040/052 | Cost: 0.2838\n",
      "Epoch: 878/1000 | Batch 050/052 | Cost: 0.1070\n",
      "Epoch: 878/1000 training accuracy: 88.66%\n",
      "Epoch: 879/1000 | Batch 000/052 | Cost: 0.1588\n",
      "Epoch: 879/1000 | Batch 010/052 | Cost: 0.3056\n",
      "Epoch: 879/1000 | Batch 020/052 | Cost: 0.1206\n",
      "Epoch: 879/1000 | Batch 030/052 | Cost: 0.2676\n",
      "Epoch: 879/1000 | Batch 040/052 | Cost: 0.3180\n",
      "Epoch: 879/1000 | Batch 050/052 | Cost: 0.5381\n",
      "Epoch: 879/1000 training accuracy: 88.66%\n",
      "Epoch: 880/1000 | Batch 000/052 | Cost: 0.1472\n",
      "Epoch: 880/1000 | Batch 010/052 | Cost: 0.2028\n",
      "Epoch: 880/1000 | Batch 020/052 | Cost: 0.4073\n",
      "Epoch: 880/1000 | Batch 030/052 | Cost: 0.1291\n",
      "Epoch: 880/1000 | Batch 040/052 | Cost: 0.2624\n",
      "Epoch: 880/1000 | Batch 050/052 | Cost: 0.2003\n",
      "Epoch: 880/1000 training accuracy: 87.21%\n",
      "Epoch: 881/1000 | Batch 000/052 | Cost: 0.1183\n",
      "Epoch: 881/1000 | Batch 010/052 | Cost: 0.3426\n",
      "Epoch: 881/1000 | Batch 020/052 | Cost: 0.2578\n",
      "Epoch: 881/1000 | Batch 030/052 | Cost: 0.3238\n",
      "Epoch: 881/1000 | Batch 040/052 | Cost: 0.3563\n",
      "Epoch: 881/1000 | Batch 050/052 | Cost: 0.2493\n",
      "Epoch: 881/1000 training accuracy: 88.33%\n",
      "Epoch: 882/1000 | Batch 000/052 | Cost: 0.2911\n",
      "Epoch: 882/1000 | Batch 010/052 | Cost: 0.2391\n",
      "Epoch: 882/1000 | Batch 020/052 | Cost: 0.4178\n",
      "Epoch: 882/1000 | Batch 030/052 | Cost: 0.1622\n",
      "Epoch: 882/1000 | Batch 040/052 | Cost: 0.1387\n",
      "Epoch: 882/1000 | Batch 050/052 | Cost: 0.5124\n",
      "Epoch: 882/1000 training accuracy: 87.65%\n",
      "Epoch: 883/1000 | Batch 000/052 | Cost: 0.3117\n",
      "Epoch: 883/1000 | Batch 010/052 | Cost: 0.4662\n",
      "Epoch: 883/1000 | Batch 020/052 | Cost: 0.1325\n",
      "Epoch: 883/1000 | Batch 030/052 | Cost: 0.0835\n",
      "Epoch: 883/1000 | Batch 040/052 | Cost: 0.2858\n",
      "Epoch: 883/1000 | Batch 050/052 | Cost: 0.1429\n",
      "Epoch: 883/1000 training accuracy: 88.10%\n",
      "Epoch: 884/1000 | Batch 000/052 | Cost: 0.1917\n",
      "Epoch: 884/1000 | Batch 010/052 | Cost: 0.4292\n",
      "Epoch: 884/1000 | Batch 020/052 | Cost: 0.3380\n",
      "Epoch: 884/1000 | Batch 030/052 | Cost: 0.2270\n",
      "Epoch: 884/1000 | Batch 040/052 | Cost: 0.3060\n",
      "Epoch: 884/1000 | Batch 050/052 | Cost: 0.3570\n",
      "Epoch: 884/1000 training accuracy: 88.89%\n",
      "Epoch: 885/1000 | Batch 000/052 | Cost: 0.1046\n",
      "Epoch: 885/1000 | Batch 010/052 | Cost: 0.1717\n",
      "Epoch: 885/1000 | Batch 020/052 | Cost: 0.2305\n",
      "Epoch: 885/1000 | Batch 030/052 | Cost: 0.4238\n",
      "Epoch: 885/1000 | Batch 040/052 | Cost: 0.2837\n",
      "Epoch: 885/1000 | Batch 050/052 | Cost: 0.3353\n",
      "Epoch: 885/1000 training accuracy: 88.10%\n",
      "Epoch: 886/1000 | Batch 000/052 | Cost: 0.3336\n",
      "Epoch: 886/1000 | Batch 010/052 | Cost: 0.3734\n",
      "Epoch: 886/1000 | Batch 020/052 | Cost: 0.1088\n",
      "Epoch: 886/1000 | Batch 030/052 | Cost: 0.2148\n",
      "Epoch: 886/1000 | Batch 040/052 | Cost: 0.2800\n",
      "Epoch: 886/1000 | Batch 050/052 | Cost: 0.3332\n",
      "Epoch: 886/1000 training accuracy: 87.99%\n",
      "Epoch: 887/1000 | Batch 000/052 | Cost: 0.3451\n",
      "Epoch: 887/1000 | Batch 010/052 | Cost: 0.3132\n",
      "Epoch: 887/1000 | Batch 020/052 | Cost: 0.2996\n",
      "Epoch: 887/1000 | Batch 030/052 | Cost: 0.3984\n",
      "Epoch: 887/1000 | Batch 040/052 | Cost: 0.2653\n",
      "Epoch: 887/1000 | Batch 050/052 | Cost: 0.3480\n",
      "Epoch: 887/1000 training accuracy: 87.65%\n",
      "Epoch: 888/1000 | Batch 000/052 | Cost: 0.2853\n",
      "Epoch: 888/1000 | Batch 010/052 | Cost: 0.5077\n",
      "Epoch: 888/1000 | Batch 020/052 | Cost: 0.3034\n",
      "Epoch: 888/1000 | Batch 030/052 | Cost: 0.3910\n",
      "Epoch: 888/1000 | Batch 040/052 | Cost: 0.3395\n",
      "Epoch: 888/1000 | Batch 050/052 | Cost: 0.2051\n",
      "Epoch: 888/1000 training accuracy: 87.09%\n",
      "Epoch: 889/1000 | Batch 000/052 | Cost: 0.1885\n",
      "Epoch: 889/1000 | Batch 010/052 | Cost: 0.3533\n",
      "Epoch: 889/1000 | Batch 020/052 | Cost: 0.5002\n",
      "Epoch: 889/1000 | Batch 030/052 | Cost: 0.2964\n",
      "Epoch: 889/1000 | Batch 040/052 | Cost: 0.1177\n",
      "Epoch: 889/1000 | Batch 050/052 | Cost: 0.4488\n",
      "Epoch: 889/1000 training accuracy: 87.88%\n",
      "Epoch: 890/1000 | Batch 000/052 | Cost: 0.2908\n",
      "Epoch: 890/1000 | Batch 010/052 | Cost: 0.3453\n",
      "Epoch: 890/1000 | Batch 020/052 | Cost: 0.1844\n",
      "Epoch: 890/1000 | Batch 030/052 | Cost: 0.3009\n",
      "Epoch: 890/1000 | Batch 040/052 | Cost: 0.0830\n",
      "Epoch: 890/1000 | Batch 050/052 | Cost: 0.3386\n",
      "Epoch: 890/1000 training accuracy: 88.33%\n",
      "Epoch: 891/1000 | Batch 000/052 | Cost: 0.2743\n",
      "Epoch: 891/1000 | Batch 010/052 | Cost: 0.1906\n",
      "Epoch: 891/1000 | Batch 020/052 | Cost: 0.1528\n",
      "Epoch: 891/1000 | Batch 030/052 | Cost: 0.1447\n",
      "Epoch: 891/1000 | Batch 040/052 | Cost: 0.3194\n",
      "Epoch: 891/1000 | Batch 050/052 | Cost: 0.4035\n",
      "Epoch: 891/1000 training accuracy: 88.55%\n",
      "Epoch: 892/1000 | Batch 000/052 | Cost: 0.2471\n",
      "Epoch: 892/1000 | Batch 010/052 | Cost: 0.4448\n",
      "Epoch: 892/1000 | Batch 020/052 | Cost: 0.3932\n",
      "Epoch: 892/1000 | Batch 030/052 | Cost: 0.1916\n",
      "Epoch: 892/1000 | Batch 040/052 | Cost: 0.1345\n",
      "Epoch: 892/1000 | Batch 050/052 | Cost: 0.0830\n",
      "Epoch: 892/1000 training accuracy: 87.99%\n",
      "Epoch: 893/1000 | Batch 000/052 | Cost: 0.2333\n",
      "Epoch: 893/1000 | Batch 010/052 | Cost: 0.1190\n",
      "Epoch: 893/1000 | Batch 020/052 | Cost: 0.2713\n",
      "Epoch: 893/1000 | Batch 030/052 | Cost: 0.2423\n",
      "Epoch: 893/1000 | Batch 040/052 | Cost: 0.4555\n",
      "Epoch: 893/1000 | Batch 050/052 | Cost: 0.2745\n",
      "Epoch: 893/1000 training accuracy: 89.00%\n",
      "Epoch: 894/1000 | Batch 000/052 | Cost: 0.2781\n",
      "Epoch: 894/1000 | Batch 010/052 | Cost: 0.3487\n",
      "Epoch: 894/1000 | Batch 020/052 | Cost: 0.1325\n",
      "Epoch: 894/1000 | Batch 030/052 | Cost: 0.3536\n",
      "Epoch: 894/1000 | Batch 040/052 | Cost: 0.1172\n",
      "Epoch: 894/1000 | Batch 050/052 | Cost: 0.2345\n",
      "Epoch: 894/1000 training accuracy: 88.89%\n",
      "Epoch: 895/1000 | Batch 000/052 | Cost: 0.1278\n",
      "Epoch: 895/1000 | Batch 010/052 | Cost: 0.2151\n",
      "Epoch: 895/1000 | Batch 020/052 | Cost: 0.1223\n",
      "Epoch: 895/1000 | Batch 030/052 | Cost: 0.1496\n",
      "Epoch: 895/1000 | Batch 040/052 | Cost: 0.1905\n",
      "Epoch: 895/1000 | Batch 050/052 | Cost: 0.2913\n",
      "Epoch: 895/1000 training accuracy: 89.23%\n",
      "Epoch: 896/1000 | Batch 000/052 | Cost: 0.4480\n",
      "Epoch: 896/1000 | Batch 010/052 | Cost: 0.2526\n",
      "Epoch: 896/1000 | Batch 020/052 | Cost: 0.2550\n",
      "Epoch: 896/1000 | Batch 030/052 | Cost: 0.5475\n",
      "Epoch: 896/1000 | Batch 040/052 | Cost: 0.1797\n",
      "Epoch: 896/1000 | Batch 050/052 | Cost: 0.4539\n",
      "Epoch: 896/1000 training accuracy: 87.99%\n",
      "Epoch: 897/1000 | Batch 000/052 | Cost: 0.3431\n",
      "Epoch: 897/1000 | Batch 010/052 | Cost: 0.2180\n",
      "Epoch: 897/1000 | Batch 020/052 | Cost: 0.0761\n",
      "Epoch: 897/1000 | Batch 030/052 | Cost: 0.1817\n",
      "Epoch: 897/1000 | Batch 040/052 | Cost: 0.3211\n",
      "Epoch: 897/1000 | Batch 050/052 | Cost: 0.1944\n",
      "Epoch: 897/1000 training accuracy: 87.65%\n",
      "Epoch: 898/1000 | Batch 000/052 | Cost: 0.1737\n",
      "Epoch: 898/1000 | Batch 010/052 | Cost: 0.0980\n",
      "Epoch: 898/1000 | Batch 020/052 | Cost: 0.3207\n",
      "Epoch: 898/1000 | Batch 030/052 | Cost: 0.1648\n",
      "Epoch: 898/1000 | Batch 040/052 | Cost: 0.3325\n",
      "Epoch: 898/1000 | Batch 050/052 | Cost: 0.1243\n",
      "Epoch: 898/1000 training accuracy: 87.88%\n",
      "Epoch: 899/1000 | Batch 000/052 | Cost: 0.1518\n",
      "Epoch: 899/1000 | Batch 010/052 | Cost: 0.1692\n",
      "Epoch: 899/1000 | Batch 020/052 | Cost: 0.3404\n",
      "Epoch: 899/1000 | Batch 030/052 | Cost: 0.3580\n",
      "Epoch: 899/1000 | Batch 040/052 | Cost: 0.2317\n",
      "Epoch: 899/1000 | Batch 050/052 | Cost: 0.1632\n",
      "Epoch: 899/1000 training accuracy: 88.44%\n",
      "Epoch: 900/1000 | Batch 000/052 | Cost: 0.3982\n",
      "Epoch: 900/1000 | Batch 010/052 | Cost: 0.3334\n",
      "Epoch: 900/1000 | Batch 020/052 | Cost: 0.2294\n",
      "Epoch: 900/1000 | Batch 030/052 | Cost: 0.1507\n",
      "Epoch: 900/1000 | Batch 040/052 | Cost: 0.1325\n",
      "Epoch: 900/1000 | Batch 050/052 | Cost: 0.1908\n",
      "Epoch: 900/1000 training accuracy: 87.54%\n",
      "Epoch: 901/1000 | Batch 000/052 | Cost: 0.2685\n",
      "Epoch: 901/1000 | Batch 010/052 | Cost: 0.1811\n",
      "Epoch: 901/1000 | Batch 020/052 | Cost: 0.3101\n",
      "Epoch: 901/1000 | Batch 030/052 | Cost: 0.1457\n",
      "Epoch: 901/1000 | Batch 040/052 | Cost: 0.4836\n",
      "Epoch: 901/1000 | Batch 050/052 | Cost: 0.3021\n",
      "Epoch: 901/1000 training accuracy: 89.11%\n",
      "Epoch: 902/1000 | Batch 000/052 | Cost: 0.3124\n",
      "Epoch: 902/1000 | Batch 010/052 | Cost: 0.2121\n",
      "Epoch: 902/1000 | Batch 020/052 | Cost: 0.2396\n",
      "Epoch: 902/1000 | Batch 030/052 | Cost: 0.2776\n",
      "Epoch: 902/1000 | Batch 040/052 | Cost: 0.2047\n",
      "Epoch: 902/1000 | Batch 050/052 | Cost: 0.1689\n",
      "Epoch: 902/1000 training accuracy: 87.88%\n",
      "Epoch: 903/1000 | Batch 000/052 | Cost: 0.1512\n",
      "Epoch: 903/1000 | Batch 010/052 | Cost: 0.2336\n",
      "Epoch: 903/1000 | Batch 020/052 | Cost: 0.2754\n",
      "Epoch: 903/1000 | Batch 030/052 | Cost: 0.3283\n",
      "Epoch: 903/1000 | Batch 040/052 | Cost: 0.3790\n",
      "Epoch: 903/1000 | Batch 050/052 | Cost: 0.4441\n",
      "Epoch: 903/1000 training accuracy: 87.99%\n",
      "Epoch: 904/1000 | Batch 000/052 | Cost: 0.3680\n",
      "Epoch: 904/1000 | Batch 010/052 | Cost: 0.1443\n",
      "Epoch: 904/1000 | Batch 020/052 | Cost: 0.2123\n",
      "Epoch: 904/1000 | Batch 030/052 | Cost: 0.2496\n",
      "Epoch: 904/1000 | Batch 040/052 | Cost: 0.3069\n",
      "Epoch: 904/1000 | Batch 050/052 | Cost: 0.1631\n",
      "Epoch: 904/1000 training accuracy: 88.78%\n",
      "Epoch: 905/1000 | Batch 000/052 | Cost: 0.5277\n",
      "Epoch: 905/1000 | Batch 010/052 | Cost: 0.2524\n",
      "Epoch: 905/1000 | Batch 020/052 | Cost: 0.2011\n",
      "Epoch: 905/1000 | Batch 030/052 | Cost: 0.1220\n",
      "Epoch: 905/1000 | Batch 040/052 | Cost: 0.3059\n",
      "Epoch: 905/1000 | Batch 050/052 | Cost: 0.1759\n",
      "Epoch: 905/1000 training accuracy: 88.66%\n",
      "Epoch: 906/1000 | Batch 000/052 | Cost: 0.2235\n",
      "Epoch: 906/1000 | Batch 010/052 | Cost: 0.5360\n",
      "Epoch: 906/1000 | Batch 020/052 | Cost: 0.1999\n",
      "Epoch: 906/1000 | Batch 030/052 | Cost: 0.2478\n",
      "Epoch: 906/1000 | Batch 040/052 | Cost: 0.2914\n",
      "Epoch: 906/1000 | Batch 050/052 | Cost: 0.1909\n",
      "Epoch: 906/1000 training accuracy: 89.67%\n",
      "Epoch: 907/1000 | Batch 000/052 | Cost: 0.5114\n",
      "Epoch: 907/1000 | Batch 010/052 | Cost: 0.3690\n",
      "Epoch: 907/1000 | Batch 020/052 | Cost: 0.4707\n",
      "Epoch: 907/1000 | Batch 030/052 | Cost: 0.2884\n",
      "Epoch: 907/1000 | Batch 040/052 | Cost: 0.2353\n",
      "Epoch: 907/1000 | Batch 050/052 | Cost: 0.2353\n",
      "Epoch: 907/1000 training accuracy: 87.99%\n",
      "Epoch: 908/1000 | Batch 000/052 | Cost: 0.0883\n",
      "Epoch: 908/1000 | Batch 010/052 | Cost: 0.2666\n",
      "Epoch: 908/1000 | Batch 020/052 | Cost: 0.2091\n",
      "Epoch: 908/1000 | Batch 030/052 | Cost: 0.2353\n",
      "Epoch: 908/1000 | Batch 040/052 | Cost: 0.5013\n",
      "Epoch: 908/1000 | Batch 050/052 | Cost: 0.3783\n",
      "Epoch: 908/1000 training accuracy: 89.23%\n",
      "Epoch: 909/1000 | Batch 000/052 | Cost: 0.2475\n",
      "Epoch: 909/1000 | Batch 010/052 | Cost: 0.2802\n",
      "Epoch: 909/1000 | Batch 020/052 | Cost: 0.4034\n",
      "Epoch: 909/1000 | Batch 030/052 | Cost: 0.2038\n",
      "Epoch: 909/1000 | Batch 040/052 | Cost: 0.2411\n",
      "Epoch: 909/1000 | Batch 050/052 | Cost: 0.1649\n",
      "Epoch: 909/1000 training accuracy: 88.66%\n",
      "Epoch: 910/1000 | Batch 000/052 | Cost: 0.1698\n",
      "Epoch: 910/1000 | Batch 010/052 | Cost: 0.2342\n",
      "Epoch: 910/1000 | Batch 020/052 | Cost: 0.3310\n",
      "Epoch: 910/1000 | Batch 030/052 | Cost: 0.1956\n",
      "Epoch: 910/1000 | Batch 040/052 | Cost: 0.2493\n",
      "Epoch: 910/1000 | Batch 050/052 | Cost: 0.4034\n",
      "Epoch: 910/1000 training accuracy: 88.44%\n",
      "Epoch: 911/1000 | Batch 000/052 | Cost: 0.2365\n",
      "Epoch: 911/1000 | Batch 010/052 | Cost: 0.1573\n",
      "Epoch: 911/1000 | Batch 020/052 | Cost: 0.2038\n",
      "Epoch: 911/1000 | Batch 030/052 | Cost: 0.6864\n",
      "Epoch: 911/1000 | Batch 040/052 | Cost: 0.1179\n",
      "Epoch: 911/1000 | Batch 050/052 | Cost: 0.1329\n",
      "Epoch: 911/1000 training accuracy: 87.99%\n",
      "Epoch: 912/1000 | Batch 000/052 | Cost: 0.0826\n",
      "Epoch: 912/1000 | Batch 010/052 | Cost: 0.1245\n",
      "Epoch: 912/1000 | Batch 020/052 | Cost: 0.1925\n",
      "Epoch: 912/1000 | Batch 030/052 | Cost: 0.3912\n",
      "Epoch: 912/1000 | Batch 040/052 | Cost: 0.2773\n",
      "Epoch: 912/1000 | Batch 050/052 | Cost: 0.2377\n",
      "Epoch: 912/1000 training accuracy: 87.88%\n",
      "Epoch: 913/1000 | Batch 000/052 | Cost: 0.4342\n",
      "Epoch: 913/1000 | Batch 010/052 | Cost: 0.2712\n",
      "Epoch: 913/1000 | Batch 020/052 | Cost: 0.2088\n",
      "Epoch: 913/1000 | Batch 030/052 | Cost: 0.2161\n",
      "Epoch: 913/1000 | Batch 040/052 | Cost: 0.2457\n",
      "Epoch: 913/1000 | Batch 050/052 | Cost: 0.4324\n",
      "Epoch: 913/1000 training accuracy: 88.89%\n",
      "Epoch: 914/1000 | Batch 000/052 | Cost: 0.2533\n",
      "Epoch: 914/1000 | Batch 010/052 | Cost: 0.5103\n",
      "Epoch: 914/1000 | Batch 020/052 | Cost: 0.2030\n",
      "Epoch: 914/1000 | Batch 030/052 | Cost: 0.1400\n",
      "Epoch: 914/1000 | Batch 040/052 | Cost: 0.3448\n",
      "Epoch: 914/1000 | Batch 050/052 | Cost: 0.2740\n",
      "Epoch: 914/1000 training accuracy: 88.33%\n",
      "Epoch: 915/1000 | Batch 000/052 | Cost: 0.1777\n",
      "Epoch: 915/1000 | Batch 010/052 | Cost: 0.2104\n",
      "Epoch: 915/1000 | Batch 020/052 | Cost: 0.3743\n",
      "Epoch: 915/1000 | Batch 030/052 | Cost: 0.4812\n",
      "Epoch: 915/1000 | Batch 040/052 | Cost: 0.1733\n",
      "Epoch: 915/1000 | Batch 050/052 | Cost: 0.2291\n",
      "Epoch: 915/1000 training accuracy: 88.44%\n",
      "Epoch: 916/1000 | Batch 000/052 | Cost: 0.3597\n",
      "Epoch: 916/1000 | Batch 010/052 | Cost: 0.2965\n",
      "Epoch: 916/1000 | Batch 020/052 | Cost: 0.2724\n",
      "Epoch: 916/1000 | Batch 030/052 | Cost: 0.4964\n",
      "Epoch: 916/1000 | Batch 040/052 | Cost: 0.2344\n",
      "Epoch: 916/1000 | Batch 050/052 | Cost: 0.2832\n",
      "Epoch: 916/1000 training accuracy: 88.55%\n",
      "Epoch: 917/1000 | Batch 000/052 | Cost: 0.2929\n",
      "Epoch: 917/1000 | Batch 010/052 | Cost: 0.2299\n",
      "Epoch: 917/1000 | Batch 020/052 | Cost: 0.1740\n",
      "Epoch: 917/1000 | Batch 030/052 | Cost: 0.4015\n",
      "Epoch: 917/1000 | Batch 040/052 | Cost: 0.1650\n",
      "Epoch: 917/1000 | Batch 050/052 | Cost: 0.5192\n",
      "Epoch: 917/1000 training accuracy: 88.44%\n",
      "Epoch: 918/1000 | Batch 000/052 | Cost: 0.2959\n",
      "Epoch: 918/1000 | Batch 010/052 | Cost: 0.2077\n",
      "Epoch: 918/1000 | Batch 020/052 | Cost: 0.3670\n",
      "Epoch: 918/1000 | Batch 030/052 | Cost: 0.2284\n",
      "Epoch: 918/1000 | Batch 040/052 | Cost: 0.1419\n",
      "Epoch: 918/1000 | Batch 050/052 | Cost: 0.1061\n",
      "Epoch: 918/1000 training accuracy: 89.23%\n",
      "Epoch: 919/1000 | Batch 000/052 | Cost: 0.1440\n",
      "Epoch: 919/1000 | Batch 010/052 | Cost: 0.4108\n",
      "Epoch: 919/1000 | Batch 020/052 | Cost: 0.4666\n",
      "Epoch: 919/1000 | Batch 030/052 | Cost: 0.2543\n",
      "Epoch: 919/1000 | Batch 040/052 | Cost: 0.2483\n",
      "Epoch: 919/1000 | Batch 050/052 | Cost: 0.5357\n",
      "Epoch: 919/1000 training accuracy: 87.54%\n",
      "Epoch: 920/1000 | Batch 000/052 | Cost: 0.2118\n",
      "Epoch: 920/1000 | Batch 010/052 | Cost: 0.3727\n",
      "Epoch: 920/1000 | Batch 020/052 | Cost: 0.1485\n",
      "Epoch: 920/1000 | Batch 030/052 | Cost: 0.2104\n",
      "Epoch: 920/1000 | Batch 040/052 | Cost: 0.5250\n",
      "Epoch: 920/1000 | Batch 050/052 | Cost: 0.3776\n",
      "Epoch: 920/1000 training accuracy: 88.44%\n",
      "Epoch: 921/1000 | Batch 000/052 | Cost: 0.2335\n",
      "Epoch: 921/1000 | Batch 010/052 | Cost: 0.1129\n",
      "Epoch: 921/1000 | Batch 020/052 | Cost: 0.2716\n",
      "Epoch: 921/1000 | Batch 030/052 | Cost: 0.4516\n",
      "Epoch: 921/1000 | Batch 040/052 | Cost: 0.0866\n",
      "Epoch: 921/1000 | Batch 050/052 | Cost: 0.2818\n",
      "Epoch: 921/1000 training accuracy: 87.99%\n",
      "Epoch: 922/1000 | Batch 000/052 | Cost: 0.2586\n",
      "Epoch: 922/1000 | Batch 010/052 | Cost: 0.4012\n",
      "Epoch: 922/1000 | Batch 020/052 | Cost: 0.0525\n",
      "Epoch: 922/1000 | Batch 030/052 | Cost: 0.2121\n",
      "Epoch: 922/1000 | Batch 040/052 | Cost: 0.1272\n",
      "Epoch: 922/1000 | Batch 050/052 | Cost: 0.1106\n",
      "Epoch: 922/1000 training accuracy: 87.54%\n",
      "Epoch: 923/1000 | Batch 000/052 | Cost: 0.1022\n",
      "Epoch: 923/1000 | Batch 010/052 | Cost: 0.2681\n",
      "Epoch: 923/1000 | Batch 020/052 | Cost: 0.2480\n",
      "Epoch: 923/1000 | Batch 030/052 | Cost: 0.2377\n",
      "Epoch: 923/1000 | Batch 040/052 | Cost: 0.3017\n",
      "Epoch: 923/1000 | Batch 050/052 | Cost: 0.1801\n",
      "Epoch: 923/1000 training accuracy: 87.77%\n",
      "Epoch: 924/1000 | Batch 000/052 | Cost: 0.1282\n",
      "Epoch: 924/1000 | Batch 010/052 | Cost: 0.1618\n",
      "Epoch: 924/1000 | Batch 020/052 | Cost: 0.2046\n",
      "Epoch: 924/1000 | Batch 030/052 | Cost: 0.1537\n",
      "Epoch: 924/1000 | Batch 040/052 | Cost: 0.0925\n",
      "Epoch: 924/1000 | Batch 050/052 | Cost: 0.3866\n",
      "Epoch: 924/1000 training accuracy: 87.99%\n",
      "Epoch: 925/1000 | Batch 000/052 | Cost: 0.2388\n",
      "Epoch: 925/1000 | Batch 010/052 | Cost: 0.2161\n",
      "Epoch: 925/1000 | Batch 020/052 | Cost: 0.2212\n",
      "Epoch: 925/1000 | Batch 030/052 | Cost: 0.1579\n",
      "Epoch: 925/1000 | Batch 040/052 | Cost: 0.1480\n",
      "Epoch: 925/1000 | Batch 050/052 | Cost: 0.2830\n",
      "Epoch: 925/1000 training accuracy: 88.44%\n",
      "Epoch: 926/1000 | Batch 000/052 | Cost: 0.3174\n",
      "Epoch: 926/1000 | Batch 010/052 | Cost: 0.1455\n",
      "Epoch: 926/1000 | Batch 020/052 | Cost: 0.2199\n",
      "Epoch: 926/1000 | Batch 030/052 | Cost: 0.1864\n",
      "Epoch: 926/1000 | Batch 040/052 | Cost: 0.1164\n",
      "Epoch: 926/1000 | Batch 050/052 | Cost: 0.2819\n",
      "Epoch: 926/1000 training accuracy: 88.33%\n",
      "Epoch: 927/1000 | Batch 000/052 | Cost: 0.1791\n",
      "Epoch: 927/1000 | Batch 010/052 | Cost: 0.1702\n",
      "Epoch: 927/1000 | Batch 020/052 | Cost: 0.0926\n",
      "Epoch: 927/1000 | Batch 030/052 | Cost: 0.2586\n",
      "Epoch: 927/1000 | Batch 040/052 | Cost: 0.0807\n",
      "Epoch: 927/1000 | Batch 050/052 | Cost: 0.2363\n",
      "Epoch: 927/1000 training accuracy: 89.00%\n",
      "Epoch: 928/1000 | Batch 000/052 | Cost: 0.5020\n",
      "Epoch: 928/1000 | Batch 010/052 | Cost: 0.1261\n",
      "Epoch: 928/1000 | Batch 020/052 | Cost: 0.2265\n",
      "Epoch: 928/1000 | Batch 030/052 | Cost: 0.1211\n",
      "Epoch: 928/1000 | Batch 040/052 | Cost: 0.3240\n",
      "Epoch: 928/1000 | Batch 050/052 | Cost: 0.3067\n",
      "Epoch: 928/1000 training accuracy: 89.00%\n",
      "Epoch: 929/1000 | Batch 000/052 | Cost: 0.3271\n",
      "Epoch: 929/1000 | Batch 010/052 | Cost: 0.0914\n",
      "Epoch: 929/1000 | Batch 020/052 | Cost: 0.2698\n",
      "Epoch: 929/1000 | Batch 030/052 | Cost: 0.4410\n",
      "Epoch: 929/1000 | Batch 040/052 | Cost: 0.4482\n",
      "Epoch: 929/1000 | Batch 050/052 | Cost: 0.2123\n",
      "Epoch: 929/1000 training accuracy: 88.44%\n",
      "Epoch: 930/1000 | Batch 000/052 | Cost: 0.3745\n",
      "Epoch: 930/1000 | Batch 010/052 | Cost: 0.3539\n",
      "Epoch: 930/1000 | Batch 020/052 | Cost: 0.5377\n",
      "Epoch: 930/1000 | Batch 030/052 | Cost: 0.4017\n",
      "Epoch: 930/1000 | Batch 040/052 | Cost: 0.2136\n",
      "Epoch: 930/1000 | Batch 050/052 | Cost: 0.1246\n",
      "Epoch: 930/1000 training accuracy: 89.11%\n",
      "Epoch: 931/1000 | Batch 000/052 | Cost: 0.1904\n",
      "Epoch: 931/1000 | Batch 010/052 | Cost: 0.5424\n",
      "Epoch: 931/1000 | Batch 020/052 | Cost: 0.2649\n",
      "Epoch: 931/1000 | Batch 030/052 | Cost: 0.1224\n",
      "Epoch: 931/1000 | Batch 040/052 | Cost: 0.1978\n",
      "Epoch: 931/1000 | Batch 050/052 | Cost: 0.2560\n",
      "Epoch: 931/1000 training accuracy: 87.99%\n",
      "Epoch: 932/1000 | Batch 000/052 | Cost: 0.2788\n",
      "Epoch: 932/1000 | Batch 010/052 | Cost: 0.2691\n",
      "Epoch: 932/1000 | Batch 020/052 | Cost: 0.3777\n",
      "Epoch: 932/1000 | Batch 030/052 | Cost: 0.1618\n",
      "Epoch: 932/1000 | Batch 040/052 | Cost: 0.2428\n",
      "Epoch: 932/1000 | Batch 050/052 | Cost: 0.1807\n",
      "Epoch: 932/1000 training accuracy: 88.66%\n",
      "Epoch: 933/1000 | Batch 000/052 | Cost: 0.6965\n",
      "Epoch: 933/1000 | Batch 010/052 | Cost: 0.2044\n",
      "Epoch: 933/1000 | Batch 020/052 | Cost: 0.4615\n",
      "Epoch: 933/1000 | Batch 030/052 | Cost: 0.2334\n",
      "Epoch: 933/1000 | Batch 040/052 | Cost: 0.4600\n",
      "Epoch: 933/1000 | Batch 050/052 | Cost: 0.2689\n",
      "Epoch: 933/1000 training accuracy: 88.33%\n",
      "Epoch: 934/1000 | Batch 000/052 | Cost: 0.4059\n",
      "Epoch: 934/1000 | Batch 010/052 | Cost: 0.1496\n",
      "Epoch: 934/1000 | Batch 020/052 | Cost: 0.3043\n",
      "Epoch: 934/1000 | Batch 030/052 | Cost: 0.3775\n",
      "Epoch: 934/1000 | Batch 040/052 | Cost: 0.0999\n",
      "Epoch: 934/1000 | Batch 050/052 | Cost: 0.3318\n",
      "Epoch: 934/1000 training accuracy: 89.23%\n",
      "Epoch: 935/1000 | Batch 000/052 | Cost: 0.3742\n",
      "Epoch: 935/1000 | Batch 010/052 | Cost: 0.0713\n",
      "Epoch: 935/1000 | Batch 020/052 | Cost: 0.3869\n",
      "Epoch: 935/1000 | Batch 030/052 | Cost: 0.2635\n",
      "Epoch: 935/1000 | Batch 040/052 | Cost: 0.2941\n",
      "Epoch: 935/1000 | Batch 050/052 | Cost: 0.0547\n",
      "Epoch: 935/1000 training accuracy: 88.33%\n",
      "Epoch: 936/1000 | Batch 000/052 | Cost: 0.6295\n",
      "Epoch: 936/1000 | Batch 010/052 | Cost: 0.4505\n",
      "Epoch: 936/1000 | Batch 020/052 | Cost: 0.1678\n",
      "Epoch: 936/1000 | Batch 030/052 | Cost: 0.2555\n",
      "Epoch: 936/1000 | Batch 040/052 | Cost: 0.2777\n",
      "Epoch: 936/1000 | Batch 050/052 | Cost: 0.1153\n",
      "Epoch: 936/1000 training accuracy: 89.00%\n",
      "Epoch: 937/1000 | Batch 000/052 | Cost: 0.6836\n",
      "Epoch: 937/1000 | Batch 010/052 | Cost: 0.3602\n",
      "Epoch: 937/1000 | Batch 020/052 | Cost: 0.1286\n",
      "Epoch: 937/1000 | Batch 030/052 | Cost: 0.2088\n",
      "Epoch: 937/1000 | Batch 040/052 | Cost: 0.3549\n",
      "Epoch: 937/1000 | Batch 050/052 | Cost: 0.1357\n",
      "Epoch: 937/1000 training accuracy: 88.10%\n",
      "Epoch: 938/1000 | Batch 000/052 | Cost: 0.1410\n",
      "Epoch: 938/1000 | Batch 010/052 | Cost: 0.1826\n",
      "Epoch: 938/1000 | Batch 020/052 | Cost: 0.0863\n",
      "Epoch: 938/1000 | Batch 030/052 | Cost: 0.2525\n",
      "Epoch: 938/1000 | Batch 040/052 | Cost: 0.4691\n",
      "Epoch: 938/1000 | Batch 050/052 | Cost: 0.3957\n",
      "Epoch: 938/1000 training accuracy: 88.55%\n",
      "Epoch: 939/1000 | Batch 000/052 | Cost: 0.3856\n",
      "Epoch: 939/1000 | Batch 010/052 | Cost: 0.3563\n",
      "Epoch: 939/1000 | Batch 020/052 | Cost: 0.2332\n",
      "Epoch: 939/1000 | Batch 030/052 | Cost: 0.2256\n",
      "Epoch: 939/1000 | Batch 040/052 | Cost: 0.2659\n",
      "Epoch: 939/1000 | Batch 050/052 | Cost: 0.1198\n",
      "Epoch: 939/1000 training accuracy: 87.65%\n",
      "Epoch: 940/1000 | Batch 000/052 | Cost: 0.2347\n",
      "Epoch: 940/1000 | Batch 010/052 | Cost: 0.1852\n",
      "Epoch: 940/1000 | Batch 020/052 | Cost: 0.1804\n",
      "Epoch: 940/1000 | Batch 030/052 | Cost: 0.0746\n",
      "Epoch: 940/1000 | Batch 040/052 | Cost: 0.3461\n",
      "Epoch: 940/1000 | Batch 050/052 | Cost: 0.4165\n",
      "Epoch: 940/1000 training accuracy: 88.89%\n",
      "Epoch: 941/1000 | Batch 000/052 | Cost: 0.1274\n",
      "Epoch: 941/1000 | Batch 010/052 | Cost: 0.1512\n",
      "Epoch: 941/1000 | Batch 020/052 | Cost: 0.1030\n",
      "Epoch: 941/1000 | Batch 030/052 | Cost: 0.2320\n",
      "Epoch: 941/1000 | Batch 040/052 | Cost: 0.0850\n",
      "Epoch: 941/1000 | Batch 050/052 | Cost: 0.1410\n",
      "Epoch: 941/1000 training accuracy: 88.89%\n",
      "Epoch: 942/1000 | Batch 000/052 | Cost: 0.0793\n",
      "Epoch: 942/1000 | Batch 010/052 | Cost: 0.0874\n",
      "Epoch: 942/1000 | Batch 020/052 | Cost: 0.1386\n",
      "Epoch: 942/1000 | Batch 030/052 | Cost: 0.3011\n",
      "Epoch: 942/1000 | Batch 040/052 | Cost: 0.3049\n",
      "Epoch: 942/1000 | Batch 050/052 | Cost: 0.1400\n",
      "Epoch: 942/1000 training accuracy: 88.10%\n",
      "Epoch: 943/1000 | Batch 000/052 | Cost: 0.3269\n",
      "Epoch: 943/1000 | Batch 010/052 | Cost: 0.3439\n",
      "Epoch: 943/1000 | Batch 020/052 | Cost: 0.4870\n",
      "Epoch: 943/1000 | Batch 030/052 | Cost: 0.2373\n",
      "Epoch: 943/1000 | Batch 040/052 | Cost: 0.2646\n",
      "Epoch: 943/1000 | Batch 050/052 | Cost: 0.1684\n",
      "Epoch: 943/1000 training accuracy: 88.22%\n",
      "Epoch: 944/1000 | Batch 000/052 | Cost: 0.0844\n",
      "Epoch: 944/1000 | Batch 010/052 | Cost: 0.3391\n",
      "Epoch: 944/1000 | Batch 020/052 | Cost: 0.2455\n",
      "Epoch: 944/1000 | Batch 030/052 | Cost: 0.1403\n",
      "Epoch: 944/1000 | Batch 040/052 | Cost: 0.3557\n",
      "Epoch: 944/1000 | Batch 050/052 | Cost: 0.3492\n",
      "Epoch: 944/1000 training accuracy: 88.10%\n",
      "Epoch: 945/1000 | Batch 000/052 | Cost: 0.0821\n",
      "Epoch: 945/1000 | Batch 010/052 | Cost: 0.2928\n",
      "Epoch: 945/1000 | Batch 020/052 | Cost: 0.1381\n",
      "Epoch: 945/1000 | Batch 030/052 | Cost: 0.4249\n",
      "Epoch: 945/1000 | Batch 040/052 | Cost: 0.4192\n",
      "Epoch: 945/1000 | Batch 050/052 | Cost: 0.2070\n",
      "Epoch: 945/1000 training accuracy: 88.89%\n",
      "Epoch: 946/1000 | Batch 000/052 | Cost: 0.3055\n",
      "Epoch: 946/1000 | Batch 010/052 | Cost: 0.4431\n",
      "Epoch: 946/1000 | Batch 020/052 | Cost: 0.1941\n",
      "Epoch: 946/1000 | Batch 030/052 | Cost: 0.1348\n",
      "Epoch: 946/1000 | Batch 040/052 | Cost: 0.3167\n",
      "Epoch: 946/1000 | Batch 050/052 | Cost: 0.1436\n",
      "Epoch: 946/1000 training accuracy: 88.10%\n",
      "Epoch: 947/1000 | Batch 000/052 | Cost: 0.1556\n",
      "Epoch: 947/1000 | Batch 010/052 | Cost: 0.3247\n",
      "Epoch: 947/1000 | Batch 020/052 | Cost: 0.3789\n",
      "Epoch: 947/1000 | Batch 030/052 | Cost: 0.3002\n",
      "Epoch: 947/1000 | Batch 040/052 | Cost: 0.2006\n",
      "Epoch: 947/1000 | Batch 050/052 | Cost: 0.3415\n",
      "Epoch: 947/1000 training accuracy: 88.44%\n",
      "Epoch: 948/1000 | Batch 000/052 | Cost: 0.2054\n",
      "Epoch: 948/1000 | Batch 010/052 | Cost: 0.2583\n",
      "Epoch: 948/1000 | Batch 020/052 | Cost: 0.1301\n",
      "Epoch: 948/1000 | Batch 030/052 | Cost: 0.2164\n",
      "Epoch: 948/1000 | Batch 040/052 | Cost: 0.3673\n",
      "Epoch: 948/1000 | Batch 050/052 | Cost: 0.4783\n",
      "Epoch: 948/1000 training accuracy: 89.00%\n",
      "Epoch: 949/1000 | Batch 000/052 | Cost: 0.1355\n",
      "Epoch: 949/1000 | Batch 010/052 | Cost: 0.1234\n",
      "Epoch: 949/1000 | Batch 020/052 | Cost: 0.3833\n",
      "Epoch: 949/1000 | Batch 030/052 | Cost: 0.1487\n",
      "Epoch: 949/1000 | Batch 040/052 | Cost: 0.4933\n",
      "Epoch: 949/1000 | Batch 050/052 | Cost: 0.5872\n",
      "Epoch: 949/1000 training accuracy: 88.22%\n",
      "Epoch: 950/1000 | Batch 000/052 | Cost: 0.1054\n",
      "Epoch: 950/1000 | Batch 010/052 | Cost: 0.2835\n",
      "Epoch: 950/1000 | Batch 020/052 | Cost: 0.0950\n",
      "Epoch: 950/1000 | Batch 030/052 | Cost: 0.1935\n",
      "Epoch: 950/1000 | Batch 040/052 | Cost: 0.5695\n",
      "Epoch: 950/1000 | Batch 050/052 | Cost: 0.1426\n",
      "Epoch: 950/1000 training accuracy: 88.44%\n",
      "Epoch: 951/1000 | Batch 000/052 | Cost: 0.2543\n",
      "Epoch: 951/1000 | Batch 010/052 | Cost: 0.1915\n",
      "Epoch: 951/1000 | Batch 020/052 | Cost: 0.2897\n",
      "Epoch: 951/1000 | Batch 030/052 | Cost: 0.1913\n",
      "Epoch: 951/1000 | Batch 040/052 | Cost: 0.3425\n",
      "Epoch: 951/1000 | Batch 050/052 | Cost: 0.3826\n",
      "Epoch: 951/1000 training accuracy: 88.66%\n",
      "Epoch: 952/1000 | Batch 000/052 | Cost: 0.1932\n",
      "Epoch: 952/1000 | Batch 010/052 | Cost: 0.5617\n",
      "Epoch: 952/1000 | Batch 020/052 | Cost: 0.3086\n",
      "Epoch: 952/1000 | Batch 030/052 | Cost: 0.2779\n",
      "Epoch: 952/1000 | Batch 040/052 | Cost: 0.2582\n",
      "Epoch: 952/1000 | Batch 050/052 | Cost: 0.0597\n",
      "Epoch: 952/1000 training accuracy: 87.77%\n",
      "Epoch: 953/1000 | Batch 000/052 | Cost: 0.0730\n",
      "Epoch: 953/1000 | Batch 010/052 | Cost: 0.2101\n",
      "Epoch: 953/1000 | Batch 020/052 | Cost: 0.2402\n",
      "Epoch: 953/1000 | Batch 030/052 | Cost: 0.1611\n",
      "Epoch: 953/1000 | Batch 040/052 | Cost: 0.1741\n",
      "Epoch: 953/1000 | Batch 050/052 | Cost: 0.2603\n",
      "Epoch: 953/1000 training accuracy: 88.22%\n",
      "Epoch: 954/1000 | Batch 000/052 | Cost: 0.3456\n",
      "Epoch: 954/1000 | Batch 010/052 | Cost: 0.3988\n",
      "Epoch: 954/1000 | Batch 020/052 | Cost: 0.4697\n",
      "Epoch: 954/1000 | Batch 030/052 | Cost: 0.2525\n",
      "Epoch: 954/1000 | Batch 040/052 | Cost: 0.2065\n",
      "Epoch: 954/1000 | Batch 050/052 | Cost: 0.1244\n",
      "Epoch: 954/1000 training accuracy: 89.00%\n",
      "Epoch: 955/1000 | Batch 000/052 | Cost: 0.1898\n",
      "Epoch: 955/1000 | Batch 010/052 | Cost: 0.1911\n",
      "Epoch: 955/1000 | Batch 020/052 | Cost: 0.1140\n",
      "Epoch: 955/1000 | Batch 030/052 | Cost: 0.0953\n",
      "Epoch: 955/1000 | Batch 040/052 | Cost: 0.4962\n",
      "Epoch: 955/1000 | Batch 050/052 | Cost: 0.2358\n",
      "Epoch: 955/1000 training accuracy: 88.89%\n",
      "Epoch: 956/1000 | Batch 000/052 | Cost: 0.1845\n",
      "Epoch: 956/1000 | Batch 010/052 | Cost: 0.2967\n",
      "Epoch: 956/1000 | Batch 020/052 | Cost: 0.1889\n",
      "Epoch: 956/1000 | Batch 030/052 | Cost: 0.2115\n",
      "Epoch: 956/1000 | Batch 040/052 | Cost: 0.1305\n",
      "Epoch: 956/1000 | Batch 050/052 | Cost: 0.4150\n",
      "Epoch: 956/1000 training accuracy: 88.66%\n",
      "Epoch: 957/1000 | Batch 000/052 | Cost: 0.2291\n",
      "Epoch: 957/1000 | Batch 010/052 | Cost: 0.0771\n",
      "Epoch: 957/1000 | Batch 020/052 | Cost: 0.2350\n",
      "Epoch: 957/1000 | Batch 030/052 | Cost: 0.4231\n",
      "Epoch: 957/1000 | Batch 040/052 | Cost: 0.2031\n",
      "Epoch: 957/1000 | Batch 050/052 | Cost: 0.1691\n",
      "Epoch: 957/1000 training accuracy: 88.66%\n",
      "Epoch: 958/1000 | Batch 000/052 | Cost: 0.4074\n",
      "Epoch: 958/1000 | Batch 010/052 | Cost: 0.2675\n",
      "Epoch: 958/1000 | Batch 020/052 | Cost: 0.4369\n",
      "Epoch: 958/1000 | Batch 030/052 | Cost: 0.3331\n",
      "Epoch: 958/1000 | Batch 040/052 | Cost: 0.2900\n",
      "Epoch: 958/1000 | Batch 050/052 | Cost: 0.3002\n",
      "Epoch: 958/1000 training accuracy: 88.78%\n",
      "Epoch: 959/1000 | Batch 000/052 | Cost: 0.2425\n",
      "Epoch: 959/1000 | Batch 010/052 | Cost: 0.1557\n",
      "Epoch: 959/1000 | Batch 020/052 | Cost: 0.1553\n",
      "Epoch: 959/1000 | Batch 030/052 | Cost: 0.3263\n",
      "Epoch: 959/1000 | Batch 040/052 | Cost: 0.1770\n",
      "Epoch: 959/1000 | Batch 050/052 | Cost: 0.2162\n",
      "Epoch: 959/1000 training accuracy: 88.66%\n",
      "Epoch: 960/1000 | Batch 000/052 | Cost: 0.2184\n",
      "Epoch: 960/1000 | Batch 010/052 | Cost: 0.2298\n",
      "Epoch: 960/1000 | Batch 020/052 | Cost: 0.2661\n",
      "Epoch: 960/1000 | Batch 030/052 | Cost: 0.1152\n",
      "Epoch: 960/1000 | Batch 040/052 | Cost: 0.3169\n",
      "Epoch: 960/1000 | Batch 050/052 | Cost: 0.3457\n",
      "Epoch: 960/1000 training accuracy: 88.22%\n",
      "Epoch: 961/1000 | Batch 000/052 | Cost: 0.1608\n",
      "Epoch: 961/1000 | Batch 010/052 | Cost: 0.2643\n",
      "Epoch: 961/1000 | Batch 020/052 | Cost: 0.2259\n",
      "Epoch: 961/1000 | Batch 030/052 | Cost: 0.3291\n",
      "Epoch: 961/1000 | Batch 040/052 | Cost: 0.3469\n",
      "Epoch: 961/1000 | Batch 050/052 | Cost: 0.5975\n",
      "Epoch: 961/1000 training accuracy: 88.66%\n",
      "Epoch: 962/1000 | Batch 000/052 | Cost: 0.2153\n",
      "Epoch: 962/1000 | Batch 010/052 | Cost: 0.3063\n",
      "Epoch: 962/1000 | Batch 020/052 | Cost: 0.3721\n",
      "Epoch: 962/1000 | Batch 030/052 | Cost: 0.1877\n",
      "Epoch: 962/1000 | Batch 040/052 | Cost: 0.2509\n",
      "Epoch: 962/1000 | Batch 050/052 | Cost: 0.1286\n",
      "Epoch: 962/1000 training accuracy: 87.54%\n",
      "Epoch: 963/1000 | Batch 000/052 | Cost: 0.2490\n",
      "Epoch: 963/1000 | Batch 010/052 | Cost: 0.3023\n",
      "Epoch: 963/1000 | Batch 020/052 | Cost: 0.1522\n",
      "Epoch: 963/1000 | Batch 030/052 | Cost: 0.1416\n",
      "Epoch: 963/1000 | Batch 040/052 | Cost: 0.2108\n",
      "Epoch: 963/1000 | Batch 050/052 | Cost: 0.2893\n",
      "Epoch: 963/1000 training accuracy: 88.55%\n",
      "Epoch: 964/1000 | Batch 000/052 | Cost: 0.1759\n",
      "Epoch: 964/1000 | Batch 010/052 | Cost: 0.2469\n",
      "Epoch: 964/1000 | Batch 020/052 | Cost: 0.3630\n",
      "Epoch: 964/1000 | Batch 030/052 | Cost: 0.4502\n",
      "Epoch: 964/1000 | Batch 040/052 | Cost: 0.0653\n",
      "Epoch: 964/1000 | Batch 050/052 | Cost: 0.3380\n",
      "Epoch: 964/1000 training accuracy: 88.22%\n",
      "Epoch: 965/1000 | Batch 000/052 | Cost: 0.1713\n",
      "Epoch: 965/1000 | Batch 010/052 | Cost: 0.4777\n",
      "Epoch: 965/1000 | Batch 020/052 | Cost: 0.0990\n",
      "Epoch: 965/1000 | Batch 030/052 | Cost: 0.2298\n",
      "Epoch: 965/1000 | Batch 040/052 | Cost: 0.3268\n",
      "Epoch: 965/1000 | Batch 050/052 | Cost: 0.2735\n",
      "Epoch: 965/1000 training accuracy: 88.33%\n",
      "Epoch: 966/1000 | Batch 000/052 | Cost: 0.3721\n",
      "Epoch: 966/1000 | Batch 010/052 | Cost: 0.1092\n",
      "Epoch: 966/1000 | Batch 020/052 | Cost: 0.3494\n",
      "Epoch: 966/1000 | Batch 030/052 | Cost: 0.1854\n",
      "Epoch: 966/1000 | Batch 040/052 | Cost: 0.2698\n",
      "Epoch: 966/1000 | Batch 050/052 | Cost: 0.1627\n",
      "Epoch: 966/1000 training accuracy: 87.77%\n",
      "Epoch: 967/1000 | Batch 000/052 | Cost: 0.3066\n",
      "Epoch: 967/1000 | Batch 010/052 | Cost: 0.1429\n",
      "Epoch: 967/1000 | Batch 020/052 | Cost: 0.1065\n",
      "Epoch: 967/1000 | Batch 030/052 | Cost: 0.3948\n",
      "Epoch: 967/1000 | Batch 040/052 | Cost: 0.2365\n",
      "Epoch: 967/1000 | Batch 050/052 | Cost: 0.1873\n",
      "Epoch: 967/1000 training accuracy: 88.55%\n",
      "Epoch: 968/1000 | Batch 000/052 | Cost: 0.3837\n",
      "Epoch: 968/1000 | Batch 010/052 | Cost: 0.2828\n",
      "Epoch: 968/1000 | Batch 020/052 | Cost: 0.1705\n",
      "Epoch: 968/1000 | Batch 030/052 | Cost: 0.3497\n",
      "Epoch: 968/1000 | Batch 040/052 | Cost: 0.1937\n",
      "Epoch: 968/1000 | Batch 050/052 | Cost: 0.1982\n",
      "Epoch: 968/1000 training accuracy: 88.33%\n",
      "Epoch: 969/1000 | Batch 000/052 | Cost: 0.2988\n",
      "Epoch: 969/1000 | Batch 010/052 | Cost: 0.1935\n",
      "Epoch: 969/1000 | Batch 020/052 | Cost: 0.1880\n",
      "Epoch: 969/1000 | Batch 030/052 | Cost: 0.1081\n",
      "Epoch: 969/1000 | Batch 040/052 | Cost: 0.4899\n",
      "Epoch: 969/1000 | Batch 050/052 | Cost: 0.1648\n",
      "Epoch: 969/1000 training accuracy: 89.00%\n",
      "Epoch: 970/1000 | Batch 000/052 | Cost: 0.1378\n",
      "Epoch: 970/1000 | Batch 010/052 | Cost: 0.3183\n",
      "Epoch: 970/1000 | Batch 020/052 | Cost: 0.3014\n",
      "Epoch: 970/1000 | Batch 030/052 | Cost: 0.3929\n",
      "Epoch: 970/1000 | Batch 040/052 | Cost: 0.3220\n",
      "Epoch: 970/1000 | Batch 050/052 | Cost: 0.1480\n",
      "Epoch: 970/1000 training accuracy: 89.45%\n",
      "Epoch: 971/1000 | Batch 000/052 | Cost: 0.1040\n",
      "Epoch: 971/1000 | Batch 010/052 | Cost: 0.2676\n",
      "Epoch: 971/1000 | Batch 020/052 | Cost: 0.5935\n",
      "Epoch: 971/1000 | Batch 030/052 | Cost: 0.2536\n",
      "Epoch: 971/1000 | Batch 040/052 | Cost: 0.2732\n",
      "Epoch: 971/1000 | Batch 050/052 | Cost: 0.2435\n",
      "Epoch: 971/1000 training accuracy: 88.22%\n",
      "Epoch: 972/1000 | Batch 000/052 | Cost: 0.4094\n",
      "Epoch: 972/1000 | Batch 010/052 | Cost: 0.3541\n",
      "Epoch: 972/1000 | Batch 020/052 | Cost: 0.2849\n",
      "Epoch: 972/1000 | Batch 030/052 | Cost: 0.4540\n",
      "Epoch: 972/1000 | Batch 040/052 | Cost: 0.2106\n",
      "Epoch: 972/1000 | Batch 050/052 | Cost: 0.1268\n",
      "Epoch: 972/1000 training accuracy: 88.22%\n",
      "Epoch: 973/1000 | Batch 000/052 | Cost: 0.6088\n",
      "Epoch: 973/1000 | Batch 010/052 | Cost: 0.0817\n",
      "Epoch: 973/1000 | Batch 020/052 | Cost: 0.1869\n",
      "Epoch: 973/1000 | Batch 030/052 | Cost: 0.2164\n",
      "Epoch: 973/1000 | Batch 040/052 | Cost: 0.3744\n",
      "Epoch: 973/1000 | Batch 050/052 | Cost: 0.1554\n",
      "Epoch: 973/1000 training accuracy: 89.00%\n",
      "Epoch: 974/1000 | Batch 000/052 | Cost: 0.4281\n",
      "Epoch: 974/1000 | Batch 010/052 | Cost: 0.2334\n",
      "Epoch: 974/1000 | Batch 020/052 | Cost: 0.0934\n",
      "Epoch: 974/1000 | Batch 030/052 | Cost: 0.1325\n",
      "Epoch: 974/1000 | Batch 040/052 | Cost: 0.2964\n",
      "Epoch: 974/1000 | Batch 050/052 | Cost: 0.3911\n",
      "Epoch: 974/1000 training accuracy: 88.66%\n",
      "Epoch: 975/1000 | Batch 000/052 | Cost: 0.4172\n",
      "Epoch: 975/1000 | Batch 010/052 | Cost: 0.3006\n",
      "Epoch: 975/1000 | Batch 020/052 | Cost: 0.0606\n",
      "Epoch: 975/1000 | Batch 030/052 | Cost: 0.5375\n",
      "Epoch: 975/1000 | Batch 040/052 | Cost: 0.2146\n",
      "Epoch: 975/1000 | Batch 050/052 | Cost: 0.2895\n",
      "Epoch: 975/1000 training accuracy: 87.77%\n",
      "Epoch: 976/1000 | Batch 000/052 | Cost: 0.3586\n",
      "Epoch: 976/1000 | Batch 010/052 | Cost: 0.3464\n",
      "Epoch: 976/1000 | Batch 020/052 | Cost: 0.2356\n",
      "Epoch: 976/1000 | Batch 030/052 | Cost: 0.0592\n",
      "Epoch: 976/1000 | Batch 040/052 | Cost: 0.3655\n",
      "Epoch: 976/1000 | Batch 050/052 | Cost: 0.2465\n",
      "Epoch: 976/1000 training accuracy: 88.44%\n",
      "Epoch: 977/1000 | Batch 000/052 | Cost: 0.3560\n",
      "Epoch: 977/1000 | Batch 010/052 | Cost: 0.2544\n",
      "Epoch: 977/1000 | Batch 020/052 | Cost: 0.3448\n",
      "Epoch: 977/1000 | Batch 030/052 | Cost: 0.3499\n",
      "Epoch: 977/1000 | Batch 040/052 | Cost: 0.2276\n",
      "Epoch: 977/1000 | Batch 050/052 | Cost: 0.5496\n",
      "Epoch: 977/1000 training accuracy: 87.54%\n",
      "Epoch: 978/1000 | Batch 000/052 | Cost: 0.2878\n",
      "Epoch: 978/1000 | Batch 010/052 | Cost: 0.4562\n",
      "Epoch: 978/1000 | Batch 020/052 | Cost: 0.1917\n",
      "Epoch: 978/1000 | Batch 030/052 | Cost: 0.1484\n",
      "Epoch: 978/1000 | Batch 040/052 | Cost: 0.2801\n",
      "Epoch: 978/1000 | Batch 050/052 | Cost: 0.2865\n",
      "Epoch: 978/1000 training accuracy: 88.66%\n",
      "Epoch: 979/1000 | Batch 000/052 | Cost: 0.0912\n",
      "Epoch: 979/1000 | Batch 010/052 | Cost: 0.2258\n",
      "Epoch: 979/1000 | Batch 020/052 | Cost: 0.2641\n",
      "Epoch: 979/1000 | Batch 030/052 | Cost: 0.5889\n",
      "Epoch: 979/1000 | Batch 040/052 | Cost: 0.3630\n",
      "Epoch: 979/1000 | Batch 050/052 | Cost: 0.1681\n",
      "Epoch: 979/1000 training accuracy: 89.00%\n",
      "Epoch: 980/1000 | Batch 000/052 | Cost: 0.4654\n",
      "Epoch: 980/1000 | Batch 010/052 | Cost: 0.2341\n",
      "Epoch: 980/1000 | Batch 020/052 | Cost: 0.2485\n",
      "Epoch: 980/1000 | Batch 030/052 | Cost: 0.3794\n",
      "Epoch: 980/1000 | Batch 040/052 | Cost: 0.2421\n",
      "Epoch: 980/1000 | Batch 050/052 | Cost: 0.2024\n",
      "Epoch: 980/1000 training accuracy: 89.45%\n",
      "Epoch: 981/1000 | Batch 000/052 | Cost: 0.2563\n",
      "Epoch: 981/1000 | Batch 010/052 | Cost: 0.5157\n",
      "Epoch: 981/1000 | Batch 020/052 | Cost: 0.1323\n",
      "Epoch: 981/1000 | Batch 030/052 | Cost: 0.2196\n",
      "Epoch: 981/1000 | Batch 040/052 | Cost: 0.3473\n",
      "Epoch: 981/1000 | Batch 050/052 | Cost: 0.1576\n",
      "Epoch: 981/1000 training accuracy: 88.55%\n",
      "Epoch: 982/1000 | Batch 000/052 | Cost: 0.1142\n",
      "Epoch: 982/1000 | Batch 010/052 | Cost: 0.6002\n",
      "Epoch: 982/1000 | Batch 020/052 | Cost: 0.2725\n",
      "Epoch: 982/1000 | Batch 030/052 | Cost: 0.1313\n",
      "Epoch: 982/1000 | Batch 040/052 | Cost: 0.1341\n",
      "Epoch: 982/1000 | Batch 050/052 | Cost: 0.1118\n",
      "Epoch: 982/1000 training accuracy: 89.00%\n",
      "Epoch: 983/1000 | Batch 000/052 | Cost: 0.2540\n",
      "Epoch: 983/1000 | Batch 010/052 | Cost: 0.2228\n",
      "Epoch: 983/1000 | Batch 020/052 | Cost: 0.3277\n",
      "Epoch: 983/1000 | Batch 030/052 | Cost: 0.3276\n",
      "Epoch: 983/1000 | Batch 040/052 | Cost: 0.5951\n",
      "Epoch: 983/1000 | Batch 050/052 | Cost: 0.1272\n",
      "Epoch: 983/1000 training accuracy: 88.33%\n",
      "Epoch: 984/1000 | Batch 000/052 | Cost: 0.3240\n",
      "Epoch: 984/1000 | Batch 010/052 | Cost: 0.1800\n",
      "Epoch: 984/1000 | Batch 020/052 | Cost: 0.3677\n",
      "Epoch: 984/1000 | Batch 030/052 | Cost: 0.1509\n",
      "Epoch: 984/1000 | Batch 040/052 | Cost: 0.1562\n",
      "Epoch: 984/1000 | Batch 050/052 | Cost: 0.3331\n",
      "Epoch: 984/1000 training accuracy: 89.45%\n",
      "Epoch: 985/1000 | Batch 000/052 | Cost: 0.3921\n",
      "Epoch: 985/1000 | Batch 010/052 | Cost: 0.1202\n",
      "Epoch: 985/1000 | Batch 020/052 | Cost: 0.2984\n",
      "Epoch: 985/1000 | Batch 030/052 | Cost: 0.2353\n",
      "Epoch: 985/1000 | Batch 040/052 | Cost: 0.1706\n",
      "Epoch: 985/1000 | Batch 050/052 | Cost: 0.4001\n",
      "Epoch: 985/1000 training accuracy: 89.34%\n",
      "Epoch: 986/1000 | Batch 000/052 | Cost: 0.1961\n",
      "Epoch: 986/1000 | Batch 010/052 | Cost: 0.5909\n",
      "Epoch: 986/1000 | Batch 020/052 | Cost: 0.0912\n",
      "Epoch: 986/1000 | Batch 030/052 | Cost: 0.1183\n",
      "Epoch: 986/1000 | Batch 040/052 | Cost: 0.2143\n",
      "Epoch: 986/1000 | Batch 050/052 | Cost: 0.1891\n",
      "Epoch: 986/1000 training accuracy: 88.33%\n",
      "Epoch: 987/1000 | Batch 000/052 | Cost: 0.2393\n",
      "Epoch: 987/1000 | Batch 010/052 | Cost: 0.1823\n",
      "Epoch: 987/1000 | Batch 020/052 | Cost: 0.2142\n",
      "Epoch: 987/1000 | Batch 030/052 | Cost: 0.3109\n",
      "Epoch: 987/1000 | Batch 040/052 | Cost: 0.3258\n",
      "Epoch: 987/1000 | Batch 050/052 | Cost: 0.0964\n",
      "Epoch: 987/1000 training accuracy: 89.23%\n",
      "Epoch: 988/1000 | Batch 000/052 | Cost: 0.2483\n",
      "Epoch: 988/1000 | Batch 010/052 | Cost: 0.5490\n",
      "Epoch: 988/1000 | Batch 020/052 | Cost: 0.5307\n",
      "Epoch: 988/1000 | Batch 030/052 | Cost: 0.1684\n",
      "Epoch: 988/1000 | Batch 040/052 | Cost: 0.4116\n",
      "Epoch: 988/1000 | Batch 050/052 | Cost: 0.2419\n",
      "Epoch: 988/1000 training accuracy: 87.88%\n",
      "Epoch: 989/1000 | Batch 000/052 | Cost: 0.0833\n",
      "Epoch: 989/1000 | Batch 010/052 | Cost: 0.2500\n",
      "Epoch: 989/1000 | Batch 020/052 | Cost: 0.3501\n",
      "Epoch: 989/1000 | Batch 030/052 | Cost: 0.2682\n",
      "Epoch: 989/1000 | Batch 040/052 | Cost: 0.3869\n",
      "Epoch: 989/1000 | Batch 050/052 | Cost: 0.6235\n",
      "Epoch: 989/1000 training accuracy: 88.44%\n",
      "Epoch: 990/1000 | Batch 000/052 | Cost: 0.2287\n",
      "Epoch: 990/1000 | Batch 010/052 | Cost: 0.2451\n",
      "Epoch: 990/1000 | Batch 020/052 | Cost: 0.1651\n",
      "Epoch: 990/1000 | Batch 030/052 | Cost: 0.3718\n",
      "Epoch: 990/1000 | Batch 040/052 | Cost: 0.1033\n",
      "Epoch: 990/1000 | Batch 050/052 | Cost: 0.3050\n",
      "Epoch: 990/1000 training accuracy: 88.55%\n",
      "Epoch: 991/1000 | Batch 000/052 | Cost: 0.3098\n",
      "Epoch: 991/1000 | Batch 010/052 | Cost: 0.0749\n",
      "Epoch: 991/1000 | Batch 020/052 | Cost: 0.3086\n",
      "Epoch: 991/1000 | Batch 030/052 | Cost: 0.0894\n",
      "Epoch: 991/1000 | Batch 040/052 | Cost: 0.1959\n",
      "Epoch: 991/1000 | Batch 050/052 | Cost: 0.2199\n",
      "Epoch: 991/1000 training accuracy: 89.00%\n",
      "Epoch: 992/1000 | Batch 000/052 | Cost: 0.1390\n",
      "Epoch: 992/1000 | Batch 010/052 | Cost: 0.5928\n",
      "Epoch: 992/1000 | Batch 020/052 | Cost: 0.3786\n",
      "Epoch: 992/1000 | Batch 030/052 | Cost: 0.1466\n",
      "Epoch: 992/1000 | Batch 040/052 | Cost: 0.2542\n",
      "Epoch: 992/1000 | Batch 050/052 | Cost: 0.0581\n",
      "Epoch: 992/1000 training accuracy: 88.89%\n",
      "Epoch: 993/1000 | Batch 000/052 | Cost: 0.0991\n",
      "Epoch: 993/1000 | Batch 010/052 | Cost: 0.3321\n",
      "Epoch: 993/1000 | Batch 020/052 | Cost: 0.2606\n",
      "Epoch: 993/1000 | Batch 030/052 | Cost: 0.2857\n",
      "Epoch: 993/1000 | Batch 040/052 | Cost: 0.2873\n",
      "Epoch: 993/1000 | Batch 050/052 | Cost: 0.5525\n",
      "Epoch: 993/1000 training accuracy: 89.56%\n",
      "Epoch: 994/1000 | Batch 000/052 | Cost: 0.3807\n",
      "Epoch: 994/1000 | Batch 010/052 | Cost: 0.1337\n",
      "Epoch: 994/1000 | Batch 020/052 | Cost: 0.1311\n",
      "Epoch: 994/1000 | Batch 030/052 | Cost: 0.1134\n",
      "Epoch: 994/1000 | Batch 040/052 | Cost: 0.3287\n",
      "Epoch: 994/1000 | Batch 050/052 | Cost: 0.1282\n",
      "Epoch: 994/1000 training accuracy: 88.55%\n",
      "Epoch: 995/1000 | Batch 000/052 | Cost: 0.4696\n",
      "Epoch: 995/1000 | Batch 010/052 | Cost: 0.2395\n",
      "Epoch: 995/1000 | Batch 020/052 | Cost: 0.3888\n",
      "Epoch: 995/1000 | Batch 030/052 | Cost: 0.3345\n",
      "Epoch: 995/1000 | Batch 040/052 | Cost: 0.4042\n",
      "Epoch: 995/1000 | Batch 050/052 | Cost: 0.3720\n",
      "Epoch: 995/1000 training accuracy: 88.78%\n",
      "Epoch: 996/1000 | Batch 000/052 | Cost: 0.4103\n",
      "Epoch: 996/1000 | Batch 010/052 | Cost: 0.1884\n",
      "Epoch: 996/1000 | Batch 020/052 | Cost: 0.1317\n",
      "Epoch: 996/1000 | Batch 030/052 | Cost: 0.3548\n",
      "Epoch: 996/1000 | Batch 040/052 | Cost: 0.2847\n",
      "Epoch: 996/1000 | Batch 050/052 | Cost: 0.1545\n",
      "Epoch: 996/1000 training accuracy: 88.78%\n",
      "Epoch: 997/1000 | Batch 000/052 | Cost: 0.5734\n",
      "Epoch: 997/1000 | Batch 010/052 | Cost: 0.2258\n",
      "Epoch: 997/1000 | Batch 020/052 | Cost: 0.5305\n",
      "Epoch: 997/1000 | Batch 030/052 | Cost: 0.2657\n",
      "Epoch: 997/1000 | Batch 040/052 | Cost: 0.3268\n",
      "Epoch: 997/1000 | Batch 050/052 | Cost: 0.4559\n",
      "Epoch: 997/1000 training accuracy: 88.33%\n",
      "Epoch: 998/1000 | Batch 000/052 | Cost: 0.3609\n",
      "Epoch: 998/1000 | Batch 010/052 | Cost: 0.2965\n",
      "Epoch: 998/1000 | Batch 020/052 | Cost: 0.1729\n",
      "Epoch: 998/1000 | Batch 030/052 | Cost: 0.2343\n",
      "Epoch: 998/1000 | Batch 040/052 | Cost: 0.4267\n",
      "Epoch: 998/1000 | Batch 050/052 | Cost: 0.1179\n",
      "Epoch: 998/1000 training accuracy: 88.55%\n",
      "Epoch: 999/1000 | Batch 000/052 | Cost: 0.2176\n",
      "Epoch: 999/1000 | Batch 010/052 | Cost: 0.2514\n",
      "Epoch: 999/1000 | Batch 020/052 | Cost: 0.3124\n",
      "Epoch: 999/1000 | Batch 030/052 | Cost: 0.2381\n",
      "Epoch: 999/1000 | Batch 040/052 | Cost: 0.4259\n",
      "Epoch: 999/1000 | Batch 050/052 | Cost: 0.2051\n",
      "Epoch: 999/1000 training accuracy: 87.88%\n",
      "Epoch: 1000/1000 | Batch 000/052 | Cost: 0.2159\n",
      "Epoch: 1000/1000 | Batch 010/052 | Cost: 0.1491\n",
      "Epoch: 1000/1000 | Batch 020/052 | Cost: 0.2022\n",
      "Epoch: 1000/1000 | Batch 030/052 | Cost: 0.1274\n",
      "Epoch: 1000/1000 | Batch 040/052 | Cost: 0.1244\n",
      "Epoch: 1000/1000 | Batch 050/052 | Cost: 0.1593\n",
      "Epoch: 1000/1000 training accuracy: 88.44%\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    \n",
    "    for features, targets in data_loader:\n",
    "        features = features.float().to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        \n",
    "    return correct_pred.float() / num_examples * 100\n",
    "    \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        features = features.float().to(device)\n",
    "        targets = targets.to(device)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        \n",
    "        # note that the PyTorch implementation of\n",
    "        # CrossEntropyLoss works with logits, not\n",
    "        # probabilities\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 10:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(X)//batch_size, cost))\n",
    "            \n",
    "    with torch.set_grad_enabled(False):\n",
    "        print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
    "              epoch+1, num_epochs, \n",
    "              compute_accuracy(model, train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    " logits, probas = model.forward(X_test.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    " _, predicted_labels = torch.max(probas, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId.tolist(), 'Survived': predicted_labels.tolist()})\n",
    "output.to_csv('deep_learning_last.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>897</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>898</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>903</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>907</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>910</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>911</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>912</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>913</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>914</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>916</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>918</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>919</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>920</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>921</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PassengerId  Survived\n",
       "0           892         0\n",
       "1           893         0\n",
       "2           894         0\n",
       "3           895         0\n",
       "4           896         0\n",
       "5           897         0\n",
       "6           898         0\n",
       "7           899         0\n",
       "8           900         1\n",
       "9           901         0\n",
       "10          902         0\n",
       "11          903         0\n",
       "12          904         1\n",
       "13          905         0\n",
       "14          906         1\n",
       "15          907         1\n",
       "16          908         0\n",
       "17          909         0\n",
       "18          910         0\n",
       "19          911         0\n",
       "20          912         0\n",
       "21          913         1\n",
       "22          914         1\n",
       "23          915         1\n",
       "24          916         1\n",
       "25          917         0\n",
       "26          918         1\n",
       "27          919         0\n",
       "28          920         0\n",
       "29          921         0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
