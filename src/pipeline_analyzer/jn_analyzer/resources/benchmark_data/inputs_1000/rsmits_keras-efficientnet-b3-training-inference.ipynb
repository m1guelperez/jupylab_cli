{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This competition provides a lot of room for interresting experimentations. In this kernel I use a rather easy way to train a standard EfficientNet B3 model with a custom head layer and Generalized mean pool. I use only basic image preprocessing with a scaling factor.\n",
    "\n",
    "To save on training time I use a different training set on each epoch. This gives a nice boost of about 0.005 to 0.008 compared to a fixed training set when using train/test split or cross-validation. The downside is that the validation has some less value.\n",
    "\n",
    "This kernel contains the inference part where I use 3 models from the training. For the complete code to train it yourself you can download it from my [github](https://github.com/RobinSmits/KaggleBengaliAIHandwrittenGraphemeClassification). I trained it for 80 epochs on my 1070 Ti (roughly 1,5 days).\n",
    "\n",
    "I hope you like it and if you find this kernel helpfull..then please don't forget to upvote it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/kerasefficientnetb3/efficientnet-1.0.0-py3-none-any.whl\r\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.6/site-packages (from efficientnet==1.0.0) (1.0.8)\r\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.6/site-packages (from efficientnet==1.0.0) (0.16.2)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (2.10.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (1.18.1)\r\n",
      "Requirement already satisfied: scipy>=0.19.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (1.4.1)\r\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (2.4)\r\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (1.1.1)\r\n",
      "Requirement already satisfied: pillow>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (5.4.1)\r\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (2.6.1)\r\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-image->efficientnet==1.0.0) (3.0.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0) (1.14.0)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=2.0->scikit-image->efficientnet==1.0.0) (4.4.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (1.1.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (2.4.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.6/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (2.8.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.0.0) (45.2.0.post20200210)\r\n",
      "Installing collected packages: efficientnet\r\n",
      "Successfully installed efficientnet-1.0.0\r\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Dense, Lambda\n",
    "from math import ceil\n",
    "\n",
    "# Install EfficientNet\n",
    "!pip install '../input/kerasefficientnetb3/efficientnet-1.0.0-py3-none-any.whl'\n",
    "import efficientnet.keras as efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "FACTOR = 0.70\n",
    "HEIGHT_NEW = int(HEIGHT * FACTOR)\n",
    "WIDTH_NEW = int(WIDTH * FACTOR)\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Dir\n",
    "DIR = '../input/bengaliai-cv19'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "# Image Size Summary\n",
    "print(HEIGHT_NEW)\n",
    "print(WIDTH_NEW)\n",
    "\n",
    "# Image Prep\n",
    "def resize_image(img, WIDTH_NEW, HEIGHT_NEW):\n",
    "    # Invert\n",
    "    img = 255 - img\n",
    "\n",
    "    # Normalize\n",
    "    img = (img * (255.0 / img.max())).astype(np.uint8)\n",
    "\n",
    "    # Reshape\n",
    "    img = img.reshape(HEIGHT, WIDTH)\n",
    "    image_resized = cv2.resize(img, (WIDTH_NEW, HEIGHT_NEW), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "    return image_resized.reshape(-1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized mean pool - GeM\n",
    "gm_exp = tf.Variable(3.0, dtype = tf.float32)\n",
    "def generalized_mean_pool_2d(X):\n",
    "    pool = (tf.reduce_mean(tf.abs(X**(gm_exp)), \n",
    "                        axis = [1, 2], \n",
    "                        keepdims = False) + 1.e-7)**(1./gm_exp)\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "def create_model(input_shape):\n",
    "    # Input Layer\n",
    "    input = Input(shape = input_shape)\n",
    "    \n",
    "    # Create and Compile Model and show Summary\n",
    "    x_model = efn.EfficientNetB3(weights = None, include_top = False, input_tensor = input, pooling = None, classes = None)\n",
    "    \n",
    "    # UnFreeze all layers\n",
    "    for layer in x_model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    # GeM\n",
    "    lambda_layer = Lambda(generalized_mean_pool_2d)\n",
    "    lambda_layer.trainable_weights.extend([gm_exp])\n",
    "    x = lambda_layer(x_model.output)\n",
    "    \n",
    "    # multi output\n",
    "    grapheme_root = Dense(168, activation = 'softmax', name = 'root')(x)\n",
    "    vowel_diacritic = Dense(11, activation = 'softmax', name = 'vowel')(x)\n",
    "    consonant_diacritic = Dense(7, activation = 'softmax', name = 'consonant')(x)\n",
    "\n",
    "    # model\n",
    "    model = Model(inputs = x_model.input, outputs = [grapheme_root, vowel_diacritic, consonant_diacritic])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "model1 = create_model(input_shape = (HEIGHT_NEW, WIDTH_NEW, CHANNELS))\n",
    "model2 = create_model(input_shape = (HEIGHT_NEW, WIDTH_NEW, CHANNELS))\n",
    "model3 = create_model(input_shape = (HEIGHT_NEW, WIDTH_NEW, CHANNELS))\n",
    "model4 = create_model(input_shape = (HEIGHT_NEW, WIDTH_NEW, CHANNELS))\n",
    "model5 = create_model(input_shape = (HEIGHT_NEW, WIDTH_NEW, CHANNELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model Weights\n",
    "model1.load_weights('../input/kerasefficientnetb3/Train1_model_59.h5') # LB 0.9681\n",
    "model2.load_weights('../input/kerasefficientnetb3/Train1_model_64.h5') # LB 0.9679\n",
    "#model2.load_weights('../input/kerasefficientnetb3/Train1_model_66.h5') # LB 0.9685\n",
    "model3.load_weights('../input/kerasefficientnetb3/Train1_model_68.h5') # LB 0.9691\n",
    "model4.load_weights('../input/kerasefficientnetb3/Train1_model_57.h5') # LB ??\n",
    "model5.load_weights('../input/kerasefficientnetb3/Train1_model_70.h5') # LB ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, X, batch_size = 16, img_size = (512, 512, 3), *args, **kwargs):\n",
    "        self.X = X\n",
    "        self.indices = np.arange(len(self.X))\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X = self.__data_generation(indices)\n",
    "        return X\n",
    "    \n",
    "    def __data_generation(self, indices):\n",
    "        X = np.empty((self.batch_size, *self.img_size))\n",
    "        \n",
    "        for i, index in enumerate(indices):\n",
    "            image = self.X[index]\n",
    "            image = np.stack((image,)*CHANNELS, axis=-1)\n",
    "            image = image.reshape(-1, HEIGHT_NEW, WIDTH_NEW, CHANNELS)\n",
    "            \n",
    "            X[i,] = image\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 7s 7s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    }
   ],
   "source": [
    "# Create Submission File\n",
    "tgt_cols = ['grapheme_root','vowel_diacritic','consonant_diacritic']\n",
    "\n",
    "# Create Predictions\n",
    "row_ids, targets = [], []\n",
    "\n",
    "# Loop through Test Parquet files (X)\n",
    "for i in range(0, 4):\n",
    "    # Test Files Placeholder\n",
    "    test_files = []\n",
    "\n",
    "    # Read Parquet file\n",
    "    df = pd.read_parquet(os.path.join(DIR, 'test_image_data_'+str(i)+'.parquet'))\n",
    "    # Get Image Id values\n",
    "    image_ids = df['image_id'].values \n",
    "    # Drop Image_id column\n",
    "    df = df.drop(['image_id'], axis = 1)\n",
    "\n",
    "    # Loop over rows in Dataframe and generate images \n",
    "    X = []\n",
    "    for image_id, index in zip(image_ids, range(df.shape[0])):\n",
    "        test_files.append(image_id)\n",
    "        X.append(resize_image(df.loc[df.index[index]].values, WIDTH_NEW, HEIGHT_NEW))\n",
    "\n",
    "    # Data_Generator\n",
    "    data_generator_test = TestDataGenerator(X, batch_size = BATCH_SIZE, img_size = (HEIGHT_NEW, WIDTH_NEW, CHANNELS))\n",
    "        \n",
    "    # Predict with all 3 models\n",
    "    preds1 = model1.predict_generator(data_generator_test, verbose = 1)\n",
    "    preds2 = model2.predict_generator(data_generator_test, verbose = 1)\n",
    "    preds3 = model3.predict_generator(data_generator_test, verbose = 1)\n",
    "    preds4 = model4.predict_generator(data_generator_test, verbose = 1)\n",
    "    preds5 = model5.predict_generator(data_generator_test, verbose = 1)\n",
    "    \n",
    "    # Loop over Preds    \n",
    "    for i, image_id in zip(range(len(test_files)), test_files):\n",
    "        \n",
    "        for subi, col in zip(range(len(preds1)), tgt_cols):\n",
    "            sub_preds1 = preds1[subi]\n",
    "            sub_preds2 = preds2[subi]\n",
    "            sub_preds3 = preds3[subi]\n",
    "            sub_preds4 = preds4[subi]\n",
    "            sub_preds5 = preds5[subi]\n",
    "\n",
    "            # Set Prediction with average of 5 predictions\n",
    "            row_ids.append(str(image_id)+'_'+col)\n",
    "            sub_pred_value = np.argmax((sub_preds1[i] + sub_preds2[i] + sub_preds3[i] + sub_preds4[i] + sub_preds5[i]) / 5)\n",
    "            targets.append(sub_pred_value)\n",
    "    \n",
    "    # Cleanup\n",
    "    del df\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         row_id  target\n",
      "0          Test_0_grapheme_root       3\n",
      "1        Test_0_vowel_diacritic       0\n",
      "2    Test_0_consonant_diacritic       0\n",
      "3          Test_1_grapheme_root      93\n",
      "4        Test_1_vowel_diacritic       2\n",
      "5    Test_1_consonant_diacritic       0\n",
      "6          Test_2_grapheme_root      19\n",
      "7        Test_2_vowel_diacritic       0\n",
      "8    Test_2_consonant_diacritic       0\n",
      "9          Test_3_grapheme_root     115\n",
      "10       Test_3_vowel_diacritic       0\n",
      "11   Test_3_consonant_diacritic       0\n",
      "12         Test_4_grapheme_root      55\n",
      "13       Test_4_vowel_diacritic       4\n",
      "14   Test_4_consonant_diacritic       0\n",
      "15         Test_5_grapheme_root     115\n",
      "16       Test_5_vowel_diacritic       2\n",
      "17   Test_5_consonant_diacritic       0\n",
      "18         Test_6_grapheme_root     147\n",
      "19       Test_6_vowel_diacritic       9\n",
      "20   Test_6_consonant_diacritic       5\n",
      "21         Test_7_grapheme_root     137\n",
      "22       Test_7_vowel_diacritic       7\n",
      "23   Test_7_consonant_diacritic       0\n",
      "24         Test_8_grapheme_root     119\n",
      "25       Test_8_vowel_diacritic       9\n",
      "26   Test_8_consonant_diacritic       0\n",
      "27         Test_9_grapheme_root     133\n",
      "28       Test_9_vowel_diacritic      10\n",
      "29   Test_9_consonant_diacritic       0\n",
      "30        Test_10_grapheme_root     148\n",
      "31      Test_10_vowel_diacritic       1\n",
      "32  Test_10_consonant_diacritic       4\n",
      "33        Test_11_grapheme_root      21\n",
      "34      Test_11_vowel_diacritic       2\n",
      "35  Test_11_consonant_diacritic       0\n"
     ]
    }
   ],
   "source": [
    "# Create and Save Submission File\n",
    "submit_df = pd.DataFrame({'row_id':row_ids,'target':targets}, columns = ['row_id','target'])\n",
    "submit_df.to_csv('submission.csv', index = False)\n",
    "print(submit_df.head(40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
