{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this competition a lot has been said and written about feature engineering...and the likely winner and top scores will definitely be achieved by heavy feature engineering.\n",
    "\n",
    "But curious to see how far we can get with a minimum of feature engineering. So I created this notebook with just some basic preparation, feature selection, count encoding and NaN elimination\n",
    "\n",
    "I'll use a simple 1D Convolutional Neural Network to showcase whats possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.initializers import glorot_uniform, lecun_uniform\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPool1D, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Random Seed\n",
    "seed = 12345\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Constants\n",
    "epochs = 25\n",
    "batch_size = 1024\n",
    "number_of_folds = 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I'll define 2 lists. One with categorical features and one with 'low-score' features. I made a simple LightGBM setup in which I determined the AUC score per feature. This usually gives a nice impression. The list below is a list of the lowest scoring ones. Very quick and dirty determined but I used this already in multiple competitions and usually it works quitte well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "cat_feats = ['ProductCD', 'addr1', 'addr2', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', \n",
    "                        'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']\n",
    "\n",
    "# Low score Features\n",
    "lowscore_feats =   ['V322','V329','V321','V336','V331','V335','V330','V332','V328','V338','V327','V137','V333','V326','V116','V339','V337',\n",
    "                    'V334','V114','V115','V163','V298','V162','V142','V141','V325','V129','V138','V161','V100','V296','V112','V105',\n",
    "                    'V113','V111','V106','V299','V98','V110','V301','V108','V135','V109','V319','V104','V300','V297','V119','V311',\n",
    "                    'V117','V41','V118','V121','V122','V286','V120','V107','V305']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the data and immediately drop the columns we don't need. Alternatively we could create a list of just the columns that we want to load. I prefer the first method as I can now easily modify my list with lowscore_feats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Dataset Shapes\n",
      "Train Transaction: (590540, 335)\n",
      "Test Transaction: (506691, 335)\n",
      "Labels: (590540,)\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "train = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')\n",
    "labels = train['isFraud']\n",
    "test = pd.read_csv('../input/ieee-fraud-detection/test_transaction.csv')\n",
    "\n",
    "# Drop Columns\n",
    "test.drop(lowscore_feats, axis = 1, inplace = True)\n",
    "train.drop(lowscore_feats, axis = 1, inplace = True)\n",
    "train.drop(['isFraud'], axis=1, inplace = True)\n",
    "\n",
    "# Summary Shapes\n",
    "print('====== Dataset Shapes')\n",
    "print('Train Transaction: {0}'.format(train.shape))\n",
    "print('Test Transaction: {0}'.format(test.shape))\n",
    "print('Labels: {0}'.format(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll append the train and test set. Only for the features with less than 10K of missing values will the missing values be replaced by the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Train and Test Datasets\n",
    "train_len = len(train)\n",
    "df = train.append(test).reset_index()\n",
    "\n",
    "# Cleanup\n",
    "del train, test\n",
    "gc.collect()\n",
    "\n",
    "# Impute Mean value for features that have less than 10K NaN values\n",
    "processed_feats = []\n",
    "for feat in [f for f in df.columns if f not in ['index', 'TransactionID', 'TransactionDT'] + cat_feats + lowscore_feats]:\n",
    "    if df[feat].isna().sum() < 10000:\n",
    "        imputer = SimpleImputer(strategy = 'mean')\n",
    "        df[feat] = imputer.fit_transform(df[feat].values.reshape(-1, 1))\n",
    "        processed_feats.append(feat)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we process TransactionDT into new features 'hour' and 'weekday'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process TransactionDT into hour and weekday\n",
    "df['hour'] = df['TransactionDT'].map(lambda x:(x // 3600) % 24)\n",
    "df['weekday'] = df['TransactionDT'].map(lambda x:(x // (3600 * 24)) % 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All remaining features will be encoded with their value counts. The nan values are included in this to make sure that we don't have any missing values when running the neural net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count encode all categorical features\n",
    "for feat in cat_feats:\n",
    "    df[feat] = df[feat].map(df[feat].value_counts(dropna = False))\n",
    "\n",
    "# Count encode all other remaining 'Numerical' Features\n",
    "for feat in [f for f in df.columns if f not in ['index', 'TransactionID', 'TransactionDT'] + cat_feats + processed_feats]:\n",
    "    df[feat] = df[feat].map(df[feat].value_counts(dropna = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last processing step I use a MinMaxScaler to scale all the features. For features with a large skew value this will also be corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Final Features\n",
    "feats = [f for f in df.columns if f not in ['index', 'TransactionID', 'TransactionDT'] + lowscore_feats]\n",
    "\n",
    "# Scale and correct extreme skew\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "for feat in feats:\n",
    "    # Scale\n",
    "    df[feat] = scaler.fit_transform(df[feat].values.reshape(-1, 1))\n",
    "    \n",
    "    # Correct Skew\n",
    "    if df[feat].skew() > 10:\n",
    "        df[feat] = np.log10(df[feat] + 1 - min(0, df[feat].min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Final Dataset Shapes\n",
      "Train Transaction: (590540, 335)\n",
      "Test Transaction: (506691, 335)\n",
      "Labels: (590540,)\n"
     ]
    }
   ],
   "source": [
    "# Split back to train and test dataset  \n",
    "train = df[:train_len]\n",
    "test = df[train_len:]\n",
    "\n",
    "# Final Summary Shapes\n",
    "print('====== Final Dataset Shapes')\n",
    "print('Train Transaction: {0}'.format(train[feats].shape))\n",
    "print('Test Transaction: {0}'.format(test[feats].shape))\n",
    "print('Labels: {0}'.format(labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the EarlyStopping, ModelCheckPoint and the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EarlyStop(patience):\n",
    "    return EarlyStopping(monitor = \"val_loss\",\n",
    "                          min_delta = 0,\n",
    "                          mode = \"min\",\n",
    "                          verbose = 1, \n",
    "                          patience = patience)\n",
    "\n",
    "def ModelCheckpointFull(model_name):\n",
    "    return ModelCheckpoint(model_name, \n",
    "                            monitor = 'val_loss', \n",
    "                            verbose = 1, \n",
    "                            save_best_only = True, \n",
    "                            save_weights_only = False, \n",
    "                            mode = 'min', \n",
    "                            period = 1)\n",
    "\n",
    "# Input Shape\n",
    "input_shape = train[feats].shape[1]\n",
    "\n",
    "# Define CNN 1D model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(96, 2, activation = 'relu', input_shape=(input_shape, 1), kernel_initializer = glorot_uniform(seed = seed)))\n",
    "    model.add(BatchNormalization())       \n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(96, 1, activation = 'relu', kernel_initializer = glorot_uniform(seed = seed)))\n",
    "    model.add(BatchNormalization())       \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.25))    \n",
    "    model.add(Dense(96, activation = 'relu', kernel_initializer = glorot_uniform(seed = seed)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation = 'sigmoid', kernel_initializer = glorot_uniform(seed = seed)))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = Adam(lr = 0.001), metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step we run a Stratified KFold and generate the submission file. After some trials I found that about 25 epochs is a good value to use. When using more the validation AUC score will increase but the LB score will start to drop slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold: 0\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12531, saving model to model.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12531 to 0.10246, saving model to model.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.10246\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10246 to 0.10001, saving model to model.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.10001\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10001\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.10001 to 0.09657, saving model to model.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09657\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09657\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09657 to 0.09058, saving model to model.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09058 to 0.08893, saving model to model.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.08893\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.08893\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.08893\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.08893 to 0.08760, saving model to model.h5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.08760\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.08760 to 0.08342, saving model to model.h5\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.08342 to 0.08309, saving model to model.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.08309 to 0.08269, saving model to model.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08269\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08269 to 0.07929, saving model to model.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.07929\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.07929\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.07929 to 0.07754, saving model to model.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.07754\n",
      "Fold  0 AUC : 0.921741\n",
      "Running Fold: 1\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13513, saving model to model.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13513 to 0.10501, saving model to model.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10501 to 0.10179, saving model to model.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.10179\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10179 to 0.10083, saving model to model.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10083\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.10083\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.10083 to 0.09666, saving model to model.h5\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09666\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09666 to 0.09366, saving model to model.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09366 to 0.09102, saving model to model.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.09102 to 0.09078, saving model to model.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.09078\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09078\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09078 to 0.08960, saving model to model.h5\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.08960 to 0.08590, saving model to model.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.08590 to 0.08467, saving model to model.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08467\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.08467\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.08467 to 0.08263, saving model to model.h5\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08263 to 0.08163, saving model to model.h5\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.08163 to 0.08161, saving model to model.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.08161 to 0.08047, saving model to model.h5\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.08047 to 0.07963, saving model to model.h5\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.07963\n",
      "Fold  1 AUC : 0.923267\n",
      "Running Fold: 2\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14440, saving model to model.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14440 to 0.10389, saving model to model.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10389 to 0.10227, saving model to model.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.10227\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.10227\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10227\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.10227 to 0.09694, saving model to model.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09694 to 0.09562, saving model to model.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09562 to 0.09320, saving model to model.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.09320\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09320 to 0.09148, saving model to model.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.09148\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.09148\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09148\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.09148\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09148 to 0.08807, saving model to model.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.08807\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.08807 to 0.08623, saving model to model.h5\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.08623 to 0.08485, saving model to model.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08485\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.08485\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.08485 to 0.08338, saving model to model.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.08338 to 0.08220, saving model to model.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.08220\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.08220\n",
      "Fold  2 AUC : 0.916094\n",
      "Running Fold: 3\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.14624, saving model to model.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.14624 to 0.10691, saving model to model.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10691 to 0.10411, saving model to model.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.10411\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10411 to 0.10268, saving model to model.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.10268\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.10268 to 0.09698, saving model to model.h5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09698 to 0.09500, saving model to model.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09500 to 0.09402, saving model to model.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09402 to 0.09359, saving model to model.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.09359\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.09359\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.09359\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.09359 to 0.08823, saving model to model.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.08823\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.08823\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.08823\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08823\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.08823 to 0.08790, saving model to model.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08790\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08790 to 0.08706, saving model to model.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.08706\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.08706 to 0.08566, saving model to model.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.08566\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.08566 to 0.08494, saving model to model.h5\n",
      "Fold  3 AUC : 0.917353\n",
      "Running Fold: 4\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13138, saving model to model.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13138 to 0.10429, saving model to model.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.10429\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10429 to 0.10200, saving model to model.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.10200 to 0.10028, saving model to model.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.10028 to 0.09854, saving model to model.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09854\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09854 to 0.09744, saving model to model.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09744 to 0.09451, saving model to model.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09451 to 0.09405, saving model to model.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09405 to 0.09319, saving model to model.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.09319\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.09319\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.09319 to 0.08950, saving model to model.h5\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.08950\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.08950 to 0.08809, saving model to model.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.08809\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08809\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.08809\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.08809 to 0.08513, saving model to model.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.08513\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.08513\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.08513 to 0.08420, saving model to model.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.08420\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.08420\n",
      "Fold  4 AUC : 0.913316\n",
      "Running Fold: 5\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13577, saving model to model.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13577 to 0.10194, saving model to model.h5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.10194\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10194 to 0.09930, saving model to model.h5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09930 to 0.09551, saving model to model.h5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09551 to 0.09463, saving model to model.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09463\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09463\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.09463 to 0.09203, saving model to model.h5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.09203 to 0.09051, saving model to model.h5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.09051 to 0.08914, saving model to model.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.08914\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08914 to 0.08709, saving model to model.h5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.08709\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.08709\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.08709 to 0.08565, saving model to model.h5\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.08565 to 0.08324, saving model to model.h5\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08324\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.08324 to 0.08292, saving model to model.h5\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.08292\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.08292\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.08292 to 0.08088, saving model to model.h5\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.08088 to 0.07710, saving model to model.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.07710\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.07710\n",
      "Fold  5 AUC : 0.921526\n",
      "Full AUC score 0.917391\n"
     ]
    }
   ],
   "source": [
    "# Reshape\n",
    "train = train[feats].values.reshape(-1, input_shape, 1)\n",
    "test = test[feats].values.reshape(-1, input_shape, 1)\n",
    "\n",
    "# CV Folds\n",
    "folds = StratifiedKFold(n_splits = number_of_folds, shuffle = True, random_state = seed)\n",
    "\n",
    "# Arrays to store predictions\n",
    "oof_preds = np.zeros(train.shape[0])\n",
    "sub_preds = np.zeros(test.shape[0])\n",
    "\n",
    "# Loop through folds\n",
    "for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train, labels)):\n",
    "    train_x, train_y = train[train_idx], labels.iloc[train_idx]\n",
    "    valid_x, valid_y = train[valid_idx], labels.iloc[valid_idx]\n",
    "\n",
    "    print('Running Fold: ' + str(n_fold))\n",
    "\n",
    "    # CNN 1D model\n",
    "    model = create_model()\n",
    "    model.fit(train_x, train_y, \n",
    "                validation_data=(valid_x, valid_y), \n",
    "                epochs=epochs, \n",
    "                batch_size=batch_size, \n",
    "                verbose=0,\n",
    "                callbacks=[EarlyStop(10), ModelCheckpointFull('model.h5')])\n",
    "\n",
    "    # Delete Model\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    # Reload Best Saved Model\n",
    "    model = load_model('model.h5')\n",
    "\n",
    "    # OOF Predictions\n",
    "    oof_preds[valid_idx] = model.predict(valid_x).reshape(-1,)\n",
    "    \n",
    "    # Submission Predictions\n",
    "    predictions = model.predict(test).reshape(-1,)\n",
    "    sub_preds += predictions / number_of_folds\n",
    "\n",
    "    # Fold AUC Score\n",
    "    print('Fold %2d AUC : %.6f' % (n_fold, roc_auc_score(valid_y, oof_preds[valid_idx])))        \n",
    "\n",
    "    # Cleanup \n",
    "    del model, train_x, train_y, valid_y, valid_x\n",
    "    K.clear_session()\n",
    "    gc.collect\n",
    "\n",
    "print('Full AUC score %.6f' % roc_auc_score(labels, oof_preds))\n",
    "\n",
    "# Generate Submission\n",
    "submission = pd.read_csv('../input/ieee-fraud-detection/sample_submission.csv')\n",
    "submission['isFraud'] = sub_preds\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you enjoyed the notebook. Let me know if you have any feedback or questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
