{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imdb_master.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import torch\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The only scary thing about this movie is the thought that whoever made it might make a sequel.<br /><br />From start to finish \"The Tooth Fairy\" was just downri\n",
      "[2, 4, 79, 666, 170, 59, 19, 26, 11, 4, 218, 20, 2444, 113, 16, 246, 114, 8, 1, 14, 15, 12, 13, 14, 15, 12, 13, 50, 387, 10]\n"
     ]
    }
   ],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.label_id = label_id\n",
    "#Класс словаря. Метод word2id возвращает номер слова, id2word - наоборот, восстанавливает слово.\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, itos, unk_index):\n",
    "        self._itos = itos\n",
    "        self._stoi = {word:i for i, word in enumerate(itos)}\n",
    "        self._unk_index = unk_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._itos)\n",
    "    \n",
    "    def word2id(self, word):\n",
    "        idx = self._stoi.get(word)\n",
    "        if idx is not None:\n",
    "            return idx\n",
    "        return self._unk_index\n",
    "    \n",
    "    def id2word(self, idx):\n",
    "        return self._itos[idx]\n",
    "from tqdm import tqdm_notebook\n",
    "#Интерфейс объекта, преобразующего тексты в последовательности номеров. transform выполняет преобразование при помощи словаря. fit_transform выучивает словарь из текста и возвращает такое же преобразование при помощи свежеполученного словаря.\n",
    "\n",
    "class TextToIdsTransformer:\n",
    "    def transform():\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def fit_transform():\n",
    "        raise NotImplementedError()\n",
    "#Простая реализация данного интерфейса. Разбиение на слова производится с помощью библиотеки NLTK. В словаре содержатся несколько спец. слов. После токенизации, к полученной последовательности слов добавляются слева и справа спец. слова для начала и конца текста.\n",
    "\n",
    "class SimpleTextTransformer(TextToIdsTransformer):\n",
    "    def __init__(self, max_vocab_size):\n",
    "        self.special_words = ['<PAD>', '</UNK>', '<S>', '</S>']\n",
    "        self.unk_index = 1\n",
    "        self.pad_index = 0\n",
    "        self.vocab = None\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return nltk.tokenize.word_tokenize(text.lower())\n",
    "        \n",
    "    def build_vocab(self, tokens):\n",
    "        itos = []\n",
    "        itos.extend(self.special_words)\n",
    "        \n",
    "        token_counts = Counter(tokens)\n",
    "        for word, _ in token_counts.most_common(self.max_vocab_size - len(self.special_words)):\n",
    "            itos.append(word)\n",
    "            \n",
    "        self.vocab = Vocab(itos, self.unk_index)\n",
    "    \n",
    "    def transform(self, texts):\n",
    "        result = []\n",
    "        for text in texts:\n",
    "            tokens = ['<S>'] + self.tokenize(text) + ['</S>']\n",
    "            ids = [self.vocab.word2id(token) for token in tokens]\n",
    "            result.append(ids)\n",
    "        return result\n",
    "    \n",
    "    def fit_transform(self, texts):\n",
    "        result = []\n",
    "        tokenized_texts = [self.tokenize(text) for text in texts]\n",
    "        self.build_vocab(itertools.chain(*tokenized_texts))\n",
    "        for tokens in tokenized_texts:\n",
    "            tokens = ['<S>'] + tokens + ['</S>']\n",
    "            ids = [self.vocab.word2id(token) for token in tokens]\n",
    "            result.append(ids)\n",
    "        return result\n",
    "#Строим экземпляр входных данных. Обеспечиваем длину последовательности номеров равной max_seq_len.\n",
    "\n",
    "def build_features(token_ids, label, max_seq_len, pad_index, label_encoding):\n",
    "    if len(token_ids) >= max_seq_len:\n",
    "        ids = token_ids[:max_seq_len]\n",
    "    else:\n",
    "        ids = token_ids + [pad_index for _ in range(max_seq_len - len(token_ids))]\n",
    "    return InputFeatures(ids, label_encoding[label])\n",
    "        \n",
    "#Собираем экземпляры в тензоры\n",
    "\n",
    "def features_to_tensor(list_of_features):\n",
    "    text_tensor = torch.tensor([example.input_ids for example in list_of_features], dtype=torch.long)\n",
    "    labels_tensor = torch.tensor([example.label_id for example in list_of_features], dtype=torch.long)\n",
    "    return text_tensor, labels_tensor\n",
    "from sklearn import model_selection\n",
    "imdb_df = pd.read_csv('../input/imdb_master.csv', encoding='latin-1')\n",
    "dev_df = imdb_df[(imdb_df.type == 'train') & (imdb_df.label != 'unsup')]\n",
    "test_df = imdb_df[(imdb_df.type == 'test')]\n",
    "train_df, val_df = model_selection.train_test_split(dev_df, test_size=0.05, stratify=dev_df.label)\n",
    "max_seq_len=200\n",
    "classes = {'neg': 0, 'pos' : 1}\n",
    "text2id = SimpleTextTransformer(10000)\n",
    "\n",
    "train_ids = text2id.fit_transform(train_df['review'])\n",
    "val_ids = text2id.transform(val_df['review'])\n",
    "test_ids = text2id.transform(test_df['review'])\n",
    "print(train_df.review.iloc[0][:160])\n",
    "print(train_ids[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n",
    "                  for token_ids, label in zip(train_ids, train_df['label'])]\n",
    "\n",
    "val_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n",
    "                  for token_ids, label in zip(val_ids, val_df['label'])]\n",
    "\n",
    "test_features = [build_features(token_ids, label,max_seq_len, text2id.pad_index, classes) \n",
    "                  for token_ids, label in zip(test_ids, test_df['label'])]\n",
    "\n",
    "train_tensor, train_labels = features_to_tensor(train_features)\n",
    "val_tensor, val_labels = features_to_tensor(val_features)\n",
    "test_tensor, test_labels = features_to_tensor(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "train_ds = TensorDataset(train_tensor,train_labels)\n",
    "val_ds = TensorDataset(val_tensor,val_labels)\n",
    "test_ds = TensorDataset(test_tensor,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_ds,batch_size=128)\n",
    "val_loader = DataLoader(val_ds, batch_size=128)\n",
    "test_loader = DataLoader(test_ds, batch_size=128)\n",
    "vocab_len = len(text2id.vocab)\n",
    "print(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_len, 100)\n",
    "        self.properties = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=100, out_channels=120, kernel_size=3, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=120, out_channels=130, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(5)\n",
    "        )\n",
    "        self.estimator = nn.Sequential(\n",
    "            nn.Linear(5200,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.transpose(1,2)\n",
    "        return  self.estimator(self.properties(x).view(x.size(0), -1))\n",
    "        \n",
    "    def train(self,train_loader,val_loader,epoch,waiting,optimizer):\n",
    "        self.cuda()\n",
    "        best_val_loss=1000\n",
    "        crit = nn.BCELoss()\n",
    "        for i in range(epoch):\n",
    "            train_loss = 0\n",
    "            val_loss = 0\n",
    "            for xx,yy in train_loader:\n",
    "                xx = xx.cuda()\n",
    "                yy=yy.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = self.forward(xx)\n",
    "                loss = crit(y_pred,yy.float())\n",
    "                train_loss += loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            train_loss = train_loss/len(train_loader)\n",
    "            with torch.no_grad():\n",
    "                for xx,yy in val_loader:\n",
    "                    xx, yy = xx.cuda(), yy.cuda()\n",
    "                    y_pred = self.forward(xx)\n",
    "                    loss = crit(y_pred,yy.float())\n",
    "                    val_loss += loss\n",
    "                val_loss = val_loss/len(val_loader)\n",
    "                \n",
    "                if best_val_loss>val_loss:\n",
    "                    torch.save(self.state_dict(), \"../best_model.py\")\n",
    "                    best_val_loss = val_loss\n",
    "                    wait=waiting\n",
    "                else:\n",
    "                    wait -=1\n",
    "                    if wait==0:\n",
    "                        break\n",
    "            print(\"train loss:\", float(train_loss), \"___best val loss:\",float(best_val_loss), \"___remaining:\", wait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([70])) that is different to the input size (torch.Size([70, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([98])) that is different to the input size (torch.Size([98, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.6535976529121399 ___best val loss: 0.49707648158073425 ___remaining: 10\n",
      "train loss: 0.4288409650325775 ___best val loss: 0.4275527894496918 ___remaining: 10\n",
      "train loss: 0.3176466226577759 ___best val loss: 0.3963506817817688 ___remaining: 10\n",
      "train loss: 0.24199694395065308 ___best val loss: 0.39536625146865845 ___remaining: 10\n",
      "train loss: 0.17196109890937805 ___best val loss: 0.39536625146865845 ___remaining: 9\n",
      "train loss: 0.14939585328102112 ___best val loss: 0.39536625146865845 ___remaining: 8\n",
      "train loss: 0.10272914171218872 ___best val loss: 0.39536625146865845 ___remaining: 7\n",
      "train loss: 0.09077750146389008 ___best val loss: 0.39536625146865845 ___remaining: 6\n",
      "train loss: 0.04835241660475731 ___best val loss: 0.39536625146865845 ___remaining: 5\n",
      "train loss: 0.014512546360492706 ___best val loss: 0.39536625146865845 ___remaining: 4\n",
      "train loss: 0.007341407239437103 ___best val loss: 0.39536625146865845 ___remaining: 3\n",
      "train loss: 0.003183248220011592 ___best val loss: 0.39536625146865845 ___remaining: 2\n",
      "train loss: 0.0018143837805837393 ___best val loss: 0.39536625146865845 ___remaining: 1\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "clf = Model()\n",
    "\n",
    "optimizer = torch.optim.Adam(clf.parameters(), lr=0.001)\n",
    "clf.train(train_loader,val_loader,20,10,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.89      0.84     12500\n",
      "           1       0.88      0.77      0.82     12500\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     25000\n",
      "   macro avg       0.83      0.83      0.83     25000\n",
      "weighted avg       0.83      0.83      0.83     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "clf.load_state_dict(torch.load(\"../best_model.py\"))\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for xx,yy in test_loader:\n",
    "    out = clf.forward(xx.cuda())\n",
    "    for i in out:\n",
    "        if i<=0.4:\n",
    "            y_pred.append(0)\n",
    "        else:\n",
    "            y_pred.append(1)\n",
    "    for i in yy:\n",
    "        y_true.append(int(i))\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
