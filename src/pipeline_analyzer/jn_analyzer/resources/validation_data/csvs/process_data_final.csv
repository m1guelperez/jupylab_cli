content,tag,output_type,original_content
SETUP,0,not_existent,"import numpy as np
 import pandas as pd
 import xgboost as xgb
 import matplotlib.pyplot as plt"
"ASSIGN = pd.read_csv(""..path"", parse_dates=['timestamp']) ASSIGN = pd.read_csv(""..path"", parse_dates=['timestamp']) ASSIGN = pd.read_csv(""..path"", parse_dates=['timestamp']) ASSIGN.head()",0,execute_result,"df_train = pd.read_csv(""../input/train.csv"", parse_dates=['timestamp'])
 df_test = pd.read_csv(""../input/test.csv"", parse_dates=['timestamp'])
 df_macro = pd.read_csv(""../input/macro.csv"", parse_dates=['timestamp'])
 
 df_train.head()"
df_train['price_doc'].hist(bins=50),0,execute_result,df_train['price_doc'].hist(bins=50)
"CHECKPOINT ASSIGN = df_train['price_doc'].values ASSIGN = df_test['id'] df_train.drop(['id', 'price_doc'], axis=1, inplace=True) df_test.drop(['id'], axis=1, inplace=True) ASSIGN = len(df_train) ASSIGN = pd.concat([df_train, df_test]) ASSIGN = pd.merge_ordered(ASSIGN, df_macro, on='timestamp', how='left') print(ASSIGN.shape) ASSIGN = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100) ASSIGN = month_year.value_counts().to_dict() ASSIGN['month_year_cnt'] = ASSIGN.map(ASSIGN) ASSIGN = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100) ASSIGN = week_year.value_counts().to_dict() ASSIGN['week_year_cnt'] = ASSIGN.map(ASSIGN) ASSIGN['month'] = ASSIGN.timestamp.dt.month ASSIGN['dow'] = ASSIGN.timestamp.dt.dayofweek ASSIGN['rel_floor'] = ASSIGN['floor'] path['max_floor'].astype(float) ASSIGN['rel_kitch_sq'] = ASSIGN['kitch_sq'] path['full_sq'].astype(float) ASSIGN.drop(['timestamp'], axis=1, inplace=True)",1,stream,"y_train = df_train['price_doc'].values
 id_test = df_test['id']
 
 df_train.drop(['id', 'price_doc'], axis=1, inplace=True)
 df_test.drop(['id'], axis=1, inplace=True)
 
 # Build df_all = (df_train+df_test).join(df_macro)
 num_train = len(df_train)
 df_all = pd.concat([df_train, df_test])
 df_all = pd.merge_ordered(df_all, df_macro, on='timestamp', how='left')
 print(df_all.shape)
 
 # Add month-year
 month_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)
 month_year_cnt_map = month_year.value_counts().to_dict()
 df_all['month_year_cnt'] = month_year.map(month_year_cnt_map)
 
 # Add week-year count
 week_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)
 week_year_cnt_map = week_year.value_counts().to_dict()
 df_all['week_year_cnt'] = week_year.map(week_year_cnt_map)
 
 # Add month and day-of-week
 df_all['month'] = df_all.timestamp.dt.month
 df_all['dow'] = df_all.timestamp.dt.dayofweek
 
 # Other feature engineering
 df_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)
 df_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)
 
 # Remove timestamp column (may overfit the model in train)
 df_all.drop(['timestamp'], axis=1, inplace=True)"
"ASSIGN = df_all.select_dtypes(exclude=['object']) ASSIGN = df_all.select_dtypes(include=['object']).copy() for c in ASSIGN: ASSIGN[c] = pd.factorize(ASSIGN[c])[0] ASSIGN = pd.concat([df_numeric, df_obj], axis=1)",1,not_existent,"# Deal with categorical values
 df_numeric = df_all.select_dtypes(exclude=['object'])
 df_obj = df_all.select_dtypes(include=['object']).copy()
 
 for c in df_obj:
     df_obj[c] = pd.factorize(df_obj[c])[0]
 
 df_values = pd.concat([df_numeric, df_obj], axis=1)"
CHECKPOINT ASSIGN = df_values.values print(ASSIGN.shape) ASSIGN = X_all[:num_train] ASSIGN = X_all[num_train:] ASSIGN = df_values.columns,1,stream,"# Convert to numpy values
 X_all = df_values.values
 print(X_all.shape)
 
 X_train = X_all[:num_train]
 X_test = X_all[num_train:]
 
 df_columns = df_values.columns"
"ASSIGN = { 'eta': 0.05, 'max_depth': 5, 'subsample': 0.7, 'colsample_bytree': 0.7, 'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': 1 } ASSIGN = xgb.DMatrix(X_train, y_train, feature_names=df_columns) ASSIGN = xgb.DMatrix(X_test, feature_names=df_columns)",1,not_existent,"xgb_params = {
     'eta': 0.05,
     'max_depth': 5,
     'subsample': 0.7,
     'colsample_bytree': 0.7,
     'objective': 'reg:linear',
     'eval_metric': 'rmse',
     'silent': 1
 }
 
 dtrain = xgb.DMatrix(X_train, y_train, feature_names=df_columns)
 dtest = xgb.DMatrix(X_test, feature_names=df_columns)"
ASSIGN = 383,1,not_existent,"# Uncomment to tune XGB `num_boost_rounds`
 
 #cv_result = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,
 #    verbose_eval=True, show_stdv=False)
 #cv_result[['train-rmse-mean', 'test-rmse-mean']].plot()
 #num_boost_rounds = len(cv_result)
 
 num_boost_round = 383"
"ASSIGN = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)",0,not_existent,"model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)"
"ASSIGN = plt.subplots(1, 1, figsize=(8, 16)) xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)",0,execute_result,"fig, ax = plt.subplots(1, 1, figsize=(8, 16))
 xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)"
"ASSIGN = model.predict(dtest) ASSIGN = pd.DataFrame({'id': id_test, 'price_doc': y_pred}) ASSIGN.to_csv('sub.csv', index=False)",0,not_existent,"y_pred = model.predict(dtest)
 
 df_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})
 
 df_sub.to_csv('sub.csv', index=False)"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed
 # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
 # For example, here's several helpful packages to load
 
 import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
 
 # Input data files are available in the read-only ""../input/"" directory
 # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
 
 import os
 for dirname, _, filenames in os.walk('/kaggle/input'):
     for filename in filenames:
         print(os.path.join(dirname, filename))
 
 # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
 # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
"ASSIGN = pd.read_csv(""path"") ASSIGN = pd.read_csv(""path"") ASSIGN=test_data",0,not_existent,"train_data = pd.read_csv(""/kaggle/input/home-data-for-ml-course/train.csv"")
 test_data = pd.read_csv(""/kaggle/input/home-data-for-ml-course/test.csv"")
 test=test_data"
train_data.head(),0,execute_result,train_data.head()
test_data.head(),0,execute_result,test_data.head()
CHECKPOINT ASSIGN = [col for col in train_data.columns if train_data[col].isnull().any()] print(ASSIGN),1,stream,"Col_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]
 print(Col_with_missing)"
CHECKPOINT ASSIGN = [col for col in test_data.columns if test_data[col].isnull().any()] print(ASSIGN),1,stream,"Col_with_missing = [col for col in test_data.columns if test_data[col].isnull().any()]
 print(Col_with_missing)"
ASSIGN=[],0,not_existent,drop_column=[]
CHECKPOINT for col in train_data.columns: if train_data[col].isnull().any(): ASSIGN=train_data[col].isnull().sum() if ASSIGN>50: print(col++str(ASSIGN)) drop_column.append(col),1,stream,"for col in train_data.columns:
     if train_data[col].isnull().any():
         x=train_data[col].isnull().sum()
         if x>50:
             print(col+""\t""+str(x))
             drop_column.append(col)"
CHECKPOINT for col in test_data.columns: if test_data[col].isnull().any(): ASSIGN=test_data[col].isnull().sum() if ASSIGN>50: print(col++str(ASSIGN)),1,stream,"for col in test_data.columns:
     if test_data[col].isnull().any():
         x=test_data[col].isnull().sum()
         if x>50:
             print(col+""\t""+str(x))"
CHECKPOINT print(drop_column),0,stream,print(drop_column)
"ASSIGN=ASSIGN.drop(drop_column,axis=1) ASSIGN=ASSIGN.drop(drop_column,axis=1)",1,not_existent,"train_data=train_data.drop(drop_column,axis=1)
 test_data=test_data.drop(drop_column,axis=1)"
"ASSIGN=train_data.drop([""SalePrice""],axis=1) ASSIGN=train_data[""SalePrice""]",1,not_existent,"X_train=train_data.drop([""SalePrice""],axis=1)
 Y_train=train_data[""SalePrice""]"
X_train.head(),0,execute_result,X_train.head()
Y_train.head(),0,execute_result,Y_train.head()
ASSIGN=[],0,not_existent,cat_column=[]
for col in X_train.columns: if X_train[col].dtype=='object': cat_column.append(col),1,not_existent,"for col in X_train.columns:
     if X_train[col].dtype=='object':
         cat_column.append(col)"
CHECKPOINT print(cat_column),0,stream,print(cat_column)
CHECKPOINT for col in cat_column: print(col) print(X_train[col].value_counts()) print(*50),0,stream,"for col in cat_column:
     print(col)
     print(X_train[col].value_counts())
     print(""-""*50)"
"SETUP def score_dataset(X_train, X_valid, y_train, y_valid): ASSIGN = XGBRegressor(n_estimators=1000, learning_rate=0.01) ASSIGN.fit(X_train, y_train, early_stopping_rounds=50, ASSIGN=[(X_valid, y_valid)], verbose=False) ASSIGN.fit(X_train, y_train) ASSIGN = my_model.predict(X_valid) return mean_absolute_error(y_valid, ASSIGN)",0,not_existent,"from xgboost import XGBRegressor
 from sklearn.metrics import mean_absolute_error
 
 # function for comparing different approaches
 def score_dataset(X_train, X_valid, y_train, y_valid):
     my_model = XGBRegressor(n_estimators=1000, learning_rate=0.01)
     my_model.fit(X_train, y_train, early_stopping_rounds=50, 
              eval_set=[(X_valid, y_valid)], verbose=False)
     my_model.fit(X_train, y_train)
     preds = my_model.predict(X_valid)
     return mean_absolute_error(y_valid, preds)"
"SETUP ASSIGN = train_test_split(X_train, Y_train, ASSIGN=0.8, test_size=0.2, ASSIGN=0) ASSIGN=test_data",1,not_existent,"from sklearn.model_selection import train_test_split
 x_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train,
                                                       train_size=0.8, test_size=0.2,
                                                       random_state=0)
 x_test=test_data"
"SETUP ASSIGN=SimpleImputer(strategy=""most_frequent"") ASSIGN= pd.DataFrame(my_imputer.fit_transform(x_train)) ASSIGN=pd.DataFrame(my_imputer.transform(x_test)) ASSIGN=pd.DataFrame(my_imputer.transform(x_valid)) ASSIGN.index = x_train.index ASSIGN.index = x_valid.index ASSIGN.index = x_test.index ASSIGN.columns=x_train.columns ASSIGN.columns=x_valid.columns ASSIGN.columns=x_test.columns",1,not_existent,"from sklearn.impute import SimpleImputer
 my_imputer=SimpleImputer(strategy=""most_frequent"")
 imputed_X_train= pd.DataFrame(my_imputer.fit_transform(x_train))
 imputed_X_test=pd.DataFrame(my_imputer.transform(x_test))
 imputed_X_valid=pd.DataFrame(my_imputer.transform(x_valid))
 imputed_X_train.index = x_train.index
 imputed_X_valid.index = x_valid.index
 imputed_X_test.index = x_test.index
 imputed_X_train.columns=x_train.columns
 imputed_X_valid.columns=x_valid.columns
 imputed_X_test.columns=x_test.columns"
CHECKPOINT Col_with_missing_2 = [col for col in imputed_X_valid.columns if imputed_X_valid[col].isnull().any()] print(Col_with_missing_2),1,stream,"Col_with_missing_2 = [col for col in imputed_X_valid.columns if imputed_X_valid[col].isnull().any()]
 print(Col_with_missing_2)"
ASSIGN=[],0,not_existent,Num_col=[]
"CHECKPOINT for col in x_train.columns: if(x_train[col].dtype!=""object""): print(col++str(x_train[col].dtype)) if col!=""Id"": Num_col.append(col)",1,stream,"for col in x_train.columns:
     if(x_train[col].dtype!=""object""):
         print(col+""\t""+str(x_train[col].dtype))
         if col!=""Id"":
             Num_col.append(col)"
CHECKPOINT print(Num_col),0,stream,print(Num_col)
""""""" ASSIGN=[] for col in Num_col: ASSIGN.append(col) """""" ASSIGN=['TotalBsmtSF', 'WoodDeckSF', 'BsmtUnfSF', 'YearRemodAdd', '3SsnPorch', 'KitchenAbvGr', '2ndFlrSF', 'ScreenPorch', 'PoolArea', 'TotRmsAbvGrd', 'MoSold', 'BedroomAbvGr', 'MiscVal', 'BsmtHalfBath', '1stFlrSF', 'GarageCars', 'OverallQual', 'YrSold', 'HalfBath', 'OpenPorchSF', 'BsmtFullBath', 'LowQualFinSF', 'LotArea', 'OverallCond', 'YearBuilt', 'EnclosedPorch', 'FullBath', 'Fireplaces', 'BsmtFinSF2', 'BsmtFinSF1', 'MSSubClass', 'GrLivArea', 'GarageArea']",1,not_existent,"#feature=[""LotArea"",""OverallQual"",""OverallCond"",""BsmtUnfSF"",""TotalBsmtSF"",""1stFlrSF"",""GrLivArea"",'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars',""GarageArea""]
 """"""
 feature=[]
 for col in Num_col:
     feature.append(col)
 """"""
 feature=['TotalBsmtSF', 'WoodDeckSF', 'BsmtUnfSF', 'YearRemodAdd', '3SsnPorch', 'KitchenAbvGr', '2ndFlrSF', 'ScreenPorch', 'PoolArea', 'TotRmsAbvGrd', 'MoSold', 'BedroomAbvGr', 'MiscVal', 'BsmtHalfBath', '1stFlrSF', 'GarageCars', 'OverallQual', 'YrSold', 'HalfBath', 'OpenPorchSF', 'BsmtFullBath', 'LowQualFinSF', 'LotArea', 'OverallCond', 'YearBuilt', 'EnclosedPorch', 'FullBath', 'Fireplaces', 'BsmtFinSF2', 'BsmtFinSF1', 'MSSubClass', 'GrLivArea', 'GarageArea']"
CHECKPOINT print(feature),0,stream,print(feature)
"ASSIGN=['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']",1,not_existent,"#cat_enc_col=[""Street"",""LotShape"",""LotConfig"",""BldgType"",""HouseStyle"",""MasVnrType"",""ExterQual"",""Foundation"",""BsmtQual"",""BsmtExposure"",""BsmtFinType1"",""KitchenQual""]
 cat_enc_col=['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']"
for col in cat_enc_col: feature.append(col),1,not_existent,"for col in cat_enc_col:
     feature.append(col)"
ASSIGN=ASSIGN[feature] ASSIGN=ASSIGN[feature] ASSIGN=ASSIGN[feature],1,not_existent,"imputed_X_train=imputed_X_train[feature]
 imputed_X_valid=imputed_X_valid[feature]
 imputed_X_test=imputed_X_test[feature]"
imputed_X_train.head(10),0,execute_result,imputed_X_train.head(10)
ASSIGN=cat_enc_col,1,not_existent,Cat_cols=cat_enc_col
"SETUP ASSIGN = OneHotEncoder(handle_unknown='ignore', sparse=False) ASSIGN = pd.DataFrame(OH_encoder.fit_transform(imputed_X_train[Cat_cols])) ASSIGN = pd.DataFrame(OH_encoder.transform(imputed_X_valid[Cat_cols])) ASSIGN = pd.DataFrame(OH_encoder.transform(imputed_X_test[Cat_cols])) ASSIGN.index = imputed_X_train.index ASSIGN.index = imputed_X_valid.index ASSIGN.index = imputed_X_test.index ASSIGN = imputed_X_train.drop(Cat_cols, axis =1) ASSIGN = imputed_X_valid.drop(Cat_cols, axis =1) ASSIGN = imputed_X_test.drop(Cat_cols, axis =1) ASSIGN = pd.concat([num_X_train, OH_cols_train], axis=1) ASSIGN = pd.concat([num_X_valid, OH_cols_valid], axis=1) ASSIGN = pd.concat([num_X_test, OH_cols_test], axis=1)",1,not_existent,"from sklearn.preprocessing import OneHotEncoder
 OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
 OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(imputed_X_train[Cat_cols]))
 OH_cols_valid = pd.DataFrame(OH_encoder.transform(imputed_X_valid[Cat_cols]))
 OH_cols_test = pd.DataFrame(OH_encoder.transform(imputed_X_test[Cat_cols]))
 OH_cols_train.index = imputed_X_train.index
 OH_cols_valid.index = imputed_X_valid.index
 OH_cols_test.index = imputed_X_test.index
 num_X_train = imputed_X_train.drop(Cat_cols, axis =1)
 num_X_valid = imputed_X_valid.drop(Cat_cols, axis =1)
 num_X_test = imputed_X_test.drop(Cat_cols, axis =1)
 OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
 OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)
 OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)"
ASSIGN = ASSIGN.apply(pd.to_numeric) ASSIGN = ASSIGN.apply(pd.to_numeric) ASSIGN = ASSIGN.apply(pd.to_numeric),1,not_existent,"OH_X_train = OH_X_train.apply(pd.to_numeric)
 OH_X_valid = OH_X_valid.apply(pd.to_numeric)
 OH_X_test = OH_X_test.apply(pd.to_numeric)"
OH_X_test.head(10),0,execute_result,OH_X_test.head(10)
"CHECKPOINT print(,end=) print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))",0,stream,"print(""MAE:"",end="" "")
 print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
"SETUP CHECKPOINT ASSIGN = GradientBoostingRegressor(loss=""ls"",learning_rate=0.01,n_estimators=1000,max_depth=4,alpha=0.08) ASSIGN.fit(OH_X_train, y_train) ASSIGN = my_model.predict(OH_X_valid) print(,mean_absolute_error(y_valid, ASSIGN))",0,stream,"from sklearn.ensemble import GradientBoostingRegressor
 my_model = GradientBoostingRegressor(loss=""ls"",learning_rate=0.01,n_estimators=1000,max_depth=4,alpha=0.08)
 my_model.fit(OH_X_train, y_train)
 preds = my_model.predict(OH_X_valid)
 print(""MAE:"",mean_absolute_error(y_valid, preds))"
"SETUP CHECKPOINT ASSIGN = XGBRegressor(n_estimators=2000, learning_rate=0.008) ASSIGN.fit(OH_X_train, y_train, early_stopping_rounds=50, ASSIGN=[(OH_X_valid, y_valid)], verbose=False) ASSIGN = my_model_1.predict(OH_X_valid) print(,mean_absolute_error(y_valid, ASSIGN))",0,stream,"from xgboost import XGBRegressor
 from sklearn.metrics import mean_absolute_error
 my_model_1 = XGBRegressor(n_estimators=2000, learning_rate=0.008)
 my_model_1.fit(OH_X_train, y_train, early_stopping_rounds=50, 
              eval_set=[(OH_X_valid, y_valid)], verbose=False)
 preds_1 = my_model_1.predict(OH_X_valid)
 print(""MAE:"",mean_absolute_error(y_valid, preds_1))"
"SETUP CHECKPOINT ASSIGN = GradientBoostingRegressor(loss=""ls"",learning_rate=0.01,n_estimators=2000,max_depth=4,alpha=0.08) ASSIGN.fit(OH_X_train, y_train) ASSIGN = my_model_2.predict(OH_X_valid) print(,mean_absolute_error(y_valid, ASSIGN))",0,stream,"from sklearn.ensemble import GradientBoostingRegressor
 my_model_2 = GradientBoostingRegressor(loss=""ls"",learning_rate=0.01,n_estimators=2000,max_depth=4,alpha=0.08)
 my_model_2.fit(OH_X_train, y_train)
 preds_2 = my_model_2.predict(OH_X_valid)
 print(""MAE:"",mean_absolute_error(y_valid, preds_2))"
"CHECKPOINT ASSIGN=(preds_1+preds_2)path print(,mean_absolute_error(y_valid, ASSIGN))",1,stream,"preds_3=(preds_1+preds_2)/2
 print(""MAE:"",mean_absolute_error(y_valid, preds_3))"
ASSIGN = my_model_1.predict(OH_X_test) ASSIGN = my_model_2.predict(OH_X_test) ASSIGN=(predictions1+predictions2)path,0,not_existent,"predictions1 = my_model_1.predict(OH_X_test)
 predictions2 = my_model_2.predict(OH_X_test)
 Preds_last=(predictions1+predictions2)/2"
CHECKPOINT Preds_last,0,execute_result,Preds_last
"ASSIGN = pd.DataFrame({'Id': test.Id,'SalePrice': Preds_last}) ASSIGN.to_csv('submission1.csv', index=False)",0,not_existent,"output = pd.DataFrame({'Id': test.Id,'SalePrice': Preds_last})
 output.to_csv('submission1.csv', index=False)"
"SETUP ASSIGN = bq_helper.BigQueryHelper(active_project=""bigquery-public-data"", ASSIGN=""github_repos"")",0,not_existent,"# import package with helper functions  import bq_helper  # create a helper object for this dataset github = bq_helper.BigQueryHelper(active_project=""bigquery-public-data"",                                               dataset_name=""github_repos"")"
"ASSIGN = ("""""" -- Select all the columns we want in our joined table SELECT L.license, COUNT(sf.path) AS number_of_files FROM `bigquery-public-data.github_repos.sample_files` as sf -- Table to merge into sample_files INNER JOIN `bigquery-public-data.github_repos.licenses` as L ON sf.repo_name = L.repo_name -- what columns should we join on? GROUP BY L.license ORDER BY number_of_files DESC """""") ASSIGN = github.query_to_pandas_safe(query, max_gb_scanned=6)",1,not_existent,"# You can use two dashes (--) to add comments in SQL query = (""""""         -- Select all the columns we want in our joined table         SELECT L.license, COUNT(sf.path) AS number_of_files         FROM `bigquery-public-data.github_repos.sample_files` as sf         -- Table to merge into sample_files         INNER JOIN `bigquery-public-data.github_repos.licenses` as L              ON sf.repo_name = L.repo_name -- what columns should we join on?         GROUP BY L.license         ORDER BY number_of_files DESC         """""")  file_count_by_license = github.query_to_pandas_safe(query, max_gb_scanned=6)"
CHECKPOINT print(file_count_by_license),0,not_existent,# print out all the returned results print(file_count_by_license)
github.list_tables(),0,not_existent,#view the list of tables inside the big query github.list_tables()
"github.head(""sample_commits"")",0,not_existent,"#lets view the a couple of lines from the table using head github.head(""sample_commits"")"
"github.head(""sample_files"")",0,not_existent,"#lets view the a couple of lines from the table using head github.head(""sample_files"")"
"CHECKPOINT ASSIGN = ("""""" -- Select all the columns we want in our joined table SELECT sf.repo_name, count(sc.commit) number_of_commits_in_python FROM `bigquery-public-data.github_repos.sample_commits` as sc -- Table to merge into sample_files INNER JOIN `bigquery-public-data.github_repos.sample_files` as sf ON sc.repo_name = sf.repo_name -- what columns should we join on? WHERE sf.path LIKE '%.py' GROUP BY sf.repo_name ORDER BY number_of_commits_in_python DESC """""") print(github.estimate_query_size(ASSIGN)) ASSIGN = github.query_to_pandas_safe(query, max_gb_scanned=6)",1,not_existent,"#How many commits (recorded in the ""sample_commits"" table) have been made in repos written in the Python programming language? (I'm looking for the number of commits per repo for all the repos written in Python.  query = (""""""         -- Select all the columns we want in our joined table         SELECT sf.repo_name, count(sc.commit) number_of_commits_in_python         FROM `bigquery-public-data.github_repos.sample_commits` as sc         -- Table to merge into sample_files         INNER JOIN `bigquery-public-data.github_repos.sample_files` as sf              ON sc.repo_name =  sf.repo_name -- what columns should we join on?         WHERE sf.path LIKE '%.py'         GROUP BY sf.repo_name         ORDER BY number_of_commits_in_python DESC         """""")  print(github.estimate_query_size(query)) file_count_by_python_files = github.query_to_pandas_safe(query, max_gb_scanned=6)"
CHECKPOINT file_count_by_python_files,0,not_existent,file_count_by_python_files
SETUP CHECKPOINT print(os.listdir()),0,not_existent,"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from numpy import sort  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."
SETUP warnings.filterwarnings('ignore'),0,not_existent,import warnings warnings.filterwarnings('ignore')
CHECKPOINT SETUP ASSIGN = multiprocessing.cpu_count() n_jobs,0,not_existent,import multiprocessing  n_jobs = multiprocessing.cpu_count() n_jobs
SETUP,0,not_existent,"#prediction and Classification Report from sklearn.metrics import classification_report  # select features using threshold from sklearn.feature_selection import SelectFromModel from sklearn.model_selection import KFold, GridSearchCV, cross_val_score, cross_val_predict from sklearn.metrics import accuracy_score, recall_score, f1_score from sklearn.metrics.scorer import make_scorer  # plot tree, importance from xgboost import plot_tree, plot_importance "
SETUP,0,not_existent,"# load xgboost, test train split import xgboost as xgb from sklearn.model_selection import train_test_split, cross_validate from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import confusion_matrix"
SETUP,0,not_existent,import pandas as pd import seaborn as sns import matplotlib.pyplot as plt  %matplotlib inline
ASSIGN = pd.read_csv('..path'),0,not_existent,df = pd.read_csv('../input/train.csv')
CHECKPOINT ASSIGN = len(list(df.columns)) num_of_cols,1,not_existent,num_of_cols = len(list(df.columns)) num_of_cols
pd.options.display.max_columns = num_of_cols,0,not_existent,pd.options.display.max_columns = num_of_cols
len(df),0,not_existent,len(df)
"ASSIGN = pd.DataFrame(df.isnull().sum()) ASSIGN.loc[(ASSIGN.loc[:, ASSIGN.dtypes != object] != 0).any(1)]",1,not_existent,"# columns with null values df_isna = pd.DataFrame(df.isnull().sum()) df_isna.loc[(df_isna.loc[:, df_isna.dtypes != object] != 0).any(1)]"
"CHECKPOINT ASSIGN = list(df_isna.loc[(df_isna.loc[:, df_isna.dtypes != object] != 0).any(1)].T.columns) nan_cols",1,not_existent,"nan_cols = list(df_isna.loc[(df_isna.loc[:, df_isna.dtypes != object] != 0).any(1)].T.columns) nan_cols"
df[nan_cols].describe(),0,not_existent,df[nan_cols].describe()
df.describe(include='all'),0,not_existent,df.describe(include='all')
df[nan_cols].sample(3000).describe(),0,not_existent,df[nan_cols].sample(3000).describe()
df['parentesco1'].loc[df.parentesco1 == 1].describe(),0,not_existent,df['parentesco1'].loc[df.parentesco1 == 1].describe()
(df['parentesco1'].loc[df.parentesco1 == 1].describe()['count']path(df))*100,0,not_existent,(df['parentesco1'].loc[df.parentesco1 == 1].describe()['count']/len(df))*100
df['idhogar'].describe(),0,not_existent,# find number of households  df['idhogar'].describe()
ASSIGN = list(df['idhogar'].unique()),1,not_existent,house_ids = list(df['idhogar'].unique())
"df[['parentesco1','idhogar']].loc[df.parentesco1 == 1].head(5)",0,not_existent,"df[['parentesco1','idhogar']].loc[df.parentesco1 == 1].head(5)"
ASSIGN = df.groupby(['idhogar'])['parentesco1'].apply(lambda x: pd.unique(x.values.ravel()).tolist()).reset_index() len(ASSIGN),1,not_existent,hid_heads = df.groupby(['idhogar'])['parentesco1'].apply(lambda x: pd.unique(x.values.ravel()).tolist()).reset_index() len(hid_heads)
"ASSIGN = pd.DataFrame(hid_heads, index=None, columns=['idhogar','parentesco1']) ASSIGN.sample(5)",1,not_existent,"df_hid = pd.DataFrame(hid_heads, index=None, columns=['idhogar','parentesco1']) df_hid.sample(5)"
"df_hid['parentesco1'] = df_hid['parentesco1'].apply(lambda x: ''.join(map(str, x)))",1,not_existent,"df_hid['parentesco1'] = df_hid['parentesco1'].apply(lambda x: ''.join(map(str, x)))"
df_hid.sample(5),0,not_existent,df_hid.sample(5)
df_hid.loc[df_hid.parentesco1 == '0'],1,not_existent,df_hid.loc[df_hid.parentesco1 == '0']
ASSIGN = list(df_hid['idhogar'].loc[df_hid.parentesco1 == '0']) len(ASSIGN),1,not_existent,# id's without head! hid_wo_heads = list(df_hid['idhogar'].loc[df_hid.parentesco1 == '0']) len(hid_wo_heads)
ASSIGN = df[df['idhogar'].isin(hid_wo_heads)],1,not_existent,df_hwoh = df[df['idhogar'].isin(hid_wo_heads)]
"df_hwoh[['idhogar', 'parentesco1','v2a1']]",0,not_existent,"df_hwoh[['idhogar', 'parentesco1','v2a1']]"
df['v2a1'].hist(),0,not_existent,df['v2a1'].hist()
df['v2a1'].loc[-df['idhogar'].isin(hid_wo_heads)].hist(),0,not_existent,df['v2a1'].loc[-df['idhogar'].isin(hid_wo_heads)].hist()
df_hwoh['v2a1'].hist(),0,not_existent,df_hwoh['v2a1'].hist()
len(df_hwoh),0,not_existent,len(df_hwoh)
df_hwoh['idhogar'].unique(),0,not_existent,# these 15 households (23 rows) doesn't have a head.. # we should exclude these from analysis and scoring perhaps... df_hwoh['idhogar'].unique()
"CHECKPOINT print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '09b195e7a']) print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == 'f2bfa75c4'])",0,not_existent,"print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '09b195e7a']) print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == 'f2bfa75c4']) "
"CHECKPOINT print(, len(df)) ASSIGN = ASSIGN.loc[-ASSIGN['idhogar'].isin(hid_wo_heads)] print(, len(ASSIGN))",1,not_existent,"# required dataframe - without households without a head!! print(""before removal: "", len(df)) df = df.loc[-df['idhogar'].isin(hid_wo_heads)] print(""after removal: "", len(df))"
df['v2a1'].describe().plot(),0,not_existent,df['v2a1'].describe().plot()
"df[['v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned']].describe().plot()",0,not_existent,"df[['v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned']].describe().plot()"
SETUP gc.collect(),0,not_existent,import gc  gc.collect()
len(df['v2a1'].unique()),0,not_existent,len(df['v2a1'].unique())
df['v2a1'].unique(),0,not_existent,df['v2a1'].unique()
df['v2a1'].max(),0,not_existent,df['v2a1'].max()
"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 > 1000000]",0,not_existent,"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 > 1000000]"
"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 >= 1000000]",0,not_existent,"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 >= 1000000]"
"df[['v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '563cc81b7']",0,not_existent,"# remove these two rows... df[['v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '563cc81b7']"
"CHECKPOINT print(, len(df)) df.drop(df[df.idhogar == '563cc81b7'].index, inplace=True) print(, len(df))",1,not_existent,"print(""before removal: "", len(df)) df.drop(df[df.idhogar == '563cc81b7'].index, inplace=True) print(""after removal: "", len(df))"
sns.kdeplot(df['v2a1']),0,not_existent,sns.kdeplot(df['v2a1'])
sns.kdeplot(df['v18q1']),0,not_existent,sns.kdeplot(df['v18q1'])
sns.kdeplot(df['rez_esc']),0,not_existent,sns.kdeplot(df['rez_esc'])
sns.kdeplot(df['meaneduc']),0,not_existent,sns.kdeplot(df['meaneduc'])
sns.kdeplot(df['SQBmeaned']),0,not_existent,sns.kdeplot(df['SQBmeaned'])
CHECKPOINT ASSIGN = list(df.columns) cols,1,not_existent,cols = list(df.columns) cols
df.sample(10),0,not_existent,df.sample(10)
set(df.dtypes),0,not_existent,set(df.dtypes)
ASSIGN = {} for col in cols: ASSIGN[col] = df[col].dtype,1,not_existent,"col_types = {}  for col in cols:     col_types[col] = df[col].dtype     # print(col, df[col].dtype)"
CHECKPOINT print(len(col_types)),0,not_existent,print(len(col_types))
"CHECKPOINT for key in sorted(col_types): print(key, col_types[key])",0,not_existent,"for key in sorted(col_types):     print(key, col_types[key])"
"CHECKPOINT ASSIGN = [] ASSIGN = [] for col in cols: if df[col].dtype == 'O': ASSIGN.append(col) print(col, df[col].dtype) else: ASSIGN.append(col)",1,not_existent,"cat_cols = [] num_cols = [] for col in cols:     if df[col].dtype == 'O':         cat_cols.append(col)         print(col, df[col].dtype)     else:         num_cols.append(col)"
CHECKPOINT cat_cols,0,not_existent,# categorical columns cat_cols
df[cat_cols].sample(10),0,not_existent,df[cat_cols].sample(10)
len(num_cols),0,not_existent,len(num_cols)
sorted(num_cols),0,not_existent,# numerical columns sorted(num_cols)
ASSIGN = sns.PairGrid(df[nan_cols]) ASSIGN = ASSIGN.map_offdiag(plt.scatter),0,not_existent,g = sns.PairGrid(df[nan_cols]) g = g.map_offdiag(plt.scatter)
"ASSIGN = ['refrig','mobilephone','television','qmobilephone','computer', 'v18q', 'v18q1', ] ASSIGN = ['v2a1', 'area1', 'area2', 'bedrooms','rooms', 'cielorazo', 'v14a', 'tamhog', 'hacdor', 'hacapo', 'r4t3', ] ASSIGN = ['age', 'agesq', 'female', 'male',] ASSIGN = ['SQBage', 'SQBdependency', 'SQBedjefe', 'SQBescolari', 'SQBhogar_nin', 'SQBhogar_total', 'SQBmeaned', 'SQBovercrowding',] ASSIGN = ['abastaguadentro', 'abastaguafuera', 'abastaguano',] ASSIGN = [ 'hhsize', 'hogar_adul', 'hogar_mayor', 'hogar_nin', 'hogar_total',] ASSIGN = ['r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3',] ASSIGN = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',] ASSIGN = ['techocane', 'techoentrepiso', 'techootro', 'techozinc',] ASSIGN = ['pisocemento', 'pisomadera', 'pisomoscer', 'pisonatur', 'pisonotiene', 'pisoother',] ASSIGN = [ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6',] ASSIGN = [ 'parentesco1', 'parentesco10', 'parentesco11', 'parentesco12', 'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9',] ASSIGN = [ 'paredblolad', 'pareddes', 'paredfibras', 'paredmad', 'paredother', 'paredpreb', 'paredzinc', 'paredzocalo',] ASSIGN = [ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',] ASSIGN = [ 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6',] ASSIGN = [ 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7',] ASSIGN = ['elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6',] ASSIGN = ['energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4',] ASSIGN = [ 'eviv1', 'eviv2', 'eviv3',] ASSIGN = [ 'etecho1', 'etecho2', 'etecho3',] ASSIGN = [ 'epared1', 'epared2', 'epared3',] ASSIGN = [ 'dis', 'escolari', 'meaneduc', 'overcrowding', 'rez_esc', 'tamhog', 'tamviv', ] ASSIGN = ['coopele', 'noelec', 'planpri', 'public',] ASSIGN = cols_electronics+cols_house_details+cols_person_details+\ ASSIGN+ASSIGN+ASSIGN+ASSIGN+ASSIGN+ASSIGN+\ ASSIGN+ASSIGN+ASSIGN+ASSIGN+\ ASSIGN+ASSIGN+ASSIGN+ASSIGN+ASSIGN+\ cols_eviv+cols_etech+cols_pared+cols_unknown+cols_elec len(ASSIGN)",1,not_existent,"cols_electronics = ['refrig','mobilephone','television','qmobilephone','computer', 'v18q', 'v18q1', ] cols_house_details = ['v2a1', 'area1', 'area2', 'bedrooms','rooms', 'cielorazo', 'v14a',                      'tamhog', 'hacdor', 'hacapo', 'r4t3', ] cols_person_details = ['age', 'agesq', 'female', 'male',] cols_SQ = ['SQBage', 'SQBdependency', 'SQBedjefe', 'SQBescolari', 'SQBhogar_nin',             'SQBhogar_total', 'SQBmeaned', 'SQBovercrowding',] cols_water = ['abastaguadentro', 'abastaguafuera', 'abastaguano',]  cols_h = [ 'hhsize', 'hogar_adul', 'hogar_mayor', 'hogar_nin', 'hogar_total',] cols_r = ['r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3',] cols_tip = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',] cols_roof = ['techocane', 'techoentrepiso', 'techootro', 'techozinc',] cols_floor = ['pisocemento', 'pisomadera', 'pisomoscer', 'pisonatur', 'pisonotiene', 'pisoother',] cols_sanitary = [ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6',] cols_parents = [ 'parentesco1', 'parentesco10', 'parentesco11', 'parentesco12', 'parentesco2', 'parentesco3',                 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9',] cols_outside_wall = [ 'paredblolad', 'pareddes', 'paredfibras', 'paredmad', 'paredother',                'paredpreb', 'paredzinc', 'paredzocalo',] cols_instlevel = [ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6',                   'instlevel7', 'instlevel8', 'instlevel9',] cols_lugar = [ 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6',] cols_estadoc = [ 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4',                  'estadocivil5', 'estadocivil6', 'estadocivil7',] cols_elim = ['elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6',] cols_energ = ['energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4',] cols_eviv = [ 'eviv1', 'eviv2', 'eviv3',] cols_etech = [ 'etecho1', 'etecho2', 'etecho3',] cols_pared = [ 'epared1', 'epared2', 'epared3',] cols_unknown = [ 'dis', 'escolari', 'meaneduc',                  'overcrowding', 'rez_esc', 'tamhog', 'tamviv', ] cols_elec = ['coopele', 'noelec', 'planpri', 'public',]  total_features = cols_electronics+cols_house_details+cols_person_details+\ cols_SQ+cols_water+cols_h+cols_r+cols_tip+cols_roof+\ cols_floor+cols_sanitary+cols_parents+cols_outside_wall+\ cols_instlevel+cols_lugar+cols_estadoc+cols_elim+cols_energ+\ cols_eviv+cols_etech+cols_pared+cols_unknown+cols_elec  len(total_features)"
df[cols_electronics].plot.area(),0,not_existent,df[cols_electronics].plot.area()
df['Target'].unique(),0,not_existent,df['Target'].unique()
ASSIGN = cols_electronics.append('Target') df[cols_electronics].corr(),1,not_existent,cols_electronics_target = cols_electronics.append('Target') df[cols_electronics].corr()
CHECKPOINT cols_electronics.remove('Target') cols_electronics,1,not_existent,cols_electronics.remove('Target') cols_electronics
df.groupby('Target')[cols_electronics].sum(),0,not_existent,df.groupby('Target')[cols_electronics].sum()
df['tamhog'].unique(),0,not_existent,df['tamhog'].unique()
"df[['tamhog','r4t3', 'tamviv']].corr()",0,not_existent,"# high correlation between  # no. of persons in the household, # persons living in the household  # and size of the household # we can use any one...!! df[['tamhog','r4t3', 'tamviv']].corr()"
"df[['r4t3','tamviv']].corr()",0,not_existent,"df[['r4t3','tamviv']].corr()"
total_features.remove('r4t3') total_features.remove('tamhog') total_features.remove('tamviv') len(total_features),1,not_existent,total_features.remove('r4t3') total_features.remove('tamhog') total_features.remove('tamviv')  len(total_features)
df['escolari'].unique(),0,not_existent,df['escolari'].unique()
df['escolari'].hist(),0,not_existent,df['escolari'].hist()
df['escolari'].describe(),0,not_existent,df['escolari'].describe()
df['escolari'].plot.line(),0,not_existent,df['escolari'].plot.line()
sns.kdeplot(df.escolari),0,not_existent,sns.kdeplot(df.escolari)
ASSIGN = df[num_cols].corr(),1,not_existent,correlations = df[num_cols].corr()
"ASSIGN = np.zeros_like(correlations, dtype=np.bool) ASSIGN[np.triu_indices_from(ASSIGN)] = True ASSIGN = plt.subplots(figsize=(17, 13)) ASSIGN = sns.diverging_palette(220, 10, as_cmap=True) sns.heatmap(correlations, ASSIGN=ASSIGN, ASSIGN=ASSIGN, vmax=.3, center=0, ASSIGN=True, linewidths=.5, cbar_kws={""shrink"": .5})",0,not_existent,"# correlation heatmap masking mask = np.zeros_like(correlations, dtype=np.bool) mask[np.triu_indices_from(mask)] = True  f, ax = plt.subplots(figsize=(17, 13)) cmap = sns.diverging_palette(220, 10, as_cmap=True)  sns.heatmap(correlations, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={""shrink"": .5})"
"ASSIGN = df[num_cols].corrwith(df.escolari, axis=0)",1,not_existent,"# difficult to look into the above one es_corr = df[num_cols].corrwith(df.escolari, axis=0)"
"CHECKPOINT for x,y in zip(num_cols, list(es_corr)): if (y >= 0.75) or (y < -0.6): print(x,y)",0,not_existent,"for x,y in zip(num_cols, list(es_corr)):     if (y >= 0.75) or (y < -0.6):         print(x,y)"
"ASSIGN = df[num_cols].corrwith(df.SQBescolari, axis=0)",1,not_existent,"sqbes_corr = df[num_cols].corrwith(df.SQBescolari, axis=0)"
"CHECKPOINT for x,y in zip(num_cols, list(sqbes_corr)): if (y >= 0.5) or (y < -0.6): print(x,y)",0,not_existent,"for x,y in zip(num_cols, list(sqbes_corr)):     if (y >= 0.5) or (y < -0.6):         print(x,y)"
total_features.remove('escolari') len(total_features),1,not_existent,total_features.remove('escolari') len(total_features)
df.loc[df.Target == 1].groupby('overcrowding').SQBescolari.value_counts().unstack().plot.bar(),0,not_existent,df.loc[df.Target == 1].groupby('overcrowding').SQBescolari.value_counts().unstack().plot.bar()
df['overcrowding'].hist(),0,not_existent,df['overcrowding'].hist()
df['overcrowding'].unique(),0,not_existent,df['overcrowding'].unique()
"df.plot.scatter(x='Target', y='overcrowding')",0,not_existent,"df.plot.scatter(x='Target', y='overcrowding')"
df.groupby('Target').overcrowding.value_counts().unstack().plot.bar(),0,not_existent,df.groupby('Target').overcrowding.value_counts().unstack().plot.bar()
df['Target'].describe(),0,not_existent,df['Target'].describe()
df['Target'].hist(),0,not_existent,df['Target'].hist()
CHECKPOINT nan_cols,0,not_existent,nan_cols
df[nan_cols].corr(),0,not_existent,# filling missing values  df[nan_cols].corr()
"CHECKPOINT for col in nan_cols: if col != 'v2a1': print(col, df[col].unique())",0,not_existent,"for col in nan_cols:     if col != 'v2a1':         print(col, df[col].unique())"
"sns.regplot(df['meaneduc'],df['SQBmeaned'], order=2)",0,not_existent,"# there's a clear quadratic relation between meaneduc and SQBmeaned # hence, we can ignore either one of these..say, meaneduc sns.regplot(df['meaneduc'],df['SQBmeaned'], order=2)"
"df['meaneduc'].fillna(0, inplace=True) df['SQBmeaned'].fillna(0, inplace=True)",1,not_existent,"# filling na values in meaneduc and SQBmeaned df['meaneduc'].fillna(0, inplace=True) df['SQBmeaned'].fillna(0, inplace=True)"
CHECKPOINT total_features.remove('meaneduc') total_features,0,not_existent,total_features.remove('meaneduc')  total_features
"df[['v18q','v18q1','idhogar']].loc[df.v18q1.isna()].describe()",0,not_existent,"# we can fill v18q1 (household tablets) with 0 as individual tablet count is 0 for all such columns df[['v18q','v18q1','idhogar']].loc[df.v18q1.isna()].describe()"
df['v18q1'] = df['v18q'].groupby(df['idhogar']).transform('sum'),1,not_existent,df['v18q1'] = df['v18q'].groupby(df['idhogar']).transform('sum')
df.sample(7),0,not_existent,df.sample(7)
"ASSIGN = pd.DataFrame(df.isnull().sum()) ASSIGN.loc[(ASSIGN.loc[:, ASSIGN.dtypes != object] != 0).any(1)]",1,not_existent,"ff = pd.DataFrame(df.isnull().sum()) ff.loc[(ff.loc[:, ff.dtypes != object] != 0).any(1)]"
df['rez_esc'].describe(),0,not_existent,# rez_esc - years behind in school df['rez_esc'].describe()
df['rez_esc'].isnull().sum(),0,not_existent,df['rez_esc'].isnull().sum()
len(df) - df['rez_esc'].isnull().sum(),0,not_existent,# only these many rows has values for years behind school len(df) - df['rez_esc'].isnull().sum()
df['v2a1'].isnull().sum(),0,not_existent,df['v2a1'].isnull().sum()
len(df) - df['v2a1'].isnull().sum(),0,not_existent,# only these many rows has values for income len(df) - df['v2a1'].isnull().sum()
"len(df.loc[(df.v2a1 >= 0)]), len(df.loc[(df.rez_esc >= 0)])",0,not_existent,"# number of rows where income and rez_esc has values len(df.loc[(df.v2a1 >= 0)]), len(df.loc[(df.rez_esc >= 0)])"
len(df.loc[(df.v2a1 >= 0) & (df.rez_esc >= 0)]),1,not_existent,# how many rows with nan values for both income and rez_esc  len(df.loc[(df.v2a1 >= 0) & (df.rez_esc >= 0)])
len(df.loc[(df.v2a1 >= 0) | (df.rez_esc >= 0)]),1,not_existent,# how many rows with nan values for either income or rez_esc  len(df.loc[(df.v2a1 >= 0) | (df.rez_esc >= 0)])
df['rez_esc'].hist(),0,not_existent,df['rez_esc'].hist()
"df[['rez_esc','v2a1']].corr()",0,not_existent,"df[['rez_esc','v2a1']].corr()"
"df[['Target','rez_esc']].corr()",0,not_existent,"df[['Target','rez_esc']].corr()"
"df[['Target','rez_esc']].fillna(0).corr()",0,not_existent,"df[['Target','rez_esc']].fillna(0).corr()"
"df[['v2a1','Target']].corr()",0,not_existent,"df[['v2a1','Target']].corr()"
"df[['v2a1','Target']].fillna(0).corr()",0,not_existent,"df[['v2a1','Target']].fillna(0).corr()"
df['rez_esc'].unique(),0,not_existent,df['rez_esc'].unique()
"plt.figure(figsize=(13,7)) sns.kdeplot(df['rez_esc']) sns.kdeplot(df['rez_esc'].fillna(0))",0,not_existent,"plt.figure(figsize=(13,7)) sns.kdeplot(df['rez_esc']) sns.kdeplot(df['rez_esc'].fillna(0))"
"plt.figure(figsize=(13,7)) sns.kdeplot(df['v2a1']) sns.kdeplot(df['v2a1'].fillna(0))",0,not_existent,"plt.figure(figsize=(13,7)) sns.kdeplot(df['v2a1']) sns.kdeplot(df['v2a1'].fillna(0))"
"ASSIGN = df['rez_esc'], df['v2a1'] plt.figure(figsize=(23,17)) ASSIGN = sns.jointplot(x, y, data=df, kind=""kde"", color=""m"") ASSIGN.plot_joint(plt.scatter, c=""w"", s=30, linewidth=1, marker=""+"") ASSIGN.ax_joint.collections[0].set_alpha(0) ASSIGN.set_axis_labels(""$X$"", ""$Y$"")",0,not_existent,"x, y = df['rez_esc'], df['v2a1'] plt.figure(figsize=(23,17)) g = sns.jointplot(x, y, data=df, kind=""kde"", color=""m"") g.plot_joint(plt.scatter, c=""w"", s=30, linewidth=1, marker=""+"") g.ax_joint.collections[0].set_alpha(0) g.set_axis_labels(""$X$"", ""$Y$"")"
"ASSIGN = df['rez_esc'].fillna(0), df['v2a1'].fillna(0) sns.jointplot(ASSIGN, data=df, kind=""kde"")",0,not_existent,"x, y = df['rez_esc'].fillna(0), df['v2a1'].fillna(0) sns.jointplot(x, y, data=df, kind=""kde"")"
"df['rez_esc'].fillna(0, inplace=True)",1,not_existent,"df['rez_esc'].fillna(0, inplace=True)"
"ASSIGN = df['Target'], df['v2a1'] plt.figure(figsize=(23,17)) ASSIGN = sns.jointplot(x, y, data=df, kind=""kde"", color=""m"") ASSIGN.plot_joint(plt.scatter, c=""w"", s=30, linewidth=1, marker=""+"") ASSIGN.ax_joint.collections[0].set_alpha(0) ASSIGN.set_axis_labels(""$X$"", ""$Y$"")",0,not_existent,"x, y = df['Target'], df['v2a1'] plt.figure(figsize=(23,17)) g = sns.jointplot(x, y, data=df, kind=""kde"", color=""m"") g.plot_joint(plt.scatter, c=""w"", s=30, linewidth=1, marker=""+"") g.ax_joint.collections[0].set_alpha(0) g.set_axis_labels(""$X$"", ""$Y$"")"
"ASSIGN = df['Target'], df['v2a1'].fillna(0) plt.figure(figsize=(23,17)) ASSIGN = sns.jointplot(x, y, data=df, kind=""kde"", color=""m"") ASSIGN.plot_joint(plt.scatter, c=""w"", s=30, linewidth=1, marker=""+"") ASSIGN.ax_joint.collections[0].set_alpha(0) ASSIGN.set_axis_labels(""$X$"", ""$Y$"")",0,not_existent,"x, y = df['Target'], df['v2a1'].fillna(0) plt.figure(figsize=(23,17)) g = sns.jointplot(x, y, data=df, kind=""kde"", color=""m"") g.plot_joint(plt.scatter, c=""w"", s=30, linewidth=1, marker=""+"") g.ax_joint.collections[0].set_alpha(0) g.set_axis_labels(""$X$"", ""$Y$"")"
df['Target'].value_counts(),0,not_existent,df['Target'].value_counts()
df.groupby('Target').count()['v2a1'],0,not_existent,df.groupby('Target').count()['v2a1']
"ASSIGN = plt.subplots(figsize=(15,7)) df.groupby('Target').count()['v2a1'].plot(ax=ax)",0,not_existent,"fig, ax = plt.subplots(figsize=(15,7)) df.groupby('Target').count()['v2a1'].plot(ax=ax)"
"ASSIGN = plt.subplots(figsize=(15,7)) df.fillna(0).groupby('Target').count()['v2a1'].plot(ax=ax)",0,not_existent,"fig, ax = plt.subplots(figsize=(15,7)) df.fillna(0).groupby('Target').count()['v2a1'].plot(ax=ax)"
"ASSIGN = plt.subplots(figsize=(15,7)) df.groupby(['Target','hhsize']).count()['v2a1'].unstack().plot(ax=ax)",0,not_existent,"fig, ax = plt.subplots(figsize=(15,7)) df.groupby(['Target','hhsize']).count()['v2a1'].unstack().plot(ax=ax)"
"ASSIGN = plt.subplots(figsize=(15,7)) df.fillna(0).groupby(['Target','hhsize']).count()['v2a1'].unstack().plot(ax=ax)",0,not_existent,"fig, ax = plt.subplots(figsize=(15,7)) df.fillna(0).groupby(['Target','hhsize']).count()['v2a1'].unstack().plot(ax=ax)"
df['hhsize'].value_counts(),0,not_existent,df['hhsize'].value_counts()
df['hogar_total'].value_counts(),0,not_existent,df['hogar_total'].value_counts()
total_features.remove('hogar_total') len(total_features),1,not_existent,"# use hhsize, ignore 'hogar_total', total_features.remove('hogar_total')  len(total_features)"
"df[['hhsize','hogar_adul']].corr()",0,not_existent,"df[['hhsize','hogar_adul']].corr()"
"df[['hhsize','Target']].corr()",0,not_existent,"df[['hhsize','Target']].corr()"
"df[['Target','hogar_adul']].corr()",0,not_existent,"df[['Target','hogar_adul']].corr()"
sns.kdeplot(df['hogar_adul']) sns.kdeplot(df['hhsize']),0,not_existent,sns.kdeplot(df['hogar_adul']) sns.kdeplot(df['hhsize']) 
sns.kdeplot(df['hogar_total']) sns.kdeplot(df['hogar_adul']),0,not_existent,sns.kdeplot(df['hogar_total']) sns.kdeplot(df['hogar_adul'])
"max(df['hogar_adul']), max(df['hogar_total'])",0,not_existent,"max(df['hogar_adul']), max(df['hogar_total'])"
"df.groupby('idhogar').sum()[['hogar_adul','hogar_total']].sample(10).plot.bar()",0,not_existent,"df.groupby('idhogar').sum()[['hogar_adul','hogar_total']].sample(10).plot.bar()"
sns.kdeplot(df['hogar_total']) sns.kdeplot(df['hogar_nin']),0,not_existent,sns.kdeplot(df['hogar_total']) sns.kdeplot(df['hogar_nin'])
df['male'].value_counts(),0,not_existent,df['male'].value_counts()
df['female'].value_counts(),0,not_existent,df['female'].value_counts()
total_features.remove('female') len(total_features),1,not_existent,# removing female total_features.remove('female')  len(total_features)
df['r4t3'].value_counts(),0,not_existent,df['r4t3'].value_counts()
df['tamhog'].value_counts(),0,not_existent,df['tamhog'].value_counts()
df['tamviv'].value_counts(),0,not_existent,df['tamviv'].value_counts()
"plt.figure(figsize=(17,13)) sns.kdeplot(df['tamviv']) sns.kdeplot(df['tamhog']) sns.kdeplot(df['r4t3']) sns.kdeplot(df['hhsize']) sns.kdeplot(df['hogar_total'])",0,not_existent,"plt.figure(figsize=(17,13)) sns.kdeplot(df['tamviv']) sns.kdeplot(df['tamhog']) sns.kdeplot(df['r4t3']) sns.kdeplot(df['hhsize']) sns.kdeplot(df['hogar_total']) #sns.kdeplot(df['hogar_adul']) "
total_features.remove('r4t3'),1,not_existent,"# removing 'r4t3', as 'hhsize' is of almost same distribution total_features.remove('r4t3')"
len(total_features),0,not_existent,len(total_features)
df['dependency'].describe(),0,not_existent,df['dependency'].describe()
df['dependency'].value_counts(),0,not_existent,df['dependency'].value_counts()
df['SQBdependency'].value_counts(),0,not_existent,df['SQBdependency'].value_counts()
df['SQBdependency'].describe(),0,not_existent,df['SQBdependency'].describe()
CHECKPOINT cat_cols,0,not_existent,cat_cols
df['edjefe'].describe(),0,not_existent,df['edjefe'].describe()
df['edjefa'].describe(),0,not_existent,df['edjefa'].describe()
df['edjefe'].value_counts(),0,not_existent,df['edjefe'].value_counts()
df['edjefa'].value_counts(),0,not_existent,df['edjefa'].value_counts()
"df.loc[df.edjefa == 'yes', 'edjefa'] = 1 df.loc[df.edjefa == 'no', 'edjefa'] = 0",1,not_existent,"df.loc[df.edjefa == 'yes', 'edjefa'] = 1 df.loc[df.edjefa == 'no', 'edjefa'] = 0 "
"df.loc[df.edjefe == 'yes', 'edjefe'] = 1 df.loc[df.edjefe == 'no', 'edjefe'] = 0",1,not_existent,"df.loc[df.edjefe == 'yes', 'edjefe'] = 1 df.loc[df.edjefe == 'no', 'edjefe'] = 0 "
"df[['edjefa','edjefe']].describe()",0,not_existent,"df[['edjefa','edjefe']].describe()"
"df[['edjefa','edjefe']] = df[['edjefa','edjefe']].apply(pd.to_numeric)",1,not_existent,"df[['edjefa','edjefe']] = df[['edjefa','edjefe']].apply(pd.to_numeric) "
"CHECKPOINT df[['edjefa','edjefe']].dtypes",0,not_existent,"df[['edjefa','edjefe']].dtypes"
total_features.append('edjefa') total_features.append('edjefe'),1,not_existent,total_features.append('edjefa') total_features.append('edjefe')
CHECKPOINT cols_water,0,not_existent,cols_water
df[cols_water].describe(),0,not_existent,df[cols_water].describe()
df[cols_water].corr(),0,not_existent,df[cols_water].corr()
df['abastaguadentro'].value_counts(),0,not_existent,df['abastaguadentro'].value_counts()
df['abastaguafuera'].value_counts(),0,not_existent,df['abastaguafuera'].value_counts()
df['abastaguano'].value_counts(),0,not_existent,df['abastaguano'].value_counts()
CHECKPOINT ASSIGN = df.groupby('Target')[cols_water].sum().reset_index() df_water_target,1,not_existent,df_water_target = df.groupby('Target')[cols_water].sum().reset_index() df_water_target
CHECKPOINT 722+1496+1133+5844,0,not_existent,722+1496+1133+5844
df_water_target.corr(),0,not_existent,df_water_target.corr()
len(total_features),0,not_existent,len(total_features) 
total_features.remove('abastaguano') total_features.remove('abastaguafuera') len(total_features),1,not_existent,total_features.remove('abastaguano') total_features.remove('abastaguafuera')  len(total_features)
df['pisocemento'].value_counts(),0,not_existent,"# cols_floor #  # ['pisocemento', 'pisomadera', 'pisomoscer', 'pisonatur', 'pisonotiene', 'pisoother',]  df['pisocemento'].value_counts()"
CHECKPOINT ASSIGN = df.groupby('Target')[cols_floor].sum().reset_index() df_floor_target,1,not_existent,df_floor_target = df.groupby('Target')[cols_floor].sum().reset_index() df_floor_target
total_features.remove('pisonatur') total_features.remove('pisonotiene') total_features.remove('pisoother') len(total_features),1,not_existent,# removing these features -> inc by 0.002 total_features.remove('pisonatur') total_features.remove('pisonotiene') total_features.remove('pisoother')  len(total_features)
CHECKPOINT ASSIGN = df.groupby('Target')[cols_outside_wall].sum().reset_index() df_wall_target,1,not_existent,df_wall_target = df.groupby('Target')[cols_outside_wall].sum().reset_index() df_wall_target
sns.kdeplot(df['paredblolad']),0,not_existent,sns.kdeplot(df['paredblolad']) 
sns.kdeplot(df['paredpreb']) sns.kdeplot(df['paredmad']),0,not_existent,sns.kdeplot(df['paredpreb']) sns.kdeplot(df['paredmad']) 
sns.kdeplot(df['paredmad']) sns.kdeplot(df['paredzocalo']),0,not_existent,sns.kdeplot(df['paredmad']) sns.kdeplot(df['paredzocalo']) 
sns.kdeplot(df['paredpreb']) sns.kdeplot(df['paredmad']) sns.kdeplot(df['paredzocalo']),0,not_existent,sns.kdeplot(df['paredpreb']) sns.kdeplot(df['paredmad']) sns.kdeplot(df['paredzocalo']) 
CHECKPOINT ASSIGN = df.groupby('Target')[cols_roof].sum().reset_index() df_roof_target,1,not_existent,df_roof_target = df.groupby('Target')[cols_roof].sum().reset_index() df_roof_target
sns.kdeplot(df['techozinc']),0,not_existent,sns.kdeplot(df['techozinc'])
sns.kdeplot(df['techozinc']) sns.kdeplot(df['techoentrepiso']),0,not_existent,sns.kdeplot(df['techozinc']) sns.kdeplot(df['techoentrepiso']) 
sns.kdeplot(df['techoentrepiso']) sns.kdeplot(df['techocane']),0,not_existent,sns.kdeplot(df['techoentrepiso']) sns.kdeplot(df['techocane'])
CHECKPOINT ASSIGN = df.groupby('Target')[cols_sanitary].sum().reset_index() df_sani_target,1,not_existent,"# [ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6',]  df_sani_target = df.groupby('Target')[cols_sanitary].sum().reset_index() df_sani_target"
sns.kdeplot(df['sanitario1']) sns.kdeplot(df['sanitario6']),0,not_existent,sns.kdeplot(df['sanitario1']) sns.kdeplot(df['sanitario6']) 
sns.kdeplot(df['sanitario3']) sns.kdeplot(df['sanitario2']),0,not_existent,sns.kdeplot(df['sanitario3']) sns.kdeplot(df['sanitario2'])
CHECKPOINT ASSIGN = df.groupby('Target')[cols_tip].sum().reset_index() df_tipo_target,1,not_existent,"# cols_tip  # ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',]  df_tipo_target = df.groupby('Target')[cols_tip].sum().reset_index() df_tipo_target"
sns.kdeplot(df['tipovivi2']),0,not_existent,sns.kdeplot(df['tipovivi2']) 
sns.kdeplot(df['tipovivi1']) sns.kdeplot(df['tipovivi3']),0,not_existent,sns.kdeplot(df['tipovivi1']) sns.kdeplot(df['tipovivi3']) 
sns.kdeplot(df['tipovivi5']) sns.kdeplot(df['tipovivi4']),0,not_existent,sns.kdeplot(df['tipovivi5']) sns.kdeplot(df['tipovivi4']) 
df['v2a1'].isna().sum(),0,not_existent,df['v2a1'].isna().sum()
df['tipovivi3'].value_counts(),0,not_existent,df['tipovivi3'].value_counts()
df.loc[(df['v2a1'].isna()) & (df.tipovivi3 == 1)],0,not_existent,df.loc[(df['v2a1'].isna()) & (df.tipovivi3 == 1)]
df['v2a1'].loc[df.parentesco1 == 1].plot.line(),0,not_existent,df['v2a1'].loc[df.parentesco1 == 1].plot.line()
df['v2a1'].loc[df.parentesco1 == 1].plot.hist(),0,not_existent,df['v2a1'].loc[df.parentesco1 == 1].plot.hist()
"df['v2a1'].loc[df.parentesco1 == 1].mean(), df['v2a1'].loc[df.parentesco1 == 1].max(), df['v2a1'].loc[df.parentesco1 == 1].min()",0,not_existent,"df['v2a1'].loc[df.parentesco1 == 1].mean(), df['v2a1'].loc[df.parentesco1 == 1].max(), df['v2a1'].loc[df.parentesco1 == 1].min()"
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (df.v2a1.isna())].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (df.v2a1.isna())].describe(include='all')"
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')"
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()"
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()"
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')"
"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 != 1].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 != 1].describe(include='all')"
"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 == 1].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 == 1].describe(include='all')"
"df['v2a1'].fillna(120000, inplace=True)",1,not_existent,"df['v2a1'].fillna(120000, inplace=True)"
'Target' in total_features,0,not_existent,'Target' in total_features
"ASSIGN = df[total_features], df['Target']",1,not_existent,"X, y = df[total_features], df['Target']"
"ASSIGN = 42 ASSIGN = 0.3 X_train, X_test, y_train, y_test = train_test_split(X, y, ASSIGN=ASSIGN, random_state=ASSIGN)",1,not_existent,"#Split the dataset into train and Test seed = 42 test_size = 0.3 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)"
CHECKPOINT ASSIGN = xgb.XGBClassifier(n_jobs=n_jobs) model1,0,not_existent,#Train the XGboost Model for Classification model1 = xgb.XGBClassifier(n_jobs=n_jobs) model1
"ASSIGN = model1.fit(X_train, y_train)",0,not_existent,"train_model1 = model1.fit(X_train, y_train)"
"CHECKPOINT ASSIGN = xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, subsample=0.5, n_jobs=n_jobs) model2",0,not_existent,"model2 = xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, subsample=0.5, n_jobs=n_jobs) model2"
"ASSIGN = model2.fit(X_train, y_train)",0,not_existent,"train_model2 = model2.fit(X_train, y_train)"
"CHECKPOINT ASSIGN = train_model1.predict(X_test) ASSIGN = train_model2.predict(X_test) print('Model 1 XGboost Report %r' % (classification_report(y_test, ASSIGN))) print('Model 2 XGboost Report %r' % (classification_report(y_test, ASSIGN)))",0,not_existent,"# predictions pred1 = train_model1.predict(X_test) pred2 = train_model2.predict(X_test)  print('Model 1 XGboost Report %r' % (classification_report(y_test, pred1))) print('Model 2 XGboost Report %r' % (classification_report(y_test, pred2)))"
"CHECKPOINT print( % (accuracy_score(y_test, pred1) * 100)) print( % (accuracy_score(y_test, pred2) * 100))",0,not_existent,"print(""Accuracy for model 1: %.2f"" % (accuracy_score(y_test, pred1) * 100)) print(""Accuracy for model 2: %.2f"" % (accuracy_score(y_test, pred2) * 100))"
"ASSIGN = xgb.XGBClassifier( ASSIGN =0.1, ASSIGN=1000, ASSIGN=5, ASSIGN=1, ASSIGN=0, ASSIGN=0.8, ASSIGN=0.8, ASSIGN= 'binary:logistic', ASSIGN=ASSIGN, ASSIGN=1, ASSIGN=27)",0,not_existent,"#Let's do a little Gridsearch, Hyperparameter Tunning model3 = xgb.XGBClassifier(  learning_rate =0.1,  n_estimators=1000,  max_depth=5,  min_child_weight=1,  gamma=0,  subsample=0.8,  colsample_bytree=0.8,  objective= 'binary:logistic',  n_jobs=n_jobs,  scale_pos_weight=1,  seed=27)"
"CHECKPOINT ASSIGN = model3.fit(X_train, y_train) ASSIGN = train_model3.predict(X_test) print( % (accuracy_score(y_test, ASSIGN) * 100))",0,not_existent,"train_model3 = model3.fit(X_train, y_train) pred3 = train_model3.predict(X_test) print(""Accuracy for model 3: %.2f"" % (accuracy_score(y_test, pred3) * 100))"
"CHECKPOINT print('Model 3 XGboost Report %r' % (classification_report(y_test, pred3)))",0,not_existent,"print('Model 3 XGboost Report %r' % (classification_report(y_test, pred3)))"
gc.collect(),0,not_existent,gc.collect()
"ASSIGN = { 'n_estimators': [100], 'max_depth': [6, 9], 'subsample': [0.9, 1.0], 'colsample_bytree': [0.9, 1.0], }",1,not_existent,"parameters = {     'n_estimators': [100],     'max_depth': [6, 9],     'subsample': [0.9, 1.0],     'colsample_bytree': [0.9, 1.0], } "
"CHECKPOINT ASSIGN = GridSearchCV(model3, ASSIGN=n_jobs, ASSIGN=""neg_log_loss"", ASSIGN=3) grid",0,not_existent,"grid = GridSearchCV(model3,                     parameters, n_jobs=n_jobs,                     scoring=""neg_log_loss"",                     cv=3) grid"
"ASSIGN = plt.subplots(figsize=(23, 17)) plot_importance(model3, ax=ax)",0,not_existent,"fig, ax = plt.subplots(figsize=(23, 17)) plot_importance(model3, ax=ax)"
"ASSIGN = ['estadocivil1','instlevel9','techocane','parentesco10','v14a', 'parentesco11','parentesco5','paredother','parentesco7','noelec', 'elimbasu4','elimbasu6']",1,not_existent,"less_imp_features = ['estadocivil1','instlevel9','techocane','parentesco10','v14a',                      'parentesco11','parentesco5','paredother','parentesco7','noelec',                      'elimbasu4','elimbasu6'] "
len(total_features),0,not_existent,# before removing less important features len(total_features)
for f in less_imp_features: if f in total_features: total_features.remove(f) len(total_features),1,not_existent,for f in less_imp_features:     if f in total_features:         total_features.remove(f)  len(total_features)
"ASSIGN = 43 ASSIGN = 0.3 X_train, X_test, y_train, y_test = train_test_split(X, y, ASSIGN=ASSIGN, random_state=ASSIGN)",1,not_existent,"#Split the dataset into train and Test seed = 43 test_size = 0.3 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)"
"CHECKPOINT ASSIGN = model3.fit(X_train, y_train) ASSIGN = train_model5.predict(X_test) print( % (accuracy_score(y_test, ASSIGN) * 100))",0,not_existent,"train_model5 = model3.fit(X_train, y_train) pred5 = train_model5.predict(X_test) print(""Accuracy for model 5: %.2f"" % (accuracy_score(y_test, pred5) * 100))"
"CHECKPOINT print('Model 5 XGboost Report %r' % (classification_report(y_test, pred5)))",0,not_existent,"print('Model 5 XGboost Report %r' % (classification_report(y_test, pred5)))"
"ASSIGN = train_model5.fit(X, y)",0,not_existent,"train_model6 = train_model5.fit(X, y) "
CHECKPOINT train_model6,0,not_existent,train_model6
CHECKPOINT ASSIGN = sort(model3.feature_importances_) thresholds,1,not_existent,thresholds = sort(model3.feature_importances_) thresholds
CHECKPOINT thresholds.shape,0,not_existent,thresholds.shape
CHECKPOINT np.unique(thresholds).shape,0,not_existent,np.unique(thresholds).shape
CHECKPOINT ASSIGN = RandomForestClassifier(n_jobs=n_jobs) model4,0,not_existent,model4 = RandomForestClassifier(n_jobs=n_jobs) model4
"CHECKPOINT ASSIGN = model4.fit(X_train, y_train) ASSIGN = train_model4.predict(X_test) print( % (accuracy_score(y_test, ASSIGN) * 100))",0,not_existent,"train_model4 = model4.fit(X_train, y_train) pred4 = train_model4.predict(X_test) print(""Accuracy for model 4: %.2f"" % (accuracy_score(y_test, pred4) * 100)) "
"CHECKPOINT print('Model 4 XGboost Report %r' % (classification_report(y_test, pred4)))",0,not_existent,"print('Model 4 XGboost Report %r' % (classification_report(y_test, pred4)))"
"confusion_matrix(y_test, pred4)",0,not_existent,"confusion_matrix(y_test, pred4)"
ASSIGN = pd.read_csv('..path'),0,not_existent,df_test = pd.read_csv('../input/test.csv')
len(df_test),0,not_existent,len(df_test)
df_test.sample(10),0,not_existent,df_test.sample(10)
"CHECKPOINT df_test.loc[df_test.edjefa == 'yes', 'edjefa'] = 1 df_test.loc[df_test.edjefa == 'no', 'edjefa'] = 0 df_test.loc[df_test.edjefe == 'yes', 'edjefe'] = 1 df_test.loc[df_test.edjefe == 'no', 'edjefe'] = 0 df_test[['edjefa','edjefe']] = df_test[['edjefa','edjefe']].apply(pd.to_numeric) df_test[['edjefa','edjefe']].dtypes",1,not_existent,"df_test.loc[df_test.edjefa == 'yes', 'edjefa'] = 1 df_test.loc[df_test.edjefa == 'no', 'edjefa'] = 0  df_test.loc[df_test.edjefe == 'yes', 'edjefe'] = 1 df_test.loc[df_test.edjefe == 'no', 'edjefe'] = 0 df_test[['edjefa','edjefe']] = df_test[['edjefa','edjefe']].apply(pd.to_numeric) df_test[['edjefa','edjefe']].dtypes"
ASSIGN = df_test[total_features],1,not_existent,X_actual_test = df_test[total_features]
CHECKPOINT X_actual_test.shape,0,not_existent,X_actual_test.shape
CHECKPOINT ASSIGN = train_model6.predict(X_actual_test) pred_actual,0,not_existent,pred_actual = train_model6.predict(X_actual_test) pred_actual
CHECKPOINT pred_actual.shape,0,not_existent,pred_actual.shape
"ASSIGN = pd.DataFrame(df['Id'], pred_actual).reset_index() ASSIGN.columns = ['Target','Id']",1,not_existent,"df_final = pd.DataFrame(df['Id'], pred_actual).reset_index() df_final.columns = ['Target','Id'] "
CHECKPOINT ASSIGN = df_final.columns.tolist() cols,1,not_existent,cols = df_final.columns.tolist() cols
CHECKPOINT ASSIGN = ASSIGN[-1:] + ASSIGN[:-1] cols,1,not_existent,cols = cols[-1:] + cols[:-1] cols 
ASSIGN = ASSIGN[cols] ASSIGN.head(7),1,not_existent,df_final = df_final[cols] df_final.head(7)
df_final.index.name = None df_final.head(7),1,not_existent,df_final.index.name = None df_final.head(7)
df_final['Target'].value_counts(),0,not_existent,df_final['Target'].value_counts() 
df_final[cols].sample(4),0,not_existent,df_final[cols].sample(4)
"df_final[cols].to_csv('sample_submission.csv', index=False)",0,not_existent,"df_final[cols].to_csv('sample_submission.csv', index=False)"
os.listdir('..path'),0,not_existent,os.listdir('../input/')
SETUP,0,not_existent,"from mpl_toolkits.mplot3d import Axes3D
 from sklearn.preprocessing import StandardScaler
 import matplotlib.pyplot as plt # plotting
 import numpy as np # linear algebra
 import os # accessing directory structure
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
"
"CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"for dirname, _, filenames in os.walk('/kaggle/input'):
     for filename in filenames:
         print(os.path.join(dirname, filename))
"
"def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow): ASSIGN = df.ASSIGN() ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]] nRow, nCol = ASSIGN.shape ASSIGN = list(df) ASSIGN = (nCol + nGraphPerRow - 1) path plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k') for i in range(min(nCol, nGraphShown)): plt.subplot(ASSIGN, nGraphPerRow, i + 1) ASSIGN = df.iloc[:, i] if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)): ASSIGN = columnDf.value_counts() ASSIGN.plot.bar() else: ASSIGN.hist() plt.ylabel('counts') plt.xticks(rotation = 90) plt.title(f'{ASSIGN[i]} (column {i})') plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0) plt.show()",0,not_existent,"# Distribution graphs (histogram/bar graph) of column data
 def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):
     nunique = df.nunique()
     df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values
     nRow, nCol = df.shape
     columnNames = list(df)
     nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow
     plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')
     for i in range(min(nCol, nGraphShown)):
         plt.subplot(nGraphRow, nGraphPerRow, i + 1)
         columnDf = df.iloc[:, i]
         if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):
             valueCounts = columnDf.value_counts()
             valueCounts.plot.bar()
         else:
             columnDf.hist()
         plt.ylabel('counts')
         plt.xticks(rotation = 90)
         plt.title(f'{columnNames[i]} (column {i})')
     plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)
     plt.show()
"
"CHECKPOINT def plotCorrelationMatrix(df, graphWidth): ASSIGN = df.dataframeName ASSIGN = ASSIGN.dropna('columns') ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]] if ASSIGN.shape[1] < 2: print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2') return ASSIGN = df.ASSIGN() plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k') ASSIGN = plt.matshow(corr, fignum = 1) plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90) plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns) plt.gca().xaxis.tick_bottom() plt.colorbar(ASSIGN) plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15) plt.show()",0,not_existent,"# Correlation matrix
 def plotCorrelationMatrix(df, graphWidth):
     filename = df.dataframeName
     df = df.dropna('columns') # drop columns with NaN
     df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values
     if df.shape[1] < 2:
         print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')
         return
     corr = df.corr()
     plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')
     corrMat = plt.matshow(corr, fignum = 1)
     plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)
     plt.yticks(range(len(corr.columns)), corr.columns)
     plt.gca().xaxis.tick_bottom()
     plt.colorbar(corrMat)
     plt.title(f'Correlation Matrix for {filename}', fontsize=15)
     plt.show()
"
"def plotScatterMatrix(df, plotSize, textSize): ASSIGN = ASSIGN.select_dtypes(include =[np.number]) ASSIGN = ASSIGN.dropna('columns') ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]] ASSIGN = list(df) if len(ASSIGN) > 10: ASSIGN = ASSIGN[:10] ASSIGN = ASSIGN[columnNames] ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde') ASSIGN = df.corr().values for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)): ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize) plt.suptitle('Scatter and Density Plot') plt.show()",0,not_existent,"# Scatter and density plots
 def plotScatterMatrix(df, plotSize, textSize):
     df = df.select_dtypes(include =[np.number]) # keep only numerical columns
     # Remove rows and columns that would lead to df being singular
     df = df.dropna('columns')
     df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values
     columnNames = list(df)
     if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots
         columnNames = columnNames[:10]
     df = df[columnNames]
     ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')
     corrs = df.corr().values
     for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):
         ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)
     plt.suptitle('Scatter and Density Plot')
     plt.show()
"
SETUP,0,not_existent,"import pandas as pd
 import numpy as np
 
 from sklearn.model_selection import KFold
"
ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path'),0,not_existent,"train_data = pd.read_csv('../input/train.csv')
 test_data = pd.read_csv('../input/test.csv')"
"CHECKPOINT ASSIGN = train_data.columns[2:] ASSIGN = [] ASSIGN = [] for dtype, feature in zip(train_data.dtypes[2:], train_data.columns[2:]): ASSIGN == object: ASSIGN.append(feature) else: ASSIGN.append(feature) categorical_features",1,execute_result,"features = train_data.columns[2:]
 
 numeric_features = []
 categorical_features = []
 
 for dtype, feature in zip(train_data.dtypes[2:], train_data.columns[2:]):
     if dtype == object:
         #print(column)
         #print(train_data[column].describe())
         categorical_features.append(feature)
     else:
         numeric_features.append(feature)
 categorical_features"
"CHECKPOINT np.random.seed(13) def impact_coding(data, feature, target='y'): ''' In this implementation we get the values and the dictionary as two different steps. This is just because initially we were ignoring the dictionary as a result variable. In this implementation the KFolds use shuffling. If you want reproducibility the cv could be moved to a parameter. ''' ASSIGN = 20 ASSIGN = 10 ASSIGN = pd.Series() ASSIGN = data[target].mean() ASSIGN = KFold(n_splits=n_folds, shuffle=True) ASSIGN = pd.DataFrame() ASSIGN = 0 for infold, oof in ASSIGN.ASSIGN(data[feature]): ASSIGN = pd.Series() ASSIGN = KFold(n_splits=n_inner_folds, shuffle=True) ASSIGN = 0 ASSIGN = pd.DataFrame() ASSIGN = data.iloc[infold][target].mean() for infold_inner, oof_inner in ASSIGN.ASSIGN(data.iloc[infold]): ASSIGN = data.iloc[infold_inner].groupby(by=feature)[target].mean() ASSIGN = ASSIGN.append(data.iloc[infold].apply( lambda x: ASSIGN[x[feature]] if x[feature] in oof_mean.index else oof_default_inner_mean , axis=1)) ASSIGN = ASSIGN.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer') ASSIGN.fillna(value=ASSIGN, inplace=True) ASSIGN += 1 ASSIGN = ASSIGN.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer') ASSIGN.fillna(value=ASSIGN, inplace=True) ASSIGN += 1 ASSIGN = ASSIGN.append(data.iloc[oof].apply( lambda x: ASSIGN.loc[x[feature]].mean() if x[feature] in inner_oof_mean_cv.index else oof_default_mean , axis=1)) return ASSIGN, ASSIGN.mean(axis=1), ASSIGN ASSIGN = {} for f in categorical_features: print(.format(f)) train_data[""impact_encoded_{}"".format(f)], impact_coding_mapping, default_coding = impact_coding(train_data, f) ASSIGN[f] = (impact_coding_mapping, default_coding) ASSIGN = impact_coding_map[f] test_data[""impact_encoded_{}"".format(f)] = test_data.apply(lambda x: mapping[x[f]] if x[f] in mapping else default_mean , axis=1)",1,stream,"# This way we have randomness and are able to reproduce the behaviour within this cell.
 np.random.seed(13)
 
 def impact_coding(data, feature, target='y'):
     '''
     In this implementation we get the values and the dictionary as two different steps.
     This is just because initially we were ignoring the dictionary as a result variable.
     
     In this implementation the KFolds use shuffling. If you want reproducibility the cv 
     could be moved to a parameter.
     '''
     n_folds = 20
     n_inner_folds = 10
     impact_coded = pd.Series()
     
     oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)
     kf = KFold(n_splits=n_folds, shuffle=True)
     oof_mean_cv = pd.DataFrame()
     split = 0
     for infold, oof in kf.split(data[feature]):
             impact_coded_cv = pd.Series()
             kf_inner = KFold(n_splits=n_inner_folds, shuffle=True)
             inner_split = 0
             inner_oof_mean_cv = pd.DataFrame()
             oof_default_inner_mean = data.iloc[infold][target].mean()
             for infold_inner, oof_inner in kf_inner.split(data.iloc[infold]):
                 # The mean to apply to the inner oof split (a 1/n_folds % based on the rest)
                 oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean()
                 impact_coded_cv = impact_coded_cv.append(data.iloc[infold].apply(
                             lambda x: oof_mean[x[feature]]
                                       if x[feature] in oof_mean.index
                                       else oof_default_inner_mean
                             , axis=1))
 
                 # Also populate mapping (this has all group -> mean for all inner CV folds)
                 inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')
                 inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)
                 inner_split += 1
 
             # Also populate mapping
             oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')
             oof_mean_cv.fillna(value=oof_default_mean, inplace=True)
             split += 1
             
             impact_coded = impact_coded.append(data.iloc[oof].apply(
                             lambda x: inner_oof_mean_cv.loc[x[feature]].mean()
                                       if x[feature] in inner_oof_mean_cv.index
                                       else oof_default_mean
                             , axis=1))
 
     return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean
 
 # Apply the encoding to training and test data, and preserve the mapping
 impact_coding_map = {}
 for f in categorical_features:
     print(""Impact coding for {}"".format(f))
     train_data[""impact_encoded_{}"".format(f)], impact_coding_mapping, default_coding = impact_coding(train_data, f)
     impact_coding_map[f] = (impact_coding_mapping, default_coding)
     mapping, default_mean = impact_coding_map[f]
     test_data[""impact_encoded_{}"".format(f)] = test_data.apply(lambda x: mapping[x[f]]
                                                                          if x[f] in mapping
                                                                          else default_mean
                                                                , axis=1)"
"train_data[['y', 'X0'] + list(train_data.columns[-8:])]",0,execute_result,"train_data[['y', 'X0'] + list(train_data.columns[-8:])]"
"SETUP CHECKPOINT rcParams['figure.figsize'] = [10,5] plt.style.use('fivethirtyeight') sns.set_style('whitegrid') warnings.filterwarnings('ignore') pd.set_option('display.max_rows', 50) pd.set_option('display.max_columns', 50) pd.options.display.float_format = '{:.5f}'.format for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"# Setting package umum  import pandas as pd import pandas_profiling as pp import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import seaborn as sns from tqdm import tqdm_notebook as tqdm import time import tensorflow as tf %matplotlib inline  from matplotlib.pylab import rcParams # For every plotting cell use this # grid = gridspec.GridSpec(n_row,n_col) # ax = plt.subplot(grid[i]) # fig, axes = plt.subplots() rcParams['figure.figsize'] = [10,5] plt.style.use('fivethirtyeight')  sns.set_style('whitegrid')  import warnings warnings.filterwarnings('ignore') from tqdm import tqdm  pd.set_option('display.max_rows', 50) pd.set_option('display.max_columns', 50) pd.options.display.float_format = '{:.5f}'.format  import os for dirname, _, filenames in os.walk('/kaggle/input'):     for filename in filenames:         print(os.path.join(dirname, filename))"
"SETUP ASSIGN = Sequential([Dense(units=1, input_shape=[1])]) ASSIGN.compile(optimizer='sgd', loss='mean_squared_error') ASSIGN = np.array([-1, 0, 1, 2, 3, 4], dtype=float) ASSIGN = np.array([-3, -1, 1, 3, 5, 7], dtype=float) ASSIGN.fit(ASSIGN, ASSIGN, epochs=500)",0,not_existent,"### Try Keras from keras import Sequential from keras.layers import Dense  # Define model model = Sequential([Dense(units=1, input_shape=[1])])  # Compile model model.compile(optimizer='sgd', loss='mean_squared_error')  # Define data xs = np.array([-1, 0, 1, 2, 3, 4], dtype=float) ys = np.array([-3, -1, 1, 3, 5, 7], dtype=float)  # Train model model.fit(xs, ys, epochs=500)"
model.predict([6]),0,not_existent,### Predict model.predict([6])
"SETUP ASSIGN = Sequential() ASSIGN.add(Dense(units=1, input_shape=[1])) ASSIGN.compile(optimizer='sgd', loss='mean_squared_error') ASSIGN = np.array([0,1,2,3,4,5,6], dtype=float) ASSIGN = np.array([0.5,1,1.5,2,2.5,3,3.5], dtype=float) ASSIGN.fit(ASSIGN, ASSIGN, epochs=500) ASSIGN.predict([7])",1,not_existent,"### House price from keras import Sequential from keras.layers import Dense  # Define model model = Sequential() model.add(Dense(units=1, input_shape=[1]))  # Compile model model.compile(optimizer='sgd', loss='mean_squared_error')  # Define data xs = np.array([0,1,2,3,4,5,6], dtype=float) ys = np.array([0.5,1,1.5,2,2.5,3,3.5], dtype=float)  # Train model model.fit(xs, ys, epochs=500)  # Predict model.predict([7])"
"SETUP (train_img, train_lab), (test_img, test_lab) = fashion_mnist.load_data()",0,not_existent,"### Load data from keras.datasets import fashion_mnist (train_img, train_lab), (test_img, test_lab) = fashion_mnist.load_data()"
plt.imshow(train_img[11]) ;,0,not_existent,### Plot one image plt.imshow(train_img[11]) ;
ASSIGN = ASSIGN path ASSIGN = ASSIGN path,1,not_existent,### Normalize data train_img = train_img / 225 test_img = test_img / 225
"SETUP CHECKPOINT ASSIGN = time.time() class EarlyStop(tf.keras.callbacks.Callback) : def __init__(self, threshold) : self.thres = threshold def on_epoch_end(self, epoch, logs={}) : if (logs.get('accuracy')>self.thres) : print('\nReached',self.thres*100,'Accuracy so stop train!') self.model.stop_training=True ASSIGN = EarlyStop(0.9) ASSIGN = Sequential() ASSIGN.add(Flatten(input_shape=(28,28))) ASSIGN.add(Dense(512, activation=tf.nn.relu)) ASSIGN.add(Dense(512, activation=tf.nn.relu)) ASSIGN.add(Dense(10, activation=tf.nn.softmax)) ASSIGN.compile(optimizer=tf.optimizers.Adam(), ASSIGN='sparse_categorical_crossentropy', ASSIGN=['accuracy']) ASSIGN.fit(train_img, train_lab, epochs=10, ASSIGN=[ASSIGN]) ASSIGN = time.time() print('Time Used :',(ASSIGN-ASSIGN)path)",0,not_existent,"### Make keras model import tensorflow as tf from keras import Sequential from keras.layers import Flatten,Dense import time  start = time.time()  # Set early stopping class EarlyStop(tf.keras.callbacks.Callback) :          def __init__(self, threshold) :         self.thres = threshold              def on_epoch_end(self, epoch, logs={}) :         if (logs.get('accuracy')>self.thres) :             print('\nReached',self.thres*100,'Accuracy so stop train!')             self.model.stop_training=True              callbacks = EarlyStop(0.9)  # Define model model = Sequential() model.add(Flatten(input_shape=(28,28))) model.add(Dense(512, activation=tf.nn.relu)) model.add(Dense(512, activation=tf.nn.relu)) model.add(Dense(10, activation=tf.nn.softmax))  # Compile model model.compile(optimizer=tf.optimizers.Adam(),               loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  # Train model model.fit(train_img, train_lab, epochs=10, callbacks=[callbacks])  end = time.time() print('Time Used :',(end-start)/60)"
"model.evaluate(test_img, test_lab)",0,not_existent,"### Evaluate model model.evaluate(test_img, test_lab)"
ASSIGN = model.predict(test_img),0,not_existent,### Predict  pred = model.predict(test_img)
np.argmax(pred[0]),0,not_existent,### See the predicted label np.argmax(pred[0])
"SETUP (train_img, train_lab), (test_img, test_lab) = mnist.load_data()",0,not_existent,"### Load data from keras.datasets import mnist (train_img, train_lab), (test_img, test_lab) = mnist.load_data()"
CHECKPOINT print(train_img.shape) print(len(set(train_lab))),0,not_existent,### See the shape of the data print(train_img.shape) print(len(set(train_lab)))
"SETUP CHECKPOINT ASSIGN = time.time() class EarlyStop(tf.keras.callbacks.Callback) : def __init__(self, threshold) : self.thres = threshold def on_epoch_end(self, epoch, logs={}) : if (logs.get('accuracy')>self.thres) : print('\nReached',self.thres*100,'Accuracy so stop train!') self.model.stop_training=True ASSIGN = EarlyStop(0.99) ASSIGN = Sequential() ASSIGN.add(Flatten(input_shape=(28,28))) ASSIGN.add(Dense(512, activation=tf.nn.relu)) ASSIGN.add(Dense(512, activation=tf.nn.relu)) ASSIGN.add(Dense(10, activation=tf.nn.softmax)) ASSIGN.compile(optimizer=tf.optimizers.Adam(), ASSIGN='sparse_categorical_crossentropy', ASSIGN=['accuracy']) ASSIGN.fit(train_img, train_lab, epochs=10, ASSIGN=[ASSIGN]) ASSIGN = time.time() print('Time Used :',(ASSIGN-ASSIGN)path)",0,not_existent,"### Make keras model import tensorflow as tf from keras import Sequential from keras.layers import Flatten,Dense import time  start = time.time()  # Set early stopping class EarlyStop(tf.keras.callbacks.Callback) :          def __init__(self, threshold) :         self.thres = threshold              def on_epoch_end(self, epoch, logs={}) :         if (logs.get('accuracy')>self.thres) :             print('\nReached',self.thres*100,'Accuracy so stop train!')             self.model.stop_training=True              callbacks = EarlyStop(0.99)  # Define model model = Sequential() model.add(Flatten(input_shape=(28,28))) model.add(Dense(512, activation=tf.nn.relu)) model.add(Dense(512, activation=tf.nn.relu)) model.add(Dense(10, activation=tf.nn.softmax))  # Compile model model.compile(optimizer=tf.optimizers.Adam(),               loss='sparse_categorical_crossentropy',               metrics=['accuracy'])  # Train model model.fit(train_img, train_lab, epochs=10, callbacks=[callbacks])  end = time.time() print('Time Used :',(end-start)/60)"
SETUP https:path\ -O path,0,not_existent,### Download the data !wget --no-check-certificate \   https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \   -O /tmp/cats_and_dogs_filtered.zip
"SETUP CHECKPOINT ASSIGN = time.time() ASSIGN = 'path' ASSIGN = zipfile.ZipFile(data_zip, 'r') ASSIGN.extractall() ASSIGN.close() print('Unzip Data Completed') ASSIGN = time.time() print('Time Used :',(ASSIGN-ASSIGN)path)",0,not_existent,"### Unzip the data import zipfile import time  start = time.time()  data_zip = '/tmp/cats_and_dogs_filtered.zip' data_ref = zipfile.ZipFile(data_zip, 'r') data_ref.extractall() data_ref.close() print('Unzip Data Completed')  end = time.time() print('Time Used :',(end-start)/60)"
"SETUP ASSIGN = 'path' ASSIGN = os.path.join(base_dir, 'train') ASSIGN = os.path.join(base_dir, 'validation') ASSIGN = os.path.join(train_dir, 'cats') ASSIGN = os.path.join(train_dir, 'dogs') ASSIGN = os.path.join(val_dir, 'cats') ASSIGN = os.path.join(val_dir, 'dogs')",0,not_existent,"### Set directory path for data train and validation import os base_dir = '/kaggle/working/cats_and_dogs_filtered' train_dir = os.path.join(base_dir, 'train') val_dir = os.path.join(base_dir, 'validation')  # Directory with our training cat/dog pictures train_cats_dir = os.path.join(train_dir, 'cats') train_dogs_dir = os.path.join(train_dir, 'dogs')  # Directory with our validation cat/dog pictures val_cats_dir = os.path.join(val_dir, 'cats') val_dogs_dir = os.path.join(val_dir, 'dogs')"
ASSIGN = os.listdir(train_cats_dir) ASSIGN = os.listdir(train_dogs_dir) ASSIGN = os.listdir(val_cats_dir) ASSIGN = os.listdir(val_dogs_dir),1,not_existent,### Get the name of the file for train and validation train_cat_fn = os.listdir(train_cats_dir) train_dog_fn = os.listdir(train_dogs_dir)  val_cat_fn = os.listdir(val_cats_dir) val_dog_fn = os.listdir(val_dogs_dir)
"SETUP rcParams['figure.figsize'] = [10,5] plt.style.use('fivethirtyeight') sns.set_style('whitegrid') ASSIGN = gridspec.GridSpec(1,2) ASSIGN = plt.subplot(grid[0]) ASSIGN = mpimg.imread(os.path.join(train_cats_dir, train_cat_fn[0])) ASSIGN.imshow(ASSIGN) ASSIGN.axis('off') ; ASSIGN = plt.subplot(grid[1]) ASSIGN = mpimg.imread(os.path.join(train_dogs_dir, train_dog_fn[0])) ASSIGN.imshow(ASSIGN) ASSIGN.axis('off') ;",0,not_existent,"### Image example import matplotlib.image as mpimg rcParams['figure.figsize'] = [10,5] plt.style.use('fivethirtyeight')  sns.set_style('whitegrid') grid = gridspec.GridSpec(1,2)  # Cat image ax = plt.subplot(grid[0]) cat_img = mpimg.imread(os.path.join(train_cats_dir, train_cat_fn[0])) ax.imshow(cat_img) ax.axis('off') ;  # Dog image ax = plt.subplot(grid[1]) dog_img = mpimg.imread(os.path.join(train_dogs_dir, train_dog_fn[0])) ax.imshow(dog_img) ax.axis('off') ;"
"SETUP ASSIGN = ImageDataGenerator(rescale=1path) ASSIGN = ImageDataGenerator(rescale=1path) ASSIGN = train_datagen.flow_from_directory(train_dir, ASSIGN=20, ASSIGN='binary', ASSIGN=(150,150)) ASSIGN = val_datagen.flow_from_directory(val_dir, ASSIGN=20, ASSIGN='binary', ASSIGN=(150,150))",0,not_existent,"### Preprocess the image from keras.preprocessing.image import ImageDataGenerator  # Define generator train_datagen = ImageDataGenerator(rescale=1/255) val_datagen = ImageDataGenerator(rescale=1/255)  # Define flow for train gen train_gen = train_datagen.flow_from_directory(train_dir,                                               batch_size=20,                                               class_mode='binary',                                               target_size=(150,150))  # Define flow for validation gen val_gen = val_datagen.flow_from_directory(val_dir,                                           batch_size=20,                                           class_mode='binary',                                           target_size=(150,150))"
"SETUP CHECKPOINT ASSIGN = time.time() class EarlyStop(tf.keras.callbacks.Callback) : def __init__(self, threshold) : self.thres = threshold def on_epoch_end(self, epoch, logs={}) : if (logs.get('val_accuracy')>self.thres) : print('\nReached',self.thres*100,' Validation Accuracy so stop train!') self.model.stop_training=True ASSIGN = EarlyStop(0.72) ASSIGN = Sequential() ASSIGN.add(Conv2D(16, (3,3), activation='relu', input_shape=(150,150,3))) ASSIGN.add(MaxPooling2D(2,2)) ASSIGN.add(Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3))) ASSIGN.add(MaxPooling2D(2,2)) ASSIGN.add(Conv2D(64, (3,3), activation='relu', input_shape=(150,150,3))) ASSIGN.add(MaxPooling2D(2,2)) ASSIGN.add(Flatten()) ASSIGN.add(Dense(512, activation='relu')) ASSIGN.add(Dense(1, activation='sigmoid')) ASSIGN.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001), ASSIGN='binary_crossentropy', ASSIGN=['accuracy']) ASSIGN = model.fit_generator(train_gen, ASSIGN=val_gen, ASSIGN=100, ASSIGN=20, ASSIGN=50, ASSIGN=1, ASSIGN=[ASSIGN]) ASSIGN = time.time() print('Time Used :',(ASSIGN-ASSIGN)path)",0,not_existent,"### Make keras model import tensorflow as tf import keras from keras import Sequential from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense import time  start = time.time()  # Set early stopping class EarlyStop(tf.keras.callbacks.Callback) :          def __init__(self, threshold) :         self.thres = threshold              def on_epoch_end(self, epoch, logs={}) :         if (logs.get('val_accuracy')>self.thres) :             print('\nReached',self.thres*100,' Validation Accuracy so stop train!')             self.model.stop_training=True              callbacks = EarlyStop(0.72)  # Define model model = Sequential() model.add(Conv2D(16, (3,3), activation='relu', input_shape=(150,150,3))) model.add(MaxPooling2D(2,2)) model.add(Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3))) model.add(MaxPooling2D(2,2)) model.add(Conv2D(64, (3,3), activation='relu', input_shape=(150,150,3))) model.add(MaxPooling2D(2,2))  model.add(Flatten()) model.add(Dense(512, activation='relu')) model.add(Dense(1, activation='sigmoid'))  # Compile model model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),               loss='binary_crossentropy',               metrics=['accuracy'])  # Train model history = model.fit_generator(train_gen,                                validation_data=val_gen,                               steps_per_epoch=100,                               epochs=20,                               validation_steps=50,                               verbose=1,                               callbacks=[callbacks])  end = time.time() print('Time Used :',(end-start)/60)"
"SETUP CHECKPOINT ASSIGN = [os.path.join(val_cats_dir, fn) for fn in sample(val_cat_fn, 5)] ASSIGN = [os.path.join(val_dogs_dir, fn) for fn in sample(val_dog_fn, 5)] ASSIGN = list_cat_fn + list_dog_fn for fn in ASSIGN : ASSIGN = load_img(fn, target_size=(150,150)) ASSIGN = img_to_array(img) ASSIGN = np.expand_dims(ASSIGN, axis=0) ASSIGN = np.vstack([vect_img]) ASSIGN = model.predict(ready_img, batch_size=10) ASSIGN == 0 : print(fn,'is a cat') else : print(fn,'is a dog')",0,not_existent,"### Try predict some image from keras.preprocessing.image import load_img, img_to_array from random import sample list_cat_fn = [os.path.join(val_cats_dir, fn) for fn in sample(val_cat_fn, 5)] list_dog_fn = [os.path.join(val_dogs_dir, fn) for fn in sample(val_dog_fn, 5)] list_fn = list_cat_fn + list_dog_fn  # Iteration to predict for fn in list_fn :          # Load and preprocess image     img = load_img(fn, target_size=(150,150))     vect_img = img_to_array(img)     vect_img = np.expand_dims(vect_img, axis=0)     ready_img = np.vstack([vect_img])          # Predict     classes = model.predict(ready_img, batch_size=10)     if classes == 0 :         print(fn,'is a cat')     else :         print(fn,'is a dog')"
"SETUP ASSIGN = [os.path.join(val_cats_dir, fn) for fn in sample(val_cat_fn, 5)] ASSIGN = [os.path.join(val_dogs_dir, fn) for fn in sample(val_dog_fn, 5)] ASSIGN = list_cat_fn + list_dog_fn ASSIGN = [layer.output for layer in model.layers[1:]] ASSIGN = keras.models.Model(inputs=model.input, outputs=succ_out) ASSIGN = load_img(choice(list_fn), target_size=(150,150)) ASSIGN = img_to_array(img) ASSIGN  = ASSIGN.reshape((1,) + ASSIGN.shape) ASSIGN = ASSIGN path ASSIGN = viz_model.predict(x) ASSIGN = [layer.name for layer in model.layers] for layer_name, feature_map in zip(ASSIGN, ASSIGN): if len(feature_map.shape) == 4: ASSIGN = feature_map.shape[-1] ASSIGN    = feature_map.shape[ 1] ASSIGN = np.zeros((size, size * n_features)) for i in range(ASSIGN): ASSIGN = feature_map[0, :, :, i] ASSIGN -= ASSIGN.mean() ASSIGN= x.std () ASSIGN *= 64 ASSIGN += 128 ASSIGN = np.clip(ASSIGN, 0, 255).astype('uint8') ASSIGN[:, i * ASSIGN : (i + 1) * ASSIGN] = ASSIGN ASSIGN = 20. path plt.figure( figsize=(ASSIGN * ASSIGN, ASSIGN) ) plt.title ( layer_name ) plt.grid ( False ) plt.imshow( ASSIGN, aspect='auto', cmap='viridis' )",0,not_existent,"### Representation of the image based on model from keras.preprocessing.image import load_img, img_to_array import keras from random import choice list_cat_fn = [os.path.join(val_cats_dir, fn) for fn in sample(val_cat_fn, 5)] list_dog_fn = [os.path.join(val_dogs_dir, fn) for fn in sample(val_dog_fn, 5)] list_fn = list_cat_fn + list_dog_fn succ_out = [layer.output for layer in model.layers[1:]]  # Define viz model viz_model = keras.models.Model(inputs=model.input, outputs=succ_out)  # Pick one image randomly img = load_img(choice(list_fn), target_size=(150,150)) x = img_to_array(img) x   = x.reshape((1,) + x.shape)   x = x / 255  # Predicting successive_feature_maps = viz_model.predict(x)  # List of layer name layer_names = [layer.name for layer in model.layers]  # Plot the visualization on each layer for layer_name, feature_map in zip(layer_names, successive_feature_maps):      if len(feature_map.shape) == 4:          #-------------------------------------------     # Just do this for the conv / maxpool layers, not the fully-connected layers     #-------------------------------------------     n_features = feature_map.shape[-1]  # number of features in the feature map     size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)          # We will tile our images in this matrix     display_grid = np.zeros((size, size * n_features))          #-------------------------------------------------     # Postprocess the feature to be visually palatable     #-------------------------------------------------     for i in range(n_features):       x  = feature_map[0, :, :, i]       x -= x.mean()       x /= x.std ()       x *=  64       x += 128       x  = np.clip(x, 0, 255).astype('uint8')       display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid      #-----------------     # Display the grid     #-----------------      scale = 20. / n_features     plt.figure( figsize=(scale * n_features, scale) )     plt.title ( layer_name )     plt.grid  ( False )     plt.imshow( display_grid, aspect='auto', cmap='viridis' ) "
"rcParams['figure.figsize'] = [10,8] plt.style.use('fivethirtyeight') sns.set_style('whitegrid') ASSIGN = gridspec.GridSpec(2,1) ASSIGN   = history.history[   'accuracy' ] ASSIGN = history.history[ 'val_accuracy' ] ASSIGN   = history.history[  'ASSIGN' ] ASSIGN = history.history['ASSIGN' ] ASSIGN   = list(range(len(acc))) ASSIGN = plt.subplot(grid[0]) ASSIGN.plot(ASSIGN, ASSIGN, label='Train Loss') ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Loss') ASSIGN.set_title('Training and Validation Loss') ASSIGN.legend() ; ASSIGN = plt.subplot(grid[1]) ASSIGN.plot(ASSIGN, ASSIGN, label='Train Acc') ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Acc') ASSIGN.set_title('Training and Validation Accuracy') ASSIGN.legend() ; plt.tight_layout()",0,not_existent,"### Plot the performance of the model rcParams['figure.figsize'] = [10,8] plt.style.use('fivethirtyeight')  sns.set_style('whitegrid') grid = gridspec.GridSpec(2,1)  # Set the variable  acc      = history.history[     'accuracy' ] val_acc  = history.history[ 'val_accuracy' ] loss     = history.history[    'loss' ] val_loss = history.history['val_loss' ] epo      = list(range(len(acc)))  # Plot the loss ax = plt.subplot(grid[0]) ax.plot(epo, loss, label='Train Loss') ax.plot(epo, val_loss, label='Validation Loss') ax.set_title('Training and Validation Loss') ax.legend() ;  # Plot the loss ax = plt.subplot(grid[1]) ax.plot(epo, acc, label='Train Acc') ax.plot(epo, val_acc, label='Validation Acc') ax.set_title('Training and Validation Accuracy') ax.legend() ;  plt.tight_layout()"
"ASSIGN   = history.history[   'accuracy' ] ASSIGN = history.history[ 'val_accuracy' ] ASSIGN   = history.history[  'ASSIGN' ] ASSIGN = history.history['ASSIGN' ] ASSIGN  = range(len(acc)) plt.plot ( ASSIGN, ASSIGN ) plt.plot ( ASSIGN, ASSIGN ) plt.title ('Training and validation accuracy') plt.figure() plt.plot ( ASSIGN, ASSIGN ) plt.plot ( ASSIGN, ASSIGN ) plt.title ('Training and validation ASSIGN' )",0,not_existent,"#----------------------------------------------------------- # Retrieve a list of list results on training and test data # sets for each training epoch #----------------------------------------------------------- acc      = history.history[     'accuracy' ] val_acc  = history.history[ 'val_accuracy' ] loss     = history.history[    'loss' ] val_loss = history.history['val_loss' ]  epochs   = range(len(acc)) # Get number of epochs  #------------------------------------------------ # Plot training and validation accuracy per epoch #------------------------------------------------ plt.plot  ( epochs,     acc ) plt.plot  ( epochs, val_acc ) plt.title ('Training and validation accuracy') plt.figure()  #------------------------------------------------ # Plot training and validation loss per epoch #------------------------------------------------ plt.plot  ( epochs,     loss ) plt.plot  ( epochs, val_loss ) plt.title ('Training and validation loss'   )"
"SETUP CHECKPOINT ASSIGN = time.time() ASSIGN = 'path' ASSIGN = zipfile.ZipFile(train_zip, 'r') ASSIGN.extractall() ASSIGN.close() print('Unzip Train Completed') ASSIGN = time.time() print('Time Used :',(ASSIGN-ASSIGN)path) ASSIGN = time.time() ASSIGN = 'path' ASSIGN = zipfile.ZipFile(test_zip, 'r') ASSIGN.extractall() ASSIGN.close() print('Unzip Test Completed') ASSIGN = time.time() print('Time Used :',(ASSIGN-ASSIGN)path)",0,not_existent,"### Load full data import zipfile import time  # Unzip train data start = time.time()  train_zip = '/kaggle/input/dogs-vs-cats/train.zip' train_ref = zipfile.ZipFile(train_zip, 'r') train_ref.extractall() train_ref.close() print('Unzip Train Completed')  end = time.time() print('Time Used :',(end-start)/60)  # Unzip test data start = time.time()  test_zip = '/kaggle/input/dogs-vs-cats/test1.zip' test_ref = zipfile.ZipFile(test_zip, 'r') test_ref.extractall() test_ref.close() print('Unzip Test Completed')  end = time.time() print('Time Used :',(end-start)/60)"
SETUP SETUP os.mkdir(TRAINING_DIR) os.mkdir(TRAINING_CAT_DIR) os.mkdir(TRAINING_DOG_DIR) os.mkdir(VALIDATION_DIR) os.mkdir(VALIDATION_CAT_DIR) os.mkdir(VALIDATION_DOG_DIR),0,not_existent,### Make directory for ImageGenerator import os TRAINING_DIR = '/kaggle/training/' TRAINING_CAT_DIR = '/kaggle/training/cat/' TRAINING_DOG_DIR = '/kaggle/training/dog/' os.mkdir(TRAINING_DIR) os.mkdir(TRAINING_CAT_DIR) os.mkdir(TRAINING_DOG_DIR)  VALIDATION_DIR = '/kaggle/validation/' VALIDATION_CAT_DIR = '/kaggle/validation/cat/' VALIDATION_DOG_DIR = '/kaggle/validation/dog/' os.mkdir(VALIDATION_DIR) os.mkdir(VALIDATION_CAT_DIR) os.mkdir(VALIDATION_DOG_DIR)
SETUP ASSIGN = os.listdir(SOURCE_DIR) ASSIGN = [fn for fn in list_fn if 'cat' in fn] ASSIGN = [fn for fn in list_fn if 'dog' in fn],0,not_existent,### List file name in train dataset SOURCE_DIR = '/kaggle/working/train/' list_fn = os.listdir(SOURCE_DIR) list_cat_fn = [fn for fn in list_fn if 'cat' in fn] list_dog_fn = [fn for fn in list_fn if 'dog' in fn]
"SETUP SETUP ASSIGN = ['cat','dog'] ASSIGN = [list_cat_fn, list_dog_fn] for i,c in enumerate(tqdm(ASSIGN)) : ASSIGN = list_name_fn[i] ASSIGN = random.sample(list_class_fn, len(list_class_fn)) ASSIGN = pure_random[:int(SPLIT_PROP*len(pure_random))] ASSIGN = pure_random[int(SPLIT_PROP*len(pure_random)):] for f in ASSIGN : copyfile(os.path.join(SOURCE_DIR,f), os.path.join(TRAIN_CLASS_DIR,f)) for f in ASSIGN : copyfile(os.path.join(SOURCE_DIR,f), os.path.join(VALID_CLASS_DIR,f)) del pure_random, random_train, random_val",1,not_existent,"### Split data import random from shutil import copyfile SPLIT_PROP = 0.8 list_class = ['cat','dog'] list_name_fn = [list_cat_fn, list_dog_fn]  for i,c in enumerate(tqdm(list_class)) :                  # Splitting     list_class_fn = list_name_fn[i]     pure_random = random.sample(list_class_fn, len(list_class_fn))     random_train = pure_random[:int(SPLIT_PROP*len(pure_random))]     random_val = pure_random[int(SPLIT_PROP*len(pure_random)):]          # Insert into new train dir     TRAIN_CLASS_DIR = os.path.join(TRAINING_DIR,c)     for f in random_train :         copyfile(os.path.join(SOURCE_DIR,f), os.path.join(TRAIN_CLASS_DIR,f))              # Insert into new valid dir     VALID_CLASS_DIR = os.path.join(VALIDATION_DIR,c)     for f in random_val :         copyfile(os.path.join(SOURCE_DIR,f), os.path.join(VALID_CLASS_DIR,f))              del pure_random, random_train, random_val"
"SETUP SETUP ASSIGN = ResNet50(weights='imagenet', ASSIGN=False, ASSIGN=(HEIGHT, WIDTH, 3)) ASSIGN = tf.keras.applications.resnet50.preprocess_input ASSIGN = tf.keras.applications.resnet50.decode_predictions",0,not_existent,"# Make pre train model from tensorflow.keras.applications.resnet50 import ResNet50 from tensorflow.keras.applications.resnet50 import preprocess_input,decode_predictions  HEIGHT = 150 WIDTH = 150 base_model = ResNet50(weights='imagenet',                   include_top=False,                   input_shape=(HEIGHT, WIDTH, 3))  prec_input = tf.keras.applications.resnet50.preprocess_input decode = tf.keras.applications.resnet50.decode_predictions"
SETUP,0,not_existent,### Define parameter BATCH_SIZE=10 EPOCHS = 10
"SETUP ASSIGN = ImageDataGenerator(rescale=1path, preprocessing_function=prec_input) ASSIGN = ImageDataGenerator(rescale=1path, preprocessing_function=prec_input) ASSIGN = train_datagen.flow_from_directory(TRAINING_DIR, ASSIGN=BATCH_SIZE, ASSIGN='bicubic', ASSIGN='categorical', ASSIGN=True, ASSIGN=(HEIGHT, WIDTH)) ASSIGN = val_datagen.flow_from_directory(VALIDATION_DIR, ASSIGN=BATCH_SIZE, ASSIGN='bicubic', ASSIGN='categorical', ASSIGN=False, ASSIGN=(HEIGHT, WIDTH))",0,not_existent,"### Preprocess the image from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Define generator train_datagen = ImageDataGenerator(rescale=1/255, preprocessing_function=prec_input) val_datagen = ImageDataGenerator(rescale=1/255, preprocessing_function=prec_input)  # Define flow for train gen train_gen = train_datagen.flow_from_directory(TRAINING_DIR,                                               batch_size=BATCH_SIZE,                                               interpolation='bicubic',                                               class_mode='categorical',                                               shuffle=True,                                               target_size=(HEIGHT, WIDTH))  # Define flow for validation gen val_gen = val_datagen.flow_from_directory(VALIDATION_DIR,                                           batch_size=BATCH_SIZE,                                           interpolation='bicubic',                                           class_mode='categorical',                                           shuffle=False,                                           target_size=(HEIGHT, WIDTH))"
"SETUP ASSIGN = [] ASSIGN = 2 ASSIGN = 0.2 def make_model() : for l in base_model.layers : l.trainable = False ASSIGN = Sequential() ASSIGN.add(base_model) ASSIGN.add(GlobalAveragePooling2D()) ASSIGN.add(Flatten()) for fc in ASSIGN: ASSIGN.add(Dense(fc, activation=swish)) ASSIGN.add(Dropout(ASSIGN)) ASSIGN.add(Dense(ASSIGN, activation='softmax')) ASSIGN.compile(Adam(), loss='categorical_crossentropy', metrics=['acc']) return model ASSIGN = make_model() ASSIGN.summary()",0,not_existent,"### Add layer and dont train the layer before from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D from tensorflow.keras import Sequential, Model from tensorflow.keras.optimizers import SGD, Adam, RMSprop from tensorflow.keras.metrics import Precision from tensorflow.keras.activations import swish fc_layers = [] num_classes = 2 dropout = 0.2  def make_model() :          # Freeze layer     for l in base_model.layers :         l.trainable = False      # Make new model     model = Sequential()     model.add(base_model)     model.add(GlobalAveragePooling2D())     model.add(Flatten())     for fc in fc_layers:          # New FC layer, random init         model.add(Dense(fc, activation=swish))         model.add(Dropout(dropout))      model.add(Dense(num_classes, activation='softmax'))      # Compile model     model.compile(Adam(), loss='categorical_crossentropy', metrics=['acc'])          return model   model = make_model() model.summary()"
"ASSIGN = model.fit_generator(train_gen, ASSIGN=EPOCHS, ASSIGN=1, ASSIGN=val_gen)",0,not_existent,"history = model.fit_generator(train_gen,                               epochs=EPOCHS,                               verbose=1,                               validation_data=val_gen) "
"ASSIGN = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(32, (3,3), activation='relu'), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(64, (3,3), activation='relu'), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(512, activation='relu'), tf.keras.layers.Dense(2, activation='softmax') ]) ASSIGN.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['acc'])",0,not_existent,"# DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS # USE AT LEAST 3 CONVOLUTION LAYERS model = tf.keras.models.Sequential([     tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),     tf.keras.layers.MaxPooling2D(2,2),     tf.keras.layers.Conv2D(32, (3,3), activation='relu'),     tf.keras.layers.MaxPooling2D(2,2),      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),      tf.keras.layers.MaxPooling2D(2,2),     tf.keras.layers.Flatten(),      tf.keras.layers.Dense(512, activation='relu'),      tf.keras.layers.Dense(2, activation='softmax')    # YOUR CODE HERE ])  model.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['acc'])"
SETUP SETUP,0,not_existent,### Get data !wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip -O /tmp/horse-or-human.zip !wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip -O /tmp/validation-horse-or-human.zip 
"SETUP CHECKPOINT ASSIGN = 'path' ASSIGN = zipfile.ZipFile(data_zip, 'r') ASSIGN.extractall('path') ASSIGN.close() print('Unzip Train Data Completed') ASSIGN = 'path' ASSIGN = zipfile.ZipFile(valid_zip, 'r') ASSIGN.extractall('path') ASSIGN.close() print('Unzip Valid Data Completed')",0,not_existent,"### Extract the zip file import zipfile import time  # Unzip train data data_zip = '/tmp/horse-or-human.zip' data_ref = zipfile.ZipFile(data_zip, 'r') data_ref.extractall('/training') data_ref.close() print('Unzip Train Data Completed')  # Unzip valid data valid_zip = '/tmp/validation-horse-or-human.zip' valid_ref = zipfile.ZipFile(valid_zip, 'r') valid_ref.extractall('/validating') valid_ref.close() print('Unzip Valid Data Completed')  %time"
"SETUP CHECKPOINT print('Total obs on horse class in train :',len(os.listdir(TRAIN_HORSE_DIR))) print('Total obs on human class in train :',len(os.listdir(TRAIN_HUMAN_DIR))) print('Total obs on horse class in validation :',len(os.listdir(VAL_HORSE_DIR))) print('Total obs on human class in validation :',len(os.listdir(VAL_HUMAN_DIR)))",0,not_existent,"### Check the proportion of classes in train and valid dataset TRAIN_HORSE_DIR = '/training/horses' TRAIN_HUMAN_DIR = '/training/humans' VAL_HORSE_DIR = '/validating/horses' VAL_HUMAN_DIR = '/validating/humans'  print('Total obs on horse class in train :',len(os.listdir(TRAIN_HORSE_DIR))) print('Total obs on human class in train :',len(os.listdir(TRAIN_HUMAN_DIR))) print('Total obs on horse class in validation :',len(os.listdir(VAL_HORSE_DIR))) print('Total obs on human class in validation :',len(os.listdir(VAL_HUMAN_DIR)))"
"SETUP SETUP https:path\ -O path ASSIGN = InceptionV3(weights=None, ASSIGN=False, ASSIGN=(INPUT_SIZE[0], INPUT_SIZE[1], 3)) ASSIGN = 'path' ASSIGN.load_weights(ASSIGN) ASSIGN.trainable = False ASSIGN.summary()",0,not_existent,"### Define pretrained model from tensorflow.keras.applications.inception_v3 import InceptionV3 from tensorflow.keras.applications.inception_v3 import preprocess_input,decode_predictions BATCH_SIZE = 20 INPUT_SIZE = (150,150)  # Get local weight !wget --no-check-certificate \     https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \     -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5      # Define inception model base_model = InceptionV3(weights=None,                           include_top=False,                           input_shape=(INPUT_SIZE[0], INPUT_SIZE[1], 3))  # Load local weight local_weight_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5' base_model.load_weights(local_weight_file)  # Freeze all layer base_model.trainable = False  # Summary of the model base_model.summary()"
"CHECKPOINT ASSIGN = base_model.get_layer('mixed7') ASSIGN = last_layer.output print('Last layer shape :',ASSIGN.output_shape)",1,not_existent,"### Get specific layer from pre-trained model  last_layer = base_model.get_layer('mixed7') last_output = last_layer.output  print('Last layer shape :',last_layer.output_shape)"
"SETUP SETUP ASSIGN = ImageDataGenerator(rescale = 1.path, ASSIGN = 40, ASSIGN = 0.2, ASSIGN = 0.2, ASSIGN = 0.2, ASSIGN = 0.2, ASSIGN = True) ASSIGN = train_datagen.flow_from_directory(TRAIN_DIR, ASSIGN = BATCH_SIZE, ASSIGN = 'binary', ASSIGN = INPUT_SIZE) ASSIGN = ImageDataGenerator(rescale = 1path) ASSIGN = val_datagen.flow_from_directory(VALID_DIR, ASSIGN = BATCH_SIZE, ASSIGN = 'binary', ASSIGN = INPUT_SIZE)",0,not_existent,"### Set the image generator from tensorflow.keras.preprocessing.image import ImageDataGenerator TRAIN_DIR = '/training' VALID_DIR = '/validating'  # Make image generator for training dataset train_datagen = ImageDataGenerator(rescale = 1./255.,                                    rotation_range = 40,                                    width_shift_range = 0.2,                                    height_shift_range = 0.2,                                    shear_range = 0.2,                                    zoom_range = 0.2,                                    horizontal_flip = True)  train_gen = train_datagen.flow_from_directory(TRAIN_DIR,                                               batch_size = BATCH_SIZE,                                               class_mode = 'binary',                                                target_size = INPUT_SIZE)    # Make image generator for validation dataset val_datagen = ImageDataGenerator(rescale = 1/255. )  val_gen =  val_datagen.flow_from_directory(VALID_DIR,                                             batch_size  = BATCH_SIZE,                                             class_mode  = 'binary',                                              target_size = INPUT_SIZE)"
"SETUP ASSIGN = Flatten()(last_output) ASSIGN = Dense(1024, activation='relu')(ASSIGN) ASSIGN = Dropout(0.2)(ASSIGN) ASSIGN = Dense(1, activation='sigmoid')(ASSIGN) ASSIGN = Model(base_model.input, x) ASSIGN.compile(optimizer = RMSprop(lr=0.0001), ASSIGN = 'binary_crossentropy', ASSIGN = 'accuracy')",0,not_existent,"### Define new model from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D from tensorflow.keras import Sequential, Model from tensorflow.keras.optimizers import SGD, Adam, RMSprop from tensorflow.keras.metrics import Precision from tensorflow.keras.activations import swish\  # model = Sequential() # model.add(base_model) # model.add(Flatten()) # model.add(Dense(1024, activation='relu')) # model.add(Dropout(0.2)) # model.add(Dense(1, activation='sigmoid'))  # Use this when you not using the whole pre trained model x = Flatten()(last_output) x = Dense(1024, activation='relu')(x) x = Dropout(0.2)(x) x = Dense(1, activation='sigmoid')(x) model = Model(base_model.input, x)  # Compile model model.compile(optimizer = RMSprop(lr=0.0001),               loss = 'binary_crossentropy',               metrics = 'accuracy') "
"CHECKPOINT class EarlyStop(tf.keras.callbacks.Callback) : def __init__(self, threshold) : self.thres = threshold def on_epoch_end(self, epoch, logs={}) : if (logs.get('accuracy')>self.thres) : print('\nReached',self.thres*100,'Accuracy so stop train!') self.model.stop_training=True ASSIGN = EarlyStop(0.99)",0,not_existent,"### Define early callback based on metrics class EarlyStop(tf.keras.callbacks.Callback) :          def __init__(self, threshold) :         self.thres = threshold              def on_epoch_end(self, epoch, logs={}) :         if (logs.get('accuracy')>self.thres) :             print('\nReached',self.thres*100,'Accuracy so stop train!')             self.model.stop_training=True              callbacks = EarlyStop(0.99)"
"SETUP ASSIGN = model.fit_generator(train_gen, ASSIGN=val_gen, ASSIGN=100, ASSIGN=50, ASSIGN=EPOCHS, ASSIGN=ASSIGN, ASSIGN=1)",0,not_existent,"### Train model EPOCHS = 99 history = model.fit_generator(train_gen,                               validation_data=val_gen,                               steps_per_epoch=100,                               validation_steps=50,                               epochs=EPOCHS,                               callbacks=callbacks,                               verbose=1) "
"SETUP def get_data(path) : ASSIGN = pd.read_csv(path) ASSIGN = to_categorical(np.array(df['ASSIGN'])) ASSIGN = [] for i in range(len(ASSIGN)) : ASSIGN = np.array(np.split(df.iloc[i,1:].values, 28)) ASSIGN.append(ASSIGN) return np.array(image), label",1,not_existent,"### Function to read the data def get_data(path) :     # Read the csv     df = pd.read_csv(path)          # Get all the label     from tensorflow.keras.utils import to_categorical     label = to_categorical(np.array(df['label']))          # Get array of image     image = []     for i in range(len(df)) :         split_img = np.array(np.split(df.iloc[i,1:].values, 28))         image.append(split_img)          return np.array(image), label     "
SETUP ASSIGN = get_data('..path') ASSIGN = get_data('..path'),0,not_existent,"### Get data train_img, train_label = get_data('../input/sign-language-mnist/sign_mnist_train.csv') val_img, val_label = get_data('../input/sign-language-mnist/sign_mnist_test.csv')  %time"
"CHECKPOINT print('Train image shape :',train_img.shape) print('Train label shape :',train_label.shape) print('Valid image shape :',val_img.shape) print('Valid label shape :',val_label.shape)",0,not_existent,"### Shape of the data print('Train image shape :',train_img.shape) print('Train label shape :',train_label.shape) print('Valid image shape :',val_img.shape) print('Valid label shape :',val_label.shape)"
"SETUP SETUP ASSIGN = np.expand_dims(ASSIGN, 3) ASSIGN = ImageDataGenerator(rescale = 1.path, ASSIGN = 40, ASSIGN = 0.2, ASSIGN = 0.2, ASSIGN = 0.2, ASSIGN = 0.2, ASSIGN = True) ASSIGN = train_datagen.flow(x=train_img, y=train_label, ASSIGN = BATCH_SIZE) ASSIGN = np.expand_dims(ASSIGN, 3) ASSIGN = ImageDataGenerator(rescale = 1path) ASSIGN = val_datagen.flow(x=val_img, y=val_label, ASSIGN = BATCH_SIZE)",0,not_existent,"### Make the image generator from tensorflow.keras.preprocessing.image import ImageDataGenerator BATCH_SIZE = 32 INPUT_SIZE = (28,28) TRAIN_DIR = '/training' VALID_DIR = '/validating'  # Make image generator for training dataset train_img = np.expand_dims(train_img, 3) train_datagen = ImageDataGenerator(rescale = 1./255.,                                    rotation_range = 40,                                    width_shift_range = 0.2,                                    height_shift_range = 0.2,                                    shear_range = 0.2,                                    zoom_range = 0.2,                                    horizontal_flip = True)  train_gen = train_datagen.flow(x=train_img, y=train_label,                               batch_size = BATCH_SIZE)    # Make image generator for validation dataset val_img = np.expand_dims(val_img, 3) val_datagen = ImageDataGenerator(rescale = 1/255. )  val_gen = val_datagen.flow(x=val_img, y=val_label,                               batch_size = BATCH_SIZE) "
"SETUP ASSIGN = Sequential() ASSIGN.add(Conv2D(64, (3,3), activation='relu', input_shape=(INPUT_SIZE[0], INPUT_SIZE[1], 1))) ASSIGN.add(MaxPooling2D(2,2)) ASSIGN.add(Conv2D(128, (3,3), activation='relu')) ASSIGN.add(MaxPooling2D(2,2)) ASSIGN.add(Flatten()) ASSIGN.add(Dense(256, activation='relu')) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Dense(25, activation='softmax')) ASSIGN.compile(optimizer = Adam(), ASSIGN = 'categorical_crossentropy', ASSIGN = 'accuracy')",0,not_existent,"### Define new model from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, MaxPooling2D, Conv2D from tensorflow.keras import Sequential, Model from tensorflow.keras.optimizers import SGD, Adam, RMSprop from tensorflow.keras.metrics import Precision from tensorflow.keras.activations import swish\  model = Sequential() model.add(Conv2D(64, (3,3), activation='relu', input_shape=(INPUT_SIZE[0], INPUT_SIZE[1], 1))) model.add(MaxPooling2D(2,2)) model.add(Conv2D(128, (3,3), activation='relu')) model.add(MaxPooling2D(2,2)) model.add(Flatten()) model.add(Dense(256, activation='relu')) model.add(Dropout(0.2)) model.add(Dense(25, activation='softmax'))  # Compile model model.compile(optimizer = Adam(),               loss = 'categorical_crossentropy',               metrics = 'accuracy') "
"CHECKPOINT class EarlyStop(tf.keras.callbacks.Callback) : def __init__(self, threshold) : self.thres = threshold def on_epoch_end(self, epoch, logs={}) : if (logs.get('val_accuracy')>self.thres) : print('\nReached',self.thres*100,'Accuracy so stop train!') self.model.stop_training=True ASSIGN = EarlyStop(0.9)",0,not_existent,"### Define early callback based on metrics class EarlyStop(tf.keras.callbacks.Callback) :          def __init__(self, threshold) :         self.thres = threshold              def on_epoch_end(self, epoch, logs={}) :         if (logs.get('val_accuracy')>self.thres) :             print('\nReached',self.thres*100,'Accuracy so stop train!')             self.model.stop_training=True              callbacks = EarlyStop(0.9)"
"SETUP ASSIGN = model.fit_generator(train_gen, ASSIGN=val_gen, ASSIGN=train_img.shape[0] path, ASSIGN=val_img.shape[0] path, ASSIGN=EPOCHS, ASSIGN=ASSIGN, ASSIGN=1)",0,not_existent,"### Train model EPOCHS = 50 history = model.fit_generator(train_gen,                               validation_data=val_gen,                               steps_per_epoch=train_img.shape[0] // BATCH_SIZE,                               validation_steps=val_img.shape[0] // BATCH_SIZE,                               epochs=EPOCHS,                               callbacks=callbacks,                               verbose=1) "
"model.evaluate(val_img, val_label)",0,not_existent,"### Evaluate model model.evaluate(val_img, val_label)"
"rcParams['figure.figsize'] = [10,8] plt.style.use('fivethirtyeight') sns.set_style('whitegrid') ASSIGN = gridspec.GridSpec(2,1) ASSIGN   = history.history[   'accuracy' ] ASSIGN = history.history[ 'val_accuracy' ] ASSIGN   = history.history[  'ASSIGN' ] ASSIGN = history.history['ASSIGN' ] ASSIGN  = range(len(acc)) ASSIGN = plt.subplot(grid[0]) ASSIGN.plot(ASSIGN, ASSIGN, label='Train Loss') ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Loss') ASSIGN.set_title('Training and Validation Loss') ASSIGN.legend() ; ASSIGN = plt.subplot(grid[1]) ASSIGN.plot(ASSIGN, ASSIGN, label='Train Acc') ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Acc') ASSIGN.set_title('Training and Validation Accuracy') ASSIGN.legend() ;",0,not_existent,"### Plot the performance of the model rcParams['figure.figsize'] = [10,8] plt.style.use('fivethirtyeight')  sns.set_style('whitegrid') grid = gridspec.GridSpec(2,1)  # Get the metrics and loss acc      = history.history[     'accuracy' ] val_acc  = history.history[ 'val_accuracy' ] loss     = history.history[    'loss' ] val_loss = history.history['val_loss' ] epo   = range(len(acc)) # Get number of epochs  # Plot the loss ax = plt.subplot(grid[0]) ax.plot(epo, loss, label='Train Loss') ax.plot(epo, val_loss, label='Validation Loss') ax.set_title('Training and Validation Loss') ax.legend() ;  # Plot the acccuracy ax = plt.subplot(grid[1]) ax.plot(epo, acc, label='Train Acc') ax.plot(epo, val_acc, label='Validation Acc') ax.set_title('Training and Validation Accuracy') ax.legend() ;"
SETUP https:path\ -O path,0,not_existent,### Get data !wget --no-check-certificate \     https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv \     -O /tmp/bbc-text.csv 
ASSIGN = pd.read_csv('path') ASSIGN = list(data['category']) ASSIGN = list(data['text']),1,not_existent,### Load data data = pd.read_csv('/tmp/bbc-text.csv') label = list(data['category']) sentences_raw = list(data['text'])
CHECKPOINT print(len(sentences_raw)) print(sentences_raw[0]),0,not_existent,### Print the first expected output print(len(sentences_raw)) print(sentences_raw[0])
"SETUP ASSIGN = [ ""a"", ""about"", ""above"", ""after"", ""again"", ""against"", ""all"", ""am"", ""an"", ""and"", ""any"", ""are"", ""as"", ""at"", ""be"", ""because"", ""been"", ""before"", ""being"", ""below"", ""between"", ""both"", ""but"", ""by"", ""could"", ""did"", ""do"", ""does"", ""doing"", ""down"", ""during"", ""each"", ""few"", ""for"", ""from"", ""further"", ""had"", ""has"", ""have"", ""having"", ""he"", ""he'd"", ""he'll"", ""he's"", ""her"", ""here"", ""here's"", ""hers"", ""herself"", ""him"", ""himself"", ""his"", ""how"", ""how's"", ""i"", ""i'd"", ""i'll"", ""i'm"", ""i've"", ""if"", ""in"", ""into"", ""is"", ""it"", ""it's"", ""its"", ""itself"", ""let's"", ""me"", ""more"", ""most"", ""my"", ""myself"", ""nor"", ""of"", ""on"", ""once"", ""only"", ""or"", ""other"", ""ought"", ""our"", ""ours"", ""ourselves"", ""out"", ""over"", ""own"", ""same"", ""she"", ""she'd"", ""she'll"", ""she's"", ""should"", ""so"", ""some"", ""such"", ""than"", ""that"", ""that's"", ""the"", ""their"", ""theirs"", ""them"", ""themselves"", ""then"", ""there"", ""there's"", ""these"", ""they"", ""they'd"", ""they'll"", ""they're"", ""they've"", ""this"", ""those"", ""through"", ""to"", ""too"", ""under"", ""until"", ""up"", ""very"", ""was"", ""we"", ""we'd"", ""we'll"", ""we're"", ""we've"", ""were"", ""what"", ""what's"", ""when"", ""when's"", ""where"", ""where's"", ""which"", ""while"", ""who"", ""who's"", ""whom"", ""why"", ""why's"", ""with"", ""would"", ""you"", ""you'd"", ""you'll"", ""you're"", ""you've"", ""your"", ""yours"", ""yourself"", ""yourselves"" ] ASSIGN = [] for t in tqdm(sentences_raw) : ASSIGN = word_tokenize(t) ASSIGN = [i for i in tokenize_text if i not in stopwords] ASSIGN.append(("" "").join(ASSIGN))",1,not_existent,"### Remove stopwords from data from nltk.tokenize import word_tokenize stopwords = [ ""a"", ""about"", ""above"", ""after"", ""again"", ""against"", ""all"", ""am"", ""an"", ""and"", ""any"", ""are"", ""as"", ""at"", ""be"", ""because"", ""been"", ""before"", ""being"", ""below"", ""between"", ""both"", ""but"", ""by"", ""could"", ""did"", ""do"", ""does"", ""doing"", ""down"", ""during"", ""each"", ""few"", ""for"", ""from"", ""further"", ""had"", ""has"", ""have"", ""having"", ""he"", ""he'd"", ""he'll"", ""he's"", ""her"", ""here"", ""here's"", ""hers"", ""herself"", ""him"", ""himself"", ""his"", ""how"", ""how's"", ""i"", ""i'd"", ""i'll"", ""i'm"", ""i've"", ""if"", ""in"", ""into"", ""is"", ""it"", ""it's"", ""its"", ""itself"", ""let's"", ""me"", ""more"", ""most"", ""my"", ""myself"", ""nor"", ""of"", ""on"", ""once"", ""only"", ""or"", ""other"", ""ought"", ""our"", ""ours"", ""ourselves"", ""out"", ""over"", ""own"", ""same"", ""she"", ""she'd"", ""she'll"", ""she's"", ""should"", ""so"", ""some"", ""such"", ""than"", ""that"", ""that's"", ""the"", ""their"", ""theirs"", ""them"", ""themselves"", ""then"", ""there"", ""there's"", ""these"", ""they"", ""they'd"", ""they'll"", ""they're"", ""they've"", ""this"", ""those"", ""through"", ""to"", ""too"", ""under"", ""until"", ""up"", ""very"", ""was"", ""we"", ""we'd"", ""we'll"", ""we're"", ""we've"", ""were"", ""what"", ""what's"", ""when"", ""when's"", ""where"", ""where's"", ""which"", ""while"", ""who"", ""who's"", ""whom"", ""why"", ""why's"", ""with"", ""would"", ""you"", ""you'd"", ""you'll"", ""you're"", ""you've"", ""your"", ""yours"", ""yourself"", ""yourselves"" ] sentences = []  for t in tqdm(sentences_raw) :     tokenize_text = word_tokenize(t)     list_text = [i for i in tokenize_text if i not in stopwords]     sentences.append(("" "").join(list_text))"
"SETUP CHECKPOINT ASSIGN = Tokenizer(oov_token='<OOV>', ) ASSIGN.fit_on_texts(sentences) ASSIGN = tokenizer.ASSIGN print(len(ASSIGN))",0,not_existent,"### Tokenize the text data from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences tokenizer = Tokenizer(oov_token='<OOV>',                       #num_words to specify maximum number of token based on frequency                      )  # Get the token dict from data tokenizer.fit_on_texts(sentences) word_index = tokenizer.word_index print(len(word_index))"
"ASSIGN = tokenizer.texts_to_sequences(sentences) ASSIGN = pad_sequences(sequences, padding='post', truncating='pre', maxlen=None)",1,not_existent,"### Pad the text data sequences = tokenizer.texts_to_sequences(sentences) padded = pad_sequences(sequences, padding='post', truncating='pre', maxlen=None)"
CHECKPOINT print(padded[0]) print(padded.shape),0,not_existent,### Print the second output print(padded[0]) print(padded.shape)
SETUP CHECKPOINT ASSIGN = Tokenizer() ASSIGN.fit_on_texts(label) ASSIGN = tokenizer_label.word_index ASSIGN = tokenizer_label.texts_to_sequences(label) print(ASSIGN) print(ASSIGN),1,not_existent,### Tokenize the label data from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences tokenizer_label = Tokenizer()  # Get the token dict from data tokenizer_label.fit_on_texts(label) label_word_index = tokenizer_label.word_index label_seq = tokenizer_label.texts_to_sequences(label)  print(label_seq) print(label_word_index)
CHECKPOINT lab_tokenizer.word_index,0,execute_result,lab_tokenizer.word_index
SETUP,0,not_existent,### Define variable for tokenizing and modelling # Reduce this parameter to reduce overfitting VOCAB_SIZE = 1000 EMBEDDING_DIM = 16 MAX_LENGTH = 120  TRUNC_TYPE = 'post' PADDING_TYPE = 'post' OOV_TOK = '<OOV>' TRAIN_PROP = .8
CHECKPOINT ASSIGN = int(TRAIN_PROP * len(sentences)) ASSIGN = sentences[:train_size] ASSIGN = label[:train_size] ASSIGN = sentences[train_size:] ASSIGN = label[train_size:] print(ASSIGN) print(len(ASSIGN)) print(len(ASSIGN)) print(len(ASSIGN)) print(len(ASSIGN)),1,stream,### Split dataset train_size = int(TRAIN_PROP * len(sentences)) train_sentences = sentences[:train_size] train_label = label[:train_size] val_sentences = sentences[train_size:] val_label = label[train_size:]  print(train_size) print(len(train_sentences)) print(len(train_label)) print(len(val_sentences)) print(len(val_label))
"SETUP ASSIGN = Tokenizer(oov_token=OOV_TOK, num_words=VOCAB_SIZE) ASSIGN.fit_on_texts(train_sentences) ASSIGN = text_tokenizer.ASSIGN ASSIGN = text_tokenizer.texts_to_sequences(train_sentences) ASSIGN = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)",1,not_existent,"### Tokenize the train sentences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences text_tokenizer = Tokenizer(oov_token=OOV_TOK, num_words=VOCAB_SIZE)  # Get the token dict from data text_tokenizer.fit_on_texts(train_sentences) word_index = text_tokenizer.word_index  # Pad the data train_sequences = text_tokenizer.texts_to_sequences(train_sentences) train_padded = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
"ASSIGN = text_tokenizer.texts_to_sequences(val_sentences) ASSIGN = pad_sequences(val_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)",1,not_existent,"### Pad the validation data val_sequences = text_tokenizer.texts_to_sequences(val_sentences) val_padded = pad_sequences(val_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
SETUP ASSIGN = Tokenizer() ASSIGN.fit_on_texts(train_label) ASSIGN = np.array(lab_tokenizer.texts_to_sequences(train_label)) ASSIGN = np.array(lab_tokenizer.texts_to_sequences(val_label)),1,not_existent,### Tokenize the label from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences lab_tokenizer = Tokenizer()  # Get the token dict from data lab_tokenizer.fit_on_texts(train_label)  # Get label sequences train_label_seq = np.array(lab_tokenizer.texts_to_sequences(train_label)) val_label_seq = np.array(lab_tokenizer.texts_to_sequences(val_label))
"SETUP ASSIGN = Sequential() ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH)) ASSIGN.add(GlobalAveragePooling1D()) ASSIGN.add(Dense(24, activation='relu')) ASSIGN.add(Dense(6, activation='softmax')) ASSIGN.compile(optimizer = Adam(), ASSIGN = 'sparse_categorical_crossentropy', ASSIGN = 'accuracy') ASSIGN.summary()",0,stream,"### Make simple Embedding MLP model from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D from tensorflow.keras import Sequential, Model from tensorflow.keras.optimizers import SGD, Adam, RMSprop from tensorflow.keras.metrics import Precision  model = Sequential() model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH)) model.add(GlobalAveragePooling1D()) model.add(Dense(24, activation='relu')) model.add(Dense(6, activation='softmax'))  # Compile model model.compile(optimizer = Adam(),               loss = 'sparse_categorical_crossentropy',               metrics = 'accuracy')  # Summary of the model model.summary() "
"ASSIGN = 30 ASSIGN = model.fit(train_padded, train_label_seq, ASSIGN=num_epochs, ASSIGN=(val_padded, val_label_seq), ASSIGN=1)",0,stream,"### Train the model num_epochs = 30 history = model.fit(train_padded,                     train_label_seq,                     epochs=num_epochs,                     validation_data=(val_padded, val_label_seq),                     verbose=1)"
"ASSIGN = dict([(value, key) for (key, value) in word_index.items()]) def decode_sentence(text): return ' '.join([ASSIGN.get(i, '?') for i in text])",1,not_existent,"### Make dictionary to reverse the number from tokenizing to text reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])  def decode_sentence(text):     return ' '.join([reverse_word_index.get(i, '?') for i in text]) "
ASSIGN = model.layers[0] ASSIGN = e.get_weights()[0],1,not_existent,### Get the embedding weight for visualization e = model.layers[0] weights = e.get_weights()[0]
"SETUP ASSIGN = io.open('vecs.tsv', 'w', encoding='utf-8') ASSIGN = io.open('meta.tsv', 'w', encoding='utf-8') for word_num in range(1, VOCAB_SIZE): ASSIGN = reverse_word_index[word_num] ASSIGN = weights[word_num] ASSIGN.write(ASSIGN + ""\n"") ASSIGN.write('\t'.join([str(x) for x in ASSIGN]) + ""\n"") ASSIGN.close() ASSIGN.close()",1,not_existent,"### Save the weight import io  out_v = io.open('vecs.tsv', 'w', encoding='utf-8') out_m = io.open('meta.tsv', 'w', encoding='utf-8') for word_num in range(1, VOCAB_SIZE):     word = reverse_word_index[word_num]     embeddings = weights[word_num]     out_m.write(word + ""\n"")     out_v.write('\t'.join([str(x) for x in embeddings]) + ""\n"") out_v.close() out_m.close()  # Use the file to make embedding visualization at https://projector.tensorflow.org/"
SETUP https:path\ -O path,0,stream,### Get data !wget --no-check-certificate \     https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv \     -O /tmp/training_cleaned.csv
"ASSIGN = pd.read_csv('path', names=['label','id','time','query','handle','text']) ASSIGN = ASSIGN.sample(frac=1).reset_index(drop=True) ASSIGN = list(data['text']) ASSIGN = list(data['ASSIGN'].map({0:0, 4:1}))",1,not_existent,"### Load the data data = pd.read_csv('/tmp/training_cleaned.csv', names=['label','id','time','query','handle','text']) data = data.sample(frac=1).reset_index(drop=True) sentences = list(data['text']) label = list(data['label'].map({0:0, 4:1}))"
SETUP,0,not_existent,### Define variable for tokenizing and modelling EMBEDDING_DIM = 100 MAX_LENGTH = 16 TRUNC_TYPE = 'post' PADDING_TYPE = 'post' OOV_TOK = '<OOV>' TRAIN_PROP = .8
"SETUP ASSIGN = Tokenizer(oov_token=OOV_TOK) ASSIGN.fit_on_texts(train_sentences) ASSIGN = text_tokenizer.ASSIGN ASSIGN = text_tokenizer.texts_to_sequences(train_sentences) ASSIGN = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)",1,not_existent,"### Tokenize the train sentences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences text_tokenizer = Tokenizer(oov_token=OOV_TOK)  # Get the token dict from data text_tokenizer.fit_on_texts(train_sentences) word_index = text_tokenizer.word_index  # Pad the data train_sequences = text_tokenizer.texts_to_sequences(train_sentences) train_padded = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
SETUP,0,not_existent,### Define vocab size VOCAB_SIZE = len(word_index) + 1
"SETUP https:path\ -O path ASSIGN = {}; with open('path') as f: for line in f: ASSIGN = line.split(); ASSIGN = values[0]; ASSIGN = np.asarray(values[1:], dtype='float32'); ASSIGN[ASSIGN] = ASSIGN; ASSIGN = np.zeros((VOCAB_SIZE, EMBEDDING_DIM)); for ASSIGN, i in word_index.items(): ASSIGN = embeddings_index.get(word); if ASSIGN is not None: ASSIGN[i] = ASSIGN;",1,stream,"### Get weight for embedding matrix in model !wget --no-check-certificate \     https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \     -O /tmp/glove.6B.100d.txt      # Load the weight embeddings_index = {}; with open('/tmp/glove.6B.100d.txt') as f:     for line in f:         values = line.split();         word = values[0];         coefs = np.asarray(values[1:], dtype='float32');         embeddings_index[word] = coefs;  # Make weight matrix embeddings_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM)); for word, i in word_index.items():     embedding_vector = embeddings_index.get(word);     if embedding_vector is not None:         embeddings_matrix[i] = embedding_vector;"
"SETUP ASSIGN = Sequential() ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH, ASSIGN=[embeddings_matrix], trainable=False)) ASSIGN.add(GlobalAveragePooling1D()) ASSIGN.add(Dense(128, activation='relu')) ASSIGN.add(Dense(1, activation='sigmoid')) ASSIGN.compile(optimizer = Adam(), ASSIGN = 'binary_crossentropy', ASSIGN = 'accuracy') ASSIGN = Sequential() ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH, ASSIGN=[embeddings_matrix], trainable=False)) ASSIGN.add(Bidirectional(LSTM(64))) ASSIGN.add(Dense(128, activation='relu')) ASSIGN.add(Dense(1, activation='sigmoid')) ASSIGN.compile(optimizer = Adam(), ASSIGN = 'binary_crossentropy', ASSIGN = 'accuracy') ASSIGN = Sequential() ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH, ASSIGN=[embeddings_matrix], trainable=False)) ASSIGN.add(Bidirectional(GRU(64))) ASSIGN.add(Dense(128, activation='relu')) ASSIGN.add(Dense(1, activation='sigmoid')) ASSIGN.compile(optimizer = Adam(), ASSIGN = 'binary_crossentropy', ASSIGN = 'accuracy') ASSIGN = Sequential() ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH, ASSIGN=[embeddings_matrix], trainable=False)) ASSIGN.add(Conv1D(64, 5, activation='relu')) ASSIGN.add(GlobalMaxPooling1D()) ASSIGN.add(Dense(128, activation='relu')) ASSIGN.add(Dense(1, activation='sigmoid')) ASSIGN.compile(optimizer = Adam(), ASSIGN = 'binary_crossentropy', ASSIGN = 'accuracy')",0,not_existent,"### Make model from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, LSTM, Bidirectional, GRU from tensorflow.keras import Sequential, Model from tensorflow.keras.optimizers import SGD, Adam, RMSprop from tensorflow.keras.metrics import Precision  # Make simple embedding model model_simple = Sequential() model_simple.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,                     weights=[embeddings_matrix], trainable=False)) model_simple.add(GlobalAveragePooling1D()) model_simple.add(Dense(128, activation='relu')) model_simple.add(Dense(1, activation='sigmoid'))  model_simple.compile(optimizer = Adam(),               loss = 'binary_crossentropy',               metrics = 'accuracy')  # Make single LSTM model model_single_lstm = Sequential() model_single_lstm.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,                     weights=[embeddings_matrix], trainable=False)) model_single_lstm.add(Bidirectional(LSTM(64))) model_single_lstm.add(Dense(128, activation='relu')) model_single_lstm.add(Dense(1, activation='sigmoid'))  model_single_lstm.compile(optimizer = Adam(),               loss = 'binary_crossentropy',               metrics = 'accuracy')  # Make single GRU model_single_gru = Sequential() model_single_gru.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,                     weights=[embeddings_matrix], trainable=False)) model_single_gru.add(Bidirectional(GRU(64))) model_single_gru.add(Dense(128, activation='relu')) model_single_gru.add(Dense(1, activation='sigmoid'))  model_single_gru.compile(optimizer = Adam(),               loss = 'binary_crossentropy',               metrics = 'accuracy')  # Make single Conv model_single_conv = Sequential() model_single_conv.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,                     weights=[embeddings_matrix], trainable=False)) model_single_conv.add(Conv1D(64, 5, activation='relu')) model_single_conv.add(GlobalMaxPooling1D()) model_single_conv.add(Dense(128, activation='relu')) model_single_conv.add(Dense(1, activation='sigmoid'))  model_single_conv.compile(optimizer = Adam(),               loss = 'binary_crossentropy',               metrics = 'accuracy')"
"def plot_history(history) : rcParams['figure.figsize'] = [10,8] plt.style.use('fivethirtyeight') sns.set_style('whitegrid') ASSIGN = gridspec.GridSpec(2,1) ASSIGN   = history.history[   'accuracy' ] ASSIGN = history.history[ 'val_accuracy' ] ASSIGN   = history.history[  'ASSIGN' ] ASSIGN = history.history['ASSIGN' ] ASSIGN  = range(len(acc)) ASSIGN = plt.subplot(grid[0]) ASSIGN.plot(ASSIGN, ASSIGN, label='Train Loss') ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Loss') ASSIGN.set_title('Training and Validation Loss') ASSIGN.legend() ; ASSIGN = plt.subplot(grid[1]) ASSIGN.plot(ASSIGN, ASSIGN, label='Train Acc') ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Acc') ASSIGN.set_title('Training and Validation Accuracy') ASSIGN.legend() ;",0,not_existent,"### Plot the performance of the model def plot_history(history) :     rcParams['figure.figsize'] = [10,8]     plt.style.use('fivethirtyeight')      sns.set_style('whitegrid')     grid = gridspec.GridSpec(2,1)      # Get the metrics and loss     acc      = history.history[     'accuracy' ]     val_acc  = history.history[ 'val_accuracy' ]     loss     = history.history[    'loss' ]     val_loss = history.history['val_loss' ]     epo   = range(len(acc)) # Get number of epochs      # Plot the loss     ax = plt.subplot(grid[0])     ax.plot(epo, loss, label='Train Loss')     ax.plot(epo, val_loss, label='Validation Loss')     ax.set_title('Training and Validation Loss')     ax.legend() ;      # Plot the acccuracy     ax = plt.subplot(grid[1])     ax.plot(epo, acc, label='Train Acc')     ax.plot(epo, val_acc, label='Validation Acc')     ax.set_title('Training and Validation Accuracy')     ax.legend() ;"
"SETUP ASSIGN = 5 ASSIGN = model_simple.fit(train_padded, np.array(train_label), ASSIGN=num_epochs, ASSIGN=(val_padded, np.array(val_label)), ASSIGN=1)",0,not_existent,"### Train simple model num_epochs = 5 history_simple = model_simple.fit(train_padded,                     np.array(train_label),                     epochs=num_epochs,                     validation_data=(val_padded, np.array(val_label)),                     verbose=1)  %time"
plot_history(history_simple),0,not_existent,### Plot simple model performance plot_history(history_simple)
"SETUP ASSIGN = 10 ASSIGN = model_single_lstm.fit(train_padded, np.array(train_label), ASSIGN=num_epochs, ASSIGN=(val_padded, np.array(val_label)), ASSIGN=1)",0,stream,"### Train single lstm model num_epochs = 10 history_single_lstm = model_single_lstm.fit(train_padded,                     np.array(train_label),                     epochs=num_epochs,                     validation_data=(val_padded, np.array(val_label)),                     verbose=1)  %time"
plot_history(history_single_lstm),0,display_data,### Plot simple model performance plot_history(history_single_lstm)
"SETUP ASSIGN = 5 ASSIGN = model_single_gru.fit(train_padded, np.array(train_label), ASSIGN=num_epochs, ASSIGN=(val_padded, np.array(val_label)), ASSIGN=1)",0,not_existent,"### Train single gru model num_epochs = 5 history_single_gru = model_single_gru.fit(train_padded,                     np.array(train_label),                     epochs=num_epochs,                     validation_data=(val_padded, np.array(val_label)),                     verbose=1)  %time"
plot_history(history_single_gru),0,not_existent,### Plot simple model performance plot_history(history_single_gru)
"SETUP ASSIGN = 5 ASSIGN = model_single_conv.fit(train_padded, np.array(train_label), ASSIGN=num_epochs, ASSIGN=(val_padded, np.array(val_label)), ASSIGN=1)",0,not_existent,"### Train single conv model num_epochs = 5 history_single_conv = model_single_conv.fit(train_padded,                     np.array(train_label),                     epochs=num_epochs,                     validation_data=(val_padded, np.array(val_label)),                     verbose=1)  %time"
plot_history(history_single_conv),0,not_existent,### Plot simple model performance plot_history(history_single_conv)
"SETUP ASSIGN = Sequential() ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH, ASSIGN=[embeddings_matrix], trainable=False)) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Conv1D(64, 5, activation='relu')) ASSIGN.add(MaxPooling1D(4)) ASSIGN.add(LSTM(64)) ASSIGN.add(Dense(1, activation='sigmoid')) ASSIGN.compile(optimizer = Adam(), ASSIGN = 'binary_crossentropy', ASSIGN = 'accuracy')",0,not_existent,"### Make complicated model from tensorflow.keras.layers import Dense, Embedding, Dropout, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, LSTM, Bidirectional, GRU from tensorflow.keras import Sequential, Model from tensorflow.keras.optimizers import SGD, Adam, RMSprop from tensorflow.keras.metrics import Precision  # Make simple embedding model model = Sequential() model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,                     weights=[embeddings_matrix], trainable=False)) model.add(Dropout(0.2)) model.add(Conv1D(64, 5, activation='relu')) model.add(MaxPooling1D(4)) model.add(LSTM(64)) model.add(Dense(1, activation='sigmoid'))  model.compile(optimizer = Adam(),               loss = 'binary_crossentropy',               metrics = 'accuracy')"
"SETUP ASSIGN = 5 ASSIGN = model.fit(train_padded, np.array(train_label), ASSIGN=num_epochs, ASSIGN=(val_padded, np.array(val_label)), ASSIGN=1)",0,not_existent,"### Train single conv model num_epochs = 5 history = model.fit(train_padded,                     np.array(train_label),                     epochs=num_epochs,                     validation_data=(val_padded, np.array(val_label)),                     verbose=1)  %time"
plot_history(history),0,not_existent,### Plot simple model performance plot_history(history)
SETUP https:path\ -O path,0,stream,### Get data !wget --no-check-certificate \     https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \     -O /tmp/sonnets.txt
ASSIGN = open('path').read() ASSIGN = data.lower().split('\n'),1,not_existent,### Load the data data = open('/tmp/sonnets.txt').read() corpus = data.lower().split('\n')
SETUP,0,not_existent,### Define variable for tokenizing and modelling EMBEDDING_DIM = 100 TRUNC_TYPE = 'post' PADDING_TYPE = 'pre' OOV_TOK = '<OOV>' TRAIN_PROP = .8
"SETUP SETUP ASSIGN = Tokenizer() ASSIGN.fit_on_texts(corpus) ASSIGN = tokenizer.ASSIGN ASSIGN = [] for text in corpus : ASSIGN = tokenizer.texts_to_sequences([text])[0] for i in range(1, len(ASSIGN)) : ASSIGN = token_list[:i+1] ASSIGN.append(ASSIGN) ASSIGN = pad_sequences(train_seq, padding=PADDING_TYPE, maxlen=MAX_LENGTH) ASSIGN = train_padded[:,:-1], train_padded[:,-1] ASSIGN = to_categorical(ASSIGN, num_classes=VOCAB_SIZE)",1,not_existent,"### Make the input sequences for prediciton word from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences tokenizer = Tokenizer()  # Tokenize the corpus tokenizer.fit_on_texts(corpus) word_index = tokenizer.word_index VOCAB_SIZE = len(word_index) + 1  # Make input sequences train_seq = [] for text in corpus :     token_list = tokenizer.texts_to_sequences([text])[0]          for i in range(1, len(token_list)) :         gram = token_list[:i+1]         train_seq.append(gram)          # Pad the sequences MAX_LENGTH = np.max([len(seq) for seq in train_seq]) train_padded = pad_sequences(train_seq, padding=PADDING_TYPE, maxlen=MAX_LENGTH)  # Split to train data and label train_data, train_label = train_padded[:,:-1], train_padded[:,-1]  # One hot label from tensorflow.keras.utils import to_categorical train_label = to_categorical(train_label, num_classes=VOCAB_SIZE)"
"SETUP ASSIGN = Sequential() ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH-1, ASSIGN=[embeddings_matrix], trainable=True)) ASSIGN.add(Bidirectional(LSTM(256, return_sequences=True))) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Bidirectional(LSTM(128))) ASSIGN.add(Dense(VOCAB_SIZE path, activation='relu', kernel_regularizer=l2(0.01))) ASSIGN.add(Dense(VOCAB_SIZE, activation='softmax')) ASSIGN.compile(optimizer = Adam(), ASSIGN = 'categorical_crossentropy', ASSIGN = 'accuracy')",0,not_existent,"### Make model from tensorflow.keras.layers import Dense, Embedding, TimeDistributed, Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, LSTM, Bidirectional, GRU from tensorflow.keras import Sequential, Model from tensorflow.keras.regularizers import l1, l2 from tensorflow.keras.optimizers import SGD, Adam, RMSprop from tensorflow.keras.metrics import Precision  # Make simple embedding model model = Sequential() model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH-1,                     weights=[embeddings_matrix], trainable=True)) model.add(Bidirectional(LSTM(256, return_sequences=True))) model.add(Dropout(0.2)) model.add(Bidirectional(LSTM(128))) model.add(Dense(VOCAB_SIZE // 2, activation='relu', kernel_regularizer=l2(0.01))) model.add(Dense(VOCAB_SIZE, activation='softmax'))  model.compile(optimizer = Adam(),               loss = 'categorical_crossentropy',               metrics = 'accuracy')"
"SETUP ASSIGN = 100 ASSIGN = model.fit(train_data, train_label, ASSIGN=num_epochs, ASSIGN=1)",0,stream,"### Train model num_epochs = 100 history = model.fit(train_data,                     train_label,                     epochs=num_epochs,                     verbose=1)  %time"
"def plot_history(history) : rcParams['figure.figsize'] = [10,8] plt.style.use('fivethirtyeight') sns.set_style('whitegrid') ASSIGN = gridspec.GridSpec(2,1) ASSIGN   = history.history[   'accuracy' ] ASSIGN   = history.history[  'ASSIGN' ] ASSIGN  = range(len(acc)) ASSIGN = plt.subplot(grid[0]) ASSIGN.plot(ASSIGN, ASSIGN, label='Train Loss') ASSIGN.set_title('Training Loss') ASSIGN.legend() ; ASSIGN = plt.subplot(grid[1]) ASSIGN.plot(ASSIGN, ASSIGN, label='Train Acc') ASSIGN.set_title('Training Accuracy') ASSIGN.legend() ;",0,not_existent,"### Plot the performance of the model def plot_history(history) :     rcParams['figure.figsize'] = [10,8]     plt.style.use('fivethirtyeight')      sns.set_style('whitegrid')     grid = gridspec.GridSpec(2,1)      # Get the metrics and loss     acc      = history.history[     'accuracy' ]     loss     = history.history[    'loss' ]     epo   = range(len(acc)) # Get number of epochs      # Plot the loss     ax = plt.subplot(grid[0])     ax.plot(epo, loss, label='Train Loss')     ax.set_title('Training Loss')     ax.legend() ;      # Plot the acccuracy     ax = plt.subplot(grid[1])     ax.plot(epo, acc, label='Train Acc')     ax.set_title('Training Accuracy')     ax.legend() ;"
plot_history(history),0,display_data,### Plot model performance - LSTM no Bidirection plot_history(history)
"CHECKPOINT ASSIGN = ""Help me Obi Wan Kenobi, you're my only hope"" ASSIGN = 100 for _ in range(ASSIGN): ASSIGN = tokenizer.texts_to_sequences([seed_text])[0] ASSIGN = pad_sequences([ASSIGN], maxlen=MAX_LENGTH-1, padding='pre') ASSIGN = model.predict_classes(token_list, verbose=0) ASSIGN = """" for word, index in tokenizer.word_index.items(): ASSIGN == predicted: ASSIGN = word break ASSIGN += "" "" + ASSIGN print(ASSIGN)",0,stream,"seed_text = ""Help me Obi Wan Kenobi, you're my only hope"" next_words = 100    for _ in range(next_words):     token_list = tokenizer.texts_to_sequences([seed_text])[0]     token_list = pad_sequences([token_list], maxlen=MAX_LENGTH-1, padding='pre')     predicted = model.predict_classes(token_list, verbose=0)     output_word = """"     for word, index in tokenizer.word_index.items():         if index == predicted:             output_word = word             break     seed_text += "" "" + output_word print(seed_text)"
SETUP,0,stream,!python -m pip install dtw  import numpy as np from dtw import dtw
"CHECKPOINT ASSIGN = np.array([1,2,3,4,3,2,1,1,1,2]) ASSIGN = np.array([0,1,1,2,3,4,3,2,1,1]) ASSIGN = lambda s1,s2: np.abs(s1 - s2) ASSIGN = dtw( s1,s2, dist=manhattan_distance) print(d)",1,stream,"   s1 = np.array([1,2,3,4,3,2,1,1,1,2])  s2 = np.array([0,1,1,2,3,4,3,2,1,1])     manhattan_distance = lambda s1,s2: np.abs(s1 - s2)  d, cost_matrix, acc_cost_matrix, path = dtw( s1,s2, dist=manhattan_distance)  print(d)"
SETUP,0,stream,!pip install dtaidistance from dtaidistance import dtw from dtaidistance import dtw_visualisation as dtwvis import random import numpy as np
"ASSIGN = np.arange(0, 20, .5) ASSIGN = np.sin(x) ASSIGN = np.sin(x - 1) random.seed(1) for idx in range(len(ASSIGN)): if random.random() < 0.05: ASSIGN[idx] += (random.random() - 0.5) path ASSIGN = dtw.warping_paths(s1, s2, window=25, psi=2) ASSIGN = dtw.ASSIGN(paths) dtwvis.plot_warpingpaths(ASSIGN, ASSIGN, paths, ASSIGN)",0,execute_result," x = np.arange(0, 20, .5) s1 = np.sin(x) s2 = np.sin(x - 1) random.seed(1) for idx in range(len(s2)):     if random.random() < 0.05:         s2[idx] += (random.random() - 0.5) / 2 d, paths = dtw.warping_paths(s1, s2, window=25, psi=2) best_path = dtw.best_path(paths) dtwvis.plot_warpingpaths(s1, s2, paths, best_path) "
"CHECKPOINT ASSIGN = dtw.ASSIGN(s1, s2) print(ASSIGN)",1,stream,"distance = dtw.distance(s1, s2) print(distance)"
SETUP,0,not_existent,import cv2  import matplotlib.pyplot as plt from matplotlib.patches import Rectangle 
"ASSIGN = cv2.CascadeClassifier('..path') ASSIGN =cv2.imread( ""..path"",0)",0,not_existent,"face_cascade = cv2.CascadeClassifier('../input/opencv-haarcascade/data/haarcascades/haarcascade_frontalface_default.xml') gray =cv2.imread( ""../input/opencv-samples-images/data/lena.jpg"",0)"
"plt.figure(figsize=(10, 10)) ASSIGN = face_cascade.detectMultiScale(gray, 1.3, 5) for (x,y,w,h) in ASSIGN: ASSIGN = plt.gca() ASSIGN.add_patch( Rectangle((x,y), w, h, ASSIGN ='none', ASSIGN ='b', ASSIGN = 4) ) plt.imshow(gray,cmap = 'gray') plt.title('template'), plt.xticks([]), plt.yticks([]) plt.show()",0,display_data,"plt.figure(figsize=(10, 10)) faces = face_cascade.detectMultiScale(gray, 1.3, 5) for (x,y,w,h) in faces:     ax = plt.gca()     ax.add_patch( Rectangle((x,y),                         w,   h,                         fc ='none',                           ec ='b',                          lw = 4) )   plt.imshow(gray,cmap = 'gray') plt.title('template'), plt.xticks([]), plt.yticks([])  plt.show()"
"SETUP CHECKPOINT print(check_output([, ]).decode())",0,stream,"from subprocess import check_output
 print(check_output([""ls"", ""../input""]).decode(""utf8""))"
"SETUP """""" A Python Module to do automated Exploratory Data Analysis and some light weight data prep. https:path """""" ASSIGN = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64'] def full_report(df, target_column=None): """"""Tries to run every possible report on the provided dataframe"""""" display(HTML(""<h1>Lazy EDA Report<path>"")) breakdown_date(df) show_dtypes(df) plot_nulls(df) plot_long_lat(df, target_column) if target_column is not None: plot_scatter_target(df, target_column) plot_hist_target(df, target_column) plot_correlations(df, target_column) def plot_correlations(df, target_column): display(HTML(""<h2>Column Data Types<path>"")) display(HTML(""<p>Below is a plot of the correlation coefficients of the dataframe's numeric columns and the target column<path>"")) ASSIGN = df.select_dtypes(include=numerics) del(ASSIGN[target_column]) ASSIGN.corrwith(df[target_column]).sort_values(ascending=False).plot( ASSIGN='barh', figsize=(12,12), title=""Correlation Coefficient with Target"") plt.show() def breakdown_date(df): """""" Creates new columns in a dataframe representing the components of a date (year, month, day of year, & week day name) """""" ASSIGN = df.dtypes[df.dtypes == 'datetime64[ns]'].index display(HTML(""<h2>Breaking down date columns<path>"")) if len(ASSIGN) > 0: display(HTML(""<p>The following columns will be broken down into year, month, day of year, and weekday columns<path> <ul>"")) for date_column in ASSIGN: display(HTML(""<li>{}<path>"".format(date_column))) df['{}_year'.format(date_column)] = df[date_column].dt.year df['{}_month'.format(date_column)] = df[date_column].dt.month df['{}_dayofyear'.format(date_column)] = df[date_column].dt.dayofyear df['{}_weekday'.format(date_column)] = df[date_column].dt.weekday_name display(HTML(""<path>"")) else: display(HTML(""<p>No Date columns found to breakdown.<path>"")) return df def plot_nulls(df): """""" Displays a horizontal bar chart representing the percentage of nulls in each column """""" display(HTML(""<h2>Plot Nulls<path>"")) ASSIGN = df.isnull().sum()path[0]*100 ASSIGN = null_percentage[null_percentage > 0].sort_values() if len(ASSIGN) > 0: display(HTML(""<p>The plot below shows the percentage of NaNs in each column in the dataframe<path>"")) ASSIGN.plot(ASSIGN='barh', figsize=(12,12), title=""Plot Null Percentages"") plt.show() else: display(HTML(""<p>The dataframe does not contain any missing data<path>"")) return null_percentage_filtered def show_dtypes(df): """"""Shows the data types of all columns"""""" display(HTML(""<h2>Column Data Types<path>"")) ASSIGN = pd.options.display.max_rows pd.options.display.max_rows = len(df.columns) ASSIGN = pd.DataFrame({""Column Name"": df.dtypes.index,""DType"": df.dtypes.values}) display(ASSIGN) pd.options.display.max_rows = ASSIGN def plot_scatter_target(df, target_column): """"""Plots a sorted scatter plot of the values in a numerical target column"""""" display(HTML(""<h2>Plot Scatter Target<path>"")) display(HTML(""<p>Below is a sorted scatter plot of the values in the target column<path>"")) plt.scatter(range(df[target_column].shape[0]), np.sort(df[target_column].values)) plt.xlabel('index', fontsize=12) plt.ylabel(target_column, fontsize=12) plt.show() def plot_hist_target(df, target_column): display(HTML(""<h2>Plot Histogram Target<path>"")) display(HTML(""<p>Below is a histogram of the values in the target column<path>"")) ASSIGN = np.percentile(df.logerror.values, 99) ASSIGN = np.percentile(df.logerror.values, 1) ASSIGN = ASSIGN df['tempTarget'].ix[df['tempTarget']>ASSIGN] = ASSIGN df['tempTarget'].ix[df['tempTarget']<ASSIGN] = ASSIGN plt.figure(figsize=(12,8)) sns.distplot(df['tempTarget']) plt.xlabel(target_column, fontsize=12) plt.show() del[df['tempTarget']] def plot_long_lat(df, target_column): if 'latitude' in df.columns.str.lower() and 'longitude' in df.columns.str.lower(): display(HTML(""<h2>Plot longitudepath<path>"")) display(HTML(""<p>Below is a scatter plot of longpath<path>"")) plt.figure(figsize=(12,12)) if target_column is None: sns.jointplot(x=df.latitude.values, y=df.longitude.values, size=10) else: ASSIGN = (ASSIGN - ASSIGN.min())path(ASSIGN.max() - ASSIGN.min()) plt.scatter(x=df.latitude.values, y=df.longitude.values, c=df['tempTarget'].values) del(df['tempTarget']) plt.ylabel('Longitude', fontsize=12) plt.xlabel('Latitude', fontsize=12) plt.show()",0,not_existent,""""""" 
 A Python Module to do automated Exploratory Data Analysis and some light weight data prep.
 https://github.com/TareqAlKhatib/Lazy-EDA
 """"""
 import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt
 import seaborn as sns
 from IPython.display import display, HTML
 
 numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
     
 def full_report(df, target_column=None):
 	""""""Tries to run every possible report on the provided dataframe""""""
 	display(HTML(""<h1>Lazy EDA Report</h1>""))
 	
 	breakdown_date(df)
 	show_dtypes(df)
 	plot_nulls(df)
 	plot_long_lat(df, target_column)
 	if target_column is not None:
 		plot_scatter_target(df, target_column)
 		plot_hist_target(df, target_column)
 		plot_correlations(df, target_column)            
 
 def plot_correlations(df, target_column):
 	display(HTML(""<h2>Column Data Types</h2>""))
 	display(HTML(""<p>Below is a plot of the correlation coefficients of the dataframe's numeric columns and the target column</p>""))
 	
 	num_df = df.select_dtypes(include=numerics)
 	del(num_df[target_column])
 	num_df.corrwith(df[target_column]).sort_values(ascending=False).plot(
 		kind='barh', figsize=(12,12), title=""Correlation Coefficient with Target"")
 	plt.show()
 	
 def breakdown_date(df):
 	""""""
 	Creates new columns in a dataframe representing the components of a date (year, month, day of year, & week day name)
 	""""""
 	date_cols = df.dtypes[df.dtypes == 'datetime64[ns]'].index
 	display(HTML(""<h2>Breaking down date columns</h2>""))
 	if len(date_cols) > 0:
 		display(HTML(""<p>The following columns will be broken down into year, month, day of year, and weekday columns</p> <ul>""))
 		
 		for date_column in date_cols:
 			display(HTML(""<li>{}</li>"".format(date_column)))
 			df['{}_year'.format(date_column)] = df[date_column].dt.year
 			df['{}_month'.format(date_column)] = df[date_column].dt.month
 			df['{}_dayofyear'.format(date_column)] = df[date_column].dt.dayofyear
 			df['{}_weekday'.format(date_column)] = df[date_column].dt.weekday_name
 		
 		display(HTML(""</ul>""))
 	else:
 		display(HTML(""<p>No Date columns found to breakdown.</p>""))
 		
 	return df
 
 def plot_nulls(df):
 	""""""
 	Displays a horizontal bar chart representing the percentage of nulls in each column
 	""""""
 	display(HTML(""<h2>Plot Nulls</h2>""))
 	
 	null_percentage = df.isnull().sum()/df.shape[0]*100
 	null_percentage_filtered = null_percentage[null_percentage > 0].sort_values()
 	
 	if len(null_percentage_filtered) > 0:
 		display(HTML(""<p>The plot below shows the percentage of NaNs in each column in the dataframe</p>""))
 		null_percentage_filtered.plot(kind='barh', figsize=(12,12), title=""Plot Null Percentages"")
 		plt.show()
 		
 	else:
 		display(HTML(""<p>The dataframe does not contain any missing data</p>""))
 	return null_percentage_filtered
 
 def show_dtypes(df):
 	""""""Shows the data types of all columns""""""
 	
 	display(HTML(""<h2>Column Data Types</h2>""))
 	
 	# Saving the old display max
 	old_max = pd.options.display.max_rows
 	pd.options.display.max_rows = len(df.columns)
 	
 	# Display DTypes
 	dtype_df = pd.DataFrame({""Column Name"": df.dtypes.index,""DType"": df.dtypes.values})
 	display(dtype_df)
 	
 	# Restoring the old display max
 	pd.options.display.max_rows = old_max
 	
 def plot_scatter_target(df, target_column):
 	""""""Plots a sorted scatter plot of the values in a numerical target column""""""
 	display(HTML(""<h2>Plot Scatter Target</h2>""))
 	display(HTML(""<p>Below is a sorted scatter plot of the values in the target column</p>""))
 	
 	plt.scatter(range(df[target_column].shape[0]), np.sort(df[target_column].values))
 	plt.xlabel('index', fontsize=12)
 	plt.ylabel(target_column, fontsize=12)
 	plt.show()
 
 def plot_hist_target(df, target_column):
 	display(HTML(""<h2>Plot Histogram Target</h2>""))
 	display(HTML(""<p>Below is a histogram of the values in the target column</p>""))
 	
 	# Filter 1st and 99th percentiles
 	ulimit = np.percentile(df.logerror.values, 99)
 	llimit = np.percentile(df.logerror.values, 1)
 	df['tempTarget'] = df[target_column]
 	df['tempTarget'].ix[df['tempTarget']>ulimit] = ulimit
 	df['tempTarget'].ix[df['tempTarget']<llimit] = llimit
 	
 	# Plot
 	plt.figure(figsize=(12,8))
 	sns.distplot(df['tempTarget'])
 	plt.xlabel(target_column, fontsize=12)
 	plt.show()
 	
 	del[df['tempTarget']]
 	
 def plot_long_lat(df, target_column):
 	if 'latitude' in df.columns.str.lower() and 'longitude' in df.columns.str.lower():
 		display(HTML(""<h2>Plot longitude/latitude</h2>""))
 		display(HTML(""<p>Below is a scatter plot of long/lat coordinate in the dataframe</p>""))
 		
 		plt.figure(figsize=(12,12))
 		
 		if target_column is None:
 			sns.jointplot(x=df.latitude.values, y=df.longitude.values, size=10)
 		else:
 			df['tempTarget'] = (df['logerror'] - df['logerror'].min())/(df['logerror'].max() - df['logerror'].min())
 			plt.scatter(x=df.latitude.values, y=df.longitude.values, c=df['tempTarget'].values)
 			del(df['tempTarget'])
 		plt.ylabel('Longitude', fontsize=12)
 		plt.xlabel('Latitude', fontsize=12)
 		plt.show()
"
"ASSIGN = pd.read_csv(""..path"", parse_dates=[""transactiondate""]) ASSIGN = pd.read_csv(""..path"", dtype={ 'hashottuborspa': 'object', 'propertycountylandusecode': 'object', 'propertyzoningdesc': 'object', 'fireplaceflag': 'object', 'taxdelinquencyflag': 'object' }) ASSIGN = pd.merge(ASSIGN, prop_df, on='parcelid', how='left')",1,not_existent,"train_df = pd.read_csv(""../input/train_2016_v2.csv"", parse_dates=[""transactiondate""])
 prop_df = pd.read_csv(""../input/properties_2016.csv"", dtype={
     'hashottuborspa': 'object', 
     'propertycountylandusecode': 'object',
     'propertyzoningdesc': 'object',
     'fireplaceflag': 'object',
     'taxdelinquencyflag': 'object'
 })
 
 train_df = pd.merge(train_df, prop_df, on='parcelid', how='left')"
"full_report(train_df, target_column='logerror')",0,display_data,"full_report(train_df, target_column='logerror')"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed
 # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
 # For example, here's several helpful packages to load in 
 
 import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
 
 # Input data files are available in the ""../input/"" directory.
 # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
 
 import os
 for dirname, _, filenames in os.walk('/kaggle/input'):
     for filename in filenames:
         print(os.path.join(dirname, filename))
 
 # Any results you write to the current directory are saved as output."
"SETUP ASSIGN = 'https:path' ASSIGN = requests.get(url, allow_redirects=True) open('.path', 'wb').write(ASSIGN.content) ASSIGN = 'https:path' ASSIGN = requests.get(url, allow_redirects=True) open('.path', 'wb').write(ASSIGN.content) ASSIGN = 'https:path' ASSIGN = requests.get(url, allow_redirects=True) open('.path', 'wb').write(ASSIGN.content) ASSIGN = 'https:path' ASSIGN = requests.get(url, allow_redirects=True) open('.path', 'wb').write(ASSIGN.content) ASSIGN = 'https:path' ASSIGN = requests.get(url, allow_redirects=True) open('.path', 'wb').write(ASSIGN.content) ASSIGN=datetime.strftime(datetime.today() - timedelta(1), 'https:path%m-%d-%Y.csv') ASSIGN = requests.get(url, allow_redirects=True) open('.path', 'wb').write(ASSIGN.content)",0,execute_result,"import requests
 url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'
 r = requests.get(url, allow_redirects=True)
 open('./time_series_covid19_confirmed_US.csv', 'wb').write(r.content)
 #
 url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
 r = requests.get(url, allow_redirects=True)
 open('./time_series_covid19_confirmed_global.csv', 'wb').write(r.content)
 #
 url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'
 r = requests.get(url, allow_redirects=True)
 open('./time_series_covid19_deaths_US.csv', 'wb').write(r.content)
 #
 url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'
 r = requests.get(url, allow_redirects=True)
 open('./time_series_covid19_deaths_global.csv', 'wb').write(r.content)
 #
 url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'
 r = requests.get(url, allow_redirects=True)
 open('./time_series_covid19_recovered_global.csv', 'wb').write(r.content)
 #
 from datetime import datetime, timedelta
 url=datetime.strftime(datetime.today() - timedelta(1), 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/%m-%d-%Y.csv')
 r = requests.get(url, allow_redirects=True)
 open('./csse_covid_19_daily_reports.csv', 'wb').write(r.content)
"
"SETUP CHECKPOINT ASSIGN = r'..path' ASSIGN = r'.path' shutil.copyfile(ASSIGN, ASSIGN) ASSIGN = r'..path' ASSIGN = r'.path' shutil.copyfile(ASSIGN, ASSIGN) ASSIGN = r'..path' ASSIGN = r'.path' shutil.copyfile(ASSIGN, ASSIGN) ASSIGN = r'..path' ASSIGN = r'.path' shutil.copyfile(ASSIGN, ASSIGN) ASSIGN = r'..path' ASSIGN = r'.path' shutil.copyfile(ASSIGN, ASSIGN) print(os.listdir())",0,stream,"import shutil
 
 original = r'../input/input1/covid19_by_country.csv'
 target = r'./covid19_by_country.csv'
 shutil.copyfile(original, target)
 original = r'../input/input1/GlobalLandTemperaturesByCountry.csv'
 target = r'./GlobalLandTemperaturesByCountry.csv'
 shutil.copyfile(original, target)
 original = r'../input/input1/GlobalLandTemperaturesByMajorCity.csv'
 target = r'./GlobalLandTemperaturesByMajorCity.csv'
 shutil.copyfile(original, target)
 original = r'../input/input1/API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv'
 target = r'./API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv'
 shutil.copyfile(original, target)
 original = r'../input/input1/shift.JPG'
 target = r'./shift.JPG'
 shutil.copyfile(original, target)
 
 print(os.listdir(""./""))
"
"SETUP CHECKPOINT sns.set(style=""white"", color_codes=True) warnings.filterwarnings(""ignore"") print(os.listdir()) pio.renderers.default = ""browser""",0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed
 # Loading datasets required for analysis
 
 import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
 import seaborn as sns
 import matplotlib.pyplot as plt
 sns.set(style=""white"", color_codes=True)
 import warnings # current version of seaborn generates a bunch of warnings that we'll ignore
 warnings.filterwarnings(""ignore"")
 
 # Input data files are available in the ""../input/"" directory.
 import os
 print(os.listdir(""./""))
 #print(os.listdir(""../input/input""))
 %load_ext autoreload
 %autoreload 2
 import plotly.io as pio
 pio.renderers.default = ""browser"""
ASSIGN=pd.read_csv('.path') ASSIGN.sample(10),0,execute_result,"codiv_country=pd.read_csv('./covid19_by_country.csv')
 codiv_country.sample(10)"
codiv_country.describe(),0,execute_result,codiv_country.describe()
codiv_country['Population 2020']=codiv_country['Population 2020']*1000 codiv_country['Tests_per_10kp']=codiv_country['Tests']*10000.0path['Population 2020'] codiv_country['Tests_per_10kp_log1p']=np.log1p(codiv_country['Tests_per_10kp']) codiv_country['GDP 2018 per_1p_log1p']=np.log1p((codiv_country['GDP 2018']path['Population 2020'])),1,not_existent,"#nomalize some datas, also change the population in real size
 codiv_country['Population 2020']=codiv_country['Population 2020']*1000
 codiv_country['Tests_per_10kp']=codiv_country['Tests']*10000.0/codiv_country['Population 2020']
 codiv_country['Tests_per_10kp_log1p']=np.log1p(codiv_country['Tests_per_10kp'])
 codiv_country['GDP 2018 per_1p_log1p']=np.log1p((codiv_country['GDP 2018']/codiv_country['Population 2020']))"
"ASSIGN=ASSIGN.drop([ 'Sex Ratio','Crime Index','Density','Tests_per_10kp','Test Pop','sex0','sex14','sex25','sex54','sex64','sex65plus','Total Infected','Total Deaths','Total Recovered','Tests','GDP 2018','Population 2020' ], axis=1) ASSIGN.sample(10)",1,execute_result,"codiv_country=codiv_country.drop([
     'Sex Ratio','Crime Index','Density','Tests_per_10kp','Test Pop','sex0','sex14','sex25','sex54','sex64','sex65plus','Total Infected','Total Deaths','Total Recovered','Tests','GDP 2018','Population 2020'
 ], axis=1)
 codiv_country.sample(10)"
"ASSIGN=pd.read_csv(os.path.join('.path', 'API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv')) ASSIGN=ASSIGN.set_index('Country Name') ASSIGN.sample(3)",0,execute_result,"Health_expenditure=pd.read_csv(os.path.join('./', 'API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv'))
 Health_expenditure=Health_expenditure.set_index('Country Name')
 Health_expenditure.sample(3)"
"codiv_country['Health expenditure Ratio'] = 0.0 for countryindex in Health_expenditure.index: if not codiv_country[codiv_country['Country']==countryindex].empty : codiv_country.at[codiv_country['Country']==countryindex,'Health expenditure Ratio']=Health_expenditure[Health_expenditure.index==countryindex]['2017'][0]",1,not_existent,"codiv_country['Health expenditure Ratio'] = 0.0
 for countryindex in Health_expenditure.index:
     # If the country exists in the other table
     if not codiv_country[codiv_country['Country']==countryindex].empty :
         codiv_country.at[codiv_country['Country']==countryindex,'Health expenditure Ratio']=Health_expenditure[Health_expenditure.index==countryindex]['2017'][0]
"
"codiv_country.hist(figsize=(12, 12)) plt.show()",0,display_data,"codiv_country.hist(figsize=(12, 12))
 plt.show()"
CHECKPOINT codiv_country.dtypes,0,execute_result,codiv_country.dtypes
codiv_country.isna().sum(),0,execute_result,codiv_country.isna().sum()
codiv_country[codiv_country['Health expenditure Ratio'].isnull()],0,execute_result,codiv_country[codiv_country['Health expenditure Ratio'].isnull()]
codiv_country['Health expenditure Ratio'] = codiv_country['Health expenditure Ratio'].fillna(codiv_country['Health expenditure Ratio'].mean()),1,not_existent,codiv_country['Health expenditure Ratio'] = codiv_country['Health expenditure Ratio'].fillna(codiv_country['Health expenditure Ratio'].mean())
codiv_country['Tests_per_10kp_log1p'] = codiv_country['Tests_per_10kp_log1p'].fillna(0),1,not_existent,codiv_country['Tests_per_10kp_log1p'] = codiv_country['Tests_per_10kp_log1p'].fillna(0) 
"codiv_country.Country[codiv_country.Country == ""United States""] = ""US""",1,not_existent,"codiv_country.Country[codiv_country.Country == ""United States""] = ""US"""
"ASSIGN= (codiv_country['Females 2018']- codiv_country['Females 2018'].mean()) path['Females 2018'].std() ASSIGN.plot() plt.hlines(-1,0,100,colors=""red"") plt.hlines(-2,0,100,colors=""yellow"") plt.hlines(-3,0,100,colors=""green"") plt.hlines(1,0,100,colors=""red"") plt.hlines(2,0,100,colors=""yellow"") plt.hlines(3,0,100,colors=""green"") plt.ylabel(""ASSIGN"") plt.xlabel(""country index"")",0,execute_result,"z_scores_Females= (codiv_country['Females 2018']- codiv_country['Females 2018'].mean()) /codiv_country['Females 2018'].std()
 z_scores_Females.plot()
 plt.hlines(-1,0,100,colors=""red"")
 plt.hlines(-2,0,100,colors=""yellow"")
 plt.hlines(-3,0,100,colors=""green"")
 plt.hlines(1,0,100,colors=""red"")
 plt.hlines(2,0,100,colors=""yellow"")
 plt.hlines(3,0,100,colors=""green"")
 plt.ylabel(""z_scores_Females"")
 plt.xlabel(""country index"")"
"codiv_country['Females 2018'].plot() plt.ylabel(""Females %"") plt.xlabel(""country index"")",0,execute_result,"codiv_country['Females 2018'].plot()
 plt.ylabel(""Females %"")
 plt.xlabel(""country index"")"
"CHECKPOINT ASSIGN = (np.abs(z_scores_Females) > 2) codiv_country['Females 2018'][ASSIGN]=codiv_country['Females 2018'].mean() print('z-scores_Females:', z_scores_Females.shape) codiv_country['Females 2018'].plot()",1,stream,"idx_Females = (np.abs(z_scores_Females) > 2)
 codiv_country['Females 2018'][idx_Females]=codiv_country['Females 2018'].mean()
 print('z-scores_Females:', z_scores_Females.shape)
 codiv_country['Females 2018'].plot()"
"ASSIGN= (codiv_country['Female Lung']- codiv_country['Female Lung'].mean()) path['Female Lung'].std() ASSIGN.plot() plt.hlines(-1,0,100,colors=""red"") plt.hlines(-2,0,100,colors=""yellow"") plt.hlines(-3,0,100,colors=""green"") plt.hlines(1,0,100,colors=""red"") plt.hlines(2,0,100,colors=""yellow"") plt.hlines(3,0,100,colors=""green"") plt.ylabel(""ASSIGN"") plt.xlabel(""country index"")",0,execute_result,"z_scores_Female_Lung= (codiv_country['Female Lung']- codiv_country['Female Lung'].mean()) /codiv_country['Female Lung'].std()
 z_scores_Female_Lung.plot()
 plt.hlines(-1,0,100,colors=""red"")
 plt.hlines(-2,0,100,colors=""yellow"")
 plt.hlines(-3,0,100,colors=""green"")
 plt.hlines(1,0,100,colors=""red"")
 plt.hlines(2,0,100,colors=""yellow"")
 plt.hlines(3,0,100,colors=""green"")
 plt.ylabel(""z_scores_Female_Lung"")
 plt.xlabel(""country index"")"
"CHECKPOINT ASSIGN = (np.abs(z_scores_Female_Lung) > 2) codiv_country['Female Lung'][ASSIGN]=codiv_country['Female Lung'].mean() print('z-scores_Female_Lung:', z_scores_Female_Lung.shape)",1,stream,"idx_Female_Lung = (np.abs(z_scores_Female_Lung) > 2)
 codiv_country['Female Lung'][idx_Female_Lung]=codiv_country['Female Lung'].mean()
 print('z-scores_Female_Lung:', z_scores_Female_Lung.shape)"
"ASSIGN= (codiv_country['Male Lung']- codiv_country['Male Lung'].mean()) path['Male Lung'].std() ASSIGN.plot() plt.hlines(-1,0,100,colors=""red"") plt.hlines(-2,0,100,colors=""yellow"") plt.hlines(-3,0,100,colors=""green"") plt.hlines(1,0,100,colors=""red"") plt.hlines(2,0,100,colors=""yellow"") plt.hlines(3,0,100,colors=""green"") plt.ylabel(""ASSIGN"") plt.xlabel(""country index"")",0,execute_result,"z_scores_Male_Lung= (codiv_country['Male Lung']- codiv_country['Male Lung'].mean()) /codiv_country['Male Lung'].std()
 z_scores_Male_Lung.plot()
 plt.hlines(-1,0,100,colors=""red"")
 plt.hlines(-2,0,100,colors=""yellow"")
 plt.hlines(-3,0,100,colors=""green"")
 plt.hlines(1,0,100,colors=""red"")
 plt.hlines(2,0,100,colors=""yellow"")
 plt.hlines(3,0,100,colors=""green"")
 plt.ylabel(""z_scores_Male_Lung"")
 plt.xlabel(""country index"")"
"CHECKPOINT ASSIGN = (np.abs(z_scores_Male_Lung) > 2) codiv_country['Male Lung'][ASSIGN]=codiv_country['Male Lung'].mean() print('z-scores_Male_Lung:', z_scores_Male_Lung.shape)",1,stream,"idx_Male_Lung = (np.abs(z_scores_Male_Lung) > 2)
 codiv_country['Male Lung'][idx_Male_Lung]=codiv_country['Male Lung'].mean()
 print('z-scores_Male_Lung:', z_scores_Male_Lung.shape)"
"ASSIGN= (codiv_country['lung']- codiv_country['lung'].mean()) path['lung'].std() ASSIGN.plot() plt.hlines(-1,0,100,colors=""red"",label=""np.abs(z_scores_Females) > 1)"") plt.hlines(-2,0,100,colors=""yellow"",label=""np.abs(z_scores_Females) > 2)"") plt.hlines(-3,0,100,colors=""green"",label=""np.abs(z_scores_Females) > 3)"") plt.hlines(1,0,100,colors=""red"",label=""np.abs(z_scores_Females) > 1)"") plt.hlines(2,0,100,colors=""yellow"",label=""np.abs(z_scores_Females) > 2)"") plt.hlines(3,0,100,colors=""green"",label=""np.abs(z_scores_Females) > 3)"") plt.ylabel(""ASSIGN"") plt.xlabel(""country index"")",0,execute_result,"z_scores_Lung= (codiv_country['lung']- codiv_country['lung'].mean()) /codiv_country['lung'].std()
 z_scores_Lung.plot()
 plt.hlines(-1,0,100,colors=""red"",label=""np.abs(z_scores_Females) > 1)"")
 plt.hlines(-2,0,100,colors=""yellow"",label=""np.abs(z_scores_Females) > 2)"")
 plt.hlines(-3,0,100,colors=""green"",label=""np.abs(z_scores_Females) > 3)"")
 plt.hlines(1,0,100,colors=""red"",label=""np.abs(z_scores_Females) > 1)"")
 plt.hlines(2,0,100,colors=""yellow"",label=""np.abs(z_scores_Females) > 2)"")
 plt.hlines(3,0,100,colors=""green"",label=""np.abs(z_scores_Females) > 3)"")
 plt.ylabel(""z_scores_Lung"")
 plt.xlabel(""country index"")"
"CHECKPOINT ASSIGN = (np.abs(z_scores_Lung) > 2) codiv_country['lung'][ASSIGN]=codiv_country['lung'].mean() print('z-scores_Lung:', z_scores_Lung.shape)",1,stream,"idx_Lung = (np.abs(z_scores_Lung) > 2)
 codiv_country['lung'][idx_Lung]=codiv_country['lung'].mean()
 print('z-scores_Lung:', z_scores_Lung.shape)"
"ASSIGN=pd.read_csv(os.path.join('.path', 'csse_covid_19_daily_reports.csv')) ASSIGN=ASSIGN.drop(['FIPS','Admin2','Last_Update','Combined_Key','Long_','Lat'], axis=1) ASSIGN.sample(10)",1,execute_result,"codiv_csse=pd.read_csv(os.path.join('./', 'csse_covid_19_daily_reports.csv'))
 codiv_csse=codiv_csse.drop(['FIPS','Admin2','Last_Update','Combined_Key','Long_','Lat'], axis=1)
 codiv_csse.sample(10)"
ASSIGN = ASSIGN.rename(columns={'Confirmed': 'Infected'}),1,not_existent,codiv_csse = codiv_csse.rename(columns={'Confirmed': 'Infected'})
ASSIGN=ASSIGN.groupby('Country_Region').sum().reset_index() ASSIGN.sample(10),1,execute_result,"codiv_csse=codiv_csse.groupby('Country_Region').sum().reset_index()
 codiv_csse.sample(10)"
"codiv_csse[""Deaths Ratio""]=codiv_csse[""Deaths""]*1000path[""Infected""] codiv_csse.sample(10)",1,execute_result,"codiv_csse[""Deaths Ratio""]=codiv_csse[""Deaths""]*1000/codiv_csse[""Infected""]
 codiv_csse.sample(10)"
set(codiv_country['Country'].unique()) - set(codiv_csse['Country_Region'].unique()),0,execute_result,set(codiv_country['Country'].unique()) - set(codiv_csse['Country_Region'].unique())
"codiv_csse.Country_Region[codiv_csse.Country_Region == ""Korea, South""] = ""South Korea"" codiv_csse.Country_Region[codiv_csse.Country_Region == ""Czechia""] = ""Czech Republic""",1,not_existent,"codiv_csse.Country_Region[codiv_csse.Country_Region == ""Korea, South""] = ""South Korea""
 codiv_csse.Country_Region[codiv_csse.Country_Region == ""Czechia""] = ""Czech Republic"""
ASSIGN=ASSIGN.set_index('Country_Region') ASSIGN.sample(10),1,execute_result,"codiv_csse=codiv_csse.set_index('Country_Region')
 codiv_csse.sample(10)"
codiv_csse.describe(),0,execute_result,codiv_csse.describe()
CHECKPOINT codiv_csse.dtypes,0,execute_result,codiv_csse.dtypes
codiv_csse.isna().sum(),0,execute_result,codiv_csse.isna().sum()
"codiv_country['Total Infected'] = 0.0 codiv_country['Total Deaths'] = 0.0 codiv_country['Total Recovered'] = 0.0 codiv_country['Total Active'] = 0.0 codiv_country['Deaths Ratio'] = 0.0 for countryindex in codiv_csse.index: if not codiv_country[codiv_country['Country']==countryindex].empty : codiv_country.at[codiv_country['Country']==countryindex,'Total Infected Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Infected'][0]) codiv_country.at[codiv_country['Country']==countryindex,'Total Infected']=codiv_csse[codiv_csse.index==countryindex]['Infected'][0] codiv_country.at[codiv_country['Country']==countryindex,'Total Deaths Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]) codiv_country.at[codiv_country['Country']==countryindex,'Total Deaths']=codiv_csse[codiv_csse.index==countryindex]['Deaths'][0] codiv_country.at[codiv_country['Country']==countryindex,'Total Recovered Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Recovered'][0]) codiv_country.at[codiv_country['Country']==countryindex,'Total Recovered']=codiv_csse[codiv_csse.index==countryindex]['Recovered'][0] codiv_country.at[codiv_country['Country']==countryindex,'Deaths Ratio']=codiv_csse[codiv_csse.index==countryindex]['Deaths Ratio'][0] if codiv_csse[codiv_csse.index==countryindex]['Active'][0] ==0.0: codiv_country.at[codiv_country['Country']==countryindex,'Total Active Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Infected'][0]-codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]-codiv_csse[codiv_csse.index==countryindex]['Recovered'][0]) codiv_country.at[codiv_country['Country']==countryindex,'Total Active']=codiv_csse[codiv_csse.index==countryindex]['Infected'][0]-codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]-codiv_csse[codiv_csse.index==countryindex]['Recovered'][0] else: codiv_country.at[codiv_country['Country']==countryindex,'Total Active Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Active'][0]) codiv_country.at[codiv_country['Country']==countryindex,'Total Active']=codiv_csse[codiv_csse.index==countryindex]['Active'][0]",1,not_existent,"codiv_country['Total Infected'] = 0.0
 codiv_country['Total Deaths'] = 0.0
 codiv_country['Total Recovered'] = 0.0
 codiv_country['Total Active'] = 0.0
 codiv_country['Deaths Ratio'] = 0.0
 for countryindex in codiv_csse.index:
     # If the country exists in the other table
     if not codiv_country[codiv_country['Country']==countryindex].empty :
         codiv_country.at[codiv_country['Country']==countryindex,'Total Infected Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Infected'][0])
         codiv_country.at[codiv_country['Country']==countryindex,'Total Infected']=codiv_csse[codiv_csse.index==countryindex]['Infected'][0]
         codiv_country.at[codiv_country['Country']==countryindex,'Total Deaths Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Deaths'][0])
         codiv_country.at[codiv_country['Country']==countryindex,'Total Deaths']=codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]
         codiv_country.at[codiv_country['Country']==countryindex,'Total Recovered Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Recovered'][0])
         codiv_country.at[codiv_country['Country']==countryindex,'Total Recovered']=codiv_csse[codiv_csse.index==countryindex]['Recovered'][0]
         codiv_country.at[codiv_country['Country']==countryindex,'Deaths Ratio']=codiv_csse[codiv_csse.index==countryindex]['Deaths Ratio'][0]
         if codiv_csse[codiv_csse.index==countryindex]['Active'][0] ==0.0:
             codiv_country.at[codiv_country['Country']==countryindex,'Total Active Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Infected'][0]-codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]-codiv_csse[codiv_csse.index==countryindex]['Recovered'][0])
             codiv_country.at[codiv_country['Country']==countryindex,'Total Active']=codiv_csse[codiv_csse.index==countryindex]['Infected'][0]-codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]-codiv_csse[codiv_csse.index==countryindex]['Recovered'][0]
         else:
             codiv_country.at[codiv_country['Country']==countryindex,'Total Active Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Active'][0])
             codiv_country.at[codiv_country['Country']==countryindex,'Total Active']=codiv_csse[codiv_csse.index==countryindex]['Active'][0]"
"codiv_country.isnull()[['Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active', 'Deaths Ratio']].sum()",0,execute_result,"codiv_country.isnull()[['Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active', 'Deaths Ratio']].sum()"
"CHECKPOINT ASSIGN=codiv_country[codiv_country['Quarantine'].notnull()] ASSIGN=codiv_country[codiv_country['Restrictions'].notnull()] ASSIGN=codiv_country[codiv_country['Restrictions'].isnull() & codiv_country['Quarantine'].isnull()] print( , ASSIGN.shape) print( , ASSIGN.shape) print( , ASSIGN.shape)",1,stream,"codiv_country_qurantine=codiv_country[codiv_country['Quarantine'].notnull()]
 codiv_country_Restrictions=codiv_country[codiv_country['Restrictions'].notnull()]
 codiv_country_without_Restrictions_qurantine=codiv_country[codiv_country['Restrictions'].isnull() & codiv_country['Quarantine'].isnull()]
 
 print(""codiv_country_qurantine shape"" , codiv_country_qurantine.shape)
 print(""codiv_country_Restrictionse shape"" , codiv_country_Restrictions.shape)
 print(""codiv_country_without_Restrictions_quarantine"" , codiv_country_without_Restrictions_qurantine.shape)"
"ASSIGN=pd.read_csv(os.path.join('.path', 'time_series_covid19_confirmed_global.csv')) ASSIGN=ASSIGN.drop(['Lat','Long'], axis=1) ASSIGN=pd.read_csv(os.path.join('.path', 'time_series_covid19_deaths_global.csv')) ASSIGN=ASSIGN.drop(['Lat','Long'], axis=1) ASSIGN=pd.read_csv(os.path.join('.path', 'time_series_covid19_recovered_global.csv')) ASSIGN=ASSIGN.drop(['Lat','Long'], axis=1) ASSIGN.sample(10)",1,execute_result,"codiv_time_confirmed=pd.read_csv(os.path.join('./', 'time_series_covid19_confirmed_global.csv'))
 codiv_time_confirmed=codiv_time_confirmed.drop(['Lat','Long'], axis=1)
 
 codiv_time_deaths=pd.read_csv(os.path.join('./', 'time_series_covid19_deaths_global.csv'))
 codiv_time_deaths=codiv_time_deaths.drop(['Lat','Long'], axis=1)
 
 codiv_time_recovered=pd.read_csv(os.path.join('./', 'time_series_covid19_recovered_global.csv'))
 codiv_time_recovered=codiv_time_recovered.drop(['Lat','Long'], axis=1)
 
 codiv_time_confirmed.sample(10)"
"CHECKPOINT ASSIGN=pd.read_csv('.path') ASSIGN=ASSIGN.drop(['AverageTemperatureUncertainty'], axis=1) ASSIGN=ASSIGN[((ASSIGN['dt'] > '2013-01-01') & (ASSIGN['dt'] < '2013-04-01')) | ((ASSIGN['dt'] > '2012-01-01') & (ASSIGN['dt'] < '2012-04-01')) | ((ASSIGN['dt'] > '2011-01-01') & (ASSIGN['dt'] < '2011-04-01'))] ASSIGN = ASSIGN.groupby(['Country'])['AverageTemperature'].mean() print(ASSIGN.sample(10)) print(,ASSIGN.shape)",1,stream,"codiv_country_temp=pd.read_csv('.//GlobalLandTemperaturesByCountry.csv')
 #codiv_country_temp.loc['2020-01-01':'2020-04-01']
 codiv_country_temp=codiv_country_temp.drop(['AverageTemperatureUncertainty'], axis=1)
 codiv_country_temp=codiv_country_temp[((codiv_country_temp['dt'] > '2013-01-01') & (codiv_country_temp['dt'] < '2013-04-01')) | 
                                       ((codiv_country_temp['dt'] > '2012-01-01') & (codiv_country_temp['dt'] < '2012-04-01')) | 
                                       ((codiv_country_temp['dt'] > '2011-01-01') & (codiv_country_temp['dt'] < '2011-04-01'))]
 codiv_country_temp = codiv_country_temp.groupby(['Country'])['AverageTemperature'].mean()
 print(codiv_country_temp.sample(10))
 print(""codiv_country_temp"",codiv_country_temp.shape)"
CHECKPOINT print(codiv_country_temp.describe()),0,stream,print(codiv_country_temp.describe())
"CHECKPOINT ASSIGN=pd.read_csv('.path') ASSIGN=ASSIGN.drop(['AverageTemperatureUncertainty','Latitude','Longitude'], axis=1) ASSIGN=ASSIGN[((ASSIGN['dt'] > '2013-01-01') & (ASSIGN['dt'] < '2013-04-01')) | ((ASSIGN['dt'] > '2012-01-01') & (ASSIGN['dt'] < '2012-04-01')) | ((ASSIGN['dt'] > '2011-01-01') & (ASSIGN['dt'] < '2011-04-01'))] ASSIGN= ASSIGN.groupby(['Country'])['AverageTemperature'].mean() print(ASSIGN.sample(10)) print(,ASSIGN.shape) print()",1,stream,"codiv_country_tempMajorCity=pd.read_csv('.//GlobalLandTemperaturesByMajorCity.csv')
 
 codiv_country_tempMajorCity=codiv_country_tempMajorCity.drop(['AverageTemperatureUncertainty','Latitude','Longitude'], axis=1)
 codiv_country_tempMajorCity=codiv_country_tempMajorCity[((codiv_country_tempMajorCity['dt'] > '2013-01-01') & (codiv_country_tempMajorCity['dt'] < '2013-04-01')) | 
                                       ((codiv_country_tempMajorCity['dt'] > '2012-01-01') & (codiv_country_tempMajorCity['dt'] < '2012-04-01')) | 
                                       ((codiv_country_tempMajorCity['dt'] > '2011-01-01') & (codiv_country_tempMajorCity['dt'] < '2011-04-01'))]
 codiv_country_tempMajorCity= codiv_country_tempMajorCity.groupby(['Country'])['AverageTemperature'].mean()
 print(codiv_country_tempMajorCity.sample(10))
 print(""codiv_country_tempMajorCity"",codiv_country_tempMajorCity.shape)
 print(""243-49=194 countries dont have temperature for big city"")"
"ASSIGN=(codiv_country_tempMajorCity-codiv_country_temp).dropna() ASSIGN=ASSIGN.sort_values( ascending=False) ASSIGN=Temperature_difference.index ASSIGN=Temperature_difference ASSIGN = plt.subplots(figsize=(20,10)) ax.scatter(ASSIGN, ASSIGN, alpha=0.5) plt.xticks(rotation=45) plt.ylabel(""Delta Temp AvrMaincity-AvrCountry"") plt.grid(True) plt.show()",0,display_data,"Temperature_difference=(codiv_country_tempMajorCity-codiv_country_temp).dropna()
 Temperature_difference=Temperature_difference.sort_values( ascending=False)
 x=Temperature_difference.index
 y=Temperature_difference
 fig, ax = plt.subplots(figsize=(20,10))
 ax.scatter(x, y, alpha=0.5)
 plt.xticks(rotation=45)
 plt.ylabel(""Delta Temp AvrMaincity-AvrCountry"")
 plt.grid(True)
 plt.show()"
"codiv_country['Temp_mean_jan_apr'] = 0.0 for countryindex in codiv_country_tempMajorCity.index: if not codiv_country[codiv_country['Country']==countryindex].empty : codiv_country.at[codiv_country['Country']==countryindex,'Temp_mean_jan_apr']=codiv_country_tempMajorCity.loc[countryindex] codiv_country['Temp_mean_jan_apr'].sample(10)",1,execute_result,"codiv_country['Temp_mean_jan_apr'] = 0.0
 for countryindex in codiv_country_tempMajorCity.index:
     if not codiv_country[codiv_country['Country']==countryindex].empty :
         codiv_country.at[codiv_country['Country']==countryindex,'Temp_mean_jan_apr']=codiv_country_tempMajorCity.loc[countryindex]
 codiv_country['Temp_mean_jan_apr'].sample(10)"
"for countryindex in codiv_country_temp.index: if not codiv_country[codiv_country['Country']==countryindex].empty: if codiv_country[codiv_country['Country']==countryindex]['Temp_mean_jan_apr'].values[0] == 0: codiv_country.at[codiv_country['Country']==countryindex,'Temp_mean_jan_apr']=codiv_country_temp.loc[countryindex] codiv_country.head()",1,execute_result,"for countryindex in codiv_country_temp.index:
     if not codiv_country[codiv_country['Country']==countryindex].empty:
         if codiv_country[codiv_country['Country']==countryindex]['Temp_mean_jan_apr'].values[0] == 0:
             codiv_country.at[codiv_country['Country']==countryindex,'Temp_mean_jan_apr']=codiv_country_temp.loc[countryindex]
 codiv_country.head()"
codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)],0,execute_result,codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)]
"ASSIGN=codiv_country.sort_values(by='Total Infected', ascending=False) ASSIGN = ASSIGN.reset_index(drop=True) ASSIGN.head(11).style.background_gradient(cmap='Blues')",1,execute_result,"transit=codiv_country.sort_values(by='Total Infected', ascending=False)
 transit = transit.reset_index(drop=True)
 transit.head(11).style.background_gradient(cmap='Blues')"
"ASSIGN=codiv_country.sort_values(by='Total Active', ascending=False) ASSIGN = ASSIGN.reset_index(drop=True) ASSIGN.head(11).style.background_gradient(cmap='Greens')",1,execute_result,"transit=codiv_country.sort_values(by='Total Active', ascending=False)
 transit = transit.reset_index(drop=True)
 transit.head(11).style.background_gradient(cmap='Greens')"
"ASSIGN=codiv_country.sort_values(by='Total Deaths', ascending=False) ASSIGN = ASSIGN.reset_index(drop=True) ASSIGN.head(11).style.background_gradient(cmap='Reds')",1,execute_result,"transit=codiv_country.sort_values(by='Total Deaths', ascending=False)
 transit = transit.reset_index(drop=True)
 transit.head(11).style.background_gradient(cmap='Reds')"
"ASSIGN=codiv_country.sort_values(by='Deaths Ratio', ascending=False) ASSIGN = ASSIGN.reset_index(drop=True) ASSIGN.head(11).style.background_gradient(cmap='Oranges')",1,execute_result,"transit=codiv_country.sort_values(by='Deaths Ratio', ascending=False)
 transit = transit.reset_index(drop=True)
 transit.head(11).style.background_gradient(cmap='Oranges')"
"CHECKPOINT ASSIGN=codiv_country.sort_values(by='Total Infected', ascending=False).head(n=60).reset_index(drop=True) codiv_country_short",1,execute_result,"codiv_country_short=codiv_country.sort_values(by='Total Infected', ascending=False).head(n=60).reset_index(drop=True)
 codiv_country_short"
codiv_country_short.describe(),0,execute_result,codiv_country_short.describe()
"sns.FacetGrid(codiv_country_short, hue=""Country"", size=6.5) \ .map(plt.scatter, ""Tests_per_10kp_log1p"", ""Total Infected Log10"") \ .add_legend() plt.grid(True)",0,display_data,"sns.FacetGrid(codiv_country_short, hue=""Country"", size=6.5) \
    .map(plt.scatter, ""Tests_per_10kp_log1p"", ""Total Infected Log10"") \
    .add_legend()
 
 plt.grid(True)"
"codiv_country_short[codiv_country_short[""Tests_per_10kp_log1p""] == 0][""Total Infected Log10""].hist(figsize=(6, 6), alpha=0.5, density=True) plt.xlabel('Total Infected Log1p') plt.ylabel('Nr Country density') plt.title('Histogram Tests_per_10kp_log1p==0')",0,execute_result,"#histogram of ""total infected"" for countries that have Tests_per_10kp_log1p == 0
 codiv_country_short[codiv_country_short[""Tests_per_10kp_log1p""] == 0][""Total Infected Log10""].hist(figsize=(6, 6), alpha=0.5, density=True)
 plt.xlabel('Total Infected Log1p')
 plt.ylabel('Nr Country density')
 plt.title('Histogram Tests_per_10kp_log1p==0')"
"codiv_country_short[codiv_country_short[""Tests_per_10kp_log1p""] != 0][""Total Infected Log10""].hist(figsize=(6,6), alpha=0.5, density=True) plt.xlabel('Total Infected Log1p') plt.ylabel('Nr Country density') plt.title('Histogram Tests_per_10kp_log1p not 0')",0,execute_result,"#histogram of ""total infected"" for countries that have Tests_per_10kp_log1p != 0
 codiv_country_short[codiv_country_short[""Tests_per_10kp_log1p""] != 0][""Total Infected Log10""].hist(figsize=(6,6), alpha=0.5, density=True)
 plt.xlabel('Total Infected Log1p')
 plt.ylabel('Nr Country density')
 plt.title('Histogram Tests_per_10kp_log1p not 0')"
"sns.FacetGrid(codiv_country_short, hue=""Country"", size=6.5) \ .map(plt.scatter, ""Tests_per_10kp_log1p"", ""Total Active Log10"") \ .add_legend() plt.grid(True)",0,display_data,"sns.FacetGrid(codiv_country_short, hue=""Country"", size=6.5) \
    .map(plt.scatter, ""Tests_per_10kp_log1p"", ""Total Active Log10"") \
    .add_legend()
 plt.grid(True)"
"codiv_country_short[codiv_country_short[""Tests_per_10kp_log1p""] == 0][""Total Active Log10""].hist(figsize=(6, 6), alpha=0.5, density=True) plt.xlabel('Total Active Log10') plt.ylabel('Nr Country density') plt.title('Histogram Tests_per_10kp_log1p==0')",0,execute_result,"codiv_country_short[codiv_country_short[""Tests_per_10kp_log1p""] == 0][""Total Active Log10""].hist(figsize=(6, 6), alpha=0.5, density=True)
 plt.xlabel('Total Active Log10')
 plt.ylabel('Nr Country density')
 plt.title('Histogram Tests_per_10kp_log1p==0')"
"codiv_country_short[codiv_country_short[""Tests_per_10kp_log1p""] != 0][""Total Active Log10""].hist(figsize=(6,6), alpha=0.5, density=True) plt.xlabel('Total Active Log10') plt.ylabel('Nr Country density') plt.title('Histogram Tests_per_10kp_log1p not 0')",0,execute_result,"codiv_country_short[codiv_country_short[""Tests_per_10kp_log1p""] != 0][""Total Active Log10""].hist(figsize=(6,6), alpha=0.5, density=True)
 plt.xlabel('Total Active Log10')
 plt.ylabel('Nr Country density')
 plt.title('Histogram Tests_per_10kp_log1p not 0')"
"codiv_country_short.drop([   'Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active'], axis=1)",1,execute_result,"codiv_country_short.drop([   'Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active'], axis=1)"
"SETUP CHECKPOINT ASSIGN=codiv_country_short.drop([ 'Deaths Ratio','Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active','Total Recovered Log10','Total Infected Log10','Total Deaths Log10','Total Active Log10'], axis=1) for col in ASSIGN.loc[:, ASSIGN.dtypes == np.number].keys(): sns.jointplot(codiv_country_short['Total Infected Log10'], col, data=codiv_country_short, height=6, s=1, color=""blue"") ASSIGN = pearsonr(codiv_country_short['Total Infected Log10'], codiv_country_short[col]) print( %(col, corr)) sns.jointplot(codiv_country_short['Total Deaths Log10'], col, data=codiv_country_short, height=6, s=1, color=""red"") ASSIGN = pearsonr(codiv_country_short['Total Deaths Log10'], codiv_country_short[col]) print( %(col, corr)) sns.jointplot(codiv_country_short['Total Active Log10'], col, data=codiv_country_short, height=6, s=1, color=""green"") ASSIGN = pearsonr(codiv_country_short['Total Active Log10'], codiv_country_short[col]) print( %(col, corr)) sns.jointplot(codiv_country_short['Deaths Ratio'], col, data=codiv_country_short, height=6, s=1, color=""orange"") ASSIGN = pearsonr(codiv_country_short['Deaths Ratio'], codiv_country_short[col]) print( %(col, corr)) plt.show()",0,stream,"import seaborn as sns
 import numpy as np
 from scipy.stats import pearsonr
 #I use a temp table to hide some columns not needed for the sns.
 codiv_country_short_tmp=codiv_country_short.drop([ 'Deaths Ratio','Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active','Total Recovered Log10','Total Infected Log10','Total Deaths Log10','Total Active Log10'], axis=1)
 for col in codiv_country_short_tmp.loc[:, codiv_country_short_tmp.dtypes == np.number].keys():
     sns.jointplot(codiv_country_short['Total Infected Log10'], col, data=codiv_country_short, height=6, s=1, color=""blue"")
     corr, _ = pearsonr(codiv_country_short['Total Infected Log10'], codiv_country_short[col])
     print(""Pearsons correlation Total Infected Log10 <-> %s : %.3f"" %(col, corr))
     sns.jointplot(codiv_country_short['Total Deaths Log10'], col, data=codiv_country_short, height=6, s=1, color=""red"")
     corr, _ = pearsonr(codiv_country_short['Total Deaths Log10'], codiv_country_short[col])
     print(""Pearsons correlation Total Deaths Log10 <-> %s : %.3f"" %(col, corr))
     sns.jointplot(codiv_country_short['Total Active Log10'], col, data=codiv_country_short, height=6, s=1, color=""green"")
     corr, _ = pearsonr(codiv_country_short['Total Active Log10'], codiv_country_short[col])
     print(""Pearsons correlation Total Active Log10 <-> %s : %.3f"" %(col, corr))
     sns.jointplot(codiv_country_short['Deaths Ratio'], col, data=codiv_country_short, height=6, s=1, color=""orange"")
     corr, _ = pearsonr(codiv_country_short['Deaths Ratio'], codiv_country_short[col])
     print(""Pearsons correlation Deaths Ratio <-> %s : %.3f"" %(col, corr))
 plt.show()"
"ASSIGN = np.polyfit(codiv_country_short[""Temp_mean_jan_apr""], codiv_country_short[""Total Infected Log10""], deg=8) sns.FacetGrid(codiv_country_short, hue=""Country"", size=6.5) \ .map(plt.scatter, ""Temp_mean_jan_apr"", ""Total Infected Log10"") \ .add_legend() for indexq in range(-80,300,1): plt.plot(indexqpath,np.polyval(ASSIGN, indexqpath), '.', color='black') plt.vlines(7,7,14,colors=""red"",linestyles='dashed') plt.vlines(24,7,14,colors=""red"",linestyles='dashed') plt.grid(True)",0,display_data,"coefs_poly3 = np.polyfit(codiv_country_short[""Temp_mean_jan_apr""], codiv_country_short[""Total Infected Log10""], deg=8) 
 sns.FacetGrid(codiv_country_short, hue=""Country"", size=6.5) \
    .map(plt.scatter, ""Temp_mean_jan_apr"", ""Total Infected Log10"") \
    .add_legend()
 for indexq in range(-80,300,1):
     plt.plot(indexq/10,np.polyval(coefs_poly3, indexq/10), '.', color='black')
 plt.vlines(7,7,14,colors=""red"",linestyles='dashed')
 plt.vlines(24,7,14,colors=""red"",linestyles='dashed')
 plt.grid(True)"
"ASSIGN = np.polyfit(codiv_country_short[""Health expenditure Ratio""], codiv_country_short[""Total Infected Log10""], 1) sns.FacetGrid(codiv_country_short, hue=""Country"", size=6.5) \ .map(plt.scatter, ""Health expenditure Ratio"", ""Total Infected Log10"") \ .add_legend() plt.plot(codiv_country_short[""Health expenditure Ratio""], m*codiv_country_short[""Health expenditure Ratio""]+b, '--k') plt.grid(True)",0,display_data,"m,b = np.polyfit(codiv_country_short[""Health expenditure Ratio""], codiv_country_short[""Total Infected Log10""], 1) 
 
 sns.FacetGrid(codiv_country_short, hue=""Country"", size=6.5) \
    .map(plt.scatter, ""Health expenditure Ratio"", ""Total Infected Log10"") \
    .add_legend()
 plt.plot(codiv_country_short[""Health expenditure Ratio""], m*codiv_country_short[""Health expenditure Ratio""]+b, '--k') 
 plt.grid(True)"
"ASSIGN = codiv_country.copy() ASSIGN['Quarantine_cat'] = ASSIGN['Quarantine'].notnull().astype(int) ASSIGN['Restrictions_cat'] = ASSIGN['Restrictions'].notnull().astype(int) ASSIGN['Schools_cat'] = ASSIGN['Schools'].notnull().astype(int) ASSIGN=ASSIGN.drop([ 'Quarantine','Schools','Restrictions', 'Country', 'Total Deaths','Total Infected','Total Active','Total Recovered', ""Total Deaths Log10"",""Total Recovered Log10"",""Total Active Log10"",""Deaths Ratio"" ], axis=1) ASSIGN.sample(10)",1,execute_result,"# Working on a copy
 codiv_country_analyze = codiv_country.copy()
 
 # Creating categorical variables
 codiv_country_analyze['Quarantine_cat'] = codiv_country_analyze['Quarantine'].notnull().astype(int)
 codiv_country_analyze['Restrictions_cat'] = codiv_country_analyze['Restrictions'].notnull().astype(int)
 codiv_country_analyze['Schools_cat'] = codiv_country_analyze['Schools'].notnull().astype(int)
 
 # 
 codiv_country_analyze=codiv_country_analyze.drop([
     'Quarantine','Schools','Restrictions', # now categorical
     'Country', # not helpful
     
     # Only keep ""Total Infected Log10""
     'Total Deaths','Total Infected','Total Active','Total Recovered', 
     ""Total Deaths Log10"",""Total Recovered Log10"",""Total Active Log10"",""Deaths Ratio""
 ], axis=1)
 
 codiv_country_analyze.sample(10)"
codiv_country_analyze[codiv_country_analyze.isnull().any(axis=1)],1,execute_result,codiv_country_analyze[codiv_country_analyze.isnull().any(axis=1)]
ASSIGN = ASSIGN[ASSIGN.notnull().all(axis=1)],1,not_existent,codiv_country_analyze = codiv_country_analyze[codiv_country_analyze.notnull().all(axis=1)]
"ASSIGN = ['method','Cross-validation'] ASSIGN=range(8) ASSIGN = pd.DataFrame(index=index, columns=columns)",1,not_existent,"columns = ['method','Cross-validation']
 index=range(8)
 df_resultat = pd.DataFrame(index=index, columns=columns)"
"CHECKPOINT codiv_country_analyze['category_balanced'], bin_edges_balanced=pd.qcut(codiv_country_analyze['Total Infected Log10'], q=6,labels=False, retbins=True) print(bin_edges_balanced)",1,stream,"codiv_country_analyze['category_balanced'], bin_edges_balanced=pd.qcut(codiv_country_analyze['Total Infected Log10'], q=6,labels=False, retbins=True)
 print(bin_edges_balanced)"
"pd.qcut(codiv_country_analyze['Total Infected Log10'], q=6)",1,execute_result,"#explanation of these values
 pd.qcut(codiv_country_analyze['Total Infected Log10'], q=6)"
codiv_country_analyze['category_balanced'] = codiv_country_analyze['category_balanced'] +1,1,not_existent,"#balanced bins deliver category for 0-5, euqidistant from 1-6, so i will add 1 to the category numbers
 codiv_country_analyze['category_balanced'] = codiv_country_analyze['category_balanced'] +1"
"CHECKPOINT ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][0] ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][1] ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][2] ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][3] ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][4] ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][5] ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][6] Level1_balanced='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,) Level2_balanced='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,) Level3_balanced='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,) Level4_balanced='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,) Level5_balanced='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,) Level6_balanced='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,) print(,Level1_balanced) print(,Level2_balanced) print(,Level3_balanced) print(,Level4_balanced) print(,Level5_balanced) print(,Level6_balanced)",1,stream,"ylevel0_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][0]
 ylevel1_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][1]
 ylevel2_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][2]
 ylevel3_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][3]
 ylevel4_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][4]
 ylevel5_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][5]
 ylevel6_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][6]
 Level1_balanced='%.3f<Infected<%.3f'%(ylevel0_balanced,ylevel1_balanced,)
 Level2_balanced='%.3f<Infected<%.3f'%(ylevel1_balanced,ylevel2_balanced,)
 Level3_balanced='%.3f<Infected<%.3f'%(ylevel2_balanced,ylevel3_balanced,)
 Level4_balanced='%.3f<Infected<%.3f'%(ylevel3_balanced,ylevel4_balanced,)
 Level5_balanced='%.3f<Infected<%.3f'%(ylevel4_balanced,ylevel5_balanced,)
 Level6_balanced='%.3f<Infected<%.3f'%(ylevel5_balanced,ylevel6_balanced,)
 print(""Level1_balanced= "",Level1_balanced)
 print(""Level2_balanced= "",Level2_balanced)
 print(""Level3_balanced= "",Level3_balanced)
 print(""Level4_balanced= "",Level4_balanced)
 print(""Level5_balanced= "",Level5_balanced)
 print(""Level6_balanced= "",Level6_balanced)"
list(codiv_country_analyze['category_balanced'].value_counts()),0,execute_result,list(codiv_country_analyze['category_balanced'].value_counts())
"CHECKPOINT ASSIGN=codiv_country_analyze['Total Infected Log10'].values.max() ASSIGN=ASSIGN+ASSIGN*0.001 print(,ASSIGN) ASSIGN=(codiv_country_analyze['Total Infected Log10'].values.min()) ASSIGN=ASSIGN-ASSIGN*0.001 print(,ASSIGN) ASSIGN=(ymax-ymin)path ASSIGN = ymin ASSIGN = ymin+yinterval*1 ASSIGN = ymin+yinterval*2 ASSIGN = ymin+yinterval*3 ASSIGN = ymin+yinterval*4 ASSIGN = ymin+yinterval*5 ASSIGN = ymin+yinterval*6 Level1_equidistant='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,) Level2_equidistant='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,) Level3_equidistant='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,) Level4_equidistant='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,) Level5_equidistant='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,) Level6_equidistant='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,) codiv_country_analyze['category_equidistant']=np.digitize(codiv_country_analyze['Total Infected Log10'].values, ASSIGN=[ylevel0_equidistant,ylevel1_equidistant,ylevel2_equidistant,ylevel3_equidistant,ylevel4_equidistant,ylevel5_equidistant,ylevel6_equidistant]) print(,Level1_equidistant) print(,Level2_equidistant) print(,Level3_equidistant) print(,Level4_equidistant) print(,Level5_equidistant) print(,Level6_equidistant)",1,stream,"ymax=codiv_country_analyze['Total Infected Log10'].values.max()
 ymax=ymax+ymax*0.001
 print(""ymax = "",ymax)
 ymin=(codiv_country_analyze['Total Infected Log10'].values.min())
 ymin=ymin-ymin*0.001
 print(""ymin = "",ymin)
 yinterval=(ymax-ymin)/6
 # 
 ylevel0_equidistant = ymin
 ylevel1_equidistant = ymin+yinterval*1
 ylevel2_equidistant = ymin+yinterval*2
 ylevel3_equidistant = ymin+yinterval*3
 ylevel4_equidistant = ymin+yinterval*4
 ylevel5_equidistant = ymin+yinterval*5
 ylevel6_equidistant = ymin+yinterval*6
 
 #
 Level1_equidistant='%.3f<Infected<%.3f'%(ylevel0_equidistant,ylevel1_equidistant,)
 Level2_equidistant='%.3f<Infected<%.3f'%(ylevel1_equidistant,ylevel2_equidistant,)
 Level3_equidistant='%.3f<Infected<%.3f'%(ylevel2_equidistant,ylevel3_equidistant,)
 Level4_equidistant='%.3f<Infected<%.3f'%(ylevel3_equidistant,ylevel4_equidistant,)
 Level5_equidistant='%.3f<Infected<%.3f'%(ylevel4_equidistant,ylevel5_equidistant,)
 Level6_equidistant='%.3f<Infected<%.3f'%(ylevel5_equidistant,ylevel6_equidistant,)
 codiv_country_analyze['category_equidistant']=np.digitize(codiv_country_analyze['Total Infected Log10'].values,
                           bins=[ylevel0_equidistant,ylevel1_equidistant,ylevel2_equidistant,ylevel3_equidistant,ylevel4_equidistant,ylevel5_equidistant,ylevel6_equidistant])
 print(""Level1_equidistant= "",Level1_equidistant)
 print(""Level2_equidistant= "",Level2_equidistant)
 print(""Level3_equidistant= "",Level3_equidistant)
 print(""Level4_equidistant= "",Level4_equidistant)
 print(""Level5_equidistant= "",Level5_equidistant)
 print(""Level6_equidistant= "",Level6_equidistant)"
"ASSIGN = codiv_country_analyze['category_equidistant'] ASSIGN = codiv_country_analyze['category_balanced'] ASSIGN = codiv_country_analyze.drop(['Total Infected Log10','category_balanced','category_equidistant'], axis=1) sns.countplot(ASSIGN)",1,execute_result,"y_equidistant = codiv_country_analyze['category_equidistant']
 y_balanced = codiv_country_analyze['category_balanced']
 X = codiv_country_analyze.drop(['Total Infected Log10','category_balanced','category_equidistant'], axis=1) 
 sns.countplot(y_balanced)"
sns.countplot(y_equidistant),0,execute_result,sns.countplot(y_equidistant)
X.sample(10),0,execute_result,X.sample(10)
"SETUP CHECKPOINT X_tr_equid, X_te_equid, y_tr_equid, y_te_equid = train_test_split(X, y_equidistant, test_size=0.15, random_state=1) print('Train set equidistant:', X_tr_equid.shape, y_tr_equid.shape) print('Testn set equidistant:', X_te_equid.shape, y_te_equid.shape) X_tr_balan, X_te_balan, y_tr_balan, y_te_balan = train_test_split(X, y_balanced, test_size=0.15, random_state=1) print('Train set balanced:', X_tr_balan.shape, y_tr_balan.shape) print('Testn set balanced:', X_te_balan.shape, y_te_balan.shape)",1,stream,"from sklearn.model_selection import train_test_split
 X_tr_equid, X_te_equid, y_tr_equid, y_te_equid = train_test_split(X, y_equidistant, test_size=0.15, random_state=1)
 print('Train set equidistant:', X_tr_equid.shape, y_tr_equid.shape)
 print('Testn set equidistant:', X_te_equid.shape, y_te_equid.shape)
 X_tr_balan, X_te_balan, y_tr_balan, y_te_balan = train_test_split(X, y_balanced, test_size=0.15, random_state=1)
 print('Train set balanced:', X_tr_balan.shape, y_tr_balan.shape)
 print('Testn set balanced:', X_te_balan.shape, y_te_balan.shape)"
sns.countplot(y_te_equid),0,execute_result,sns.countplot(y_te_equid)
sns.countplot(y_te_balan),0,execute_result,sns.countplot(y_te_balan)
"SETUP ASSIGN = KFold(n_splits=5, shuffle=False, random_state=None)",1,not_existent,"from sklearn.model_selection import KFold
 cv_strategy = KFold(n_splits=5, shuffle=False, random_state=None)"
"SETUP CHECKPOINT ASSIGN = DummyClassifier(strategy=""most_frequent"") ASSIGN.fit(X, y_equidistant) ASSIGN.predict(X) print(, ASSIGN.score(X, y_equidistant))",0,stream,"from sklearn.dummy import DummyClassifier
 from sklearn.model_selection import cross_validate
 from sklearn.model_selection import cross_val_score
 dummy_clf = DummyClassifier(strategy=""most_frequent"")
 dummy_clf.fit(X, y_equidistant)
 dummy_clf.predict(X)
 print(""scores baseline mostfrequent classifier equidistant = "", dummy_clf.score(X, y_equidistant))"
"CHECKPOINT df_resultat.at[df_resultat.index==0,""method""]=""baseline_equidistant"" ASSIGN = cross_validate(dummy_clf, X, y_equidistant, cv=cv_strategy) df_resultat.at[df_resultat.index==0,""Cross-validation""]=np.mean(ASSIGN['test_score']) print(, np.mean(ASSIGN['test_score']) )",1,stream,"df_resultat.at[df_resultat.index==0,""method""]=""baseline_equidistant""
 Baseline_training_scores = cross_validate(dummy_clf, X, y_equidistant, cv=cv_strategy)
 df_resultat.at[df_resultat.index==0,""Cross-validation""]=np.mean(Baseline_training_scores['test_score'])
 print(""crossvalidation baseline mostfrequent classifier equidistant = "", np.mean(Baseline_training_scores['test_score']) )"
"SETUP CHECKPOINT ASSIGN= DummyClassifier(strategy=""most_frequent"") ASSIGN.fit(X_tr_equid, y_tr_equid) ASSIGN=dummy_clf.predict(X_te_equid) ASSIGN = confusion_matrix(y_true=y_te_equid, y_pred=y_pred_equid) print(ASSIGN)",0,stream,"from sklearn.metrics import confusion_matrix
 dummy_clf= DummyClassifier(strategy=""most_frequent"")
 dummy_clf.fit(X_tr_equid, y_tr_equid)
 y_pred_equid=dummy_clf.predict(X_te_equid)
 matrix = confusion_matrix(y_true=y_te_equid, y_pred=y_pred_equid)
 print(matrix)"
"SETUP plot_confusion_matrix(dummy_clf, X_te_equid, y_te_equid, cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])",0,execute_result,"from sklearn.metrics import plot_confusion_matrix
 plot_confusion_matrix(dummy_clf, X_te_equid, y_te_equid, cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
"CHECKPOINT df_resultat.at[df_resultat.index==1,""method""]=""baseline balanced bins"" ASSIGN = cross_validate(dummy_clf, X, y_balanced, cv=cv_strategy) df_resultat.at[df_resultat.index==1,""Cross-validation""]=np.mean(ASSIGN['test_score']) print(, np.mean(ASSIGN['test_score']) )",1,stream,"df_resultat.at[df_resultat.index==1,""method""]=""baseline balanced bins""
 Baseline_training_scores = cross_validate(dummy_clf, X, y_balanced, cv=cv_strategy)
 df_resultat.at[df_resultat.index==1,""Cross-validation""]=np.mean(Baseline_training_scores['test_score'])
 print(""crossvalidation baseline mostfrequent classifier balanced = "", np.mean(Baseline_training_scores['test_score']) )"
"ASSIGN = DummyClassifier(strategy=""most_frequent"") ASSIGN.fit(X_tr_balan, y_tr_balan) ASSIGN=dummy_clf.predict(X_te_balan)",0,not_existent,"dummy_clf = DummyClassifier(strategy=""most_frequent"")
 dummy_clf.fit(X_tr_balan, y_tr_balan)
 y_pred_balan=dummy_clf.predict(X_te_balan)"
"CHECKPOINT ASSIGN = confusion_matrix(y_true=y_te_balan, y_pred=y_pred_balan) print(ASSIGN)",0,stream,"matrix = confusion_matrix(y_true=y_te_balan, y_pred=y_pred_balan)
 print(matrix)"
"SETUP plot_confusion_matrix(dummy_clf, X_te_balan, y_te_balan, cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])",0,execute_result,"from sklearn.metrics import plot_confusion_matrix
 plot_confusion_matrix(dummy_clf, X_te_balan, y_te_balan, cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
SETUP,0,not_existent,from sklearn.tree import DecisionTreeClassifier
"CHECKPOINT ASSIGN=range(1,15) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for depth in ASSIGN: DecisionTree = DecisionTreeClassifier(criterion='gini', random_state=0, max_depth=depth) DecisionTree.fit(X, y_equidistant) ASSIGN = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(DecisionTree.score(X, y_equidistant)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_equidistant)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"depths=range(1,15)
 scoring='accuracy'
 cv_val_scores_list = []
 cv_val_scores_std = []
 cv_val_scores_mean = []
 accuracy_scores = []
 for depth in depths:
     DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)
     DecisionTree.fit(X, y_equidistant)
     cv_val_scores = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)
     cv_val_scores_list.append(cv_val_scores)
     cv_val_scores_mean.append(cv_val_scores.mean())
     cv_val_scores_std.append(cv_val_scores.std())
     accuracy_scores.append(DecisionTree.score(X, y_equidistant))
     Text=""depth %d, cv_val_scores_mean %f  score %f""%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_equidistant))
     print(Text)
 cv_val_scores_mean = np.array(cv_val_scores_mean)
 cv_val_scores_std = np.array(cv_val_scores_std)
 accuracy_scores = np.array(accuracy_scores)"
"CHECKPOINT print(,np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1) OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1",1,stream,"print(""The maximum is by depth = "",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)
 OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1"
"ASSIGN = plt.subplots(1,1, figsize=(15,5)) ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9) ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2) ASSIGN = plt.ASSIGN() ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9) ASSIGN=""decision tree accurency"" ax.set_title(ASSIGN, fontsize=16) ax.set_xlabel('Tree depth', fontsize=14) ax.set_ylabel('accuracy', fontsize=14) ax.set_xticks(depths) ax.legend()",0,execute_result,"fig, ax = plt.subplots(1,1, figsize=(15,5))
 ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)
 ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)
 ylim = plt.ylim()
 ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)
 title=""decision tree accurency""
 ax.set_title(title, fontsize=16)
 ax.set_xlabel('Tree depth', fontsize=14)
 ax.set_ylabel('accuracy', fontsize=14)
 ax.set_xticks(depths)
 ax.legend()"
"CHECKPOINT DecisionTree = DecisionTreeClassifier( ASSIGN='gini', random_state=0, max_depth=OptDepth) DecisionTree.fit(X, y_equidistant) ASSIGN = cross_validate(DecisionTree, X, y_equidistant, cv=cv_strategy) df_resultat.at[df_resultat.index==2,""method""]=""decision-tree equidistant"" df_resultat.at[df_resultat.index==2,""Cross-validation""]=np.mean(ASSIGN['test_score']) print(,DecisionTree.score(X, y_equidistant)) print(,np.mean(ASSIGN['test_score']))",0,stream,"DecisionTree = DecisionTreeClassifier(
     criterion='gini',  random_state=0, max_depth=OptDepth)
 # Fit decision tree
 DecisionTree.fit(X, y_equidistant)
 Tree_scores = cross_validate(DecisionTree, X, y_equidistant, cv=cv_strategy)
 df_resultat.at[df_resultat.index==2,""method""]=""decision-tree equidistant""
 df_resultat.at[df_resultat.index==2,""Cross-validation""]=np.mean(Tree_scores['test_score'])
                                                               
 print(""score DecistionTree equidistant ="",DecisionTree.score(X, y_equidistant))
 print(""cross validation DecisionTree equidistant ="",np.mean(Tree_scores['test_score']))"
"SETUP DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth) DecisionTree_confusion.fit(X_tr_equid, y_tr_equid) confusion_matrix(y_te_equid, DecisionTree_confusion.predict(X_te_equid))",0,execute_result,"from sklearn.metrics import confusion_matrix
 DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)
 DecisionTree_confusion.fit(X_tr_equid, y_tr_equid)
 confusion_matrix(y_te_equid, DecisionTree_confusion.predict(X_te_equid))"
"SETUP plot_confusion_matrix(DecisionTree_confusion, X_te_equid, y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])",0,execute_result,"from sklearn.metrics import plot_confusion_matrix
 plot_confusion_matrix(DecisionTree_confusion, X_te_equid, y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
"SETUP os.environ[""PATH""] += os.pathsep + 'D:path(x86)path' ASSIGN = export_graphviz( DecisionTree, out_file=None, ASSIGN=X.columns, class_names=[Level1_equidistant,Level2_equidistant, Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant], ASSIGN=True, rounded=True, proportion=True ) graphviz.Source(ASSIGN)",0,execute_result,"from sklearn.tree import export_graphviz
 import os
 os.environ[""PATH""] += os.pathsep + 'D:/Program Files (x86)/Graphviz2.38/bin/'
 # Export decision tree
 dot_data = export_graphviz(
     DecisionTree, out_file=None,
     feature_names=X.columns, class_names=[Level1_equidistant,Level2_equidistant, Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant],
     filled=True, rounded=True, proportion=True   
 )
 import graphviz
 
 # Display decision tree
 graphviz.Source(dot_data)"
"CHECKPOINT ASSIGN=range(1,15) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for depth in ASSIGN: DecisionTree = DecisionTreeClassifier(criterion='gini', random_state=0, max_depth=depth) DecisionTree.fit(X, y_balanced) ASSIGN = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(DecisionTree.score(X, y_balanced)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"depths=range(1,15)
 scoring='accuracy'
 cv_val_scores_list = []
 cv_val_scores_std = []
 cv_val_scores_mean = []
 accuracy_scores = []
 for depth in depths:
     DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)
     DecisionTree.fit(X, y_balanced)
     cv_val_scores = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring)
     cv_val_scores_list.append(cv_val_scores)
     cv_val_scores_mean.append(cv_val_scores.mean())
     cv_val_scores_std.append(cv_val_scores.std())
     accuracy_scores.append(DecisionTree.score(X, y_balanced))
     Text=""depth %d, cv_val_scores_mean %f  score %f""%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced))
     print(Text)
 cv_val_scores_mean = np.array(cv_val_scores_mean)
 cv_val_scores_std = np.array(cv_val_scores_std)
 accuracy_scores = np.array(accuracy_scores)"
"CHECKPOINT print(,np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1) OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1",1,stream,"print(""The maximum is by depth_balanced = "",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)
 OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1"
"ASSIGN = plt.subplots(1,1, figsize=(15,5)) ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9) ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2) ASSIGN = plt.ASSIGN() ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9) ASSIGN=""decision tree accurency - _balanced bins"" ax.set_title(ASSIGN, fontsize=16) ax.set_xlabel('Tree depth', fontsize=14) ax.set_ylabel('accuracy', fontsize=14) ax.set_xticks(depths) ax.legend()",0,execute_result,"fig, ax = plt.subplots(1,1, figsize=(15,5))
 ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)
 ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)
 ylim = plt.ylim()
 ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)
 title=""decision tree accurency - _balanced bins""
 ax.set_title(title, fontsize=16)
 ax.set_xlabel('Tree depth', fontsize=14)
 ax.set_ylabel('accuracy', fontsize=14)
 ax.set_xticks(depths)
 ax.legend()"
"CHECKPOINT DecisionTree = DecisionTreeClassifier( ASSIGN='gini', random_state=0, max_depth=OptDepth) DecisionTree.fit(X, y_balanced) ASSIGN = cross_validate(DecisionTree, X, y_balanced, cv=cv_strategy) df_resultat.at[df_resultat.index==3,""method""]=""decision-tree _balanced bins"" df_resultat.at[df_resultat.index==3,""Cross-validation""]=np.mean(ASSIGN['test_score']) print(,DecisionTree.score(X, y_balanced)) print(,np.mean(ASSIGN['test_score']))",0,stream,"DecisionTree = DecisionTreeClassifier(
     criterion='gini',  random_state=0, max_depth=OptDepth)
 # Fit decision tree
 DecisionTree.fit(X, y_balanced)
 # Get score
 Tree_scores = cross_validate(DecisionTree, X, y_balanced, cv=cv_strategy)
 df_resultat.at[df_resultat.index==3,""method""]=""decision-tree _balanced bins""
 df_resultat.at[df_resultat.index==3,""Cross-validation""]=np.mean(Tree_scores['test_score'])
 print(""score DecisionTree _balanced bins ="",DecisionTree.score(X, y_balanced))
 print(""cross validation DecisionTree balanced bins="",np.mean(Tree_scores['test_score']))"
"SETUP DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth) DecisionTree_confusion.fit(X_tr_balan, y_tr_balan) confusion_matrix(y_te_balan, DecisionTree_confusion.predict(X_te_balan))",0,execute_result,"from sklearn.metrics import confusion_matrix
 DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)
 DecisionTree_confusion.fit(X_tr_balan, y_tr_balan)
 confusion_matrix(y_te_balan, DecisionTree_confusion.predict(X_te_balan))"
"SETUP plot_confusion_matrix(DecisionTree_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])",0,execute_result,"from sklearn.metrics import plot_confusion_matrix
 plot_confusion_matrix(DecisionTree_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
"SETUP os.environ[""PATH""] += os.pathsep + 'D:path(x86)path' ASSIGN = export_graphviz( DecisionTree, out_file=None, ASSIGN=X.columns, class_names=[Level1_balanced,Level2_balanced, Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced], ASSIGN=True, rounded=True, proportion=True ) graphviz.Source(ASSIGN)",0,execute_result,"from sklearn.tree import export_graphviz
 import os
 os.environ[""PATH""] += os.pathsep + 'D:/Program Files (x86)/Graphviz2.38/bin/'
 # Export decision tree
 dot_data = export_graphviz(
     DecisionTree, out_file=None,
     feature_names=X.columns, class_names=[Level1_balanced,Level2_balanced, Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced],
     filled=True, rounded=True, proportion=True   
 )
 import graphviz
 
 # Display decision tree
 graphviz.Source(dot_data)"
"SETUP CHECKPOINT ASSIGN=range(100,2000,100) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for Estimator in ASSIGN: ASSIGN = RandomForestClassifier(n_estimators=Estimator,random_state=0) ASSIGN.fit(X, y_equidistant) ASSIGN = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(ASSIGN.fit(X, y_equidistant).score(X, y_equidistant)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"from sklearn.ensemble import RandomForestClassifier
 Estimators=range(100,2000,100)
 scoring='accuracy'
 cv_val_scores_list = []
 cv_val_scores_std = []
 cv_val_scores_mean = []
 accuracy_scores = []
 for Estimator in Estimators:
     forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)
     forest.fit(X, y_equidistant)
     cv_val_scores = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)
     cv_val_scores_list.append(cv_val_scores)
     cv_val_scores_mean.append(cv_val_scores.mean())
     cv_val_scores_std.append(cv_val_scores.std())
     accuracy_scores.append(forest.fit(X, y_equidistant).score(X, y_equidistant))
     Text=""depth %d, cv_val_scores_mean %f  score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))
     print(Text)
 cv_val_scores_mean = np.array(cv_val_scores_mean)
 cv_val_scores_std = np.array(cv_val_scores_std)
 accuracy_scores = np.array(accuracy_scores)"
"CHECKPOINT print(,(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100) OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100",1,stream,"print(""The maximum is by Estimator_balanced = "",(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)
 OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100"
"ASSIGN = plt.subplots(1,1, figsize=(15,5)) ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9) ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2) ASSIGN = plt.ASSIGN() ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9) ASSIGN=""Forest accurency - _balanced bins"" ax.set_title(ASSIGN, fontsize=16) ax.set_xlabel('Forest Estimator', fontsize=14) ax.set_ylabel('accuracy', fontsize=14) ax.set_xticks(Estimators) ax.legend()",0,execute_result,"fig, ax = plt.subplots(1,1, figsize=(15,5))
 ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)
 ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)
 ylim = plt.ylim()
 ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)
 title=""Forest accurency - _balanced bins""
 ax.set_title(title, fontsize=16)
 ax.set_xlabel('Forest Estimator', fontsize=14)
 ax.set_ylabel('accuracy', fontsize=14)
 ax.set_xticks(Estimators)
 ax.legend()"
"ASSIGN = RandomForestClassifier(n_estimators=OptEstimator,random_state=0) ASSIGN.fit(X, y_equidistant)",0,execute_result,"# Build a forest and compute the feature importances
 forest = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)
 forest.fit(X, y_equidistant)"
"CHECKPOINT ASSIGN = cross_validate(forest, X, y_equidistant, cv=cv_strategy) print('Forest by equidistant interval score {:.3f}'.format(forest.score(X, y_equidistant))) df_resultat.at[df_resultat.index==4,""method""]=""Forest equidistant"" df_resultat.at[df_resultat.index==4,""Cross-validation""]=np.mean(ASSIGN['test_score']) print('Forest by equidistant interval cross validation {:.3f}'.format(np.mean(ASSIGN['test_score'])))",1,stream,"# Mean test score of a 250x decision tree 
 Forest_scores = cross_validate(forest, X, y_equidistant, cv=cv_strategy)
 print('Forest by equidistant interval score {:.3f}'.format(forest.score(X, y_equidistant)))
 df_resultat.at[df_resultat.index==4,""method""]=""Forest equidistant""
 df_resultat.at[df_resultat.index==4,""Cross-validation""]=np.mean(Forest_scores['test_score'])
 print('Forest by equidistant interval cross validation {:.3f}'.format(np.mean(Forest_scores['test_score'])))"
"ASSIGN = RandomForestClassifier(n_estimators=OptEstimator,random_state=0) ASSIGN.fit(X_tr_equid, y_tr_equid) confusion_matrix(y_te_equid, ASSIGN.predict(X_te_equid))",0,execute_result,"forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)
 forest_confusion.fit(X_tr_equid, y_tr_equid)
 confusion_matrix(y_te_equid, forest_confusion.predict(X_te_equid))"
"plot_confusion_matrix(forest_confusion, X_te_equid,y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])",0,execute_result,"plot_confusion_matrix(forest_confusion, X_te_equid,y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
"ASSIGN = forest.feature_importances_ ASSIGN = np.ASSIGN([tree.feature_importances_ for tree in forest.estimators_],axis=0) ASSIGN = np.argsort(importances)[::-1]",1,not_existent,"importances = forest.feature_importances_
 std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)
 indices = np.argsort(importances)[::-1]"
"CHECKPOINT print() for f in range(X.shape[1]): print( % (f + 1, X.columns[indices[f]], importances[indices[f]])) plt.figure(figsize=(20,10)) plt.title(""Feature importances"") plt.bar(range(X.shape[1]), importances[indices], ASSIGN=""r"", yerr=std[indices], align=""center"") plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65) plt.xlim([-1, X.shape[1]]) plt.show()",0,stream,"# Print the feature ranking
 print(""Feature ranking by equidistand interval:"")
 
 for f in range(X.shape[1]):
     print(""%d. feature %s (%f)"" % (f + 1, X.columns[indices[f]], importances[indices[f]]))
 
 # Plot the feature importances of the forest
 plt.figure(figsize=(20,10))
 plt.title(""Feature importances"")
 plt.bar(range(X.shape[1]), importances[indices],
        color=""r"", yerr=std[indices], align=""center"")
 plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)
 plt.xlim([-1, X.shape[1]])
 plt.show()"
"CHECKPOINT ASSIGN=range(100,2000,100) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for Estimator in ASSIGN: ASSIGN = RandomForestClassifier(n_estimators=Estimator,random_state=0) ASSIGN.fit(X, y_balanced) ASSIGN = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(ASSIGN.fit(X, y_balanced).score(X, y_balanced)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"Estimators=range(100,2000,100)
 scoring='accuracy'
 cv_val_scores_list = []
 cv_val_scores_std = []
 cv_val_scores_mean = []
 accuracy_scores = []
 for Estimator in Estimators:
     forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)
     forest.fit(X, y_balanced)
     cv_val_scores = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring)
     cv_val_scores_list.append(cv_val_scores)
     cv_val_scores_mean.append(cv_val_scores.mean())
     cv_val_scores_std.append(cv_val_scores.std())
     accuracy_scores.append(forest.fit(X, y_balanced).score(X, y_balanced))
     Text=""depth %d, cv_val_scores_mean %f  score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced))
     print(Text)
 cv_val_scores_mean = np.array(cv_val_scores_mean)
 cv_val_scores_std = np.array(cv_val_scores_std)
 accuracy_scores = np.array(accuracy_scores)"
"SETUP CHECKPOINT ASSIGN = cross_validate(forest, X, y_balanced, cv=cv_strategy) df_resultat.at[df_resultat.index==5,""method""]=""Forest balanced bins"" df_resultat.at[df_resultat.index==5,""Cross-validation""]=np.mean(ASSIGN['test_score']) print('Forest by balanced bins of element score {:.3f}'.format(forest.score(X, y_balanced))) print('Forest by balanced bins of element - mean test {:.3f}'.format(np.mean(ASSIGN['test_score'])))",1,stream,"from sklearn.model_selection import cross_validate
 Forest_scores = cross_validate(forest, X, y_balanced, cv=cv_strategy)
 df_resultat.at[df_resultat.index==5,""method""]=""Forest balanced bins""
 df_resultat.at[df_resultat.index==5,""Cross-validation""]=np.mean(Forest_scores['test_score'])
 print('Forest by balanced bins of element score {:.3f}'.format(forest.score(X, y_balanced)))
 print('Forest by balanced bins of element - mean test {:.3f}'.format(np.mean(Forest_scores['test_score'])))"
"ASSIGN = RandomForestClassifier(n_estimators=OptEstimator,random_state=0) ASSIGN.fit(X_tr_balan, y_tr_balan) confusion_matrix(y_te_balan, ASSIGN.predict(X_te_balan))",0,execute_result,"forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)
 forest_confusion.fit(X_tr_balan, y_tr_balan)
 confusion_matrix(y_te_balan, forest_confusion.predict(X_te_balan))"
"SETUP plot_confusion_matrix(forest_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])",0,execute_result,"from sklearn.metrics import plot_confusion_matrix
 plot_confusion_matrix(forest_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
"ASSIGN = forest.feature_importances_ ASSIGN = np.ASSIGN([tree.feature_importances_ for tree in forest.estimators_], ASSIGN=0) ASSIGN = np.argsort(importances)[::-1]",1,not_existent,"importances = forest.feature_importances_
 std = np.std([tree.feature_importances_ for tree in forest.estimators_],
              axis=0)
 indices = np.argsort(importances)[::-1]"
"CHECKPOINT print() for f in range(X.shape[1]): print( % (f + 1, X.columns[indices[f]], importances[indices[f]])) plt.figure(figsize=(20,10)) plt.title(""Feature importances"") plt.bar(range(X.shape[1]), importances[indices], ASSIGN=""r"", yerr=std[indices], align=""center"") plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65) plt.xlim([-1, X.shape[1]]) plt.show()",0,stream,"# Print the feature ranking
 print(""Feature ranking by balanced bins:"")
 
 for f in range(X.shape[1]):
     print(""%d. feature %s (%f)"" % (f + 1, X.columns[indices[f]], importances[indices[f]]))
 
 # Plot the feature importances of the forest
 plt.figure(figsize=(20,10))
 plt.title(""Feature importances"")
 plt.bar(range(X.shape[1]), importances[indices],
        color=""r"", yerr=std[indices], align=""center"")
 plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)
 plt.xlim([-1, X.shape[1]])
 plt.show()"
"SETUP ASSIGN = PCA(n_components=2) ASSIGN.fit(X, y=None); ASSIGN= pca.transform(X) ASSIGN=pd.DataFrame(feature_2) ASSIGN=feature_2_df.shape",1,not_existent,"from sklearn.decomposition import PCA
 pca = PCA(n_components=2)
 # Apply PCA
 pca.fit(X, y=None); # Unsupervised learning, no y variable
 feature_2= pca.transform(X)
 feature_2_df=pd.DataFrame(feature_2)
 len_cluster, col=feature_2_df.shape"
"SETUP CHECKPOINT X_fea_2_tr_equid, X_fea_2_te_equid, y_fea_2_tr_equid, y_fea_2_te_equid = train_test_split(feature_2, y_equidistant, test_size=0.15, random_state=1) print('Train set equidistant:', X_fea_2_tr_equid.shape, y_fea_2_tr_equid.shape) print('Testn set equidistant:', X_fea_2_te_equid.shape, y_fea_2_te_equid.shape) X_fea_2_tr_balan, X_fea_2_te_balan, y_fea_2_tr_balan, y_fea_2_te_balan = train_test_split(feature_2, y_balanced, test_size=0.15, random_state=1) print('Train set balanced:', X_fea_2_tr_balan.shape, y_fea_2_tr_balan.shape) print('Testn set balanced:', X_fea_2_te_balan.shape, y_fea_2_te_balan.shape)",1,stream,"from sklearn.model_selection import train_test_split
 X_fea_2_tr_equid, X_fea_2_te_equid, y_fea_2_tr_equid, y_fea_2_te_equid = train_test_split(feature_2, y_equidistant, test_size=0.15, random_state=1)
 print('Train set equidistant:', X_fea_2_tr_equid.shape, y_fea_2_tr_equid.shape)
 print('Testn set equidistant:', X_fea_2_te_equid.shape, y_fea_2_te_equid.shape)
 X_fea_2_tr_balan, X_fea_2_te_balan, y_fea_2_tr_balan, y_fea_2_te_balan = train_test_split(feature_2, y_balanced, test_size=0.15, random_state=1)
 print('Train set balanced:', X_fea_2_tr_balan.shape, y_fea_2_tr_balan.shape)
 print('Testn set balanced:', X_fea_2_te_balan.shape, y_fea_2_te_balan.shape)"
"SETUP ASSIGN = Pipeline([ ('scaler', None), ('knn', KNeighborsClassifier( ASSIGN=-1 )) ])",0,not_existent,"from sklearn.neighbors import KNeighborsClassifier
 from sklearn.pipeline import Pipeline
 from sklearn.preprocessing import StandardScaler
 
 # Create k-NN classifier
 pipe = Pipeline([
     #('scaler', StandardScaler()), # With standardization
     ('scaler', None), # Better performance without standardization!
     ('knn', KNeighborsClassifier(
         n_jobs=-1 # As many parallel jobs as possible
     ))
 ])
"
"SETUP CHECKPOINT ASSIGN = list(range(1,35)) ASSIGN = [] for n_neighborss in ASSIGN: ASSIGN='distance' ASSIGN=pd.DataFrame(feature_2).loc[:][0] ASSIGN=pd.DataFrame(feature_2).loc[:][1] ASSIGN = .02 ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, ASSIGN=ASSIGN)) ]) ASSIGN.fit(feature_2, y_equidistant) ASSIGN = cross_val_score(knn_pipe,feature_2,y_equidistant,cv=cv_strategy ) ASSIGN.append(ASSIGN.mean()) Missclassification_Error = [1-x for x in ASSIGN] print(,format(Missclassification_Error.index(min(Missclassification_Error)))) ASSIGN=Missclassification_Error.index(min(Missclassification_Error))",0,stream,"from matplotlib.colors import ListedColormap
 from sklearn import neighbors, datasets
 from sklearn import metrics
 from sklearn.model_selection import cross_val_score
 neighbors = list(range(1,35))
 CV_scores = []
 
 
 for n_neighborss in neighbors:
     weights='distance'
     feature1=pd.DataFrame(feature_2).loc[:][0]
     feature2=pd.DataFrame(feature_2).loc[:][1]
     h = .02  # step size in the mesh
     # we create an instance of Neighbours Classifier and fit the data.
     # Create a k-NN pipeline
     knn_pipe = Pipeline([
         ('scaler', StandardScaler()),
         ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights)) ])
     # Fit estimator
     knn_pipe.fit(feature_2, y_equidistant)
     scores = cross_val_score(knn_pipe,feature_2,y_equidistant,cv=cv_strategy )
     CV_scores.append(scores.mean())
 Missclassification_Error = [1-x for x in CV_scores]
 print(""the optimal k is "",format(Missclassification_Error.index(min(Missclassification_Error))))
 k_Optimal=Missclassification_Error.index(min(Missclassification_Error))"
"SETUP plt.plot(Missclassification_Error) plt.xlabel(""number of neighbors K"") plt.ylabel(""Missclassification Error"") plt.show()",0,display_data,"from matplotlib.legend_handler import HandlerLine2D
 plt.plot(Missclassification_Error)
 plt.xlabel(""number of neighbors K"")
 plt.ylabel(""Missclassification Error"")
 plt.show()"
"SETUP ASSIGN = ListedColormap([' ASSIGN = ListedColormap([' ASSIGN=pd.DataFrame(feature_2).loc[:][0] ASSIGN=pd.DataFrame(feature_2).loc[:][1] ASSIGN='distance' ASSIGN = .02 ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=k_Optimal)) ]) ASSIGN.fit(feature_2, y_equidistant) feature1_min, feature1_max = ASSIGN.min() - 1, ASSIGN.max() + 1 feature2_min, feature2_max = ASSIGN.min() - 1, ASSIGN.max() + 1 ASSIGN = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h)) ASSIGN = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()]) ASSIGN = Z.reshape(xx.shape)",1,not_existent,"from matplotlib.colors import ListedColormap
 from sklearn import neighbors, datasets
 from sklearn.metrics import accuracy_score
 
 # Create color maps
 cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])
 cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])
 
 feature1=pd.DataFrame(feature_2).loc[:][0]
 feature2=pd.DataFrame(feature_2).loc[:][1]
 
 weights='distance'
 h = .02  # step size in the mesh
 # we create an instance of Neighbours Classifier and fit the data.
 # Create a k-NN pipeline
 knn_pipe = Pipeline([
     ('scaler', StandardScaler()),
     ('knn', KNeighborsClassifier(n_neighbors=k_Optimal))
     #, weights=weights))
 ])
 # Fit estimator
 knn_pipe.fit(feature_2, y_equidistant)
 # Plot the decision boundary. For that, we will assign a color to each
 # point in the mesh [x_min, x_max]x[y_min, y_max].
 feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1
 feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1
 xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))
 Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])
 Zo = Z.reshape(xx.shape)"
"plt.figure(figsize=(18, 10)) plt.pcolormesh(xx, yy, Zo, cmap=cmap_light) Label1=pd.DataFrame([1,2,3,4,5]) plt.scatter(feature1, feature2, c=y_equidistant, cmap=cmap_bold) ASSIGN = plt.colorbar() ASSIGN.ax.set_yticklabels([Level1_equidistant,Level2_equidistant,Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant]) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.xlabel(""feature 1"") plt.ylabel(""feature 2"") plt.title(""6-Class classification,  equidistant bins - total infected log10 (k = %i, weights = '%s')"" % (k_Optimal, weights)) plt.show()",0,display_data,"# Put the result into a color plot
 plt.figure(figsize=(18, 10))
 plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)
 #Label1=pd.DataFrame(list(class_index.items()))
 Label1=pd.DataFrame([1,2,3,4,5])
 # Plot also the training points
 plt.scatter(feature1, feature2, c=y_equidistant, cmap=cmap_bold)
 #color bar with label
 cbar = plt.colorbar() 
 cbar.ax.set_yticklabels([Level1_equidistant,Level2_equidistant,Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant])
 plt.xlim(xx.min(), xx.max())
 plt.ylim(yy.min(), yy.max())
 plt.xlabel(""feature 1"")
 plt.ylabel(""feature 2"")
 plt.title(""6-Class classification,   equidistant bins -  total infected log10 (k = %i, weights = '%s')"" % (k_Optimal, weights))
 plt.show()"
"SETUP CHECKPOINT ASSIGN = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy) df_resultat.at[df_resultat.index==6,""method""]=""knn equidistant bins"" df_resultat.at[df_resultat.index==6,""Cross-validation""]=np.mean(ASSIGN['test_score']) print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant))) print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(ASSIGN['test_score'])))",1,stream,"from sklearn.model_selection import cross_validate
 knn_scores = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy)
 df_resultat.at[df_resultat.index==6,""method""]=""knn equidistant bins""
 df_resultat.at[df_resultat.index==6,""Cross-validation""]=np.mean(knn_scores['test_score'])
 print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant)))
 print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score'])))"
"ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights)) ]) ASSIGN=knn_pipe.fit(X_fea_2_tr_equid, y_fea_2_tr_equid) ASSIGN = knn_pipe.predict(X_fea_2_te_equid) confusion_matrix(y_fea_2_te_equid, ASSIGN)",0,execute_result,"knn_pipe = Pipeline([
     ('scaler', StandardScaler()),
     ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))
 ])
 # Fit estimator
 knn_confusion=knn_pipe.fit(X_fea_2_tr_equid, y_fea_2_tr_equid)
 knn_predict = knn_pipe.predict(X_fea_2_te_equid)
 confusion_matrix(y_fea_2_te_equid, knn_predict)"
"plot_confusion_matrix(knn_confusion, X_fea_2_te_equid, y_fea_2_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])",0,execute_result,"plot_confusion_matrix(knn_confusion, X_fea_2_te_equid, y_fea_2_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
"SETUP CHECKPOINT print(classification_report(y_true=y_fea_2_te_equid, y_pred=knn_predict))",0,stream,"from sklearn.metrics import classification_report
 print(classification_report(y_true=y_fea_2_te_equid, y_pred=knn_predict))"
"SETUP CHECKPOINT ASSIGN = list(range(1,35)) ASSIGN = [] for n_neighborss in ASSIGN: ASSIGN='distance' ASSIGN=pd.DataFrame(feature_2).loc[:][0] ASSIGN=pd.DataFrame(feature_2).loc[:][1] ASSIGN = .02 ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, ASSIGN=ASSIGN)) ]) ASSIGN.fit(feature_2, y_balanced) ASSIGN = cross_val_score(knn_pipe,feature_2,y_balanced,cv=cv_strategy ) ASSIGN.append(ASSIGN.mean()) Missclassification_Error = [1-x for x in ASSIGN] print(,format(Missclassification_Error.index(min(Missclassification_Error)))) ASSIGN=Missclassification_Error.index(min(Missclassification_Error)) plt.plot( Missclassification_Error) plt.xlabel(""number of ASSIGN K"") plt.ylabel(""Missclassification Error"") plt.show()",0,stream,"neighbors = list(range(1,35))
 CV_scores = []
 
 for n_neighborss in neighbors:
     weights='distance'
     feature1=pd.DataFrame(feature_2).loc[:][0]
     feature2=pd.DataFrame(feature_2).loc[:][1]
     h = .02  # step size in the mesh
     # we create an instance of Neighbours Classifier and fit the data.
     # Create a k-NN pipeline
     knn_pipe = Pipeline([
         ('scaler', StandardScaler()),
         ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights))
     ])
     # Fit estimator
     knn_pipe.fit(feature_2, y_balanced)
     #
     scores = cross_val_score(knn_pipe,feature_2,y_balanced,cv=cv_strategy )
     CV_scores.append(scores.mean())
     
 Missclassification_Error = [1-x for x in CV_scores]
 print(""the optimal k is "",format(Missclassification_Error.index(min(Missclassification_Error))))
 k_Optimal=Missclassification_Error.index(min(Missclassification_Error))
     
 from matplotlib.legend_handler import HandlerLine2D
 plt.plot( Missclassification_Error)
 plt.xlabel(""number of neighbors K"")
 plt.ylabel(""Missclassification Error"")
 plt.show()"
"ASSIGN = ListedColormap([' ASSIGN = ListedColormap([' ASSIGN=pd.DataFrame(feature_2).loc[:][0] ASSIGN=pd.DataFrame(feature_2).loc[:][1] ASSIGN='distance' ASSIGN = .02 ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, ASSIGN=ASSIGN)) ]) ASSIGN.fit(feature_2, y_balanced) feature1_min, feature1_max = ASSIGN.min() - 1, ASSIGN.max() + 1 feature2_min, feature2_max = ASSIGN.min() - 1, ASSIGN.max() + 1 ASSIGN = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h)) ASSIGN = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()]) ASSIGN = Z.reshape(xx.shape)",1,not_existent,"# Create color maps
 cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])
 cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])
 #
 feature1=pd.DataFrame(feature_2).loc[:][0]
 feature2=pd.DataFrame(feature_2).loc[:][1]
 #
 weights='distance'
 h = .02  # step size in the mesh
 # Create a k-NN pipeline
 knn_pipe = Pipeline([
     ('scaler', StandardScaler()),
     ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))
 ])
 # Fit estimator
 knn_pipe.fit(feature_2, y_balanced)
 # Plot the decision boundary. For that, we will assign a color to each
 # point in the mesh [x_min, x_max]x[y_min, y_max].
 feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1
 feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1
 xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))
 Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])
 Zo = Z.reshape(xx.shape)"
"plt.figure(figsize=(18, 10)) plt.pcolormesh(xx, yy, Zo, cmap=cmap_light) Label1=pd.DataFrame([1,2,3,4,5]) plt.scatter(feature1, feature2, c=y_balanced, cmap=cmap_bold) ASSIGN = plt.colorbar() ASSIGN.ax.set_yticklabels([Level1_balanced,Level2_balanced,Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced]) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.xlabel(""feature 1"") plt.ylabel(""feature 2"") plt.title(""6-Class classification,  balanced bins - total infected log10 (k = %i, weights = '%s')"" % (k_Optimal, weights)) plt.show()",0,display_data,"# Put the result into a color plot
 plt.figure(figsize=(18, 10))
 plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)
 #Label1=pd.DataFrame(list(class_index.items()))
 Label1=pd.DataFrame([1,2,3,4,5])
 # Plot also the training points
 plt.scatter(feature1, feature2, c=y_balanced, cmap=cmap_bold)
 cbar = plt.colorbar() 
 cbar.ax.set_yticklabels([Level1_balanced,Level2_balanced,Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced])
 plt.xlim(xx.min(), xx.max())
 plt.ylim(yy.min(), yy.max())
 plt.xlabel(""feature 1"")
 plt.ylabel(""feature 2"")
 plt.title(""6-Class classification,   balanced bins -  total infected log10 (k = %i, weights = '%s')"" % (k_Optimal, weights))
 
 plt.show()"
"SETUP CHECKPOINT ASSIGN = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy) df_resultat.at[df_resultat.index==7,""method""]=""knn balanced bins"" df_resultat.at[df_resultat.index==7,""Cross-validation""]=np.mean(ASSIGN['test_score']) print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced))) print('knn by equidistant bins of element - mean test_score {:.3f}'.format(np.mean(ASSIGN['test_score'])))",1,stream,"from sklearn.model_selection import cross_validate
 knn_scores = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy)
 df_resultat.at[df_resultat.index==7,""method""]=""knn balanced bins""
 df_resultat.at[df_resultat.index==7,""Cross-validation""]=np.mean(knn_scores['test_score'])
 print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced)))
 print('knn by equidistant bins of element - mean test_score {:.3f}'.format(np.mean(knn_scores['test_score'])))"
"ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights)) ]) ASSIGN=knn_pipe.fit(X_fea_2_tr_balan, y_fea_2_tr_balan) ASSIGN = knn_pipe.predict(X_fea_2_te_balan)",0,not_existent,"knn_pipe = Pipeline([
     ('scaler', StandardScaler()),
     ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))
 ])
 # Fit estimator
 knn_confusion=knn_pipe.fit(X_fea_2_tr_balan, y_fea_2_tr_balan)
 knn_predict = knn_pipe.predict(X_fea_2_te_balan)"
"confusion_matrix(y_fea_2_te_balan, knn_predict)",0,execute_result,"confusion_matrix(y_fea_2_te_balan, knn_predict)"
"plot_confusion_matrix(knn_confusion, X_fea_2_te_balan, y_fea_2_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])",0,execute_result,"plot_confusion_matrix(knn_confusion, X_fea_2_te_balan, y_fea_2_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
CHECKPOINT df_resultat,0,execute_result,df_resultat
"sns.barplot(x=df_resultat[""Cross-validation""],y=df_resultat['method']) plt.show()",0,display_data,"sns.barplot(x=df_resultat[""Cross-validation""],y=df_resultat['method'])
 plt.show()"
"ASSIGN = codiv_country.copy() ASSIGN['Quarantine_cat'] = ASSIGN['Quarantine'].notnull().astype(int) ASSIGN['Restrictions_cat'] = ASSIGN['Restrictions'].notnull().astype(int) ASSIGN['Schools_cat'] = ASSIGN['Schools'].notnull().astype(int) ASSIGN=ASSIGN.drop([ 'Quarantine','Schools','Restrictions', 'Country', 'Total Deaths','Total Infected','Total Active','Total Recovered', ""Total Deaths Log10"",""Total Recovered Log10"",""Total Active Log10"",""Total Infected Log10"" ], axis=1) ASSIGN.sample(10)",1,execute_result,"# Working on a copy
 codiv_country_analyze = codiv_country.copy()
 
 # Creating categorical variables
 codiv_country_analyze['Quarantine_cat'] = codiv_country_analyze['Quarantine'].notnull().astype(int)
 codiv_country_analyze['Restrictions_cat'] = codiv_country_analyze['Restrictions'].notnull().astype(int)
 codiv_country_analyze['Schools_cat'] = codiv_country_analyze['Schools'].notnull().astype(int)
 
 # 
 codiv_country_analyze=codiv_country_analyze.drop([
     'Quarantine','Schools','Restrictions', # now categorical
     'Country', # not helpful
     'Total Deaths','Total Infected','Total Active','Total Recovered', 
     ""Total Deaths Log10"",""Total Recovered Log10"",""Total Active Log10"",""Total Infected Log10""
 ], axis=1)
 
 codiv_country_analyze.sample(10)"
codiv_country_analyze['Deaths Ratio'].describe(),0,execute_result,codiv_country_analyze['Deaths Ratio'].describe()
"codiv_country_analyze['category_balanced'], bin_edges_balanced=pd.qcut(codiv_country_analyze['Deaths Ratio'], q=6,labels=False, retbins=True) codiv_country_analyze['category_balanced'] = codiv_country_analyze['category_balanced'] +1",1,not_existent,"codiv_country_analyze['category_balanced'], bin_edges_balanced=pd.qcut(codiv_country_analyze['Deaths Ratio'], q=6,labels=False, retbins=True)
 #balanced bins deliver category for 0-5, euqidistant from 1-6, so i will add 1 to the category numbers
 codiv_country_analyze['category_balanced'] = codiv_country_analyze['category_balanced'] +1"
"CHECKPOINT ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][0] ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][1] ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][2] ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][3] ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][4] ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][5] ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][6] Level1_balanced='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,) Level2_balanced='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,) Level3_balanced='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,) Level4_balanced='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,) Level5_balanced='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,) Level6_balanced='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,) print(,Level1_balanced) print(,Level2_balanced) print(,Level3_balanced) print(,Level4_balanced) print(,Level5_balanced) print(,Level6_balanced)",1,stream,"ylevel0_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][0]
 ylevel1_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][1]
 ylevel2_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][2]
 ylevel3_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][3]
 ylevel4_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][4]
 ylevel5_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][5]
 ylevel6_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][6]
 Level1_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel0_balanced,ylevel1_balanced,)
 Level2_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel1_balanced,ylevel2_balanced,)
 Level3_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel2_balanced,ylevel3_balanced,)
 Level4_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel3_balanced,ylevel4_balanced,)
 Level5_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel4_balanced,ylevel5_balanced,)
 Level6_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel5_balanced,ylevel6_balanced,)
 print(""Level1_balanced= "",Level1_balanced)
 print(""Level2_balanced= "",Level2_balanced)
 print(""Level3_balanced= "",Level3_balanced)
 print(""Level4_balanced= "",Level4_balanced)
 print(""Level5_balanced= "",Level5_balanced)
 print(""Level6_balanced= "",Level6_balanced)"
"CHECKPOINT ASSIGN=codiv_country_analyze['Deaths Ratio'].values.max() print(ASSIGN) ASSIGN=ASSIGN+ASSIGN*0.001 print(,ASSIGN) ASSIGN=(codiv_country_analyze['Deaths Ratio'].values.min()) ASSIGN=ASSIGN-ASSIGN*0.001 print(,ASSIGN) ASSIGN=(ymax-ymin)path print(ASSIGN,ASSIGN) ASSIGN = ymin ASSIGN = ymin+yinterval*1 ASSIGN = ymin+yinterval*2 ASSIGN = ymin+yinterval*3 ASSIGN = ymin+yinterval*4 ASSIGN = ymin+yinterval*5 ASSIGN = ymin+yinterval*6 Level1_equidistant='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,) Level2_equidistant='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,) Level3_equidistant='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,) Level4_equidistant='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,) Level5_equidistant='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,) Level6_equidistant='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,) codiv_country_analyze['category_equidistant']=np.digitize(codiv_country_analyze['Deaths Ratio'].values, ASSIGN=[ylevel0_equidistant,ylevel1_equidistant,ylevel2_equidistant,ylevel3_equidistant,ylevel4_equidistant,ylevel5_equidistant,ylevel6_equidistant]) print(,Level1_equidistant) print(,Level2_equidistant) print(,Level3_equidistant) print(,Level4_equidistant) print(,Level5_equidistant) print(,Level6_equidistant)",1,stream,"ymax=codiv_country_analyze['Deaths Ratio'].values.max()
 print(ymax)
 ymax=ymax+ymax*0.001
 print(""ymax = "",ymax)
 ymin=(codiv_country_analyze['Deaths Ratio'].values.min())
 ymin=ymin-ymin*0.001
 print(""ymin = "",ymin)
 yinterval=(ymax-ymin)/6
 print(ymin,ymax)
 # 
 ylevel0_equidistant = ymin
 ylevel1_equidistant = ymin+yinterval*1
 ylevel2_equidistant = ymin+yinterval*2
 ylevel3_equidistant = ymin+yinterval*3
 ylevel4_equidistant = ymin+yinterval*4
 ylevel5_equidistant = ymin+yinterval*5
 ylevel6_equidistant = ymin+yinterval*6
 
 #
 Level1_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel0_equidistant,ylevel1_equidistant,)
 Level2_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel1_equidistant,ylevel2_equidistant,)
 Level3_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel2_equidistant,ylevel3_equidistant,)
 Level4_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel3_equidistant,ylevel4_equidistant,)
 Level5_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel4_equidistant,ylevel5_equidistant,)
 Level6_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel5_equidistant,ylevel6_equidistant,)
 codiv_country_analyze['category_equidistant']=np.digitize(codiv_country_analyze['Deaths Ratio'].values,
                           bins=[ylevel0_equidistant,ylevel1_equidistant,ylevel2_equidistant,ylevel3_equidistant,ylevel4_equidistant,ylevel5_equidistant,ylevel6_equidistant])
 print(""Level1_equidistant= "",Level1_equidistant)
 print(""Level2_equidistant= "",Level2_equidistant)
 print(""Level3_equidistant= "",Level3_equidistant)
 print(""Level4_equidistant= "",Level4_equidistant)
 print(""Level5_equidistant= "",Level5_equidistant)
 print(""Level6_equidistant= "",Level6_equidistant)"
"ASSIGN = codiv_country_analyze['category_equidistant'] ASSIGN = codiv_country_analyze['category_balanced'] ASSIGN = codiv_country_analyze.drop(['Deaths Ratio','category_balanced','category_equidistant'], axis=1) sns.countplot(ASSIGN)",0,execute_result,"y_equidistant = codiv_country_analyze['category_equidistant']
 y_balanced = codiv_country_analyze['category_balanced']
 X = codiv_country_analyze.drop(['Deaths Ratio','category_balanced','category_equidistant'], axis=1) 
 sns.countplot(y_balanced)"
"CHECKPOINT ASSIGN = DummyClassifier(strategy=""most_frequent"") ASSIGN.fit(X, y_equidistant) ASSIGN.predict(X) print(, ASSIGN.score(X, y_equidistant))",0,stream,"dummy_clf = DummyClassifier(strategy=""most_frequent"")
 dummy_clf.fit(X, y_equidistant)
 dummy_clf.predict(X)
 print(""scores baseline mostfrequent classifier equidistant = "", dummy_clf.score(X, y_equidistant))"
"CHECKPOINT ASSIGN = DummyClassifier(strategy=""most_frequent"") ASSIGN.fit(X, y_balanced) ASSIGN.predict(X) print(, ASSIGN.score(X, y_balanced))",0,stream,"dummy_clf = DummyClassifier(strategy=""most_frequent"")
 dummy_clf.fit(X, y_balanced)
 dummy_clf.predict(X)
 print(""training_scores baseline mostfrequent classifier balanced = "", dummy_clf.score(X, y_balanced))"
"CHECKPOINT ASSIGN=range(1,15) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for depth in ASSIGN: DecisionTree = DecisionTreeClassifier(criterion='gini', random_state=0, max_depth=depth) DecisionTree.fit(X, y_equidistant) ASSIGN = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(DecisionTree.score(X, y_equidistant)) ASSIGN=""depth %d, cv_val_scores_mean %f std %f score %f""%(depth,cv_val_scores.mean(),cv_val_scores.std(),DecisionTree.score(X, y_equidistant)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"depths=range(1,15)
 scoring='accuracy'
 cv_val_scores_list = []
 cv_val_scores_std = []
 cv_val_scores_mean = []
 accuracy_scores = []
 for depth in depths:
     DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)
     DecisionTree.fit(X, y_equidistant)
     cv_val_scores = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)
     cv_val_scores_list.append(cv_val_scores)
     cv_val_scores_mean.append(cv_val_scores.mean())
     cv_val_scores_std.append(cv_val_scores.std())
     accuracy_scores.append(DecisionTree.score(X, y_equidistant))
     Text=""depth %d, cv_val_scores_mean %f std %f score %f""%(depth,cv_val_scores.mean(),cv_val_scores.std(),DecisionTree.score(X, y_equidistant))
     print(Text)
 cv_val_scores_mean = np.array(cv_val_scores_mean)
 cv_val_scores_std = np.array(cv_val_scores_std)
 accuracy_scores = np.array(accuracy_scores)"
"CHECKPOINT print(,np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1) OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1",1,stream,"print(""The maximum is by depth = "",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)
 OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1
"
"SETUP os.environ[""PATH""] += os.pathsep + 'D:path(x86)path' ASSIGN = export_graphviz( DecisionTree, out_file=None, ASSIGN=X.columns, class_names=[Level1_equidistant,Level2_equidistant, Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant], ASSIGN=True, rounded=True, proportion=True ) graphviz.Source(ASSIGN)",0,execute_result,"os.environ[""PATH""] += os.pathsep + 'D:/Program Files (x86)/Graphviz2.38/bin/'
 # Export decision tree
 dot_data = export_graphviz(
     DecisionTree, out_file=None,
     feature_names=X.columns, class_names=[Level1_equidistant,Level2_equidistant, Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant],
     filled=True, rounded=True, proportion=True   
 )
 import graphviz
 
 # Display decision tree
 graphviz.Source(dot_data)"
"CHECKPOINT ASSIGN=range(100,2000,100) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for Estimator in ASSIGN: ASSIGN = RandomForestClassifier(n_estimators=Estimator,random_state=0) ASSIGN.fit(X, y_equidistant) ASSIGN = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(ASSIGN.fit(X, y_equidistant).score(X, y_equidistant)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"Estimators=range(100,2000,100)
 scoring='accuracy'
 cv_val_scores_list = []
 cv_val_scores_std = []
 cv_val_scores_mean = []
 accuracy_scores = []
 for Estimator in Estimators:
     forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)
     forest.fit(X, y_equidistant)
     cv_val_scores = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)
     cv_val_scores_list.append(cv_val_scores)
     cv_val_scores_mean.append(cv_val_scores.mean())
     cv_val_scores_std.append(cv_val_scores.std())
     accuracy_scores.append(forest.fit(X, y_equidistant).score(X, y_equidistant))
     Text=""depth %d, cv_val_scores_mean %f  score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))
     print(Text)
 cv_val_scores_mean = np.array(cv_val_scores_mean)
 cv_val_scores_std = np.array(cv_val_scores_std)
 accuracy_scores = np.array(accuracy_scores)"
"SETUP plot_confusion_matrix(forest_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])",0,execute_result,"from sklearn.metrics import plot_confusion_matrix
 plot_confusion_matrix(forest_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
"ASSIGN = PCA(n_components=2) ASSIGN.fit(X, y=None); ASSIGN= pca.transform(X) ASSIGN=pd.DataFrame(feature_2) ASSIGN=feature_2_df.shape",1,not_existent,"pca = PCA(n_components=2)
 # Apply PCA
 pca.fit(X, y=None); # Unsupervised learning, no y variable
 feature_2= pca.transform(X)
 feature_2_df=pd.DataFrame(feature_2)
 len_cluster, col=feature_2_df.shape"
"CHECKPOINT X_fea_2_tr_equid, X_fea_2_te_equid, y_fea_2_tr_equid, y_fea_2_te_equid = train_test_split(feature_2, y_equidistant, test_size=0.15, random_state=1) print('Train set equidistant:', X_fea_2_tr_equid.shape, y_fea_2_tr_equid.shape) print('Testn set equidistant:', X_fea_2_te_equid.shape, y_fea_2_te_equid.shape) X_fea_2_tr_balan, X_fea_2_te_balan, y_fea_2_tr_balan, y_fea_2_te_balan = train_test_split(feature_2, y_balanced, test_size=0.15, random_state=1) print('Train set balanced:', X_fea_2_tr_balan.shape, y_fea_2_tr_balan.shape) print('Testn set balanced:', X_fea_2_te_balan.shape, y_fea_2_te_balan.shape)",1,stream,"X_fea_2_tr_equid, X_fea_2_te_equid, y_fea_2_tr_equid, y_fea_2_te_equid = train_test_split(feature_2, y_equidistant, test_size=0.15, random_state=1)
 print('Train set equidistant:', X_fea_2_tr_equid.shape, y_fea_2_tr_equid.shape)
 print('Testn set equidistant:', X_fea_2_te_equid.shape, y_fea_2_te_equid.shape)
 X_fea_2_tr_balan, X_fea_2_te_balan, y_fea_2_tr_balan, y_fea_2_te_balan = train_test_split(feature_2, y_balanced, test_size=0.15, random_state=1)
 print('Train set balanced:', X_fea_2_tr_balan.shape, y_fea_2_tr_balan.shape)
 print('Testn set balanced:', X_fea_2_te_balan.shape, y_fea_2_te_balan.shape)"
"SETUP CHECKPOINT ASSIGN = list(range(1,25)) ASSIGN = [] for n_neighborss in ASSIGN: ASSIGN='distance' ASSIGN=pd.DataFrame(feature_2).loc[:][0] ASSIGN=pd.DataFrame(feature_2).loc[:][1] ASSIGN = .02 ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, ASSIGN=ASSIGN)) ]) ASSIGN.fit(feature_2, y_equidistant) ASSIGN = knn_pipe.predict(feature_2) ASSIGN = cross_val_score(knn_pipe,feature_2,y_equidistant,cv=cv_strategy ) ASSIGN.append(ASSIGN.mean()) Missclassification_Error = [1-x for x in ASSIGN] print(,format(Missclassification_Error.index(min(Missclassification_Error)))) ASSIGN=Missclassification_Error.index(min(Missclassification_Error)) plt.plot( Missclassification_Error) plt.xlabel(""number of ASSIGN K"") plt.ylabel(""Missclassification Error"") plt.show()",0,stream,"neighbors = list(range(1,25))
 CV_scores = []
 
 for n_neighborss in neighbors:
     weights='distance'
     feature1=pd.DataFrame(feature_2).loc[:][0]
     feature2=pd.DataFrame(feature_2).loc[:][1]
     h = .02  # step size in the mesh
     # we create an instance of Neighbours Classifier and fit the data.
     # Create a k-NN pipeline
     knn_pipe = Pipeline([
         ('scaler', StandardScaler()),
         ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights))
     ])
     # Fit estimator
     knn_pipe.fit(feature_2, y_equidistant)
     #
     Z = knn_pipe.predict(feature_2)
     scores = cross_val_score(knn_pipe,feature_2,y_equidistant,cv=cv_strategy )
     CV_scores.append(scores.mean())
     
 Missclassification_Error = [1-x for x in CV_scores]
 print(""the optimal k is "",format(Missclassification_Error.index(min(Missclassification_Error))))
 k_Optimal=Missclassification_Error.index(min(Missclassification_Error))
     
 from matplotlib.legend_handler import HandlerLine2D
 plt.plot( Missclassification_Error)
 plt.xlabel(""number of neighbors K"")
 plt.ylabel(""Missclassification Error"")
 plt.show()"
"ASSIGN = ListedColormap([' ASSIGN = ListedColormap([' ASSIGN=pd.DataFrame(feature_2).loc[:][0] ASSIGN=pd.DataFrame(feature_2).loc[:][1] ASSIGN='distance' ASSIGN = .02 ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, ASSIGN=ASSIGN)) ]) ASSIGN.fit(feature_2, y_equidistant) feature1_min, feature1_max = ASSIGN.min() - 1, ASSIGN.max() + 1 feature2_min, feature2_max = ASSIGN.min() - 1, ASSIGN.max() + 1 ASSIGN = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h)) ASSIGN = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()]) ASSIGN = Z.reshape(xx.shape)",1,not_existent,"# Create color maps
 cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])
 cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])
 
 feature1=pd.DataFrame(feature_2).loc[:][0]
 feature2=pd.DataFrame(feature_2).loc[:][1]
 
 weights='distance'
 h = .02  # step size in the mesh
 # we create an instance of Neighbours Classifier and fit the data.
 # Create a k-NN pipeline
 knn_pipe = Pipeline([
     ('scaler', StandardScaler()),
     ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))
 ])
 # Fit estimator
 knn_pipe.fit(feature_2, y_equidistant)
 # Plot the decision boundary. For that, we will assign a color to each
 # point in the mesh [x_min, x_max]x[y_min, y_max].
 feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1
 feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1
 xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))
 Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])
 Zo = Z.reshape(xx.shape)"
"plt.figure(figsize=(18, 10)) plt.pcolormesh(xx, yy, Zo, cmap=cmap_light) Label1=pd.DataFrame([1,2,3,4,5]) plt.scatter(feature1, feature2, c=y_equidistant, cmap=cmap_bold) ASSIGN = plt.colorbar() ASSIGN.ax.set_yticklabels([Level1_equidistant,Level2_equidistant,Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant]) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.xlabel(""feature 1"") plt.ylabel(""feature 2"") plt.title(""6-Class classification,  equidistant bins - total infected log10 (k = %i, weights = '%s')"" % (k_Optimal, weights)) plt.show()",0,display_data,"# Put the result into a color plot
 plt.figure(figsize=(18, 10))
 plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)
 #Label1=pd.DataFrame(list(class_index.items()))
 Label1=pd.DataFrame([1,2,3,4,5])
 # Plot also the training points
 plt.scatter(feature1, feature2, c=y_equidistant, cmap=cmap_bold)
 cbar = plt.colorbar() 
 cbar.ax.set_yticklabels([Level1_equidistant,Level2_equidistant,Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant])
 plt.xlim(xx.min(), xx.max())
 plt.ylim(yy.min(), yy.max())
 plt.xlabel(""feature 1"")
 plt.ylabel(""feature 2"")
 plt.title(""6-Class classification,   equidistant bins -  total infected log10 (k = %i, weights = '%s')"" % (k_Optimal, weights))
 
 plt.show()"
"CHECKPOINT ASSIGN = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy) df_resultat.at[df_resultat.index==6,""method""]=""knn equidistant bins"" df_resultat.at[df_resultat.index==6,""Cross-validation""]=np.mean(ASSIGN['test_score']) print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant))) print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(ASSIGN['test_score'])))",1,stream,"knn_scores = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy)
 df_resultat.at[df_resultat.index==6,""method""]=""knn equidistant bins""
 df_resultat.at[df_resultat.index==6,""Cross-validation""]=np.mean(knn_scores['test_score'])
 print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant)))
 print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score'])))
"
"SETUP CHECKPOINT ASSIGN = list(range(1,25)) ASSIGN = [] for n_neighborss in ASSIGN: ASSIGN='distance' ASSIGN=pd.DataFrame(feature_2).loc[:][0] ASSIGN=pd.DataFrame(feature_2).loc[:][1] ASSIGN = .02 ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, ASSIGN=ASSIGN)) ]) ASSIGN.fit(feature_2, y_balanced) ASSIGN = knn_pipe.predict(feature_2) ASSIGN = cross_val_score(knn_pipe,feature_2,y_balanced,cv=cv_strategy ) ASSIGN.append(ASSIGN.mean()) Missclassification_Error = [1-x for x in ASSIGN] print(,format(Missclassification_Error.index(min(Missclassification_Error)))) ASSIGN=Missclassification_Error.index(min(Missclassification_Error)) plt.plot( Missclassification_Error) plt.xlabel(""number of ASSIGN K"") plt.ylabel(""Missclassification Error"") plt.show()",0,stream,"neighbors = list(range(1,25))
 CV_scores = []
 
 for n_neighborss in neighbors:
     weights='distance'
     feature1=pd.DataFrame(feature_2).loc[:][0]
     feature2=pd.DataFrame(feature_2).loc[:][1]
     h = .02  # step size in the mesh
     # we create an instance of Neighbours Classifier and fit the data.
     # Create a k-NN pipeline
     knn_pipe = Pipeline([
         ('scaler', StandardScaler()),
         ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights))
     ])
     # Fit estimator
     knn_pipe.fit(feature_2, y_balanced)
     #
     Z = knn_pipe.predict(feature_2)
     scores = cross_val_score(knn_pipe,feature_2,y_balanced,cv=cv_strategy )
     CV_scores.append(scores.mean())
     
 Missclassification_Error = [1-x for x in CV_scores]
 print(""the optimal k is "",format(Missclassification_Error.index(min(Missclassification_Error))))
 k_Optimal=Missclassification_Error.index(min(Missclassification_Error))
     
 from matplotlib.legend_handler import HandlerLine2D
 plt.plot( Missclassification_Error)
 plt.xlabel(""number of neighbors K"")
 plt.ylabel(""Missclassification Error"")
 plt.show()"
"ASSIGN = ListedColormap([' ASSIGN = ListedColormap([' ASSIGN=pd.DataFrame(feature_2).loc[:][0] ASSIGN=pd.DataFrame(feature_2).loc[:][1] ASSIGN='distance' ASSIGN = .02 ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, ASSIGN=ASSIGN)) ]) ASSIGN.fit(feature_2, y_balanced) feature1_min, feature1_max = ASSIGN.min() - 1, ASSIGN.max() + 1 feature2_min, feature2_max = ASSIGN.min() - 1, ASSIGN.max() + 1 ASSIGN = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h)) ASSIGN = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()]) ASSIGN = Z.reshape(xx.shape)",1,not_existent,"# Create color maps
 cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])
 cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])
 
 feature1=pd.DataFrame(feature_2).loc[:][0]
 feature2=pd.DataFrame(feature_2).loc[:][1]
 
 weights='distance'
 h = .02  # step size in the mesh
 # we create an instance of Neighbours Classifier and fit the data.
 # Create a k-NN pipeline
 knn_pipe = Pipeline([
     ('scaler', StandardScaler()),
     ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))
 ])
 # Fit estimator
 knn_pipe.fit(feature_2, y_balanced)
 # Plot the decision boundary. For that, we will assign a color to each
 # point in the mesh [x_min, x_max]x[y_min, y_max].
 feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1
 feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1
 xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))
 Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])
 Zo = Z.reshape(xx.shape)"
"CHECKPOINT ASSIGN = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy) df_resultat.at[df_resultat.index==7,""method""]=""knn balanced bins"" df_resultat.at[df_resultat.index==7,""Cross-validation""]=np.mean(ASSIGN['test_score']) print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced))) print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(ASSIGN['test_score'])))",1,stream,"
 knn_scores = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy)
 df_resultat.at[df_resultat.index==7,""method""]=""knn balanced bins""
 df_resultat.at[df_resultat.index==7,""Cross-validation""]=np.mean(knn_scores['test_score'])
 print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced)))
 print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score'])))
"
CHECKPOINT codiv_time_confirmed,0,execute_result,codiv_time_confirmed
"CHECKPOINT ASSIGN=codiv_time_confirmed.groupby('Countrypath').sum().reset_index() ASSIGN=codiv_time_confirmed_grouped.agg(['sum']) SLICE=""total_confirmed"" ASSIGN=codiv_time_confirmed_grouped.append(dfsum) ASSIGN=codiv_time_recovered.groupby('Countrypath').sum().reset_index() ASSIGN=codiv_time_recovered_grouped.agg(['sum']) SLICE=""total_recovered"" ASSIGN=codiv_time_recovered_grouped.append(dfsum) ASSIGN=codiv_time_deaths.groupby('Countrypath').sum().reset_index() ASSIGN=codiv_time_deaths_grouped.agg(['sum']) SLICE=""total_deaths"" ASSIGN=codiv_time_deaths_grouped.append(dfsum) codiv_time_deaths_grouped_sum",1,execute_result,"codiv_time_confirmed_grouped=codiv_time_confirmed.groupby('Country/Region').sum().reset_index()
 dfsum=codiv_time_confirmed_grouped.agg(['sum'])
 dfsum['Country/Region']=""total_confirmed""
 codiv_time_confirmed_grouped_sum=codiv_time_confirmed_grouped.append(dfsum)
 #
 codiv_time_recovered_grouped=codiv_time_recovered.groupby('Country/Region').sum().reset_index()
 dfsum=codiv_time_recovered_grouped.agg(['sum'])
 dfsum['Country/Region']=""total_recovered""
 codiv_time_recovered_grouped_sum=codiv_time_recovered_grouped.append(dfsum)
 #
 codiv_time_deaths_grouped=codiv_time_deaths.groupby('Country/Region').sum().reset_index()
 dfsum=codiv_time_deaths_grouped.agg(['sum'])
 dfsum['Country/Region']=""total_deaths""
 codiv_time_deaths_grouped_sum=codiv_time_deaths_grouped.append(dfsum)
 codiv_time_deaths_grouped_sum"
ASSIGN=1 for countryInd in codiv_country_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_Restrictions['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_without_Restrictions_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_Restrictions['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_without_Restrictions_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_Restrictions['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_without_Restrictions_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd]),0,not_existent,"#Recovered
 Start=1
 for countryInd in codiv_country_qurantine['Country']:
     if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:
         if Start==1:
             codiv_country_qurantine_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]
             Start=0
         else:
             codiv_country_qurantine_recovered=codiv_country_qurantine_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd])
 #
 Start=1
 for countryInd in codiv_country_Restrictions['Country']:
     if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:
         if Start==1:
             codiv_country_Restrictions_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]
             Start=0
         else:
             codiv_country_Restrictions_recovered=codiv_country_Restrictions_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd])
 #
 Start=1
 for countryInd in codiv_country_without_Restrictions_qurantine['Country']:
     if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:
         if Start==1:
             codiv_country_without_Restrictions_qurantine_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]
             Start=0
         else:
             codiv_country_without_Restrictions_qurantine_recovered=codiv_country_without_Restrictions_qurantine_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]) 
 #deaths   
 Start=1
 for countryInd in codiv_country_qurantine['Country']:
     if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:
         if Start==1:
             codiv_country_qurantine_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]
             Start=0
         else:
             codiv_country_qurantine_deaths=codiv_country_qurantine_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd])
 #
 Start=1
 for countryInd in codiv_country_Restrictions['Country']:
     if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:
         if Start==1:
             codiv_country_Restrictions_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]
             Start=0
         else:
             codiv_country_Restrictions_deaths=codiv_country_Restrictions_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd])
 #
 Start=1
 for countryInd in codiv_country_without_Restrictions_qurantine['Country']:
     if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:
         if Start==1:
             codiv_country_without_Restrictions_qurantine_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]
             Start=0
         else:
             codiv_country_without_Restrictions_qurantine_deaths=codiv_country_without_Restrictions_qurantine_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]) 
 #confirmed
 
 Start=1
 for countryInd in codiv_country_qurantine['Country']:
     if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:
         if Start==1:
             codiv_country_qurantine_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]
             Start=0
         else:
             codiv_country_qurantine_confirmed=codiv_country_qurantine_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd])
 
 Start=1
 for countryInd in codiv_country_Restrictions['Country']:
     if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:
         if Start==1:
             codiv_country_Restrictions_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]
             Start=0
         else:
             codiv_country_Restrictions_confirmed=codiv_country_Restrictions_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd])
 
 #codiv_country_Restrictions_confirmed=codiv_country_Restrictions_confirmed.set_index('Country/Region')
 #
 Start=1
 for countryInd in codiv_country_without_Restrictions_qurantine['Country']:
     if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:
         if Start==1:
             codiv_country_without_Restrictions_qurantine_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]
             Start=0
         else:
             codiv_country_without_Restrictions_qurantine_confirmed=codiv_country_without_Restrictions_qurantine_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]) 
 
"
ASSIGN= codiv_country_without_Restrictions_qurantine_confirmed.set_index('Countrypath')-codiv_country_without_Restrictions_qurantine_deaths.set_index('Countrypath')-codiv_country_without_Restrictions_qurantine_recovered.set_index('Countrypath') ASSIGN= codiv_country_qurantine_confirmed.set_index('Countrypath')-codiv_country_qurantine_deaths.set_index('Countrypath')-codiv_country_qurantine_recovered.set_index('Countrypath') ASSIGN= codiv_country_Restrictions_confirmed.set_index('Countrypath')- codiv_country_Restrictions_deaths.set_index('Countrypath')-codiv_country_Restrictions_recovered.set_index('Countrypath'),1,not_existent,"codiv_country_without_Restrictions_qurantine_active= codiv_country_without_Restrictions_qurantine_confirmed.set_index('Country/Region')-codiv_country_without_Restrictions_qurantine_deaths.set_index('Country/Region')-codiv_country_without_Restrictions_qurantine_recovered.set_index('Country/Region')
 codiv_country_qurantine_active= codiv_country_qurantine_confirmed.set_index('Country/Region')-codiv_country_qurantine_deaths.set_index('Country/Region')-codiv_country_qurantine_recovered.set_index('Country/Region')
 codiv_country_Restrictions_active= codiv_country_Restrictions_confirmed.set_index('Country/Region')- codiv_country_Restrictions_deaths.set_index('Country/Region')-codiv_country_Restrictions_recovered.set_index('Country/Region')"
"SETUP AutoMinorLocator) ASSIGN= codiv_country_qurantine_confirmed.shape fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40)) ASSIGN = pd.Series( np.random.randn(col-1), ASSIGN=pd.date_range('20path', periods=col-1)) ASSIGN = pd.DataFrame((np.log1p(codiv_country_qurantine_confirmed.set_index('Countrypath').T).to_numpy()), ASSIGN=ts.ASSIGN, ASSIGN=list(codiv_country_qurantine_confirmed.set_index('Countrypath').index)) ASSIGN = pd.DataFrame((np.log1p(codiv_country_Restrictions_confirmed.set_index('Countrypath').T).to_numpy()), ASSIGN=ts.ASSIGN, ASSIGN=list(codiv_country_Restrictions_confirmed.set_index('Countrypath').index)) ASSIGN = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_confirmed.set_index('Countrypath').T).to_numpy()), ASSIGN=ts.ASSIGN, ASSIGN=list(codiv_country_without_Restrictions_qurantine_confirmed.set_index('Countrypath').index)) ax0.plot(ASSIGN.ASSIGN,ASSIGN) ax1.plot(ASSIGN.ASSIGN, ASSIGN) ax2.plot(ASSIGN.ASSIGN, ASSIGN) ax0.set_title(""quarantine_confirmed"") ax1.set_title(""Restrictions_confirmed"") ax2.set_title(""without_Restrictions_quarantine_confirmed"") ax0.legend(codiv_country_qurantine_confirmed['Countrypath'].tolist(),loc='upper left') ax1.legend(codiv_country_Restrictions_confirmed['Countrypath'].tolist(),loc='upper left') ax2.legend(codiv_country_without_Restrictions_qurantine_confirmed['Countrypath'].tolist(),loc='upper left') ax0.set_ylabel(""quarantine_confirmed"") ax1.set_ylabel(""Restrictions_confirmed"") ax2.set_ylabel(""without_Restrictions_quarantine_confirmed"") ax0.grid(True) ax0.xaxis.set_minor_locator(AutoMinorLocator()) ax1.grid(True) ax1.xaxis.set_minor_locator(AutoMinorLocator()) ax2.grid(True) ax2.xaxis.set_minor_locator(AutoMinorLocator()) ax0.tick_params(axis=""x"", rotation=45) ax1.tick_params(axis=""x"", rotation=45) ax2.tick_params(axis=""x"", rotation=45) for countryindex in codiv_country_qurantine_confirmed['Countrypath']: if not codiv_country[codiv_country['Country']==countryindex].empty : if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=""2000-01-01"" and countryindex!=""New Zealand"": QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0] QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), ""%mpath%dpath%Y"") ASSIGN=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0] ax0.annotate('Quarantine', (QuarantineTimeD, ASSIGN), ASSIGN=(0.2, 0.95), textcoords='axes fraction', ASSIGN=dict(facecolor='red', shrink=0.001), ASSIGN=16, ASSIGN='right', verticalalignment='top') for countryindex in codiv_country_Restrictions_confirmed['Countrypath']: if not codiv_country[codiv_country['Country']==countryindex].empty : if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=""2000-01-01"": RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0] RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), ""%mpath%dpath%Y"") ASSIGN=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0] ax1.annotate('Restrictions', (RestrictionsTimeD, ASSIGN), ASSIGN=(0.2, 0.95), textcoords='axes fraction', ASSIGN=dict(facecolor='red', shrink=0.001), ASSIGN=16, ASSIGN='right', verticalalignment='top')",0,display_data,"import datetime
 from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,
                                AutoMinorLocator)
 row, col= codiv_country_qurantine_confirmed.shape
 fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))
 ts = pd.Series(
     np.random.randn(col-1),
     index=pd.date_range('20/1/2020', periods=col-1))
 df_qua_conf = pd.DataFrame((np.log1p(codiv_country_qurantine_confirmed.set_index('Country/Region').T).to_numpy()),
     index=ts.index,
     columns=list(codiv_country_qurantine_confirmed.set_index('Country/Region').index))
 df_res_conf = pd.DataFrame((np.log1p(codiv_country_Restrictions_confirmed.set_index('Country/Region').T).to_numpy()),
     index=ts.index,
     columns=list(codiv_country_Restrictions_confirmed.set_index('Country/Region').index))
 df_whi_conf = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_confirmed.set_index('Country/Region').T).to_numpy()),
     index=ts.index,
     columns=list(codiv_country_without_Restrictions_qurantine_confirmed.set_index('Country/Region').index))
 
 ax0.plot(df_qua_conf.index,df_qua_conf)
 ax1.plot(df_res_conf.index, df_res_conf)
 ax2.plot(df_whi_conf.index, df_whi_conf)
 ax0.set_title(""quarantine_confirmed"")
 ax1.set_title(""Restrictions_confirmed"")
 ax2.set_title(""without_Restrictions_quarantine_confirmed"")
 ax0.legend(codiv_country_qurantine_confirmed['Country/Region'].tolist(),loc='upper left')
 ax1.legend(codiv_country_Restrictions_confirmed['Country/Region'].tolist(),loc='upper left')
 ax2.legend(codiv_country_without_Restrictions_qurantine_confirmed['Country/Region'].tolist(),loc='upper left')
 ax0.set_ylabel(""quarantine_confirmed"")
 ax1.set_ylabel(""Restrictions_confirmed"")
 ax2.set_ylabel(""without_Restrictions_quarantine_confirmed"")
 
 ax0.grid(True)
 ax0.xaxis.set_minor_locator(AutoMinorLocator())
 ax1.grid(True)
 ax1.xaxis.set_minor_locator(AutoMinorLocator())
 ax2.grid(True)
 ax2.xaxis.set_minor_locator(AutoMinorLocator())
 ax0.tick_params(axis=""x"", rotation=45)
 ax1.tick_params(axis=""x"", rotation=45)
 ax2.tick_params(axis=""x"", rotation=45)
 ####
 for countryindex in codiv_country_qurantine_confirmed['Country/Region']:
     if not codiv_country[codiv_country['Country']==countryindex].empty :
         if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=""2000-01-01"" and countryindex!=""New Zealand"":
             QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]
             QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), ""%m/%d/%Y"")
             Centervalue=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]
             ax0.annotate('Quarantine', (QuarantineTimeD, Centervalue),
                 xytext=(0.2, 0.95), textcoords='axes fraction',
                 arrowprops=dict(facecolor='red', shrink=0.001),
                 fontsize=16,
                 horizontalalignment='right', verticalalignment='top')
 ####
 for countryindex in codiv_country_Restrictions_confirmed['Country/Region']:
     if not codiv_country[codiv_country['Country']==countryindex].empty :
         if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=""2000-01-01"":
             RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]
             RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), ""%m/%d/%Y"")
             Centervalue=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]
             ax1.annotate('Restrictions', (RestrictionsTimeD, Centervalue),
                 xytext=(0.2, 0.95), textcoords='axes fraction',
                 arrowprops=dict(facecolor='red', shrink=0.001),
                 fontsize=16,
                 horizontalalignment='right', verticalalignment='top')"
"SETUP AutoMinorLocator) ASSIGN= codiv_country_qurantine_deaths.shape fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40)) ASSIGN = pd.Series( np.random.randn(col-1), ASSIGN=pd.date_range('20path', periods=col-1)) ASSIGN = pd.DataFrame((np.log1p(codiv_country_qurantine_deaths.set_index('Countrypath').T).to_numpy()), ASSIGN=ts.ASSIGN, ASSIGN=list(codiv_country_qurantine_deaths.set_index('Countrypath').index)) ASSIGN = pd.DataFrame((np.log1p(codiv_country_Restrictions_deaths.set_index('Countrypath').T).to_numpy()), ASSIGN=ts.ASSIGN, ASSIGN=list(codiv_country_Restrictions_deaths.set_index('Countrypath').index)) ASSIGN = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_deaths.set_index('Countrypath').T).to_numpy()), ASSIGN=ts.ASSIGN, ASSIGN=list(codiv_country_without_Restrictions_qurantine_deaths.set_index('Countrypath').index)) ax0.plot(ASSIGN.ASSIGN,ASSIGN) ax1.plot(ASSIGN.ASSIGN, ASSIGN) ax2.plot(ASSIGN.ASSIGN, ASSIGN) ax0.set_title(""quarantine_deaths"") ax1.set_title(""Restrictions_deaths"") ax2.set_title(""without_Restrictions_quarantine_deaths"") ax0.legend(codiv_country_qurantine_deaths['Countrypath'].tolist(),loc='upper left') ax1.legend(codiv_country_Restrictions_deaths['Countrypath'].tolist(),loc='upper left') ax2.legend(codiv_country_without_Restrictions_qurantine_deaths['Countrypath'].tolist(),loc='upper left') ax0.set_ylabel(""quarantine_deaths"") ax1.set_ylabel(""Restrictions_deaths"") ax2.set_ylabel(""without_Restrictions_quarantine_deaths"") ax0.grid(True) ax0.xaxis.set_minor_locator(AutoMinorLocator()) ax1.grid(True) ax1.xaxis.set_minor_locator(AutoMinorLocator()) ax2.grid(True) ax2.xaxis.set_minor_locator(AutoMinorLocator()) ax0.tick_params(axis=""x"", rotation=45) ax1.tick_params(axis=""x"", rotation=45) ax2.tick_params(axis=""x"", rotation=45) for countryindex in codiv_country_qurantine_deaths['Countrypath']: if not codiv_country[codiv_country['Country']==countryindex].empty : if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=""2000-01-01"" and countryindex!=""New Zealand"": QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0] QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), ""%mpath%dpath%Y"") ASSIGN=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0] ax0.annotate('Quarantine', (QuarantineTimeD, ASSIGN), ASSIGN=(0.2, 0.95), textcoords='axes fraction', ASSIGN=dict(facecolor='red', shrink=0.001), ASSIGN=16, ASSIGN='right', verticalalignment='top') for countryindex in codiv_country_Restrictions_deaths['Countrypath']: if not codiv_country[codiv_country['Country']==countryindex].empty : if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=""2000-01-01"": RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0] RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), ""%mpath%dpath%Y"") ASSIGN=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0] ax1.annotate('Restrictions', (RestrictionsTimeD, ASSIGN), ASSIGN=(0.2, 0.95), textcoords='axes fraction', ASSIGN=dict(facecolor='red', shrink=0.001), ASSIGN=16, ASSIGN='right', verticalalignment='top')",0,display_data,"import datetime
 from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,
                                AutoMinorLocator)
 row, col= codiv_country_qurantine_deaths.shape
 fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))
 ts = pd.Series(
     np.random.randn(col-1),
     index=pd.date_range('20/1/2020', periods=col-1))
 df_qua_conf = pd.DataFrame((np.log1p(codiv_country_qurantine_deaths.set_index('Country/Region').T).to_numpy()),
     index=ts.index,
     columns=list(codiv_country_qurantine_deaths.set_index('Country/Region').index))
 df_res_conf = pd.DataFrame((np.log1p(codiv_country_Restrictions_deaths.set_index('Country/Region').T).to_numpy()),
     index=ts.index,
     columns=list(codiv_country_Restrictions_deaths.set_index('Country/Region').index))
 df_whi_conf = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_deaths.set_index('Country/Region').T).to_numpy()),
     index=ts.index,
     columns=list(codiv_country_without_Restrictions_qurantine_deaths.set_index('Country/Region').index))
 
 ax0.plot(df_qua_conf.index,df_qua_conf)
 ax1.plot(df_res_conf.index, df_res_conf)
 ax2.plot(df_whi_conf.index, df_whi_conf)
 ax0.set_title(""quarantine_deaths"")
 ax1.set_title(""Restrictions_deaths"")
 ax2.set_title(""without_Restrictions_quarantine_deaths"")
 ax0.legend(codiv_country_qurantine_deaths['Country/Region'].tolist(),loc='upper left')
 ax1.legend(codiv_country_Restrictions_deaths['Country/Region'].tolist(),loc='upper left')
 ax2.legend(codiv_country_without_Restrictions_qurantine_deaths['Country/Region'].tolist(),loc='upper left')
 ax0.set_ylabel(""quarantine_deaths"")
 ax1.set_ylabel(""Restrictions_deaths"")
 ax2.set_ylabel(""without_Restrictions_quarantine_deaths"")
 
 ax0.grid(True)
 ax0.xaxis.set_minor_locator(AutoMinorLocator())
 ax1.grid(True)
 ax1.xaxis.set_minor_locator(AutoMinorLocator())
 ax2.grid(True)
 ax2.xaxis.set_minor_locator(AutoMinorLocator())
 ax0.tick_params(axis=""x"", rotation=45)
 ax1.tick_params(axis=""x"", rotation=45)
 ax2.tick_params(axis=""x"", rotation=45)
 ####
 for countryindex in codiv_country_qurantine_deaths['Country/Region']:
     if not codiv_country[codiv_country['Country']==countryindex].empty :
         if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=""2000-01-01"" and countryindex!=""New Zealand"":
             QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]
             QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), ""%m/%d/%Y"")
             Centervalue=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]
             ax0.annotate('Quarantine', (QuarantineTimeD, Centervalue),
                 xytext=(0.2, 0.95), textcoords='axes fraction',
                 arrowprops=dict(facecolor='red', shrink=0.001),
                 fontsize=16,
                 horizontalalignment='right', verticalalignment='top')
 ####
 for countryindex in codiv_country_Restrictions_deaths['Country/Region']:
     if not codiv_country[codiv_country['Country']==countryindex].empty :
         if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=""2000-01-01"":
             RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]
             RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), ""%m/%d/%Y"")
             Centervalue=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]
             ax1.annotate('Restrictions', (RestrictionsTimeD, Centervalue),
                 xytext=(0.2, 0.95), textcoords='axes fraction',
                 arrowprops=dict(facecolor='red', shrink=0.001),
                 fontsize=16,
                 horizontalalignment='right', verticalalignment='top')"
"SETUP AutoMinorLocator) ASSIGN= codiv_country_qurantine_active.shape fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40)) ASSIGN = pd.Series( np.random.randn(col), ASSIGN=pd.date_range('20path', periods=col)) ASSIGN = pd.DataFrame((np.log1p(codiv_country_qurantine_active.T).to_numpy()), ASSIGN=ts.ASSIGN, ASSIGN=list(codiv_country_qurantine_active.index)) ASSIGN = pd.DataFrame((np.log1p(codiv_country_Restrictions_active.T).to_numpy()), ASSIGN=ts.ASSIGN, ASSIGN=list(codiv_country_Restrictions_active.index)) ASSIGN = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_active.T).to_numpy()), ASSIGN=ts.ASSIGN, ASSIGN=list(codiv_country_without_Restrictions_qurantine_active.index)) ax0.plot(ASSIGN.ASSIGN,ASSIGN) ax1.plot(ASSIGN.ASSIGN, ASSIGN) ax2.plot(ASSIGN.ASSIGN, ASSIGN) ax0.set_title(""quarantine_active"") ax1.set_title(""Restrictions_active"") ax2.set_title(""without_Restrictions_quarantine_active"") ax0.legend(codiv_country_qurantine_deaths['Countrypath'].tolist(),loc='upper left') ax1.legend(codiv_country_Restrictions_deaths['Countrypath'].tolist(),loc='upper left') ax2.legend(codiv_country_without_Restrictions_qurantine_deaths['Countrypath'].tolist(),loc='upper left') ax0.set_ylabel(""quarantine_active"") ax1.set_ylabel(""Restrictions_active"") ax2.set_ylabel(""without_Restrictions_qurantine_active"") ax0.grid(True) ax0.xaxis.set_minor_locator(AutoMinorLocator()) ax1.grid(True) ax1.xaxis.set_minor_locator(AutoMinorLocator()) ax2.grid(True) ax2.xaxis.set_minor_locator(AutoMinorLocator()) ax0.tick_params(axis=""x"", rotation=45) ax1.tick_params(axis=""x"", rotation=45) ax2.tick_params(axis=""x"", rotation=45) for countryindex in codiv_country_qurantine_deaths['Countrypath']: if not codiv_country[codiv_country['Country']==countryindex].empty : if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=""2000-01-01"" and countryindex!=""New Zealand"": QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0] QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), ""%mpath%dpath%Y"") ASSIGN=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0] ax0.annotate('Quarantine', (QuarantineTimeD, ASSIGN), ASSIGN=(0.2, 0.95), textcoords='axes fraction', ASSIGN=dict(facecolor='red', shrink=0.001), ASSIGN=16, ASSIGN='right', verticalalignment='top') for countryindex in codiv_country_Restrictions_deaths['Countrypath']: if not codiv_country[codiv_country['Country']==countryindex].empty : if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=""2000-01-01"": RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0] RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), ""%mpath%dpath%Y"") ASSIGN=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0] ax1.annotate('Restrictions', (RestrictionsTimeD, ASSIGN), ASSIGN=(0.2, 0.95), textcoords='axes fraction', ASSIGN=dict(facecolor='red', shrink=0.001), ASSIGN=16, ASSIGN='right', verticalalignment='top')",0,display_data,"import datetime
 from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,
                                AutoMinorLocator)
 row, col= codiv_country_qurantine_active.shape
 fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))
 ts = pd.Series(
     np.random.randn(col),
     index=pd.date_range('20/1/2020', periods=col))
 df_qua_conf = pd.DataFrame((np.log1p(codiv_country_qurantine_active.T).to_numpy()),
     index=ts.index,
     columns=list(codiv_country_qurantine_active.index))
 df_res_conf = pd.DataFrame((np.log1p(codiv_country_Restrictions_active.T).to_numpy()),
     index=ts.index,
     columns=list(codiv_country_Restrictions_active.index))
 df_whi_conf = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_active.T).to_numpy()),
     index=ts.index,
     columns=list(codiv_country_without_Restrictions_qurantine_active.index))
 
 ax0.plot(df_qua_conf.index,df_qua_conf)
 ax1.plot(df_res_conf.index, df_res_conf)
 ax2.plot(df_whi_conf.index, df_whi_conf)
 ax0.set_title(""quarantine_active"")
 ax1.set_title(""Restrictions_active"")
 ax2.set_title(""without_Restrictions_quarantine_active"")
 ax0.legend(codiv_country_qurantine_deaths['Country/Region'].tolist(),loc='upper left')
 ax1.legend(codiv_country_Restrictions_deaths['Country/Region'].tolist(),loc='upper left')
 ax2.legend(codiv_country_without_Restrictions_qurantine_deaths['Country/Region'].tolist(),loc='upper left')
 ax0.set_ylabel(""quarantine_active"")
 ax1.set_ylabel(""Restrictions_active"")
 ax2.set_ylabel(""without_Restrictions_qurantine_active"")
 
 ax0.grid(True)
 ax0.xaxis.set_minor_locator(AutoMinorLocator())
 ax1.grid(True)
 ax1.xaxis.set_minor_locator(AutoMinorLocator())
 ax2.grid(True)
 ax2.xaxis.set_minor_locator(AutoMinorLocator())
 ax0.tick_params(axis=""x"", rotation=45)
 ax1.tick_params(axis=""x"", rotation=45)
 ax2.tick_params(axis=""x"", rotation=45)
 ####
 for countryindex in codiv_country_qurantine_deaths['Country/Region']:
     if not codiv_country[codiv_country['Country']==countryindex].empty :
         if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=""2000-01-01"" and countryindex!=""New Zealand"":
             QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]
             QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), ""%m/%d/%Y"")
             Centervalue=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]
             ax0.annotate('Quarantine', (QuarantineTimeD, Centervalue),
                 xytext=(0.2, 0.95), textcoords='axes fraction',
                 arrowprops=dict(facecolor='red', shrink=0.001),
                 fontsize=16,
                 horizontalalignment='right', verticalalignment='top')
 ####
 for countryindex in codiv_country_Restrictions_deaths['Country/Region']:
     if not codiv_country[codiv_country['Country']==countryindex].empty :
         if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=""2000-01-01"":
             RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]
             RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), ""%m/%d/%Y"")
             Centervalue=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]
             ax1.annotate('Restrictions', (RestrictionsTimeD, Centervalue),
                 xytext=(0.2, 0.95), textcoords='axes fraction',
                 arrowprops=dict(facecolor='red', shrink=0.001),
                 fontsize=16,
                 horizontalalignment='right', verticalalignment='top')"
"SETUP Image(filename=os.path.join('.path', 'shift.JPG'))",0,execute_result,"from IPython.display import Image
 Image(filename=os.path.join('.//', 'shift.JPG'))"
"CHECKPOINT StartDate=codiv_country_Restrictions_active.T.index[0] print(,StartDate) ASSIGN = [""date""] ASSIGN = pd.DataFrame(columns = column_names) for Dateindex in range(20): ASSIGN = pd.DataFrame ([[str(pd.Timestamp(StartDate,unit=""D"")+ pd.Timedelta(days=Dateindex))]], columns = [""date""]) ASSIGN=ASSIGN.append(df_date1, ignore_index = True)",1,stream,"# evalue the stadt date of the stats
 StartDate=codiv_country_Restrictions_active.T.index[0]
 print("" Epidemy StartDate = "",StartDate)
 column_names = [""date""]
 dfdate = pd.DataFrame(columns = column_names)
 for Dateindex in range(20):
     df_date1 = pd.DataFrame ([[str(pd.Timestamp(StartDate,unit=""D"")+ pd.Timedelta(days=Dateindex))]], columns = [""date""])
     dfdate=dfdate.append(df_date1, ignore_index = True)
"
"def rmse(y, y_pred): return np.sqrt(np.mean(np.square(y - y_pred)))",0,not_existent,"# Root mean squared error (RMSE)
 def rmse(y, y_pred):
     return np.sqrt(np.mean(np.square(y - y_pred)))"
"SETUP def Polifq(Country,Version=""restrictions"",shift=0,trigger=40): ASSIGN==""restrictions"": ASSIGN=codiv_country_Restrictions_active.T[Country] else: ASSIGN==""quarantine"": ASSIGN=codiv_country_qurantine_active.T[Country] else: ASSIGN=codiv_country_without_Restrictions_qurantine_active.T[Country] StartCountryIndex=0 ShiftIndex=0 ASSIGN=0 ASSIGN=0 ASSIGN=1.0 LargeFactor=1.0 ASSIGN = np.abs(df_codiv - df_codiv.mean()) > (3 * df_codiv.std()) ASSIGN = df_codiv.loc[filter0] ASSIGN = ASSIGN.drop(outliers.index, axis=0) ASSIGN=max(df_codiv) ASSIGN = pd.DataFrame(columns = [Country,""Value"",""time""], dtype='int') for Index in range(0,len(ASSIGN)): ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [Country,""Value"",""time""]) ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True) ASSIGN=df_country[""Value""] ASSIGN=df_country[Country] ASSIGN = np.polyfit(x_tr1_Country, y_tr1_Country, deg=10) ASSIGN=0 ASSIGN==1: ASSIGN=1 ASSIGN=max(codiv_country_qurantine_active.T[""China""]) for Index in range(0,len(ASSIGN)): if ASSIGN[Index]>trigger*(ASSIGN*1.0path) and ASSIGN==1 and shift==1: if ASSIGN>ASSIGN: ASSIGN=ASSIGN*(maxCountry*1.5path) StartCountryIndex=Index ASSIGN=0 if ASSIGN[Index]==ASSIGN: ASSIGN=Index LargeCountry=ASSIGN-StartCountryIndex ASSIGN=codiv_country_qurantine_active.T[""China""] ASSIGN = np.abs(df_codiv_China_ref - df_codiv_China_ref.mean()) > (3 * df_codiv_China_ref.std()) ASSIGN = df_codiv_China_ref.loc[filter0] ASSIGN = df_codiv_China_ref.drop(outliers.index, axis=0) ASSIGN=max(df_codiv_China_ref) ASSIGN=1 for Index in range(0,len(ASSIGN)): if ASSIGN[Index]>60 and ASSIGN==1 and shift==1: StartChinaIndex=Index ASSIGN=0 if ASSIGN[Index]==ASSIGN: ASSIGN=Index ASSIGN=maxCountry*1.0path LargeChina=ASSIGN-StartChinaIndex LargeFactor=LargeCountry*1.0path ShiftIndex=StartCountryIndex-StartChinaIndex ASSIGN=0.02 ASSIGN = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int') for Index in range(0,len(ASSIGN)): ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""]) ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True) ASSIGN=df_country[""Value""] ASSIGN=df_country[""China""] ASSIGN = rmse(y_tr1_China_model,y_tr1_Country) ASSIGN=0 for runNr in range(1,60,1): if ASSIGN<ASSIGN: LargeFactor=LargeFactor+ASSIGN ASSIGN=error1 ASSIGN = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int') for Index in range(0,len(ASSIGN)): ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""]) ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True) ASSIGN=df_country[""Value""] ASSIGN=df_country[""China""] ASSIGN = rmse(y_tr1_China_model,y_tr1_Country) if ASSIGN<ASSIGN: LargeFactor=LargeFactor+ASSIGN ASSIGN=error1 ASSIGN = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int') for Index in range(0,len(ASSIGN)): ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""]) ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True) ASSIGN=df_country[""Value""] ASSIGN=df_country[""China""] ASSIGN = rmse(y_tr1_China_model,y_tr1_Country) if ASSIGN<ASSIGN: ASSIGN=ASSIGN+step ASSIGN=error1 ASSIGN = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int') for Index in range(0,len(ASSIGN)): ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""]) ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True) ASSIGN=df_country[""Value""] ASSIGN=df_country[""China""] ASSIGN = rmse(y_tr1_China_model,y_tr1_Country) if ASSIGN<ASSIGN: ASSIGN=ASSIGN+step ASSIGN=error1 ASSIGN = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int') for Index in range(0,len(ASSIGN)): ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""]) ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True) ASSIGN=df_country[""Value""] ASSIGN=df_country[""China""] ASSIGN = rmse(y_tr1_China_model,y_tr1_Country) ASSIGN = np.polyfit(x_tr1_China_model, y_tr1_China_model, deg=10) return coefs_Country_poly10,x_tr1_Country,y_tr1_Country,coefs_predict_poly10,x_tr1_China_model,y_tr1_China_model,StartCountryIndex",1,not_existent,"import scipy.stats as stats
 import datetime
 def Polifq(Country,Version=""restrictions"",shift=0,trigger=40):
     # Version= works for restrictions, quarantin and without_Restrictions_qurantine.
     if Version==""restrictions"":
         df_codiv=codiv_country_Restrictions_active.T[Country]
     else:
         if Version==""quarantine"":
             df_codiv=codiv_country_qurantine_active.T[Country]
         else:
             df_codiv=codiv_country_without_Restrictions_qurantine_active.T[Country]
     #######################################
     ######################## parameter setup
     StartCountryIndex=0
     ShiftIndex=0
     x_tr1_China_model=0
     y_tr1_China_model=0
     highfactor=1.0
     LargeFactor=1.0
     ##############################################################################
     ####################################### fitting the Country ##################
     ######################## filter outlier for the Country
     filter0 = np.abs(df_codiv - df_codiv.mean()) > (3 * df_codiv.std())
     outliers = df_codiv.loc[filter0]
     df_codiv = df_codiv.drop(outliers.index, axis=0)
     ###########
     # max actual value of the country
     maxCountry=max(df_codiv)
     #######################################
     ########### Build the dataframe for the country
     df_country = pd.DataFrame(columns = [Country,""Value"",""time""], dtype='int')
     for Index in range(0,len(df_codiv)):
         df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [Country,""Value"",""time""])
         df_country=df_country.append(df_codiv1, ignore_index = True)
     #######################################
     #################### polyfit 10gr for the country value
     x_tr1_Country=df_country[""Value""]
     y_tr1_Country=df_country[Country]
     #
     # * Polyfit with degree 10
     coefs_Country_poly10 = np.polyfit(x_tr1_Country, y_tr1_Country, deg=10) # Fit to train data
     ##############################################################################
     ##############################################################################
     coefs_predict_poly10=0
     #######################################
     ############### Now shifting the china model to fitt to the country
     if shift==1:
         Start=1
         ####### for the country
         maxChina=max(codiv_country_qurantine_active.T[""China""])
         for Index in range(0,len(df_codiv)):
             #look for the start position of the country epidemy, it should be higher then the trigger value, it will be setup only by start of procedure ( STart=1)
             if df_codiv[Index]>trigger*(maxCountry*1.0/maxChina) and Start==1 and shift==1:
                 
                 if maxCountry>maxChina:
                     trigger=trigger*(maxCountry*1.5/maxChina)
                 StartCountryIndex=Index
                 Start=0
             # register the index of the coutry max position
             if df_codiv[Index]==maxCountry:
                 maxCountryIndex=Index
         # deliver the actual distance between start and maximum for the country
         LargeCountry=maxCountryIndex-StartCountryIndex
         #################################
         #######################################
         ####### for china
         df_codiv_China_ref=codiv_country_qurantine_active.T[""China""]
         #######################################
         ###### outiler
         filter0 = np.abs(df_codiv_China_ref - df_codiv_China_ref.mean()) > (3 * df_codiv_China_ref.std())
         outliers = df_codiv_China_ref.loc[filter0]
         df_codiv = df_codiv_China_ref.drop(outliers.index, axis=0)
         #######################################
         ###### check for the start point by 40 infected
         maxChina=max(df_codiv_China_ref)
         Start=1
         for Index in range(0,len(df_codiv_China_ref)):
             if df_codiv_China_ref[Index]>60 and Start==1 and shift==1:
                 StartChinaIndex=Index
                 Start=0
             if df_codiv_China_ref[Index]==maxChina:
                 maxChinaIndex=Index
         #######################################
         ###### start value for the fitting
         highfactor=maxCountry*1.0/maxChina
         LargeChina=maxChinaIndex-StartChinaIndex
         LargeFactor=LargeCountry*1.0/LargeChina
         ShiftIndex=StartCountryIndex-StartChinaIndex
         ##########OPTIMISATION 
         step=0.02
         ############# the target
         # Predictions with the current a,b values
         ################# generate the function country is china
         df_country = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int')
         for Index in range(0,len(df_codiv)):
             df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""])
             df_country=df_country.append(df_codiv1, ignore_index = True)
         x_tr1_China_model=df_country[""Value""]
         y_tr1_China_model=df_country[""China""]      
         error0 = rmse(y_tr1_China_model,y_tr1_Country)
         error1=0
         for runNr in range(1,60,1):
             if error1<error0:
                 LargeFactor=LargeFactor+step
                 error0=error1
                 ################# generate the function
                 df_country = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int')
                 for Index in range(0,len(df_codiv)):
                     df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""])
                     df_country=df_country.append(df_codiv1, ignore_index = True)
                 x_tr1_China_model=df_country[""Value""]
                 y_tr1_China_model=df_country[""China""] 
                 error1 = rmse(y_tr1_China_model,y_tr1_Country)
             if error1<error0:
                 LargeFactor=LargeFactor+step
                 error0=error1
                 ################# generate the function
                 df_country = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int')
                 for Index in range(0,len(df_codiv)):
                     df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""])
                     df_country=df_country.append(df_codiv1, ignore_index = True)
                 x_tr1_China_model=df_country[""Value""]
                 y_tr1_China_model=df_country[""China""] 
                 error1 = rmse(y_tr1_China_model,y_tr1_Country)
             ##############
             if error1<error0:
                 highfactor=highfactor+step
                 error0=error1
                 ################# generate the function
                 df_country = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int')
                 for Index in range(0,len(df_codiv)):
                     df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""])
                     df_country=df_country.append(df_codiv1, ignore_index = True)
                 x_tr1_China_model=df_country[""Value""]
                 y_tr1_China_model=df_country[""China""] 
                 error1 = rmse(y_tr1_China_model,y_tr1_Country)
             #################
             if error1<error0:
                 highfactor=highfactor+step
                 error0=error1
                 ################# generate the function
                 df_country = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int')
                 for Index in range(0,len(df_codiv)):
                     df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""])
                     df_country=df_country.append(df_codiv1, ignore_index = True)
                 x_tr1_China_model=df_country[""Value""]
                 y_tr1_China_model=df_country[""China""] 
                 error1 = rmse(y_tr1_China_model,y_tr1_Country)
         #after having this fitting, we can fit the modified china data  on a 10grade polynome
         #
         # * Polyfit with degree 10
         coefs_predict_poly10 = np.polyfit(x_tr1_China_model, y_tr1_China_model, deg=10) # Fit to train data
     return coefs_Country_poly10,x_tr1_Country,y_tr1_Country,coefs_predict_poly10,x_tr1_China_model,y_tr1_China_model,StartCountryIndex"
"ASSIGN=codiv_country_Restrictions_active.shape ASSIGN = np.linspace(0, lendata+40, num=lendata+30)",1,not_existent,"# how much data are available for a country
 number,lendata=codiv_country_Restrictions_active.shape
 x_values = np.linspace(0, lendata+40, num=lendata+30)"
"coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(""China"",shift=0,Version=""quarantine"",trigger=70) ASSIGN=max(x_tr_China)-7 ASSIGN = np.polyval(coefs_China, x_values) ASSIGN=int(max(y_tr_China)*1.5)",1,not_existent,"#compute the ymax china
 coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(""China"",shift=0,Version=""quarantine"",trigger=70)
 lendataChina=max(x_tr_China)-7
 y_China = np.polyval(coefs_China, x_values)
 ymaxchina=int(max(y_tr_China)*1.5)"
"StartDate=codiv_country_Restrictions_active.T.index[0] ASSIGN = [""date""] ASSIGN = pd.DataFrame(columns = column_names) for Dateindex in range(0,lendata*20,10): ASSIGN = pd.DataFrame ([[str(pd.Timestamp(StartDate,unit=""D"")+ pd.Timedelta(days=Dateindex))]], columns = [""date""]) ASSIGN=ASSIGN.append(df_date1, ignore_index = True)",1,not_existent,"StartDate=codiv_country_Restrictions_active.T.index[0]
 column_names = [""date""]
 dfdate = pd.DataFrame(columns = column_names)
 for Dateindex in range(0,lendata*20,10):
     df_date1 = pd.DataFrame ([[str(pd.Timestamp(StartDate,unit=""D"")+ pd.Timedelta(days=Dateindex))]], columns = [""date""])
     dfdate=dfdate.append(df_date1, ignore_index = True)"
"for countryindex in codiv_country_qurantine_active.T: ASSIGN==""China"": coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=0,Version=""quarantine"",trigger=70) ASSIGN = np.polyval(coefs_Country, x_values) ASSIGN=0 else: coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=""quarantine"",trigger=170) ASSIGN = np.polyval(coefs_Country, x_values) ASSIGN = np.polyval(coefs_Country_prev, x_values) ASSIGN=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40) if ymaxchina>ASSIGN: ASSIGN=ymaxchina ASSIGN = plt.figure(figsize= (18, 10)) plt.scatter(x_tr_China, y_tr_China, s=10) plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=""blue"") ASSIGN!=""China"": plt.scatter(x_tr_Country, y_tr_Country, s=10) plt.plot(x_values[0:lendataChina], ASSIGN[0:lendataChina], label=countryindex,c=""magenta"") plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=""black"") PredictionName='%s-prediction'%(countryindex,) plt.plot(x_values, ASSIGN, label=PredictionName,c=""brown"",ls='dashed') plt.ylim(-5,ASSIGN) plt.ylabel(""Number"") plt.grid(True) ASSIGN = range(0,len(dfdate),10) plt.xticks(ticks = ASSIGN ,labels = dfdate[""date""], rotation = 75) ASSIGN!=""China"": ASSIGN='%s-prediction - Quarantine'%(countryindex,) plt.title(ASSIGN) else: plt.title(""China"") ASSIGN="""" ASSIGN==""Belgium"": ASSIGN=""Belgium only success to slow the infection. The model wait for the peak to fit better"" ASSIGN==""France"": ASSIGN=""France was relaxing the quarantine? the model need the next days data to fit better. the datas are not clear"" ASSIGN==""Germany"" or countryindex==""Austria"": ASSIGN=""Germany was relaxing the quarantine. but the model fit mostly good"" ASSIGN==""India"" or countryindex==""Argentina"" or countryindex==""Peru"" or countryindex==""Colombia"": ASSIGN="" The model wait for the peak to fit better"" ASSIGN==""Spain"": ASSIGN="" Spain had no clear data, the model need the next days data to fit better"" ASSIGN==""China"": ASSIGN="" China is the reference country, but we see they have to face to some epidemic restart ( imported case)."" plt.text(1, ASSIGN-10000, ASSIGN, fontsize=15) plt.legend() plt.show()",0,display_data,"for countryindex in codiv_country_qurantine_active.T:
 #codiv_country_quarantine_active.T:
     if countryindex==""China"":
         coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=0,Version=""quarantine"",trigger=70)
         y_Country = np.polyval(coefs_Country, x_values)
         ymax=0
     else:
         coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=""quarantine"",trigger=170)
         y_Country = np.polyval(coefs_Country, x_values)
         y_Country_prev = np.polyval(coefs_Country_prev, x_values)
         #adjust the ymax of the graphic
         ymax=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40)
     if ymaxchina>ymax:
         ymax=ymaxchina
     #
     fig = plt.figure(figsize= (18, 10))
     #china reference country
     plt.scatter(x_tr_China, y_tr_China, s=10)
     plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=""blue"")
     #plot the second country
     if countryindex!=""China"":
         plt.scatter(x_tr_Country, y_tr_Country, s=10)
         plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=""magenta"")
         plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=""black"")
         PredictionName='%s-prediction'%(countryindex,)
         plt.plot(x_values, y_Country_prev, label=PredictionName,c=""brown"",ls='dashed')
     plt.ylim(-5,ymax)
     plt.ylabel(""Number"")
     plt.grid(True)
     tickvalues = range(0,len(dfdate),10)
     plt.xticks(ticks = tickvalues ,labels = dfdate[""date""], rotation = 75)
     if countryindex!=""China"":
         Title='%s-prediction - Quarantine'%(countryindex,)
         plt.title(Title)
     else:
         plt.title(""China"")
     Texte=""""
     if countryindex==""Belgium"":
         Texte=""Belgium only success to slow the infection. The model wait for the peak to fit better""
     if countryindex==""France"":
         Texte=""France was relaxing the quarantine? the model need the next days data to fit better. the datas are not clear""      
     if countryindex==""Germany"" or countryindex==""Austria"":
         Texte=""Germany was relaxing the quarantine. but the model fit mostly good""      
     if countryindex==""India"" or countryindex==""Argentina"" or countryindex==""Peru"" or countryindex==""Colombia"":
         Texte="" The model wait for the peak to fit better""       
     if countryindex==""Spain"":
         Texte="" Spain had no clear data, the model need the next days data to fit better""  
     if countryindex==""China"":
         Texte="" China is the reference country, but we see they have to face to some epidemic restart ( imported case).""  
     plt.text(1, ymax-10000, Texte, fontsize=15)
     plt.legend()
     plt.show()"
"ASSIGN=codiv_country_Restrictions_active.shape ASSIGN = np.linspace(0, lendata+40, num=lendata+30) coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(""China"",shift=0,Version=""quarantine"",trigger=70) ASSIGN=max(x_tr_China)-7 ASSIGN = np.polyval(coefs_China, x_values) ASSIGN=int(max(y_tr_China)*1.5)",1,not_existent,"number,lendata=codiv_country_Restrictions_active.shape
 x_values = np.linspace(0, lendata+40, num=lendata+30)
 coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(""China"",shift=0,Version=""quarantine"",trigger=70)
 lendataChina=max(x_tr_China)-7
 y_China = np.polyval(coefs_China, x_values)
 ymaxchina=int(max(y_tr_China)*1.5)"
"for countryindex in codiv_country_Restrictions_active.T: ASSIGN==""Japan"": ASSIGN=4500 else: ASSIGN=250 coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=""restrictions"",ASSIGN=ASSIGN) ASSIGN = np.polyval(coefs_Country, x_values) ASSIGN = np.polyval(coefs_Country_prev, x_values) ASSIGN=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40) if ymaxchina>ASSIGN: ASSIGN=ymaxchina ASSIGN = plt.figure(figsize= (18, 10)) plt.scatter(x_tr_China, y_tr_China, s=10) plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=""blue"") ASSIGN!=""China"": plt.scatter(x_tr_Country, y_tr_Country, s=10) plt.plot(x_values[0:lendataChina], ASSIGN[0:lendataChina], label=countryindex,c=""magenta"") plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=""black"") PredictionName='%s-prediction'%(countryindex,) plt.plot(x_values, ASSIGN, label=PredictionName,c=""brown"",ls='dashed') plt.ylim(-5,ASSIGN) plt.ylabel(""Number"") plt.grid(True) ASSIGN = range(0,len(dfdate),10) plt.xticks(ticks = ASSIGN ,labels = dfdate[""date""], rotation = 75) ASSIGN!=""China"": ASSIGN='%s-prediction - Restrictions'%(countryindex,) plt.title(ASSIGN) else: plt.title(""China"") ASSIGN="""" ASSIGN==""Ireland"": ASSIGN=""Ireland had no clear data, the model need the next days data to fit better. Are the information trustfull?"" ASSIGN==""Japan"": ASSIGN=""Japan tried long time to contain the epidemy, after it gone up. the start threshold should be higher"" ASSIGN==""Canada"" or countryindex==""Netherlands"" or countryindex==""Peru"" or countryindex==""Colombia""or countryindex==""Portugal""or countryindex==""Sweden""or countryindex==""United Kingdom"": ASSIGN="" The model wait for the peak to fit better"" ASSIGN==""Norway"" or countryindex==""Poland"": ASSIGN="" The model wait for the peak to fit better,they take long time to reach it"" plt.text(1, ASSIGN-10000, ASSIGN, fontsize=15) plt.legend() plt.show()",0,display_data,"for countryindex in codiv_country_Restrictions_active.T:
 #codiv_country_quarantine_active.T:
     # China is only in quarantine group
     if countryindex==""Japan"":
         trigger=4500
     else:
         trigger=250
     coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=""restrictions"",trigger=trigger)
     y_Country = np.polyval(coefs_Country, x_values)
     y_Country_prev = np.polyval(coefs_Country_prev, x_values)
     ymax=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40)
     #adjust the ymax limit
     if ymaxchina>ymax:
         ymax=ymaxchina
     #
     fig = plt.figure(figsize= (18, 10))
     plt.scatter(x_tr_China, y_tr_China, s=10)
     plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=""blue"")
     if countryindex!=""China"":
         plt.scatter(x_tr_Country, y_tr_Country, s=10)
         plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=""magenta"")
         plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=""black"")
         PredictionName='%s-prediction'%(countryindex,)
         plt.plot(x_values, y_Country_prev, label=PredictionName,c=""brown"",ls='dashed')
     plt.ylim(-5,ymax)
     plt.ylabel(""Number"")
     plt.grid(True)
     tickvalues = range(0,len(dfdate),10)
     plt.xticks(ticks = tickvalues ,labels = dfdate[""date""], rotation = 75)
     if countryindex!=""China"":
         Title='%s-prediction - Restrictions'%(countryindex,)
         plt.title(Title)
     else:
         plt.title(""China"")
     ##
     Texte=""""
     if countryindex==""Ireland"":
         Texte=""Ireland had no clear data, the model need the next days data to fit better. Are the information trustfull?""
     if countryindex==""Japan"":
         Texte=""Japan tried long time to contain the epidemy, after it gone up. the start threshold should be higher"" 
     if countryindex==""Canada"" or countryindex==""Netherlands"" or countryindex==""Peru"" or countryindex==""Colombia""or countryindex==""Portugal""or countryindex==""Sweden""or countryindex==""United Kingdom"":
         Texte="" The model wait for the peak to fit better""     
     if countryindex==""Norway"" or countryindex==""Poland"":
         Texte="" The model wait for the peak to fit better,they take long time to reach it"" 
     plt.text(1, ymax-10000, Texte, fontsize=15)
     ##
     plt.legend()
     plt.show()"
"for countryindex in codiv_country_without_Restrictions_qurantine_active.T: ASSIGN==""Singapore"" or countryindex==""Qatar"" or countryindex==""Kuwait"": ASSIGN=2000 else: ASSIGN=300 coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=""NoQuaNoRES"",ASSIGN=ASSIGN) ASSIGN = np.polyval(coefs_Country, x_values) ASSIGN = np.polyval(coefs_Country_prev, x_values) ASSIGN==""US"": ASSIGN=1500000 else: ASSIGN==""Brazil"": ASSIGN=180000 else: ASSIGN=60000 ASSIGN = plt.figure(figsize= (18, 10)) plt.scatter(x_tr_China, y_tr_China, s=10) plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=""blue"") ASSIGN!=""China"": plt.scatter(x_tr_Country, y_tr_Country, s=10) plt.plot(x_values[0:lendataChina], ASSIGN[0:lendataChina], label=countryindex,c=""magenta"") plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=""black"") PredictionName='%s-prediction'%(countryindex,) plt.plot(x_values, ASSIGN, label=PredictionName,c=""brown"",ls='dashed') plt.ylim(-5,ASSIGN) plt.ylabel(""Number"") plt.grid(True) ASSIGN = range(0,len(dfdate),10) plt.xticks(ticks = ASSIGN ,labels = dfdate[""date""], rotation = 75) ASSIGN!=""China"": ASSIGN='%s-prediction - No-Restrictions No-quarantine'%(countryindex,) plt.title(ASSIGN) else: plt.title(""China"") ASSIGN="""" ASSIGN==""Armenia"" or countryindex==""Bahrain"" or countryindex==""Bangladesh"" or countryindex==""Belarus""or countryindex==""Brazil""or countryindex==""Chile""or countryindex==""United Kingdom""or countryindex==""US"": ASSIGN="" The model wait for the peak to fit better"" ASSIGN==""Ecuador"" or countryindex==""Indonesia"" or countryindex==""Kuwait"" or countryindex==""Mexico"" or countryindex==""Oman"" or countryindex==""Pakistan"" or countryindex==""Quatar"" or countryindex==""South Africa"": ASSIGN="" The model wait for the peak to fit better"" ASSIGN==""Dominican Republic"" or countryindex==""Ghana"": ASSIGN="" The model wait for more data to fit better"" plt.text(1, ASSIGN-10000, ASSIGN, fontsize=15) plt.legend() plt.show()",0,display_data,"for countryindex in codiv_country_without_Restrictions_qurantine_active.T:
 #codiv_country_quarantine_active.T:
     # china is only in quarntine group
     if countryindex==""Singapore"" or countryindex==""Qatar"" or countryindex==""Kuwait"":
         trigger=2000
     else:
         trigger=300
     coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=""NoQuaNoRES"",trigger=trigger)
     y_Country = np.polyval(coefs_Country, x_values)
     y_Country_prev = np.polyval(coefs_Country_prev, x_values)
     # adjust the ymax graph limit
     if countryindex==""US"":
         ymax=1500000
     else:
         if countryindex==""Brazil"":
             ymax=180000
         else:
             ymax=60000
     fig = plt.figure(figsize= (18, 10))
     plt.scatter(x_tr_China, y_tr_China, s=10)
     plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=""blue"")
     if countryindex!=""China"":
         plt.scatter(x_tr_Country, y_tr_Country, s=10)
         plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=""magenta"")
         plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=""black"")
         PredictionName='%s-prediction'%(countryindex,)
         plt.plot(x_values, y_Country_prev, label=PredictionName,c=""brown"",ls='dashed')
     plt.ylim(-5,ymax)
     plt.ylabel(""Number"")
     plt.grid(True)
     tickvalues = range(0,len(dfdate),10)
     plt.xticks(ticks = tickvalues ,labels = dfdate[""date""], rotation = 75)
     if countryindex!=""China"":
         Title='%s-prediction - No-Restrictions No-quarantine'%(countryindex,)
         plt.title(Title)
     else:
         plt.title(""China"")
     ##
     Texte=""""
     if countryindex==""Armenia"" or countryindex==""Bahrain"" or countryindex==""Bangladesh"" or countryindex==""Belarus""or countryindex==""Brazil""or countryindex==""Chile""or countryindex==""United Kingdom""or countryindex==""US"":
         Texte="" The model wait for the peak to fit better""  
     if countryindex==""Ecuador"" or countryindex==""Indonesia"" or countryindex==""Kuwait"" or countryindex==""Mexico"" or countryindex==""Oman"" or countryindex==""Pakistan"" or countryindex==""Quatar"" or countryindex==""South Africa"":
         Texte="" The model wait for the peak to fit better""
     if countryindex==""Dominican Republic"" or countryindex==""Ghana"":
         Texte="" The model wait for more data to fit better""
     plt.text(1, ymax-10000, Texte, fontsize=15)
     plt.legend()
     plt.show()"
SETUP warnings.filterwarnings('ignore'),0,not_existent,"#importando bibliotecas
 
 import numpy as np # linear algebra
 import pandas as pd # data processing
 import matplotlib.pyplot as plt # visualization
 import seaborn as sns # visualization
 from scipy import stats
 from scipy.stats import norm 
 import warnings 
 warnings.filterwarnings('ignore') #ignore warnings
 
 %matplotlib inline
 import gc"
"ASSIGN=pd.read_csv(""C:path"")",0,error,"stores=pd.read_csv(""C:/Users/sony/OneDrive/Documentos/teste_ds_ze/stores.csv"")"
CHECKPOINT stores,0,error,stores
"ASSIGN = pd.read_csv(""C:\\Users\\sony\\OneDrive\\Documentos\\ASSIGN.csv"")",0,error,"features = pd.read_csv(""C:\\Users\\sony\\OneDrive\\Documentos\\features.csv"")
"
CHECKPOINT features,0,error,features
"ASSIGN = pd.read_csv(""C:\\Users\\sony\\OneDrive\\Documentos\\ASSIGN.csv"")",0,error,"test = pd.read_csv(""C:\\Users\\sony\\OneDrive\\Documentos\\test.csv"")"
CHECKPOINT test,0,error,test
"ASSIGN = pd.read_csv(""C:\\Users\\sony\\OneDrive\\Documentos\\ASSIGN.csv"")",0,error,"train = pd.read_csv(""C:\\Users\\sony\\OneDrive\\Documentos\\train.csv"")"
CHECKPOINT train,0,error,train
"CHECKPOINT print(, train.shape) print(, test.shape) print(, (round(train.shape[0]*100path(train.shape[0]+test.shape[0])),100-round(train.shape[0]*100path(train.shape[0]+test.shape[0]))))",0,error,"print(""the structure of train data is "", train.shape)
 print(""the structure of test  data is "", test.shape)
 print(""the ratio of train data : test data is "", (round(train.shape[0]*100/(train.shape[0]+test.shape[0])),100-round(train.shape[0]*100/(train.shape[0]+test.shape[0]))))"
"ASSIGN=ASSIGN.merge(stores, on='Store', how='left') ASSIGN.head()",1,error,"train=train.merge(stores, on='Store', how='left')
 train.head()"
"CHECKPOINT ASSIGN = pd.merge(stores, features) dataset",1,error,"dataset  =  pd.merge(stores, features) 
 dataset"
sns.pairplot(dataset),0,error,sns.pairplot(dataset)
"def scatter(dataset, column): plt.figure() plt.scatter(dataset[column] , dataset['weeklySales']) plt.ylabel('weeklySales') plt.xlabel(column)",0,not_existent,"def scatter(dataset, column):
     plt.figure()
     plt.scatter(dataset[column] , dataset['weeklySales'])
     plt.ylabel('weeklySales')
     plt.xlabel(column)"
"ASSIGN = plt.figure(figsize=(18, 14)) ASSIGN = dataset.ASSIGN() ASSIGN = plt.pcolor(corr) plt.yticks(np.arange(0.5, len(ASSIGN.index), 1), ASSIGN.index) plt.xticks(np.arange(0.5, len(ASSIGN.columns), 1), ASSIGN.columns) ASSIGN.colorbar(ASSIGN)",0,error,"fig = plt.figure(figsize=(18, 14))
 corr = dataset.corr()
 c = plt.pcolor(corr)
 plt.yticks(np.arange(0.5, len(corr.index), 1), corr.index)
 plt.xticks(np.arange(0.5, len(corr.columns), 1), corr.columns)
 fig.colorbar(c)"
"sns.pairplot(dataset, vars=['weeklySales', 'Fuel_Price', 'Size', 'CPI', 'Dept', 'Temperature', 'Unemployment'])",0,error,"sns.pairplot(dataset, vars=['weeklySales', 'Fuel_Price', 'Size', 'CPI', 'Dept', 'Temperature', 'Unemployment'])"
"sns.pairplot(dataset, vars=[ 'Fuel_Price', 'Size', 'CPI', 'Dept', 'Temperature', 'Unemployment'])",0,error,"sns.pairplot(dataset, vars=[ 'Fuel_Price', 'Size', 'CPI', 'Dept', 'Temperature', 'Unemployment'])"
ASSIGN=pd.Series(train['ASSIGN'].unique()) ASSIGN=pd.Series(train['ASSIGN'].unique()) ASSIGN=pd.Series(train['ASSIGN'].unique()) ASSIGN=pd.Series(train['ASSIGN'].unique()) ASSIGN=pd.Series(train['ASSIGN'].unique()),1,error,"Year=pd.Series(train['Year'].unique())
 Week=pd.Series(train['Week'].unique())
 Month=pd.Series(train['Month'].unique())
 Day=pd.Series(train['Day'].unique())
 n_days=pd.Series(train['n_days'].unique())"
"CHECKPOINT print(, stores.shape) print(, stores['Store'].unique()) print(, stores['Type'].unique())",0,error,"print(""the shape of stores data set is"", stores.shape)
 print(""the unique value of store is"", stores['Store'].unique())
 print(""the unique value of Type is"", stores['Type'].unique())"
CHECKPOINT print(stores.head()) ASSIGN=stores.groupby('Type') print(ASSIGN.describe()['Size'].round(2)),1,error,"print(stores.head())
 grouped=stores.groupby('Type')
 print(grouped.describe()['Size'].round(2))"
"plt.style.use('ggplot') ASSIGN=['A store','B store','C store'] ASSIGN=grouped.describe()['Size'].round(1) ASSIGN=[(22path(17+6+22))*100,(17path(17+6+22))*100,(6path(17+6+22))*100] ASSIGN = plt.subplots(1,1, figsize=(10,10)) ASSIGN={'edgecolor':'black', 'linewidth':2} ASSIGN = {'fontsize':30} axes.pie(ASSIGN, ASSIGN=ASSIGN, ASSIGN=(0.02,0,0), ASSIGN='%1.1f%%', ASSIGN=0.6, ASSIGN=1.2, ASSIGN=wprops, ASSIGN=tprops, ASSIGN=0.8, ASSIGN=(0.5,0.5)) plt.show()",0,error,"plt.style.use('ggplot')
 labels=['A store','B store','C store']
 sizes=grouped.describe()['Size'].round(1)
 sizes=[(22/(17+6+22))*100,(17/(17+6+22))*100,(6/(17+6+22))*100] # convert to the proportion
 
 
 fig, axes = plt.subplots(1,1, figsize=(10,10))
 
 wprops={'edgecolor':'black',
       'linewidth':2}
 
 tprops = {'fontsize':30}
 
 
 axes.pie(sizes,
         labels=labels,
         explode=(0.02,0,0),
         autopct='%1.1f%%',
         pctdistance=0.6,
         labeldistance=1.2,
         wedgeprops=wprops,
         textprops=tprops,
         radius=0.8,
         center=(0.5,0.5))
 plt.show()
"
"ASSIGN = pd.concat([stores['Type'], stores['Size']], axis=1) ASSIGN = plt.subplots(figsize=(8, 6)) ASSIGN = sns.boxplot(x='Type', y='Size', data=data)",0,error,"data = pd.concat([stores['Type'], stores['Size']], axis=1)
 f, ax = plt.subplots(figsize=(8, 6))
 fig = sns.boxplot(x='Type', y='Size', data=data)"
"ASSIGN = pd.concat([train['Type'], train['Weekly_Sales']], axis=1) ASSIGN = plt.subplots(figsize=(8, 6)) ASSIGN = sns.boxplot(x='Type', y='Weekly_Sales', data=data, showfliers=False)",0,error,"data = pd.concat([train['Type'], train['Weekly_Sales']], axis=1)
 f, ax = plt.subplots(figsize=(8, 6))
 fig = sns.boxplot(x='Type', y='Weekly_Sales', data=data, showfliers=False)"
"plt.style.use('ggplot') ASSIGN=plt.figure() ASSIGN=fig.add_subplot(111) ASSIGN.scatter(train['Size'],train['Weekly_Sales'], alpha=0.5) plt.show()",0,error,"plt.style.use('ggplot')
 
 fig=plt.figure()
 ax=fig.add_subplot(111)
 
 ax.scatter(train['Size'],train['Weekly_Sales'], alpha=0.5)
 
 plt.show()"
"ASSIGN=stores['Type'].unique() plt.style.use('ggplot') ASSIGN=plt.figure(figsize=(10,5)) ASSIGN=fig.add_subplot(111) for t in ASSIGN: ASSIGN=train.loc[train['Type']==t, 'Size'] ASSIGN=train.loc[train['Type']==t, 'Weekly_Sales'] ASSIGN.scatter(ASSIGN,ASSIGN,alpha=0.5, label=t) ASSIGN.set_title('Scatter plot size and sales by store type') ASSIGN.set_xlabel('Size') ASSIGN.set_ylabel('Weekly_Sales') ASSIGN.legend(loc='higher right',fontsize=12) plt.show()",0,error,"types=stores['Type'].unique()
 
 plt.style.use('ggplot')
 
 fig=plt.figure(figsize=(10,5))
 ax=fig.add_subplot(111)
 
 for t in types:
     x=train.loc[train['Type']==t, 'Size']
     y=train.loc[train['Type']==t, 'Weekly_Sales']
     
     ax.scatter(x,y,alpha=0.5, label=t)
 
 ax.set_title('Scatter plot size and sales by store type')
 ax.set_xlabel('Size')
 ax.set_ylabel('Weekly_Sales')
 
 ax.legend(loc='higher right',fontsize=12)
 
 plt.show()"
train.head(),0,error,train.head()
"ASSIGN = pd.concat([train['Store'], train['Weekly_Sales'], train['Type']], axis=1) ASSIGN = plt.subplots(figsize=(25, 8)) ASSIGN = sns.boxplot(x='Store', y='Weekly_Sales', data=data, showfliers=False, hue=""Type"")",0,error,"data = pd.concat([train['Store'], train['Weekly_Sales'], train['Type']], axis=1)
 f, ax = plt.subplots(figsize=(25, 8))
 fig = sns.boxplot(x='Store', y='Weekly_Sales', data=data, showfliers=False, hue=""Type"")"
"ASSIGN = pd.concat([train['Store'], train['Weekly_Sales'], train['IsHoliday']], axis=1) ASSIGN = plt.subplots(figsize=(25, 8)) ASSIGN = sns.boxplot(x='Store', y='Weekly_Sales', data=data, showfliers=False, hue=""IsHoliday"")",0,error,"data = pd.concat([train['Store'], train['Weekly_Sales'], train['IsHoliday']], axis=1)
 f, ax = plt.subplots(figsize=(25, 8))
 fig = sns.boxplot(x='Store', y='Weekly_Sales', data=data, showfliers=False, hue=""IsHoliday"")"
"ASSIGN = pd.concat([train['Dept'], train['Weekly_Sales'], train['Type']], axis=1) ASSIGN = plt.subplots(figsize=(25, 10)) ASSIGN = sns.boxplot(x='Dept', y='Weekly_Sales', data=data, showfliers=False)",0,error,"data = pd.concat([train['Dept'], train['Weekly_Sales'], train['Type']], axis=1)
 f, ax = plt.subplots(figsize=(25, 10))
 fig = sns.boxplot(x='Dept', y='Weekly_Sales', data=data, showfliers=False)"
"ASSIGN = pd.concat([train['Dept'], train['Weekly_Sales'], train['Type']], axis=1) ASSIGN = plt.subplots(figsize=(10, 50)) ASSIGN = sns.boxplot(y='Dept', x='Weekly_Sales', data=data, showfliers=False, hue=""Type"",orient=""h"")",0,error,"data = pd.concat([train['Dept'], train['Weekly_Sales'], train['Type']], axis=1)
 f, ax = plt.subplots(figsize=(10, 50))
 fig = sns.boxplot(y='Dept', x='Weekly_Sales', data=data, showfliers=False, hue=""Type"",orient=""h"") "
"ASSIGN = pd.concat([train['Dept'], train['Weekly_Sales'], train['IsHoliday']], axis=1) ASSIGN = plt.subplots(figsize=(25, 10)) ASSIGN = sns.boxplot(x='Dept', y='Weekly_Sales', data=data, showfliers=False, hue=""IsHoliday"")",0,error,"data = pd.concat([train['Dept'], train['Weekly_Sales'], train['IsHoliday']], axis=1)
 f, ax = plt.subplots(figsize=(25, 10))
 fig = sns.boxplot(x='Dept', y='Weekly_Sales', data=data, showfliers=False, hue=""IsHoliday"")"
"plt.style.use('ggplot') ASSIGN = plt.subplots(1,2, figsize = (20,5)) fig.subplots_adjust(wspace=1, hspace=1) fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9) ASSIGN=train[['IsHoliday','Weekly_Sales']] ASSIGN=[sales_holiday['Weekly_Sales'].loc[sales_holiday['IsHoliday']==True],sales_holiday['Weekly_Sales'].loc[sales_holiday['IsHoliday']==False]] ASSIGN=['Holiday','Not Holiday'] ASSIGN={'color':' 'linewidth': 2, 'linestyle':'-'} ASSIGN={'color' : ' 'marker' : 'o', 'markerfacecolor': ' 'markeredgecolor':'white', 'markersize' : 3, 'linestyle' : 'None', 'linewidth' : 0.1} axes[0].boxplot(ASSIGN,ASSIGN=ASSIGN, patch_artist = 'Patch', ASSIGN=True, ASSIGN=flierprop, ASSIGN=medianprop) axes[1].boxplot(ASSIGN,ASSIGN=ASSIGN, patch_artist = 'Patch', ASSIGN=True, ASSIGN=flierprop, ASSIGN=medianprop) axes[1].set_ylim(-6000,80000) plt.show()",0,error,"plt.style.use('ggplot')
 fig, axes = plt.subplots(1,2, figsize = (20,5))
 fig.subplots_adjust(wspace=1, hspace=1)
 fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)
 
 sales_holiday=train[['IsHoliday','Weekly_Sales']]
 target=[sales_holiday['Weekly_Sales'].loc[sales_holiday['IsHoliday']==True],sales_holiday['Weekly_Sales'].loc[sales_holiday['IsHoliday']==False]]
 labels=['Holiday','Not Holiday']
 
 #median
 medianprop={'color':'#2196F3',
             'linewidth': 2,
             'linestyle':'-'}
 
 # outliers
 
 flierprop={'color' : '#EC407A',
           'marker' : 'o',
           'markerfacecolor': '#2196F3',
           'markeredgecolor':'white',
           'markersize' : 3,
           'linestyle' : 'None',
           'linewidth' : 0.1}
 
 
 
 axes[0].boxplot(target,labels=labels, patch_artist = 'Patch',
                   showmeans=True,
                   flierprops=flierprop,
                   medianprops=medianprop)
 
 
 
 
 axes[1].boxplot(target,labels=labels, patch_artist = 'Patch',
                   showmeans=True,
                   flierprops=flierprop,
                   medianprops=medianprop)
 
 axes[1].set_ylim(-6000,80000)
 
 plt.show()
 
 
"
CHECKPOINT print(train[train['IsHoliday']==True]['Weekly_Sales'].describe().round(1)) print(train[train['IsHoliday']==False]['Weekly_Sales'].describe().round(1)),0,error,"print(train[train['IsHoliday']==True]['Weekly_Sales'].describe().round(1))
 print(train[train['IsHoliday']==False]['Weekly_Sales'].describe().round(1))"
"ASSIGN = pd.concat([train['Month'], train['Weekly_Sales']], axis=1) ASSIGN = plt.subplots(figsize=(8, 6)) ASSIGN = sns.boxplot(x='Month', y=""Weekly_Sales"", data=data, showfliers=False)",0,error,"data = pd.concat([train['Month'], train['Weekly_Sales']], axis=1)
 f, ax = plt.subplots(figsize=(8, 6))
 fig = sns.boxplot(x='Month', y=""Weekly_Sales"", data=data, showfliers=False)"
"ASSIGN = pd.concat([train['Month'], train['Weekly_Sales'],train['IsHoliday']], axis=1) ASSIGN = plt.subplots(figsize=(8, 6)) ASSIGN = sns.boxplot(x='Month', y=""Weekly_Sales"", data=data, showfliers=False, hue='IsHoliday')",0,error,"data = pd.concat([train['Month'], train['Weekly_Sales'],train['IsHoliday']], axis=1)
 f, ax = plt.subplots(figsize=(8, 6))
 fig = sns.boxplot(x='Month', y=""Weekly_Sales"", data=data, showfliers=False, hue='IsHoliday')"
"ASSIGN = pd.concat([train['Month'], train['Weekly_Sales'],train['Type']], axis=1) ASSIGN = plt.subplots(figsize=(8, 6)) ASSIGN = sns.boxplot(x='Month', y=""Weekly_Sales"", data=data, showfliers=False, hue='Type')",0,error,"data = pd.concat([train['Month'], train['Weekly_Sales'],train['Type']], axis=1)
 f, ax = plt.subplots(figsize=(8, 6))
 fig = sns.boxplot(x='Month', y=""Weekly_Sales"", data=data, showfliers=False, hue='Type')"
"ASSIGN = pd.concat([train['Year'], train['Weekly_Sales']], axis=1) ASSIGN = plt.subplots(figsize=(8, 6)) ASSIGN = sns.boxplot(x='Year', y=""Weekly_Sales"", data=data, showfliers=False)",0,error,"data = pd.concat([train['Year'], train['Weekly_Sales']], axis=1)
 f, ax = plt.subplots(figsize=(8, 6))
 fig = sns.boxplot(x='Year', y=""Weekly_Sales"", data=data, showfliers=False)"
"ASSIGN = pd.concat([train['Week'], train['Weekly_Sales']], axis=1) ASSIGN = plt.subplots(figsize=(20, 6)) ASSIGN = sns.boxplot(x='Week', y=""Weekly_Sales"", data=data, showfliers=False)",0,error,"data = pd.concat([train['Week'], train['Weekly_Sales']], axis=1)
 f, ax = plt.subplots(figsize=(20, 6))
 fig = sns.boxplot(x='Week', y=""Weekly_Sales"", data=data, showfliers=False)"
"ASSIGN = plt.subplots(figsize=(8, 6)) sns.distplot(train['Weekly_Sales'])",0,error,"f, ax = plt.subplots(figsize=(8, 6))
 sns.distplot(train['Weekly_Sales'])"
"CHECKPOINT print(, train['Weekly_Sales'].skew()) #skewness print(, train['Weekly_Sales'].kurt()) #kurtosis",0,error,"print(""Skewness: "", train['Weekly_Sales'].skew()) #skewness
 print(""Kurtosis: "", train['Weekly_Sales'].kurt()) #kurtosis"
train['Weekly_Sales'].min(),0,error,train['Weekly_Sales'].min()
"ASSIGN = plt.figure(figsize = (10,5)) ASSIGN.add_subplot(1,2,1) ASSIGN = stats.probplot(train.loc[train['Weekly_Sales']>0,'Weekly_Sales'], plot=plt) ASSIGN.add_subplot(1,2,2) ASSIGN = stats.probplot(np.log1p(train.loc[train['Weekly_Sales']>0,'Weekly_Sales']), plot=plt)",0,error,"fig = plt.figure(figsize = (10,5))
 
 fig.add_subplot(1,2,1)
 res = stats.probplot(train.loc[train['Weekly_Sales']>0,'Weekly_Sales'], plot=plt)
 
 fig.add_subplot(1,2,2)
 res = stats.probplot(np.log1p(train.loc[train['Weekly_Sales']>0,'Weekly_Sales']), plot=plt)"
train.describe()['Weekly_Sales'],0,error,train.describe()['Weekly_Sales']
"ASSIGN=train[train['Weekly_Sales']>0] ASSIGN=train[train['Weekly_Sales']<=0] ASSIGN = np.log1p(train_over_zero['Weekly_Sales']) ASSIGN = plt.subplots(figsize=(8, 6)) sns.distplot(ASSIGN)",0,error,"train_over_zero=train[train['Weekly_Sales']>0]
 train_below_zero=train[train['Weekly_Sales']<=0]
 sales_over_zero = np.log1p(train_over_zero['Weekly_Sales'])
 #histogram
 f, ax = plt.subplots(figsize=(8, 6))
 sns.distplot(sales_over_zero)"
"CHECKPOINT print(, sales_over_zero.skew()) #skewness print(, sales_over_zero.kurt()) #kurtosis",0,error,"print(""Skewness: "", sales_over_zero.skew()) #skewness
 print(""Kurtosis: "", sales_over_zero.kurt()) #kurtosis"
"CHECKPOINT ASSIGN=train.groupby(['Dept','Date']).mean().round(0).reset_index() print(ASSIGN.shape) print(ASSIGN.head()) ASSIGN=grouped[['Dept','Date','Weekly_Sales']] ASSIGN=train['Dept'].unique() ASSIGN.sort() ASSIGN=dept[0:20] ASSIGN=dept[20:40] ASSIGN=dept[40:60] ASSIGN=dept[60:] ASSIGN = plt.subplots(2,2,figsize=(20,10)) fig.subplots_adjust(wspace=0.5, hspace=0.5) fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9) for i in ASSIGN : ASSIGN=data[data['Dept']==i] ax[0,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'],label='Dept_1_mean_sales') for i in ASSIGN : ASSIGN=data[data['Dept']==i] ax[0,1].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'],label='Dept_1_mean_sales') for i in ASSIGN : ASSIGN=data[data['Dept']==i] ax[1,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'],label='Dept_1_mean_sales') for i in ASSIGN : ASSIGN=data[data['Dept']==i] ax[1,1].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'],label='Dept_1_mean_sales') ax[0,0].set_title('Mean sales record by department(0~19)') ax[0,1].set_title('Mean sales record by department(20~39)') ax[1,0].set_title('Mean sales record by department(40~59)') ax[1,1].set_title('Mean sales record by department(60~)') ax[0,0].set_ylabel('Mean sales') ax[0,0].set_xlabel('Date') ax[0,1].set_ylabel('Mean sales') ax[0,1].set_xlabel('Date') ax[1,0].set_ylabel('Mean sales') ax[1,0].set_xlabel('Date') ax[1,1].set_ylabel('Mean sales') ax[1,1].set_xlabel('Date') plt.show()",0,error,"grouped=train.groupby(['Dept','Date']).mean().round(0).reset_index()
 print(grouped.shape)
 print(grouped.head())
 data=grouped[['Dept','Date','Weekly_Sales']]
 
 
 dept=train['Dept'].unique()
 dept.sort()
 dept_1=dept[0:20]
 dept_2=dept[20:40]
 dept_3=dept[40:60]
 dept_4=dept[60:]
 
 fig, ax = plt.subplots(2,2,figsize=(20,10))
 fig.subplots_adjust(wspace=0.5, hspace=0.5)
 fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)
 
 for i in dept_1 :
     data_1=data[data['Dept']==i]
     ax[0,0].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')
 
 for i in dept_2 :
     data_1=data[data['Dept']==i]
     ax[0,1].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')
     
 for i in dept_3 :
     data_1=data[data['Dept']==i]
     ax[1,0].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')    
 
 for i in dept_4 :
     data_1=data[data['Dept']==i]
     ax[1,1].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')        
     
 ax[0,0].set_title('Mean sales record by department(0~19)')
 ax[0,1].set_title('Mean sales record by department(20~39)')
 ax[1,0].set_title('Mean sales record by department(40~59)')
 ax[1,1].set_title('Mean sales record by department(60~)')
 
 
 ax[0,0].set_ylabel('Mean sales')
 ax[0,0].set_xlabel('Date')
 ax[0,1].set_ylabel('Mean sales')
 ax[0,1].set_xlabel('Date')
 ax[1,0].set_ylabel('Mean sales')
 ax[1,0].set_xlabel('Date')
 ax[1,1].set_ylabel('Mean sales')
 ax[1,1].set_xlabel('Date')
 
 
 plt.show()"
"ASSIGN=train.groupby(['Store','Date']).mean().round(0).reset_index() grouped.shape ASSIGN.head() ASSIGN=grouped[['Store','Date','Weekly_Sales']] type(ASSIGN) ASSIGN=train['Store'].unique() ASSIGN.sort() ASSIGN=store[0:5] ASSIGN=store[5:10] ASSIGN=store[10:15] ASSIGN=store[15:20] ASSIGN=store[20:25] ASSIGN=store[25:30] ASSIGN=store[30:35] ASSIGN=store[35:40] ASSIGN=store[40:] ASSIGN = plt.subplots(5,2,figsize=(20,15)) fig.subplots_adjust(wspace=0.5, hspace=0.5) fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9) for i in ASSIGN : ASSIGN=data[data['Store']==i] ax[0,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales']) for i in ASSIGN : ASSIGN=data[data['Store']==i] ax[0,1].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales']) for i in ASSIGN : ASSIGN=data[data['Store']==i] ax[1,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales']) for i in ASSIGN : ASSIGN=data[data['Store']==i] ax[1,1].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales']) for i in ASSIGN : ASSIGN=data[data['Store']==i] ax[2,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales']) for i in ASSIGN : ASSIGN=data[data['Store']==i] ax[2,1].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales']) for i in ASSIGN : ASSIGN=data[data['Store']==i] ax[3,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales']) for i in ASSIGN : ASSIGN=data[data['Store']==i] ax[3,1].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales']) for i in ASSIGN : ASSIGN=data[data['Store']==i] ax[4,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales']) ax[0,0].set_title('Mean sales record by ASSIGN(0~4)') ax[0,1].set_title('Mean sales record by ASSIGN(5~9)') ax[1,0].set_title('Mean sales record by ASSIGN(10~14)') ax[1,1].set_title('Mean sales record by ASSIGN(15~19)') ax[2,0].set_title('Mean sales record by ASSIGN(20~24)') ax[2,1].set_title('Mean sales record by ASSIGN(25~29)') ax[3,0].set_title('Mean sales record by ASSIGN(30~34)') ax[3,1].set_title('Mean sales record by ASSIGN(35~39)') ax[4,0].set_title('Mean sales record by ASSIGN(40~)') ax[0,0].set_ylabel('Mean sales') ax[0,0].set_xlabel('Date') ax[0,1].set_ylabel('Mean sales') ax[0,1].set_xlabel('Date') ax[1,0].set_ylabel('Mean sales') ax[1,0].set_xlabel('Date') ax[1,1].set_ylabel('Mean sales') ax[1,1].set_xlabel('Date') ax[2,0].set_ylabel('Mean sales') ax[2,0].set_xlabel('Date') ax[2,1].set_ylabel('Mean sales') ax[2,1].set_xlabel('Date') ax[3,0].set_ylabel('Mean sales') ax[3,0].set_xlabel('Date') ax[3,1].set_ylabel('Mean sales') ax[3,1].set_xlabel('Date') ax[4,0].set_ylabel('Mean sales') ax[4,0].set_xlabel('Date') plt.show()",0,error,"grouped=train.groupby(['Store','Date']).mean().round(0).reset_index()
 grouped.shape
 grouped.head()
 
 data=grouped[['Store','Date','Weekly_Sales']]
 type(data)
 
 
 store=train['Store'].unique()
 store.sort()
 store_1=store[0:5]
 store_2=store[5:10]
 store_3=store[10:15]
 store_4=store[15:20]
 store_5=store[20:25]
 store_6=store[25:30]
 store_7=store[30:35]
 store_8=store[35:40]
 store_9=store[40:]
 
 fig, ax = plt.subplots(5,2,figsize=(20,15))
 
 fig.subplots_adjust(wspace=0.5, hspace=0.5)
 fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)
 
 for i in store_1 :
     data_1=data[data['Store']==i]
     ax[0,0].plot(data_1['Date'], data_1['Weekly_Sales'])
     
 for i in store_2 :
     data_2=data[data['Store']==i]
     ax[0,1].plot(data_2['Date'], data_2['Weekly_Sales'])
     
 for i in store_3 :
     data_3=data[data['Store']==i]
     ax[1,0].plot(data_3['Date'], data_3['Weekly_Sales'])
 
 for i in store_4 :
     data_4=data[data['Store']==i]
     ax[1,1].plot(data_4['Date'], data_4['Weekly_Sales'])
     
 for i in store_5 :
     data_5=data[data['Store']==i]
     ax[2,0].plot(data_5['Date'], data_5['Weekly_Sales'])  
 
 for i in store_6 :
     data_6=data[data['Store']==i]
     ax[2,1].plot(data_6['Date'], data_6['Weekly_Sales'])  
 
 for i in store_7 :
     data_7=data[data['Store']==i]
     ax[3,0].plot(data_7['Date'], data_7['Weekly_Sales'])      
 
 for i in store_8 :
     data_8=data[data['Store']==i]
     ax[3,1].plot(data_8['Date'], data_8['Weekly_Sales'])     
     
 for i in store_9 :
     data_9=data[data['Store']==i]
     ax[4,0].plot(data_9['Date'], data_9['Weekly_Sales'])     
 
     
 ax[0,0].set_title('Mean sales record by store(0~4)')
 ax[0,1].set_title('Mean sales record by store(5~9)')
 ax[1,0].set_title('Mean sales record by store(10~14)')
 ax[1,1].set_title('Mean sales record by store(15~19)')
 ax[2,0].set_title('Mean sales record by store(20~24)')
 ax[2,1].set_title('Mean sales record by store(25~29)')
 ax[3,0].set_title('Mean sales record by store(30~34)')
 ax[3,1].set_title('Mean sales record by store(35~39)')
 ax[4,0].set_title('Mean sales record by store(40~)')
 
 
 
 ax[0,0].set_ylabel('Mean sales')
 ax[0,0].set_xlabel('Date')
 ax[0,1].set_ylabel('Mean sales')
 ax[0,1].set_xlabel('Date')
 ax[1,0].set_ylabel('Mean sales')
 ax[1,0].set_xlabel('Date')
 ax[1,1].set_ylabel('Mean sales')
 ax[1,1].set_xlabel('Date')
 ax[2,0].set_ylabel('Mean sales')
 ax[2,0].set_xlabel('Date')
 ax[2,1].set_ylabel('Mean sales')
 ax[2,1].set_xlabel('Date')
 ax[3,0].set_ylabel('Mean sales')
 ax[3,0].set_xlabel('Date')
 ax[3,1].set_ylabel('Mean sales')
 ax[3,1].set_xlabel('Date')
 ax[4,0].set_ylabel('Mean sales')
 ax[4,0].set_xlabel('Date')
 
 
 
 plt.show()"
"SETUP ASSIGN = pd.read_csv(""..path"")",0,error,"import pandas as pd
 features = pd.read_csv(""../input/features.csv"")"
"SETUP ASSIGN = pd.read_csv(""..path"")",0,error,"import pandas as pd
 test = pd.read_csv(""../input/test.csv"")"
"SETUP ASSIGN = pd.read_csv(""..path"")",0,error,"import pandas as pd
 train = pd.read_csv(""../input/train.csv"")"
"SETUP ASSIGN = pd.read_csv(""..path(v4).csv"") ASSIGN = pd.read_csv(""..path"") np.random.seed(0)",0,not_existent,"# modules we'll use import pandas as pd import numpy as np  # read in all our data nfl_data = pd.read_csv(""../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2017 (v4).csv"") sf_permits = pd.read_csv(""../input/building-permit-applications-data/Building_Permits.csv"")  # set seed for reproducibility np.random.seed(0) "
nfl_data.sample(5),0,not_existent,# look at a few rows of the nfl_data file. I can see a handful of missing data already! nfl_data.sample(5)
sf_permits.sample(5),0,not_existent,# your turn! Look at a couple of rows from the sf_permits dataset. Do you notice any missing data?  # your code goes here :)  sf_permits.sample(5)
ASSIGN = nfl_data.isnull().sum() ASSIGN[0:10],1,not_existent,# get the number of missing data points per column missing_values_count = nfl_data.isnull().sum()  # look at the # of missing points in the first ten columns missing_values_count[0:10]
ASSIGN = np.product(nfl_data.shape) ASSIGN = missing_values_count.sum() (total_missingpath) * 100,1,not_existent,# how many total missing values do we have? total_cells = np.product(nfl_data.shape) total_missing = missing_values_count.sum()  # percent of data that is missing (total_missing/total_cells) * 100
ASSIGN = sf_permits.isnull().sum() ASSIGN = np.product(sf_permits.shape) ASSIGN = missing_values_count_sf_permits.sum() (total_missing_sf_permitspath) * 100,1,not_existent,# your turn! Find out what percent of the sf_permits dataset is missing  # get the number of missing data points per column for the sf_permits missing_values_count_sf_permits = sf_permits.isnull().sum()  # count the total cells of sf_permits and its missing data total_cells_sf_permits = np.product(sf_permits.shape) total_missing_sf_permits = missing_values_count_sf_permits.sum()  # percent of data that is missing on sf_permits (total_missing_sf_permits/total_cells_sf_permits) * 100
missing_values_count[0:10],0,not_existent,# look at the # of missing points in the first ten columns missing_values_count[0:10]
CHECKPOINT print(.format((sf_permits['Street Number Suffix'].isnull().sum()path['Street Number Suffix'].shape[0])*100)) print(.format((sf_permits['Zipcode'].isnull().sum()path['Zipcode'].shape[0])*100)),0,not_existent,"# look for the percentage of missing data in order to have some idea of it. print(""Percentage of missinf field 'Street Number Suffix': {0:.2f} %"".format((sf_permits['Street Number Suffix'].isnull().sum()/sf_permits['Street Number Suffix'].shape[0])*100)) print(""Percentageof missinf field 'Zipcode': {0:.2f} %"".format((sf_permits['Zipcode'].isnull().sum()/sf_permits['Zipcode'].shape[0])*100)) #sf_permits['Zipcode'] "
nfl_data.dropna(),0,not_existent,# remove all the rows that contain a missing value nfl_data.dropna()
ASSIGN = nfl_data.dropna(axis=1) ASSIGN.head(),1,not_existent,# remove all columns with at least one missing value columns_with_na_dropped = nfl_data.dropna(axis=1) columns_with_na_dropped.head()
CHECKPOINT print( % nfl_data.shape[1]) print( % columns_with_na_dropped.shape[1]),0,not_existent,"# just how much data did we lose? print(""Columns in original dataset: %d \n"" % nfl_data.shape[1]) print(""Columns with na's dropped: %d"" % columns_with_na_dropped.shape[1])"
sf_permits.dropna(),0,not_existent,# Your turn! Try removing all the rows from the sf_permits dataset that contain missing values. How many are left? # remove all the rows that contain a missing value sf_permits.dropna()
CHECKPOINT ASSIGN = sf_permits.dropna(axis=1) print( % sf_permits.shape[1]) print( % ASSIGN.shape[1]),1,not_existent,"# Now try removing all the columns with empty values. Now how much of your data is left? columns_with_na_dropped_sf = sf_permits.dropna(axis=1)  # just how much data did we lose? print(""Columns in original dataset: %d \n"" % sf_permits.shape[1]) print(""Columns with na's dropped: %d"" % columns_with_na_dropped_sf.shape[1])"
"CHECKPOINT ASSIGN = nfl_data.loc[:, 'EPA':'Season'].head() subset_nfl_data",1,not_existent,"# get a small subset of the NFL dataset subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head() subset_nfl_data"
subset_nfl_data.fillna(0),0,not_existent,# replace all NA's with 0 subset_nfl_data.fillna(0)
"subset_nfl_data.fillna(method = 'bfill', axis=0).fillna(""0"")",1,not_existent,"# replace all NA's the value that comes directly after it in the same column,  # then replace all the reamining na's with 0 subset_nfl_data.fillna(method = 'bfill', axis=0).fillna(""0"")"
"ASSIGN = sf_permits.fillna(method=""bfill"", axis=0).fillna(0) ASSIGN.sample(5)",1,not_existent,"# Your turn! Try replacing all the NaN's in the sf_permits data with the one that # comes directly after it and then   sf_permits_sub = sf_permits.fillna(method=""bfill"", axis=0).fillna(0)  sf_permits_sub.sample(5)"
SETUP CHECKPOINT print(os.listdir()),0,not_existent,"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import norm from scipy.stats import skew from scipy import stats from sklearn.metrics import mean_squared_error, make_scorer %matplotlib inline # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."
"CHECKPOINT ASSIGN = pd.read_csv('..path', header=0, sep=',') ASSIGN = ASSIGN.loc[:,'MSSubClass':] print('Training data columns:', ASSIGN.columns) print('Training data shape', ASSIGN.shape) ASSIGN = pd.read_csv('..path', header=0, sep=',') ASSIGN = ASSIGN.loc[:,'MSSubClass':] print('Training data columns:', ASSIGN.columns) print('Test data shape\n', ASSIGN.shape)",0,not_existent,"train_df = pd.read_csv('../input/train.csv', header=0, sep=',') train_df = train_df.loc[:,'MSSubClass':] #print('Training data\n', train_df.head()) print('Training data columns:', train_df.columns) print('Training data shape', train_df.shape)  test_df = pd.read_csv('../input/test.csv', header=0, sep=',') test_df = test_df.loc[:,'MSSubClass':] #print('Test data\n', test_df.head()) print('Training data columns:', test_df.columns) print('Test data shape\n', test_df.shape)"
"CHECKPOINT print('Missing data points in every features') ASSIGN = train_df.isnull().sum().sort_values(ascending=False) ASSIGN = (train_df.isnull().sum()path().count()).sort_values(ascending=False) ASSIGN = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) print(ASSIGN[:20])",1,not_existent,"# Missing data print('Missing data points in every features') total = train_df.isnull().sum().sort_values(ascending=False) percent = (train_df.isnull().sum()/train_df.isnull().count()).sort_values(ascending=False) missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent']) print(missing_data[:20])"
"CHECKPOINT ASSIGN = missing_data[missing_data['Percent'] > 0.0].index ASSIGN = train_df[missing_data[missing_data['Percent'] <= 0.0].index] ASSIGN = filr_train_df.dtypes[filr_train_df.dtypes != ""object""].index # Numerical columns ASSIGN = filr_train_df.dtypes[filr_train_df.dtypes == ""object""].index # Categorical columns print(ASSIGN)",1,not_existent,"# Findout how many of the columns are caregorical or numerical nan_columns = missing_data[missing_data['Percent'] > 0.0].index filr_train_df = train_df[missing_data[missing_data['Percent'] <= 0.0].index] numr_cols = filr_train_df.dtypes[filr_train_df.dtypes != ""object""].index # Numerical columns catg_cols = filr_train_df.dtypes[filr_train_df.dtypes == ""object""].index # Categorical columns print(catg_cols)"
"CHECKPOINT for ind, col in filr_train_df[catg_cols].iteritems(): print(ind, set(list(col)))",0,not_existent,"#dealing with missing data for ind, col in filr_train_df[catg_cols].iteritems(): # What kind of data in categorical columns     print(ind, set(list(col)))"
"CHECKPOINT print('Number of features:', len(filr_train_df.columns)) ASSIGN = filr_train_df.corr() ASSIGN = filr_train_df_corr[(filr_train_df_corr['SalePrice'] > 0.1)].index ASSIGN = filr_train_df.loc[:,highly_corr].corr() print('Number of features with corr:', len(ASSIGN)) plt.figure(figsize=(10,10)) sns.heatmap(ASSIGN, annot=True, square=True)",0,not_existent,"# Corrilation of numerical features with sales prize print('Number of features:', len(filr_train_df.columns)) filr_train_df_corr = filr_train_df.corr() highly_corr = filr_train_df_corr[(filr_train_df_corr['SalePrice'] > 0.1)].index # No negative correlation in data corr_sale = filr_train_df.loc[:,highly_corr].corr() print('Number of features with corr:', len(corr_sale)) plt.figure(figsize=(10,10)) sns.heatmap(corr_sale, annot=True, square=True)"
"CHECKPOINT ASSIGN = filr_train_df[highly_corr].apply(lambda x: skew(x.dropna())) print('Skewness in feature data\n',ASSIGN)",1,not_existent,"# Now check for skewnwss of numerical features skewed_feats = filr_train_df[highly_corr].apply(lambda x: skew(x.dropna())) #compute skewness print('Skewness in feature data\n',skewed_feats)"
"CHECKPOINT ASSIGN = [] ASSIGN = pd.DataFrame() for ind, skew in skewed_feats.iteritems(): if (skew > 0.5): ASSIGN = pd.concat([ASSIGN, np.log1p(filr_train_df[ind])], axis=1) ASSIGN.append(ind) else: ASSIGN = pd.concat([ASSIGN, filr_train_df[ind]], axis=1) print('Example feature:', ASSIGN[0]) ASSIGN = pd.DataFrame({'Not_trsf': filr_train_df[LT_columns[0]], 'log_trsf':numLT_train_df[LT_columns[0]]}) ASSIGN.hist()",1,not_existent,"# Correct skewness for features with positive skewness > 0.5 with log1p transformation LT_columns = [] numLT_train_df = pd.DataFrame() for ind, skew in skewed_feats.iteritems():     if (skew > 0.5):         numLT_train_df = pd.concat([numLT_train_df, np.log1p(filr_train_df[ind])], axis=1)         LT_columns.append(ind)     else:         numLT_train_df = pd.concat([numLT_train_df, filr_train_df[ind]], axis=1)          # Example of skewness print('Example feature:', LT_columns[0]) skew_ex = pd.DataFrame({'Not_trsf': filr_train_df[LT_columns[0]], 'log_trsf':numLT_train_df[LT_columns[0]]}) skew_ex.hist()"
"sns.set() sns.distplot(numLT_train_df[LT_columns[0]], fit=norm); ASSIGN = plt.figure() ASSIGN = stats.probplot(numLT_train_df[LT_columns[0]], plot=plt)",0,not_existent,"# Can also look at normal prob plot #histogram and normal probability plot sns.set() sns.distplot(numLT_train_df[LT_columns[0]], fit=norm); fig = plt.figure() res = stats.probplot(numLT_train_df[LT_columns[0]], plot=plt)"
"CHECKPOINT print(numLT_train_df.shape) sns.pairplot(numLT_train_df.iloc[:,:5])",0,not_existent,"print(numLT_train_df.shape) sns.pairplot(numLT_train_df.iloc[:,:5])"
CHECKPOINT catg_cols highly_corr LT_columns,0,not_existent,# What features we want to take catg_cols # these are our categorical features highly_corr # numeric features which we have selected based on correlation LT_columns # numeric features which needs to be transformed
"CHECKPOINT ASSIGN = pd.concat([train_df.loc[:,'MSSubClass':], test_df.loc[:,'MSSubClass':]], ignore_index=True) print(ASSIGN.shape) print(ASSIGN.head())",1,not_existent,"# Concat test and training for preprocessing tot_data = pd.concat([train_df.loc[:,'MSSubClass':], test_df.loc[:,'MSSubClass':]], ignore_index=True) print(tot_data.shape) print(tot_data.head())"
ASSIGN = pd.get_dummies(tot_data[catg_cols]) ASSIGN.head(),1,not_existent,# Create dummy variable for categorical features tot_data_cat = pd.get_dummies(tot_data[catg_cols]) tot_data_cat.head()
"CHECKPOINT ASSIGN = tot_data[highly_corr] print(ASSIGN.shape) for cols in LT_columns: ASSIGN.loc[:,cols] = np.log1p(ASSIGN.loc[:,cols]) print(ASSIGN.shape) ASSIGN.head()",1,not_existent,"# Log transform numerical data tot_data_num = tot_data[highly_corr] print(tot_data_num.shape) for cols in LT_columns:     tot_data_num.loc[:,cols] = np.log1p(tot_data_num.loc[:,cols]) print(tot_data_num.shape) tot_data_num.head()"
"ASSIGN = pd.concat([tot_data_cat, tot_data_num],axis=1) ASSIGN.head()",1,not_existent,"# Combining preprocessed data tot_data_pro = pd.concat([tot_data_cat, tot_data_num],axis=1) tot_data_pro.head()"
"CHECKPOINT ASSIGN = tot_data_pro[:1000] ASSIGN = tot_data_pro[1000:1400] ASSIGN = ASSIGN.fillna(ASSIGN.mean()) print(ASSIGN.shape, ASSIGN.shape) ASSIGN = pr_trainData['SalePrice'] X_trainData = ASSIGN.drop('SalePrice', axis=1) X_testData = ASSIGN.drop('SalePrice', axis=1) print(X_trainData.shape, X_testData.shape)",1,not_existent,"# Creating matrix for sklern  pr_trainData = tot_data_pro[:1000] pr_testData = tot_data_pro[1000:1400] pr_testData = pr_testData.fillna(pr_testData.mean()) print(pr_trainData.shape, pr_testData.shape) Y = pr_trainData['SalePrice'] X_trainData = pr_trainData.drop('SalePrice', axis=1) X_testData = pr_testData.drop('SalePrice', axis=1) print(X_trainData.shape, X_testData.shape)"
"SETUP ASSIGN = make_scorer(mean_squared_error, greater_is_better = False) def rmse_cv_train(model): ASSIGN= np.sqrt(-cross_val_score(model, X_trainData, Y, scoring = scorer, cv = 5)) return(ASSIGN)",0,not_existent,"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV from sklearn.model_selection import cross_val_score, train_test_split  # Define error measure for official scoring : RMSE scorer = make_scorer(mean_squared_error, greater_is_better = False)  def rmse_cv_train(model):     rmse= np.sqrt(-cross_val_score(model, X_trainData, Y, scoring = scorer, cv = 5))     return(rmse)"
"CHECKPOINT ASSIGN = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60]) ASSIGN.fit(X_trainData, Y) ASSIGN = ridge.alpha_ print(, ASSIGN) print( + str(ASSIGN)) ASSIGN = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, ASSIGN * .9, ASSIGN * .95, ASSIGN, ASSIGN * 1.05, ASSIGN * 1.1, ASSIGN * 1.15, ASSIGN * 1.25, ASSIGN * 1.3, ASSIGN * 1.35, ASSIGN * 1.4], ASSIGN = 5) ASSIGN.fit(X_trainData, Y) ASSIGN = ridge.alpha_ print(, ASSIGN) print(, rmse_cv_train(ASSIGN).mean()) ASSIGN = ridge.predict(X_trainData) ASSIGN = ridge.predict(X_testData) sns.set() plt.scatter(ASSIGN, ASSIGN - Y, c = ""blue"", marker = ""o"", label = ""Training data"", ASSIGN=0.7) plt.scatter(ASSIGN, ASSIGN - pr_testData['SalePrice'], c = ""green"", marker = ""o"", label = ""Validation data"", ASSIGN=0.7) plt.title(""Linear regression with Ridge regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Residuals"") plt.legend(loc = ""upper left"") plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = ""red"") plt.show() plt.scatter(ASSIGN, Y, c = ""blue"", marker = ""o"", label = ""Training data"", ASSIGN=0.7) plt.scatter(ASSIGN, pr_testData['SalePrice'], c = ""green"", marker = ""o"", label = ""Validation data"", ASSIGN=0.7) plt.title(""Linear regression with Ridge regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Real values"") plt.legend(loc = ""upper left"") plt.plot([10.5, 13.5], [10.5, 13.5], c = ""red"") plt.show() ASSIGN = pd.Series(ridge.coef_, index = X_trainData.columns) print( + str(sum(ASSIGN != 0)) + + \ str(sum(ASSIGN == 0)) + "" features"") ASSIGN = pd.concat([coefs.sort_values().head(10), ASSIGN.sort_values().tail(10)]) ASSIGN.plot(kind = ""barh"") plt.title(""Coefficients in the Ridge Model"") plt.show()",0,not_existent,"ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60]) ridge.fit(X_trainData, Y) alpha = ridge.alpha_ print(""Best alpha :"", alpha)  print(""Try again for more precision with alphas centered around "" + str(alpha)) ridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85,                            alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,                           alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4],                  cv = 5) ridge.fit(X_trainData, Y) alpha = ridge.alpha_ print(""Best alpha :"", alpha)  print(""Ridge RMSE on Training set :"", rmse_cv_train(ridge).mean()) y_train_rdg = ridge.predict(X_trainData) y_test_rdg = ridge.predict(X_testData) # Plot residuals sns.set() plt.scatter(y_train_rdg, y_train_rdg - Y, c = ""blue"", marker = ""o"", label = ""Training data"", alpha=0.7) plt.scatter(y_test_rdg, y_test_rdg - pr_testData['SalePrice'], c = ""green"", marker = ""o"", label = ""Validation data"", alpha=0.7) plt.title(""Linear regression with Ridge regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Residuals"") plt.legend(loc = ""upper left"") plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = ""red"") plt.show()  # Plot predictions plt.scatter(y_train_rdg, Y, c = ""blue"", marker = ""o"", label = ""Training data"", alpha=0.7) plt.scatter(y_test_rdg, pr_testData['SalePrice'], c = ""green"", marker = ""o"", label = ""Validation data"", alpha=0.7) plt.title(""Linear regression with Ridge regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Real values"") plt.legend(loc = ""upper left"") plt.plot([10.5, 13.5], [10.5, 13.5], c = ""red"") plt.show()  # Plot important coefficients coefs = pd.Series(ridge.coef_, index = X_trainData.columns) print(""Ridge picked "" + str(sum(coefs != 0)) + "" features and eliminated the other "" +  \       str(sum(coefs == 0)) + "" features"") imp_coefs = pd.concat([coefs.sort_values().head(10),                      coefs.sort_values().tail(10)]) imp_coefs.plot(kind = ""barh"") plt.title(""Coefficients in the Ridge Model"") plt.show()"
"CHECKPOINT ASSIGN = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1], ASSIGN = 50000, cv = 10) ASSIGN.fit(X_trainData, Y) ASSIGN = lasso.alpha_ print(, ASSIGN) print( + str(ASSIGN)) ASSIGN = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, ASSIGN * .85, ASSIGN * .9, ASSIGN * .95, ASSIGN, ASSIGN * 1.05, ASSIGN * 1.1, ASSIGN * 1.15, ASSIGN * 1.25, ASSIGN * 1.3, ASSIGN * 1.35, ASSIGN * 1.4], ASSIGN = 50000, cv = 10) ASSIGN.fit(X_trainData, Y) ASSIGN = lasso.alpha_ print(, ASSIGN) print(, rmse_cv_train(ASSIGN).mean()) ASSIGN = lasso.predict(X_trainData) ASSIGN = lasso.predict(X_testData) plt.scatter(ASSIGN, ASSIGN - Y, c = ""blue"", marker = ""s"", label = ""Training data"", ASSIGN=0.7) plt.scatter(ASSIGN, ASSIGN - pr_testData['SalePrice'], c = ""green"", marker = ""s"", label = ""Validation data"", ASSIGN=0.7) plt.title(""Linear regression with Lasso regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Residuals"") plt.legend(loc = ""upper left"") plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = ""red"") plt.show() plt.scatter(ASSIGN, Y, c = ""blue"", marker = ""s"", label = ""Training data"", ASSIGN=0.7) plt.scatter(ASSIGN, pr_testData['SalePrice'], c = ""green"", marker = ""s"", label = ""Validation data"", ASSIGN=0.7) plt.title(""Linear regression with Lasso regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Real values"") plt.legend(loc = ""upper left"") plt.plot([10.5, 13.5], [10.5, 13.5], c = ""red"") plt.show() ASSIGN = pd.Series(lasso.coef_, index = X_trainData.columns) print( + str(sum(ASSIGN != 0)) + + \ str(sum(ASSIGN == 0)) + "" features"") ASSIGN = pd.concat([coefs.sort_values().head(10), ASSIGN.sort_values().tail(10)]) ASSIGN.plot(kind = ""barh"") plt.title(""Coefficients in the Lasso Model"") plt.show()",0,not_existent,"lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1],                  max_iter = 50000, cv = 10) lasso.fit(X_trainData, Y) alpha = lasso.alpha_ print(""Best alpha :"", alpha)  print(""Try again for more precision with alphas centered around "" + str(alpha)) lasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8,                            alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05,                            alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35,                            alpha * 1.4],                  max_iter = 50000, cv = 10) lasso.fit(X_trainData, Y) alpha = lasso.alpha_ print(""Best alpha :"", alpha)  print(""Lasso RMSE on Training set :"", rmse_cv_train(lasso).mean()) y_train_las = lasso.predict(X_trainData) y_test_las = lasso.predict(X_testData)  # Plot residuals plt.scatter(y_train_las, y_train_las - Y, c = ""blue"", marker = ""s"", label = ""Training data"", alpha=0.7) plt.scatter(y_test_las, y_test_las - pr_testData['SalePrice'], c = ""green"", marker = ""s"", label = ""Validation data"", alpha=0.7) plt.title(""Linear regression with Lasso regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Residuals"") plt.legend(loc = ""upper left"") plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = ""red"") plt.show()  # Plot predictions plt.scatter(y_train_las, Y, c = ""blue"", marker = ""s"", label = ""Training data"", alpha=0.7) plt.scatter(y_test_las, pr_testData['SalePrice'], c = ""green"", marker = ""s"", label = ""Validation data"", alpha=0.7) plt.title(""Linear regression with Lasso regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Real values"") plt.legend(loc = ""upper left"") plt.plot([10.5, 13.5], [10.5, 13.5], c = ""red"") plt.show()  # Plot important coefficients coefs = pd.Series(lasso.coef_, index = X_trainData.columns) print(""Lasso picked "" + str(sum(coefs != 0)) + "" features and eliminated the other "" +  \       str(sum(coefs == 0)) + "" features"") imp_coefs = pd.concat([coefs.sort_values().head(10),                      coefs.sort_values().tail(10)]) imp_coefs.plot(kind = ""barh"") plt.title(""Coefficients in the Lasso Model"") plt.show()"
SETUP py.init_notebook_mode(connected=True),0,display_data,"# Importing the relevant libraries
 import IPython.display
 import pandas as pd
 import seaborn as sns
 import plotly.offline as py
 py.init_notebook_mode(connected=True)
 import plotly.graph_objs as go
 from matplotlib import pyplot as plt"
"ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"",parse_dates=['date']) ASSIGN = pd.read_csv(""..path"", parse_dates=['date'])",1,stream,"items = pd.read_csv(""../input/items.csv"")
 holiday_events = pd.read_csv(""../input/holidays_events.csv"")
 stores = pd.read_csv(""../input/stores.csv"")
 oil = pd.read_csv(""../input/oil.csv"")
 transactions = pd.read_csv(""../input/transactions.csv"",parse_dates=['date'])
 train = pd.read_csv(""../input/train.csv"", parse_dates=['date'])"
CHECKPOINT print(train.shape) train.head(),0,stream,"print(train.shape)
 train.head()"
"CHECKPOINT print(.format(train.columns.values, train.isnull().any().values)) print('---') print(.format(oil.columns.values,oil.isnull().any().values)) print('---') print(.format(holiday_events.columns.values,holiday_events.isnull().any().values)) print('---') print(.format(stores.columns.values,stores.isnull().any().values)) print('---') print(.format(transactions.columns.values,transactions.isnull().any().values))",0,stream,"# Check for NULL values in all files
 print(""Nulls in train: {0} => {1}"".format(train.columns.values, train.isnull().any().values))
 print('---')
 print(""Nulls in oil: {0} => {1}"".format(oil.columns.values,oil.isnull().any().values))
 print('---')
 print(""Nulls in holiday_events: {0} => {1}"".format(holiday_events.columns.values,holiday_events.isnull().any().values))
 print('---')
 print(""Nulls in stores: {0} => {1}"".format(stores.columns.values,stores.isnull().any().values))
 print('---')
 print(""Nulls in transactions: {0} => {1}"".format(transactions.columns.values,transactions.isnull().any().values))"
oil.head(3),0,execute_result,"# EDA for oil.csv begins here #
 oil.head(3)"
"ASSIGN = go.Scatter( ASSIGN='Oil prices', ASSIGN=oil['date'], ASSIGN=oil['dcoilwtico'].dropna(), ASSIGN='lines', ) ASSIGN = [trace] ASSIGN = go.Layout( ASSIGN = dict(title = 'Daily Oil price'), ASSIGN = True) ASSIGN = go.Figure(data = data, layout = layout) py.iplot(ASSIGN, filename='pandas-time-series-error-bars')",1,display_data,"# Plotting graph for oil price trends
 trace = go.Scatter(
     name='Oil prices',
     x=oil['date'],
     y=oil['dcoilwtico'].dropna(),
     mode='lines',
    )
 
 data = [trace]
 
 layout = go.Layout(
     yaxis = dict(title = 'Daily Oil price'),
     showlegend = True)
 fig = go.Figure(data = data, layout = layout)
 py.iplot(fig, filename='pandas-time-series-error-bars')"
holiday_events.head(3),0,execute_result,"# EDA for holiday_events.csv begins here #
 holiday_events.head(3)"
holiday_events.type.unique(),0,execute_result,holiday_events.type.unique()
"plt.style.use('seaborn-white') ASSIGN = holiday_events.groupby(['locale_name', 'type']).size() ASSIGN.unstack().plot(kind='bar',stacked=True, colormap= 'magma_r', figsize=(12,10), grid=False) plt.ylabel('Count of entries') plt.show()",0,display_data,"plt.style.use('seaborn-white')
 holiday_local_type = holiday_events.groupby(['locale_name', 'type']).size()
 holiday_local_type.unstack().plot(kind='bar',stacked=True, colormap= 'magma_r', figsize=(12,10),  grid=False)
 plt.ylabel('Count of entries')
 plt.show()"
items.head(),0,execute_result,"# EDA for items.csv begins here
 items.head()"
"ASSIGN = (list(x) for x in zip(*sorted(zip(items.family.value_counts().index, items.family.value_counts().values), ASSIGN = False))) ASSIGN = go.Bar( ASSIGN = items.family.value_counts().values, ASSIGN = items.family.value_counts().index ) ASSIGN = dict( ASSIGN='Counts of items per family category', ASSIGN = 900, height = 600, ASSIGN=dict( ASSIGN = True, ASSIGN = True, ASSIGN = True )) ASSIGN = go.Figure(data=[trace2]) ASSIGN['ASSIGN'].update(ASSIGN) py.iplot(ASSIGN, filename='plots')",0,display_data,"# BAR PLOT FOR ITEMS V/S FAMILY TYPE
 x, y = (list(x) for x in zip(*sorted(zip(items.family.value_counts().index, 
                                          items.family.value_counts().values), 
                                         reverse = False)))
 trace2 = go.Bar(
     y = items.family.value_counts().values,
     x = items.family.value_counts().index
 )
 
 layout = dict(
     title='Counts of items per family category',
      width = 900, height = 600,
     yaxis=dict(
         showgrid = True,
         showline = True,
         showticklabels = True
     ))
 
 fig1 = go.Figure(data=[trace2])
 fig1['layout'].update(layout)
 py.iplot(fig1, filename='plots')"
"plt.style.use('seaborn-white') ASSIGN = items.groupby(['family', 'perishable']).size() ASSIGN.unstack().plot(kind='bar',stacked=True, colormap = 'coolwarm', figsize=(12,10), grid = True) plt.ylabel('count') plt.show()",0,display_data,"# Persihable or not 
 plt.style.use('seaborn-white')
 fam_perishable = items.groupby(['family', 'perishable']).size()
 fam_perishable.unstack().plot(kind='bar',stacked=True, colormap = 'coolwarm', figsize=(12,10),  grid = True)
 plt.ylabel('count')
 plt.show()"
stores.head(3),0,execute_result,"# EDA for stores.csv begins here #
 stores.head(3)"
"ASSIGN = plt.subplots() fig.set_size_inches(8, 8) ASSIGN = sns.countplot(y = stores['state'], data = stores)",0,display_data,"# store distribution across states
 fig, ax = plt.subplots()
 fig.set_size_inches(8, 8)
 ax = sns.countplot(y = stores['state'], data = stores) "
"ASSIGN = plt.subplots() fig.set_size_inches(8, 8) ASSIGN = sns.countplot(y = stores['city'], data = stores)",0,display_data,"# store distribution across cities
 fig, ax = plt.subplots()
 fig.set_size_inches(8, 8)
 ax = sns.countplot(y = stores['city'], data = stores) "
stores.state.unique(),0,execute_result,"# Unique state names
 stores.state.unique()"
stores.city.unique(),0,execute_result,"# Unique state names
 stores.city.unique()"
"ASSIGN = plt.subplots() fig.set_size_inches(8, 5) ASSIGN = sns.countplot(x = ""type"", data = stores, palette=""Paired"")",0,display_data,"# Various types of stores and their count
 fig, ax = plt.subplots()
 fig.set_size_inches(8, 5)
 ax = sns.countplot(x = ""type"", data = stores, palette=""Paired"")"
"ASSIGN = pd.crosstab(stores.state, stores.type) ASSIGN.plot.bar(figsize = (12, 6), stacked=True) plt.legend(title='type') plt.show()",0,display_data,"ct = pd.crosstab(stores.state, stores.type)
 ct.plot.bar(figsize = (12, 6), stacked=True)
 plt.legend(title='type')
 plt.show()"
"ASSIGN = pd.crosstab(stores.city, stores.type) ASSIGN.plot.bar(figsize = (12, 6), stacked=True) plt.legend(title='type') plt.show()",0,display_data,"ct = pd.crosstab(stores.city, stores.type)
 
 ct.plot.bar(figsize = (12, 6), stacked=True)
 plt.legend(title='type')
 
 plt.show()"
stores.store_nbr.nunique(),0,execute_result,"# total no. of unique stores 
 stores.store_nbr.nunique()"
stores.cluster.sum(),0,execute_result,"# total no. of stores (including non-unique)
 stores.cluster.sum()"
"ASSIGN = plt.subplots() fig.set_size_inches(12, 7) ASSIGN = sns.countplot(x = ""cluster"", data = stores)",0,display_data,"fig, ax = plt.subplots()
 fig.set_size_inches(12, 7)
 ax = sns.countplot(x = ""cluster"", data = stores)"
transactions.head(3),0,execute_result,"# EDA for transactions.csv begins here #
 transactions.head(3)"
"print(""There are {0} transactions"". format(transactions.shape[0], transactions.shape[1]))",0,stream,"# Finding out the no.of transactions (rows)
 print(""There are {0} transactions"".
       format(transactions.shape[0], transactions.shape[1]))"
"plt.style.use('seaborn-white') plt.figure(figsize=(13,11)) plt.plot(transactions.date.values, transactions.transactions.values, color='grey') plt.axvline(x='2015-12-23',color='red',alpha=0.3) plt.axvline(x='2016-12-23',color='red',alpha=0.3) plt.axvline(x='2014-12-23',color='red',alpha=0.3) plt.axvline(x='2013-12-23',color='red',alpha=0.3) plt.axvline(x='2013-05-12',color='green',alpha=0.2, linestyle= '--') plt.axvline(x='2015-05-10',color='green',alpha=0.2, linestyle= '--') plt.axvline(x='2016-05-08',color='green',alpha=0.2, linestyle= '--') plt.axvline(x='2014-05-11',color='green',alpha=0.2, linestyle= '--') plt.axvline(x='2017-05-14',color='green',alpha=0.2, linestyle= '--') plt.ylabel('Transactions per day', fontsize= 16) plt.xlabel('Date', fontsize= 16) plt.show()",0,display_data,"# Time series plot for transaction #
 plt.style.use('seaborn-white')
 plt.figure(figsize=(13,11))
 plt.plot(transactions.date.values, transactions.transactions.values, color='grey')
 plt.axvline(x='2015-12-23',color='red',alpha=0.3)
 plt.axvline(x='2016-12-23',color='red',alpha=0.3)
 plt.axvline(x='2014-12-23',color='red',alpha=0.3)
 plt.axvline(x='2013-12-23',color='red',alpha=0.3)
 plt.axvline(x='2013-05-12',color='green',alpha=0.2, linestyle= '--')
 plt.axvline(x='2015-05-10',color='green',alpha=0.2, linestyle= '--')
 plt.axvline(x='2016-05-08',color='green',alpha=0.2, linestyle= '--')
 plt.axvline(x='2014-05-11',color='green',alpha=0.2, linestyle= '--')
 plt.axvline(x='2017-05-14',color='green',alpha=0.2, linestyle= '--')
 plt.ylabel('Transactions per day', fontsize= 16)
 plt.xlabel('Date', fontsize= 16)
 plt.show()"
SETUP np.random.seed(0),0,not_existent,# modules we'll use import pandas as pd import numpy as np  # helpful character encoding module import chardet  # set seed for reproducibility np.random.seed(0)
"ASSIGN = ""This is the euro symbol: "" type(ASSIGN)",1,not_existent,"# start with a string before = ""This is the euro symbol: ""  # check to see what datatype it is type(before)"
"ASSIGN = before.encode(""utf-8"", errors = ""replace"") type(ASSIGN)",1,not_existent,"# encode it to a different encoding, replacing characters that raise errors after = before.encode(""utf-8"", errors = ""replace"")  # check the type type(after)"
CHECKPOINT after,0,not_existent,# take a look at what the bytes look like after
CHECKPOINT print(after.decode()),0,not_existent,"# convert it back to utf-8 print(after.decode(""utf-8""))"
CHECKPOINT print(after.decode()),0,not_existent,"# try to decode our bytes with the ascii encoding print(after.decode(""ascii""))"
"CHECKPOINT ASSIGN = ""This is the euro symbol: "" ASSIGN = before.encode(""ascii"", errors = ""replace"") print(ASSIGN.decode())",1,not_existent,"# start with a string before = ""This is the euro symbol: ""  # encode it to a different encoding, replacing characters that raise errors after = before.encode(""ascii"", errors = ""replace"")  # convert it back to utf-8 print(after.decode(""ascii""))  # We've lost the original underlying byte string! It's been  # replaced with the underlying byte string for the unknown character :("
"CHECKPOINT ASSIGN = ""i'll try the recommended $, #,  and  and see what happens."" ASSIGN = my_text.encode(""ascii"", errors = ""replace"") print(ASSIGN.decode())",0,not_existent,"# Your turn! Try encoding and decoding different symbols to ASCII and # see what happens. I'd recommend $, #,  and  but feel free to # try other characters. What happens? When would this cause problems?  # start with a string my_text = ""i'll try the recommended $, #,  and   and see what happens.""  # encode it to a different encoding, replacing characters that raise errors my_text_encoded = my_text.encode(""ascii"", errors = ""replace"")  # convert it back to utf-8 print(my_text_encoded.decode(""ascii"")) "
"ASSIGN = pd.read_csv(""..path"")",0,not_existent,"# try to read in a file not in UTF-8 kickstarter_2016 = pd.read_csv(""../input/kickstarter-projects/ks-projects-201612.csv"")"
"CHECKPOINT with open(""..path"", 'rb') as rawdata: ASSIGN = chardet.detect(rawdata.read(10000)) print(ASSIGN)",0,not_existent,"# look at the first ten thousand bytes to guess the character encoding with open(""../input/kickstarter-projects/ks-projects-201801.csv"", 'rb') as rawdata:     result = chardet.detect(rawdata.read(10000))  # check what the character encoding might be print(result)"
"ASSIGN = pd.read_csv(""..path"", encoding='Windows-1252') ASSIGN.head()",0,not_existent,"# read in the file with the encoding detected by chardet kickstarter_2016 = pd.read_csv(""../input/kickstarter-projects/ks-projects-201612.csv"", encoding='Windows-1252')  # look at the first few lines kickstarter_2016.head()"
"ASSIGN=[1,10,100,1000,10000,100000,1000000] with open(""..path"", 'rb') as rawdata1: for i in ASSIGN: ASSIGN = chardet.detect(rawdata1.read(i)) print (i, ASSIGN)",1,not_existent,"# Your Turn! Trying to read in this file gives you an error. Figure out # what the correct encoding should be and read in the file. :)  read_times=[1,10,100,1000,10000,100000,1000000] with open(""../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv"", 'rb') as rawdata1:     for i in read_times:         result1 = chardet.detect(rawdata1.read(i))         print (i, result1)"
"ASSIGN = pd.read_csv(""..path"", encoding='Windows-1252') ASSIGN.head()",0,not_existent,"police_killings = pd.read_csv(""../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv"", encoding='Windows-1252')  # look at the first few lines police_killings.head()"
"kickstarter_2016.to_csv(""ks-projects-201801-utf8.csv"")",0,not_existent,"# save our file (will be saved as UTF-8 by default!) kickstarter_2016.to_csv(""ks-projects-201801-utf8.csv"")"
"police_killings.to_csv(""PoliceKillingsUS-utf8.csv"")",0,not_existent,"# Your turn! Save out a version of the police_killings dataset with UTF-8 encoding  police_killings.to_csv(""PoliceKillingsUS-utf8.csv"")"
SETUP,0,not_existent,!pip install pycaret --quiet
SETUP,0,not_existent,import numpy as np import pandas as pd from sklearn.model_selection import KFold  #import regression module from pycaret.regression import *
"SETUP ASSIGN = pd.read_csv(f""{BASE_PATH}path"") ASSIGN = pd.read_csv(f""{BASE_PATH}path"") ASSIGN = pd.read_csv(f""{BASE_PATH}path"")",0,not_existent,"BASE_PATH = '../input/trends-assessment-prediction'  fnc_df = pd.read_csv(f""{BASE_PATH}/fnc.csv"") loading_df = pd.read_csv(f""{BASE_PATH}/loading.csv"") labels_df = pd.read_csv(f""{BASE_PATH}/train_scores.csv"")"
"CHECKPOINT ASSIGN = list(fnc_df.columns[1:]), list(loading_df.columns[1:]) ASSIGN = fnc_df.merge(loading_df, on=""Id"") labels_df[""is_train""] = True ASSIGN = ASSIGN.merge(labels_df, on=""Id"", how=""left"") ASSIGN = df[df[""is_train""] != True].copy() ASSIGN = ASSIGN[ASSIGN[""is_train""] == True].copy() print(f'Shape of train data: {ASSIGN.shape}, Shape of test data: {ASSIGN.shape}')",1,not_existent,"fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:]) df = fnc_df.merge(loading_df, on=""Id"") labels_df[""is_train""] = True df = df.merge(labels_df, on=""Id"", how=""left"")  test_df = df[df[""is_train""] != True].copy() df = df[df[""is_train""] == True].copy() print(f'Shape of train data: {df.shape}, Shape of test data: {test_df.shape}')"
"SETUP ASSIGN = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2'] df.drop(['is_train'], axis=1, inplace=True) ASSIGN = ASSIGN.drop(target_cols + ['is_train'], axis=1) df[fnc_features] *= FNC_SCALE ASSIGN[fnc_features] *= FNC_SCALE",1,not_existent,"target_cols = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2'] df.drop(['is_train'], axis=1, inplace=True) test_df = test_df.drop(target_cols + ['is_train'], axis=1)   # Giving less importance to FNC features since they are easier to overfit due to high dimensionality. FNC_SCALE = 1/500 df[fnc_features] *= FNC_SCALE test_df[fnc_features] *= FNC_SCALE"
"def get_train_data(target): ASSIGN = [tar for tar in target_cols if tar != target] ASSIGN = df.drop( other_targets, axis=1) return train_df",1,not_existent,"def get_train_data(target):     other_targets = [tar for tar in target_cols if tar != target]     train_df = df.drop( other_targets, axis=1)     return train_df"
"ASSIGN = 'age' ASSIGN = get_train_data(target) ASSIGN = setup( ASSIGN = train_df, ASSIGN = ASSIGN, ASSIGN=0.8, ASSIGN = 'mean', ASSIGN = True )",1,not_existent,"target = 'age'  train_df = get_train_data(target)  setup_reg = setup(     data = train_df,     target = target,     train_size=0.8,     numeric_imputation = 'mean',     silent = True )"
ASSIGN = ['tr'],1,not_existent,blacklist_models = ['tr']
"compare_models( ASSIGN = blacklist_models, ASSIGN = 10, ASSIGN = 'MAE', ASSIGN = True )",1,not_existent,"compare_models(     blacklist = blacklist_models,     fold = 10,     sort = 'MAE', ## competition metric     turbo = True ) "
"ASSIGN = create_model( ASSIGN='br', ASSIGN=10 )",0,not_existent,"br_age = create_model(     estimator='br',     fold=10 )"
"ASSIGN = tune_model( ASSIGN='br', ASSIGN=10, ASSIGN = 'mae', ASSIGN=50 )",0,not_existent,"# here we are tuning the above created model tuned_br_age = tune_model(     estimator='br',     fold=10,     optimize = 'mae',     n_iter=50 )"
"plot_model(estimator = tuned_br_age, plot = 'learning')",0,not_existent,"# plot_model(estimator = None, plot = residuals) plot_model(estimator = tuned_br_age, plot = 'learning')"
"plot_model(estimator = tuned_br_age, plot = 'residuals')",0,not_existent,"plot_model(estimator = tuned_br_age, plot = 'residuals')"
"plot_model(estimator = tuned_br_age, plot = 'feature')",0,not_existent,"plot_model(estimator = tuned_br_age, plot = 'feature')"
evaluate_model(estimator=tuned_br_age),0,not_existent,evaluate_model(estimator=tuned_br_age)
"ASSIGN = predict_model(tuned_br_age, data=test_df)",0,not_existent,"predictions =  predict_model(tuned_br_age, data=test_df)"
predictions.head(),0,not_existent,predictions.head()
"ASSIGN = target_cols[0] ASSIGN = get_train_data(target) ASSIGN = setup( ASSIGN = train_df, ASSIGN = ASSIGN, ASSIGN=0.8, ASSIGN = 'mean', ASSIGN = True ) compare_models( ASSIGN = blacklist_models, ASSIGN = 10, ASSIGN = 'MAE', ASSIGN = True )",1,not_existent,"target = target_cols[0] train_df = get_train_data(target)  setup_reg = setup(     data = train_df,     target = target,     train_size=0.8,     numeric_imputation = 'mean',     silent = True )  compare_models(     blacklist = blacklist_models,     fold = 10,     sort = 'MAE',     turbo = True )"
"ASSIGN = target_cols[1] ASSIGN = get_train_data(target) ASSIGN = setup( ASSIGN = train_df, ASSIGN = ASSIGN, ASSIGN=0.8, ASSIGN = 'mean', ASSIGN = True ) compare_models( ASSIGN = blacklist_models, ASSIGN = 10, ASSIGN = 'MAE', ASSIGN = True )",1,not_existent,"target = target_cols[1] train_df = get_train_data(target)  setup_reg = setup(     data = train_df,     target = target,     train_size=0.8,     numeric_imputation = 'mean',     silent = True )  compare_models(     blacklist = blacklist_models,     fold = 10,     sort = 'MAE',     turbo = True )"
"ASSIGN = target_cols[2] ASSIGN = get_train_data(target) ASSIGN = setup( ASSIGN = train_df, ASSIGN = ASSIGN, ASSIGN=0.8, ASSIGN = 'mean', ASSIGN = True ) compare_models( ASSIGN = blacklist_models, ASSIGN = 10, ASSIGN = 'MAE', ASSIGN = True )",1,not_existent,"target = target_cols[2] train_df = get_train_data(target)  setup_reg = setup(     data = train_df,     target = target,     train_size=0.8,     numeric_imputation = 'mean',     silent = True )  compare_models(     blacklist = blacklist_models,     fold = 10,     sort = 'MAE',     turbo = True )"
"ASSIGN = target_cols[3] ASSIGN = get_train_data(target) ASSIGN = setup( ASSIGN = train_df, ASSIGN = ASSIGN, ASSIGN=0.8, ASSIGN = 'mean', ASSIGN = True ) compare_models( ASSIGN = blacklist_models, ASSIGN = 10, ASSIGN = 'MAE', ASSIGN = True )",1,not_existent,"target = target_cols[3] train_df = get_train_data(target)  setup_reg = setup(     data = train_df,     target = target,     train_size=0.8,     numeric_imputation = 'mean',     silent = True )  compare_models(     blacklist = blacklist_models,     fold = 10,     sort = 'MAE',     turbo = True )"
"ASSIGN = target_cols[4] ASSIGN = get_train_data(target) ASSIGN = setup( ASSIGN = train_df, ASSIGN = ASSIGN, ASSIGN=0.8, ASSIGN = 'mean', ASSIGN = True ) compare_models( ASSIGN = blacklist_models, ASSIGN = 10, ASSIGN = 'MAE', ASSIGN = True )",1,not_existent,"target = target_cols[4] train_df = get_train_data(target)  setup_reg = setup(     data = train_df,     target = target,     train_size=0.8,     numeric_imputation = 'mean',     silent = True )  compare_models(     blacklist = blacklist_models,     fold = 10,     sort = 'MAE',     turbo = True )"
"ASSIGN = [] ASSIGN = { 'age': 'br', 'domain1_var1':'catboost', 'domain1_var2':'svm', 'domain2_var1':'catboost', 'domain2_var2':'catboost', } def tune_and_ensemble(target): ASSIGN = get_train_data(target) ASSIGN = setup( ASSIGN = train_df, ASSIGN = ASSIGN, ASSIGN=0.8, ASSIGN = 'mean', ASSIGN = True ) ASSIGN = target_models_dict[target] ASSIGN = tune_model(model_name, fold=10) ASSIGN = ensemble_model(tuned_model, fold=10) return model",0,not_existent,"# mapping targets to their corresponding models  models = []  target_models_dict = {     'age': 'br',     'domain1_var1':'catboost',     'domain1_var2':'svm',     'domain2_var1':'catboost',     'domain2_var2':'catboost', }  def tune_and_ensemble(target):     train_df = get_train_data(target)         exp_reg = setup(         data = train_df,         target = target,         train_size=0.8,         numeric_imputation = 'mean',         silent = True     )          model_name = target_models_dict[target]     tuned_model = tune_model(model_name, fold=10)     model = ensemble_model(tuned_model, fold=10)     return model "
ASSIGN = target_cols[0] ASSIGN = tune_and_ensemble(target) models.append(ASSIGN),1,not_existent,target = target_cols[0] model = tune_and_ensemble(target) models.append(model)
SETUP ASSIGN = target_cols[1] ASSIGN = tune_and_ensemble(target) models.append(ASSIGN),1,not_existent,target = target_cols[1] model = tune_and_ensemble(target) models.append(model) %tb
CHECKPOINT target,0,not_existent,target
ASSIGN = target_cols[2] ASSIGN = tune_and_ensemble(target) models.append(ASSIGN),1,not_existent,target = target_cols[2] model = tune_and_ensemble(target) models.append(model)
ASSIGN = target_cols[3] ASSIGN = tune_and_ensemble(target) models.append(ASSIGN),1,not_existent,target = target_cols[3] model = tune_and_ensemble(target) models.append(model)
ASSIGN = target_cols[4] ASSIGN = tune_and_ensemble(target) models.append(ASSIGN),1,not_existent,target = target_cols[4] model = tune_and_ensemble(target) models.append(model)
"def finalize_model_pipeline(model, target): finalize_model(model) save_model(model, f'{target}_{target_models_dict[target]}', verbose=True) ASSIGN = predict_model(model, data=test_df) test_df[target] = ASSIGN['Label'].values",0,not_existent,"### create a pipeline or function to run for all targets  def finalize_model_pipeline(model, target):     # this will train the model on holdout data     finalize_model(model)     save_model(model, f'{target}_{target_models_dict[target]}', verbose=True)     # making predictions on test data     predictions = predict_model(model, data=test_df)     test_df[target] = predictions['Label'].values"
"for index, target in enumerate(target_cols): ASSIGN = models[index] finalize_model_pipeline(ASSIGN,target)",1,not_existent,"for index, target in enumerate(target_cols):     model = models[index]     finalize_model_pipeline(model,target)"
"ASSIGN = pd.melt(test_df[[""Id"", ""age"", ""domain1_var1"", ""domain1_var2"", ""domain2_var1"", ""domain2_var2""]], id_vars=[""Id""], value_name=""Predicted"") ASSIGN[""Id""] = ASSIGN[""Id""].astype(""str"") + ""_"" + ASSIGN[""variable""].astype(""str"") ASSIGN = ASSIGN.drop(""variable"", axis=1).sort_values(""Id"") assert ASSIGN.shape[0] == test_df.shape[0]*5 ASSIGN.to_csv(""submission1.csv"", index=False)",1,not_existent,"sub_df = pd.melt(test_df[[""Id"", ""age"", ""domain1_var1"", ""domain1_var2"", ""domain2_var1"", ""domain2_var2""]], id_vars=[""Id""], value_name=""Predicted"") sub_df[""Id""] = sub_df[""Id""].astype(""str"") + ""_"" +  sub_df[""variable""].astype(""str"")  sub_df = sub_df.drop(""variable"", axis=1).sort_values(""Id"") assert sub_df.shape[0] == test_df.shape[0]*5  sub_df.to_csv(""submission1.csv"", index=False)"
sub_df.head(),0,not_existent,sub_df.head()
SETUP CHECKPOINT print(os.listdir()),0,stream,"import pandas as pd
 import numpy as np
 import os
 print(os.listdir(""../input""))"
"ASSIGN = pd.read_csv('..path', encoding='latin-1')",0,not_existent,"Rest = pd.read_csv('../input/zomato.csv', encoding='latin-1')
"
Rest.head(),0,execute_result,"Rest.head()
"
CHECKPOINT Rest.shape,0,execute_result,Rest.shape
"Rest.to_csv(""Rest.csv"")",0,not_existent,"Rest.to_csv(""Rest.csv"")
"
"SETUP InteractiveShell.ast_node_interactivity = ""all"" warnings.filterwarnings(""ignore"")",0,not_existent,"#STEP 1: Get right arrows in quiver
 
 import numpy as np
 import pandas as pd
 
 import matplotlib.pyplot as plt
 import seaborn as sns
 import plotly.express as px
 
 from sklearn.preprocessing import StandardScaler,normalize
 from sklearn.model_selection import train_test_split
 from sklearn.mixture import GaussianMixture as GMM
 from sklearn.manifold import TSNE
 
 import warnings #To hide warnings
 
 #1.1: Set the stage
 from IPython.core.interactiveshell import InteractiveShell
 from IPython.core.display import display, HTML
 
 InteractiveShell.ast_node_interactivity = ""all""
 warnings.filterwarnings(""ignore"")"
ASSIGN = pd.read_csv('path') ASSIGN.info() cc[cc.columns[cc.isna().any()]].isna().sum().to_frame().T ASSIGN.sample(5) ASSIGN.describe(),1,stream,"cc = pd.read_csv('/kaggle/input/ccdata/CC GENERAL.csv')
 cc.info()
 cc[cc.columns[cc.isna().any()]].isna().sum().to_frame().T
 cc.sample(5)
 cc.describe()"
"cc.quantile([0.75,0.8,.85,.9,.95,1])",0,execute_result,"cc.quantile([0.75,0.8,.85,.9,.95,1])"
display(HTML('<h4>There are '+str(np.sum(cc.BALANCE>cc.CREDIT_LIMIT)) +' customers in the list who have more balance than the credit limit assigned. ' +'It may be due to more payament than usage andpath<path>')),0,display_data,"display(HTML('<h4>There are '+str(np.sum(cc.BALANCE>cc.CREDIT_LIMIT))
              +' customers in the list who have more balance than the credit limit assigned. '
              +'It may be due to more payament than usage and/or continuous pre-payment.</h4>'))"
"cc.rename(columns = {col:col.lower() for col in cc.columns.values},inplace=True) sns.jointplot(cc.credit_limit,cc.minimum_payments,kind = 'kde', dropna=True)",0,execute_result,"cc.rename(columns = {col:col.lower() for col in cc.columns.values},inplace=True)
 sns.jointplot(cc.credit_limit,cc.minimum_payments,kind = 'kde', dropna=True)"
"cc.fillna(cc.median(),inplace=True) ASSIGN = cc.cust_id cc.drop(columns = ['cust_id'],inplace=True)",1,not_existent,"cc.fillna(cc.median(),inplace=True) #More outliers thus median in both cases
 cust = cc.cust_id
 cc.drop(columns = ['cust_id'],inplace=True)"
"ASSIGN = StandardScaler() X= normalize(ASSIGN.fit_transform(cc.copy())) ASSIGN = pd.DataFrame(ASSIGN,columns=cc.columns.values)",1,not_existent,"ss = StandardScaler()
 X= normalize(ss.fit_transform(cc.copy()))
 X = pd.DataFrame(X,columns=cc.columns.values)"
"ASSIGN = plt.subplots(6,3, figsize=(20, 20)) for i in range(17): ASSIGN = sns.distplot(cc[cc.columns[i]], ax=axs[ipath,i%3],kde_kws = {'bw':2}) ASSIGN = sns.despine() plt.show()",0,display_data,"fig, axs = plt.subplots(6,3, figsize=(20, 20))
 for i in range(17):
         p = sns.distplot(cc[cc.columns[i]], ax=axs[i//3,i%3],kde_kws = {'bw':2})
         p = sns.despine()
 plt.show()"
"X.boxplot(figsize = (30,25),grid=True,fontsize=25,rot=90)",0,execute_result,"X.boxplot(figsize = (30,25),grid=True,fontsize=25,rot=90)"
"plt.figure(figsize=(16,12)) ASSIGN = sns.heatmap(cc.corr(),annot=True,cmap='jet').set_title(""Correlation of credit card data\'s features"",fontsize=20) plt.show()",0,execute_result,"plt.figure(figsize=(16,12))
 p = sns.heatmap(cc.corr(),annot=True,cmap='jet').set_title(""Correlation of credit card data\'s features"",fontsize=20)
 plt.show()"
"ASSIGN = [GMM(n,random_state=0).fit(X) for n in range(1,12)] ASSIGN = pd.DataFrame({'BIC Score':[m.bic(X) for m in models], 'AIC Score': [m.aic(X) for m in ASSIGN]},index=np.arange(1,12)) ASSIGN.plot(use_index=True,title='AIC and BIC Scores for GMM wrt n_Compnents',figsize = (10,5),fontsize=12)",0,execute_result,"#Selecting correct number of components for GMM
 models = [GMM(n,random_state=0).fit(X) for n in range(1,12)]
 d = pd.DataFrame({'BIC Score':[m.bic(X) for m in models],
                   'AIC Score': [m.aic(X) for m in models]},index=np.arange(1,12))
 d.plot(use_index=True,title='AIC and BIC Scores for GMM wrt n_Compnents',figsize = (10,5),fontsize=12)"
"SETUP class GMClusters(GMM, ClusterMixin): def __init__(self, n_clusters=1, **kwargs): kwargs[""n_components""] = n_clusters kwargs['covariance_type'] = 'full' super(GMClusters, self).__init__(**kwargs) def fit(self, X): super(GMClusters, self).fit(X) self.labels_ = self.predict(X) return self ASSIGN = KElbow(GMClusters(), k=(2,12), force_model=True) ASSIGN.fit(X) ASSIGN.show()",0,execute_result,"from sklearn.base import ClusterMixin
 from yellowbrick.cluster import KElbow
 
 class GMClusters(GMM, ClusterMixin):
 
     def __init__(self, n_clusters=1, **kwargs):
         kwargs[""n_components""] = n_clusters
         kwargs['covariance_type'] = 'full'
         super(GMClusters, self).__init__(**kwargs)
 
     def fit(self, X):
         super(GMClusters, self).fit(X)
         self.labels_ = self.predict(X)
         return self 
 
 oz = KElbow(GMClusters(), k=(2,12), force_model=True)
 oz.fit(X)
 oz.show()"
CHECKPOINT ASSIGN= models[6] ASSIGN.n_init = 10 model,0,execute_result,"model= models[6]
 model.n_init = 10
 model"
ASSIGN = model.fit_predict(X) display(HTML('<b>The model has converged :<path>'+str(model.converged_))) display(HTML('<b>The model has taken iterations :<path>'+str(model.n_iter_))),0,display_data,"clusters = model.fit_predict(X)
 display(HTML('<b>The model has converged :</b>'+str(model.converged_)))
 display(HTML('<b>The model has taken iterations :</b>'+str(model.n_iter_)))"
"sns.countplot(clusters).set_title('Cluster sizes',fontsize=20)",0,execute_result,"sns.countplot(clusters).set_title('Cluster sizes',fontsize=20)"
"ASSIGN = cc.copy() ASSIGN['cluster']=clusters for c in ASSIGN: if c != 'cluster': ASSIGN= sns.FacetGrid(cc1, col='cluster',sharex=False,sharey=False) ASSIGN = grid.map(sns.distplot, c,kde_kws = {'bw':2}) plt.show()",0,display_data,"cc1 = cc.copy()
 cc1['cluster']=clusters
 for c in cc1:
     if c != 'cluster':
         grid= sns.FacetGrid(cc1, col='cluster',sharex=False,sharey=False)
         p = grid.map(sns.distplot, c,kde_kws = {'bw':2})
 plt.show()"
for i in range(7): display(HTML('<h2>Cluster'+str(i)+'<path>')) cc1[cc1.cluster == i].describe(),0,display_data,"#cc1.groupby('cluster').agg({np.min,np.max,np.mean}).T
 for i in range(7):
     display(HTML('<h2>Cluster'+str(i)+'</h2>'))
     cc1[cc1.cluster == i].describe()"
"ASSIGN = TSNE(n_components = 2) ASSIGN = tsne.fit_transform(X.copy()) plt.scatter(ASSIGN[:, 0], ASSIGN[:, 1], ASSIGN=10, ASSIGN=10, ASSIGN=5, ASSIGN=clusters )",0,execute_result,"tsne = TSNE(n_components = 2)
 tsne_out = tsne.fit_transform(X.copy())
 
 plt.scatter(tsne_out[:, 0], tsne_out[:, 1],
             marker=10,
             s=10,              # marker size
             linewidths=5,      # linewidth of marker edges
             c=clusters   # Colour as per gmm
             )"
"CHECKPOINT ASSIGN = model.score_samples(X) ASSIGN = np.percentile(density,4) cc1['cluster']=clusters cc1['Anamoly'] = ASSIGN<ASSIGN cc1",0,execute_result,"density = model.score_samples(X)
 density_threshold = np.percentile(density,4)
 cc1['cluster']=clusters
 cc1['Anamoly'] = density<density_threshold
 cc1"
"ASSIGN = cc1.melt(['Anamoly'], var_name='cols', value_name='vals') ASSIGN = sns.FacetGrid(df, row='cols', hue=""Anamoly"", palette=""Set1"",sharey=False,sharex=False,aspect=3) ASSIGN = (ASSIGN.map(sns.distplot, ""vals"", hist=True, rug=True,kde_kws = {'bw':2}).add_legend())",0,display_data,"df = cc1.melt(['Anamoly'], var_name='cols',  value_name='vals')
 
 g = sns.FacetGrid(df, row='cols', hue=""Anamoly"", palette=""Set1"",sharey=False,sharex=False,aspect=3)
 g = (g.map(sns.distplot, ""vals"", hist=True, rug=True,kde_kws = {'bw':2}).add_legend())"
"ASSIGN = X[density>=density_threshold] ASSIGN = clusters[density>=density_threshold] ASSIGN = TSNE(n_components = 2) ASSIGN = tsne.fit_transform(unanomaly) plt.figure(figsize=(15,10)) plt.scatter(ASSIGN[:, 0], ASSIGN[:, 1],marker='x',s=10, linewidths=5, ASSIGN=ASSIGN)",0,execute_result,"unanomaly = X[density>=density_threshold]
 c = clusters[density>=density_threshold]
 tsne = TSNE(n_components = 2)
 tsne_out = tsne.fit_transform(unanomaly)
 plt.figure(figsize=(15,10))
 plt.scatter(tsne_out[:, 0], tsne_out[:, 1],marker='x',s=10, linewidths=5, c=c)"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream," import os for dirname, _, filenames in os.walk('/kaggle/input'):     for filename in filenames:         print(os.path.join(dirname, filename)) "
SETUP ASSIGN = pd.read_csv('path') ASSIGN = pd.read_csv('path'),0,not_existent,"import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import seaborn as sb import matplotlib.pyplot as pl  %matplotlib inline  train = pd.read_csv('/kaggle/input/titanic/train.csv') test  = pd.read_csv('/kaggle/input/titanic/test.csv')"
test.head(),0,execute_result,test.head()
CHECKPOINT train.shape,0,execute_result,train.shape
CHECKPOINT test.shape,0,execute_result,test.shape
train.info(),0,stream,train.info()
test.info(),0,stream,test.info()
train.isnull().sum(),0,execute_result,train.isnull().sum()
test.isnull().sum(),0,execute_result,test.isnull().sum()
"def grafico(feature): ASSIGN = train[train['Survived']==1][feature].value_counts() ASSIGN = train[train['Survived']==0][feature].value_counts() ASSIGN = pd.DataFrame([survived,dead]) ASSIGN.index = ['Survived','Dead'] ASSIGN.plot(kind='bar', stacked=True, figsize=(10,5))",0,not_existent,"def grafico(feature):     survived = train[train['Survived']==1][feature].value_counts()     dead = train[train['Survived']==0][feature].value_counts()     df = pd.DataFrame([survived,dead])     df.index = ['Survived','Dead']     df.plot(kind='bar', stacked=True, figsize=(10,5))"
grafico('Sex'),0,display_data,grafico('Sex')
grafico('Pclass'),0,display_data,grafico('Pclass')
grafico('SibSp'),0,display_data,grafico('SibSp')
grafico('Parch'),0,display_data,grafico('Parch')
grafico('Embarked'),0,display_data,grafico('Embarked')
"ASSIGN = [train, test] for dataset in ASSIGN: ASSIGN = ASSIGN.str.extract(' ([A-Za-z]+)\.', expand=False)",1,not_existent,"train_test = [train, test] for dataset in train_test:     dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)"
train['Title'].value_counts(),0,execute_result,train['Title'].value_counts()
test['Title'].value_counts(),0,execute_result,test['Title'].value_counts()
"ASSIGN = {""Mr"": 0, ""Miss"": 1, ""Mrs"": 2, ""Master"": 3, ""Dr"": 3, ""Rev"": 3, ""Col"": 3, ""Major"": 3, ""Mlle"": 3, ""Ms"": 3, ""Don"": 3, ""Lady"": 3, ""Jonkheer"": 3, ""Countess"": 3, ""Mme"": 3, ""Sir"": 3, ""Capt"": 3} for dataset in train_test: ASSIGN = ASSIGN.map(title_map)",1,not_existent,"title_map = {""Mr"": 0,             ""Miss"": 1,             ""Mrs"": 2,             ""Master"": 3,             ""Dr"": 3,             ""Rev"": 3,             ""Col"": 3,             ""Major"": 3,             ""Mlle"": 3,             ""Ms"": 3,             ""Don"": 3,             ""Lady"": 3,             ""Jonkheer"": 3,             ""Countess"": 3,             ""Mme"": 3,             ""Sir"": 3,             ""Capt"": 3} for dataset in train_test:     dataset['Title'] = dataset['Title'].map(title_map)"
"test[""Title""].fillna(0, inplace=True)",1,not_existent,"test[""Title""].fillna(0, inplace=True)"
grafico('Title'),0,display_data,grafico('Title')
"train.drop('Name', axis=1, inplace=True) test.drop('Name', axis=1, inplace=True)",1,not_existent,"train.drop('Name', axis=1, inplace=True) test.drop('Name', axis=1, inplace=True)"
"ASSIGN = {""male"": 0, ""female"": 1} for dataset in train_test: ASSIGN = ASSIGN.map(sex_map)",1,not_existent,"sex_map = {""male"": 0, ""female"": 1} for dataset in train_test:     dataset['Sex'] = dataset['Sex'].map(sex_map)"
train.head(100),0,execute_result,train.head(100)
"train[""Age""].fillna(train.groupby(""Title"")[""Age""].transform(""median""), inplace=True) test[""Age""].fillna(test.groupby(""Title"")[""Age""].transform(""median""), inplace=True)",1,not_existent,"train[""Age""].fillna(train.groupby(""Title"")[""Age""].transform(""median""), inplace=True) test[""Age""].fillna(test.groupby(""Title"")[""Age""].transform(""median""), inplace=True)"
"ASSIGN = sb.FacetGrid(train, hue=""Survived"", aspect=4) ASSIGN.map(sb.kdeplot, 'Age', shade=True) ASSIGN.set(xlim=(0, train['Age'].max())) ASSIGN.add_legend() pl.show()",0,display_data,"facet = sb.FacetGrid(train, hue=""Survived"", aspect=4) facet.map(sb.kdeplot, 'Age', shade=True) facet.set(xlim=(0, train['Age'].max())) facet.add_legend() pl.show()"
"for dataset in train_test: dataset.loc[ dataset['Age'] <= 16, 'Age'] =0 dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1 dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2 dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3 dataset.loc[(dataset['Age'] > 62), 'Age'] = 4",1,not_existent,"for dataset in train_test:     dataset.loc[ dataset['Age'] <= 16, 'Age'] =0     dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1     dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2     dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3     dataset.loc[(dataset['Age'] > 62), 'Age'] = 4"
grafico('Age'),0,display_data,grafico('Age')
"Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts() Pclass2 = train[train['Pclass']==2]['Embarked'].value_counts() Pclass3 = train[train['Pclass']==3]['Embarked'].value_counts() ASSIGN = pd.DataFrame([Pclass1,Pclass2,Pclass3]) ASSIGN.index = ['1st class', '2nd class', '3rd class'] ASSIGN.plot(kind='bar', stacked=True, figsize=(10,5))",0,execute_result,"Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts() Pclass2 = train[train['Pclass']==2]['Embarked'].value_counts() Pclass3 = train[train['Pclass']==3]['Embarked'].value_counts() df = pd.DataFrame([Pclass1,Pclass2,Pclass3]) df.index = ['1st class', '2nd class', '3rd class'] df.plot(kind='bar', stacked=True, figsize=(10,5))"
for dataset in train_test: ASSIGN = ASSIGN.fillna('S'),1,not_existent,for dataset in train_test:     dataset['Embarked'] =  dataset['Embarked'].fillna('S')
"ASSIGN = {""S"": 0, ""C"": 1, ""Q"": 2} for dataset in train_test: ASSIGN = ASSIGN.map(emb_map)",1,not_existent,"emb_map = {""S"": 0,            ""C"": 1,            ""Q"": 2} for dataset in train_test:     dataset['Embarked'] = dataset['Embarked'].map(emb_map)"
"train[""Fare""].fillna(train.groupby(""Pclass"")[""Fare""].transform(""median""), inplace=True) test[""Fare""].fillna(test.groupby(""Pclass"")[""Fare""].transform(""median""), inplace=True)",1,not_existent,"train[""Fare""].fillna(train.groupby(""Pclass"")[""Fare""].transform(""median""), inplace=True) test[""Fare""].fillna(test.groupby(""Pclass"")[""Fare""].transform(""median""), inplace=True)"
"for dataset in train_test: dataset.loc[ dataset['Fare'] <= 17, 'Fare'] =0 dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1 dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2 dataset.loc[(dataset['Fare'] > 100), 'Fare'] = 3",1,not_existent,"for dataset in train_test:     dataset.loc[ dataset['Fare'] <= 17, 'Fare'] =0     dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1     dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2     dataset.loc[(dataset['Fare'] > 100), 'Fare'] = 3"
train.Cabin.value_counts(),0,execute_result,train.Cabin.value_counts()
for dataset in train_test: ASSIGN = ASSIGN.str[:1],1,not_existent,for dataset in train_test:     dataset['Cabin'] = dataset['Cabin'].str[:1]
"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts() Pclass2 = train[train['Pclass']==2]['Cabin'].value_counts() Pclass3 = train[train['Pclass']==3]['Cabin'].value_counts() ASSIGN = pd.DataFrame([Pclass1,Pclass2,Pclass3]) ASSIGN.index = ['1st class', '2nd class', '3rd class'] ASSIGN.plot(kind='bar', stacked=True, figsize=(10,5))",1,execute_result,"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts() Pclass2 = train[train['Pclass']==2]['Cabin'].value_counts() Pclass3 = train[train['Pclass']==3]['Cabin'].value_counts() df = pd.DataFrame([Pclass1,Pclass2,Pclass3]) df.index = ['1st class', '2nd class', '3rd class'] df.plot(kind='bar', stacked=True, figsize=(10,5))"
"ASSIGN = {""A"": 0, ""B"": 0.4, ""C"": 0.8, ""D"": 1.2, ""E"": 1.6, ""F"": 2.0, ""G"": 2.4, ""T"": 2.8} for dataset in train_test: ASSIGN = ASSIGN.map(cab_map)",1,not_existent,"cab_map = {""A"": 0,            ""B"": 0.4,            ""C"": 0.8,            ""D"": 1.2,            ""E"": 1.6,            ""F"": 2.0,            ""G"": 2.4,            ""T"": 2.8} for dataset in train_test:     dataset['Cabin'] = dataset['Cabin'].map(cab_map)"
"train[""Cabin""].fillna(train.groupby(""Pclass"")[""Cabin""].transform(""median""), inplace=True) test[""Cabin""].fillna(test.groupby(""Pclass"")[""Cabin""].transform(""median""), inplace=True)",1,not_existent,"train[""Cabin""].fillna(train.groupby(""Pclass"")[""Cabin""].transform(""median""), inplace=True) test[""Cabin""].fillna(test.groupby(""Pclass"")[""Cabin""].transform(""median""), inplace=True)"
ASSIGN = ASSIGN + ASSIGN + 1 ASSIGN = ASSIGN + ASSIGN + 1,1,not_existent,"train[""FamilySize""] = train[""SibSp""] + train[""Parch""]  + 1 test[""FamilySize""] = test[""SibSp""] + test[""Parch""]  + 1"
"ASSIGN = sb.FacetGrid(train, hue=""Survived"", aspect=4) ASSIGN.map(sb.kdeplot, 'FamilySize', shade= True) ASSIGN.set(xlim=(0, train['FamilySize'].max())) ASSIGN.add_legend()",0,execute_result,"facet = sb.FacetGrid(train, hue=""Survived"", aspect=4) facet.map(sb.kdeplot, 'FamilySize', shade= True) facet.set(xlim=(0, train['FamilySize'].max())) facet.add_legend()"
"ASSIGN = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4} for dataset in train_test: ASSIGN = ASSIGN.map(family_map)",1,not_existent,"family_map = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4} for dataset in train_test:     dataset['FamilySize'] = dataset['FamilySize'].map(family_map)"
"ASSIGN = ['Ticket', 'SibSp', 'Parch'] ASSIGN = ASSIGN.drop(features_drop, axis=1) ASSIGN = ASSIGN.drop(features_drop, axis=1) ASSIGN = ASSIGN.drop(['PassengerId'], axis=1) ASSIGN = train.drop('Survived', axis=1) ASSIGN = train['Survived'] train_data.shape, target.shape",1,execute_result,"features_drop = ['Ticket', 'SibSp', 'Parch'] train = train.drop(features_drop, axis=1) test = test.drop(features_drop, axis=1) train = train.drop(['PassengerId'], axis=1) train_data = train.drop('Survived', axis=1) target = train['Survived']  train_data.shape, target.shape"
train_data.head(10),0,execute_result,train_data.head(10)
SETUP,0,not_existent,from sklearn.neighbors import KNeighborsClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import GaussianNB from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis from sklearn import linear_model from sklearn.svm import SVC  import numpy as np
"SETUP ASSIGN = KFold(n_splits=10, shuffle=True, random_state=0)",1,not_existent,"from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score k_fold = KFold(n_splits=10, shuffle=True, random_state=0)"
"CHECKPOINT ASSIGN = KNeighborsClassifier(n_neighbors = 13) ASSIGN = 'accuracy' ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring) print(ASSIGN) round(np.mean(ASSIGN)*100, 2)",0,stream,"clf = KNeighborsClassifier(n_neighbors = 13) scoring = 'accuracy' score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring) print(score) round(np.mean(score)*100, 2)"
"CHECKPOINT ASSIGN = DecisionTreeClassifier() ASSIGN = 'accuracy' ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring) print(ASSIGN) round(np.mean(ASSIGN)*100, 2)",0,stream,"clf = DecisionTreeClassifier() scoring = 'accuracy' score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring) print(score) round(np.mean(score)*100, 2)"
"CHECKPOINT ASSIGN = RandomForestClassifier(n_estimators=13) ASSIGN = 'accuracy' ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring) print(ASSIGN) round(np.mean(ASSIGN)*100, 2)",0,stream,"clf = RandomForestClassifier(n_estimators=13) scoring = 'accuracy' score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring) print(score) round(np.mean(score)*100, 2)"
"CHECKPOINT ASSIGN = GaussianNB() ASSIGN = 'accuracy' ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring) print(ASSIGN) round(np.mean(ASSIGN)*100, 2)",0,stream,"clf = GaussianNB() scoring = 'accuracy' score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring) print(score) round(np.mean(score)*100, 2)"
"CHECKPOINT ASSIGN = SVC() ASSIGN = 'accuracy' ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring) print(ASSIGN) round(np.mean(ASSIGN)*100,2)",0,stream,"clf = SVC() scoring = 'accuracy' score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring) print(score) round(np.mean(score)*100,2)"
"CHECKPOINT ASSIGN = QuadraticDiscriminantAnalysis() ASSIGN = 'accuracy' ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring) print(ASSIGN) round(np.mean(ASSIGN)*100,2)",0,stream,"clf = QuadraticDiscriminantAnalysis() scoring = 'accuracy' score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring) print(score) round(np.mean(score)*100,2)"
"CHECKPOINT ASSIGN = linear_model.LinearRegression() ASSIGN = 'accuracy' ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1) print(ASSIGN) round(np.mean(ASSIGN)*100,2)",0,stream,"clf = linear_model.LinearRegression() scoring = 'accuracy' score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1) print(score) round(np.mean(score)*100,2)"
"CHECKPOINT ASSIGN = SVC() ASSIGN.fit(train_data, target) ASSIGN = test.drop(""PassengerId"", axis=1).copy() ASSIGN = clf.predict(test_data) ASSIGN = pd.read_csv('path') ASSIGN = clf.predict(test_data2) print(ASSIGN)",1,stream,"clf = SVC()  clf.fit(train_data, target)  test_data = test.drop(""PassengerId"", axis=1).copy()  prediction = clf.predict(test_data)  test_data2 = pd.read_csv('/kaggle/input/testes/teste.csv') prediction2 = clf.predict(test_data2)  print(prediction2)"
"ASSIGN = pd.DataFrame({ ""PassengerId"": test[""PassengerId""], ""Survived"": prediction }) ASSIGN.to_csv('path', index=False)",0,not_existent,"submission = pd.DataFrame({         ""PassengerId"": test[""PassengerId""],         ""Survived"": prediction     })  submission.to_csv('/kaggle/working/submission.csv', index=False)"
ASSIGN = pd.read_csv('path') ASSIGN.head(),0,execute_result,submission = pd.read_csv('/kaggle/working/submission.csv') submission.head()
SETUP warnings.filterwarnings('ignore'),0,not_existent,"import pandas as pd
 import matplotlib.pyplot as plt
 import seaborn as sns
 import numpy as np
 from scipy.stats import norm
 from sklearn.preprocessing import StandardScaler
 from sklearn.mixture import GaussianMixture
 import scipy.stats as st
 import warnings
 warnings.filterwarnings('ignore')
 %matplotlib inline
 import missingno as msno"
SETUP ASSIGN = pd.read_csv(TRAIN_DATASET_PATH) ASSIGN.head(),0,execute_result,"TRAIN_DATASET_PATH = '/kaggle/input/realestatepriceprediction/train.csv'
 
 df_train = pd.read_csv(TRAIN_DATASET_PATH)
 
 df_train.head()"
CHECKPOINT df_train.dtypes,0,execute_result,df_train.dtypes
df_train.describe(),0,execute_result,df_train.describe()
msno.matrix(df_train.sample(250));,0,display_data,msno.matrix(df_train.sample(250));
"ASSIGN = df_train.isnull().sum().sort_values(ascending=False) ASSIGN = (df_train.isnull().sum()path().count()).sort_values(ascending=False) ASSIGN = pd.concat([total, percentage], axis=1, keys=['Total', 'Persent']) ASSIGN.head()",1,execute_result,"total = df_train.isnull().sum().sort_values(ascending=False)
 percentage = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)
 missing_data = pd.concat([total, percentage], axis=1, keys=['Total', 'Persent'])
 missing_data.head()"
"ASSIGN = df_train.corr() ASSIGN = plt.subplots(figsize=(12, 9)) sns.heatmap(ASSIGN, vmax=.8, annot=True, fmt=' .2f', square=True);",0,display_data,"corrmat = df_train.corr()
 f, ax = plt.subplots(figsize=(12, 9))
 sns.heatmap(corrmat, vmax=.8, annot=True, fmt=' .2f', square=True);"
"ASSIGN = 10 ASSIGN = corrmat.nlargest(k, 'Price')['Price'].index sns.set(font_scale=1.5) ASSIGN = sns.heatmap(df_train[cols].corr(), annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values);",0,display_data,"k = 10
 cols = corrmat.nlargest(k, 'Price')['Price'].index
 sns.set(font_scale=1.5)
 hm = sns.heatmap(df_train[cols].corr(), annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values);"
"sns.set() ASSIGN = ['Price','DistrictId', 'Rooms', 'Square', 'Social_3', 'Floor', 'Helthcare_2', 'Shops_1', 'Healthcare_1'] sns.pairplot(df_train[ASSIGN], size = 2.8) plt.show()",0,display_data,"sns.set()
 cols = ['Price','DistrictId', 'Rooms', 'Square', 'Social_3', 'Floor', 'Helthcare_2', 'Shops_1', 'Healthcare_1']
 sns.pairplot(df_train[cols], size = 2.8)
 plt.show()"
"ASSIGN = ['Price','DistrictId', 'Rooms', 'Square', 'LifeSquare', 'Social_1', 'Shops_1'] ASSIGN = pd.concat([df_train[cols], pd.Series(np.int8(df_train['Rooms'] == 0), name='flag')], axis=1) ASSIGN.loc[ASSIGN['Rooms'] == 0]",1,execute_result,"cols = ['Price','DistrictId', 'Rooms', 'Square', 'LifeSquare', 'Social_1', 'Shops_1']
 df_train_temp = pd.concat([df_train[cols], pd.Series(np.int8(df_train['Rooms'] == 0), name='flag')], axis=1)
 df_train_temp.loc[df_train_temp['Rooms'] == 0]"
"sns.pairplot(df_train_temp, size = 2.5, hue='flag') plt.show()",0,display_data,"sns.pairplot(df_train_temp, size = 2.5, hue='flag')
 plt.show()"
"plt.figure(figsize = (45, 10)) sns.countplot(x = 'DistrictId', data = df_train) ASSIGN = plt.xticks(rotation=90)",0,display_data,"plt.figure(figsize = (45, 10))
 sns.countplot(x = 'DistrictId', data = df_train)
 xt = plt.xticks(rotation=90)"
"ASSIGN = df_train['DistrictId'].value_counts() ASSIGN = district[district > 50] ASSIGN = district[district <= 50] ASSIGN = {'count_districts_gr_50': district_gr_50.count(), 'count_districts_ls_50': district_ls_50.count()} ASSIGN = {'pop_districts_gr_50': district_gr_50.sum(), 'pop_districts_ls_50': district_ls_50.sum()} ASSIGN = plt.subplots(1, 2) fig.set_size_inches(15, 8) fig.subplots_adjust(wspace=0.3, hspace=0.3) ax[0].set_title('Districts count') sns.barplot(list(ASSIGN.keys()), list(ASSIGN.values()), ax=ax[0]) ax[1].set_title('Districts pop') sns.barplot(list(ASSIGN.keys()), list(ASSIGN.values()), ax=ax[1]) plt.show()",0,display_data,"district = df_train['DistrictId'].value_counts()
 district_gr_50 = district[district > 50]
 district_ls_50 = district[district <= 50]
 districts = {'count_districts_gr_50': district_gr_50.count(), 'count_districts_ls_50': district_ls_50.count()}
 districts_2 = {'pop_districts_gr_50': district_gr_50.sum(), 'pop_districts_ls_50': district_ls_50.sum()}
 
 fig, ax = plt.subplots(1, 2)
 
 fig.set_size_inches(15, 8)
 fig.subplots_adjust(wspace=0.3, hspace=0.3)
 
 #plt.figure(figsize=(12,8))    
 ax[0].set_title('Districts count')
 sns.barplot(list(districts.keys()), list(districts.values()), ax=ax[0])
 
 ax[1].set_title('Districts pop')
 sns.barplot(list(districts_2.keys()), list(districts_2.values()), ax=ax[1])
 
 plt.show()"
sns.distplot(df_train['Square']);,0,display_data,sns.distplot(df_train['Square']);
"sns.distplot(df_train.loc[df_train['Square'] < 200,'Square']) plt.plot([25 for x in range(330)], [xpath(330)], ls='--', c='r') plt.show()",0,display_data,"sns.distplot(df_train.loc[df_train['Square'] < 200,'Square'])
 plt.plot([25 for x in range(330)], [x/10000 for x in range(330)], ls='--', c='r')
 plt.show()"
"df_train.loc[df_train['Square'] < 25, 'Square'].count()",0,execute_result,"df_train.loc[df_train['Square'] < 25, 'Square'].count()"
"df_train.loc[df_train['Rooms'] > 5, ['DistrictId', 'Square', 'KitchenSquare', 'Rooms', 'Price']]",0,execute_result,"df_train.loc[df_train['Rooms'] > 5, ['DistrictId', 'Square', 'KitchenSquare', 'Rooms', 'Price']]"
"df_train.loc[df_train['Rooms'] == 0, ['DistrictId', 'Square', 'KitchenSquare', 'Rooms', 'Price']]",1,execute_result,"df_train.loc[df_train['Rooms'] == 0, ['DistrictId', 'Square', 'KitchenSquare', 'Rooms', 'Price']]"
"plt.scatter(df_train['Square'], df_train['KitchenSquare']) plt.plot([x for x in range(120)], [y for y in range(120)], c = 'r') plt.plot([y for y in range(200)], [ 0 for x in range(200)], c = 'g') plt.xlabel('Square') plt.ylabel('KitchenSquare') plt.show()",0,display_data,"plt.scatter(df_train['Square'], df_train['KitchenSquare'])
 plt.plot([x for x in range(120)], [y for y in range(120)], c = 'r')
 plt.plot([y for y in range(200)], [ 0 for x in range(200)], c = 'g')
 plt.xlabel('Square')
 plt.ylabel('KitchenSquare')
 plt.show()"
"ASSIGN = df_train['KitchenSquare'] < df_train['Square'] + 0.5*df_train['Square'].std() plt.scatter(df_train.loc[ASSIGN, 'Square'], df_train.loc[ASSIGN, 'KitchenSquare']) plt.plot([x for x in range(120)], [y for y in range(120)], c = 'r') plt.plot([y for y in range(300)], [ 0 for x in range(300)], c = 'g') plt.xlabel('Square') plt.ylabel('KitchenSquare') plt.show()",0,display_data,"cond = df_train['KitchenSquare'] < df_train['Square'] + 0.5*df_train['Square'].std()
 plt.scatter(df_train.loc[cond, 'Square'], df_train.loc[cond, 'KitchenSquare'])
 plt.plot([x for x in range(120)], [y for y in range(120)], c = 'r')
 plt.plot([y for y in range(300)], [ 0 for x in range(300)], c = 'g')
 plt.xlabel('Square')
 plt.ylabel('KitchenSquare')
 plt.show()"
"ASSIGN = (df_train['KitchenSquare'] >= 3) & (abs(df_train['Square'] - df_train['KitchenSquare']) > 10) &\ (df_train['KitchenSquare'] < 50) & (df_train['Square'] < 200) ASSIGN = df_train.loc[cond, ['Square', 'KitchenSquare']] ASSIGN = sns.jointplot(temp['Square'], temp['KitchenSquare'], kind='reg') plt.plot(np.arange(0, 40), np.arange(0, 40), color = 'red', linestyle='--') ASSIGN.fig.set_figwidth(8) ASSIGN.fig.set_figheight(8)",0,display_data,"cond = (df_train['KitchenSquare'] >= 3) & (abs(df_train['Square'] - df_train['KitchenSquare']) > 10) &\
                     (df_train['KitchenSquare'] < 50) &  (df_train['Square'] < 200)
 temp = df_train.loc[cond, ['Square', 'KitchenSquare']]
 grid = sns.jointplot(temp['Square'], temp['KitchenSquare'], kind='reg')
 plt.plot(np.arange(0, 40), np.arange(0, 40), color = 'red', linestyle='--')
 grid.fig.set_figwidth(8)
 grid.fig.set_figheight(8)"
"ASSIGN = ~df_train['LifeSquare'].isna() plt.plot([x for x in range(600)], [y for y in range(600)], c = 'r') sns.scatterplot(df_train.loc[ASSIGN, 'Square'], df_train.loc[ASSIGN, 'LifeSquare']);",0,display_data,"cond = ~df_train['LifeSquare'].isna()
 plt.plot([x for x in range(600)], [y for y in range(600)], c = 'r')
 sns.scatterplot(df_train.loc[cond, 'Square'], df_train.loc[cond, 'LifeSquare']);"
"ASSIGN = (~df_train['LifeSquare'].isna()) & (df_train['LifeSquare'] < df_train['LifeSquare'].quantile(q = 0.999)) & \ (df_train['Square'] < df_train['Square'].quantile(q = 0.999)) ASSIGN = sns.jointplot(df_train.loc[cond, 'Square'], df_train.loc[cond, 'LifeSquare'], kind='reg') plt.plot(np.arange(0, 200), np.arange(0, 200), color = 'red', linestyle='--') ASSIGN.fig.set_figwidth(8) ASSIGN.fig.set_figheight(8)",0,display_data,"cond = (~df_train['LifeSquare'].isna()) & (df_train['LifeSquare'] < df_train['LifeSquare'].quantile(q = 0.999)) & \
                                         (df_train['Square'] < df_train['Square'].quantile(q = 0.999))
 grid = sns.jointplot(df_train.loc[cond, 'Square'], df_train.loc[cond, 'LifeSquare'], kind='reg')
 plt.plot(np.arange(0, 200), np.arange(0, 200), color = 'red', linestyle='--')
 grid.fig.set_figwidth(8)
 grid.fig.set_figheight(8)"
df_train['HouseYear'].unique(),0,execute_result,df_train['HouseYear'].unique()
"df_train.loc[(df_train['HouseYear'] == 20052011) | (df_train['HouseYear'] == 4968), 'HouseYear'].value_counts()",1,execute_result,"df_train.loc[(df_train['HouseYear'] == 20052011) | (df_train['HouseYear'] == 4968), 'HouseYear'].value_counts()"
"ASSIGN = ['g' if i == 0.0 else 'b' for i in df_train.loc[df_train['HouseFloor'] < 60, 'HouseFloor']] ASSIGN.count('g')",1,execute_result,"colors = ['g' if i == 0.0 else 'b' for i in df_train.loc[df_train['HouseFloor'] < 60, 'HouseFloor']]
 colors.count('g')"
"plt.scatter(df_train.loc[df_train['HouseFloor'] < 60, 'HouseFloor'], df_train.loc[df_train['HouseFloor'] < 60, 'Floor'], c=colors) plt.plot([x for x in range(40)], [y for y in range(40)], c = 'r') plt.plot([x for x in range(40)], [y + 2 for y in range(40)], c = 'black') plt.xlabel('HouseFloor') plt.ylabel('Floor') plt.show()",0,display_data,"
 plt.scatter(df_train.loc[df_train['HouseFloor'] < 60, 'HouseFloor'], df_train.loc[df_train['HouseFloor'] < 60, 'Floor'], c=colors)
 
 plt.plot([x for x in range(40)], [y for y in range(40)], c = 'r')
 plt.plot([x for x in range(40)], [y + 2 for y in range(40)], c = 'black')
 plt.xlabel('HouseFloor')
 plt.ylabel('Floor')
 plt.show()"
"sns.scatterplot(df_train.loc[~df_train['Healthcare_1'].isna(), 'Healthcare_1'], df_train.loc[~df_train['Healthcare_1'].isna(), 'DistrictId']);",0,display_data,"sns.scatterplot(df_train.loc[~df_train['Healthcare_1'].isna(), 'Healthcare_1'], df_train.loc[~df_train['Healthcare_1'].isna(), 'DistrictId']);"
"plt.figure(figsize=(35, 8)) ASSIGN = ~df_train['Healthcare_1'].isna() ASSIGN = sns.boxplot(df_train.loc[cond, 'DistrictId'], df_train.loc[cond, 'Healthcare_1']) ASSIGN.set_xticklabels(ASSIGN.get_xticklabels(), rotation=90) plt.xlabel('DistrictId') plt.ylabel('Healthcare_1') plt.show()",0,display_data,"plt.figure(figsize=(35, 8))
 cond = ~df_train['Healthcare_1'].isna()
 
 s = sns.boxplot(df_train.loc[cond, 'DistrictId'], df_train.loc[cond, 'Healthcare_1'])
 s.set_xticklabels(s.get_xticklabels(), rotation=90)
 
 plt.xlabel('DistrictId')
 plt.ylabel('Healthcare_1')
 plt.show()"
df_train['Price'].describe(),0,execute_result,df_train['Price'].describe()
CHECKPOINT print( % df_train['Price'].skew()) print( % df_train['Price'].kurt()),0,stream,"print(""Skewness: %f"" % df_train['Price'].skew())
 print(""Kurtosis: %f"" % df_train['Price'].kurt())"
"ASSIGN = df_train['Price'] ASSIGN = plt.subplots(1, 2) fig.set_size_inches(14, 6) fig.subplots_adjust(wspace=0.3, hspace=0.3) sns.distplot(ASSIGN, kde=False, fit=st.norm, ax=ax[0]) sns.distplot(pd.Series(np.log(ASSIGN), name ='LogPrice'), kde=False, fit=st.norm, ax=ax[1]) plt.show()",0,display_data,"y = df_train['Price']
 
 fig, ax = plt.subplots(1, 2)
 
 fig.set_size_inches(14, 6)
 fig.subplots_adjust(wspace=0.3, hspace=0.3)
 
 sns.distplot(y, kde=False, fit=st.norm, ax=ax[0])
 
 sns.distplot(pd.Series(np.log(y), name ='LogPrice'), kde=False, fit=st.norm, ax=ax[1])
 
 plt.show()"
"ASSIGN = df_train['Price'] ASSIGN = plt.subplots(1, 2) fig.set_size_inches(14, 6) fig.subplots_adjust(wspace=0.3, hspace=0.3) sns.distplot(ASSIGN, kde=False, fit=st.johnsonsu, ax=ax[0]) ASSIGN = st.johnsonsu.fit(y) ASSIGN = (y-params[2])path[3] ASSIGN = params[0] + params[1]*np.log(t + np.sqrt(np.power(t, 2) + 1)) sns.distplot(pd.Series(ASSIGN, name ='TransformedToNormalPrice'), fit=st.norm, ax=ax[1]) plt.show()",0,display_data,"y = df_train['Price']
 
 fig, ax = plt.subplots(1, 2)
 
 fig.set_size_inches(14, 6)
 fig.subplots_adjust(wspace=0.3, hspace=0.3)
 
 sns.distplot(y, kde=False, fit=st.johnsonsu, ax=ax[0])
 
 #      
 params = st.johnsonsu.fit(y)
 t = (y-params[2])/params[3]
 y_norm = params[0] + params[1]*np.log(t + np.sqrt(np.power(t, 2) + 1))
 
 sns.distplot(pd.Series(y_norm, name ='TransformedToNormalPrice'), fit=st.norm, ax=ax[1])
 
 plt.show()"
"ASSIGN = params[2]*np.sinh((y_norm - params[0])path[1]) + params[3] sns.distplot(ASSIGN, fit=st.johnsonsu) plt.show()",0,display_data,"y_back_to_Johnson = params[2]*np.sinh((y_norm - params[0])/params[1]) + params[3]
 
 sns.distplot(y_back_to_Johnson, fit=st.johnsonsu)
 
 plt.show()"
"SETUP CHECKPOINT warnings.filterwarnings(""ignore"") for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"
 
 import numpy as np 
 import pandas as pd 
 import matplotlib.pyplot as plt
 from collections import Counter
 import warnings
 warnings.filterwarnings(""ignore"")
 
 
 
 import os
 for dirname, _, filenames in os.walk('/kaggle/input'):
     for filename in filenames:
         print(os.path.join(dirname, filename))
 
"
"ASSIGN = pd.read_csv(""path"") ASSIGN.info()",0,stream,"data = pd.read_csv(""/kaggle/input/fish-market/Fish.csv"")
 data.info()"
CHECKPOINT ASSIGN = data.isnull().sum().sum() data.columns,1,execute_result,"x = data.isnull().sum().sum()
 data.columns"
ASSIGN = data.Species Counter(ASSIGN),1,execute_result,"species_list = data.Species
 Counter(species_list)"
"ASSIGN = ""Perch"" , ""Bream"" , ""Roach"" , ""Pike"" , ""Smelt"" , ""Parkki"" , ""Whitefish"" ASSIGN = [56,35,20,17,14,11,6] ASSIGN = (0,0,0,0,0,0,0) fig1,ax1 = plt.subplots() ax1.pie(ASSIGN, ASSIGN=ASSIGN, ASSIGN=ASSIGN, autopct='%1.1f%%',shadow=True, startangle=90) ax1.axis('equal') plt.show()",0,display_data,"labels = ""Perch"" , ""Bream"" , ""Roach"" , ""Pike"" , ""Smelt"" , ""Parkki"" , ""Whitefish""
 sizes = [56,35,20,17,14,11,6]
 explode = (0,0,0,0,0,0,0)
 fig1,ax1 = plt.subplots()
 ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)
 ax1.axis('equal')
 plt.show()"
"plt.scatter(data.index[data.Species == ""Bream""] , data.Weight[data.Species == ""Bream""],c=""red"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Perch""] , data.Weight[data.Species == ""Perch""],c=""aqua"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Roach""] , data.Weight[data.Species == ""Roach""],c=""orange"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Pike""] , data.Weight[data.Species == ""Pike""],c=""purple"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Smelt""] , data.Weight[data.Species == ""Smelt""],c=""black"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Parkki""] , data.Weight[data.Species == ""Parkki""],c=""green"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Whitefish""] , data.Weight[data.Species == ""Whitefish""],c=""brown"" , alpha = 0.5) plt.xlabel(""index of Spices"") plt.ylabel(""weight of fish in Gram g"") plt.grid()",0,display_data,"plt.scatter(data.index[data.Species == ""Bream""] , data.Weight[data.Species == ""Bream""],c=""red"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Perch""] , data.Weight[data.Species == ""Perch""],c=""aqua"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Roach""] , data.Weight[data.Species == ""Roach""],c=""orange"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Pike""] , data.Weight[data.Species == ""Pike""],c=""purple"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Smelt""] , data.Weight[data.Species == ""Smelt""],c=""black"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Parkki""] , data.Weight[data.Species == ""Parkki""],c=""green"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Whitefish""] , data.Weight[data.Species == ""Whitefish""],c=""brown"" , alpha = 0.5)
 plt.xlabel(""index of Spices"")
 plt.ylabel(""weight of fish in Gram g"")
 plt.grid()"
"plt.scatter(data.index[data.Species == ""Bream""] , data.Length1[data.Species == ""Bream""],c=""red"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Perch""] , data.Length1[data.Species == ""Perch""],c=""aqua"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Roach""] , data.Length1[data.Species == ""Roach""],c=""orange"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Pike""] , data.Length1[data.Species == ""Pike""],c=""purple"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Smelt""] , data.Length1[data.Species == ""Smelt""],c=""black"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Parkki""] , data.Length1[data.Species == ""Parkki""],c=""green"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Whitefish""] , data.Length1[data.Species == ""Whitefish""],c=""brown"" , alpha = 0.5) plt.xlabel(""index of Spices"") plt.ylabel(""vertical length in cm"") plt.grid()",0,display_data,"plt.scatter(data.index[data.Species == ""Bream""] , data.Length1[data.Species == ""Bream""],c=""red"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Perch""] , data.Length1[data.Species == ""Perch""],c=""aqua"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Roach""] , data.Length1[data.Species == ""Roach""],c=""orange"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Pike""] , data.Length1[data.Species == ""Pike""],c=""purple"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Smelt""] , data.Length1[data.Species == ""Smelt""],c=""black"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Parkki""] , data.Length1[data.Species == ""Parkki""],c=""green"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Whitefish""] , data.Length1[data.Species == ""Whitefish""],c=""brown"" , alpha = 0.5)
 plt.xlabel(""index of Spices"")
 plt.ylabel(""vertical length in cm"")
 plt.grid()"
"plt.scatter(data.index[data.Species == ""Bream""] , data.Length2[data.Species == ""Bream""],c=""red"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Perch""] , data.Length2[data.Species == ""Perch""],c=""aqua"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Roach""] , data.Length2[data.Species == ""Roach""],c=""orange"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Pike""] , data.Length2[data.Species == ""Pike""],c=""purple"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Smelt""] , data.Length2[data.Species == ""Smelt""],c=""black"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Parkki""] , data.Length2[data.Species == ""Parkki""],c=""green"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Whitefish""] , data.Length2[data.Species == ""Whitefish""],c=""brown"" , alpha = 0.5) plt.xlabel(""index of Spices"") plt.ylabel(""diagonal length in cm"") plt.grid()",0,display_data,"plt.scatter(data.index[data.Species == ""Bream""] , data.Length2[data.Species == ""Bream""],c=""red"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Perch""] , data.Length2[data.Species == ""Perch""],c=""aqua"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Roach""] , data.Length2[data.Species == ""Roach""],c=""orange"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Pike""] , data.Length2[data.Species == ""Pike""],c=""purple"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Smelt""] , data.Length2[data.Species == ""Smelt""],c=""black"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Parkki""] , data.Length2[data.Species == ""Parkki""],c=""green"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Whitefish""] , data.Length2[data.Species == ""Whitefish""],c=""brown"" , alpha = 0.5)
 plt.xlabel(""index of Spices"")
 plt.ylabel(""diagonal length in cm"")
 plt.grid()"
"plt.scatter(data.index[data.Species == ""Bream""] , data.Length3[data.Species == ""Bream""],c=""red"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Perch""] , data.Length3[data.Species == ""Perch""],c=""aqua"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Roach""] , data.Length3[data.Species == ""Roach""],c=""orange"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Pike""] , data.Length3[data.Species == ""Pike""],c=""purple"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Smelt""] , data.Length3[data.Species == ""Smelt""],c=""black"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Parkki""] , data.Length3[data.Species == ""Parkki""],c=""green"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Whitefish""] , data.Length3[data.Species == ""Whitefish""],c=""brown"" , alpha = 0.5) plt.xlabel(""index of Spices"") plt.ylabel(""cross length in cm"") plt.grid()",0,display_data,"plt.scatter(data.index[data.Species == ""Bream""] , data.Length3[data.Species == ""Bream""],c=""red"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Perch""] , data.Length3[data.Species == ""Perch""],c=""aqua"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Roach""] , data.Length3[data.Species == ""Roach""],c=""orange"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Pike""] , data.Length3[data.Species == ""Pike""],c=""purple"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Smelt""] , data.Length3[data.Species == ""Smelt""],c=""black"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Parkki""] , data.Length3[data.Species == ""Parkki""],c=""green"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Whitefish""] , data.Length3[data.Species == ""Whitefish""],c=""brown"" , alpha = 0.5)
 plt.xlabel(""index of Spices"")
 plt.ylabel(""cross length in cm"")
 plt.grid()"
"plt.scatter(data.index[data.Species == ""Bream""] , data.Height[data.Species == ""Bream""],c=""red"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Perch""] , data.Height[data.Species == ""Perch""],c=""aqua"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Roach""] , data.Height[data.Species == ""Roach""],c=""orange"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Pike""] , data.Height[data.Species == ""Pike""],c=""purple"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Smelt""] , data.Height[data.Species == ""Smelt""],c=""black"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Parkki""] , data.Height[data.Species == ""Parkki""],c=""green"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Whitefish""] , data.Height[data.Species == ""Whitefish""],c=""brown"" , alpha = 0.5) plt.xlabel(""index of Spices"") plt.ylabel(""height in cm"") plt.grid()",0,display_data,"plt.scatter(data.index[data.Species == ""Bream""] , data.Height[data.Species == ""Bream""],c=""red"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Perch""] , data.Height[data.Species == ""Perch""],c=""aqua"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Roach""] , data.Height[data.Species == ""Roach""],c=""orange"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Pike""] , data.Height[data.Species == ""Pike""],c=""purple"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Smelt""] , data.Height[data.Species == ""Smelt""],c=""black"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Parkki""] , data.Height[data.Species == ""Parkki""],c=""green"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Whitefish""] , data.Height[data.Species == ""Whitefish""],c=""brown"" , alpha = 0.5)
 plt.xlabel(""index of Spices"")
 plt.ylabel(""height in cm"")
 plt.grid()"
"plt.scatter(data.index[data.Species == ""Bream""] , data.Width[data.Species == ""Bream""],c=""red"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Perch""] , data.Width[data.Species == ""Perch""],c=""aqua"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Roach""] , data.Width[data.Species == ""Roach""],c=""orange"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Pike""] , data.Width[data.Species == ""Pike""],c=""purple"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Smelt""] , data.Width[data.Species == ""Smelt""],c=""black"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Parkki""] , data.Width[data.Species == ""Parkki""],c=""green"" , alpha = 0.5) plt.scatter(data.index[data.Species == ""Whitefish""] , data.Width[data.Species == ""Whitefish""],c=""brown"" , alpha = 0.5) plt.xlabel(""index of Spices"") plt.ylabel(""diagonal width in cm"") plt.grid()",0,display_data,"plt.scatter(data.index[data.Species == ""Bream""] , data.Width[data.Species == ""Bream""],c=""red"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Perch""] , data.Width[data.Species == ""Perch""],c=""aqua"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Roach""] , data.Width[data.Species == ""Roach""],c=""orange"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Pike""] , data.Width[data.Species == ""Pike""],c=""purple"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Smelt""] , data.Width[data.Species == ""Smelt""],c=""black"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Parkki""] , data.Width[data.Species == ""Parkki""],c=""green"" , alpha = 0.5)
 plt.scatter(data.index[data.Species == ""Whitefish""] , data.Width[data.Species == ""Whitefish""],c=""brown"" , alpha = 0.5)
 plt.xlabel(""index of Spices"")
 plt.ylabel(""diagonal width in cm"")
 plt.grid()"
"SETUP ASSIGN = data.Species ASSIGN = data.drop([""Species""],axis = 1) ASSIGN = train_test_split(x,y,test_size = 0.2,random_state = 42)",1,not_existent,"y = data.Species
 x = data.drop([""Species""],axis = 1)
 from sklearn.model_selection import train_test_split
 x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)"
"SETUP ASSIGN = 0.0 ASSIGN = 1 ASSIGN = [] for each in range(1,100): ASSIGN = KNeighborsClassifier(n_neighbors = each) ASSIGN.fit(x_train,y_train) ASSIGN.append(ASSIGN.score(x_test,y_test)) if (ASSIGN < ASSIGN.score(x_test,y_test) ): ASSIGN = knn.score(x_test,y_test) ASSIGN = ASSIGN+1 plt.plot(ASSIGN,color = ""purple"" , alpha = 1 ) plt.grid()",0,display_data,"knn_score = 0.0
 highest_indx = 1
 from sklearn.neighbors import KNeighborsClassifier
 score_list = []
 for each in range(1,100):
     knn = KNeighborsClassifier(n_neighbors = each)
     knn.fit(x_train,y_train)
     score_list.append(knn.score(x_test,y_test))
     if (knn_score < knn.score(x_test,y_test) ):
         knn_score = knn.score(x_test,y_test)
         highest_indx = highest_indx+1
         
 plt.plot(score_list,color = ""purple"" , alpha = 1 )    
 plt.grid()         "
"CHECKPOINT print(,knn_score)",0,stream,"print(""KNN Max Accuracy : "",knn_score)"
"SETUP CHECKPOINT ASSIGN = LogisticRegression() ASSIGN.fit(x_train,y_train) ASSIGN = lr.score(x_test,y_test) print(,ASSIGN)",0,stream,"from sklearn.linear_model import LogisticRegression
 lr = LogisticRegression()
 lr.fit(x_train,y_train)
 lr_score = lr.score(x_test,y_test)
 print(""Logistic Regression Accuracy : "",lr_score)"
"SETUP CHECKPOINT ASSIGN = GaussianNB() ASSIGN.fit(x_train,y_train) ASSIGN = naive_bayes.score(x_test,y_test) print(,ASSIGN)",0,stream,"from sklearn.naive_bayes import GaussianNB
 naive_bayes = GaussianNB()
 naive_bayes.fit(x_train,y_train)
 nb_score = naive_bayes.score(x_test,y_test)
 print(""Naive Bayes Accuracy : "",nb_score)"
"SETUP CHECKPOINT ASSIGN = RandomForestClassifier(n_estimators = 100) ASSIGN.fit(x_train,y_train) ASSIGN = rfc.score(x_test,y_test) print(,ASSIGN)",0,stream,"from sklearn.ensemble import RandomForestClassifier
 rfc = RandomForestClassifier(n_estimators = 100)
 rfc.fit(x_train,y_train)
 rf_score = rfc.score(x_test,y_test)
 print(""Random Forest Accuracy : "",rf_score)"
"CHECKPOINT ASSIGN = {""Logistic Regression"" : lr_score,""Random Forest"" : rf_score,""K-Nearest Neighbour"" : knn_score ,""Naive Bayes"": nb_score ,""K-Nearest Neighbour"" : knn_score } dict1",1,execute_result,"dict1 = {""Logistic Regression"" : lr_score,""Random Forest"" : rf_score,""K-Nearest Neighbour"" : knn_score ,""Naive Bayes"": nb_score ,""K-Nearest Neighbour"" : knn_score }
 dict1"
SETUP,0,not_existent,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"import os
 for dirname, _, filenames in os.walk('/kaggle/input'):
     for filename in filenames:
         print(os.path.join(dirname, filename))"
"ASSIGN = pd.read_csv(""path"") ASSIGN = pd.read_csv(""path"") ASSIGN.head()",0,execute_result,"H = pd.read_csv(""/kaggle/input/house-prices-advanced-regression-techniques/train.csv"")
 H_T = pd.read_csv(""/kaggle/input/house-prices-advanced-regression-techniques/test.csv"")
 H.head()"
CHECKPOINT H.shape,0,execute_result,H.shape
H.info(),0,stream,H.info()
H[H.duplicated()],0,execute_result,"H[H.duplicated()]
 #No duplicates"
"H[H.loc[:,~H.columns.isin(['SalePrice'])].duplicated()]",0,execute_result,"H[H.loc[:,~H.columns.isin(['SalePrice'])].duplicated()]
 #No duplicates"
ASSIGN = H.apply(pd.Series.nunique) ASSIGN[ASSIGN == 1],1,execute_result,"Unique_data = H.apply(pd.Series.nunique)
 Unique_data[Unique_data == 1]
 #No single unique values in the dataset"
"ASSIGN = H.isnull().sum().sort_values(ascending = False) ASSIGN = ((H.isnull().sum()*100)path()[0]).sort_values(ascending = False) NullValues = pd.concat([ASSIGN, ASSIGN], axis = 1, keys = [""ASSIGN"", ""ASSIGN""]) NullValues[NullValues.ASSIGN > 0]",1,execute_result,"Sum = H.isnull().sum().sort_values(ascending = False)
 Percent = ((H.isnull().sum()*100)/H.count()[0]).sort_values(ascending = False)
 NullValues = pd.concat([Sum, Percent], axis = 1, keys = [""Sum"", ""Percent""])
 NullValues[NullValues.Sum > 0]"
"H.drop(['Id','Alley', 'PoolQC', 'Fence','MiscFeature','MiscVal','FireplaceQu','LotFrontage'], axis = 1, inplace = True)",1,not_existent,"#Deleting columns that have more than 20% missing values and Id column
 H.drop(['Id','Alley', 'PoolQC', 'Fence','MiscFeature','MiscVal','FireplaceQu','LotFrontage'], axis = 1, inplace = True)"
"H[H[""GarageArea""] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]",1,execute_result,"H[H[""GarageArea""] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]
 
 # Missing values exist as there is no garage for these homes"
"H.fillna({'GarageType': 'NoGarage', 'GarageYrBlt': 0, 'GarageFinish': 'NoGarage', 'GarageQual': 'NoGarage','GarageCond': 'NoGarage'} , inplace = True)",1,not_existent,"H.fillna({'GarageType': 'NoGarage', 'GarageYrBlt': 0, 'GarageFinish': 'NoGarage', 'GarageQual': 'NoGarage','GarageCond': 'NoGarage'} , inplace = True)
 
 #Filling appropriate values for nulls "
"H[H[""GarageArea""] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]",0,execute_result,"H[H[""GarageArea""] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]"
"H[H['TotalBsmtSF'] == 0][['BsmtQual', 'BsmtCond','BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].isnull().sum()",1,execute_result,"H[H['TotalBsmtSF'] == 0][['BsmtQual', 'BsmtCond','BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].isnull().sum()
 
 # Missing values exist as there is no basement for these homes"
"H[H['TotalBsmtSF'] > 0][['BsmtQual', 'BsmtCond','BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].isnull().sum()",0,execute_result,"H[H['TotalBsmtSF'] > 0][['BsmtQual', 'BsmtCond','BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].isnull().sum()
 
 # BsmtExposure and BsmtFinType2 have missing values though these homes have a basement"
"ASSIGN = H['BsmtExposure'].ASSIGN()[0] H.loc[(H['TotalBsmtSF'] > 0) & (H['BsmtExposure'].isnull()), 'BsmtExposure'] = ASSIGN ASSIGN = H['BsmtFinType2'].ASSIGN()[0] H.loc[(H['TotalBsmtSF'] > 0) & (H['BsmtFinType2'].isnull()), 'BsmtFinType2'] = ASSIGN",1,not_existent,"mode = H['BsmtExposure'].mode()[0]
 H.loc[(H['TotalBsmtSF'] > 0) & (H['BsmtExposure'].isnull()), 'BsmtExposure'] = mode
 
 mode = H['BsmtFinType2'].mode()[0]
 H.loc[(H['TotalBsmtSF'] > 0) & (H['BsmtFinType2'].isnull()), 'BsmtFinType2'] = mode
 
 #Filling these nulls with mode"
"H.fillna({'BsmtQual': 'NoBasement', 'BsmtCond': 'NoBasement','BsmtExposure': 'NoBasement', 'BsmtFinType1': 'NoBasement', 'BsmtFinType2': 'NoBasement'} , inplace = True)",1,not_existent,"H.fillna({'BsmtQual': 'NoBasement', 'BsmtCond': 'NoBasement','BsmtExposure': 'NoBasement', 'BsmtFinType1': 'NoBasement', 'BsmtFinType2': 'NoBasement'} , inplace = True)
 
 
 #Filling appropriate values for other nulls "
"ASSIGN = H['Electrical'].ASSIGN()[0] H['Electrical'].fillna(ASSIGN, inplace = True) ASSIGN = H['MasVnrType'].ASSIGN()[0] H['MasVnrType'].fillna(ASSIGN, inplace = True) ASSIGN = H['MasVnrArea'].ASSIGN() H['MasVnrArea'].fillna(ASSIGN, inplace = True)",1,not_existent,"mode = H['Electrical'].mode()[0]
 H['Electrical'].fillna(mode, inplace = True)
 mode = H['MasVnrType'].mode()[0]
 H['MasVnrType'].fillna(mode, inplace = True)
 median = H['MasVnrArea'].median()
 H['MasVnrArea'].fillna(median, inplace = True)
 
 # Filling missing values in MasVnrArea,MasVnrType,Electrical with mode"
"ASSIGN = H.isnull().sum().sort_values(ascending = False) ASSIGN = ((H.isnull().sum()*100)path()[0]).sort_values(ascending = False) NullValues = pd.concat([ASSIGN, ASSIGN], axis = 1, keys = [""ASSIGN"", ""ASSIGN""]) NullValues[NullValues.ASSIGN > 0]",1,execute_result,"Sum = H.isnull().sum().sort_values(ascending = False)
 Percent = ((H.isnull().sum()*100)/H.count()[0]).sort_values(ascending = False)
 NullValues = pd.concat([Sum, Percent], axis = 1, keys = [""Sum"", ""Percent""])
 NullValues[NullValues.Sum > 0]
 
 #No null values"
"ASSIGN = 2011 - ASSIGN ASSIGN = 2011 - ASSIGN ASSIGN = 2011 - ASSIGN ASSIGN = 2011 - ASSIGN H.loc[H['AgeOfGarage'] > 100 , 'AgeOfGarage'] = 0 H.drop(['YearBuilt','YearRemodAdd','YrSold','GarageYrBlt'], axis = 1, inplace = True)",1,not_existent,"H[""AgeOfHouse""] = 2011 - H[""YearBuilt""]
 H[""AgeOfRemod""] = 2011 - H[""YearRemodAdd""]
 H['AgeOfSell'] = 2011 - H['YrSold']
 H['AgeOfGarage'] = 2011 - H['GarageYrBlt']
 H.loc[H['AgeOfGarage'] > 100 , 'AgeOfGarage'] = 0
 H.drop(['YearBuilt','YearRemodAdd','YrSold','GarageYrBlt'], axis = 1, inplace = True)
 
 #Using age instead of year for better intution and ease"
ASSIGN = ASSIGN + (0.5 * ASSIGN) ASSIGN = ASSIGN + (0.5 * ASSIGN),1,not_existent,"H['BsmtBath'] = H['BsmtFullBath'] + (0.5 * H['BsmtHalfBath'])
 H['Bath'] = H['FullBath'] + (0.5 * H['HalfBath'])"
ASSIGN = ASSIGN + ASSIGN + ASSIGN + ASSIGN,1,not_existent,H['TotalPorchArea'] = H['OpenPorchSF'] + H['EnclosedPorch'] + H['3SsnPorch'] + H['ScreenPorch']
"ASSIGN = ['SalePrice','LotArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','GarageArea','WoodDeckSF','TotalPorchArea','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MasVnrArea','AgeOfGarage', 'AgeOfHouse', 'AgeOfRemod','AgeOfSell'] ASSIGN = ['BsmtBath','Bath','BedroomAbvGr','BldgType','BsmtHalfBath','BsmtFullBath','Condition1','Condition2','Electrical','Exterior1st','Exterior2nd','Fireplaces','Foundation','FullBath','Functional','GarageCars','GarageFinish','GarageType','HalfBath','Heating','HouseStyle','KitchenAbvGr','LandContour','LandSlope','LotConfig','LotShape','MSSubClass','MSZoning','MasVnrType','MoSold','Neighborhood','PavedDrive','RoofMatl','RoofStyle','SaleCondition','SaleType','Street','TotRmsAbvGrd','Utilities'] ASSIGN = [ ""OverallQual"",""OverallCond"",""ExterQual"",""ExterCond"",""BsmtQual"",'BsmtCond',""BsmtExposure"",""HeatingQC"",""KitchenQual"",""GarageQual"",""GarageCond"", 'BsmtFinType1', 'BsmtFinType2','CentralAir']",1,not_existent,"numerical_columns = ['SalePrice','LotArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','GarageArea','WoodDeckSF','TotalPorchArea','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MasVnrArea','AgeOfGarage', 'AgeOfHouse', 'AgeOfRemod','AgeOfSell']
 categorical_columns = ['BsmtBath','Bath','BedroomAbvGr','BldgType','BsmtHalfBath','BsmtFullBath','Condition1','Condition2','Electrical','Exterior1st','Exterior2nd','Fireplaces','Foundation','FullBath','Functional','GarageCars','GarageFinish','GarageType','HalfBath','Heating','HouseStyle','KitchenAbvGr','LandContour','LandSlope','LotConfig','LotShape','MSSubClass','MSZoning','MasVnrType','MoSold','Neighborhood','PavedDrive','RoofMatl','RoofStyle','SaleCondition','SaleType','Street','TotRmsAbvGrd','Utilities']
 ordinal_columns = [ ""OverallQual"",""OverallCond"",""ExterQual"",""ExterCond"",""BsmtQual"",'BsmtCond',""BsmtExposure"",""HeatingQC"",""KitchenQual"",""GarageQual"",""GarageCond"", 'BsmtFinType1', 'BsmtFinType2','CentralAir']"
H[ordinal_columns],0,execute_result,H[ordinal_columns]
"ASSIGN = ASSIGN.map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5}) ASSIGN = ASSIGN.map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5}) ASSIGN = ASSIGN.map({'NoBasement' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5}) ASSIGN = ASSIGN.map({'NoBasement' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5}) ASSIGN = ASSIGN.map({'Gd' : 4, 'Av' : 3, 'Mn' : 2, 'No' : 1, 'NoBasement' : 0}) ASSIGN = ASSIGN.map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5}) ASSIGN = ASSIGN.map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5}) ASSIGN = ASSIGN.map({'NoGarage' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5}) ASSIGN = ASSIGN.map({'NoGarage' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5}) H['BsmtFinType1'] = H['BsmtFinType1'].map({'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'NoBasement' : 0}) H['BsmtFinType2'] = H['BsmtFinType2'].map({'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'NoBasement' : 0}) ASSIGN = ASSIGN.map({'N' : 0, 'Y' : 1})",1,not_existent,"H['ExterQual'] = H['ExterQual'].map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})
 H['ExterCond'] = H['ExterCond'].map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})
 H['BsmtQual'] = H['BsmtQual'].map({'NoBasement' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})
 H['BsmtCond'] = H['BsmtCond'].map({'NoBasement' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})
 H['BsmtExposure'] = H['BsmtExposure'].map({'Gd' : 4, 'Av' : 3, 'Mn' : 2, 'No' : 1, 'NoBasement' : 0})
 H['HeatingQC'] = H['HeatingQC'].map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})
 H['KitchenQual'] = H['KitchenQual'].map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})
 H['GarageQual'] = H['GarageQual'].map({'NoGarage' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})
 H['GarageCond'] = H['GarageCond'].map({'NoGarage' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})
 H['BsmtFinType1'] = H['BsmtFinType1'].map({'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'NoBasement' : 0})
 H['BsmtFinType2'] = H['BsmtFinType2'].map({'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'NoBasement' : 0})
 H['CentralAir'] = H['CentralAir'].map({'N' : 0, 'Y' : 1})"
H[ordinal_columns].head(5),0,execute_result,H[ordinal_columns].head(5)
H[ordinal_columns].isnull().sum(),0,execute_result,H[ordinal_columns].isnull().sum()
CHECKPOINT H[categorical_columns].dtypes,0,execute_result,H[categorical_columns].dtypes
"for i in range(0, len(categorical_columns)): if (H[categorical_columns[i]].dtype == 'int64') | (H[categorical_columns[i]].dtype == 'float64'): H[categorical_columns[i]] = H[categorical_columns[i]].apply(str)",1,not_existent,"for i in range(0, len(categorical_columns)):
     if (H[categorical_columns[i]].dtype == 'int64') | (H[categorical_columns[i]].dtype == 'float64'):
         H[categorical_columns[i]] = H[categorical_columns[i]].apply(str)
         
 #Changing data type to string/object"
H[categorical_columns].head(),0,execute_result,H[categorical_columns].head()
"H[""BsmtBath""].value_counts()",0,execute_result,"H[""BsmtBath""].value_counts()"
"H.loc[H[""BsmtBath""] == '3.0', 'BsmtBath'] = '2.0'",1,not_existent,"H.loc[H[""BsmtBath""] == '3.0', 'BsmtBath'] = '2.0'
 
 #Merging 3 baths to 2 as there is only 1 record"
"H[""BedroomAbvGr""].value_counts()",0,execute_result,"H[""BedroomAbvGr""].value_counts()"
"H.loc[H[""BedroomAbvGr""] == '8', 'BedroomAbvGr'] = '6'",1,not_existent,"H.loc[H[""BedroomAbvGr""] == '8', 'BedroomAbvGr'] = '6'
 #Merging 8 to closer one 6"
"H[""BsmtFullBath""].value_counts()",0,execute_result,"H[""BsmtFullBath""].value_counts()"
"H.loc[H[""BsmtFullBath""] == '3', 'BsmtFullBath'] = '2'",1,not_existent,"H.loc[H[""BsmtFullBath""] == '3', 'BsmtFullBath'] = '2'
 #Merging 3 to closer one 2"
"H[""Condition1""].value_counts()",0,execute_result,"H[""Condition1""].value_counts()
 # Not merging as they seem to have an importance"
"H[""Condition2""].value_counts()",0,execute_result,"H[""Condition2""].value_counts()"
"H.loc[H[""Condition2""].isin(['PosA','RRAn','RRAe']), 'Condition2'] = 'PosA_RRAn_RRAe'",1,not_existent,"H.loc[H[""Condition2""].isin(['PosA','RRAn','RRAe']), 'Condition2'] = 'PosA_RRAn_RRAe'
 #Merging 'PosA','RRAn','RRAe' to one field"
"H[""Electrical""].value_counts()",0,execute_result,"H[""Electrical""].value_counts()
 # Not merging as they seem to have some importance"
"H[""Exterior1st""].value_counts()",0,execute_result,"H[""Exterior1st""].value_counts()"
"H.loc[H[""Exterior1st""].isin(['Stone','BrkComm','CBlock','AsphShn','ImStucc']), 'Exterior1st'] = 'Other'",1,not_existent,"H.loc[H[""Exterior1st""].isin(['Stone','BrkComm','CBlock','AsphShn','ImStucc']), 'Exterior1st'] = 'Other'
 #Renaming 'Stone','BrkComm','CBlock','AsphShn','ImStucc' to other"
"H[""Exterior2nd""].value_counts()",0,execute_result,"H[""Exterior2nd""].value_counts()"
"H.loc[H[""Exterior2nd""].isin(['Stone','Brk Cmn','CBlock','AsphShn','ImStucc','Other']), 'Exterior2nd'] = 'Other'",1,not_existent,"H.loc[H[""Exterior2nd""].isin(['Stone','Brk Cmn','CBlock','AsphShn','ImStucc','Other']), 'Exterior2nd'] = 'Other'
 # Renaming 'Stone','BrkComm','CBlock','AsphShn','ImStucc' to Other"
"H[""Utilities""].value_counts()",0,execute_result,"H[""Utilities""].value_counts()"
"H.drop(['Utilities'], axis = 1, inplace = True) categorical_columns.remove('Utilities')",1,not_existent,"H.drop(['Utilities'], axis = 1, inplace = True)
 categorical_columns.remove('Utilities')
 
 #Droping Utilities as it has 99% data as 1 unique value"
"H[""Heating""].value_counts()",0,execute_result,"H[""Heating""].value_counts()"
"H.loc[H[""Heating""] == 'Floor', 'Heating'] = 'OthW'",1,not_existent,"H.loc[H[""Heating""] == 'Floor', 'Heating'] = 'OthW'
 #Merging 'Floor' to 'OthW'"
"H[""RoofMatl""].value_counts()",0,execute_result,"H[""RoofMatl""].value_counts()"
"H.loc[H[""RoofMatl""].isin(['Roll','Membran','Metal','ClyTile']), 'RoofMatl'] = 'Other'",1,not_existent,"H.loc[H[""RoofMatl""].isin(['Roll','Membran','Metal','ClyTile']), 'RoofMatl'] = 'Other'
 #Clubbing 'Roll','Membran','Metal','ClyTile' to other"
"H[""TotRmsAbvGrd""].value_counts()",0,execute_result,"H[""TotRmsAbvGrd""].value_counts()"
"H.loc[H[""TotRmsAbvGrd""] == '2', 'TotRmsAbvGrd'] = '3' H.loc[H[""TotRmsAbvGrd""] == '14', 'TotRmsAbvGrd'] = '12'",1,not_existent,"H.loc[H[""TotRmsAbvGrd""] == '2', 'TotRmsAbvGrd'] = '3'
 H.loc[H[""TotRmsAbvGrd""] == '14', 'TotRmsAbvGrd'] = '12'
 #Merging outliers to closer values"
"ASSIGN = ['SalePrice','LotArea','BsmtFinSF1','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','GrLivArea','GarageArea','WoodDeckSF','TotalPorchArea','MasVnrArea','AgeOfGarage', 'AgeOfHouse', 'AgeOfRemod','AgeOfSell'] plt.figure(figsize=(15,60)) for i in range(0, len(ASSIGN)): plt.subplot(12,2,(i+1)) sns.distplot(H[ASSIGN[i]])",0,display_data,"numerical_columns_1 = ['SalePrice','LotArea','BsmtFinSF1','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','GrLivArea','GarageArea','WoodDeckSF','TotalPorchArea','MasVnrArea','AgeOfGarage', 'AgeOfHouse', 'AgeOfRemod','AgeOfSell']
 plt.figure(figsize=(15,60))
 for i in range(0, len(numerical_columns_1)):
     plt.subplot(12,2,(i+1))
     sns.distplot(H[numerical_columns_1[i]])"
"SETUP H['SalePrice'], fitted_lambda = stats.boxcox(H['SalePrice']) H['LotArea'], fitted_lambda = stats.boxcox(H['LotArea']) H['1stFlrSF'], fitted_lambda = stats.boxcox(H['1stFlrSF']) H['GrLivArea'], fitted_lambda = stats.boxcox(H['GrLivArea'])",1,not_existent,"from scipy import stats
 
 # correcting target variable
 H['SalePrice'], fitted_lambda = stats.boxcox(H['SalePrice'])
 
 #Correcting some normally distributed but skewed numerical data
 H['LotArea'], fitted_lambda = stats.boxcox(H['LotArea'])
 H['1stFlrSF'], fitted_lambda = stats.boxcox(H['1stFlrSF'])
 H['GrLivArea'], fitted_lambda = stats.boxcox(H['GrLivArea'])
 
"
H['TotalBsmtSF'].describe(),0,execute_result,H['TotalBsmtSF'].describe()
sns.distplot(H['1stFlrSF']),0,execute_result,sns.distplot(H['1stFlrSF'])
sns.distplot(H['GrLivArea']),0,execute_result,sns.distplot(H['GrLivArea'])
"plt.figure(figsize=(15,15)) ASSIGN = H[numerical_columns].corr() sns.heatmap(ASSIGN, annot = True)",0,execute_result,"plt.figure(figsize=(15,15))
 correlation = H[numerical_columns].corr()
 sns.heatmap(correlation, annot = True)"
"plt.figure(figsize=(15,60)) for i in range(0, len(categorical_columns)): plt.subplot(20,2,(i+1)) sns.boxplot(data = H, x = categorical_columns[i], y = 'SalePrice'  )",0,display_data,"plt.figure(figsize=(15,60))
 for i in range(0, len(categorical_columns)):
     plt.subplot(20,2,(i+1))
     sns.boxplot(data = H, x = categorical_columns[i], y = 'SalePrice'  )
 
 #Plotting all categorical with box plot to ponder and check for any obvious issues with data    "
"plt.figure(figsize=(15,60)) for i in range(0, len(numerical_columns)): plt.subplot(12,2,(i+1)) sns.scatterplot(data = H, x = numerical_columns[i], y = 'SalePrice'  )",0,display_data,"plt.figure(figsize=(15,60))
 for i in range(0, len(numerical_columns)):
     plt.subplot(12,2,(i+1))
     sns.scatterplot(data = H, x = numerical_columns[i], y = 'SalePrice'  )
     
 #Ploting all numerical data in scatter plots to ponder"
"plt.figure(figsize=(15,35)) for i in range(0, len(ordinal_columns)): plt.subplot(7,2,(i+1)) sns.barplot(data = H, x = ordinal_columns[i], y = 'SalePrice'  )",0,display_data,"plt.figure(figsize=(15,35))
 for i in range(0, len(ordinal_columns)):
     plt.subplot(7,2,(i+1))
     sns.barplot(data = H, x = ordinal_columns[i], y = 'SalePrice'  )
     
 #Ploting all ordinal values with Sala price in a bar graph 
 # Sale price doesnt seem to change much with any ordinal data"
SETUP warnings.filterwarnings('ignore'),0,not_existent,"from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import MinMaxScaler
 from sklearn.preprocessing import StandardScaler
 import warnings
 warnings.filterwarnings('ignore')
 from sklearn import linear_model, metrics
 from sklearn.linear_model import LinearRegression
 from sklearn.linear_model import Ridge
 from sklearn.linear_model import Lasso
 from sklearn.linear_model import ElasticNet
 from sklearn.model_selection import GridSearchCV
 from sklearn.feature_selection import RFE
"
"ASSIGN = H['SalePrice'] ASSIGN = H.drop(['SalePrice'], axis = 1)",1,not_existent,"y = H['SalePrice']
 
 X = H.drop(['SalePrice'], axis = 1)"
CHECKPOINT X.shape,0,execute_result,X.shape
"House_Dummies = pd.get_dummies(H[categorical_columns], drop_first = True) House_Dummies.head()",1,execute_result,"House_Dummies = pd.get_dummies(H[categorical_columns], drop_first = True)
 House_Dummies.head()"
len(categorical_columns),0,execute_result,len(categorical_columns)
"ASSIGN = ASSIGN.drop(categorical_columns, axis = 1) ASSIGN = pd.concat([ASSIGN, House_Dummies], axis=1)",1,not_existent,"X = X.drop(categorical_columns, axis = 1)
 X = pd.concat([X, House_Dummies], axis=1)"
"X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.7, test_size = 0.3, random_state = 100)",1,not_existent,"X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.7, test_size = 0.3, random_state = 100)"
ASSIGN = StandardScaler() numerical_columns.remove('SalePrice') X_train[numerical_columns+ordinal_columns] = ASSIGN.fit_transform(X_train[numerical_columns+ordinal_columns]) X_test[numerical_columns+ordinal_columns] = ASSIGN.transform(X_test[numerical_columns+ordinal_columns]),1,not_existent,"scaler = StandardScaler()
 numerical_columns.remove('SalePrice')
 X_train[numerical_columns+ordinal_columns] = scaler.fit_transform(X_train[numerical_columns+ordinal_columns])
 X_test[numerical_columns+ordinal_columns] = scaler.transform(X_test[numerical_columns+ordinal_columns])"
CHECKPOINT X_train.shape,0,execute_result,X_train.shape
CHECKPOINT X_test.shape,0,execute_result,X_test.shape
"ASSIGN = {'alpha': [0.00001,0.00005,0.0001, 0.0005,0.001,0.01, 0.02]} ASSIGN = Lasso() ASSIGN = 5 ASSIGN = GridSearchCV(estimator = lasso, ASSIGN = params, ASSIGN= 'neg_mean_absolute_error', ASSIGN = folds, ASSIGN=True, ASSIGN = 1) ASSIGN.fit(X_train, y_train)",0,stream,"params = {'alpha': [0.00001,0.00005,0.0001, 0.0005,0.001,0.01, 0.02]}
 #arams = {'alpha': [0.1, 1,10,100,200,300,500,1000]}
 
 
 lasso = Lasso()
 
 
 folds = 5
 #Taking 5 folds for Cross validation
 
 model_cv = GridSearchCV(estimator = lasso, 
                         param_grid = params, 
                         scoring= 'neg_mean_absolute_error', 
                         cv = folds, 
                         return_train_score=True,
                         verbose = 1)            
 
 model_cv.fit(X_train, y_train)"
"ASSIGN = pd.DataFrame(model_cv.cv_results_) ASSIGN['param_alpha'] = ASSIGN['param_alpha'].astype('float32') plt.plot(ASSIGN['param_alpha'], ASSIGN['mean_train_score']) plt.plot(ASSIGN['param_alpha'], ASSIGN['mean_test_score']) plt.xlabel('alpha') plt.ylabel('Negative Mean Absolute Error') plt.title(""Negative Mean Absolute Error and alpha"") plt.legend(['train score', 'test score'], loc='upper left') plt.show()",0,display_data,"cv_results = pd.DataFrame(model_cv.cv_results_)
 cv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')
 plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])
 plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])
 plt.xlabel('alpha')
 plt.ylabel('Negative Mean Absolute Error')
 
 plt.title(""Negative Mean Absolute Error and alpha"")
 plt.legend(['train score', 'test score'], loc='upper left')
 plt.show()"
"ASSIGN =0.001 ASSIGN = Lasso(alpha=alpha) ASSIGN.fit(X_train, y_train)",0,execute_result,"alpha =0.001
 lasso = Lasso(alpha=alpha)  
 lasso.fit(X_train, y_train) "
"ASSIGN = pd.DataFrame({""Feature"":X_train.columns.tolist(),""Coefficients"":lasso.coef_}) ASSIGN[ASSIGN['Coefficients'] != 0 ].sort_values(by = ""Coefficients"" , ascending = False).count()",1,execute_result,"Lasso_coef = pd.DataFrame({""Feature"":X_train.columns.tolist(),""Coefficients"":lasso.coef_})
 Lasso_coef[Lasso_coef['Coefficients'] != 0 ].sort_values(by = ""Coefficients"" , ascending = False).count()
 
 #Feature selection is done by Lasso and features narrowed down to 46"
ASSIGN = lasso.predict(X_test) ASSIGN = lasso.predict(X_train),0,not_existent,"y_test_lasso_predict = lasso.predict(X_test)
 y_train_lasso_predict = lasso.predict(X_train)"
"CHECKPOINT print(metrics.r2_score(y_true=y_test, y_pred=y_test_lasso_predict)) print(metrics.r2_score(y_true=y_train, y_pred=y_train_lasso_predict))",0,stream,"print(metrics.r2_score(y_true=y_test, y_pred=y_test_lasso_predict))
 print(metrics.r2_score(y_true=y_train, y_pred=y_train_lasso_predict))"
"ASSIGN = {'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 1.5, 2, 10, 100, 1000]} ASSIGN = Ridge() ASSIGN = 5 ASSIGN = GridSearchCV(estimator = ridge, ASSIGN = params, ASSIGN= 'neg_mean_absolute_error', ASSIGN = folds, ASSIGN=True, ASSIGN = 1) ASSIGN.fit(X_train, y_train)",0,stream,"
 params = {'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 1.5, 2, 10, 100, 1000]}
 
 
 ridge = Ridge()
 
 # cross validation with 5 folds
 folds = 5
 model_cv = GridSearchCV(estimator = ridge, 
                         param_grid = params, 
                         scoring= 'neg_mean_absolute_error', 
                         cv = folds, 
                         return_train_score=True,
                         verbose = 1)            
 model_cv.fit(X_train, y_train) "
"ASSIGN =100 ASSIGN = Ridge(alpha=alpha) ASSIGN.fit(X_train, y_train)",0,execute_result,"alpha =100
 ridge = Ridge(alpha=alpha)  
 ridge.fit(X_train, y_train) "
ASSIGN = ridge.predict(X_test) ASSIGN = ridge.predict(X_train),0,not_existent,"y_test_ridge_predict = ridge.predict(X_test)
 y_train_ridge_predict = ridge.predict(X_train)"
"CHECKPOINT print(metrics.r2_score(y_true=y_test, y_pred=y_test_ridge_predict)) print(metrics.r2_score(y_true=y_train, y_pred=y_train_ridge_predict))",0,stream,"print(metrics.r2_score(y_true=y_test, y_pred=y_test_ridge_predict))
 print(metrics.r2_score(y_true=y_train, y_pred=y_train_ridge_predict))"
"ASSIGN = pd.DataFrame({""Feature"":X_train.columns.tolist(),""Coefficients"":ridge.coef_}) ASSIGN[ASSIGN['Coefficients'] != 0 ].sort_values(by = ""Coefficients"" , ascending = False).count()",1,execute_result,"Ridge_coef = pd.DataFrame({""Feature"":X_train.columns.tolist(),""Coefficients"":ridge.coef_})
 Ridge_coef[Ridge_coef['Coefficients'] != 0 ].sort_values(by = ""Coefficients"" , ascending = False).count()
 #No feature selection happened"
"ASSIGN = {'alpha': [0,0.0001, 0.0005, 0.001, 0.01]} ASSIGN = ElasticNet() ASSIGN = GridSearchCV(estimator = elasticnet, ASSIGN = params, ASSIGN= 'neg_mean_absolute_error', ASSIGN = folds, ASSIGN=True, ASSIGN = 1) ASSIGN.fit(X_train, y_train)",0,stream,"params = {'alpha': [0,0.0001, 0.0005, 0.001, 0.01]}
 
 elasticnet = ElasticNet()
 
 # cross validation
 model_cv = GridSearchCV(estimator = elasticnet, 
                         param_grid = params, 
                         scoring= 'neg_mean_absolute_error', 
                         cv = folds, 
                         return_train_score=True,
                         verbose = 1)            
 
 model_cv.fit(X_train, y_train) "
"ASSIGN =0.001 ASSIGN = ElasticNet(alpha=alpha) ASSIGN.fit(X_train, y_train)",0,execute_result,"alpha =0.001
 elasticnet = ElasticNet(alpha=alpha)  
 elasticnet.fit(X_train, y_train) "
ASSIGN = elasticnet.predict(X_test) ASSIGN = elasticnet.predict(X_train),0,not_existent,"y_test_elasticnet_predict = elasticnet.predict(X_test)
 y_train_elasticnet_predict = elasticnet.predict(X_train)"
"CHECKPOINT print(metrics.r2_score(y_true=y_test, y_pred=y_test_elasticnet_predict)) print(metrics.r2_score(y_true=y_train, y_pred=y_train_elasticnet_predict))",0,stream,"print(metrics.r2_score(y_true=y_test, y_pred=y_test_elasticnet_predict))
 print(metrics.r2_score(y_true=y_train, y_pred=y_train_elasticnet_predict))"
"ASSIGN = pd.DataFrame({""Feature"":X_train.columns.tolist(),""Coefficients"":elasticnet.coef_}) ASSIGN[ASSIGN['Coefficients'] != 0 ].sort_values(by = ""Coefficients"" , ascending = False).count()",1,execute_result,"elasticnet_coef = pd.DataFrame({""Feature"":X_train.columns.tolist(),""Coefficients"":elasticnet.coef_})
 elasticnet_coef[elasticnet_coef['Coefficients'] != 0 ].sort_values(by = ""Coefficients"" , ascending = False).count()
 #Feature selection happened but not as good as Lasso"
"ASSIGN = ASSIGN[ASSIGN['Coefficients'] != 0 ].sort_values(by = ""Coefficients"" , ascending = False).reset_index()",1,not_existent,"Lasso_coef = Lasso_coef[Lasso_coef['Coefficients'] != 0 ].sort_values(by = ""Coefficients"" , ascending = False).reset_index()"
"Lasso_coef.drop(['index'], axis = 1, inplace = True)",1,not_existent,"Lasso_coef.drop(['index'], axis = 1, inplace = True)"
Lasso_coef.head(10),0,execute_result,Lasso_coef.head(10)
Lasso_coef.tail(10),0,execute_result,Lasso_coef.tail(10)
Lasso_coef['Feature'].to_list(),0,execute_result,Lasso_coef['Feature'].to_list()
"plt.figure(figsize=(15,15)) sns.barplot(x=""Coefficients"", y=""Feature"", data=Lasso_coef, palette=""vlag"") plt.xlabel(""Feature Importance"") plt.tight_layout()",0,display_data,"
 plt.figure(figsize=(15,15))
 sns.barplot(x=""Coefficients"", y=""Feature"", data=Lasso_coef, palette=""vlag"")
 plt.xlabel(""Feature Importance"")
 plt.tight_layout()"
SETUP,0,not_existent,import os import torch import torchvision import tarfile import torch.nn as nn import numpy as np import torch.nn.functional as F from torchvision.datasets.utils import download_url from torchvision.datasets import ImageFolder from torch.utils.data import DataLoader import torchvision.transforms as tt from torch.utils.data import random_split from torchvision.utils import make_grid import matplotlib.pyplot as plt %matplotlib inline
ASSIGN='course_project',1,not_existent,project_name='course_project'
CHECKPOINT ASSIGN = '..path' print(os.listdir(ASSIGN)),0,stream,data_dir = '../input/russian-handwritten-letters'  print(os.listdir(data_dir)) 
SETUP,0,not_existent,from torchvision.datasets import ImageFolder from torchvision.transforms import ToTensor
"ASSIGN = ImageFolder(data_dir, transform=ToTensor())",1,not_existent,"dataset = ImageFolder(data_dir, transform=ToTensor())"
"CHECKPOINT CHECKPOINT ASSIGN = dataset[0] print(img.shape, label) img",1,stream,"img, label = dataset[0] print(img.shape, label) img"
CHECKPOINT print(dataset.classes),0,stream,print(dataset.classes)
"SETUP CHECKPOINT def show_example(img, label): print('Label: ', dataset.classes[label], +str(label)+) plt.imshow(img.permute(1, 2, 0))",0,not_existent,"import matplotlib.pyplot as plt  def show_example(img, label):     print('Label: ', dataset.classes[label], ""(""+str(label)+"")"")     plt.imshow(img.permute(1, 2, 0))"
show_example(*dataset[0]),0,stream,show_example(*dataset[0])
"ASSIGN = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) ASSIGN = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'), tt.RandomHorizontalFlip(), tt.ToTensor(), tt.Normalize(*ASSIGN,inplace=True)]) ASSIGN = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])",1,not_existent,"# Data transforms (normalization & data augmentation) stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) train_tfms = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'),                           tt.RandomHorizontalFlip(),                           tt.ToTensor(),                           tt.Normalize(*stats,inplace=True)]) valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])"
ASSIGN = 400,1,not_existent,batch_size = 400
"ASSIGN = 5000 ASSIGN = len(dataset) - val_size ASSIGN = random_split(dataset, [train_size, val_size]) len(train_ds), len(val_ds)",1,execute_result,"val_size = 5000 train_size = len(dataset) - val_size  train_ds, val_ds = random_split(dataset, [train_size, val_size]) len(train_ds), len(val_ds)"
"ASSIGN = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True) ASSIGN = DataLoader(val_ds, batch_size*2, num_workers=3, pin_memory=True)",1,not_existent,"# PyTorch data loaders train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True) valid_dl = DataLoader(val_ds, batch_size*2, num_workers=3, pin_memory=True)"
ASSIGN = 42 torch.manual_seed(ASSIGN);,1,not_existent,random_seed = 42 torch.manual_seed(random_seed);
"ASSIGN = 784 ASSIGN = 256 ASSIGN = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True) ASSIGN = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)",1,not_existent,"image_size = 784 hidden_size = 256 train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True) val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
"CHECKPOINT SETUP def show_batch(dl): for images, labels in dl: ASSIGN = plt.subplots(figsize=(12, 12)) ax.set_xticks([]); ax.set_yticks([]) ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0)) break",0,not_existent,"from torchvision.utils import make_grid  def show_batch(dl):     for images, labels in dl:         fig, ax = plt.subplots(figsize=(12, 12))         ax.set_xticks([]); ax.set_yticks([])         ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))         break"
show_batch(train_dl),0,display_data,show_batch(train_dl)
"def get_default_device(): """"""Pick GPU if available, else CPU"""""" if torch.cuda.is_available(): return torch.device('cuda') else: return torch.device('cpu') def to_device(data, device): """"""Move tensor(s) to chosen device"""""" if isinstance(data, (list,tuple)): return [to_device(x, device) for x in data] return data.to(device, non_blocking=True) class DeviceDataLoader(): """"""Wrap a dataloader to move data to a device"""""" def __init__(self, dl, device): self.dl = dl self.device = device def __iter__(self): """"""Yield a batch of data after moving it to device"""""" for b in self.dl: yield to_device(b, self.device) def __len__(self): """"""Number of batches"""""" return len(self.dl)",1,not_existent,"def get_default_device():     """"""Pick GPU if available, else CPU""""""     if torch.cuda.is_available():         return torch.device('cuda')     else:         return torch.device('cpu')      def to_device(data, device):     """"""Move tensor(s) to chosen device""""""     if isinstance(data, (list,tuple)):         return [to_device(x, device) for x in data]     return data.to(device, non_blocking=True)  class DeviceDataLoader():     """"""Wrap a dataloader to move data to a device""""""     def __init__(self, dl, device):         self.dl = dl         self.device = device              def __iter__(self):         """"""Yield a batch of data after moving it to device""""""         for b in self.dl:              yield to_device(b, self.device)      def __len__(self):         """"""Number of batches""""""         return len(self.dl)"
CHECKPOINT ASSIGN = get_default_device() device,1,execute_result,device = get_default_device() device
"ASSIGN = DeviceDataLoader(ASSIGN, device) ASSIGN = DeviceDataLoader(ASSIGN, device)",1,not_existent,"train_dl = DeviceDataLoader(train_dl, device) valid_dl = DeviceDataLoader(valid_dl, device)"
"class SimpleResidualBlock(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1) self.relu1 = nn.ReLU() self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1) self.relu2 = nn.ReLU() def forward(self, x): ASSIGN = self.conv1(x) ASSIGN = self.relu1(ASSIGN) ASSIGN = self.conv2(ASSIGN) return self.relu2(out) + x",0,not_existent,"class SimpleResidualBlock(nn.Module):     def __init__(self):         super().__init__()         self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)         self.relu1 = nn.ReLU()         self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)         self.relu2 = nn.ReLU()              def forward(self, x):         out = self.conv1(x)         out = self.relu1(out)         out = self.conv2(out)         return self.relu2(out) + x # ReLU can be applied before or after adding the input"
"CHECKPOINT ASSIGN = to_device(SimpleResidualBlock(), device) for images, labels in train_dl: ASSIGN = simple_resnet(images) print(ASSIGN.shape) break del simple_resnet, images, labels torch.cuda.empty_cache()",0,stream,"simple_resnet = to_device(SimpleResidualBlock(), device)  for images, labels in train_dl:     out = simple_resnet(images)     print(out.shape)     break      del simple_resnet, images, labels torch.cuda.empty_cache()"
"def accuracy(outputs, labels): ASSIGN = torch.max(outputs, dim=1) return torch.tensor(torch.sum(preds == labels).item() path(preds)) class ImageClassificationBase(nn.Module): def training_step(self, batch): ASSIGN = batch ASSIGN = self(images) ASSIGN = F.cross_entropy(out, labels) return loss def validation_step(self, batch): ASSIGN = batch ASSIGN = self(images) ASSIGN = F.cross_entropy(out, labels) ASSIGN = accuracy(out, labels) return {'val_loss': ASSIGN.detach(), 'val_acc': ASSIGN} def validation_epoch_end(self, outputs): ASSIGN = [x['val_loss'] for x in outputs] ASSIGN = torch.stack(batch_losses).mean() ASSIGN = [x['val_acc'] for x in outputs] ASSIGN = torch.stack(batch_accs).mean() return {'val_loss': ASSIGN.item(), 'val_acc': ASSIGN.item()} def epoch_end(self, epoch, result): print(""Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}"".format( epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))",0,not_existent,"def accuracy(outputs, labels):     _, preds = torch.max(outputs, dim=1)     return torch.tensor(torch.sum(preds == labels).item() / len(preds))  class ImageClassificationBase(nn.Module):     def training_step(self, batch):         images, labels = batch          out = self(images)                  # Generate predictions         loss = F.cross_entropy(out, labels) # Calculate loss         return loss          def validation_step(self, batch):         images, labels = batch          out = self(images)                    # Generate predictions         loss = F.cross_entropy(out, labels)   # Calculate loss         acc = accuracy(out, labels)           # Calculate accuracy         return {'val_loss': loss.detach(), 'val_acc': acc}              def validation_epoch_end(self, outputs):         batch_losses = [x['val_loss'] for x in outputs]         epoch_loss = torch.stack(batch_losses).mean()   # Combine losses         batch_accs = [x['val_acc'] for x in outputs]         epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies         return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}          def epoch_end(self, epoch, result):         print(""Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}"".format(             epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
"def conv_block(in_channels, out_channels, pool=False): ASSIGN = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)] if pool: ASSIGN.append(nn.MaxPool2d(2)) return nn.Sequential(*ASSIGN) class ResNet9(ImageClassificationBase): def __init__(self, in_channels, num_classes): super().__init__() self.conv1 = conv_block(in_channels, 64) self.conv2 = conv_block(64, 128, pool=True) self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128)) self.conv3 = conv_block(128, 256, pool=True) self.conv4 = conv_block(256, 512, pool=True) self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512)) self.classifier = nn.Sequential(nn.MaxPool2d(4), nn.Flatten(), nn.Linear(512, num_classes)) def forward(self, xb): ASSIGN = self.conv1(xb) ASSIGN = self.conv2(ASSIGN) ASSIGN = self.res1(ASSIGN) + ASSIGN ASSIGN = self.conv3(ASSIGN) ASSIGN = self.conv4(ASSIGN) ASSIGN = self.res2(ASSIGN) + ASSIGN ASSIGN = self.classifier(ASSIGN) return out",0,not_existent,"def conv_block(in_channels, out_channels, pool=False):     layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),                nn.BatchNorm2d(out_channels),                nn.ReLU(inplace=True)]     if pool: layers.append(nn.MaxPool2d(2))     return nn.Sequential(*layers)  class ResNet9(ImageClassificationBase):     def __init__(self, in_channels, num_classes):         super().__init__()                  self.conv1 = conv_block(in_channels, 64)         self.conv2 = conv_block(64, 128, pool=True)         self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))                  self.conv3 = conv_block(128, 256, pool=True)         self.conv4 = conv_block(256, 512, pool=True)         self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))                  self.classifier = nn.Sequential(nn.MaxPool2d(4),                                          nn.Flatten(),                                          nn.Linear(512, num_classes))              def forward(self, xb):         out = self.conv1(xb)         out = self.conv2(out)         out = self.res1(out) + out         out = self.conv3(out)         out = self.conv4(out)         out = self.res2(out) + out         out = self.classifier(out)         return out"
"CHECKPOINT ASSIGN = to_device(ResNet9(3, 10), device) model",0,execute_result,"model = to_device(ResNet9(3, 10), device) model"
"@torch.no_grad() def evaluate(model, val_loader): model.eval() ASSIGN = [model.validation_step(batch) for batch in val_loader] return model.validation_epoch_end(ASSIGN) def get_lr(optimizer): for param_group in optimizer.param_groups: return param_group['lr'] def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, ASSIGN=0, grad_clip=None, opt_func=torch.optim.SGD): torch.cuda.empty_cache() ASSIGN = [] ASSIGN = opt_func(model.parameters(), max_lr, weight_decay=weight_decay) ASSIGN = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, ASSIGN=len(train_loader)) for epoch in range(epochs): model.train() ASSIGN = [] ASSIGN = [] for batch in train_loader: ASSIGN = model.training_step(batch) ASSIGN.append(ASSIGN) ASSIGN.backward() if grad_clip: nn.utils.clip_grad_value_(model.parameters(), grad_clip) ASSIGN.step() ASSIGN.zero_grad() ASSIGN.append(get_lr(ASSIGN)) ASSIGN.step() ASSIGN = evaluate(model, val_loader) ASSIGN['train_loss'] = torch.stack(ASSIGN).mean().item() ASSIGN = lrs model.epoch_end(epoch, ASSIGN) ASSIGN.append(ASSIGN) return history",0,not_existent,"@torch.no_grad() def evaluate(model, val_loader):     model.eval()     outputs = [model.validation_step(batch) for batch in val_loader]     return model.validation_epoch_end(outputs)  def get_lr(optimizer):     for param_group in optimizer.param_groups:         return param_group['lr']  def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,                    weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):     torch.cuda.empty_cache()     history = []          # Set up cutom optimizer with weight decay     optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)     # Set up one-cycle learning rate scheduler     sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,                                                  steps_per_epoch=len(train_loader))          for epoch in range(epochs):         # Training Phase          model.train()         train_losses = []         lrs = []         for batch in train_loader:             loss = model.training_step(batch)             train_losses.append(loss)             loss.backward()                          # Gradient clipping             if grad_clip:                  nn.utils.clip_grad_value_(model.parameters(), grad_clip)                          optimizer.step()             optimizer.zero_grad()                          # Record & update learning rate             lrs.append(get_lr(optimizer))             sched.step()                  # Validation phase         result = evaluate(model, val_loader)         result['train_loss'] = torch.stack(train_losses).mean().item()         result['lrs'] = lrs         model.epoch_end(epoch, result)         history.append(result)     return history"
ASSIGN = 8 ASSIGN = 0.01 ASSIGN = 0.1 ASSIGN = 1e-4 ASSIGN = torch.optim.Adam,0,not_existent,epochs = 8 max_lr = 0.01 grad_clip = 0.1 weight_decay = 1e-4 opt_func = torch.optim.Adam
"SETUP history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)",0,stream,"%%time history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)"
"def plot_accuracies(history): ASSIGN = [x['val_acc'] for x in history] plt.plot(ASSIGN, '-x') plt.xlabel('epoch') plt.ylabel('accuracy') plt.title('Accuracy vs. No. of epochs');",0,not_existent,"def plot_accuracies(history):     accuracies = [x['val_acc'] for x in history]     plt.plot(accuracies, '-x')     plt.xlabel('epoch')     plt.ylabel('accuracy')     plt.title('Accuracy vs. No. of epochs');"
"CHECKPOINT ASSIGN = [evaluate(model, valid_dl)] history",0,error,"history = [evaluate(model, valid_dl)] history"
plot_accuracies(history),0,not_existent,plot_accuracies(history)
"def plot_losses(history): ASSIGN = [x.get('train_loss') for x in history] ASSIGN = [x['val_loss'] for x in history] plt.plot(ASSIGN, '-bx') plt.plot(ASSIGN, '-rx') plt.xlabel('epoch') plt.ylabel('loss') plt.legend(['Training', 'Validation']) plt.title('Loss vs. No. of epochs');",0,not_existent,"def plot_losses(history):     train_losses = [x.get('train_loss') for x in history]     val_losses = [x['val_loss'] for x in history]     plt.plot(train_losses, '-bx')     plt.plot(val_losses, '-rx')     plt.xlabel('epoch')     plt.ylabel('loss')     plt.legend(['Training', 'Validation'])     plt.title('Loss vs. No. of epochs');"
plot_losses(history),0,not_existent,plot_losses(history)
"def plot_lrs(history): ASSIGN = np.concatenate([x.get('ASSIGN', []) for x in history]) plt.plot(ASSIGN) plt.xlabel('Batch no.') plt.ylabel('Learning rate') plt.title('Learning Rate vs. Batch no.');",0,not_existent,"def plot_lrs(history):     lrs = np.concatenate([x.get('lrs', []) for x in history])     plt.plot(lrs)     plt.xlabel('Batch no.')     plt.ylabel('Learning rate')     plt.title('Learning Rate vs. Batch no.');"
plot_lrs(history),0,not_existent,plot_lrs(history)
"SETUP jovian.commit(project=project_name) ASSIGN='..path' jovian.log_dataset(ASSIGN=ASSIGN, val_size=val_size, random_seed=random_seed)",0,not_existent,"!pip install jovian --upgrade --quiet import jovian jovian.commit(project=project_name) dataset_url='../input/russian-handwritten-letters' jovian.log_dataset(dataset_url=dataset_url, val_size=val_size, random_seed=random_seed)"
SETUP CHECKPOINT ASSIGN=tf.keras.regularizers.l2(0.01) print(tf.__version__),0,stream,"from __future__ import print_function
 import keras
 from keras.preprocessing.image import ImageDataGenerator
 from keras.models import Sequential
 from keras.layers import Dense, Dropout, Activation, Flatten
 from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, MaxPool2D, AveragePooling2D
 import os
 
 import numpy as np
 
 import seaborn as sns
 import matplotlib
 import matplotlib.pyplot as plt
 
 from sklearn.metrics import confusion_matrix, classification_report
 import itertools
 
 %matplotlib inline
 # TensorFlow and tf.keras
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras.models import Sequential
 from tensorflow.keras.layers import Dense, Dropout,Flatten
 from tensorflow.keras.optimizers import Adam, SGD
 from tensorflow.keras.losses import SparseCategoricalCrossentropy
 from tensorflow.keras.callbacks import TensorBoard,EarlyStopping
 %load_ext tensorboard
 
 # Helper libraries
 import numpy as np
 import random
 import matplotlib.pyplot as plt
 import datetime
 
 from keras.models import Sequential
 from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout
 from keras.regularizers import l2
 from keras.regularizers import l1
 from tensorflow.keras import regularizers
 
 import tensorflow as tf
 from tensorflow import keras
 from tensorflow.keras.models import Sequential
 from tensorflow.keras.callbacks import TensorBoard,EarlyStopping
 
 kernel_regularizer=tf.keras.regularizers.l2(0.01)
 import tensorflow as tf
 from tensorflow.keras.preprocessing import image_dataset_from_directory
 
 print(tf.__version__)"
"SETUP ASSIGN = '..path' ASSIGN = '..path' ASSIGN = 150 ASSIGN = 150 ASSIGN = 100 ASSIGN = 32 ASSIGN = 5736 ASSIGN = 2460 ASSIGN = ImageDataGenerator(rescale=1. path, ASSIGN=40, ASSIGN=0.2, ASSIGN=0.2, ASSIGN=0.2, ASSIGN=0.2, ASSIGN=True, ASSIGN='nearest') ASSIGN = ImageDataGenerator(rescale=1. path) ASSIGN = train_datagen.flow_from_directory(train_data_path, ASSIGN=(img_rows, img_cols), ASSIGN=ASSIGN, ASSIGN='categorical') ASSIGN = test_datagen.flow_from_directory(test_data_path, ASSIGN=(img_rows, img_cols), ASSIGN=ASSIGN, ASSIGN='categorical')",0,stream,"import numpy as np
 from keras import backend as K
 from keras.models import Sequential
 from keras.layers.core import Dense, Dropout, Activation, Flatten
 from keras.layers.convolutional import Convolution2D, MaxPooling2D
 from keras.preprocessing.image import ImageDataGenerator
 from sklearn.metrics import classification_report, confusion_matrix
 
 #Start
 train_data_path = '../input/animal1209/animal_dataset_intermediate_new/train_split/train'
 test_data_path = '../input/animal1209/animal_dataset_intermediate_new/train_split/val'
 img_rows = 150
 img_cols = 150
 epochs = 100
 batch_size = 32
 num_of_train_samples = 5736
 num_of_test_samples = 2460
 
 #Image Generator
 train_datagen = ImageDataGenerator(rescale=1. / 255,
                                    rotation_range=40,
                                    width_shift_range=0.2,
                                    height_shift_range=0.2,
                                    shear_range=0.2,
                                    zoom_range=0.2,
                                    horizontal_flip=True,
                                    fill_mode='nearest')
 
 test_datagen = ImageDataGenerator(rescale=1. / 255)
 
 train_generator = train_datagen.flow_from_directory(train_data_path,
                                                     target_size=(img_rows, img_cols),
                                                     batch_size=batch_size,
                                                     class_mode='categorical')
 
 validation_generator = test_datagen.flow_from_directory(test_data_path,
                                                         target_size=(img_rows, img_cols),
                                                         batch_size=batch_size,
                                                         class_mode='categorical')
"
"SETUP ASSIGN = image_dataset_from_directory( ASSIGN=r""..path"", ASSIGN = ""inferred"", label_mode = 'int', ASSIGN = 0.2, ASSIGN = ""training"", ASSIGN = 1337, ASSIGN=(224, 224), ASSIGN=32 ) plt.figure(figsize=(10, 10)) for images, ASSIGN in ASSIGN.take(1): for i in range(9): ASSIGN = plt.subplot(3, 3, i + 1) plt.imshow(images[i].numpy().astype(""uint8"")) plt.title(int(ASSIGN[i])) plt.axis(""off"")",0,stream,"
 train_generator_2 = image_dataset_from_directory(
     directory=r""../input/animal1209/animal_dataset_intermediate_new/train_split/train"",
     labels = ""inferred"", label_mode = 'int',
     validation_split = 0.2,
     subset = ""training"",
     seed = 1337,
     image_size=(224, 224),
     batch_size=32
 )
 
 #visualizing the data
 import matplotlib.pyplot as plt
 plt.figure(figsize=(10, 10))
 for images, labels in train_generator_2.take(1):
     for i in range(9):
         ax = plt.subplot(3, 3, i + 1)
         plt.imshow(images[i].numpy().astype(""uint8""))
         plt.title(int(labels[i]))
         plt.axis(""off"")"
"SETUP ASSIGN=STEP_SIZE_TRAIN ASSIGN = tf.keras.optimizers.schedules.InverseTimeDecay( 0.005, decay_steps=ASSIGN*1000,decay_rate=1,staircase=False) ASSIGN = SGD(lr_schedule) ASSIGN= SGD(lr = 0.01)",0,not_existent,"#Define optimizer 
 
 STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size
 steps_per_epoch=STEP_SIZE_TRAIN 
 
 lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(
   0.005, decay_steps=steps_per_epoch*1000,decay_rate=1,staircase=False)
 
 optimizer_2 = SGD(lr_schedule)
 optimizer= SGD(lr = 0.01)"
"ASSIGN = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, ASSIGN=5, min_lr=0.001) ASSIGN=[reduce_lr] ASSIGN = EarlyStopping( ASSIGN='val_loss', min_delta=0, patience=10, verbose=0, mode='auto', ASSIGN=None, restore_best_weights=True)",0,not_existent,"# Define  Callbacks 
 
 reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                               patience=5, min_lr=0.001)
 callbacks=[reduce_lr]
 
 earlystopping_callback = EarlyStopping(
     monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto',
     baseline=None, restore_best_weights=True)"
"ASSIGN = Sequential() ASSIGN.add(Flatten(input_shape=(224, 224, 3))) ASSIGN.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l1(0.01))) ASSIGN.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l1(0.01))) ASSIGN.add(Dropout(0.5)) ASSIGN.add(BatchNormalization()) ASSIGN.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l1(0.01))) ASSIGN.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l1(0.01))) ASSIGN.add(Dropout(0.5)) ASSIGN.add(BatchNormalization()) ASSIGN.add(Dense(5, activation='softmax')) ASSIGN.summary() ASSIGN.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics=['accuracy'])",0,stream,"#MLP model 62%/42%
 
 model3 = Sequential()
 #input layer size is 784 after flattening
 model3.add(Flatten(input_shape=(224, 224, 3)))
   
 #hidden layer with 512 neurons
 model3.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l1(0.01)))
 model3.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l1(0.01)))
 model3.add(Dropout(0.5))
 model3.add(BatchNormalization())
 
 model3.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l1(0.01)))
 model3.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l1(0.01)))
 model3.add(Dropout(0.5))
 model3.add(BatchNormalization())
 model3.add(Dense(5, activation='softmax'))
 
 model3.summary()
 
 
 # compile model
 model3.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics=['accuracy'])"
"ASSIGN = Sequential() ASSIGN.add(Convolution2D(32, (3, 3), input_shape=(img_rows, img_cols, 3), padding='valid')) ASSIGN.add(Activation('relu')) ASSIGN.add(MaxPooling2D(pool_size=(2, 2))) ASSIGN.add(Convolution2D(32, (3, 3), padding='valid')) ASSIGN.add(Activation('relu')) ASSIGN.add(MaxPooling2D(pool_size=(2, 2))) ASSIGN.add(Convolution2D(64, (3, 3), padding='valid')) ASSIGN.add(Activation('relu')) ASSIGN.add(MaxPooling2D(pool_size=(2, 2))) ASSIGN.add(Flatten()) ASSIGN.add(Dense(64)) ASSIGN.add(Activation('relu')) ASSIGN.add(Dropout(0.5)) ASSIGN.add(Dense(5)) ASSIGN.add(Activation('softmax')) ASSIGN.summary() ASSIGN.compile(loss='categorical_crossentropy', ASSIGN='rmsprop', ASSIGN=['accuracy'])",0,stream,"# CNN model
 
 model = Sequential()
 model.add(Convolution2D(32, (3, 3), input_shape=(img_rows, img_cols, 3), padding='valid'))
 model.add(Activation('relu'))
 model.add(MaxPooling2D(pool_size=(2, 2)))
 
 model.add(Convolution2D(32, (3, 3), padding='valid'))
 model.add(Activation('relu'))
 model.add(MaxPooling2D(pool_size=(2, 2)))
 
 model.add(Convolution2D(64, (3, 3), padding='valid'))
 model.add(Activation('relu'))
 model.add(MaxPooling2D(pool_size=(2, 2)))
 
 model.add(Flatten())
 model.add(Dense(64))
 model.add(Activation('relu'))
 model.add(Dropout(0.5))
 model.add(Dense(5))
 model.add(Activation('softmax'))
 
 model.summary()
 
 model.compile(loss='categorical_crossentropy',
               optimizer='rmsprop',
               metrics=['accuracy'])"
"model.fit_generator(train_generator, ASSIGN=num_of_train_samples path, ASSIGN=ASSIGN, ASSIGN=validation_generator, ASSIGN=num_of_test_samples path)",0,stream,"#Train
 model.fit_generator(train_generator,
                     steps_per_epoch=num_of_train_samples // batch_size,
                     epochs=epochs,
                     validation_data=validation_generator,
                     validation_steps=num_of_test_samples // batch_size)
 
"
model.save_weights('model.h5'),0,not_existent,model.save_weights('model.h5')
"SETUP CHECKPOINT print(tf.__version__) ASSIGN = ""logspath"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"") ASSIGN = TensorBoard(log_dir=log_dir, histogram_freq=1)",0,stream,"
 # TensorFlow and tf.keras
 
 %load_ext tensorboard
 import datetime
 print(tf.__version__)
 
 # run the tensorboard command to view the visualizations.
 
 log_dir = ""logs/fit/"" + datetime.datetime.now().strftime(""%Y%m%d-%H%M%S"")
 tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)
 %tensorboard --logdir logs/fit"
"SETUP model.evaluate(validation_generator, ASSIGN=STEP_SIZE_VALID)",0,stream,"#Evaluate the model 
 STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size
 STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size
 model.evaluate(validation_generator,
 steps=STEP_SIZE_VALID)"
"ASSIGN = model.predict_generator(validation_generator, num_of_test_samples) ASSIGN = np.argmax(Y_pred, axis=1) ASSIGN[200]",0,execute_result,"
 Y_pred = model.predict_generator(validation_generator, num_of_test_samples)
 y_pred = np.argmax(Y_pred, axis=1)
 
 y_pred[200]
 
"
CHECKPOINT y_pred.shape,0,execute_result,y_pred.shape
CHECKPOINT ASSIGN = [labels[i] for i in y_pred] predictions,1,execute_result,"predictions = [labels[i] for i in y_pred]
 predictions"
"CHECKPOINT ASSIGN = ['elefante_train', 'farfalla_train', 'mucca_train','pecora_train','scoiattolo_train'] print(classification_report(validation_generator.labels, y_pred, ASSIGN=ASSIGN))",1,stream,"
 target_names = ['elefante_train', 'farfalla_train', 'mucca_train','pecora_train','scoiattolo_train']
 print(classification_report(validation_generator.labels, y_pred, target_names=target_names))"
"ASSIGN = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.0path) ASSIGN = test_datagen.flow_from_directory(""..path"", ASSIGN = 'categorical', ASSIGN = (150, 150))",0,stream,"#Extract the test data => I didnt find a way without creating a new folder on colab
 
 test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.0/255.)
 
 test_generator = test_datagen.flow_from_directory(""../input/animal-ori/animal_dataset_intermediate"",
                                                     
                                                     class_mode = 'categorical', 
                                                     target_size = (150, 150))"
"CHECKPOINT ASSIGN = 9106 ASSIGN = model.predict_generator(test_generator) ASSIGN = np.argmax(Y_pred_test, axis=1) y_pred_test",1,execute_result,"#prediction on test data 
 
 num = 9106
 
 Y_pred_test = model.predict_generator(test_generator)
 y_pred_test = np.argmax(Y_pred_test, axis=1)
 y_pred_test
"
CHECKPOINT Y_pred_test,0,execute_result,Y_pred_test
ASSIGN = y_pred_test[0:910],1,not_existent,y_final = y_pred_test[0:910]
"CHECKPOINT print(*y_final, sep = )",0,stream,"print(*y_final, sep = "", "")  "
"ASSIGN = pd.DataFrame(y_final) ASSIGN.columns = [""prediction""] ASSIGN.to_csv(""prediction_results.csv"")   # the csv file will be saved locally on the same location where this notebook is located.",1,not_existent,"res = pd.DataFrame(y_final) #preditcions are nothing but the final predictions of your model on input features of your new unseen test data
  # its important for comparison. Here ""test_new"" is your new test dataset
 res.columns = [""prediction""]
 res.to_csv(""prediction_results.csv"")      # the csv file will be saved locally on the same location where this notebook is located."
CHECKPOINT res,0,execute_result,res
CHECKPOINT ASSIGN = pd.read_csv('..path') im,0,execute_result,"im = pd.read_csv('../input/animal-ori/animal_dataset_intermediate/Testing_set_animals.csv')
 im
 
"
"ASSIGN = pd.concat([im,res], axis = 1)",1,not_existent,"rei = pd.concat([im,res],  axis = 1) "
"rei.drop('target', axis = 1)",1,execute_result,"rei.drop('target', axis = 1)"
CHECKPOINT ASSIGN = ASSIGN rei,1,execute_result,"rei['animal'] = rei['prediction'] 
 rei"
"ASSIGN =ASSIGN.replace(to_replace=2,value = ""mucca"")",1,not_existent,"rei['animal'] =rei['animal'].replace(to_replace=2,value = ""mucca"") "
rei.head(100),0,execute_result,rei.head(100)
"ASSIGN =ASSIGN.replace(to_replace=3,value = ""pecora"")",1,not_existent,"rei['animal'] =rei['animal'].replace(to_replace=3,value = ""pecora"") "
"ASSIGN =ASSIGN.replace(to_replace=4,value = ""scoiattolo"")",1,not_existent,"
 rei['animal'] =rei['animal'].replace(to_replace=4,value = ""scoiattolo"") "
"ASSIGN =ASSIGN.replace(to_replace=0,value = ""elefante"")",1,not_existent,"rei['animal'] =rei['animal'].replace(to_replace=0,value = ""elefante"") "
CHECKPOINT rei,0,execute_result,rei
"ASSIGN =ASSIGN.replace(to_replace=1,value = ""farfalla"")",1,not_existent,"rei['animal'] =rei['animal'].replace(to_replace=1,value = ""farfalla"") "
rei.head(200),0,execute_result,rei.head(200)
rei.to_csv('final'),0,not_existent,rei.to_csv('final')
CHECKPOINT y_final.shape,0,execute_result,y_final.shape
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'IRIS_TYPE_CLF.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,stream,"nRowsRead = 1000 # specify 'None' if want to read whole file
 # IRIS_TYPE_CLF.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows
 df1 = pd.read_csv('/kaggle/input/IRIS_TYPE_CLF.csv', delimiter=',', nrows = nRowsRead)
 df1.dataframeName = 'IRIS_TYPE_CLF.csv'
 nRow, nCol = df1.shape
 print(f'There are {nRow} rows and {nCol} columns')"
df1.head(5),0,execute_result,df1.head(5)
"plotPerColumnDistribution(df1, 10, 5)",0,display_data,"plotPerColumnDistribution(df1, 10, 5)"
"plotCorrelationMatrix(df1, 8)",0,display_data,"plotCorrelationMatrix(df1, 8)"
"plotScatterMatrix(df1, 12, 10)",0,display_data,"plotScatterMatrix(df1, 12, 10)"
SETUP,0,not_existent,import os import cv2 import numpy as np
"SETUP def DisplayImage(image,title,cols=1): ASSIGN=len(image)+1 ASSIGN=0 plt.figure(figsize=(8, 8)) for i in range(1,ASSIGN): ASSIGN+=1 plt.subplot(1,cols,ASSIGN),plt.imshow(image[i-1],cmap = 'gray'), plt.title(title[i-1]), plt.axis('off') if ( i%cols==0): plt.show(),plt.figure(figsize=(8, 8)) ASSIGN=0",0,not_existent,"def DisplayImage(image,title,cols=1):        import matplotlib.pyplot as plt          image_no=len(image)+1     postion=0      plt.figure(figsize=(8, 8))     for i in range(1,image_no):         postion+=1         plt.subplot(1,cols,postion),plt.imshow(image[i-1],cmap = 'gray'), plt.title(title[i-1]), plt.axis('off')                 if ( i%cols==0):             plt.show(),plt.figure(figsize=(8, 8))             postion=0"
"ASSIGN=[] ASSIGN=[] for dirname, _, filenames in os.walk('..path'): for filename in filenames[0:9]: ASSIGN.append( cv2.imread(dirname+""path""+ filename,0)) ASSIGN.append(""1"") DisplayImage(ASSIGN,ASSIGN,3)",0,display_data,"images=[] titles=[] for dirname, _, filenames in os.walk('../input/digits-in-noise/Test'):     for filename in filenames[0:9]:                 images.append( cv2.imread(dirname+""/""+ filename,0))         titles.append(""1"")                   DisplayImage(images,titles,3)"
"SETUP warnings.filterwarnings(""ignore"") init_notebook_mode(connected=True) matplotlib.rc('font', size=20) matplotlib.rc('axes', titlesize=20) matplotlib.rc('axes', labelsize=20) matplotlib.rc('xtick', labelsize=20) matplotlib.rc('ytick', labelsize=20) matplotlib.rc('legend', fontsize=20) matplotlib.rc('figure', titlesize=20)",0,not_existent,"import gc import warnings warnings.filterwarnings(""ignore"")  import pandas as pd import numpy as np from IPython.display import display from IPython.core.display import HTML import plotly.plotly as py from plotly.offline import init_notebook_mode, iplot init_notebook_mode(connected=True) import plotly.graph_objs as go import matplotlib.pyplot as plt import matplotlib #matplotlib.rc['font.size'] = 9.0 matplotlib.rc('font', size=20) matplotlib.rc('axes', titlesize=20) matplotlib.rc('axes', labelsize=20) matplotlib.rc('xtick', labelsize=20) matplotlib.rc('ytick', labelsize=20) matplotlib.rc('legend', fontsize=20) matplotlib.rc('figure', titlesize=20) import seaborn as sns  %matplotlib inline"
"SETUP CHECKPOINT def file_len(fname): ASSIGN = subprocess.Popen(['wc', '-l', fname], stdout=subprocess.PIPE, ASSIGN=subprocess.PIPE) ASSIGN = p.communicate() if ASSIGN.returncode != 0: raise IOError(err) return int(result.strip().split()[0]) ASSIGN = file_len('..path') print('Number of ASSIGN in is:', ASSIGN)",1,not_existent,"import subprocess #from https://stackoverflow.com/questions/845058/how-to-get-line-count-cheaply-in-python , Olafur's answer def file_len(fname):     p = subprocess.Popen(['wc', '-l', fname], stdout=subprocess.PIPE,                                                stderr=subprocess.PIPE)     result, err = p.communicate()     if p.returncode != 0:         raise IOError(err)     return int(result.strip().split()[0])  lines = file_len('../input/data.csv') print('Number of lines in ""train.csv"" is:', lines)"
"CHECKPOINT ASSIGN = np.random.choice(np.arange(1, lines), size=lines-1-1000000, replace=False) ASSIGN=np.sort(ASSIGN) print('lines to skip:', len(ASSIGN)) ASSIGN = pd.read_csv(""..path"", skiprows=skiplines)",1,not_existent,"skiplines = np.random.choice(np.arange(1, lines), size=lines-1-1000000, replace=False) skiplines=np.sort(skiplines) print('lines to skip:', len(skiplines))  data = pd.read_csv(""../input/data.csv"", skiprows=skiplines)"
data.sample(5),0,not_existent,data.sample(5)
data.isnull().sum(0),0,not_existent,data.isnull().sum(0)
"ASSIGN={ 1:""Jan"", 2:""Feb"", 3:""Mar"", 4:""Apr"", 5:""May"", 6:""June"", 7:""July"", 8:""Aug"", 9:""Sept"", 10:""Oct"", 11:""Nov"", 12:""Dec"" } ASSIGN = data.month.apply(lambda x: ASSIGN)",1,not_existent,"# Just a helper module to make visualizations more intuitive num_to_month={     1:""Jan"",     2:""Feb"",     3:""Mar"",     4:""Apr"",     5:""May"",     6:""June"",     7:""July"",     8:""Aug"",     9:""Sept"",     10:""Oct"",     11:""Nov"",     12:""Dec"" } data['month'] = data.month.apply(lambda x: num_to_month[x])"
"ASSIGN = data.pivot_table(index='year', columns='month', values='day', aggfunc=len) ASSIGN = [""#8B8B00"", ""#8B7E66"", ""#EE82EE"", ""#00C78C"", ""#00E5EE"", ""#FF6347"", ""#EED2EE"", ""#63B8FF"", ""#00FF7F"", ""#B9D3EE"", ""#836FFF"", ""#7D26CD""] ASSIGN.loc[:,['Jan','Feb', 'Mar', 'Apr','May','June', 'July','Aug','Sept', 'Oct','Nov','Dec']].plot.bar(stacked=True, figsize=(20,10), color=ASSIGN) plt.xlabel(""Years"") plt.ylabel(""Ridership"") plt.legend(loc=10) plt.show()",0,not_existent,"pivot = data.pivot_table(index='year', columns='month', values='day', aggfunc=len) colors = [""#8B8B00"", ""#8B7E66"", ""#EE82EE"", ""#00C78C"",            ""#00E5EE"", ""#FF6347"", ""#EED2EE"",            ""#63B8FF"", ""#00FF7F"", ""#B9D3EE"",            ""#836FFF"", ""#7D26CD""] pivot.loc[:,['Jan','Feb', 'Mar',             'Apr','May','June',             'July','Aug','Sept',             'Oct','Nov','Dec']].plot.bar(stacked=True, figsize=(20,10), color=colors) plt.xlabel(""Years"") plt.ylabel(""Ridership"") plt.legend(loc=10) plt.show()"
"ASSIGN = plt.subplots(1,2, figsize=(20,7)) ASSIGN = [' ASSIGN = ax[0].ASSIGN(list(data['gender'].value_counts()), ASSIGN=list(data.gender.unique()), ASSIGN='%1.1f%%', shadow=True, startangle=90, colors=colors) ASSIGN = sns.countplot(x='usertype', data=data, ax=ax[1], color='g', alpha=0.75) ax[0].set_title(""Gender Distribution in Ridership"") ax[1].set_xlabel(""Type of Rider"") ax[1].set_ylabel(""Ridership"") ax[1].set_title(""Type of Customers"")",0,not_existent,"f, ax = plt.subplots(1,2, figsize=(20,7)) colors = ['#66b3ff','#ff9999'] pie = ax[0].pie(list(data['gender'].value_counts()),                     labels=list(data.gender.unique()),                   autopct='%1.1f%%', shadow=True, startangle=90, colors=colors) count = sns.countplot(x='usertype', data=data, ax=ax[1], color='g', alpha=0.75) ax[0].set_title(""Gender Distribution in Ridership"") ax[1].set_xlabel(""Type of Rider"") ax[1].set_ylabel(""Ridership"") ax[1].set_title(""Type of Customers"")"
data.usertype.value_counts(),0,not_existent,data.usertype.value_counts()
"ASSIGN = data[['from_station_name','latitude_start','longitude_start']].drop_duplicates(subset='from_station_name')",1,not_existent,"station_info = data[['from_station_name','latitude_start','longitude_start']].drop_duplicates(subset='from_station_name')"
station_info.sample(5),0,not_existent,station_info.sample(5)
ASSIGN = list(station_info.latitude_start) ASSIGN = [str(i) for i in ASSIGN] ASSIGN = list(station_info.longitude_start) ASSIGN = [str(i) for i in ASSIGN] ASSIGN = list(station_info.from_station_name),1,not_existent,lat_list = list(station_info.latitude_start) lat_list = [str(i) for i in lat_list] lon_list = list(station_info.longitude_start) lon_list = [str(i) for i in lon_list] names = list(station_info.from_station_name)
"display(HTML("""""" <div> <a href=""https:path~sominwpath?share_key=y6irxkKqSVolnuF0l4w420"" target=""_blank"" title=""Chicago Cycle Sharing Stations"" style=""display: block; text-align: center;""><img src=""https:path~sominwpath?share_key=y6irxkKqSVolnuF0l4w420"" alt=""Chicago Cycle Sharing Stations"" style=""max-width: 100%;width: 600px;""  width=""600"" onerror=""this.onerror=null;this.src='https:path';"" path><path> <script data-plotly=""sominw:6"" sharekey-plotly=""y6irxkKqSVolnuF0l4w420"" src=""https:path"" async><path> <path>""""""))",0,not_existent,"display(HTML("""""" <div>     <a href=""https://plot.ly/~sominw/6/?share_key=y6irxkKqSVolnuF0l4w420"" target=""_blank"" title=""Chicago Cycle Sharing Stations"" style=""display: block; text-align: center;""><img src=""https://plot.ly/~sominw/6.png?share_key=y6irxkKqSVolnuF0l4w420"" alt=""Chicago Cycle Sharing Stations"" style=""max-width: 100%;width: 600px;""  width=""600"" onerror=""this.onerror=null;this.src='https://plot.ly/404.png';"" /></a>     <script data-plotly=""sominw:6"" sharekey-plotly=""y6irxkKqSVolnuF0l4w420"" src=""https://plot.ly/embed.js"" async></script> </div>""""""))"
SETUP,0,not_existent,"import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 %matplotlib inline
 import seaborn as sns
 import time"
SETUP,0,not_existent,"#Borrowed from Faron's Road to 4 kernel
 DATA_DIR = ""../input""
 
 ID_COLUMN = 'Id'
 TARGET_COLUMN = 'Response'
 
 SEED = 0
 CHUNKSIZE = 10000
 NROWS = 50000
 
 TRAIN_NUMERIC = ""{0}/train_numeric.csv"".format(DATA_DIR)
 TRAIN_DATE = ""{0}/train_date.csv"".format(DATA_DIR)
 
 TEST_NUMERIC = ""{0}/test_numeric.csv"".format(DATA_DIR)
 TEST_DATE = ""{0}/test_date.csv"".format(DATA_DIR)"
"ASSIGN = pd.read_csv(TRAIN_NUMERIC, usecols=[ID_COLUMN, TARGET_COLUMN], nrows=NROWS) ASSIGN = pd.read_csv(TEST_NUMERIC, usecols=[ID_COLUMN], nrows=NROWS) ASSIGN = -1 ASSIGN = -1 ASSIGN = -1 ASSIGN = -1",1,not_existent,"#Borrowed from Faron's Road to 4 kernel
 
 train = pd.read_csv(TRAIN_NUMERIC, usecols=[ID_COLUMN, TARGET_COLUMN], nrows=NROWS)
 test = pd.read_csv(TEST_NUMERIC, usecols=[ID_COLUMN], nrows=NROWS)
 
 train[""StartTime""] = -1
 test[""StartTime""] = -1
 train[""EndTime""] = -1
 test[""EndTime""] = -1"
"CHECKPOINT ASSIGN = 0 print ('ASSIGN',) ASSIGN = pd.read_csv(TRAIN_DATE, chunksize=CHUNKSIZE, iterator=True) ASSIGN = pd.read_csv(TEST_DATE, chunksize=CHUNKSIZE, iterator=True) for i in range(int(NROWSpath) + 1): ASSIGN = train_reader.get_chunk() ASSIGN = test_reader.get_chunk() ASSIGN = np.setdiff1d(tr.columns, [ID_COLUMN]) ASSIGN = tr[feats].min(axis=1).values ASSIGN = te[feats].min(axis=1).values ASSIGN = tr[feats].max(axis=1).values ASSIGN = te[feats].max(axis=1).values train.loc[train.Id.isin(ASSIGN.Id), 'StartTime'] = ASSIGN test.loc[test.Id.isin(ASSIGN.Id), 'StartTime'] = ASSIGN train.loc[train.Id.isin(ASSIGN.Id), 'EndTime'] = ASSIGN test.loc[test.Id.isin(ASSIGN.Id), 'EndTime'] = ASSIGN ASSIGN += CHUNKSIZE print (ASSIGN,) if ASSIGN >= NROWS: break",1,stream,"#Borrowed from Faron's Road to 4 kernel
 
 nrows = 0
 print ('nrows',)
 
 train_reader = pd.read_csv(TRAIN_DATE, chunksize=CHUNKSIZE, iterator=True)
 test_reader = pd.read_csv(TEST_DATE, chunksize=CHUNKSIZE, iterator=True)
 for i in range(int(NROWS/CHUNKSIZE) + 1):
     tr = train_reader.get_chunk()
     te = test_reader.get_chunk()
 
 #for tr, te in zip(pd.read_csv(TRAIN_DATE, chunksize=CHUNKSIZE), pd.read_csv(TEST_DATE, chunksize=CHUNKSIZE)):
     feats = np.setdiff1d(tr.columns, [ID_COLUMN])    
 
     stime_tr = tr[feats].min(axis=1).values
     stime_te = te[feats].min(axis=1).values
 
     etime_tr = tr[feats].max(axis=1).values
     etime_te = te[feats].max(axis=1).values
     
     train.loc[train.Id.isin(tr.Id), 'StartTime'] = stime_tr
     test.loc[test.Id.isin(te.Id), 'StartTime'] = stime_te
 
     train.loc[train.Id.isin(tr.Id), 'EndTime'] = etime_tr
     test.loc[test.Id.isin(te.Id), 'EndTime'] = etime_te
     
     nrows += CHUNKSIZE
     print (nrows,)
     if nrows >= NROWS:
         break"
"ASSIGN = train.shape[0] ASSIGN = pd.concat((train, test)).reset_index(drop=True).reset_index(drop=False) ASSIGN['Duration'] = ASSIGN['EndTime'] - ASSIGN['StartTime'] ASSIGN['magic1'] = ASSIGN[ID_COLUMN].diff().fillna(9999999).astype(int) ASSIGN['magic2'] = ASSIGN[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int) ASSIGN = ASSIGN.sort_values(by=['StartTime', 'Id'], ascending=True) ASSIGN['magic3'] = ASSIGN[ID_COLUMN].diff().fillna(9999999).astype(int) ASSIGN['magic4'] = ASSIGN[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int) ASSIGN = ASSIGN.sort_values(by=['index']).drop(['index'], axis=1) ASSIGN = train_test.iloc[:ntrain, :]",1,not_existent,"#Borrowed from Faron's Road to 4 kernel
 #HAD to change the names so they are easy to type. what can I say \_()_/
 
 ntrain = train.shape[0]
 train_test = pd.concat((train, test)).reset_index(drop=True).reset_index(drop=False)
 
 train_test['Duration'] = train_test['EndTime'] - train_test['StartTime']
 
 train_test['magic1'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)
 train_test['magic2'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)
 
 train_test = train_test.sort_values(by=['StartTime', 'Id'], ascending=True)
 
 train_test['magic3'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)
 train_test['magic4'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)
 
 train_test = train_test.sort_values(by=['index']).drop(['index'], axis=1)
 train = train_test.iloc[:ntrain, :]"
"CHECKPOINT def twoplot(df, col, xaxis=None): ''' scatter plot a feature split into response values as two subgraphs ''' if col not in df.columns.values: print('ERROR: %s not a column' % col) ASSIGN = pd.DataFrame(index = df.index) ASSIGN = ASSIGN ASSIGN = ASSIGN if xaxis else df.index ASSIGN = ASSIGN ASSIGN = sns.FacetGrid(ndf, col=""Response"", hue=""Response"") ASSIGN.map(plt.scatter, xaxis, col, alpha=.7, s=1) ASSIGN.add_legend(); del ndf",1,not_existent,"def twoplot(df, col, xaxis=None):
     ''' scatter plot a feature split into response values as two subgraphs '''
     if col not in df.columns.values:
         print('ERROR: %s not a column' % col)
     ndf = pd.DataFrame(index = df.index)
     ndf[col] = df[col]
     ndf[xaxis] = df[xaxis] if xaxis else df.index
     ndf['Response'] = df['Response']
     
     g = sns.FacetGrid(ndf, col=""Response"", hue=""Response"")
     g.map(plt.scatter, xaxis, col, alpha=.7, s=1)
     g.add_legend();
     
     del ndf"
"twoplot(train, 'magic1')",0,display_data,"twoplot(train, 'magic1')"
"twoplot(train, 'magic2')",0,display_data,"twoplot(train, 'magic2')"
"twoplot(train, 'magic3')",0,display_data,"twoplot(train, 'magic3')"
"twoplot(train, 'magic4')",0,display_data,"twoplot(train, 'magic4')"
"twoplot(train, 'Duration')",0,display_data,"twoplot(train, 'Duration')"
"twoplot(train, 'StartTime')",0,display_data,"twoplot(train, 'StartTime')"
"twoplot(train, 'EndTime')",0,display_data,"twoplot(train, 'EndTime')"
SETUP CHECKPOINT print(os.listdir()) print(os.listdir()),0,not_existent,"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input"")) print(os.listdir(""../input/A_Z Handwritten Data""))  # Any results you write to the current directory are saved as output."
"ASSIGN= ""..path"" ASSIGN= ""..path"" ASSIGN=pd.read_csv(data_train_file) ASSIGN= pd.read_csv(data_test_file)",0,not_existent,"data_train_file= ""../input/A_Z Handwritten Data/A_Z Handwritten Data.csv"" data_test_file= ""../input/A_Z Handwritten Data/A_Z Handwritten Data.csv""  df_train=pd.read_csv(data_train_file) df_test= pd.read_csv(data_test_file) "
df_train.head(),0,not_existent,df_train.head()
"SETUP for i in range(5000,5005): ASSIGN =np.reshape(df_test[df_test.columns[1:]].iloc[i].valuespath,(28,28)) plt.figure() plt.title(""labelled cs {}"".format(df_test[""0""].iloc[i])) plt.imshow(ASSIGN,'gray')",0,not_existent,"import matplotlib.pyplot as plt  %matplotlib inline for i in range(5000,5005):     sample =np.reshape(df_test[df_test.columns[1:]].iloc[i].values/255,(28,28))     plt.figure()     plt.title(""labelled cs {}"".format(df_test[""0""].iloc[i]))     plt.imshow(sample,'gray')"
"SETUP ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"") np.random.seed(0)",0,not_existent,"# modules we'll use import pandas as pd import numpy as np import seaborn as sns import datetime  # read in our data earthquakes = pd.read_csv(""../input/earthquake-database/database.csv"") landslides = pd.read_csv(""../input/landslide-events/catalog.csv"") volcanos = pd.read_csv(""../input/volcanic-eruptions/database.csv"")  # set seed for reproducibility np.random.seed(0)"
CHECKPOINT print(landslides['date'].head()),0,not_existent,# print the first few rows of the date column print(landslides['date'].head())
CHECKPOINT landslides['date'].dtype,0,not_existent,# check the data type of our date column landslides['date'].dtype
CHECKPOINT earthquakes['Date'].dtype,0,not_existent,# Your turn! Check the data type of the Date column in the earthquakes dataframe # (note the capital 'D' in date!) earthquakes['Date'].dtype
"landslides['date_parsed'] = pd.to_datetime(landslides['date'], format = ""%mpath%dpath%y"")",1,not_existent,"# create a new column, date_parsed, with the parsed dates landslides['date_parsed'] = pd.to_datetime(landslides['date'], format = ""%m/%d/%y"")"
landslides['date_parsed'].head(),0,not_existent,# print the first few rows landslides['date_parsed'].head()
"CHECKPOINT print(.format(earthquakes['Date'].dtype)) print(earthquakes['Date'].head()) earthquakes['date_parsed']=pd.to_datetime(earthquakes['Date'], format = ""%mpath%dpath%Y"") earthquakes['date_parsed'].head()",1,not_existent,"# Your turn! Create a new column, date_parsed, in the earthquakes # dataset that has correctly parsed dates in it. (Don't forget to  # double-check that the dtype is correct!) print(""the data type is '{}' "".format(earthquakes['Date'].dtype)) print(earthquakes['Date'].head())  #since the date format is supposebly american i will assume it is month/day/year earthquakes['date_parsed']=pd.to_datetime(earthquakes['Date'], format = ""%m/%d/%Y"") earthquakes['date_parsed'].head()"
earthquakes.iloc[3370:3390],0,not_existent,#check larger range-found it! earthquakes.iloc[3370:3390]
"earthquakes['date_parsed']=pd.to_datetime(earthquakes['Date'], infer_datetime_format=True) earthquakes['date_parsed'].head()",1,not_existent,"earthquakes['date_parsed']=pd.to_datetime(earthquakes['Date'], infer_datetime_format=True) earthquakes['date_parsed'].head()"
ASSIGN = landslides['date'].dt.day,1,not_existent,# try to get the day of the month from the date column day_of_month_landslides = landslides['date'].dt.day
ASSIGN = landslides['date_parsed'].dt.day,1,not_existent,# get the day of the month from the date_parsed column day_of_month_landslides = landslides['date_parsed'].dt.day
ASSIGN = earthquakes['date_parsed'].dt.day,1,not_existent,"# Your turn! get the day of the month from the date_parsed column  # Didn'understand much, i will do for earthquakes day_of_month_earthquakes = earthquakes['date_parsed'].dt.day"
"ASSIGN = ASSIGN.dropna() sns.distplot(ASSIGN, kde=False, bins=31)",0,not_existent,"# remove na's day_of_month_landslides = day_of_month_landslides.dropna()  # plot the day of the month sns.distplot(day_of_month_landslides, kde=False, bins=31)"
"ASSIGN = ASSIGN.dropna() sns.distplot(ASSIGN, kde=False, bins=31)",0,not_existent,"# Your turn! Plot the days of the month from your # earthquake dataset and make sure they make sense.  day_of_month_earthquakes = day_of_month_earthquakes.dropna() sns.distplot(day_of_month_earthquakes, kde=False, bins=31)"
volcanos['Last Known Eruption'].sample(5),0,not_existent,volcanos['Last Known Eruption'].sample(5)
SETUP,0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed
 # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
 # For example, here's several helpful packages to load in 
 
 import os
 import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. wpd.read_csv)
 import seaborn as sns
 import matplotlib.pyplot as plt
 from skopt import gp_minimize
"
CHECKPOINT ASSIGN = pd.read_csv('path') print(f'Classifier output data shape: {ASSIGN.shape}') ASSIGN.head(10),0,stream,"classifier_output = pd.read_csv('/kaggle/input/tractable_ds_excercise_data/classifier_output.csv')
 print(f'Classifier output data shape:  {classifier_output.shape}')
 classifier_output.head(10)"
CHECKPOINT classifier_output.dropna(inplace=True) print(f'Classifier output data shape without nans:  {classifier_output.shape}'),1,stream,"# Rows with no urr_score aren't needed for this analysis.  We're missing a lot for some reason
 classifier_output.dropna(inplace=True)
 print(f'Classifier output data shape without nans:  {classifier_output.shape}')"
"CHECKPOINT ASSIGN = [] for dirname, _, filenames in os.walk('path'): for filename in filenames: ASSIGN.append(os.path.join(dirname, filename)) ASSIGN = pd.concat([pd.read_csv(filepath) for filepath in metadata_files]) print(f'Line data shape: {ASSIGN.shape}') ASSIGN.head(10)",1,stream,"# Get line data
 
 metadata_files = []
 for dirname, _, filenames in os.walk('/kaggle/input/tractable_ds_excercise_data/metadata'):
     for filename in filenames:
         metadata_files.append(os.path.join(dirname, filename))
 
 line_data = pd.concat([pd.read_csv(filepath) for filepath in metadata_files])
 print(f'Line data shape: {line_data.shape}')
 line_data.head(10)"
CHECKPOINT print(f'Unique claims: {len(line_data[].unique())}'),0,stream,"# We seem to be missing about 5000 of the promised claims - perhaps ones where no repairs or replacements were made
 
 print(f'Unique claims: {len(line_data[""claim_id""].unique())}')"
"CHECKPOINT ASSIGN = classifier_output.merge(line_data[['claim_id', 'make', 'model', 'year','poi']].drop_duplicates(subset=['claim_id'], keep='first'), ASSIGN='left', on='claim_id') print(f'Classifier outputs not associated with a claim: {ASSIGN[].isna().sum()}') ASSIGN.dropna(subset=['make'], inplace=True) ASSIGN = pd.merge(claim_merged, line_data[['claim_id', 'line_num', 'part', 'operation', 'part_price', 'labour_amt']], ASSIGN='left', on=['claim_id', 'part']) ASSIGN['operation'].fillna('undamaged', inplace=True) print(f'Merge ASSIGN shape: {ASSIGN.shape}') ASSIGN.head(10)",1,stream,"# Merge the claim-level data first, and then the line-level data
 claim_merged = classifier_output.merge(line_data[['claim_id', 'make', 'model', 'year','poi']].drop_duplicates(subset=['claim_id'], keep='first'),
                                        how='left', on='claim_id')
 
 print(f'Classifier outputs not associated with a claim: {claim_merged[""make""].isna().sum()}')
 # Remove any classifier outputs that can't be associated with a claim
 claim_merged.dropna(subset=['make'], inplace=True)
 
 data = pd.merge(claim_merged, line_data[['claim_id', 'line_num', 'part', 'operation', 'part_price', 'labour_amt']],
                 how='left', on=['claim_id', 'part'])
 
 data['operation'].fillna('undamaged', inplace=True)
 print(f'Merge data shape: {data.shape}')
 data.head(10)"
"data['rounded_urr_score'] = data['urr_score'].apply(lambda x: round(x, 2)) ASSIGN = (data[(data['set']==2)][['rounded_urr_score', 'operation', 'urr_score']] .groupby(['rounded_urr_score', 'operation']) .count() .reset_index() .rename(columns={'urr_score': 'count'}) .set_index('rounded_urr_score') .pivot(columns='operation', values='count') .fillna(0) ) ASSIGN = ASSIGN[['undamaged', 'repair', 'replace']] ASSIGN.head(10)",1,execute_result,"# Visualise the effectiveness of the classifier on the test set
 
 data['rounded_urr_score'] = data['urr_score'].apply(lambda x: round(x, 2))
 
 bucket_counts = (data[(data['set']==2)][['rounded_urr_score', 'operation', 'urr_score']]
                  .groupby(['rounded_urr_score', 'operation'])
                  .count()
                  .reset_index()
                  .rename(columns={'urr_score': 'count'})
                  .set_index('rounded_urr_score')
                  .pivot(columns='operation', values='count')
                  .fillna(0)
                 )
 
 bucket_counts = bucket_counts[['undamaged', 'repair', 'replace']]
 
 bucket_counts.head(10)"
bucket_counts.sum(axis=1).plot.bar(),0,execute_result,bucket_counts.sum(axis=1).plot.bar()
"ASSIGN = bucket_counts.divide(bucket_counts.sum(axis=1), axis=0) ASSIGN.plot.area()",0,execute_result,"bucket_counts_divided = bucket_counts.divide(bucket_counts.sum(axis=1), axis=0)
 
 bucket_counts_divided.plot.area()"
"ASSIGN = {'undamaged': 0, 'repair': 1, 'replace': 2} data['operation_rank'] = data['operation'].apply(lambda x: ASSIGN[x]) def mae_single_point(urr_score, operation_rank, repair_threshold, replace_threshold): ASSIGN = int(urr_score > repair_threshold) + int(urr_score > replace_threshold) return abs(ASSIGN - operation_rank) assert(mae_single_point(0.9, 0, 0.4, 0.7) == 2) assert(mae_single_point(0.5, 1, 0.4, 0.7) == 0) assert(mae_single_point(0.5, 2, 0.4, 0.7) == 1)",0,not_existent,"operation_ranks = {'undamaged': 0,
                    'repair': 1,
                    'replace': 2}
 
 data['operation_rank'] = data['operation'].apply(lambda x: operation_ranks[x])
 
 def mae_single_point(urr_score, operation_rank, repair_threshold, replace_threshold):
     classified_outcome_rank = int(urr_score > repair_threshold) + int(urr_score > replace_threshold)
 
     return abs(classified_outcome_rank - operation_rank)
 
 assert(mae_single_point(0.9, 0, 0.4, 0.7) == 2)
 assert(mae_single_point(0.5, 1, 0.4, 0.7) == 0)
 assert(mae_single_point(0.5, 2, 0.4, 0.7) == 1)
     "
"def mae_dataset(data, repair_threshold, replace_threshold): ASSIGN =[] for i in range(2): ASSIGN = data[(data['operation_rank']==i)] ASSIGN = sum(class_data .apply(lambda row: mae_single_point(row['urr_score'], row['operation_rank'], repair_threshold, replace_threshold), axis=1))path(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN = sum(class_maes)path return total_mae",1,not_existent,"def mae_dataset(data, repair_threshold, replace_threshold):
     class_maes =[]
     for i in range(2):
         class_data = data[(data['operation_rank']==i)]
         class_mae = sum(class_data
                         .apply(lambda row: mae_single_point(row['urr_score'], row['operation_rank'], repair_threshold, replace_threshold), axis=1))/len(class_data)
         class_maes.append(class_mae)
     total_mae = sum(class_maes)/3
     return total_mae"
"CHECKPOINT ASSIGN = data[(data['set']==2)][['urr_score', 'operation_rank']] def mae(thresholds): return mae_dataset(ASSIGN, thresholds[0], thresholds[1]) ASSIGN = gp_minimize(mae, dimensions=[(0.0, 1.0, 'uniform'), (0.0, 1.0, 'uniform')], n_calls=50, verbose=True) print(f'Best thresholds: {ASSIGN.x}') print(f'Best average mse: {ASSIGN.fun}')",0,stream,"# Use only the test set to evaluate the best thresholds
 
 test_set = data[(data['set']==2)][['urr_score', 'operation_rank']]
 
 def mae(thresholds):
     return mae_dataset(test_set, thresholds[0], thresholds[1])
 
 # Calculating mse is somewhat expensive at a couple of seconds a time, so use an optimizer and small number of iterations
 # Takes about 2.5 minutes
 opt = gp_minimize(mae, dimensions=[(0.0, 1.0, 'uniform'), (0.0, 1.0, 'uniform')], n_calls=50, verbose=True)
 
 print(f'Best thresholds: {opt.x}')
 print(f'Best average mse: {opt.fun}')"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"", index_col=0) pd.set_option(""display.max_rows"", 5) print()",0,stream,"import pandas as pd
 
 reviews = pd.read_csv(""../input/wine-reviews/winemag-data-130k-v2.csv"", index_col=0)
 pd.set_option(""display.max_rows"", 5)
 
 from learntools.core import binder; binder.bind(globals())
 from learntools.pandas.indexing_selecting_and_assigning import *
 print(""Setup complete."")"
reviews.head(),0,execute_result,reviews.head()
"SETUP CHECKPOINT init_notebook_mode(connected=True) for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,display_data,"
 
 #Importing relevant libraries
 import numpy as np 
 import pandas as pd 
 import plotly as py
 import seaborn as sns
 import plotly.express as px
 import plotly.graph_objs as go
 from plotly.subplots import make_subplots
 from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
 init_notebook_mode(connected=True)
 
 ##Importing data into notebook
 import os
 for dirname, _, filenames in os.walk('/kaggle/input'):
     for filename in filenames:
         print(os.path.join(dirname, filename))
 
"
ASSIGN=pd.read_csv('path'),0,not_existent,"#Reading the data by pandas..Trying this you may have to change location according to local location
 
 corona_data=pd.read_csv('/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv')
"
corona_data.head(3),0,execute_result,"#Viewing first three rows of data for quick insight
 corona_data.head(3)
"
corona_data.tail(2),0,execute_result,"#Viewing last two rows of data
 corona_data.tail(2)
"
"ASSIGN=px.choropleth(corona_data, ASSIGN=""Countrypath"", ASSIGN = ""country names"", ASSIGN=""Confirmed"", ASSIGN=""Countrypath"", ASSIGN=""ObservationDate"" ) ASSIGN.update_layout( ASSIGN = 'Global Spread of Coronavirus', ASSIGN = 0.5, ASSIGN=dict( ASSIGN = False, ASSIGN = False, )) ASSIGN.show()",0,display_data,"choro_map=px.choropleth(corona_data, 
                     locations=""Country/Region"", 
                     locationmode = ""country names"",
                     color=""Confirmed"", 
                     hover_name=""Country/Region"", 
                     animation_frame=""ObservationDate""
                    )
 
 choro_map.update_layout(
     title_text = 'Global Spread of Coronavirus',
     title_x = 0.5,
     geo=dict(
         showframe = False,
         showcoastlines = False,
     ))
     
 choro_map.show()"
"ASSIGN = px.pie(corona_data, values = 'Confirmed',names='Countrypath', height=600) ASSIGN.update_traces(textposition='inside', textinfo='percent+label') ASSIGN.update_layout( ASSIGN = 0.5, ASSIGN=dict( ASSIGN = False, ASSIGN = False, )) ASSIGN.show()",0,display_data,"pie_chart = px.pie(corona_data, values = 'Confirmed',names='Country/Region', height=600)
 pie_chart.update_traces(textposition='inside', textinfo='percent+label')
 
 pie_chart.update_layout(
     title_x = 0.5,
     geo=dict(
         showframe = False,
         showcoastlines = False,
     ))
 
 pie_chart.show()"
"ASSIGN = corona_data.groupby(['Countrypath', 'ObservationDate']).sum().reset_index().sort_values('Confirmed', ascending=False) ASSIGN = ASSIGN.drop_duplicates(subset = ['Countrypath']) ASSIGN = ASSIGN.iloc[0:10]",1,not_existent,"#Manipulating the dataframe
 top10 = corona_data.groupby(['Country/Region', 'ObservationDate']).sum().reset_index().sort_values('Confirmed', ascending=False)
 top10  = top10.drop_duplicates(subset = ['Country/Region'])
 top10 = top10.iloc[0:10]
"
"ASSIGN = px.pie(top10, values = 'Confirmed',names='Countrypath', height=600) ASSIGN.update_traces(textposition='inside', textinfo='percent+label') ASSIGN.update_layout( ASSIGN = 0.5, ASSIGN=dict( ASSIGN = False, ASSIGN = False, )) ASSIGN.show()",0,display_data,"pie_chart_top10 = px.pie(top10, values = 'Confirmed',names='Country/Region', height=600)
 pie_chart_top10.update_traces(textposition='inside', textinfo='percent+label')
 
 pie_chart_top10.update_layout(
     title_x = 0.5,
     geo=dict(
         showframe = False,
         showcoastlines = False,
     ))
 
 pie_chart_top10.show()"
"CHECKPOINT ASSIGN = corona_data.groupby(['Countrypath', 'ObservationDate']).sum().reset_index().sort_values('Confirmed', ascending=False) ASSIGN = ASSIGN.drop_duplicates(subset = ['Countrypath']) ASSIGN = ASSIGN.iloc[-20:-1] last20",1,execute_result,"#Manipulating the dataframe
 last20 = corona_data.groupby(['Country/Region', 'ObservationDate']).sum().reset_index().sort_values('Confirmed', ascending=False)
 last20  = last20.drop_duplicates(subset = ['Country/Region'])
 last20 = last20.iloc[-20:-1]
 last20"
"ASSIGN = px.pie(last20, values = 'Confirmed',names='Countrypath', height=600) ASSIGN.update_traces(textposition='inside', textinfo='percent+label') ASSIGN.update_layout( ASSIGN = 0.5, ASSIGN=dict( ASSIGN = False, ASSIGN = False, )) ASSIGN.show()",0,display_data,"pie_chart_last20 = px.pie(last20, values = 'Confirmed',names='Country/Region', height=600)
 pie_chart_last20.update_traces(textposition='inside', textinfo='percent+label')
 
 pie_chart_last20.update_layout(
     title_x = 0.5,
     geo=dict(
         showframe = False,
         showcoastlines = False,
     ))
 
 pie_chart_last20.show()"
"ASSIGN = corona_data.groupby(['Countrypath', 'ObservationDate'])['Confirmed', 'Deaths', 'Recovered'].sum().reset_index().sort_values('ObservationDate', ascending=True)",1,not_existent,"bar_data = corona_data.groupby(['Country/Region', 'ObservationDate'])['Confirmed', 'Deaths', 'Recovered'].sum().reset_index().sort_values('ObservationDate', ascending=True)
"
"ASSIGN = px.bar(bar_data, x=""ObservationDate"", y=""Confirmed"", color='Countrypath', text = 'Confirmed', orientation='v', height=1300,width=1000, ASSIGN='Increase in COVID-19 Cases') ASSIGN.show()",0,display_data,"bar_fig = px.bar(bar_data, x=""ObservationDate"", y=""Confirmed"", color='Country/Region', text = 'Confirmed', orientation='v', height=1300,width=1000,
              title='Increase in COVID-19 Cases')
 bar_fig.show()"
"ASSIGN = px.bar(bar_data, x=""ObservationDate"", y=""Deaths"", color='Countrypath', text = 'Deaths', orientation='v', height=1000,width=900, ASSIGN='COVID-19 Deaths since February to April 11th') ASSIGN.show()",0,display_data,"bar_fig2 = px.bar(bar_data, x=""ObservationDate"", y=""Deaths"", color='Country/Region', text = 'Deaths', orientation='v', height=1000,width=900,
              title='COVID-19 Deaths since February to April 11th')
 bar_fig2.show()"
"ASSIGN = px.bar(bar_data, x=""ObservationDate"", y=""Recovered"", color='Countrypath', text = 'Recovered', orientation='v', height=1000,width=900, ASSIGN='COVID-19 Recovered Cases since February to April 11th') ASSIGN.show()",0,display_data,"bar_fig3 = px.bar(bar_data, x=""ObservationDate"", y=""Recovered"", color='Country/Region', text = 'Recovered', orientation='v', height=1000,width=900,
              title='COVID-19 Recovered Cases since February to April 11th')
 bar_fig3.show()"
"ASSIGN = corona_data.groupby('ObservationDate').sum().reset_index() ASSIGN = ASSIGN.melt(id_vars='ObservationDate', ASSIGN=['Confirmed', 'Recovered', 'Deaths'], ASSIGN='Ratio', ASSIGN='Value') ASSIGN = px.line(line_data, x=""ObservationDate"", y=""Value"", line_shape=""spline"",color='Ratio', ASSIGN='Confirmed cases, Recovered cases, and Death Over Time') ASSIGN.show()",0,display_data,"line_data = corona_data.groupby('ObservationDate').sum().reset_index()
 
 line_data = line_data.melt(id_vars='ObservationDate', 
                  value_vars=['Confirmed', 
                              'Recovered', 
                              'Deaths'], 
                  var_name='Ratio', 
                  value_name='Value')
 
 line_fig = px.line(line_data, x=""ObservationDate"", y=""Value"", line_shape=""spline"",color='Ratio', 
               title='Confirmed cases, Recovered cases, and Death Over Time')
 line_fig.show()"
SETUP CHECKPOINT print(os.listdir()),0,not_existent,"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."
ASSIGN = '..path' ASSIGN = '..path' ASSIGN = 'submission.csv' ASSIGN = pd.read_csv(train_file_path) ASSIGN = pd.read_csv(test_file_path),0,not_existent,train_file_path = '../input/train.csv'  test_file_path = '../input/test.csv'  submission_file_path = 'submission.csv'   train = pd.read_csv(train_file_path) test = pd.read_csv(test_file_path) 
CHECKPOINT train.fillna(0) test.fillna(0) train.dtypes,0,not_existent,train.fillna(0) test.fillna(0)  train.dtypes
"CHECKPOINT ASSIGN = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare'] ASSIGN = train['Survived'] ASSIGN = train[data_predictors] ASSIGN = test[data_predictors] print(ASSIGN.head())",1,not_existent,"data_predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare']  train_y = train['Survived'] train_X = train[data_predictors] test_X = test[data_predictors]  print(test_X.head())"
CHECKPOINT ASSIGN = pd.get_dummies(train_X) ASSIGN = pd.get_dummies(test_X) ASSIGN = ASSIGN.fillna(0) train_x,1,not_existent,train_x = pd.get_dummies(train_X) test_x = pd.get_dummies(test_X) train_x = train_x.fillna(0) train_x
"SETUP ASSIGN = GradientBoostingRegressor() ASSIGN.fit(train_x, train_y) ASSIGN = plot_partial_dependence(plot_model, ASSIGN=[1, 3], X=train_x, ASSIGN=['Pclass', 'Age', 'SibSp', 'Fare'], ASSIGN=10)",0,not_existent,"from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence from sklearn.ensemble import GradientBoostingRegressor  plot_model = GradientBoostingRegressor() plot_model.fit(train_x, train_y) dep_plots = plot_partial_dependence(plot_model,                                           features=[1, 3],                                     X=train_x,            # raw predictors data.                                    feature_names=['Pclass', 'Age', 'SibSp', 'Fare'], # labels on graphs                                    grid_resolution=10) # number of values to plot on x axis"
"SETUP CHECKPOINT ASSIGN = make_pipeline(Imputer(), XGBRegressor()) ASSIGN.fit(train_x, train_y) ASSIGN = cross_val_score(data_model, train_x, train_y, scoring='neg_mean_absolute_error') print(ASSIGN) print('Mean Absolute Error %2f' %(-1 * ASSIGN.mean()))",0,not_existent,"from xgboost import XGBRegressor from sklearn.preprocessing import Imputer from sklearn.pipeline import make_pipeline from sklearn.model_selection import cross_val_score  data_model = make_pipeline(Imputer(), XGBRegressor()) data_model.fit(train_x, train_y)  scores = cross_val_score(data_model, train_x, train_y, scoring='neg_mean_absolute_error') print(scores) print('Mean Absolute Error %2f' %(-1 * scores.mean()))"
CHECKPOINT ASSIGN = data_model.predict(test_x) print('Estimated survivors: ' + str(ASSIGN.astype(int).sum()) + ' passengers'),0,not_existent,prediction = data_model.predict(test_x) print('Estimated survivors: ' + str(prediction.astype(int).sum()) + ' passengers')
"ASSIGN = test.assign(Survived=prediction.astype(int)) ASSIGN.to_csv(submission_file_path,sep=',',columns=['PassengerId', 'Survived'], index=False)",1,not_existent,"result = test.assign(Survived=prediction.astype(int)) result.to_csv(submission_file_path,sep=',',columns=['PassengerId', 'Survived'], index=False)"
"SETUP filterwarnings('ignore') pd.set_option('display.max_columns', None)",0,stream,"!pip install ycimpute
 
 from sklearn.preprocessing import StandardScaler
 from sklearn.decomposition import PCA
 from sklearn.preprocessing import LabelEncoder
 from sklearn import datasets, metrics, model_selection, svm
 import missingno as msno
 from ycimpute.imputer import iterforest,EM
 from fancyimpute import KNN
 from sklearn.preprocessing import OrdinalEncoder
 
 import numpy as np
 import pandas as pd 
 import statsmodels.api as sm
 import statsmodels.formula.api as smf
 import seaborn as sns
 from sklearn.preprocessing import scale 
 from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
 from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
 from sklearn.metrics import roc_auc_score,roc_curve
 import statsmodels.formula.api as smf
 import matplotlib.pyplot as plt
 from sklearn.neighbors import KNeighborsClassifier
 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
 from sklearn.linear_model import LogisticRegression
 from sklearn.svm import SVC
 from sklearn.naive_bayes import GaussianNB
 from sklearn.tree import DecisionTreeClassifier
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.ensemble import GradientBoostingClassifier
 from xgboost import XGBClassifier
 from lightgbm import LGBMClassifier
 from catboost import CatBoostClassifier
 
 from warnings import filterwarnings
 filterwarnings('ignore')
 
 pd.set_option('display.max_columns', None)
 import gc"
"SETUP CHECKPOINT def reduce_mem_usage2(df): """""" iterate through all the columns of a dataframe and modify the data type to reduce memory usage. """""" ASSIGN = df.memory_usage().sum() path**2 print('Memory usage of dataframe is {:.2f} MB'.format(ASSIGN)) for col in df.columns: ASSIGN = df[col].dtype if ASSIGN != object: ASSIGN = df[col].min() ASSIGN = df[col].max() if str(ASSIGN)[:3] == 'int': if ASSIGN > np.iinfo(np.int8).min and ASSIGN < np.iinfo(np.int8).max: ASSIGN = ASSIGN.astype(np.int8) elif ASSIGN > np.iinfo(np.int16).min and ASSIGN < np.iinfo(np.int16).max: ASSIGN = ASSIGN.astype(np.int16) elif ASSIGN > np.iinfo(np.int32).min and ASSIGN < np.iinfo(np.int32).max: ASSIGN = ASSIGN.astype(np.int32) elif ASSIGN > np.iinfo(np.int64).min and ASSIGN < np.iinfo(np.int64).max: ASSIGN = ASSIGN.astype(np.int64) else: if ASSIGN > np.finfo(np.float16).min and ASSIGN < np.finfo(np.float16).max: ASSIGN = ASSIGN.astype(np.float16) elif ASSIGN > np.finfo(np.float32).min and ASSIGN < np.finfo(np.float32).max: ASSIGN = ASSIGN.astype(np.float32) else: ASSIGN = ASSIGN.astype(np.float64) else: ASSIGN = ASSIGN.astype('category') ASSIGN = df.memory_usage().sum() path**2 print('Memory usage after optimization is: {:.2f} MB'.format(ASSIGN)) print('Decreased by {:.1f}%'.format(100 * (ASSIGN - ASSIGN) path)) return df",1,stream,"%%time
 # From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage
 # WARNING! THIS CAN DAMAGE THE DATA 
 def reduce_mem_usage2(df):
     """""" iterate through all the columns of a dataframe and modify the data type
         to reduce memory usage.        
     """"""
     start_mem = df.memory_usage().sum() / 1024**2
     print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))
     
     for col in df.columns:
         col_type = df[col].dtype
         
         if col_type != object:
             c_min = df[col].min()
             c_max = df[col].max()
             if str(col_type)[:3] == 'int':
                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                     df[col] = df[col].astype(np.int8)
                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                     df[col] = df[col].astype(np.int16)
                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                     df[col] = df[col].astype(np.int32)
                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                     df[col] = df[col].astype(np.int64)  
             else:
                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                     df[col] = df[col].astype(np.float16)
                 elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                     df[col] = df[col].astype(np.float32)
                 else:
                     df[col] = df[col].astype(np.float64)
         else:
             df[col] = df[col].astype('category')
 
     end_mem = df.memory_usage().sum() / 1024**2
     print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))
     print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))
     
     return df"
"ASSIGN=OrdinalEncoder() ASSIGN=KNN() def encode(data): '''function to encode non-null data and replace it in the original data''' ASSIGN = np.array(data.dropna()) ASSIGN = nonulls.reshape(-1,1) ASSIGN = encoder.fit_transform(impute_reshape) data.loc[data.notnull()] = np.squeeze(ASSIGN) return data",1,not_existent,"encoder=OrdinalEncoder()
 imputer=KNN()
 
 def encode(data):
     '''function to encode non-null data and replace it in the original data'''
     #retains only non-null values
     nonulls = np.array(data.dropna())
     #reshapes the data for encoding
     impute_reshape = nonulls.reshape(-1,1)
     #encode date
     impute_ordinal = encoder.fit_transform(impute_reshape)
     #Assign back encoded values to non-null values
     data.loc[data.notnull()] = np.squeeze(impute_ordinal)
     return data"
"ASSIGN = pd.read_csv('path') ASSIGN = pd.read_csv(""path"") ASSIGN = pd.read_csv(""path"") ASSIGN = pd.read_csv(""path"")",0,not_existent,"Ktest_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv')
 Ktest_transaction = pd.read_csv(""/kaggle/input/ieee-fraud-detection/test_transaction.csv"")
 Ktrain_identity = pd.read_csv(""/kaggle/input/ieee-fraud-detection/train_identity.csv"")
 Ktrain_transaction = pd.read_csv(""/kaggle/input/ieee-fraud-detection/train_transaction.csv"")"
"ASSIGN= pd.merge(Ktrain_transaction, Ktrain_identity, on='TransactionID', how='left', left_index=True, right_index=True) ASSIGN= pd.merge(Ktest_transaction, Ktest_identity, on='TransactionID', how='left', left_index=True, right_index=True)",1,not_existent,"Ktrain= pd.merge(Ktrain_transaction, Ktrain_identity, on='TransactionID', how='left', left_index=True, right_index=True)
 Ktest= pd.merge(Ktest_transaction, Ktest_identity, on='TransactionID', how='left', left_index=True, right_index=True)
"
ASSIGN=Ktrain.select_dtypes(include='object') ASSIGN =Ktest.select_dtypes(include='object'),1,not_existent,"Ktrain_cat=Ktrain.select_dtypes(include='object')
 Ktest_cat =Ktest.select_dtypes(include='object')"
"Ktrain_cat1=Ktrain_cat.drop(['P_emaildomain','R_emaildomain','id_30','id_31','id_33','DeviceInfo'], axis=1) Ktest_cat1=Ktest_cat.drop(['P_emaildomain','R_emaildomain','id-30','id-31','id-33','DeviceInfo'], axis=1)",1,not_existent,"Ktrain_cat1=Ktrain_cat.drop(['P_emaildomain','R_emaildomain','id_30','id_31','id_33','DeviceInfo'], axis=1)
 Ktest_cat1=Ktest_cat.drop(['P_emaildomain','R_emaildomain','id-30','id-31','id-33','DeviceInfo'], axis=1)"
for i in Ktrain_cat1: encode(Ktrain[i]) for i in Ktest_cat1: encode(Ktest[i]),1,not_existent,"for i in Ktrain_cat1:
     encode(Ktrain[i])
 for i in Ktest_cat1:
     encode(Ktest[i])"
"Ktrain_cat2=pd.concat([Ktrain['P_emaildomain'],Ktrain['R_emaildomain'],Ktrain['id_30'],Ktrain['id_31'],Ktrain['id_33'],Ktrain['DeviceInfo']], axis=1) Ktest_cat2=pd.concat([Ktest['P_emaildomain'],Ktest['R_emaildomain'],Ktest['id-30'],Ktest['id-31'],Ktest['id-33'],Ktest['DeviceInfo']], axis=1)",1,not_existent,"Ktrain_cat2=pd.concat([Ktrain['P_emaildomain'],Ktrain['R_emaildomain'],Ktrain['id_30'],Ktrain['id_31'],Ktrain['id_33'],Ktrain['DeviceInfo']], axis=1)
 Ktest_cat2=pd.concat([Ktest['P_emaildomain'],Ktest['R_emaildomain'],Ktest['id-30'],Ktest['id-31'],Ktest['id-33'],Ktest['DeviceInfo']], axis=1)
"
for i in Ktrain_cat2: encode(Ktrain[i]) for i in Ktest_cat2: encode(Ktest[i]),1,not_existent,"for i in Ktrain_cat2:
     encode(Ktrain[i])
 for i in Ktest_cat2:
     encode(Ktest[i])"
SETUP del Ktest_identity del Ktest_transaction del Ktrain_identity del Ktrain_transaction del Ktrain_cat1 del Ktest_cat1 del Ktrain_cat2 del Ktest_cat2 gc.collect(),0,execute_result,"import gc
 del Ktest_identity
 del Ktest_transaction
 del Ktrain_identity
 del Ktrain_transaction
 del Ktrain_cat1
 del Ktest_cat1
 del Ktrain_cat2
 del Ktest_cat2
 gc.collect()"
ASSIGN = reduce_mem_usage2(ASSIGN) ASSIGN = reduce_mem_usage2(ASSIGN),1,stream,"Ktest = reduce_mem_usage2(Ktest)
 Ktrain = reduce_mem_usage2(Ktrain)"
CHECKPOINT Ktrain.shape,0,execute_result,Ktrain.shape
CHECKPOINT Ktest.shape,0,execute_result,Ktest.shape
Ktest.head(),0,execute_result,Ktest.head()
"ASSIGN= Ktest.loc[:,'id-01':'id-38'].columns.str.replace('-','_')",1,not_existent,"z= Ktest.loc[:,'id-01':'id-38'].columns.str.replace('-','_')"
CHECKPOINT ASSIGN=list(ASSIGN) z,1,execute_result,"z=list(z)
 z"
"for x,y in zip(Ktest.loc[:,'id-01':'id-38'].columns, z): SLICE=SLICE del Ktest[x] gc.collect()",1,execute_result,"for x,y in zip(Ktest.loc[:,'id-01':'id-38'].columns, z):
     Ktest[y]=Ktest[x]
     del Ktest[x]
 gc.collect()"
"ASSIGN=Ktrain[""isFraud""] X=Ktrain.drop([""isFraud"", ""TransactionID""], axis=1).astype('float64') X= X.fillna(-999) ASSIGN = Ktest['TransactionID'] ASSIGN = Ktest.drop(['TransactionID'], axis=1).astype('float64') ASSIGN = ASSIGN.fillna(-999) ASSIGN = ASSIGN[X.columns]",1,not_existent,"y=Ktrain[""isFraud""]
 X=Ktrain.drop([""isFraud"", ""TransactionID""], axis=1).astype('float64')
 X= X.fillna(-999)
 
 Ktest_id = Ktest['TransactionID']
 X_Ktest = Ktest.drop(['TransactionID'], axis=1).astype('float64')
 X_Ktest = X_Ktest.fillna(-999)
 
 X_Ktest = X_Ktest[X.columns]"
"''' ASSIGN=StandardScaler().fit_transform(X) ASSIGN=PCA().fit(X_fit) plt.plot(np.cumsum(ASSIGN.explained_variance_ratio_)) plt.title('All columns included', color='gray') plt.xlabel(""Number of Component"", color='green') plt.ylabel(""Cumulative Variance Ratio"", color='green') plt.grid(color='gray', linestyle='-', linewidth=0.3) plt.show() ASSIGN=StandardScaler().fit_transform(X_Ktest) ASSIGN=PCA().fit(X_Ktest_fit) plt.plot(np.cumsum(ASSIGN.explained_variance_ratio_)) plt.title('All columns included', color='gray') plt.xlabel(""Number of Component"", color='green') plt.ylabel(""Cumulative Variance Ratio"", color='green') plt.grid(color='gray', linestyle='-', linewidth=0.3) plt.show() '''",1,execute_result,"'''
 X_fit=StandardScaler().fit_transform(X)
 X_pca=PCA().fit(X_fit)
 plt.plot(np.cumsum(X_pca.explained_variance_ratio_))
 plt.title('All columns included', color='gray')
 plt.xlabel(""Number of Component"", color='green')
 plt.ylabel(""Cumulative Variance Ratio"", color='green')
 plt.grid(color='gray', linestyle='-', linewidth=0.3)
 plt.show()
 
 X_Ktest_fit=StandardScaler().fit_transform(X_Ktest)
 X_Ktest_pca=PCA().fit(X_Ktest_fit)
 plt.plot(np.cumsum(X_Ktest_pca.explained_variance_ratio_))
 plt.title('All columns included', color='gray')
 plt.xlabel(""Number of Component"", color='green')
 plt.ylabel(""Cumulative Variance Ratio"", color='green')
 plt.grid(color='gray', linestyle='-', linewidth=0.3)
 plt.show()
 '''"
"CHECKPOINT """""" ASSIGN = PCA(n_components=100).fit(X_fit) ASSIGN = X_pca.fit_transform(ASSIGN) ASSIGN.explained_variance_ratio_ ASSIGN = PCA(n_components=100).fit(X_Ktest_fit) ASSIGN = X_Ktest_pca.fit_transform(ASSIGN) print(ASSIGN.explained_variance_ratio_.sum()) print(ASSIGN.explained_variance_ratio_.sum()) """"""",1,execute_result,"# PCA Analysis is misleading here because we filled missing data. 
 
 """"""
 #Final Model for Ktrain
 X_pca = PCA(n_components=100).fit(X_fit)
 X_fit = X_pca.fit_transform(X_fit)
 X_pca.explained_variance_ratio_
 
 #Final Model for Ktest
 
 X_Ktest_pca = PCA(n_components=100).fit(X_Ktest_fit)
 X_Ktest_fit = X_Ktest_pca.fit_transform(X_Ktest_fit)
 print(X_pca.explained_variance_ratio_.sum())
 print(X_Ktest_pca.explained_variance_ratio_.sum())
 """""""
"''' ASSIGN=[] for i in range(1,101): ASSIGN.append(""a""+str(i)) ASSIGN= pd.DataFrame(data=X_fit, columns= xlist) ASSIGN= pd.DataFrame(data=X_Ktest_fit, columns= xlist) ASSIGN.head() '''",1,execute_result,"'''
 xlist=[]
 for i in range(1,101):
     xlist.append(""a""+str(i))
 
 X_new= pd.DataFrame(data=X_fit, columns= xlist)
 X_Ktest_new= pd.DataFrame(data=X_Ktest_fit, columns= xlist)
 X_Ktest_new.head()
 '''"
"X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.25, random_state=42)",1,not_existent,"X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.25, random_state=42)"
ASSIGN = reduce_mem_usage2(ASSIGN) ASSIGN = reduce_mem_usage2(ASSIGN),1,stream,"X = reduce_mem_usage2(X)
 X_Ktest = reduce_mem_usage2(X_Ktest)"
CHECKPOINT print(Ktrain.shape) print(Ktest.shape) print(X.shape) print(X_Ktest.shape),0,stream,"print(Ktrain.shape)
 print(Ktest.shape)
 print(X.shape)
 print(X_Ktest.shape)
"
"CHECKPOINT '''models = [LogisticRegression, KNeighborsClassifier, GaussianNB, SVC, DecisionTreeClassifier, RandomForestClassifier, GradientBoostingClassifier, LGBMClassifier, XGBClassifier ] def compML (df, y, algorithm): ASSIGN=algorithm().fit(X_train, y_train) ASSIGN=model.predict(X_test) ASSIGN= accuracy_score(y_test, y_pred) ASSIGN= algorithm.__name__ print(ASSIGN,,ASSIGN) for i in models: compML(X,""isFraud"",i) '''",0,execute_result,"# It gets so much time to run with each models, so passed for now.
 
 '''models = [LogisticRegression,
           KNeighborsClassifier,
           GaussianNB,
           SVC,
           DecisionTreeClassifier,
           RandomForestClassifier,
           GradientBoostingClassifier,
           LGBMClassifier,
           XGBClassifier
           #CatBoostClassifier
          ]
 
 def compML (df, y, algorithm):
     
     #y=df[y]
     #X=df.drop([""PassengerId"",""Survived""], axis=1).astype('float64')
     #X_train, X_test,y_train,y_test=train_test_split(X,y, test_size=0.25, random_state=42)
     
     model=algorithm().fit(X_train, y_train)
     y_pred=model.predict(X_test)
     accuracy= accuracy_score(y_test, y_pred)
     #return accuracy
     model_name= algorithm.__name__
     print(model_name,"": "",accuracy)
     
     
 for i in models:
     compML(X,""isFraud"",i)
     
 '''"
"ASSIGN= LGBMClassifier().fit(X_train, y_train) ASSIGN=model.predict(X_test) accuracy_score(y_test,ASSIGN)",0,execute_result,"model= LGBMClassifier().fit(X_train, y_train)
 y_pred=model.predict(X_test)
 accuracy_score(y_test,y_pred)"
CHECKPOINT model,0,execute_result,model
"ASSIGN=model.predict(X_Ktest) ASSIGN=pd.DataFrame({""TransactionID"":Ktest_id, ""isFraud"":predictions}) ASSIGN.to_csv('submission_model.csv', index=False)",0,not_existent,"predictions=model.predict(X_Ktest)
 output=pd.DataFrame({""TransactionID"":Ktest_id, ""isFraud"":predictions})
 output.to_csv('submission_model.csv', index=False)"
SETUP,0,not_existent,import pandas as pd import numpy as np from sklearn.model_selection import cross_val_score
"ASSIGN = pd.read_csv(""..path"", index_col = 'Id')",0,not_existent,"train = pd.read_csv(""../input/train.csv"", index_col = 'Id')"
SETUP,0,not_existent,from sklearn.neighbors import KNeighborsRegressor
ASSIGN = KNeighborsRegressor(38),0,not_existent,knn = KNeighborsRegressor(38)
"ASSIGN = cross_val_score(knn, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')",1,not_existent,"knn_score = cross_val_score(knn, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')"
knn_score.mean(),0,not_existent,knn_score.mean()
SETUP,0,not_existent,from sklearn.linear_model import LassoCV
ASSIGN = LassoCV(cv = 10),1,not_existent,lasso = LassoCV(cv = 10)
"ASSIGN = cross_val_score(lasso, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')",1,not_existent,"lasso_score = cross_val_score(lasso, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')"
lasso_score.mean(),0,not_existent,lasso_score.mean()
SETUP,0,not_existent,from sklearn.linear_model import RidgeCV
ASSIGN = RidgeCV(cv = 10),1,not_existent,ridge = RidgeCV(cv = 10)
"ASSIGN = cross_val_score(ridge, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')",1,not_existent,"ridge_score = cross_val_score(ridge, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')"
ridge_score.mean(),0,not_existent,ridge_score.mean()
"SETUP CHECKPOINT ASSIGN = '..path' ASSIGN = pd.read_csv(iowa_file_path) ASSIGN = home_data.SalePrice ASSIGN = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd'] ASSIGN = home_data[features] train_X, val_X, train_y, val_y = train_test_split(ASSIGN, ASSIGN, random_state=1) ASSIGN = DecisionTreeRegressor(random_state=1) ASSIGN.fit(train_X, train_y) ASSIGN = iowa_model.predict(val_X) ASSIGN = mean_absolute_error(val_predictions, val_y) print(.format(ASSIGN)) binder.bind(globals()) print()",0,stream,"# Code you have previously used to load data
 import pandas as pd
 from sklearn.metrics import mean_absolute_error
 from sklearn.model_selection import train_test_split
 from sklearn.tree import DecisionTreeRegressor
 
 
 # Path of the file to read
 iowa_file_path = '../input/home-data-for-ml-course/train.csv'
 
 home_data = pd.read_csv(iowa_file_path)
 # Create target object and call it y
 y = home_data.SalePrice
 # Create X
 features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']
 X = home_data[features]
 
 # Split into validation and training data
 train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)
 
 # Specify Model
 iowa_model = DecisionTreeRegressor(random_state=1)
 # Fit Model
 iowa_model.fit(train_X, train_y)
 
 # Make validation predictions and calculate mean absolute error
 val_predictions = iowa_model.predict(val_X)
 val_mae = mean_absolute_error(val_predictions, val_y)
 print(""Validation MAE: {:,.0f}"".format(val_mae))
 
 # Set up code checking
 from learntools.core import binder
 binder.bind(globals())
 from learntools.machine_learning.ex5 import *
 print(""\nSetup complete"")"
"def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y): ASSIGN = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0) ASSIGN.fit(train_X, train_y) ASSIGN = model.predict(val_X) ASSIGN = mean_absolute_error(val_y, preds_val) return(ASSIGN)",0,not_existent,"def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):
     model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)
     model.fit(train_X, train_y)
     preds_val = model.predict(val_X)
     mae = mean_absolute_error(val_y, preds_val)
     return(mae)"
SETUP,0,not_existent,import numpy as np import pandas as pd import matplotlib.pyplot as plt
"ASSIGN = pd.read_csv(""..path"", index_col = 'Id') ASSIGN.head()",0,not_existent,"train = pd.read_csv(""../input/featdataset/train_data.csv"", index_col = 'Id') train.head()"
"plt.figure(figsize=(15,10)) (train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[:54]].plot(kind = 'bar')",0,not_existent,"plt.figure(figsize=(15,10)) (train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[:54]].plot(kind = 'bar')"
(train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[54:57]].plot(kind = 'bar'),0,not_existent,(train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[54:57]].plot(kind = 'bar')
SETUP,0,not_existent,import sklearn from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import cross_val_score
ASSIGN = train[train.columns[0:57]] ASSIGN.head(),1,not_existent,Xtrain = train[train.columns[0:57]] Xtrain.head()
ASSIGN = train['ham'] ASSIGN.head(),1,not_existent,Ytrain = train['ham'] Ytrain.head()
ASSIGN = GaussianNB(),0,not_existent,NB = GaussianNB() 
"CHECKPOINT ASSIGN = cross_val_score(NB, Xtrain,Ytrain, cv=10) scores",0,not_existent,"scores = cross_val_score(NB, Xtrain,Ytrain, cv=10) scores"
"ASSIGN = pd.read_csv('..path', index_col = 'Id') ASSIGN.head()",0,not_existent,"test = pd.read_csv('../input/featdataset/test_features.csv', index_col = 'Id') test.head()"
"NB.fit(Xtrain,Ytrain) ASSIGN = NB.predict(test)",0,not_existent,"NB.fit(Xtrain,Ytrain) Ytest = NB.predict(test)"
"ASSIGN = pd.DataFrame(index = test.index) ASSIGN = Ytest ASSIGN.to_csv('submission.csv',index = True)",1,not_existent,"pred = pd.DataFrame(index = test.index) pred['ham'] = Ytest pred.to_csv('submission.csv',index = True)"
"SETUP CHECKPOINT print(check_output([, ]).decode())",0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed
 # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
 # For example, here's several helpful packages to load in 
 
 import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
 
 # Input data files are available in the ""../input/"" directory.
 # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
 
 from subprocess import check_output
 print(check_output([""ls"", ""../input""]).decode(""utf8""))
 
 # Any results you write to the current directory are saved as output."
SETUP,0,not_existent,import pandas as pd
"ASSIGN = pd.read_csv(""..path"") ASSIGN.head()",0,not_existent,"df = pd.read_csv(""../input/titanic/titanic.csv"") df.head()"
"df.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)",1,not_existent,"df.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)"
df.head(),0,not_existent,df.head()
"ASSIGN = df.drop('Survived',axis='columns') ASSIGN = df.Survived",1,not_existent,"inputs = df.drop('Survived',axis='columns') target = df.Survived"
"inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})",1,not_existent,"inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})"
inputs.Age[:10],0,not_existent,inputs.Age[:10]
inputs.Age = inputs.Age.fillna(inputs.Age.mean()),1,not_existent,inputs.Age = inputs.Age.fillna(inputs.Age.mean()) 
SETUP ASSIGN = LabelEncoder() ASSIGN = LabelEncoder() ASSIGN = LabelEncoder() ASSIGN=LabelEncoder(),0,not_existent,from sklearn.preprocessing import LabelEncoder le_Pclass = LabelEncoder() le_Sex = LabelEncoder() le_Age = LabelEncoder() le_Fare=LabelEncoder()
inputs['Pclass_n'] = le_Pclass.fit_transform(inputs['Pclass']) inputs['Sex_n'] = le_Sex.fit_transform(inputs['Sex']) inputs['Age_n'] = le_Age.fit_transform(inputs['Age']) inputs['Fare_n'] = le_Fare.fit_transform(inputs['Fare']),1,not_existent,inputs['Pclass_n'] = le_Pclass.fit_transform(inputs['Pclass']) inputs['Sex_n'] = le_Sex.fit_transform(inputs['Sex']) inputs['Age_n'] = le_Age.fit_transform(inputs['Age']) inputs['Fare_n'] = le_Fare.fit_transform(inputs['Fare'])
CHECKPOINT inputs,0,not_existent,inputs
"ASSIGN = inputs.drop(['Age','Sex','Fare','Pclass'],axis='columns')",1,not_existent,"inputs_n = inputs.drop(['Age','Sex','Fare','Pclass'],axis='columns')"
CHECKPOINT inputs_n,0,not_existent,inputs_n
SETUP ASSIGN = tree.DecisionTreeClassifier(),0,not_existent,from sklearn import tree model = tree.DecisionTreeClassifier()
"model.fit(inputs_n,target)",0,not_existent,"model.fit(inputs_n,target)"
"model.score(inputs_n,target)",0,not_existent,"model.score(inputs_n,target)"
"model.predict([[0,30,2,80]])",0,not_existent,"model.predict([[0,30,2,80]])"
SETUP,0,not_existent,"from mpl_toolkits.mplot3d import Axes3D from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt # plotting import numpy as np # linear algebra import os # accessing directory structure import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) "
CHECKPOINT print(os.listdir('..path')),0,not_existent,print(os.listdir('../input'))
"def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow): ASSIGN = df.ASSIGN() ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]] nRow, nCol = ASSIGN.shape ASSIGN = list(df) ASSIGN = (nCol + nGraphPerRow - 1) path plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k') for i in range(min(nCol, nGraphShown)): plt.subplot(ASSIGN, nGraphPerRow, i + 1) ASSIGN = df.iloc[:, i] if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)): ASSIGN = columnDf.value_counts() ASSIGN.plot.bar() else: ASSIGN.hist() plt.ylabel('counts') plt.xticks(rotation = 90) plt.title(f'{ASSIGN[i]} (column {i})') plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0) plt.show()",0,not_existent,"# Distribution graphs (histogram/bar graph) of column data def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):     nunique = df.nunique()     df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values     nRow, nCol = df.shape     columnNames = list(df)     nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow     plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')     for i in range(min(nCol, nGraphShown)):         plt.subplot(nGraphRow, nGraphPerRow, i + 1)         columnDf = df.iloc[:, i]         if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):             valueCounts = columnDf.value_counts()             valueCounts.plot.bar()         else:             columnDf.hist()         plt.ylabel('counts')         plt.xticks(rotation = 90)         plt.title(f'{columnNames[i]} (column {i})')     plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)     plt.show() "
"CHECKPOINT def plotCorrelationMatrix(df, graphWidth): ASSIGN = df.dataframeName ASSIGN = ASSIGN.dropna('columns') ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]] if ASSIGN.shape[1] < 2: print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2') return ASSIGN = df.ASSIGN() plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k') ASSIGN = plt.matshow(corr, fignum = 1) plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90) plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns) plt.gca().xaxis.tick_bottom() plt.colorbar(ASSIGN) plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15) plt.show()",0,not_existent,"# Correlation matrix def plotCorrelationMatrix(df, graphWidth):     filename = df.dataframeName     df = df.dropna('columns') # drop columns with NaN     df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values     if df.shape[1] < 2:         print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')         return     corr = df.corr()     plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')     corrMat = plt.matshow(corr, fignum = 1)     plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)     plt.yticks(range(len(corr.columns)), corr.columns)     plt.gca().xaxis.tick_bottom()     plt.colorbar(corrMat)     plt.title(f'Correlation Matrix for {filename}', fontsize=15)     plt.show() "
"def plotScatterMatrix(df, plotSize, textSize): ASSIGN = ASSIGN.select_dtypes(include =[np.number]) ASSIGN = ASSIGN.dropna('columns') ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]] ASSIGN = list(df) if len(ASSIGN) > 10: ASSIGN = ASSIGN[:10] ASSIGN = ASSIGN[columnNames] ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde') ASSIGN = df.corr().values for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)): ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize) plt.suptitle('Scatter and Density Plot') plt.show()",0,not_existent,"# Scatter and density plots def plotScatterMatrix(df, plotSize, textSize):     df = df.select_dtypes(include =[np.number]) # keep only numerical columns     # Remove rows and columns that would lead to df being singular     df = df.dropna('columns')     df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values     columnNames = list(df)     if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots         columnNames = columnNames[:10]     df = df[columnNames]     ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')     corrs = df.corr().values     for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):         ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)     plt.suptitle('Scatter and Density Plot')     plt.show() "
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'bank.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,not_existent,"nRowsRead = 1000 # specify 'None' if want to read whole file # bank.csv has 11162 rows in reality, but we are only loading/previewing the first 1000 rows df1 = pd.read_csv('../input/bank.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'bank.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')"
"plotScatterMatrix(df1, 20, 10)",0,not_existent,"plotScatterMatrix(df1, 20, 10)"
"SETUP ASSIGN=pd.read_csv(""path"") ASSIGN.head()",0,execute_result,"import pandas as pd 
 trd=pd.read_csv(""/kaggle/input/house-prices-advanced-regression-techniques/train.csv"")
 trd.head()"
"SETUP ASSIGN=pd.read_csv(""path"") ASSIGN.head()",0,execute_result,"import pandas as pd 
 tsd=pd.read_csv(""/kaggle/input/house-prices-advanced-regression-techniques/test.csv"")
 tsd.head()"
"CHECKPOINT ASSIGN=trd.loc[trd.YrSold<2008][trd.SaleCondition=='Normal'] ASSIGN= len(w)path(trd) print(,ASSIGN)",1,stream,"w=trd.loc[trd.YrSold<2008][trd.SaleCondition=='Normal']
 #print(w)
 rw= len(w)/len(trd)
 print(""percentage of houses are for sale before 2008 are normal is : "",rw)"
"CHECKPOINT ASSIGN=trd.loc[trd.LotArea<10000][trd.SaleCondition=='Normal'] ASSIGN= len(x)path(trd) print(,ASSIGN)",1,stream,"x=trd.loc[trd.LotArea<10000][trd.SaleCondition=='Normal']
 #print(w)
 rw= len(x)/len(trd)
 print(""percentage of houses are having area less than 10000 m^2 are normal is : "",rw)"
CHECKPOINT ASSIGN=trd.loc[trd.MSSubClass>60] print(ASSIGN),1,stream,"value=trd.loc[trd.MSSubClass>60]
 print(value)"
"SETUP """"""author    s_agnik1511"""""" ASSIGN = pd.read_csv('path') ASSIGN = trd.SalePrice ASSIGN = ['LotArea'] ASSIGN = trd[predictor_cols] ASSIGN = RandomForestRegressor() ASSIGN.fit(ASSIGN, ASSIGN)",0,execute_result,"""""""author    s_agnik1511""""""
 import numpy as np
 import pandas as pd
 from sklearn.ensemble import RandomForestRegressor
 trd = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')
 train_y = trd.SalePrice
 predictor_cols = ['LotArea']
 train_X = trd[predictor_cols]
 my_model = RandomForestRegressor()
 my_model.fit(train_X, train_y)"
trd.head(),0,execute_result,trd.head()
"SETUP """"""author s_agnik1511"""""" ASSIGN = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices}) ASSIGN.to_csv('submission_sagnik.csv', index=False)",0,error,"""""""author s_agnik1511""""""
 import pandas as pd
 my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})
 my_submission.to_csv('submission_sagnik.csv', index=False)"
"CHECKPOINT SETUP ASSIGN=pd.read_csv(""path"") k.shape",0,execute_result,"import pandas as pd
 k=pd.read_csv(""/kaggle/input/house-prices-advanced-regression-techniques/test.csv"")
 k.shape"
"SETUP ASSIGN=tsd.predict(""path"") SLICE=ASSIGN Submission.to_csv(""submission_sagnik123"",index=False)",0,error,"import pandas as pd
 import numpy as np
 m=tsd.predict(""/kaggle/input/house-prices-advanced-regression-techniques/test.csv"")
 Submission[""taregt""]=m
 Submission.to_csv(""submission_sagnik123"",index=False)
"
"CHECKPOINT """"""author s_agnik1511"""""" ASSIGN=pd.read_csv('path') ASSIGN=test[predictor_cols] ASSIGN=my_model.predict(test_X) print(ASSIGN)",0,stream,"""""""author s_agnik1511""""""
 test=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')
 test_X=test[predictor_cols]
 predicted_prices=my_model.predict(test_X)
 print(predicted_prices)"
"SETUP ASSIGN=LinearRegression() ASSIGN=pd.read_csv(""path"") ASSIGN.fit(ASSIGN) ASSIGN.predict(ASSIGN)",0,error,"from sklearn.linear_model import LinearRegression
 import pandas as pd
 Model=LinearRegression()
 x_test=pd.read_csv(""/kaggle/input/house-prices-advanced-regression-techniques/train.csv"")
 Model.fit(x_test)
 Model.predict(x_test)"
SETUP,0,stream,"
 import pandas as pd # for reading the data frame
 import numpy as np # for numerical calculation
 import matplotlib.pyplot as plt # use for visualization
 import seaborn as sns   # mostly used for statistical visualization 
 %matplotlib inline      # used for inline ploting
"
"CHECKPOINT ASSIGN = pd.read_csv(""path"") ASSIGN = pd.read_csv(""path"") print(, train.shape) #  rows : 1459 columns : 81 print(, test.shape)  # rows : 1459 columns : 80",0,stream,"train = pd.read_csv(""/kaggle/input/house-prices-advanced-regression-techniques/train.csv"")
 test = pd.read_csv(""/kaggle/input/house-prices-advanced-regression-techniques/test.csv"")
 
 print(""Shape of train: "", train.shape) #  rows : 1459 columns : 81
 print(""Shape of test: "", test.shape)  # rows : 1459 columns : 80"
train.head(20),0,execute_result,train.head(20)
test.head(10),0,execute_result,test.head(10)
"CHECKPOINT ASSIGN = pd.concat((train, test)) ASSIGN = df print(, ASSIGN.shape)",1,stream,"df = pd.concat((train, test)) # here we concat the test and train data set
 temp_df = df
 print(""Shape of df: "", df.shape)"
temp_df.head(),0,execute_result,temp_df.head() # by default its selected 5 rows and all the columns
temp_df.tail(),0,execute_result,temp_df.tail() # for vewig the last five rows
"pd.set_option(""display.max_columns"",2000)  # used for viewing all the columns at onces pd.set_option(""display.max_rows"",85)",0,not_existent,"# To show the all columns
 pd.set_option(""display.max_columns"",2000)  # used for viewing all the columns at onces
 pd.set_option(""display.max_rows"",85)"
df.head(),0,execute_result,df.head() 
CHECKPOINT df.shape,0,execute_result,df.shape
df.info(),0,stream,df.info()   # lets view the information about our data set like find the data types of our columns
df.describe(),0,execute_result,"df.describe() # used for finding the describtion about the data set like,  mean , standard deviation given below"
"df.select_dtypes(include=['int64', 'float64']).columns",1,execute_result,"df.select_dtypes(include=['int64', 'float64']).columns  # extracrt the columns whose dtype is intege and float"
df.select_dtypes(include=['object']).columns,1,execute_result,df.select_dtypes(include=['object']).columns  # find the columns whose dtype is object
"ASSIGN = ASSIGN.set_index(""Id"")",1,not_existent,"# Set index as Id column
 df = df.set_index(""Id"")"
"plt.figure(figsize=(16,9)) sns.heatmap(df.isnull())",0,execute_result,"# Show the null values using heatmap
 plt.figure(figsize=(16,9))
 sns.heatmap(df.isnull())       
 
 # useing heat map we can see the missing values ... the white stripes indicates the missing values 
"
df.isnull().sum(),0,execute_result,df.isnull().sum()   # from this we can see which columns has how many missig values  like LotFrontsge has 486 missing vales
CHECKPOINT ASSIGN = df.isnull().sum()path[0]*100 null_percent,1,execute_result,"# Get the percentages of null value
 null_percent = df.isnull().sum()/df.shape[0]*100
 null_percent
 
 
 # from this we can say LotFrontage has 16 % and Alley has 93 % missing vslues"
ASSIGN = null_percent[null_percent > 20].keys(),1,not_existent,col_for_drop = null_percent[null_percent > 20].keys() # if the null value % 20 or > 20 so need to drop it
"CHECKPOINT ASSIGN = ASSIGN.drop(col_for_drop, ""columns"") df.shape",1,execute_result,"# drop columns
 df = df.drop(col_for_drop, ""columns"")
 df.shape"
CHECKPOINT ASSIGN = df.isnull().sum()path[0]*100 null_percent,1,execute_result,"null_percent = df.isnull().sum()/df.shape[0]*100
 null_percent # shows values which has less than 20 % missing values"
CHECKPOINT for i in df.columns: print(i +  + str(len(df[i].unique()))),0,stream,"# find the unique value count
 for i in df.columns:
     print(i + ""\t"" + str(len(df[i].unique())))"
"CHECKPOINT for i in df.columns: print(.format(i, len(df[i].unique()), df[i].unique()))",0,stream,"# find unique values of each column
 for i in df.columns:
     print(""Unique value of:>>> {} ({})\n{}\n"".format(i, len(df[i].unique()), df[i].unique()))"
"train[""SalePrice""].describe()",0,execute_result,"# Describe the target 
 train[""SalePrice""].describe()"
"plt.figure(figsize=(10,8)) ASSIGN = sns.distplot(train[""SalePrice""])",0,display_data,"# Plot the distplot of target
 plt.figure(figsize=(10,8))
 bar = sns.distplot(train[""SalePrice""])"
"plt.figure(figsize=(25,25)) ASSIGN = sns.heatmap(train.corr(), cmap = ""coolwarm"", annot=True, linewidth=2)",0,display_data,"# correlation heatmap
 plt.figure(figsize=(25,25))
 ax = sns.heatmap(train.corr(), cmap = ""coolwarm"", annot=True, linewidth=2)
 
 # here we use piearson corelation"
"CHECKPOINT ASSIGN = train.corr() ASSIGN = hig_corr.index[abs(hig_corr[""SalePrice""]) >= 0.5] hig_corr_features",1,execute_result,"# correlation heatmap of higly correlated features with SalePrice
 hig_corr = train.corr()
 hig_corr_features = hig_corr.index[abs(hig_corr[""SalePrice""]) >= 0.5]
 hig_corr_features
"
"plt.figure(figsize=(10,8)) ASSIGN = sns.distplot(train[""LotFrontage""])",0,display_data,"plt.figure(figsize=(10,8))
 bar = sns.distplot(train[""LotFrontage""])"
"plt.figure(figsize=(10,8)) ASSIGN = sns.distplot(train[""MSSubClass""])",0,display_data,"plt.figure(figsize=(10,8))
 bar = sns.distplot(train[""MSSubClass""])"
SETUP,0,not_existent,"import numpy as np  import pandas as pd import matplotlib import matplotlib.pyplot as plt from matplotlib import image import seaborn as sns %matplotlib inline    from scipy.spatial import distance from keras.utils.np_utils import to_categorical # convert to one-hot-encoding from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D from keras.optimizers import RMSprop from tensorflow.keras.utils  import plot_model, model_to_dot from keras.preprocessing.image import ImageDataGenerator from keras.callbacks import EarlyStopping,LearningRateScheduler,ReduceLROnPlateau from sklearn.model_selection import train_test_split, StratifiedShuffleSplit"
"SETUP ASSIGN = np.load(DSPATH+""olivetti_faces.npy"") ASSIGN = np.load(DSPATH+""olivetti_faces_target.npy"") ThiefImage={} SLICE= image.imread(""..path"") SLICE=image.imread(""..path"")",0,not_existent,"DSPATH=""../input/olivetti-faces/"" X = np.load(DSPATH+""olivetti_faces.npy"") y = np.load(DSPATH+""olivetti_faces_target.npy"")    ThiefImage={} ThiefImage[""False""]= image.imread(""../input/thief-images/False.jpg"") ThiefImage[""True""]=image.imread(""../input/thief-images/True.jpg"")       "
ClASSES=np.unique(y),1,not_existent,ClASSES=np.unique(y) # N_CLASSES=len(np.unique(labels))
"StratifiedSplit = StratifiedShuffleSplit( test_size=0.4, random_state=0) StratifiedSplit.get_n_splits(X, y) for train_index, test_index in StratifiedSplit.split(X, y): X_train, X_test, y_train, y_test= X[train_index], X[test_index], y[train_index], y[test_index]",1,not_existent,"# Split into train/val  StratifiedSplit = StratifiedShuffleSplit( test_size=0.4, random_state=0) StratifiedSplit.get_n_splits(X, y) for train_index, test_index in StratifiedSplit.split(X, y):     X_train, X_test, y_train, y_test= X[train_index], X[test_index], y[train_index], y[test_index]      # X_train, X_test, y_train, y_test = train_test_split(     #     X, y, test_size=.40, random_state=42)  "
SETUP ASSIGN = sns.countplot(y_train),0,display_data,import seaborn as sns g = sns.countplot(y_train)  
ASSIGN=[] for i in range(len(X_train)): ASSIGN.append(X_train[y_train==i].mean(axis = 0)),1,stream,# calculate class mean  class_mean=[] for i in range(len(X_train)):     class_mean.append(X_train[y_train==i].mean(axis = 0))
"def ShowTrainingData(showNclasses=5): if showNclasses>=40: ASSIGN=ClASSES ASSIGN=2,4 for i in range(ASSIGN+1): ASSIGN = plt.subplots(rows,cols ) ASSIGN=0 for face in X_train[y_train==i]: ASSIGN+=1 ASSIGN==cols: ASSIGN=5 ASSIGN=plt.subplot(rows,cols,j) ASSIGN.imshow(face ,'gray' ) ASSIGN = plt.subplot(1,cols,cols) ASSIGN.imshow( class_mean[i], 'gray' ) plt.xlabel(""Class ""+str(i)+"" mean "" ) fig.tight_layout(pad=1.0) plt.show()",0,not_existent,"#Show Data  def ShowTrainingData(showNclasses=5):     if showNclasses>=40:         showNclasses=ClASSES     rows,cols=2,4          for i in range(showNclasses+1):         fig,ax =  plt.subplots(rows,cols )         j=0         for face in X_train[y_train==i]:             j+=1             if j==cols:                 j=5             ax=plt.subplot(rows,cols,j)             ax.imshow(face ,'gray' )          ax = plt.subplot(1,cols,cols)                 ax.imshow( class_mean[i], 'gray' )         plt.xlabel(""Class ""+str(i)+"" mean "" )         fig.tight_layout(pad=1.0)         plt.show()     "
"def ShowTrainingData2(showNclasses): if showNclasses>=40: ASSIGN=ClASSES ASSIGN=2,4 for i in range(ASSIGN+1): ASSIGN = plt.figure(figsize=(8, 4)) ASSIGN=0 for face in X_train[y_train==i]: ASSIGN+=1 ASSIGN==cols: ASSIGN=5 ASSIGN.add_subplot(rows, cols, ASSIGN) plt.imshow(face, cmap = plt.get_cmap('gray')) plt.axis('off') ASSIGN.add_subplot(1,cols,cols) plt.imshow(class_mean[i], cmap = plt.get_cmap('gray')) plt.title(""class_mean {}"".format(i), fontsize=16) plt.axis('off') plt.suptitle(""There are 6 image for class {}"".format(i), fontsize=15) plt.show()",0,not_existent,"#Show Data  def ShowTrainingData2(showNclasses):     if showNclasses>=40:         showNclasses=ClASSES     rows,cols=2,4          for i in range(showNclasses+1):         fig = plt.figure(figsize=(8, 4))              j=0         for face in X_train[y_train==i]:             j+=1             if j==cols:                 j=5             fig.add_subplot(rows, cols, j)             plt.imshow(face, cmap = plt.get_cmap('gray'))               plt.axis('off')                   fig.add_subplot(1,cols,cols)         plt.imshow(class_mean[i], cmap = plt.get_cmap('gray'))         plt.title(""class_mean {}"".format(i), fontsize=16)         plt.axis('off')  #         fig.tight_layout(pad=1.0)           plt.suptitle(""There are 6 image for class {}"".format(i), fontsize=15)         plt.show()      "
"def ShowPredictions(predic_Model,ShowNPredictions=5): if ShowNPredictions>=len(y_predictions): ShowNPredictions=len(y_predictions) ASSIGN=1,3 for index, row in y_predictions.iterrows(): if (index>ShowNPredictions): break ASSIGN=int(row[""ASSIGN""]) ASSIGN=int(row[""ASSIGN""]) ASSIGN=int(row[predic_Model+""_predic""]) IsTrue=str(row[predic_Model+""_True""]) ASSIGN = plt.subplots(rows,cols ) ASSIGN=1 ASSIGN=plt.subplot(rows,cols,j) ASSIGN.imshow(X_test[ASSIGN] ,'gray' ) plt.xlabel(""Test Number :""+str(ASSIGN) ) ASSIGN=2 ASSIGN=plt.subplot(rows,cols,j) ASSIGN.imshow(class_mean[ASSIGN] ,'gray' ) plt.xlabel(""Class ""+str(ASSIGN)+"" mean "" ) ASSIGN=3 ASSIGN=plt.subplot(rows,cols,j) ASSIGN.imshow(ThiefImage[IsTrue] ,'gray' ) plt.xlabel(""Class ""+str(ASSIGN)+"" mean "" ) fig.tight_layout(pad=2.0) plt.show()",0,not_existent,"def ShowPredictions(predic_Model,ShowNPredictions=5):          if ShowNPredictions>=len(y_predictions):         ShowNPredictions=len(y_predictions)     rows,cols=1,3       for index, row in y_predictions.iterrows():         if (index>ShowNPredictions):             break                      x=int(row[""x""])         actually=int(row[""actually""])         y_predic=int(row[predic_Model+""_predic""])         IsTrue=str(row[predic_Model+""_True""])          fig,ax =  plt.subplots(rows,cols )         j=1         ax=plt.subplot(rows,cols,j)         ax.imshow(X_test[x] ,'gray' )         plt.xlabel(""Test Number :""+str(x)  )          j=2         ax=plt.subplot(rows,cols,j)         ax.imshow(class_mean[y_predic] ,'gray' )         plt.xlabel(""Class ""+str(y_predic)+"" mean "" )          j=3         ax=plt.subplot(rows,cols,j)         ax.imshow(ThiefImage[IsTrue] ,'gray' )         plt.xlabel(""Class ""+str(actually)+"" mean "" )            fig.tight_layout(pad=2.0)         plt.show()       "
ShowTrainingData2(5),0,display_data,ShowTrainingData2(5)
"ASSIGN=np.array([(i,y_test[i],c,distance.euclidean(X_test[i].flatten() , class_mean[c].flatten() )) for c in ClASSES for i in range(len(X_test))])",1,not_existent,"distanceTable=np.array([(i,y_test[i],c,distance.euclidean(X_test[i].flatten() , class_mean[c].flatten() )) for c in ClASSES  for i in range(len(X_test))])"
CHECKPOINT distanceTable,0,execute_result,distanceTable
ASSIGN=ASSIGN.T,1,not_existent,"distanceTable=distanceTable.T # distanceTable.shape=(4,6400)"
"ASSIGN = {'x': distanceTable[0], 'actually':distanceTable[1],'KNN_predic':distanceTable[2],'distance':distanceTable[3]} ASSIGN= pd.DataFrame(data=d) ASSIGN.head()",1,execute_result,"  d = {'x': distanceTable[0], 'actually':distanceTable[1],'KNN_predic':distanceTable[2],'distance':distanceTable[3]} df= pd.DataFrame(data=d) df.head() "
df[df.x==0],0,execute_result,df[df.x==0]
"ASSIGN=pd.merge(df ,df.groupby([""x"",""actually""]).distance.min(), how = 'inner', on=[""x"",""actually"",""distance""])",1,not_existent,"y_predictions=pd.merge(df ,df.groupby([""x"",""actually""]).distance.min(), how = 'inner',  on=[""x"",""actually"",""distance""])"
"y_predictions[""KNN_True""]=y_predictions[""KNN_predic""]==y_predictions[""actually""]",1,not_existent,"y_predictions[""KNN_True""]=y_predictions[""KNN_predic""]==y_predictions[""actually""]"
"CHECKPOINT ASSIGN = np.nonzero(y_predictions[""KNN_True""].values==1)[0] ASSIGN = np.nonzero(y_predictions[""KNN_True""].values==0)[0] print(len(ASSIGN),) print(len(ASSIGN),)",1,stream,"correct_predictions = np.nonzero(y_predictions[""KNN_True""].values==1)[0] incorrect_predictions = np.nonzero(y_predictions[""KNN_True""].values==0)[0] print(len(correct_predictions),"" classified correctly"") print(len(incorrect_predictions),"" classified incorrectly"")"
"CHECKPOINT print() print() ShowPredictions(""KNN"",5)",0,stream,"print(""KNN_predic"") print(""============="")  ShowPredictions(""KNN"",5)"
"CHECKPOINT ASSIGN = ASSIGN.reshape(-1,64,64,1) ASSIGN = ASSIGN.reshape(-1,64,64,1) print(,ASSIGN.shape,,y_train.shape) print(, ASSIGN.shape,,y_test.shape)",1,stream,"X_train = X_train.reshape(-1,64,64,1) X_test = X_test.reshape(-1,64,64,1)    print(""X_train shape: "",X_train.shape,""y_train shape: "",y_train.shape) print(""x_test shape: "", X_test.shape,""y_test shape: "",y_test.shape)"
"ASSIGN = Sequential() ASSIGN.add(Conv2D(filters = 20, kernel_size = (5,5),padding = 'Same', ASSIGN ='relu', input_shape = (64,64,1))) ASSIGN.add(MaxPool2D(pool_size=(2,2))) ASSIGN.add(Dropout(0.25)) ASSIGN.add(Conv2D(filters = 50, kernel_size = (6,6),padding = 'Same', ASSIGN ='relu')) ASSIGN.add(MaxPool2D(pool_size=(2,2))) ASSIGN.add(Dropout(0.25)) ASSIGN.add(Conv2D(filters = 150, kernel_size = (5,5),padding = 'Same', ASSIGN ='relu', input_shape = (64,64,1))) ASSIGN.add(Flatten()) ASSIGN.add(Dense(256, ASSIGN = ""relu"")) ASSIGN.add(Dropout(0.5)) ASSIGN.add(Dense(40, ASSIGN = ""softmax""))",0,not_existent,"model = Sequential()  model.add(Conv2D(filters = 20, kernel_size = (5,5),padding = 'Same',                   activation ='relu', input_shape = (64,64,1)))  model.add(MaxPool2D(pool_size=(2,2))) model.add(Dropout(0.25))  model.add(Conv2D(filters = 50, kernel_size = (6,6),padding = 'Same',                   activation ='relu'))  model.add(MaxPool2D(pool_size=(2,2))) model.add(Dropout(0.25))  model.add(Conv2D(filters = 150, kernel_size = (5,5),padding = 'Same',                   activation ='relu', input_shape = (64,64,1)))  model.add(Flatten()) model.add(Dense(256, activation = ""relu"")) model.add(Dropout(0.5)) model.add(Dense(40, activation = ""softmax""))  "
"ASSIGN = ReduceLROnPlateau(monitor='val_acc', ASSIGN=3, ASSIGN=1, ASSIGN=0.7, ASSIGN=0.00000000001) ASSIGN = EarlyStopping(patience=2)",0,not_existent,"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',                                              patience=3,                                              verbose=1,                                              factor=0.7,                                              min_lr=0.00000000001) early_stopping_monitor = EarlyStopping(patience=2)"
model.summary(),0,stream,model.summary()
"plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)",0,execute_result,"plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
"ASSIGN = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) model.compile(ASSIGN = ASSIGN , loss='sparse_categorical_crossentropy', ASSIGN=['sparse_categorical_accuracy']) ASSIGN = 37 ASSIGN = 20 ASSIGN = ImageDataGenerator( ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=5, ASSIGN = 0.05, ASSIGN=0, ASSIGN=0, ASSIGN=False, ASSIGN=False) ASSIGN.fit(X_train) ASSIGN = model.fit_generator( ASSIGN.flow(X_train,y_train, ASSIGN=ASSIGN), ASSIGN = epoch, ASSIGN = (X_test,y_test), ASSIGN = 2, ASSIGN=X_train.shape[0] path, ASSIGN=[learning_rate_reduction] )",0,stream,"optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) model.compile(optimizer = optimizer , loss='sparse_categorical_crossentropy',             metrics=['sparse_categorical_accuracy']) epoch = 37 batch_size = 20  datagen = ImageDataGenerator(         featurewise_center=False,  # set input mean to 0 over the dataset         samplewise_center=False,  # set each sample mean to 0         featurewise_std_normalization=False,  # divide inputs by std of the dataset         samplewise_std_normalization=False,  # divide each input by its std         zca_whitening=False,  # apply ZCA whitening         rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)         zoom_range = 0.05, # Randomly zoom image          width_shift_range=0,  # randomly shift images horizontally (fraction of total width)         height_shift_range=0,  # randomly shift images vertically (fraction of total height)         horizontal_flip=False,  # randomly flip images         vertical_flip=False)  # randomly flip images datagen.fit(X_train)  history = model.fit_generator(                               datagen.flow(X_train,y_train, batch_size=batch_size),                               epochs = epoch,                                validation_data = (X_test,y_test),                               verbose = 2,                                steps_per_epoch=X_train.shape[0] // batch_size,                               callbacks=[learning_rate_reduction]                              )"
"CHECKPOINT print(history.history.keys()) plt.plot(history.history['sparse_categorical_accuracy']) plt.plot(history.history['val_sparse_categorical_accuracy']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['train', 'test'], loc='upper left') plt.show() plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'test'], loc='upper left') plt.show()",0,stream,"print(history.history.keys()) # summarize history for accuracy plt.plot(history.history['sparse_categorical_accuracy']) plt.plot(history.history['val_sparse_categorical_accuracy']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['train', 'test'], loc='upper left') plt.show() # summarize history for loss plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'test'], loc='upper left') plt.show()"
"CHECKPOINT print() print() ASSIGN = model.evaluate(X_test,y_test,batch_size=32) print(ASSIGN)",0,stream,"print(""CNN_predic"") print(""============="")  score = model.evaluate(X_test,y_test,batch_size=32) print(score)"
ASSIGN=model.predict_classes(X_test),0,not_existent,CNN_predic=model.predict_classes(X_test)
"ASSIGN = ASSIGN.reshape(-1,64,64)",1,not_existent,"X_test = X_test.reshape(-1,64,64)"
"del y_predictions ASSIGN=np.array([(i,y_test[i],CNN_predic[i] ) for i in range(len(X_test))]) ASSIGN=ASSIGN.T ASSIGN = {'x': distanceTable[0], 'actually':distanceTable[1],'CNN_predic':distanceTable[2] } ASSIGN= pd.DataFrame(data=d) ASSIGN.head() ASSIGN[""CNN_True""]=ASSIGN[""CNN_predic""]==ASSIGN[""actually""]",1,not_existent,"del y_predictions  distanceTable=np.array([(i,y_test[i],CNN_predic[i] )  for i in range(len(X_test))]) distanceTable=distanceTable.T d = {'x': distanceTable[0], 'actually':distanceTable[1],'CNN_predic':distanceTable[2] } y_predictions= pd.DataFrame(data=d) y_predictions.head() y_predictions[""CNN_True""]=y_predictions[""CNN_predic""]==y_predictions[""actually""]  "
y_predictions.head(100),0,execute_result,y_predictions.head(100)
"CHECKPOINT ASSIGN = np.nonzero(y_predictions[""CNN_True""].values==1)[0] ASSIGN = np.nonzero(y_predictions[""CNN_True""].values==0)[0] print(len(ASSIGN),) print(len(ASSIGN),)",1,stream,"correct_predictions = np.nonzero(y_predictions[""CNN_True""].values==1)[0] incorrect_predictions = np.nonzero(y_predictions[""CNN_True""].values==0)[0] print(len(correct_predictions),"" classified correctly"") print(len(incorrect_predictions),"" classified incorrectly"") "
"ShowPredictions(""CNN"")",0,display_data,"  ShowPredictions(""CNN"")   "
SETUP warnings.filterwarnings('ignore'),0,not_existent,"import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
 import matplotlib.pyplot as plt
 import warnings    #warnings to ignore any kind of warnings that we may recieve.
 warnings.filterwarnings('ignore')"
"def display_all(df): ''' input: dataframe description: it takes a dataframe and allows use to show a mentioned no. of rows and columns in the screen ''' with pd.option_context(""display.max_rows"",10,""display.max_columns"",9):  #you might want to change these numbers. display(df)",0,not_existent,"def display_all(df):
     '''
     input: dataframe
     description: it takes a dataframe and allows use to show a mentioned no. of rows and columns in the screen
     '''
     with pd.option_context(""display.max_rows"",10,""display.max_columns"",9):  #you might want to change these numbers.
         display(df)"
CHECKPOINT ASSIGN=pd.read_csv('..path') df.shape,0,execute_result,"df=pd.read_csv('../input/diabetes.csv')
 df.shape"
display_all(df),0,display_data,display_all(df)
"def missing_values_table(df): ASSIGN = df.isnull().sum() ASSIGN = 100 * df.isnull().sum() path(df) ASSIGN = pd.concat([mis_val, mis_val_percent], axis=1) ASSIGN = mis_val_table.rename( ASSIGN = {0 : 'Missing Values', 1 : '% of Total Values'}) ASSIGN = ASSIGN[ ASSIGN.iloc[:,1] != 0].sort_values( '% of Total Values', ascending=False).round(1) print (""Your selected dataframe has "" + str(df.shape[1]) + "" ASSIGN.\n"" ""There are "" + str(ASSIGN.shape[0]) + "" ASSIGN that have missing values."") return mis_val_table_ren_columns",1,not_existent,"def missing_values_table(df):
         # Total missing values
         mis_val = df.isnull().sum()
         
         # Percentage of missing values
         mis_val_percent = 100 * df.isnull().sum() / len(df)
         
         # Make a table with the results
         mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
         
         # Rename the columns
         mis_val_table_ren_columns = mis_val_table.rename(
         columns = {0 : 'Missing Values', 1 : '% of Total Values'})
         
         # Sort the table by percentage of missing descending
         mis_val_table_ren_columns = mis_val_table_ren_columns[
             mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(
         '% of Total Values', ascending=False).round(1)
         
         # Print some summary information
         print (""Your selected dataframe has "" + str(df.shape[1]) + "" columns.\n""      
             ""There are "" + str(mis_val_table_ren_columns.shape[0]) +
               "" columns that have missing values."")
         
         
         return mis_val_table_ren_columns"
missing_values_table(df),0,stream,missing_values_table(df)
"ASSIGN=['BMI','SkinThickness','BloodPressure','Insulin','Glucose']",1,not_existent,"features_with_missing_values=['BMI','SkinThickness','BloodPressure','Insulin','Glucose']
"
"for i in features_with_missing_values: SLICE=SLICE.replace(0,np.median(SLICE.values))",1,not_existent,"for i in features_with_missing_values:
     df[i]=df[i].replace(0,np.median(df[i].values))"
"ASSIGN=df['Outcome'].values df.drop(['Outcome'],inplace=True,axis=1)",1,not_existent,"target=df['Outcome'].values
 df.drop(['Outcome'],inplace=True,axis=1)"
SETUP ASSIGN=StandardScaler() ASSIGN=sta.fit_transform(df),1,not_existent,"#from sklearn importing standard scalar that will convert the provided dataframe into standardised one.
 from sklearn.preprocessing import StandardScaler                                              
 sta=StandardScaler()
 input=sta.fit_transform(df)    #will give numpy array as output"
"SETUP X_train,X_test,y_train,y_test=train_test_split(input,target,test_size=0.1,random_state=0)",1,not_existent,"from sklearn.model_selection import train_test_split
 X_train,X_test,y_train,y_test=train_test_split(input,target,test_size=0.1,random_state=0)"
SETUP,0,not_existent,from sklearn.neighbors import KNeighborsClassifier
ASSIGN=KNeighborsClassifier(n_neighbors=7),0,not_existent,knn=KNeighborsClassifier(n_neighbors=7)
"knn.fit(X_train,y_train)",0,execute_result,"knn.fit(X_train,y_train)"
"knn.score(X_test,y_test)",0,execute_result,"knn.score(X_test,y_test)"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed
 # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
 
 # Input data files are available in the ""../input/"" directory.
 # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
 
 import os
 for dirname, _, filenames in os.walk('/kaggle/input'):
     for filename in filenames:
         print(os.path.join(dirname, filename))
 
 # Any results you write to the current directory are saved as output."
"SETUP sns.set_style(""darkgrid"")",0,not_existent,"# Imports for Exploratory Data Analysis
 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 
 # Imports for Machine Learning Models
 import string
 import nltk
 from nltk.corpus import stopwords
 from sklearn.feature_extraction.text import CountVectorizer
 from sklearn.feature_extraction.text import TfidfTransformer
 from sklearn.naive_bayes import MultinomialNB
 from sklearn.model_selection import train_test_split
 from sklearn.pipeline import Pipeline
 
 # Validation
 from sklearn.metrics import classification_report, confusion_matrix
 
 # Set the style of the plots' background
 sns.set_style(""darkgrid"")
 
 # Show the plot in the same window as the notebook
 %matplotlib inline"
"ASSIGN = pd.read_csv(""path"") ASSIGN.head()",0,execute_result,"jobs = pd.read_csv(""/kaggle/input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv"")
 jobs.head()"
"plt.figure(figsize=(12,8)) sns.heatmap(jobs.isnull(), cmap=""coolwarm"", yticklabels=False, cbar=False)",0,execute_result,"plt.figure(figsize=(12,8))
 sns.heatmap(jobs.isnull(), cmap=""coolwarm"", yticklabels=False, cbar=False)"
"jobs.drop(columns=[""department"", ""salary_range"", ""benefits""], inplace=True)",1,not_existent,"jobs.drop(columns=[""department"", ""salary_range"", ""benefits""], inplace=True)"
jobs.info(),0,stream,jobs.info()
jobs.describe(),0,execute_result,jobs.describe()
jobs.isnull().sum(),0,execute_result,jobs.isnull().sum()
"ASSIGN = [""company_profile"", ""description"", ""requirements""] for col in ASSIGN: jobs.loc[(jobs[col].isnull()) & (jobs[""fraudulent""] == 0), col] = ""none"" jobs.loc[(jobs[col].isnull()) & (jobs[""fraudulent""] == 1), col] = ""missing""",1,not_existent,"# List with the columns to check the length of the text
 feature_lst = [""company_profile"", ""description"", ""requirements""]
 
 # For loop to treat the missing values in the columns of the feature_lst.
 for col in feature_lst:
     # If the job post is real, change the missing values to ""none""
     jobs.loc[(jobs[col].isnull()) & (jobs[""fraudulent""] == 0), col] = ""none""
     
     # If the job post is fake, change the missing values to ""missing""
     jobs.loc[(jobs[col].isnull()) & (jobs[""fraudulent""] == 1), col] = ""missing"""
"for num,col in enumerate(feature_lst): jobs[str(num)] = jobs[col].apply(len)",1,not_existent,"# For loop to create new columns with the lengths of the ones in the feature_lst
 for num,col in enumerate(feature_lst):
     jobs[str(num)] = jobs[col].apply(len)"
"ASSIGN = ASSIGN.rename({""0"": ""profile_length"", ""1"": ""description_length"", ""2"": ""requirements_length""}, axis=1)",1,not_existent,"# Rename the new columns created above
 jobs = jobs.rename({""0"": ""profile_length"", ""1"": ""description_length"", ""2"": ""requirements_length""}, axis=1)"
"jobs[""fraudulent""].value_counts()",0,execute_result,"jobs[""fraudulent""].value_counts()"
"sns.pairplot(data=jobs[[""fraudulent"", ""profile_length"", ""description_length"", ""requirements_length""]], ASSIGN=""fraudulent"", height=2, aspect=2);",0,display_data,"sns.pairplot(data=jobs[[""fraudulent"", ""profile_length"", ""description_length"", ""requirements_length""]],
              hue=""fraudulent"", height=2, aspect=2);"
"ASSIGN = sns.FacetGrid(jobs, col=""fraudulent"", aspect=1.5, height=4, sharey=False) ASSIGN = ASSIGN.map(plt.hist, ""profile_length"", bins=40) ASSIGN = profile_grid.ASSIGN.flatten() ASSIGN[0].set_title(""Non-Fraudulent (0)"", fontsize=14) ASSIGN[1].set_title(""Fraudulent (1)"", fontsize=14) ASSIGN[0].set_ylabel(""Count"", fontsize=14) for ax in ASSIGN: ax.set_xlabel(""Profile Text Length"", fontsize=14)",0,display_data,"profile_grid = sns.FacetGrid(jobs, col=""fraudulent"", aspect=1.5, height=4, sharey=False)
 profile_grid = profile_grid.map(plt.hist, ""profile_length"", bins=40)
 
 # Flatten the axes. Create an iterator
 axes = profile_grid.axes.flatten()
 
 # Title
 axes[0].set_title(""Non-Fraudulent (0)"", fontsize=14)
 axes[1].set_title(""Fraudulent (1)"", fontsize=14)
 
 # Labels
 axes[0].set_ylabel(""Count"", fontsize=14)
 for ax in axes:
     ax.set_xlabel(""Profile Text Length"", fontsize=14)"
"ASSIGN = sns.FacetGrid(jobs, col=""fraudulent"", aspect=1.5, height=4, sharey=False) ASSIGN = ASSIGN.map(plt.hist, ""description_length"", bins=40) ASSIGN = description_grid.ASSIGN.flatten() ASSIGN[0].set_title(""Non-Fraudulent (0)"", fontsize=14) ASSIGN[1].set_title(""Fraudulent (1)"", fontsize=14) ASSIGN[0].set_ylabel(""Count"", fontsize=14) for ax in ASSIGN: ax.set_xlabel(""Description Text Length"", fontsize=14)",0,display_data,"description_grid = sns.FacetGrid(jobs, col=""fraudulent"", aspect=1.5, height=4, sharey=False)
 description_grid = description_grid.map(plt.hist, ""description_length"", bins=40)
 
 # Flatten the axes. Create an iterator
 axes = description_grid.axes.flatten()
 
 # Title
 axes[0].set_title(""Non-Fraudulent (0)"", fontsize=14)
 axes[1].set_title(""Fraudulent (1)"", fontsize=14)
 
 # Labels
 axes[0].set_ylabel(""Count"", fontsize=14)
 for ax in axes:
     ax.set_xlabel(""Description Text Length"", fontsize=14)"
"ASSIGN = sns.FacetGrid(jobs, col=""fraudulent"", aspect=1.5, height=4, sharey=False) ASSIGN = ASSIGN.map(plt.hist, ""requirements_length"", bins=40) ASSIGN = requirements_grid.ASSIGN.flatten() ASSIGN[0].set_title(""Non Fraudulent (0)"", fontsize=14) ASSIGN[1].set_title(""Fraudulent (1)"", fontsize=14) ASSIGN[0].set_ylabel(""Count"", fontsize=14) for ax in ASSIGN: ax.set_xlabel(""Requirements Text Length"", fontsize=14)",0,display_data,"requirements_grid = sns.FacetGrid(jobs, col=""fraudulent"", aspect=1.5, height=4, sharey=False)
 requirements_grid = requirements_grid.map(plt.hist, ""requirements_length"", bins=40)
 
 # Another option. Makes less obviuos which axes is to be labelled
 #requirements_grid.set_axis_labels(""Requirement Length"", ""Count"")
 
 # Flatten the axes. Create an iterator
 axes = requirements_grid.axes.flatten()
 
 # Title
 axes[0].set_title(""Non Fraudulent (0)"", fontsize=14)
 axes[1].set_title(""Fraudulent (1)"", fontsize=14)
 
 # Labels
 axes[0].set_ylabel(""Count"", fontsize=14)
 for ax in axes:
     ax.set_xlabel(""Requirements Text Length"", fontsize=14)"
"sns.catplot(x=""has_company_logo"", hue=""fraudulent"", data=jobs, kind=""count"", aspect=2, height=4); plt.xlabel(""Company Logo"", fontsize=14) plt.xticks([0, 1], (""Has"", ""Doesn't have""), fontsize=12) plt.ylabel(""Count"", fontsize=14);",0,display_data,"sns.catplot(x=""has_company_logo"", hue=""fraudulent"", data=jobs, kind=""count"", aspect=2, height=4);
 
 plt.xlabel(""Company Logo"", fontsize=14)
 plt.xticks([0, 1], (""Has"", ""Doesn't have""), fontsize=12)
 plt.ylabel(""Count"", fontsize=14);"
"ASSIGN = plt.subplots(1, 2, figsize=(18,8)) ASSIGN = sns.countplot(x=jobs[""employment_type""].dropna(), hue=jobs[""fraudulent""], palette=""Set1"", ax=axes[0]) axes[0].set_xlabel(""Employment Type"", fontsize=15) axes[0].set_ylabel(""Count"", fontsize=15) axes[0].set_title(""Employment Type Count"", fontsize=15) axes[0].legend("""") for p in ASSIGN.patches: ASSIGN.annotate(""{:.0f}"".format(p.get_height()), (p.get_x() + p.get_width() path, p.get_height()), ASSIGN='center', va='center', fontsize=14, color='black', xytext=(0, 12), ASSIGN='offset points') ASSIGN = sns.countplot(x=jobs[""employment_type""].dropna(), hue=jobs[""fraudulent""], palette=""Set1"", ax=axes[1]) axes[1].set_xlabel(""Employment Type"", fontsize=15) axes[1].set_ylim((0, 1500)) axes[1].set_ylabel("""") axes[1].set_title(""Employment Type Count Zoom"", fontsize=15) axes[1].legend(title=""Fraudulent"", title_fontsize=14, fontsize=12, bbox_to_anchor=(1.2, 0.6));",0,display_data,"# Create a 1 by 2 figure and axes
 fig, axes = plt.subplots(1, 2, figsize=(18,8))
 
 # Plot a countplot on the first axes
 employ = sns.countplot(x=jobs[""employment_type""].dropna(), hue=jobs[""fraudulent""], palette=""Set1"", ax=axes[0])
 axes[0].set_xlabel(""Employment Type"", fontsize=15)
 axes[0].set_ylabel(""Count"", fontsize=15)
 axes[0].set_title(""Employment Type Count"", fontsize=15)
 axes[0].legend("""")
 
 # Write the height of the bars on top
 for p in employ.patches:
     employ.annotate(""{:.0f}"".format(p.get_height()), 
                         (p.get_x() + p.get_width() / 2., p.get_height()),
                         ha='center', va='center', fontsize=14, color='black', xytext=(0, 12),
                         textcoords='offset points')
 
 #############################################################
 
 # Plot a countplot on the second axes
 employ_zoom = sns.countplot(x=jobs[""employment_type""].dropna(), hue=jobs[""fraudulent""], palette=""Set1"", ax=axes[1])
 axes[1].set_xlabel(""Employment Type"", fontsize=15)
 axes[1].set_ylim((0, 1500))
 axes[1].set_ylabel("""")
 axes[1].set_title(""Employment Type Count Zoom"", fontsize=15)
 axes[1].legend(title=""Fraudulent"", title_fontsize=14, fontsize=12, bbox_to_anchor=(1.2, 0.6));"
CHECKPOINT jobs.columns,0,execute_result,jobs.columns
"ASSIGN = jobs[""company_profile""] ASSIGN = jobs[""fraudulent""] X1_profile_train, X1_profile_test, y1_train, y1_test = train_test_split(ASSIGN, ASSIGN, test_size=0.2, random_state=42)",1,not_existent,"X1_profile = jobs[""company_profile""]
 y1 = jobs[""fraudulent""]
 X1_profile_train, X1_profile_test, y1_train, y1_test = train_test_split(X1_profile, y1, test_size=0.2, random_state=42)"
"def text_process(text): ASSIGN = [char for char in text if char not in string.punctuation] ASSIGN = """".join(ASSIGN) return [word for word in ASSIGN.split() if word.lower() not in stopwords.words(""english"")]",1,not_existent,"def text_process(text):
     # Remove the punctuation
     nopunc = [char for char in text if char not in string.punctuation]
     
     # Join the list of characters to form strings
     nopunc = """".join(nopunc)
     
     # Remove stopwords
     return [word for word in nopunc.split() if word.lower() not in stopwords.words(""english"")]"
"ASSIGN = Pipeline([(""bow no func"", CountVectorizer()), (""NB_classifier"", MultinomialNB())])",1,not_existent,"NB_pipeline = Pipeline([(""bow no func"", CountVectorizer()),
                        (""NB_classifier"", MultinomialNB())])"
"NB_pipeline.fit(X1_profile_train, y1_train)",0,execute_result,"NB_pipeline.fit(X1_profile_train, y1_train)"
ASSIGN = NB_pipeline.predict(X1_profile_test),0,not_existent,NB_pred = NB_pipeline.predict(X1_profile_test)
"CHECKPOINT print(classification_report(y1_test, NB_pred))",0,stream,"print(classification_report(y1_test, NB_pred))"
"CHECKPOINT print(confusion_matrix(y1_test, NB_pred))",0,stream,"print(confusion_matrix(y1_test, NB_pred))"
"ASSIGN = Pipeline([(""bow with func"", CountVectorizer(analyzer=text_process)), (""NB_classifier"", MultinomialNB())])",1,not_existent,"NB_func_pipeline = Pipeline([(""bow with func"", CountVectorizer(analyzer=text_process)),
                             (""NB_classifier"", MultinomialNB())])"
"ASSIGN = jobs[""company_profile""] ASSIGN = jobs[""fraudulent""] X2_profile_train, X2_profile_test, y2_train, y2_test = train_test_split(ASSIGN, ASSIGN, test_size=0.2, random_state=42)",1,not_existent,"X2_profile = jobs[""company_profile""]
 y2 = jobs[""fraudulent""]
 X2_profile_train, X2_profile_test, y2_train, y2_test = train_test_split(X2_profile, y2, test_size=0.2, random_state=42)"
"NB_func_pipeline.fit(X2_profile_train, y2_train)",0,execute_result,"NB_func_pipeline.fit(X2_profile_train, y2_train)"
ASSIGN = NB_func_pipeline.predict(X2_profile_test),0,not_existent,NB_func_pred = NB_func_pipeline.predict(X2_profile_test)
"CHECKPOINT print(classification_report(y2_test, NB_func_pred))",0,stream,"print(classification_report(y2_test, NB_func_pred))"
"CHECKPOINT print(confusion_matrix(y2_test, NB_func_pred))",0,stream,"print(confusion_matrix(y2_test, NB_func_pred))"
"ASSIGN = Pipeline([(""bow no func"", CountVectorizer()), (""tfidf"", TfidfTransformer()), (""NB_classifier"", MultinomialNB())])",1,not_existent,"NB_tfidf_pipeline = Pipeline([(""bow no func"", CountVectorizer()),
                               (""tfidf"", TfidfTransformer()),
                               (""NB_classifier"", MultinomialNB())])"
"ASSIGN = jobs[""company_profile""] ASSIGN = jobs[""fraudulent""] X3_profile_train, X3_profile_test, y3_train, y3_test = train_test_split(ASSIGN, ASSIGN, test_size=0.2, random_state=42)",1,not_existent,"X3_profile = jobs[""company_profile""]
 y3 = jobs[""fraudulent""]
 X3_profile_train, X3_profile_test, y3_train, y3_test = train_test_split(X3_profile, y3, test_size=0.2, random_state=42)"
"NB_tfidf_pipeline.fit(X3_profile_train, y3_train)",0,execute_result,"NB_tfidf_pipeline.fit(X3_profile_train, y3_train)"
ASSIGN = NB_tfidf_pipeline.predict(X3_profile_test),0,not_existent,NB_tfidf_pred = NB_tfidf_pipeline.predict(X3_profile_test)
"CHECKPOINT print(classification_report(y3_test, NB_tfidf_pred))",0,stream,"print(classification_report(y3_test, NB_tfidf_pred))"
"CHECKPOINT print(confusion_matrix(y3_test, NB_tfidf_pred))",0,stream,"print(confusion_matrix(y3_test, NB_tfidf_pred))"
"ASSIGN = Pipeline([(""bow with func"", CountVectorizer(analyzer=text_process)), (""tfidf"", TfidfTransformer()), (""NB_classifier"", MultinomialNB())])",1,not_existent,"NB_func_tfidf_pipeline = Pipeline([(""bow with func"", CountVectorizer(analyzer=text_process)),
                               (""tfidf"", TfidfTransformer()),
                               (""NB_classifier"", MultinomialNB())])"
"ASSIGN = jobs[""company_profile""] ASSIGN = jobs[""fraudulent""] X4_profile_train, X4_profile_test, y4_train, y4_test = train_test_split(ASSIGN, ASSIGN, test_size=0.2, random_state=42)",1,not_existent,"X4_profile = jobs[""company_profile""]
 y4 = jobs[""fraudulent""]
 X4_profile_train, X4_profile_test, y4_train, y4_test = train_test_split(X4_profile, y4, test_size=0.2, random_state=42)"
"NB_func_tfidf_pipeline.fit(X4_profile_train, y4_train)",0,execute_result,"NB_func_tfidf_pipeline.fit(X4_profile_train, y4_train)"
ASSIGN = NB_func_tfidf_pipeline.predict(X4_profile_test),0,not_existent,NB_func_tfidf_pred = NB_func_tfidf_pipeline.predict(X4_profile_test)
"CHECKPOINT print(classification_report(y4_test, NB_func_tfidf_pred))",0,stream,"print(classification_report(y4_test, NB_func_tfidf_pred))"
"CHECKPOINT print(confusion_matrix(y4_test, NB_func_tfidf_pred))",0,stream,"print(confusion_matrix(y4_test, NB_func_tfidf_pred))"
"SETUP CHECKPOINT sns.set_style(""whitegrid"") plt.style.use(""fivethirtyeight"") for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename)) warnings.filterwarnings('ignore')",0,stream,"import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns sns.set_style(""whitegrid"") plt.style.use(""fivethirtyeight"")  #Showing full path of datasets import os for dirname, _, filenames in os.walk('/kaggle/input'):     for filename in filenames:         print(os.path.join(dirname, filename))           #Disable warnings import warnings warnings.filterwarnings('ignore')"
"ASSIGN = pd.read_csv(""path"")",0,not_existent,"df = pd.read_csv(""/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv"")"
CHECKPOINT df.shape,0,execute_result,#Number of rows and columns in our dataset df.shape
CHECKPOINT df.columns,0,execute_result,#The 24 columns  df.columns
"df.drop(['RISK_MM'],axis=1,inplace=True)",1,not_existent,"#As mentioned in the dataset description ,  #we should exclude the variable Risk-MM when training a binary classification model. #Not excluding it will leak the answers to your model and reduce its predictability.  df.drop(['RISK_MM'],axis=1,inplace=True)"
df.info(),0,stream,#Basic Information of dataset  df.info()
"ASSIGN = ['Evaporation','Sunshine','Cloud9am','Cloud3pm','Date','Location'] df.drop(columns=ASSIGN,inplace=True,axis=1)",1,not_existent,"#Before looking at the description of the data #We can see that there are few columns with very less data #Evaporation,Sunshine,Cloud9am,Cloud3pm #It is better to remove these four columns as it will affect our prediction even if we #fill the na values...  #Date and Location is also not required #As we are predicting rain in australia and not when and where in australia   drop_cols = ['Evaporation','Sunshine','Cloud9am','Cloud3pm','Date','Location']  df.drop(columns=drop_cols,inplace=True,axis=1)"
df.info(),0,stream,df.info()
df.describe(),0,execute_result,#Basic description of our data #Numerical features first df.describe()
df.describe(include='object'),1,execute_result,#Including Categorical features with include object df.describe(include='object')
df.describe(include='all'),0,execute_result,#Now including all the features df.describe(include='all')
df.isna().sum(),0,execute_result,#Our dataset consists of 142193 rows and the count for many features is less than 142193. #This shows presence of Null values. #Let's look at the null values..  df.isna().sum()
df.skew(),0,execute_result,df.skew()
"ASSIGN = [col for col in df.columns if df[col].dtype==""float64""] for col in ASSIGN: df[col].fillna(df[col].median(),inplace=True) ASSIGN = [col for col in df.columns if df[col].dtype==""O""] for col in ASSIGN: df[col].fillna(df[col].mode()[0],inplace=True)",1,not_existent,"#Filling missing values  #We can see that there are outliers in our data #So the best way to fill the na values in our numerical features is with median #Because median deals the best with outliers  #Let's separate numerical and categorical #data type of numerical features is equal to float64 #With the help of following list comprehension we separate the numerical features...  num = [col for col in df.columns if df[col].dtype==""float64""]  for col in num:     df[col].fillna(df[col].median(),inplace=True)      cat = [col for col in df.columns if df[col].dtype==""O""] for col in cat:     df[col].fillna(df[col].mode()[0],inplace=True)"
df.isna().sum(),0,execute_result,#Check missing values df.isna().sum()
"df.corr().style.background_gradient(cmap=""Reds"")",0,execute_result,"df.corr().style.background_gradient(cmap=""Reds"")"
"ASSIGN = df.ASSIGN() ASSIGN = plt.figure(figsize=(12,12)) sns.heatmap(ASSIGN,annot=True,fmt="".1f"",linewidths=""0.1"")",0,execute_result,"#With the use of heatmap corr = df.corr()  fig = plt.figure(figsize=(12,12)) sns.heatmap(corr,annot=True,fmt="".1f"",linewidths=""0.1"")"
CHECKPOINT print(.format(num)) print(.format(len(num))),0,stream,"print(""Numerical features :: {}\n"".format(num)) print(""No of Numerical features :: {}"".format(len(num)))"
"plt.figure(figsize=(15,15)) plt.subplots_adjust(hspace=0.5) ASSIGN=1 ASSIGN = ['Red','Blue','Green','Cyan', 'Red','Blue','Green','Cyan', 'Red','Blue','Green','Cyan'] ASSIGN=0 for col in num: plt.subplot(3,4,ASSIGN) ASSIGN = sns.distplot(df[col],color=colors[j]) ASSIGN+=1 ASSIGN+=1",0,display_data,"plt.figure(figsize=(15,15)) plt.subplots_adjust(hspace=0.5)  i=1 colors = ['Red','Blue','Green','Cyan',          'Red','Blue','Green','Cyan',          'Red','Blue','Green','Cyan'] j=0 for col in num:     plt.subplot(3,4,i)     a1 = sns.distplot(df[col],color=colors[j])     i+=1     j+=1"
"plt.figure(figsize=(15,30)) plt.subplots_adjust(hspace=0.5) ASSIGN=1 for col in num: plt.subplot(6,2,ASSIGN) ASSIGN = sns.boxplot(data=df,x=""RainTomorrow"",y=col) ASSIGN+=1",0,display_data,"plt.figure(figsize=(15,30)) plt.subplots_adjust(hspace=0.5)  i=1 for col in num:     plt.subplot(6,2,i)     a1 = sns.boxplot(data=df,x=""RainTomorrow"",y=col)     i+=1"
"SETUP CHECKPOINT ASSIGN = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm'] for col in ASSIGN: Lower_Bound = df[col].quantile(0.25) - (IQR*3) Upper_Bound = df[col].quantile(0.75) + (IQR*3) print(.format(col,Lower_Bound,Upper_Bound)) ASSIGN = df[col].min() ASSIGN = df[col].max() print(.format(col,ASSIGN,ASSIGN)) if ASSIGN>Upper_Bound: print(.format(col,Upper_Bound)) elif ASSIGN<Lower_Bound: print(.format(col,Lower_Bound))",1,stream,"#Create a loop that finds the outliers in train and test  and removes it features_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']  for col in features_to_examine:     IQR = df[col].quantile(0.75) - df[col].quantile(0.25)      Lower_Bound = df[col].quantile(0.25) - (IQR*3)     Upper_Bound = df[col].quantile(0.75) + (IQR*3)          print(""The outliers in {} feature are values <<< {} and >>> {}"".format(col,Lower_Bound,Upper_Bound))          minimum = df[col].min()     maximum = df[col].max()     print(""The minimum value in {} is {} and maximum value is {}"".format(col,minimum,maximum))          if maximum>Upper_Bound:           print(""The outliers for {} are value greater than {}\n"".format(col,Upper_Bound))     elif minimum<Lower_Bound:           print(""The outliers for {} are value smaller than {}\n"".format(col,Lower_Bound))"
"plt.figure(figsize=(15,30)) plt.subplots_adjust(hspace=0.5) ASSIGN=1 for col in num: plt.subplot(6,2,ASSIGN) ASSIGN = sns.barplot(data=df,x=""RainTomorrow"",y=col) ASSIGN+=1",0,display_data,"plt.figure(figsize=(15,30)) plt.subplots_adjust(hspace=0.5)  i=1 for col in num:     plt.subplot(6,2,i)     a1 = sns.barplot(data=df,x=""RainTomorrow"",y=col)     i+=1"
"plt.figure(figsize=(15,5)) plt.subplots_adjust(hspace=0.5) ASSIGN=1 ASSIGN = [""MaxTemp"",""Temp9am"",""Temp3pm""] for feature in ASSIGN: plt.subplot(1,3,ASSIGN) sns.scatterplot(data=df,x=""MinTemp"",y=feature,hue=""RainTomorrow"") ASSIGN+=1",0,display_data,"plt.figure(figsize=(15,5)) plt.subplots_adjust(hspace=0.5)  i=1 features_list = [""MaxTemp"",""Temp9am"",""Temp3pm""] for feature in features_list:     plt.subplot(1,3,i)     sns.scatterplot(data=df,x=""MinTemp"",y=feature,hue=""RainTomorrow"")     i+=1"
"plt.figure(figsize=(15,8)) plt.subplots_adjust(hspace=0.5) plt.subplot(3,2,1) sns.scatterplot(data=df,x=""WindSpeed9am"",y=""WindGustSpeed"",hue=""RainTomorrow"") plt.subplot(3,2,2) sns.scatterplot(data=df,x=""WindSpeed3pm"",y=""WindGustSpeed"",hue=""RainTomorrow"") plt.subplot(3,2,3) sns.scatterplot(data=df,x=""Humidity9am"",y=""Humidity3pm"",hue=""RainTomorrow"") plt.subplot(3,2,4) sns.scatterplot(data=df,x=""Temp9am"",y=""Temp3pm"",hue=""RainTomorrow"") plt.subplot(3,2,5) sns.scatterplot(data=df,x=""MaxTemp"",y=""Temp9am"",hue=""RainTomorrow"") plt.subplot(3,2,6) sns.scatterplot(data=df,x=""Humidity3pm"",y=""Temp3pm"",hue=""RainTomorrow"")",0,execute_result,"plt.figure(figsize=(15,8)) plt.subplots_adjust(hspace=0.5)  plt.subplot(3,2,1) sns.scatterplot(data=df,x=""WindSpeed9am"",y=""WindGustSpeed"",hue=""RainTomorrow"")  plt.subplot(3,2,2) sns.scatterplot(data=df,x=""WindSpeed3pm"",y=""WindGustSpeed"",hue=""RainTomorrow"")  plt.subplot(3,2,3) sns.scatterplot(data=df,x=""Humidity9am"",y=""Humidity3pm"",hue=""RainTomorrow"")  plt.subplot(3,2,4) sns.scatterplot(data=df,x=""Temp9am"",y=""Temp3pm"",hue=""RainTomorrow"")  plt.subplot(3,2,5) sns.scatterplot(data=df,x=""MaxTemp"",y=""Temp9am"",hue=""RainTomorrow"")  plt.subplot(3,2,6) sns.scatterplot(data=df,x=""Humidity3pm"",y=""Temp3pm"",hue=""RainTomorrow"")"
CHECKPOINT cat,0,execute_result,cat
df['WindGustDir'].value_counts(),0,execute_result,df['WindGustDir'].value_counts()
"ASSIGN = plt.figure(figsize=(15,5)) sns.countplot(data=df,x=""WindGustDir"",hue=""RainTomorrow"");",0,display_data,"fig = plt.figure(figsize=(15,5)) sns.countplot(data=df,x=""WindGustDir"",hue=""RainTomorrow"");"
df['WindDir9am'].value_counts(),0,execute_result,df['WindDir9am'].value_counts()
"ASSIGN = plt.figure(figsize=(15,5)) sns.countplot(data=df,x=""WindDir9am"",hue=""RainTomorrow"");",0,display_data,"fig = plt.figure(figsize=(15,5)) sns.countplot(data=df,x=""WindDir9am"",hue=""RainTomorrow"");"
df['WindDir3pm'].value_counts(),0,execute_result,df['WindDir3pm'].value_counts()
"ASSIGN = plt.figure(figsize=(15,5)) sns.countplot(data=df,x=""WindDir3pm"",hue=""RainTomorrow"");",0,display_data,"fig = plt.figure(figsize=(15,5)) sns.countplot(data=df,x=""WindDir3pm"",hue=""RainTomorrow"");"
df['RainTomorrow'].value_counts(),0,execute_result,df['RainTomorrow'].value_counts()
"sns.countplot(data=df,x=""RainTomorrow"")",0,execute_result,"sns.countplot(data=df,x=""RainTomorrow"")"
"SETUP ASSIGN=df[['RainTomorrow']] X=df.drop(['RainTomorrow'],axis=1) X_train,X_test,y_train,y_test = tts(X,ASSIGN,test_size=0.3,random_state=0)",1,not_existent,"from sklearn.model_selection import train_test_split as tts y=df[['RainTomorrow']] X=df.drop(['RainTomorrow'],axis=1)  X_train,X_test,y_train,y_test = tts(X,y,test_size=0.3,random_state=0)"
CHECKPOINT X_train,0,execute_result,X_train
CHECKPOINT X_test,0,execute_result,X_test
"plt.figure(figsize=(15,30)) plt.subplots_adjust(hspace=0.5) ASSIGN = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm'] ASSIGN=1 for col in ASSIGN: plt.subplot(6,2,ASSIGN) ASSIGN = df[col].hist(bins=10) ASSIGN.set_xlabel(col) ASSIGN.set_ylabel('RainTomorrow') ASSIGN+=1",0,display_data,"#We'll plot these four as subplots   plt.figure(figsize=(15,30)) plt.subplots_adjust(hspace=0.5)  features_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm'] i=1 for col in features_to_examine:     plt.subplot(6,2,i)     fig = df[col].hist(bins=10)     fig.set_xlabel(col)     fig.set_ylabel('RainTomorrow')     i+=1"
"def remove_outliers(df,col,Lower_Bound,Upper_Bound): ASSIGN = df[col].min() ASSIGN = df[col].max() if ASSIGN>Upper_Bound: return np.where(df[col]>Upper_Bound,Upper_Bound,df[col]) elif ASSIGN<Lower_Bound: return np.where(df[col]<Lower_Bound,Lower_Bound,df[col])",1,not_existent,"def remove_outliers(df,col,Lower_Bound,Upper_Bound):         minimum = df[col].min()     maximum = df[col].max()          if maximum>Upper_Bound:         return np.where(df[col]>Upper_Bound,Upper_Bound,df[col])                elif minimum<Lower_Bound:         return np.where(df[col]<Lower_Bound,Lower_Bound,df[col])"
"for df1 in [X_train,X_test]: df1['Rainfall'] = remove_outliers(df1,'Rainfall',-1.799,2.4) df1['WindGustSpeed'] = remove_outliers(df1,'WindGustSpeed',-14.0,91.0) df1['WindSpeed9am'] = remove_outliers(df1,'WindSpeed9am',-29.0,55.0) df1['WindSpeed3pm'] = remove_outliers(df1,'WindSpeed3pm',-20.0,57.0)",1,not_existent,"for df1 in [X_train,X_test]:     df1['Rainfall'] = remove_outliers(df1,'Rainfall',-1.799,2.4)     df1['WindGustSpeed'] = remove_outliers(df1,'WindGustSpeed',-14.0,91.0)     df1['WindSpeed9am'] = remove_outliers(df1,'WindSpeed9am',-29.0,55.0)     df1['WindSpeed3pm'] = remove_outliers(df1,'WindSpeed3pm',-20.0,57.0)"
"plt.figure(figsize=(15,30)) plt.subplots_adjust(hspace=0.5) ASSIGN = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm'] ASSIGN=1 for col in ASSIGN: plt.subplot(6,2,ASSIGN) ASSIGN = sns.boxplot(data=X_train,y=col) ASSIGN.set_xlabel(col) ASSIGN.set_ylabel('RainTomorrow') ASSIGN+=1",0,display_data,"#If we look at their boxplots we can see that the outliers are now capped... plt.figure(figsize=(15,30)) plt.subplots_adjust(hspace=0.5)  features_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm'] i=1 for col in features_to_examine:     plt.subplot(6,2,i)     fig = sns.boxplot(data=X_train,y=col)     fig.set_xlabel(col)     fig.set_ylabel('RainTomorrow')     i+=1"
X_train[features_to_examine].describe(),0,execute_result,#Describe helps us understand more about the mean and max values  X_train[features_to_examine].describe()
X_test[features_to_examine].describe(),0,execute_result,X_test[features_to_examine].describe()
"for df2 in [y_train,y_test]: df2['RainTomorrow'] = df2['RainTomorrow'].replace({""Yes"":1, ""No"":0})",1,not_existent,"#Our next step is to encode all the categorical variables. #first we will convert our target variable  for df2 in [y_train,y_test]:     df2['RainTomorrow'] = df2['RainTomorrow'].replace({""Yes"":1,                                                     ""No"":0})  "
SETUP ASSIGN = ce.BinaryEncoder(cols=['RainToday']) ASSIGN = encoder.fit_transform(ASSIGN) ASSIGN = encoder.transform(ASSIGN),1,not_existent,import category_encoders as ce  encoder = ce.BinaryEncoder(cols=['RainToday'])  X_train = encoder.fit_transform(X_train)  X_test = encoder.transform(X_test)
"ASSIGN = pd.concat([ASSIGN[num],ASSIGN[['RainToday_0','RainToday_1']], pd.get_dummies(ASSIGN['WindGustDir']), pd.get_dummies(ASSIGN['WindDir9am']), pd.get_dummies(ASSIGN['WindDir3pm'])],axis=1)",1,not_existent,"#Now we will make our training dataset  X_train = pd.concat([X_train[num],X_train[['RainToday_0','RainToday_1']],                     pd.get_dummies(X_train['WindGustDir']),                     pd.get_dummies(X_train['WindDir9am']),                     pd.get_dummies(X_train['WindDir3pm'])],axis=1) "
"ASSIGN = pd.concat([ASSIGN[num],ASSIGN[['RainToday_0','RainToday_1']], pd.get_dummies(ASSIGN['WindGustDir']), pd.get_dummies(ASSIGN['WindDir9am']), pd.get_dummies(ASSIGN['WindDir3pm'])],axis=1)",1,not_existent,"#Same for testing set  X_test = pd.concat([X_test[num],X_test[['RainToday_0','RainToday_1']],                     pd.get_dummies(X_test['WindGustDir']),                     pd.get_dummies(X_test['WindDir9am']),                     pd.get_dummies(X_test['WindDir3pm'])],axis=1)"
X_test.head(),0,execute_result,X_test.head()
SETUP ASSIGN = X_train.columns ASSIGN = MinMaxScaler() ASSIGN = scaler.fit_transform(ASSIGN) ASSIGN = scaler.transform(ASSIGN),1,not_existent,"#our training and testing set is ready for our model #But ,before that we need to bring all the features to same scale with feature scaling #For this we will use MinMaxScaler #As there our negative values in our dataset and MinMaxScaler scales our data in range -1 to 1.  cols = X_train.columns  from sklearn.preprocessing import MinMaxScaler  scaler = MinMaxScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) "
"ASSIGN = pd.DataFrame(ASSIGN,columns=cols) ASSIGN = pd.DataFrame(ASSIGN,columns=cols)",1,not_existent,"X_train = pd.DataFrame(X_train,columns=cols) X_test = pd.DataFrame(X_test,columns=cols)"
"SETUP ASSIGN = LogisticRegression(solver='liblinear', random_state=0) ASSIGN.fit(X_train, y_train)",0,execute_result,"from sklearn.linear_model import LogisticRegression  # instantiate the model logreg = LogisticRegression(solver='liblinear', random_state=0)   # fit the model logreg.fit(X_train, y_train)"
CHECKPOINT ASSIGN = logreg.predict(X_test) y_pred_test,0,execute_result,#Prediction on Xtest  y_pred_test = logreg.predict(X_test)  y_pred_test
logreg.predict_proba(X_test),0,execute_result,#using predict_proba gives the probability value for the target feature  logreg.predict_proba(X_test)
"logreg.predict_proba(X_test)[:,0]",0,execute_result,"#probability of getting no rain (0)  logreg.predict_proba(X_test)[:,0]"
"logreg.predict_proba(X_test)[:,1]",0,execute_result,"#probability of getting rain (1)  logreg.predict_proba(X_test)[:,1]"
"SETUP CHECKPOINT ASSIGN = accuracy_score(y_test,y_pred_test) print(.format(ASSIGN))",0,stream,"#Check accuracy with accuracy_score  from sklearn.metrics import accuracy_score  predict_test = accuracy_score(y_test,y_pred_test)  print(""Accuracy of model on test set :: {}"".format(predict_test))"
"SETUP CHECKPOINT ASSIGN = ASSIGN(y_test, y_pred_test) print(ASSIGN)",0,stream,"#Creating confusion matrix  from sklearn.metrics import confusion_matrix  confusion_matrix = confusion_matrix(y_test, y_pred_test) print(confusion_matrix)"
"SETUP CHECKPOINT print(classification_report(y_test, y_pred_test))",0,stream,"#Classification report from sklearn.metrics import classification_report  print(classification_report(y_test, y_pred_test))"
CHECKPOINT ASSIGN = logreg.predict(X_train) y_pred_train,0,execute_result,#Comparing train and test accuracy  y_pred_train = logreg.predict(X_train) y_pred_train
"CHECKPOINT ASSIGN = accuracy_score(y_train,y_pred_train) print(.format(ASSIGN))",0,stream,"#Check accuracy of our model with train set  predict_train = accuracy_score(y_train,y_pred_train) print(""Accuracy of our model on train set :: {}"".format(predict_train))"
"CHECKPOINT print(.format(logreg.score(X_test,y_test)))",0,stream,"#Overall Accuracy  print(""Accuracy of our model :: {}"".format(logreg.score(X_test,y_test)))"
"CHECKPOINT ASSIGN = LogisticRegression(solver='liblinear',C=100, random_state=0) ASSIGN.fit(X_train, y_train) ASSIGN = logreg100.predict(X_test) y_pred_test",0,execute_result,"#C=100  # instantiate the model logreg100 = LogisticRegression(solver='liblinear',C=100, random_state=0)   # fit the model logreg100.fit(X_train, y_train)  #Prediction on Xtest  y_pred_test = logreg100.predict(X_test)  y_pred_test"
"CHECKPOINT ASSIGN = accuracy_score(y_test,y_pred_test) print(.format(ASSIGN))",0,stream,"predict_test = accuracy_score(y_test,y_pred_test)  print(""Accuracy of model on test set :: {}"".format(predict_test))"
"CHECKPOINT print(.format(logreg100.score(X_test,y_test)))",0,stream,"#Overall Accuracy  print(""Accuracy of our model :: {}"".format(logreg100.score(X_test,y_test)))"
"SETUP CHECKPOINT ASSIGN = ASSIGN(y_test, y_pred_test) print(ASSIGN)",0,stream,"#Confusion matrix from sklearn.metrics import confusion_matrix  confusion_matrix = confusion_matrix(y_test, y_pred_test) print(confusion_matrix)"
"CHECKPOINT print(classification_report(y_test, y_pred_test))",0,stream,"#Classification report print(classification_report(y_test, y_pred_test))"
"CHECKPOINT ASSIGN = LogisticRegression(solver='liblinear',C=0.01, random_state=0) ASSIGN.fit(X_train, y_train) ASSIGN = logreg001.predict(X_test) y_pred_test",0,execute_result,"#Let's increase the regularization strength  #C=0.01  # instantiate the model logreg001 = LogisticRegression(solver='liblinear',C=0.01, random_state=0)   # fit the model logreg001.fit(X_train, y_train)  #Prediction on Xtest  y_pred_test = logreg001.predict(X_test)  y_pred_test"
"CHECKPOINT print(.format(logreg001.score(X_test,y_test)))",0,stream,"#Overall Accuracy  print(""Accuracy of our model :: {}"".format(logreg001.score(X_test,y_test)))"
"ASSIGN = logreg100.predict_proba(X_test)[:, 1] ASSIGN = logreg100.predict_proba(X_test)[:, 0]",0,not_existent,"# store the predicted probabilities for class 1 - Probability of rain  y_pred1 = logreg100.predict_proba(X_test)[:, 1] y_pred0 = logreg100.predict_proba(X_test)[:, 0]"
"plt.rcParams['font.size'] = 12 plt.hist(y_pred1, bins = 10) plt.hist(y_pred0, bins = 10) plt.title('Histogram of predicted probabilities') plt.xlim(0,1) plt.legend('upper left' , labels = ['Rain','No Rain']) plt.xlabel('Predicted probabilities') plt.ylabel('Frequency')",0,execute_result,"# plot histogram of predicted probabilities   # adjust the font size  plt.rcParams['font.size'] = 12   # plot histogram with 10 bins plt.hist(y_pred1, bins = 10) plt.hist(y_pred0, bins = 10)  # set the title of predicted probabilities plt.title('Histogram of predicted probabilities')   # set the x-axis limit plt.xlim(0,1)  #Set legend plt.legend('upper left' , labels = ['Rain','No Rain'])  # set the title plt.xlabel('Predicted probabilities') plt.ylabel('Frequency')"
"SETUP ASSIGN = pd.read_csv(""..path"") np.random.seed(0)",0,not_existent,"# modules we'll use import pandas as pd import numpy as np  # for Box-Cox Transformation from scipy import stats  # for min_max scaling from mlxtend.preprocessing import minmax_scaling  # plotting modules import seaborn as sns import matplotlib.pyplot as plt  # read in all our data kickstarters_2017 = pd.read_csv(""../input/kickstarter-projects/ks-projects-201801.csv"")  # set seed for reproducibility np.random.seed(0)"
"ASSIGN = np.random.exponential(size = 1000) ASSIGN = minmax_scaling(original_data, columns = [0]) ASSIGN=plt.subplots(1,2) sns.distplot(ASSIGN, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Scaled data"")",0,not_existent,"# generate 1000 data points randomly drawn from an exponential distribution original_data = np.random.exponential(size = 1000)  # mix-max scale the data between 0 and 1 scaled_data = minmax_scaling(original_data, columns = [0])  # plot both together to compare fig, ax=plt.subplots(1,2) sns.distplot(original_data, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(scaled_data, ax=ax[1]) ax[1].set_title(""Scaled data"")"
"ASSIGN = stats.boxcox(original_data) ASSIGN=plt.subplots(1,2) sns.distplot(original_data, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN[0], ax=ax[1]) ax[1].set_title(""Normalized data"")",0,not_existent,"# normalize the exponential data with boxcox normalized_data = stats.boxcox(original_data)  # plot both together to compare fig, ax=plt.subplots(1,2) sns.distplot(original_data, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(normalized_data[0], ax=ax[1]) ax[1].set_title(""Normalized data"")"
"ASSIGN = kickstarters_2017.usd_goal_real ASSIGN = minmax_scaling(usd_goal, columns = [0]) ASSIGN=plt.subplots(1,2) sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Scaled data"")",0,not_existent,"# select the usd_goal_real column usd_goal = kickstarters_2017.usd_goal_real  # scale the goals from 0 to 1 scaled_data = minmax_scaling(usd_goal, columns = [0])  # plot the original & scaled data together to compare fig, ax=plt.subplots(1,2) sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(scaled_data, ax=ax[1]) ax[1].set_title(""Scaled data"")"
"ASSIGN = kickstarters_2017.ASSIGN ASSIGN = minmax_scaling(goal, columns = [0]) ASSIGN=plt.subplots(1,2) sns.distplot(kickstarters_2017.ASSIGN, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Scaled data"")",0,not_existent,"# Your turn!   # We just scaled the ""usd_goal_real"" column. What about the ""goal"" column? goal = kickstarters_2017.goal goal_scaled = minmax_scaling(goal, columns = [0]) fig_goal, ax_goal=plt.subplots(1,2)  sns.distplot(kickstarters_2017.goal, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(goal_scaled, ax=ax[1]) ax[1].set_title(""Scaled data"")"
"ASSIGN = kickstarters_2017.usd_pledged_real > 0 ASSIGN = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges] ASSIGN = stats.boxcox(positive_pledges)[0] ASSIGN=plt.subplots(1,2) sns.distplot(ASSIGN, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Normalized data"")",0,not_existent,"# get the index of all positive pledges (Box-Cox only takes postive values) index_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0  # get only positive pledges (using their indexes) positive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]  # normalize the pledges (w/ Box-Cox) normalized_pledges = stats.boxcox(positive_pledges)[0]  # plot both together to compare fig, ax=plt.subplots(1,2) sns.distplot(positive_pledges, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(normalized_pledges, ax=ax[1]) ax[1].set_title(""Normalized data"")"
"ASSIGN = kickstarters_2017.pledged > 0 ASSIGN = kickstarters_2017.pledged.loc[index_of_positive] ASSIGN = stats.boxcox(positive_pledges_x)[0] ASSIGN=plt.subplots(1,2) sns.distplot(ASSIGN, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Normalized data"")",0,not_existent,"# Your turn!  # We looked as the usd_pledged_real column. What about the ""pledged"" column? Does it have the same info?  index_of_positive = kickstarters_2017.pledged > 0  # get only positive pledges (using their indexes) positive_pledges_x = kickstarters_2017.pledged.loc[index_of_positive]  # normalize the pledges (w/ Box-Cox) normalized_pledges_x = stats.boxcox(positive_pledges_x)[0]  # plot both together to compare fig, ax=plt.subplots(1,2) sns.distplot(positive_pledges_x, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(normalized_pledges_x, ax=ax[1]) ax[1].set_title(""Normalized data"")"
"SETUP CHECKPOINT print(os.listdir()) ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"") ASSIGN = test_final",0,not_existent,"from sklearn.impute import SimpleImputer import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split import math import os print(os.listdir(""../input"")) df = pd.read_csv(""../input/train.csv"") test_final = pd.read_csv(""../input/test.csv"") test_final_id = test_final"
"def Filterdataset (dataset): ASSIGN = ASSIGN.copy() ASSIGN['has_alley'] = df['Alley'].fillna(0).apply(lambda _: 0 if _ == 0 else 1) ASSIGN = ASSIGN.fillna(value= {'Alley':'No alley access'}) ASSIGN['has_BsmtQual'] = df['BsmtQual'].fillna(0).apply(lambda _: 0 if _ == 0 else 1) ASSIGN = ASSIGN.fillna(value= {'BsmtQual':'No Basement'}) ASSIGN['has_BsmtCond'] = df['BsmtCond'].fillna(0).apply(lambda _: 0 if _ == 0 else 1) ASSIGN = ASSIGN.fillna(value= {'BsmtCond':'No Basement'}) ASSIGN = ASSIGN - ASSIGN ASSIGN = ASSIGN - ASSIGN ASSIGN = (ASSIGN + ASSIGN + ASSIGN + ASSIGN).astype('float32') ASSIGN = SimpleImputer(missing_values=np.nan, strategy='most_frequent') ASSIGN = pd.get_dummies(ASSIGN, drop_first=True) ASSIGN = dataset ASSIGN = imp_mean.fit_transform(ASSIGN) ASSIGN = pd.DataFrame(data = ASSIGN, index = datasetc.index, columns = datasetc.columns) if 'Id' in ASSIGN.columns: ASSIGN = ASSIGN.drop(['Id'], axis=1) if 'SalePrice' in ASSIGN.columns: ASSIGN = ASSIGN.drop(['SalePrice'], axis=1) return dataset",1,not_existent,"def Filterdataset (dataset):         dataset = dataset.copy()          dataset['has_alley'] = df['Alley'].fillna(0).apply(lambda _: 0 if _ == 0 else 1)     dataset = dataset.fillna(value= {'Alley':'No alley access'})     dataset['has_BsmtQual'] = df['BsmtQual'].fillna(0).apply(lambda _: 0 if _ == 0 else 1)     dataset = dataset.fillna(value= {'BsmtQual':'No Basement'})     dataset['has_BsmtCond'] = df['BsmtCond'].fillna(0).apply(lambda _: 0 if _ == 0 else 1)     dataset = dataset.fillna(value= {'BsmtCond':'No Basement'})     dataset['Age'] = dataset['YrSold'] - dataset['YearBuilt']     dataset['AgeSinceRemode'] = dataset['YrSold'] - dataset['YearRemodAdd']     dataset['WholeArea'] = (dataset['GrLivArea'] + dataset['GarageArea'] + dataset['1stFlrSF'] + dataset['2ndFlrSF']).astype('float32')     #dataset = dataset.select_dtypes(include=['float64','int'])     imp_mean = SimpleImputer(missing_values=np.nan, strategy='most_frequent')     dataset = pd.get_dummies(dataset, drop_first=True)     datasetc = dataset     dataset = imp_mean.fit_transform(dataset)     dataset = pd.DataFrame(data = dataset, index = datasetc.index, columns = datasetc.columns)     if 'Id' in dataset.columns:         dataset = dataset.drop(['Id'], axis=1)     if 'SalePrice' in dataset.columns:         dataset = dataset.drop(['SalePrice'], axis=1)     return dataset"
"CHECKPOINT ASSIGN = df['SalePrice'] ASSIGN = df.drop('SalePrice',axis=1) X_train, X_test, y_train, y_test = train_test_split( ASSIGN, ASSIGN, test_size=0.10, random_state=0) ASSIGN = Filterdataset(ASSIGN) ASSIGN = Filterdataset(ASSIGN) ASSIGN = Filterdataset(ASSIGN) ASSIGN = [] for c in ASSIGN.ASSIGN: if c in ASSIGN.ASSIGN: if c in ASSIGN.ASSIGN: ASSIGN.append(c) ASSIGN = ASSIGN[columns] ASSIGN = ASSIGN[columns] ASSIGN = ASSIGN[columns] ASSIGN = RandomForestRegressor(max_depth=50, random_state=0, n_estimators=100) ASSIGN.fit(ASSIGN, np.log(y_train)) print(len(ASSIGN.ASSIGN), len(ASSIGN.ASSIGN), len(ASSIGN.ASSIGN))",1,not_existent,"y = df['SalePrice'] X = df.drop('SalePrice',axis=1)  X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.10, random_state=0)  X_train = Filterdataset(X_train) X_test = Filterdataset(X_test) test_final = Filterdataset(test_final) columns = [] for c in X_train.columns:     if c in X_test.columns:         if c in test_final.columns:             columns.append(c) X_train = X_train[columns] test_final = test_final[columns] X_test = X_test[columns] regr = RandomForestRegressor(max_depth=50, random_state=0, n_estimators=100) regr.fit(X_train, np.log(y_train)) print(len(X_train.columns), len(X_test.columns), len(test_final.columns))"
CHECKPOINT ASSIGN = regr.predict(X_train) ASSIGN = regr.predict(X_test) ASSIGN = np.exp(regr.predict(test_final)) print(ASSIGN),0,not_existent,y_pred_train = regr.predict(X_train) y_pred_test = regr.predict(X_test) y_pred_final = np.exp(regr.predict(test_final)) print(y_pred_final)
"def Mrmse(y_true,y_pred): ASSIGN = np.log(ASSIGN) ASSIGN = math.sqrt(mean_squared_error(y_true, y_pred)) return rmse",0,not_existent,"def Mrmse(y_true,y_pred):     y_true = np.log(y_true)     #y_pred = np.log(y_pred)     rmse = math.sqrt(mean_squared_error(y_true, y_pred))     return rmse"
"CHECKPOINT print(Mrmse(y_train,y_pred_train)) print(Mrmse(y_test,y_pred_test))",0,not_existent,"print(Mrmse(y_train,y_pred_train)) print(Mrmse(y_test,y_pred_test))"
"ASSIGN = pd.DataFrame({'Id': test_final_id['Id'], 'SalePrice': y_pred_final}) ASSIGN.to_csv('submission.csv', index=False)",0,not_existent,"my_submission = pd.DataFrame({'Id': test_final_id['Id'], 'SalePrice': y_pred_final}) # you could use any filename. We choose submission here my_submission.to_csv('submission.csv', index=False) #my_submission"
SETUP,0,not_existent,# import libraries import torch import numpy as np
"SETUP ASSIGN = 0 ASSIGN = 20 ASSIGN = 0.2 ASSIGN = transforms.ToTensor() ASSIGN = datasets.MNIST(root='data', train=True, ASSIGN=True, transform=transform) ASSIGN = datasets.MNIST(root='data', train=False, ASSIGN=True, transform=transform) ASSIGN = len(train_data) ASSIGN = list(range(num_train)) np.random.shuffle(ASSIGN) ASSIGN = int(np.floor(valid_size * num_train)) ASSIGN = indices[split:], indices[:split] ASSIGN = SubsetRandomSampler(train_idx) ASSIGN = SubsetRandomSampler(valid_idx) ASSIGN = torch.utils.data.DataLoader(train_data, batch_size=batch_size, ASSIGN=train_sampler, num_workers=num_workers) ASSIGN = torch.utils.data.DataLoader(train_data, batch_size=batch_size, ASSIGN=valid_sampler, num_workers=num_workers) ASSIGN = torch.utils.data.DataLoader(test_data, batch_size=batch_size, ASSIGN=ASSIGN)",1,not_existent,"from torchvision import datasets import torchvision.transforms as transforms from torch.utils.data.sampler import SubsetRandomSampler  # number of subprocesses to use for data loading num_workers = 0 # how many samples per batch to load batch_size = 20 # percentage of training set to use as validation valid_size = 0.2  # convert data to torch.FloatTensor transform = transforms.ToTensor()  # choose the training and test datasets train_data = datasets.MNIST(root='data', train=True,                                    download=True, transform=transform) test_data = datasets.MNIST(root='data', train=False,                                   download=True, transform=transform)  # obtain training indices that will be used for validation num_train = len(train_data) indices = list(range(num_train)) np.random.shuffle(indices) split = int(np.floor(valid_size * num_train)) train_idx, valid_idx = indices[split:], indices[:split]  # define samplers for obtaining training and validation batches train_sampler = SubsetRandomSampler(train_idx) valid_sampler = SubsetRandomSampler(valid_idx)  # prepare data loaders train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,     sampler=train_sampler, num_workers=num_workers) valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,      sampler=valid_sampler, num_workers=num_workers) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,      num_workers=num_workers)"
"SETUP ASSIGN = iter(train_loader) ASSIGN = dataiter.next() ASSIGN = ASSIGN.numpy() ASSIGN = plt.figure(figsize=(25, 4)) for idx in np.arange(20): ASSIGN = fig.add_subplot(2, 20path, idx+1, xticks=[], yticks=[]) ASSIGN.imshow(np.squeeze(ASSIGN[idx]), cmap='gray') ASSIGN.set_title(str(labels[idx].item()))",0,not_existent,"import matplotlib.pyplot as plt %matplotlib inline      # obtain one batch of training images dataiter = iter(train_loader) images, labels = dataiter.next() images = images.numpy()  # plot the images in the batch, along with the corresponding labels fig = plt.figure(figsize=(25, 4)) for idx in np.arange(20):     ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])     ax.imshow(np.squeeze(images[idx]), cmap='gray')     # print out the correct label for each image     # .item() gets the value contained in a Tensor     ax.set_title(str(labels[idx].item()))"
"ASSIGN = np.squeeze(images[1]) ASSIGN = plt.figure(figsize = (12,12)) ASSIGN = fig.add_subplot(111) ASSIGN.imshow(ASSIGN, cmap='gray') ASSIGN = img.shape ASSIGN = img.max()path for x in range(width): for y in range(height): ASSIGN = round(img[x][y],2) if img[x][y] !=0 else 0 ASSIGN.annotate(str(ASSIGN), xy=(y,x), ASSIGN='center', ASSIGN='center', ASSIGN='white' if img[x][y]<thresh else 'black')",0,not_existent,"img = np.squeeze(images[1])  fig = plt.figure(figsize = (12,12))  ax = fig.add_subplot(111) ax.imshow(img, cmap='gray') width, height = img.shape thresh = img.max()/2.5 for x in range(width):     for y in range(height):         val = round(img[x][y],2) if img[x][y] !=0 else 0         ax.annotate(str(val), xy=(y,x),                     horizontalalignment='center',                     verticalalignment='center',                     color='white' if img[x][y]<thresh else 'black')"
"SETUP CHECKPOINT class Net(nn.Module): def __init__(self): super(Net, self).__init__() ASSIGN = 512 ASSIGN = 512 self.fc1 = nn.Linear(28 * 28, ASSIGN) self.fc2 = nn.Linear(ASSIGN, ASSIGN) self.fc3 = nn.Linear(ASSIGN, 10) self.dropout = nn.Dropout(0.2) def forward(self, x): ASSIGN = ASSIGN.view(-1, 28 * 28) ASSIGN = F.relu(self.fc1(ASSIGN)) ASSIGN = self.dropout(ASSIGN) ASSIGN = F.relu(self.fc2(ASSIGN)) ASSIGN = self.dropout(ASSIGN) ASSIGN = self.fc3(ASSIGN) return x ASSIGN = Net() print(ASSIGN)",0,not_existent,"import torch.nn as nn import torch.nn.functional as F  # define the NN architecture class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         # number of hidden nodes in each layer (512)         hidden_1 = 512         hidden_2 = 512         # linear layer (784 -> hidden_1)         self.fc1 = nn.Linear(28 * 28, hidden_1)         # linear layer (n_hidden -> hidden_2)         self.fc2 = nn.Linear(hidden_1, hidden_2)         # linear layer (n_hidden -> 10)         self.fc3 = nn.Linear(hidden_2, 10)         # dropout layer (p=0.2)         # dropout prevents overfitting of data         self.dropout = nn.Dropout(0.2)      def forward(self, x):         # flatten image input         x = x.view(-1, 28 * 28)         # add hidden layer, with relu activation function         x = F.relu(self.fc1(x))         # add dropout layer         x = self.dropout(x)         # add hidden layer, with relu activation function         x = F.relu(self.fc2(x))         # add dropout layer         x = self.dropout(x)         # add output layer         x = self.fc3(x)         return x  # initialize the NN model = Net() print(model)"
"ASSIGN = nn.CrossEntropyLoss() ASSIGN = torch.optim.SGD(model.parameters(), lr=0.01)",0,not_existent,"# specify loss function (categorical cross-entropy) criterion = nn.CrossEntropyLoss()  # specify optimizer (stochastic gradient descent) and learning rate = 0.01 optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
"CHECKPOINT ASSIGN = 50 ASSIGN = np.Inf for epoch in range(ASSIGN): ASSIGN = 0.0 ASSIGN = 0.0 model.train() for data, target in train_loader: optimizer.zero_grad() ASSIGN = model(data) ASSIGN = criterion(output, target) ASSIGN.backward() optimizer.step() ASSIGN += ASSIGN.item()*data.size(0) model.eval() for data, target in valid_loader: ASSIGN = model(data) ASSIGN = criterion(output, target) ASSIGN += ASSIGN.item()*data.size(0) ASSIGN = train_losspath(train_loader.sampler) ASSIGN = valid_losspath(valid_loader.sampler) print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format( epoch+1, ASSIGN, valid_loss )) if ASSIGN <= ASSIGN: print('Validation ASSIGN decreased ({:.6f} --> {:.6f}). Saving model ...'.format( ASSIGN, ASSIGN)) torch.save(model.state_dict(), 'model.pt') ASSIGN = valid_loss",0,not_existent,"# number of epochs to train the model n_epochs = 50  # initialize tracker for minimum validation loss valid_loss_min = np.Inf # set initial ""min"" to infinity  for epoch in range(n_epochs):     # monitor training loss     train_loss = 0.0     valid_loss = 0.0          ###################     # train the model #     ###################     model.train() # prep model for training     for data, target in train_loader:         # clear the gradients of all optimized variables         optimizer.zero_grad()         # forward pass: compute predicted outputs by passing inputs to the model         output = model(data)         # calculate the loss         loss = criterion(output, target)         # backward pass: compute gradient of the loss with respect to model parameters         loss.backward()         # perform a single optimization step (parameter update)         optimizer.step()         # update running training loss         train_loss += loss.item()*data.size(0)              ######################         # validate the model #     ######################     model.eval() # prep model for evaluation     for data, target in valid_loader:         # forward pass: compute predicted outputs by passing inputs to the model         output = model(data)         # calculate the loss         loss = criterion(output, target)         # update running validation loss          valid_loss += loss.item()*data.size(0)              # print training/validation statistics      # calculate average loss over an epoch     train_loss = train_loss/len(train_loader.sampler)     valid_loss = valid_loss/len(valid_loader.sampler)          print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(         epoch+1,          train_loss,         valid_loss         ))          # save model if validation loss has decreased     if valid_loss <= valid_loss_min:         print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(         valid_loss_min,         valid_loss))         torch.save(model.state_dict(), 'model.pt')         valid_loss_min = valid_loss"
model.load_state_dict(torch.load('model.pt')),0,not_existent,model.load_state_dict(torch.load('model.pt'))
"CHECKPOINT ASSIGN = 0.0 ASSIGN = list(0. for i in range(10)) ASSIGN = list(0. for i in range(10)) model.eval() for data, target in test_loader: ASSIGN = model(data) ASSIGN = criterion(output, target) ASSIGN += ASSIGN.item()*data.size(0) ASSIGN = torch.max(output, 1) ASSIGN = np.squeeze(pred.eq(target.data.view_as(pred))) for i in range(len(target)): ASSIGN = target.data[i] ASSIGN[ASSIGN] += ASSIGN[i].item() ASSIGN[ASSIGN] += 1 ASSIGN = test_losspath(test_loader.sampler) print('Test Loss: {:.6f}\n'.format(ASSIGN)) for i in range(10): if ASSIGN[i] > 0: print('Test Accuracy of %5s: %2d%% (%2dpath%2d)' % ( str(i), 100 * ASSIGN[i] path[i], np.sum(ASSIGN[i]), np.sum(ASSIGN[i]))) else: print('Test Accuracy of %5s: Npath(no training examples)' % (classes[i])) print('\nTest Accuracy (Overall): %2d%% (%2dpath%2d)' % ( 100. * np.sum(ASSIGN) path(ASSIGN), np.sum(ASSIGN), np.sum(ASSIGN)))",0,not_existent,"# initialize lists to monitor test loss and accuracy test_loss = 0.0 class_correct = list(0. for i in range(10)) class_total = list(0. for i in range(10))  model.eval() # prep model for evaluation  for data, target in test_loader:     # forward pass: compute predicted outputs by passing inputs to the model     output = model(data)     # calculate the loss     loss = criterion(output, target)     # update test loss      test_loss += loss.item()*data.size(0)     # convert output probabilities to predicted class     _, pred = torch.max(output, 1)     # compare predictions to true label     correct = np.squeeze(pred.eq(target.data.view_as(pred)))     # calculate test accuracy for each object class     for i in range(len(target)):         label = target.data[i]         class_correct[label] += correct[i].item()         class_total[label] += 1  # calculate and print avg test loss test_loss = test_loss/len(test_loader.sampler) print('Test Loss: {:.6f}\n'.format(test_loss))  for i in range(10):     if class_total[i] > 0:         print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (             str(i), 100 * class_correct[i] / class_total[i],             np.sum(class_correct[i]), np.sum(class_total[i])))     else:         print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))  print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (     100. * np.sum(class_correct) / np.sum(class_total),     np.sum(class_correct), np.sum(class_total)))"
"ASSIGN = iter(test_loader) ASSIGN = dataiter.next() ASSIGN = model(images) ASSIGN = torch.max(output, 1) ASSIGN = ASSIGN.numpy() ASSIGN = plt.figure(figsize=(25, 4)) for idx in np.arange(20): ASSIGN = fig.add_subplot(2, 20path, idx+1, xticks=[], yticks=[]) ASSIGN.imshow(np.squeeze(ASSIGN[idx]), cmap='gray') ASSIGN.set_title(""{} ({})"".format(str(preds[idx].item()), str(labels[idx].item())), ASSIGN=(""green"" if preds[idx]==labels[idx] else ""red""))",0,not_existent,"# obtain one batch of test images dataiter = iter(test_loader) images, labels = dataiter.next()  # get sample outputs output = model(images) # convert output probabilities to predicted class _, preds = torch.max(output, 1) # prep images for display images = images.numpy()  # plot the images in the batch, along with predicted and true labels fig = plt.figure(figsize=(25, 4)) for idx in np.arange(20):     ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])     ax.imshow(np.squeeze(images[idx]), cmap='gray')     ax.set_title(""{} ({})"".format(str(preds[idx].item()), str(labels[idx].item())),                  color=(""green"" if preds[idx]==labels[idx] else ""red""))"
SETUP np.random.seed(0),0,not_existent,# modules we'll use import pandas as pd import numpy as np  # helpful modules import fuzzywuzzy from fuzzywuzzy import process import chardet  # set seed for reproducibility np.random.seed(0)
"CHECKPOINT with open(""..path(30-November-2017).csv"", 'rb') as rawdata: ASSIGN = chardet.detect(rawdata.read(100000)) print(ASSIGN)",0,not_existent,"# look at the first ten thousand bytes to guess the character encoding with open(""../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv"", 'rb') as rawdata:     result = chardet.detect(rawdata.read(100000))  # check what the character encoding might be print(result)"
"ASSIGN = pd.read_csv(""..path(30-November-2017).csv"", ASSIGN='Windows-1252')",0,not_existent,"# read in our dat suicide_attacks = pd.read_csv(""../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv"",                                encoding='Windows-1252')"
suicide_attacks.head(),0,not_existent,suicide_attacks.head()
CHECKPOINT ASSIGN = suicide_attacks['City'].unique() ASSIGN.sort() cities,1,not_existent,# get all the unique values in the 'City' column cities = suicide_attacks['City'].unique()  # sort them alphabetically and then take a closer look cities.sort() cities
suicide_attacks['City'] = suicide_attacks['City'].str.lower() suicide_attacks['City'] = suicide_attacks['City'].str.strip(),1,not_existent,# convert to lower case suicide_attacks['City'] = suicide_attacks['City'].str.lower() # remove trailing white spaces suicide_attacks['City'] = suicide_attacks['City'].str.strip()
CHECKPOINT ASSIGN = suicide_attacks['Province'].unique() ASSIGN.sort() provinces,1,not_existent,"# Your turn! Take a look at all the unique values in the ""Province"" column.  # Then convert the column to lowercase and remove any trailing white spaces  # get all the unique values in the 'Province' column provinces = suicide_attacks['Province'].unique()  # sort them alphabetically and then take a closer look provinces.sort() provinces"
CHECKPOINT suicide_attacks['Province'] = suicide_attacks['Province'].str.lower() suicide_attacks['Province'] = suicide_attacks['Province'].str.strip() ASSIGN = suicide_attacks['Province'].unique() ASSIGN.sort() provinces,1,not_existent,# convert to lower case suicide_attacks['Province'] = suicide_attacks['Province'].str.lower() # remove trailing white spaces suicide_attacks['Province'] = suicide_attacks['Province'].str.strip()  # get all the unique values in the 'City' column provinces = suicide_attacks['Province'].unique()  # sort them alphabetically and then take a closer look provinces.sort() provinces
"CHECKPOINT ASSIGN = fuzzywuzzy.process.extract(""d.i khan"", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio) matches",1,not_existent,"# get the top 10 closest matches to ""d.i khan"" matches = fuzzywuzzy.process.extract(""d.i khan"", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)  # take a look at them matches"
"CHECKPOINT def replace_matches_in_column(df, column, string_to_match, min_ratio = 90): ASSIGN = df[column].unique() ASSIGN = fuzzywuzzy.process.extract(string_to_match, strings, ASSIGN=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio) ASSIGN = [matches[0] for matches in matches if matches[1] >= min_ratio] ASSIGN = df[column].isin(close_matches) df.loc[ASSIGN, column] = string_to_match print()",1,not_existent,"# function to replace rows in the provided column of the provided dataframe # that match the provided string above the provided ratio with the provided string def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):     # get a list of unique strings     strings = df[column].unique()          # get the top 10 closest matches to our input string     matches = fuzzywuzzy.process.extract(string_to_match, strings,                                           limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)      # only get matches with a ratio > 90     close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]      # get the rows of all the close matches in our dataframe     rows_with_matches = df[column].isin(close_matches)      # replace all rows with close matches with the input matches      df.loc[rows_with_matches, column] = string_to_match          # let us know the function's done     print(""All done!"")"
"replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=""d.i khan"")",1,not_existent,"# use the function we just wrote to replace close matches to ""d.i khan"" with ""d.i khan"" replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=""d.i khan"")"
"CHECKPOINT ASSIGN = fuzzywuzzy.process.extract(""kuram agency"", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio) matches",1,not_existent,"# Your turn! It looks like 'kuram agency' and 'kurram agency' should # be the same city. Correct the dataframe so that they are.  # get the top 10 closest matches to ""d.i khan"" matches = fuzzywuzzy.process.extract(""kuram agency"", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)  # take a look at them matches"
"replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=""kuram agency"", min_ratio = 94)",1,not_existent,"replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=""kuram agency"", min_ratio = 94)"
SETUP,0,not_existent,"import numpy as np
 import pandas as pd
 import matplotlib.pyplot as plt
"
"ASSIGN = pd.read_csv('path', index_col=""ID"")",0,not_existent,"train = pd.read_csv('/kaggle/input/cs-challenge/training_set.csv', index_col=""ID"")"
ASSIGN = ASSIGN.dropna(axis=1),1,not_existent,train = train.dropna(axis=1)
ASSIGN = ASSIGN = train.columns.to_list() ASSIGN = [x for x in column_list if x.find('_max') == -1 and x.find('_min') == -1 and x.find('_c') == -1],1,not_existent,"column_list = column_list = train.columns.to_list()
 non_redundant_cols = [x for x in column_list if x.find('_max') == -1 and x.find('_min') == -1 and x.find('_c') == -1]"
CHECKPOINT non_redundant_cols,0,execute_result,non_redundant_cols
ASSIGN=train,1,not_existent,"#train_non_re = train[non_redundant_cols]
 train_non_re=train"
"def ind_max(l): M=l[0] ASSIGN=0 for i in range(1,len(l)): if l[i]>M: M=l[i] ASSIGN=i return ind",1,not_existent,"def ind_max(l):
     M=l[0]
     ind=0
     for i in range(1,len(l)):
         if l[i]>M:
             M=l[i]
             ind=i
     return ind"
"CHECKPOINT ASSIGN=[-5+ipath(21)] ASSIGN.remove(0.0) ASSIGN=[] for col in column_list: if col != 'MAC_CODE': ASSIGN=[] for p in ASSIGN: if (p%1==0 or not any(train[col]<0)) and (p>0 or not any(train[col]==0)): ASSIGN.append(abs(train_non_re['TARGET'].ASSIGN(train_non_re[col]**p))) else: ASSIGN.append(0) ASSIGN=pows[ind_max(corr)] ASSIGN.append(ASSIGN) train_non_re[col] = np.power(train_non_re[col], ASSIGN) res",1,execute_result,"#squared_cols = []
 pows=[-5+i/2 for i in range(21)]
 pows.remove(0.0)
 res=[]
 #for col in non_redundant_cols:
 for col in column_list:
     if col != 'MAC_CODE':
         corr=[]
         for p in pows:
             if (p%1==0 or not any(train[col]<0)) and (p>0 or not any(train[col]==0)):                
                     corr.append(abs(train_non_re['TARGET'].corr(train_non_re[col]**p)))
             else:
                 corr.append(0)
         p=pows[ind_max(corr)]
         res.append(p)
         train_non_re[col] = np.power(train_non_re[col], p)
 res
             
             
             #cor1 = abs(train_non_re['TARGET'].corr(train_non_re[col]))
             #cor2 = abs(train_non_re['TARGET'].corr(train_non_re[col]**2))
             #if(cor2 > cor1):
                 #train_non_re[col] = np.power(train_non_re[col], 2)
                 #squared_cols.append(col)
         "
"SETUP ASSIGN = ColumnTransformer([ ('MAC_CODE', OneHotEncoder(dtype='int'),['MAC_CODE'])], ASSIGN = StandardScaler()) ASSIGN = RidgeCV(cv=5) ASSIGN = Pipeline([('col_transformer', col_transformer),('reg', reg)], verbose=True)",1,not_existent,"from sklearn.preprocessing import OneHotEncoder, StandardScaler
 from sklearn.linear_model import RidgeCV
 from sklearn.pipeline import Pipeline
 from sklearn.compose import ColumnTransformer
 
 
 col_transformer = ColumnTransformer([
     ('MAC_CODE', OneHotEncoder(dtype='int'),['MAC_CODE'])],
     remainder = StandardScaler())
 
 reg = RidgeCV(cv=5)
 
 pipe = Pipeline([('col_transformer', col_transformer),('reg', reg)], verbose=True)
"
CHECKPOINT train_non_re,0,execute_result,train_non_re
"SETUP Xtr, Xte, ytr,  yte = train_test_split(train_non_re.drop('TARGET', axis=1), train_non_re['TARGET'], test_size=0.2) pipe.fit(Xtr,ytr)",1,stream,"from sklearn.model_selection import train_test_split
 
 Xtr, Xte, ytr,  yte = train_test_split(train_non_re.drop('TARGET', axis=1), train_non_re['TARGET'], test_size=0.2)
 
 pipe.fit(Xtr,ytr)"
"SETUP mean_absolute_error(yte, pipe.predict(Xte))",0,execute_result,"from sklearn.metrics import mean_absolute_error
 
 mean_absolute_error(yte, pipe.predict(Xte))"
"pipe.fit(train_non_re.drop('TARGET', axis=1),train_non_re['TARGET'])",0,stream,"pipe.fit(train_non_re.drop('TARGET', axis=1),train_non_re['TARGET'])"
"ASSIGN = pd.read_csv('path', index_col=""ID"") ASSIGN = ASSIGN[[x for x in column_list if x != 'TARGET']] for col in squared_cols: ASSIGN = np.power(ASSIGN,2) ASSIGN = pipe.ASSIGN(test)",1,error,"test = pd.read_csv('/kaggle/input/cs-challenge/test_set.csv', index_col=""ID"")
 #test = test[[x for x in non_redundant_cols if x != 'TARGET']]
 test = test[[x for x in column_list if x != 'TARGET']]
 
 for col in squared_cols:
     test[col] = np.power(test[col],2)
 
 predict = pipe.predict(test)"
"ASSIGN = predict test['TARGET'].to_csv(""squared_ridge.csv"")",0,error,"test['TARGET'] = predict
 test['TARGET'].to_csv(""squared_ridge.csv"")"
"SETUP CHECKPOINT sns.set_style(""darkgrid"") for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed
 # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
 # For example, here's several helpful packages to load in 
 
 # Important imports for the analysis of the dataset
 import pandas as pd
 import numpy as np
 import matplotlib.pyplot as plt
 import seaborn as sns
 sns.set_style(""darkgrid"")
 
 # Input data files are available in the ""../input/"" directory.
 # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
 
 import os
 for dirname, _, filenames in os.walk('/kaggle/input'):
     for filename in filenames:
         print(os.path.join(dirname, filename))
 
 # Any results you write to the current directory are saved as output."
"ASSIGN = pd.read_csv(""path"") ASSIGN.head(8)",0,execute_result,"# Create the dataframe and check the first 8 rows
 app_df = pd.read_csv(""/kaggle/input/17k-apple-app-store-strategy-games/appstore_games.csv"")
 app_df.head(8)"
"ASSIGN = app_df.drop(columns=['URL', 'Subtitle', 'Icon URL'])",1,not_existent,"# Dropping columns that I will not use for this analysis
 app_df_cut = app_df.drop(columns=['URL', 'Subtitle', 'Icon URL'])"
app_df_cut.info(),0,stream,app_df_cut.info()
"ASSIGN = ASSIGN.sort_values(by=""User Rating Count"", ascending=False) ASSIGN.head(5)",1,execute_result,"# Most reviewed app
 #app_df_cut.iloc[app_df_cut[""User Rating Count""].idxmax()]
 
 # A better way of seeing the most reviwed apps 
 app_df_cut = app_df_cut.sort_values(by=""User Rating Count"", ascending=False)
 app_df_cut.head(5)"
"app_df_cut.loc[(app_df_cut[""User Rating Count""].isnull()) | (app_df_cut[""Average User Rating""].isnull()), [""Average User Rating"", ""User Rating Count""]] = 0",1,not_existent,"# Get the columns ""User Rating Count"" and ""Average User Rating"" where they are both equal to NaN and set the
 # values to 0.
 app_df_cut.loc[(app_df_cut[""User Rating Count""].isnull()) | (app_df_cut[""Average User Rating""].isnull()),
                [""Average User Rating"", ""User Rating Count""]] = 0"
"app_df_cut.loc[(app_df_cut[""User Rating Count""].isnull()) | (app_df_cut[""Average User Rating""].isnull())]",0,execute_result,"# Check if there are any other missing values in those columns
 app_df_cut.loc[(app_df_cut[""User Rating Count""].isnull()) | (app_df_cut[""Average User Rating""].isnull())]"
"app_df_cut.loc[app_df_cut[""In-app Purchases""].isnull(), ""In-app Purchases""] = 0",1,not_existent,"# Get the column ""In-app Purchases"" where the value is NaN and set it to zero
 app_df_cut.loc[app_df_cut[""In-app Purchases""].isnull(),
                ""In-app Purchases""] = 0"
"app_df_cut.loc[app_df_cut[""In-app Purchases""].isnull()]",0,execute_result,"# Check if there are any NaN value in the ""In-app Purchases"" column
 app_df_cut.loc[app_df_cut[""In-app Purchases""].isnull()]"
"app_df_cut.loc[(app_df_cut[""ID""] == 0) | (app_df_cut[""ID""].isnull()), ""ID""]",0,execute_result,"# Check if there are missing or 0 ID's
 app_df_cut.loc[(app_df_cut[""ID""] == 0) | (app_df_cut[""ID""].isnull()),
               ""ID""]"
"len(app_df_cut[""ID""]) - len(app_df_cut[""ID""].unique())",0,execute_result,"# Check for duplicates in the ID column
 len(app_df_cut[""ID""]) - len(app_df_cut[""ID""].unique())
 
 # The number of unique values is lower than the total amount of ID's, therefore there are duplicates among them."
"CHECKPOINT app_df_cut.drop_duplicates(subset=""ID"", inplace=True) app_df_cut.shape",1,execute_result,"# Drop every duplicate ID row
 app_df_cut.drop_duplicates(subset=""ID"", inplace=True)
 app_df_cut.shape"
"app_df_cut[(app_df_cut[""Size""].isnull()) | (app_df_cut['Size'] == 0)]",0,execute_result,"# Check if there are null values in the Size column
 app_df_cut[(app_df_cut[""Size""].isnull()) | (app_df_cut['Size'] == 0)]"
"app_df_cut.drop([16782], axis=0, inplace=True)",1,not_existent,"# Drop the only row in which the game has no size
 app_df_cut.drop([16782], axis=0, inplace=True)"
"app_df_cut[""Size""] = round(app_df_cut[""Size""]path) app_df_cut.head(5)",1,execute_result,"# Convert the size to MB
 app_df_cut[""Size""] = round(app_df_cut[""Size""]/1000000)
 app_df_cut.head(5)"
"ASSIGN = ASSIGN.drop(ASSIGN.loc[ASSIGN[""Price""].isnull()].index)",1,not_existent,"# Drop the row with NaN values in the ""Price"" column
 app_df_cut = app_df_cut.drop(app_df_cut.loc[app_df_cut[""Price""].isnull()].index)"
"app_df_cut.loc[app_df_cut[""Price""].isnull()]",0,execute_result,"# Check if there are any null values on the price column
 app_df_cut.loc[app_df_cut[""Price""].isnull()]"
"ASSIGN = ASSIGN.drop(ASSIGN.loc[ASSIGN[""Languages""].isnull()].index)",1,not_existent,"# Drop the rows with NaN values in the ""Languages"" column
 app_df_cut = app_df_cut.drop(app_df_cut.loc[app_df_cut[""Languages""].isnull()].index)"
"app_df_cut.loc[app_df_cut[""Languages""].isnull()]",0,execute_result,"# Check if there are any null values on the ""Languages"" column
 app_df_cut.loc[app_df_cut[""Languages""].isnull()]"
"app_df_cut.to_csv(""app_df_clean.csv"", index=False)",0,not_existent,"app_df_cut.to_csv(""app_df_clean.csv"", index=False)"
"ASSIGN = pd.read_csv(""ASSIGN.csv"") ASSIGN.head()",0,execute_result,"app_df_clean = pd.read_csv(""app_df_clean.csv"")
 app_df_clean.head()"
"app_df_clean[""Original Release Date""] = pd.to_datetime(app_df_clean[""Original Release Date""]) app_df_clean[""Current Version Release Date""] = pd.to_datetime(app_df_clean[""Current Version Release Date""])",1,not_existent,"# Transform the the string dates into datetime objects
 app_df_clean[""Original Release Date""] = pd.to_datetime(app_df_clean[""Original Release Date""])
 app_df_clean[""Current Version Release Date""] = pd.to_datetime(app_df_clean[""Current Version Release Date""])"
app_df_clean.info(),0,stream,app_df_clean.info()
"plt.figure(figsize=(16,10)) ASSIGN = app_df_clean[""Original Release Date""].apply(lambda date: date.year) ASSIGN = app_df_clean[""Size""] ASSIGN = sns.color_palette(""muted"") ASSIGN = sns.swarmplot(x=years, y=ASSIGN, palette=palette) ASSIGN.set_ylabel(""Size (in MB)"", fontsize=16) ASSIGN.set_xlabel(""Original Release Date"", fontsize=16) ASSIGN.set_title(""Time Evolution of the Apps' Sizes"", fontsize=20) plt.show()",0,display_data,"# Make the figure
 plt.figure(figsize=(16,10))
 
 # Variables
 years = app_df_clean[""Original Release Date""].apply(lambda date: date.year)
 size = app_df_clean[""Size""]
 
 # Plot a swarmplot
 palette = sns.color_palette(""muted"")
 size = sns.swarmplot(x=years, y=size, palette=palette)
 size.set_ylabel(""Size (in MB)"", fontsize=16)
 size.set_xlabel(""Original Release Date"", fontsize=16)
 size.set_title(""Time Evolution of the Apps' Sizes"", fontsize=20)
 plt.show()"
"plt.figure(figsize=(16,10)) ASSIGN = sns.color_palette(""inferno_r"") ASSIGN = sns.countplot(x=years, data=app_df_clean, palette=palette1) ASSIGN.set_xlabel(""Year of Release"", fontsize=16) ASSIGN.set_ylabel(""Amount"", fontsize=16) ASSIGN.set_title(""Quantity of Apps per Year"", fontsize=20) for p in ASSIGN.patches: ASSIGN.annotate(""{}"".format(p.get_height()), (p.get_x() + p.get_width() path, p.get_height() + 40), ASSIGN=""center"", ha=""center"", fontsize=16)",0,display_data,"# Make the figure
 plt.figure(figsize=(16,10))
 
 # Plot a countplot
 palette1 = sns.color_palette(""inferno_r"")
 apps_per_year = sns.countplot(x=years, data=app_df_clean, palette=palette1)
 apps_per_year.set_xlabel(""Year of Release"", fontsize=16)
 apps_per_year.set_ylabel(""Amount"", fontsize=16)
 apps_per_year.set_title(""Quantity of Apps per Year"", fontsize=20)
 
 # Write the height of each bar on top of them
 for p in apps_per_year.patches:
     apps_per_year.annotate(""{}"".format(p.get_height()),
                           (p.get_x() + p.get_width() / 2, p.get_height() + 40),
                           va=""center"", ha=""center"", fontsize=16)"
"ASSIGN = [year for year in range(2014,2019)] for year in ASSIGN: ASSIGN = app_df_clean[""Original Release Date""].apply(lambda date: (date.year == year) & (date.month >= 8)).sum() ASSIGN = app_df_clean[""Original Release Date""].apply(lambda date: date.year == year).sum() print(""In {year}, {percentage}% games were produced from August to December."" .format(year=year, ASSIGN=round((from_Augustpath)*100, 1)))",1,stream,"#Make a list of years from 2014 to 2018
 years_lst = [year for year in range(2014,2019)]
 
 #For loop to get a picture of the amount of games produced from August to December
 for year in years_lst:
     from_August = app_df_clean[""Original Release Date""].apply(lambda date: (date.year == year) & (date.month >= 8)).sum()
     total = app_df_clean[""Original Release Date""].apply(lambda date: date.year == year).sum()
     print(""In {year}, {percentage}% games were produced from August to December.""
           .format(year=year,
                   percentage=round((from_August/total)*100, 1)))"
"plt.figure(figsize=(16,10)) ASSIGN = app_df_clean[""Price""] ASSIGN = sns.light_palette(""green"", reverse=True) ASSIGN = sns.countplot(x=price, palette=palette2) ASSIGN.set_xlabel(""Price (in US dollars)"", fontsize=16) ASSIGN.set_xticklabels(ASSIGN.get_xticklabels(), fontsize=12, rotation=45) ASSIGN.set_ylabel(""Amount"", fontsize=16) ASSIGN.set_title(""Quantity of Each App per Price"", fontsize=20) for p in ASSIGN.patches: price_vis.annotate(""{:.0f}"".format(p.get_height()), # Text that will appear on the screen (p.get_x() + p.get_width() path+ 0.1, p.get_height()), ASSIGN='center', va='center', fontsize=14, color='black', xytext=(0, 10), ASSIGN='offset points')",0,display_data,"# Make the figure
 plt.figure(figsize=(16,10))
 
 # Variables
 price = app_df_clean[""Price""]
 
 # Plot a Countplot
 palette2 = sns.light_palette(""green"", reverse=True)
 price_vis = sns.countplot(x=price, palette=palette2)
 price_vis.set_xlabel(""Price (in US dollars)"", fontsize=16)
 price_vis.set_xticklabels(price_vis.get_xticklabels(), fontsize=12, rotation=45)
 price_vis.set_ylabel(""Amount"", fontsize=16)
 price_vis.set_title(""Quantity of Each App per Price"", fontsize=20)
 
 # Write the height of the bars on top
 for p in price_vis.patches:
     price_vis.annotate(""{:.0f}"".format(p.get_height()), # Text that will appear on the screen
                        (p.get_x() + p.get_width() / 2 + 0.1, p.get_height()), # (x, y) has to be a tuple
                        ha='center', va='center', fontsize=14, color='black', xytext=(0, 10), # Customizations
                        textcoords='offset points')"
"plt.figure(figsize=(16,10)) ASSIGN = app_df_clean[""In-app Purchases""].str.split("","").apply(lambda lst: len(lst)) ASSIGN = sns.color_palette(""BuGn_r"", 23) ASSIGN = sns.stripplot(x=price, y=in_app_purchases, palette=palette3) ASSIGN.set_xlabel(""Game Price (in US dollars)"", fontsize=16) ASSIGN.set_xticklabels(ASSIGN.get_xticklabels(), fontsize=12, rotation=45) ASSIGN.set_ylabel(""In-app Purchases Available"", fontsize=16) ASSIGN.set_title(""Quantity of In-app Purchases per Game Price"", fontsize=20) plt.show()",0,display_data,"# Make the figure
 plt.figure(figsize=(16,10))
 
 # Variables
 in_app_purchases = app_df_clean[""In-app Purchases""].str.split("","").apply(lambda lst: len(lst))
 
 # Plot a stripplot
 palette3 = sns.color_palette(""BuGn_r"", 23)
 in_app_purchases_vis = sns.stripplot(x=price, y=in_app_purchases, palette=palette3)
 in_app_purchases_vis.set_xlabel(""Game Price (in US dollars)"", fontsize=16)
 in_app_purchases_vis.set_xticklabels(in_app_purchases_vis.get_xticklabels(), fontsize=12, rotation=45)
 in_app_purchases_vis.set_ylabel(""In-app Purchases Available"", fontsize=16)
 in_app_purchases_vis.set_title(""Quantity of In-app Purchases per Game Price"", fontsize=20)
 plt.show()"
"plt.figure(figsize=(16,10)) ASSIGN = sns.color_palette(""BuPu_r"") ASSIGN = sns.countplot(app_df_clean.iloc[:200][""Price""], palette=palette4) ASSIGN.set_xlabel(""Price (in US dollars)"", fontsize=16) ASSIGN.set_xticklabels(ASSIGN.get_xticklabels(), fontsize=12) ASSIGN.set_ylabel(""Amount"", fontsize=16) ASSIGN.set_title(""Quantity of Each App per Price"", fontsize=20) for p in ASSIGN.patches: ASSIGN.annotate(""{:.0f}"".format(p.get_height()), (p.get_x() + p.get_width() path, p.get_height()), ASSIGN='center', va='center', fontsize=14, color='black', xytext=(0, 8), ASSIGN='offset points')",0,display_data,"# Plot a distribution of the top 200 apps by their price
 
 # Make the figure
 plt.figure(figsize=(16,10))
 
 # Plot a Countplot
 palette4 = sns.color_palette(""BuPu_r"")
 top_prices = sns.countplot(app_df_clean.iloc[:200][""Price""], palette=palette4)
 top_prices.set_xlabel(""Price (in US dollars)"", fontsize=16)
 top_prices.set_xticklabels(top_prices.get_xticklabels(), fontsize=12)
 top_prices.set_ylabel(""Amount"", fontsize=16)
 top_prices.set_title(""Quantity of Each App per Price"", fontsize=20)
 
 # Write the height of the bars on top
 for p in top_prices.patches:
     top_prices.annotate(""{:.0f}"".format(p.get_height()), 
                         (p.get_x() + p.get_width() / 2., p.get_height()),
                         ha='center', va='center', fontsize=14, color='black', xytext=(0, 8),
                         textcoords='offset points')"
"ASSIGN = app_df_clean[app_df_clean[""Price""] > 0] ASSIGN = len(paid) ASSIGN = app_df_clean[app_df_clean[""Price""] == 0] ASSIGN = len(free) ASSIGN = plt.subplots(1, 2, figsize=(16,10)) ASSIGN = sns.countplot(x=""Average User Rating"", data=free, ax=axes[0]) ASSIGN.set_xlabel(""Average User Rating"", fontsize=16) ASSIGN.set_ylabel(""Amount"", fontsize=16) ASSIGN.set_title(""Free Apps"", fontsize=20) for p in ASSIGN.patches: ASSIGN.annotate(""{:.1f}%"".format(100 * (p.get_height()path)), (p.get_x() + p.get_width() path+ 0.1, p.get_height()), ASSIGN='center', va='center', fontsize=14, color='black', xytext=(0, 8), ASSIGN='offset points') ASSIGN = sns.countplot(x=""Average User Rating"", data=paid, ax=axes[1]) ASSIGN.set_xlabel(""Average User Rating"", fontsize=16) ASSIGN.set_ylabel("" "", fontsize=16) ASSIGN.set_title(""Paid Apps"", fontsize=20) for p in ASSIGN.patches: ASSIGN.annotate(""{:.1f}%"".format(100 * (p.get_height()path)), (p.get_x() + p.get_width() path+ 0.1, p.get_height()), ASSIGN='center', va='center', fontsize=14, color='black', xytext=(0, 8), ASSIGN='offset points')",0,display_data,"# Create the DataFrames needed
 paid = app_df_clean[app_df_clean[""Price""] > 0]
 total_paid = len(paid)
 free = app_df_clean[app_df_clean[""Price""] == 0]
 total_free = len(free)
 
 # Make the figure and the axes (1 row, 2 columns)
 fig, axes = plt.subplots(1, 2, figsize=(16,10))
 
 # Free apps countplot
 free_vis = sns.countplot(x=""Average User Rating"", data=free, ax=axes[0])
 free_vis.set_xlabel(""Average User Rating"", fontsize=16)
 free_vis.set_ylabel(""Amount"", fontsize=16)
 free_vis.set_title(""Free Apps"", fontsize=20)
 
 # Display the percentages on top of the bars
 for p in free_vis.patches:
      free_vis.annotate(""{:.1f}%"".format(100 * (p.get_height()/total_free)),
                        (p.get_x() + p.get_width() / 2 + 0.1, p.get_height()),
                         ha='center', va='center', fontsize=14, color='black', xytext=(0, 8),
                         textcoords='offset points')
     
 # Paid apps countplot
 paid_vis = sns.countplot(x=""Average User Rating"", data=paid, ax=axes[1])
 paid_vis.set_xlabel(""Average User Rating"", fontsize=16)
 paid_vis.set_ylabel("" "", fontsize=16)
 paid_vis.set_title(""Paid Apps"", fontsize=20)
 
 # Display the percentages on top of the bars
 for p in paid_vis.patches:
     paid_vis.annotate(""{:.1f}%"".format(100 * (p.get_height()/total_paid)),
                       (p.get_x() + p.get_width() / 2 + 0.1, p.get_height()),
                        ha='center', va='center', fontsize=14, color='black', xytext=(0, 8),
                        textcoords='offset points')"
"plt.figure(figsize=(16,10)) ASSIGN = sns.color_palette(""BuGn_r"") ASSIGN = sns.countplot(x=app_df_clean[""Age Rating""], order=[""4+"", ""9+"", ""12+"", ""17+""], palette=palette5) ASSIGN.set_xlabel(""Age Rating"", fontsize=16) ASSIGN.set_ylabel(""Amount"", fontsize=16) ASSIGN.set_title(""Amount of Games per Age Restriction"", fontsize=20) for p in ASSIGN.patches: ASSIGN.annotate(""{:.0f}"".format(p.get_height()), (p.get_x() + p.get_width() path, p.get_height()), ASSIGN='center', va='center', fontsize=14, color='black', xytext=(0, 8), ASSIGN='offset points')",0,display_data,"# Make the figure
 plt.figure(figsize=(16,10))
 
 # Make a countplot
 palette5 = sns.color_palette(""BuGn_r"")
 age_vis = sns.countplot(x=app_df_clean[""Age Rating""], order=[""4+"", ""9+"", ""12+"", ""17+""], palette=palette5)
 age_vis.set_xlabel(""Age Rating"", fontsize=16)
 age_vis.set_ylabel(""Amount"", fontsize=16)
 age_vis.set_title(""Amount of Games per Age Restriction"", fontsize=20)
 
 # Write the height of the bars on top
 for p in age_vis.patches:
     age_vis.annotate(""{:.0f}"".format(p.get_height()), 
                         (p.get_x() + p.get_width() / 2., p.get_height()),
                         ha='center', va='center', fontsize=14, color='black', xytext=(0, 8),
                         textcoords='offset points')"
"app_df_clean[""numLang""] = app_df_clean[""Languages""].apply(lambda x: len(x.split("","")))",1,not_existent,"# Create a new column that contains the amount of languages that app has available
 app_df_clean[""numLang""] = app_df_clean[""Languages""].apply(lambda x: len(x.split("","")))"
"plt.figure(figsize=(16,10)) ASSIGN = app_df_clean.loc[app_df_clean[""numLang""] <= 25, ""numLang""] ASSIGN = sns.color_palette(""PuBuGn_r"") ASSIGN = sns.countplot(x=lang, data=app_df_clean, palette=palette6) ASSIGN.set_xlabel(""Quantity of Languages"", fontsize=16) ASSIGN.set_ylabel(""Amount of Games"", fontsize=16) ASSIGN.set_title(""Quantity of Languages Available per Game"", fontsize=20) for p in ASSIGN.patches: ASSIGN.annotate(""{:.0f}"".format(p.get_height()), (p.get_x() + p.get_width() path+ .1, p.get_height()), ASSIGN='center', va='center', fontsize=12, color='black', xytext=(0, 12), ASSIGN='offset points')",0,display_data,"#Make the figure
 plt.figure(figsize=(16,10))
 
 #Variables
 lang = app_df_clean.loc[app_df_clean[""numLang""] <= 25, ""numLang""]
 
 #Plot a countplot
 palette6 = sns.color_palette(""PuBuGn_r"")
 numLang_vis = sns.countplot(x=lang, data=app_df_clean, palette=palette6)
 numLang_vis.set_xlabel(""Quantity of Languages"", fontsize=16)
 numLang_vis.set_ylabel(""Amount of Games"", fontsize=16)
 numLang_vis.set_title(""Quantity of Languages Available per Game"", fontsize=20)
 
 # Write the height of the bars on top
 for p in numLang_vis.patches:
     numLang_vis.annotate(""{:.0f}"".format(p.get_height()), 
                         (p.get_x() + p.get_width() / 2. + .1, p.get_height()),
                         ha='center', va='center', fontsize=12, color='black', xytext=(0, 12),
                         textcoords='offset points')"
"len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] == ""EN"")])",0,execute_result,"#Amount of games that have only the English language
 len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] == ""EN"")])"
"len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] != ""EN"")])",0,execute_result,"#Amount of games that have only one language and is not English
 len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] != ""EN"")])"
SETUP ASSIGN=pd.read_csv('..path'),0,not_existent,"import matplotlib.pyplot as plt
 df=pd.read_csv('../input/boston-housing-dataset/HousingData.csv')"
CHECKPOINT print(df),0,stream,print(df)
CHECKPOINT df.shape,0,execute_result, df.shape
CHECKPOINT df.dtypes,0,execute_result, df.dtypes
ASSIGN = ASSIGN.astype('float64') ASSIGN = ASSIGN.astype('float64'),1,not_existent,"df['RAD'] = df['RAD'].astype('float64')
 df['TAX'] = df['TAX'].astype('float64')"
df.isnull().any(),0,execute_result,df.isnull().any()
df.dropna(inplace=True),1,not_existent,df.dropna(inplace=True)
df['CHAS'].value_counts(dropna=False),0,execute_result,df['CHAS'].value_counts(dropna=False)
"CHECKPOINT ASSIGN = df[['CRIM','ZN','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV','CHAS']] print(ASSIGN)",1,stream,"df1 = df[['CRIM','ZN','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV','CHAS']]
 print(df1)"
ASSIGN = ASSIGN.dropna(),1,not_existent,df1 = df1.dropna()
"ASSIGN = df1.iloc[:, :-1].values ASSIGN = df1.iloc[:,13].values",1,not_existent,"X = df1.iloc[:, :-1].values
 y = df1.iloc[:,13].values"
CHECKPOINT df1.dtypes,0,execute_result,df1.dtypes
CHECKPOINT y.shape,0,execute_result,y.shape
"SETUP X_train,X_test,y_train,y_test =train_test_split(X,y,test_size =0.2,random_state =0)",1,not_existent,"from sklearn.model_selection import train_test_split
 X_train,X_test,y_train,y_test =train_test_split(X,y,test_size =0.2,random_state =0)"
CHECKPOINT y_train,0,execute_result,y_train
"SETUP ASSIGN = LinearRegression() ASSIGN.fit(X_train,y_train)",0,execute_result,"from sklearn.linear_model import LinearRegression
 regressor = LinearRegression()
 regressor.fit(X_train,y_train)"
ASSIGN =regressor.predict(X_test),0,not_existent,y_pred =regressor.predict(X_test)
"SETUP ASSIGN = np.append(arr=np.ones((394,1)).astype(int),values=ASSIGN,axis=1)",1,not_existent,"import statsmodels.formula.api as sm
 X = np.append(arr=np.ones((394,1)).astype(int),values=X,axis=1)"
CHECKPOINT print(X),0,stream,print(X)
SETUP,0,not_existent,import statsmodels.api as sm
"ASSIGN = X[:,:13] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,execute_result,"X_opt = X[:,:13] 
 regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()
 regressor_OLS.summary()"
"ASSIGN =X[:,[1,3,5,7,8,9,10,11,12]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,execute_result,"X_opt =X[:,[1,3,5,7,8,9,10,11,12]]
 regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()
 regressor_OLS.summary()"
"ASSIGN =X[:,[1,3,5,7,8,9,10,11]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,execute_result,"X_opt =X[:,[1,3,5,7,8,9,10,11]]
 regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()
 regressor_OLS.summary()"
"ASSIGN =X[:,[3,5,8,9,10,11]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,execute_result,"X_opt =X[:,[3,5,8,9,10,11]]
 regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()
 regressor_OLS.summary()"
"ASSIGN =X[:,[0,1,4,10,14]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,error,"X_opt =X[:,[0,1,4,10,14]]
 regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()
 regressor_OLS.summary()"
SETUP CHECKPOINT binder.bind(globals()) print(),0,stream,"# Set up feedack system
 from learntools.core import binder
 binder.bind(globals())
 from learntools.sql.ex1 import *
 print(""Setup Complete"")"
"SETUP ASSIGN = bigquery.Client() ASSIGN = client.dataset(""chicago_crime"", project=""bigquery-public-data"") ASSIGN = client.get_dataset(dataset_ref)",1,stream,"from google.cloud import bigquery
 
 # Create a ""Client"" object
 client = bigquery.Client()
 
 # Construct a reference to the ""chicago_crime"" dataset
 dataset_ref = client.dataset(""chicago_crime"", project=""bigquery-public-data"")
 
 # API request - fetch the dataset
 dataset = client.get_dataset(dataset_ref)"
ASSIGN = list(client.list_tables(dataset)),1,not_existent,"# Write the code you need here to figure out the answer
 tables = list(client.list_tables(dataset))"
"CHECKPOINT ASSIGN=client.get_table(dataset_ref.ASSIGN(""crime"")) table.schema",1,execute_result,"# Write the code to figure out the answer
 table=client.get_table(dataset_ref.table(""crime""))
 table.schema"
"client.list_rows(table, max_results=5).to_dataframe()",0,execute_result,"# Write the code here to explore the data so you can find the answer
 client.list_rows(table, max_results=5).to_dataframe()"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load  import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from sklearn import preprocessing import matplotlib.pyplot as plt import seaborn as sns  # Input data files are available in the read-only ""../input/"" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory  import os for dirname, _, filenames in os.walk('/kaggle/input'):     for filename in filenames:         print(os.path.join(dirname, filename))  # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All""  # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
X=pd.read_csv('path') ASSIGN=pd.read_csv('path'),0,not_existent,#read data files X=pd.read_csv('/kaggle/input/titanic/train.csv') test=pd.read_csv('/kaggle/input/titanic/test.csv')
X.head(),0,execute_result,X.head() 
test.head(),0,execute_result,test.head() 
"SLICE=np.nan ASSIGN=pd.concat([X,test])",1,not_existent,"test['Survived']=np.nan full=pd.concat([X,test])"
"CHECKPOINT def data_inv(df): print('Number of Persons: ',df.shape[0]) print('dataset variables: ',df.shape[1]) print('-'*20) print('dateset columns: \n') print(df.columns) print('-'*20) print('data-type of each column: \n') print(df.dtypes) print('-'*20) print('missing rows in each column: \n') ASSIGN=df.isnull().sum() print(ASSIGN[ASSIGN>0]) print('-'*20) print('Missing vaules %age vise:\n') print((100*(df.isnull().sum()path(df.index)))) print('-'*20) print('Pictorial Representation:') plt.figure(figsize=(8,6)) sns.heatmap(df.isnull(), yticklabels=False,cbar=False, cmap='viridis') plt.show() data_inv(full)",0,stream," def data_inv(df):     print('Number of Persons: ',df.shape[0])     print('dataset variables: ',df.shape[1])     print('-'*20)     print('dateset columns: \n')     print(df.columns)     print('-'*20)     print('data-type of each column: \n')     print(df.dtypes)     print('-'*20)     print('missing rows in each column: \n')     c=df.isnull().sum()     print(c[c>0])     print('-'*20)     print('Missing vaules %age vise:\n')     print((100*(df.isnull().sum()/len(df.index))))     print('-'*20)     print('Pictorial Representation:')     plt.figure(figsize=(8,6))     sns.heatmap(df.isnull(), yticklabels=False,cbar=False, cmap='viridis')     plt.show()    data_inv(full)#function call"
"sns.heatmap(full.corr(), annot = True)",0,execute_result,"sns.heatmap(full.corr(), annot = True)"
"SETUP SLICE=SLICE.fillna(mode(SLICE)) full['Fare'].fillna(full['Fare'].dropna().median(),inplace=True) ASSIGN = full.groupby(""Pclass"")['Age'].transform(lambda x: x.fillna(x.median())) full.isnull().sum()",1,execute_result,"#fillna from statistics import mode full['Embarked']=full['Embarked'].fillna(mode(full['Embarked']))  full['Fare'].fillna(full['Fare'].dropna().median(),inplace=True) full['Age'] = full.groupby(""Pclass"")['Age'].transform(lambda x: x.fillna(x.median())) full.isnull().sum()"
SLICE=SLICE+SLICE,1,not_existent,full['Fam']=full['Parch']+full['SibSp']
"ASSIGN=pd.get_dummies(data=ASSIGN,columns=['Sex','Embarked'],drop_first=True) ASSIGN.info()",1,stream,"full=pd.get_dummies(data=full,columns=['Sex','Embarked'],drop_first=True) full.info()"
"ASSIGN=ASSIGN.drop(['Cabin','Ticket','Name','Parch','SibSp'],axis=1)",1,not_existent,"full=full.drop(['Cabin','Ticket','Name','Parch','SibSp'],axis=1) "
preprocessing.StandardScaler().fit(full).transform(full.astype(float)),1,execute_result,#Data Standardization  preprocessing.StandardScaler().fit(full).transform(full.astype(float))
"ASSIGN = full[full['Survived'].isna()].drop(['Survived'], axis = 1) ASSIGN = full[full['Survived'].notna()] ASSIGN.info()",1,stream,"test = full[full['Survived'].isna()].drop(['Survived'], axis = 1) train = full[full['Survived'].notna()] train.info() "
"X=train[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']] ASSIGN=train[['Survived']].astype(np.int8)",1,not_existent,"   X=train[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']]  y=train[['Survived']].astype(np.int8)"
"SETUP X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)",1,not_existent,"from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)"
"SETUP ASSIGN=[] ASSIGN=[] for cols in range(50,55): for rows in range(3,5): ASSIGN=(cols,rows) MLPClassifierModel = MLPClassifier(activation='logistic', ASSIGN='lbfgs', ASSIGN=0.1 ,hidden_layer_sizes=hidden_layer,random_state=33) MLPClassifierModel.fit(X_train, y_train) ASSIGN = MLPClassifierModel.predict(X_test) ASSIGN.append(MLPClassifierModel.score(X_test, y_test)) ASSIGN.append(str(ASSIGN))",0,stream,"Scores=[] hidden_layer_sizes=[]   for cols in range(50,55):     for rows in range(3,5):         hidden_layer=(cols,rows)          from sklearn.neural_network import MLPClassifier         MLPClassifierModel = MLPClassifier(activation='logistic', # can be also identity , tanh,logistic , relu                                            solver='lbfgs',  # can be lbfgs also sgd , adam                                            alpha=0.1 ,hidden_layer_sizes=hidden_layer,random_state=33)         MLPClassifierModel.fit(X_train, y_train)          MLPClassifier_y_pred = MLPClassifierModel.predict(X_test)         Scores.append(MLPClassifierModel.score(X_test, y_test))         hidden_layer_sizes.append(str(hidden_layer))            "
"ASSIGN = pd.DataFrame({ 'hidden_layer': hidden_layer_sizes, 'Score': Scores}) ASSIGN.sort_values(by='Score', ascending=False )",1,execute_result,"models = pd.DataFrame({     'hidden_layer': hidden_layer_sizes,     'Score': Scores}) models.sort_values(by='Score', ascending=False )   "
"plt.plot(hidden_layer_sizes,Scores) plt.ylabel('Accuracy ') plt.xlabel('hidden_layer_sizes ') plt.tight_layout() plt.show()",0,display_data,"  plt.plot(hidden_layer_sizes,Scores) plt.ylabel('Accuracy ') plt.xlabel('hidden_layer_sizes ') plt.tight_layout() plt.show()"
"SETUP MLPClassifierModel = MLPClassifier(activation='logistic', ASSIGN='lbfgs', ASSIGN='adaptive', ASSIGN= False, ASSIGN=0.1 ,hidden_layer_sizes=(52, 3),random_state=33) MLPClassifierModel.fit(X_train, y_train) ASSIGN = MLPClassifierModel.predict(X_test) MLPClassifierModel.fit(X, y) ASSIGN= MLPClassifierModel.predict(test[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']])",0,stream,"from sklearn.neural_network import MLPClassifier MLPClassifierModel = MLPClassifier(activation='logistic', # can be also identity , tanh,logistic , relu                                    solver='lbfgs',  # can be lbfgs also sgd , adam                                    learning_rate='adaptive', # can be constant also invscaling , adaptive                                    early_stopping= False,                                    alpha=0.1 ,hidden_layer_sizes=(52, 3),random_state=33) MLPClassifierModel.fit(X_train, y_train)  MLPClassifier_y_pred = MLPClassifierModel.predict(X_test) MLPClassifierModel.fit(X, y) MLPClassifier_y_pred= MLPClassifierModel.predict(test[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']])"
"ASSIGN=test['PassengerId'] ASSIGN=pd.DataFrame({'PassengerId':Id,'Survived':MLPClassifier_y_pred}) ASSIGN.to_csv('submission.csv',index=False) ASSIGN.head()",0,execute_result,"Id=test['PassengerId'] sub_df=pd.DataFrame({'PassengerId':Id,'Survived':MLPClassifier_y_pred}) sub_df.to_csv('submission.csv',index=False) sub_df.head()"
SETUP,0,not_existent,"%matplotlib inline
"
SETUP ASSIGN = Image(filename='path') display(ASSIGN),0,error,"from IPython.display import Image 
 pil_img = Image(filename='/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/C/s/color_18_0100.png')
 
 display(pil_img)"
CHECKPOINT path,0,error,/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/C/a/color_0_0002.png
"CHECKPOINT SETUP ASSIGN = next(os.walk(""path"")) ASSIGN = len(files) file_count",0,error,"#TO_DO
 #1 prepare the X_train , X_test 
 
 import os
 
 path, dirs, files = next(os.walk(""/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/b""))
 file_count = len(files)
 
 file_count"
"CHECKPOINT for filename in glob.glob(os.path.join(directory_a, '*.png')): ASSIGN =cv2.imread(filename,0) print(ASSIGN.shape)",0,error,"for filename in glob.glob(os.path.join(directory_a, '*.png')):
     im1 =cv2.imread(filename,0)
     print(im1.shape)"
"SETUP def giveMeFeatures(image): ASSIGN = hog(image, orientations=8, pixels_per_cell=(16,16),cells_per_block=(4, 4),block_norm= 'L2') return res",1,not_existent,"from skimage.feature import hog
 def giveMeFeatures(image):
     res = hog(image, orientations=8, pixels_per_cell=(16,16),cells_per_block=(4, 4),block_norm= 'L2')
 #     =  hog(img, orientations=9, pixels_per_cell=(6, 6),cells_per_block=(2, 2),block_norm='L1', visualize=False,transform_sqrt=False,feature_vector=True)
     return res
 
 
     
"
"SETUP ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] for filename in glob.glob(os.path.join(ASSIGN, '*.png')): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN = giveMeFeatures(im1) ASSIGN.append(ASSIGN) ASSIGN.append(0) for filename in glob.glob(os.path.join(ASSIGN, '*.png')): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN = giveMeFeatures(im1) ASSIGN.append(ASSIGN) ASSIGN.append(1) ASSIGN = np.array(np.float32(ASSIGN)) ASSIGN = np.array(np.float32(ASSIGN)) ASSIGN = np.random.RandomState(321) ASSIGN = rand.permutation(len(X)) ASSIGN = ASSIGN[shuffle] ASSIGN = ASSIGN[shuffle]",1,not_existent,"import glob
 import cv2
 directory_a = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/a'
 directory_b = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/b'
 
 X = []
 y = []
 
 for filename in glob.glob(os.path.join(directory_a, '*.png')):
     im1 =cv2.imread(filename,0)
     im1 = cv2.resize(im1,(64,64))
     features = giveMeFeatures(im1)
     X.append(features)
     y.append(0)
 
 for filename in glob.glob(os.path.join(directory_b, '*.png')):
     im1 =cv2.imread(filename,0)
     im1 = cv2.resize(im1,(64,64))
     features = giveMeFeatures(im1)
     X.append(features)
     y.append(1)
     
 X = np.array(np.float32(X))
 y = np.array(np.float32(y))
 
 
 rand = np.random.RandomState(321)
 shuffle = rand.permutation(len(X))
 X = X[shuffle]
 y = y[shuffle]
     
     
"
"SETUP X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)",1,error,"from sklearn.model_selection import train_test_split
 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
"SETUP CHECKPOINT ASSIGN=svm.SVC() ASSIGN = {'C': [2,3,4,5,6,7,8,9,10,11,12], 'kernel': ['rbf']} ASSIGN = GridSearchCV(model, param_grid=params, n_jobs=-1) ASSIGN.fit(X_train,y_train) print(,ASSIGN.best_params_) ASSIGN=model1.predict(X_test)",0,error,"#With Hyper Parameters Tuning
 #2-3,SVM
 #importing modules
 from sklearn.model_selection import GridSearchCV
 from sklearn import svm
 #making the instance
 model=svm.SVC()
 #Hyper Parameters Set
 params = {'C': [2,3,4,5,6,7,8,9,10,11,12], 
           'kernel': ['rbf']}
 #Making models with hyper parameters sets
 model1 = GridSearchCV(model, param_grid=params, n_jobs=-1)
 #Learning
 model1.fit(X_train,y_train)
 #The best hyper parameters set
 print(""Best Hyper Parameters:\n"",model1.best_params_)
 #Prediction
 prediction=model1.predict(X_test)
"
"SETUP classification_report(y_test, prediction)",0,error,"from sklearn.metrics import classification_report,accuracy_score
 classification_report(y_test, prediction)"
"CHECKPOINT print(+str(accuracy_score(y_test, prediction)))",0,error,"print(""Accuracy: ""+str(accuracy_score(y_test, prediction)))
 # model1.score(X_test,y_test)"
"CHECKPOINT def testModel(path): ASSIGN =cv2.imread(path,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN=[] ASSIGN.append(giveMeFeatures(ASSIGN)) ASSIGN = np.array(np.float32(ASSIGN)) ASSIGN =model1.predict(features) if(ASSIGN[0]==0): return 'fist' else: return 'palm' return",0,not_existent,"def testModel(path):
     im1 =cv2.imread(path,0)
     im1 = cv2.resize(im1,(64,64))
     features=[]
     features.append(giveMeFeatures(im1))
     features = np.array(np.float32(features))    
     res =model1.predict(features)
     if(res[0]==0):
         return 'fist'
     else:
         return 'palm'
         
     return 
     "
testModel('path'),0,error,testModel('/kaggle/input/testdata2/palm2.jpg')
testModel('path'),0,error,testModel('/kaggle/input/testdata2/palm1.jpg')
SETUP,0,stream,"import pandas as pd
 import numpy as np
 import os
 import matplotlib.pyplot as plt 
 import cv2 as cv
 
 from keras.layers import Conv2D, Input, LeakyReLU, Dense, Activation, Flatten, Dropout, MaxPool2D
 from keras import models
 from keras.optimizers import Adam,RMSprop 
 from keras.preprocessing.image import ImageDataGenerator
 from keras.callbacks import ReduceLROnPlateau
 
 import pickle
 
 %matplotlib inline"
ASSIGN = models.Sequential(),0,not_existent,model = models.Sequential()
"model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1))) model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same')) model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu')) model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same')) model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu')) model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same')) model.add(Flatten()) model.add(Dense(units = 512 , activation = 'relu')) model.add(Dropout(0.2)) model.add(Dense(units = 2 , activation = 'softmax')) model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy']) model.summary()",0,stream,"model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))
 model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
 model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))
 model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
 model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))
 model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))
 model.add(Flatten())
 model.add(Dense(units = 512 , activation = 'relu'))
 model.add(Dropout(0.2))
 model.add(Dense(units = 2 , activation = 'softmax'))
 model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])
 model.summary()"
"ASSIGN = 0.001 ASSIGN = ""sparse_categorical_crossentropy"" model.compile(Adam(lr=ASSIGN), ASSIGN=ASSIGN ,metrics=['accuracy']) model.summary()",0,stream,"initial_lr = 0.001
 loss = ""sparse_categorical_crossentropy""
 model.compile(Adam(lr=initial_lr), loss=loss ,metrics=['accuracy'])
 model.summary()"
"SETUP CHECKPOINT ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN = [] ASSIGN = [] ASSIGN = ['*.png', '*.jpg'] ASSIGN=0 ASSIGN=0 for typ in ASSIGN: for directory in ASSIGN: for filename in glob.glob(os.path.join(directory, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(28,28)) ASSIGN.append(ASSIGN) ASSIGN.append([1,0]) ASSIGN+=1 for directory in ASSIGN: for filename in glob.glob(os.path.join(directory, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(28,28)) ASSIGN.append(ASSIGN) ASSIGN.append([0,1]) ASSIGN+=1 print('A: ', ASSIGN) print('B: ', ASSIGN)",1,stream,"import glob
 import cv2
 directory_a = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/a'
 directory_b = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/b'
 directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'
 directory_s = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/s'
 
 ADirectories = []
 BDirectories = []
 
 ADirectories.append(directory_a)
 ADirectories.append(directory_s)
 BDirectories.append(directory_b)
 BDirectories.append(directory_5)
 
 
 X = []
 y = []
 types = ['*.png', '*.jpg']
 countA=0
 countB=0
 for typ in types:
     for directory in ADirectories:
         for filename in glob.glob(os.path.join(directory, typ)):
             im1 =cv2.imread(filename,0)
             im1 = cv2.resize(im1,(28,28))
             X.append(im1)
             y.append([1,0])
             countA+=1
     for directory in BDirectories:
         for filename in glob.glob(os.path.join(directory, typ)):
             im1 =cv2.imread(filename,0)
             im1 = cv2.resize(im1,(28,28))
             X.append(im1)
             y.append([0,1])
             countB+=1
 print('A: ', countA)
 print('B: ', countB)
"
ASSIGN = np.asarray(ASSIGN) ASSIGN = np.asarray(ASSIGN),1,not_existent,"X = np.asarray(X)
 y = np.asarray(y)"
X=Xpath,1,not_existent,X=X/255
"ASSIGN = ASSIGN.reshape(4527, 28, 28,1)",1,error,"# y = y.reshape(4527,1)
 X = X.reshape(4527, 28, 28,1)"
CHECKPOINT y[2].shape,0,execute_result,y[2].shape
SETUP ASSIGN = LabelBinarizer() ASSIGN = label_binarizer.fit_transform(ASSIGN) ASSIGN = label_binarizer.fit_transform(ASSIGN),1,not_existent,"from sklearn.preprocessing import LabelBinarizer
 label_binarizer = LabelBinarizer()
 y_train = label_binarizer.fit_transform(y_train)
 y_test = label_binarizer.fit_transform(y_test)"
"ASSIGN = ImageDataGenerator( ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=10, ASSIGN = 0.1, ASSIGN=0.1, ASSIGN=0.1, ASSIGN=False, ASSIGN=False) ASSIGN.fit(X_train)",1,error,"# With data augmentation to prevent overfitting
 
 datagen = ImageDataGenerator(
         featurewise_center=False,  # set input mean to 0 over the dataset
         samplewise_center=False,  # set each sample mean to 0
         featurewise_std_normalization=False,  # divide inputs by std of the dataset
         samplewise_std_normalization=False,  # divide each input by its std
         zca_whitening=False,  # apply ZCA whitening
         rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)
         zoom_range = 0.1, # Randomly zoom image 
         width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
         height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
         horizontal_flip=False,  # randomly flip images
         vertical_flip=False)  # randomly flip images
 
 
 datagen.fit(X_train)"
"ASSIGN = 20 ASSIGN = 256 ASSIGN = model.fit(datagen.flow(X_train,y_train, batch_size = 128) ,epochs = 20 , validation_data = (X_test, y_test))",0,error,"epochs = 20
 batch_size = 256
 # history_1 = model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=[X_test,y_test])
 history_1 = model.fit(datagen.flow(X_train,y_train, batch_size = 128) ,epochs = 20 , validation_data = (X_test, y_test))"
"ASSIGN = plt.figure(figsize=(20,7)) ASSIGN.add_subplot(121) plt.plot(history_1.epoch,history_1.history['accuracy'],label = ""accuracy"") # Accuracy curve for training set plt.plot(history_1.epoch,history_1.history['val_accuracy'],label = ""val_accuracy"") # Accuracy curve for validation set plt.title(""Accuracy Curve"",fontsize=18) plt.xlabel(""Epochs"",fontsize=15) plt.ylabel(""Accuracy"",fontsize=15) plt.grid(alpha=0.3) plt.legend() ASSIGN.add_subplot(122) plt.plot(history_1.epoch,history_1.history['loss'],label=""loss"") # Loss curve for training set plt.plot(history_1.epoch,history_1.history['val_loss'],label=""val_loss"") # Loss curve for validation set plt.title(""Loss Curve"",fontsize=18) plt.xlabel(""Epochs"",fontsize=15) plt.ylabel(""Loss"",fontsize=15) plt.grid(alpha=0.3) plt.legend() plt.show()",0,error,"# Diffining Figure
 f = plt.figure(figsize=(20,7))
 
 #Adding Subplot 1 (For Accuracy)
 f.add_subplot(121)
 
 plt.plot(history_1.epoch,history_1.history['accuracy'],label = ""accuracy"") # Accuracy curve for training set
 plt.plot(history_1.epoch,history_1.history['val_accuracy'],label = ""val_accuracy"") # Accuracy curve for validation set
 
 plt.title(""Accuracy Curve"",fontsize=18)
 plt.xlabel(""Epochs"",fontsize=15)
 plt.ylabel(""Accuracy"",fontsize=15)
 plt.grid(alpha=0.3)
 plt.legend()
 
 #Adding Subplot 1 (For Loss)
 f.add_subplot(122)
 
 plt.plot(history_1.epoch,history_1.history['loss'],label=""loss"") # Loss curve for training set
 plt.plot(history_1.epoch,history_1.history['val_loss'],label=""val_loss"") # Loss curve for validation set
 
 plt.title(""Loss Curve"",fontsize=18)
 plt.xlabel(""Epochs"",fontsize=15)
 plt.ylabel(""Loss"",fontsize=15)
 plt.grid(alpha=0.3)
 plt.legend()
 
 plt.show()"
"CHECKPOINT def testCNNModel(path,model): ASSIGN =cv2.imread(path,0) ASSIGN = cv2.resize(ASSIGN,(28,28)) ASSIGN = [] ASSIGN.append(ASSIGN.reshape(28,28)) ASSIGN = np.asarray(ASSIGN) ASSIGN = ASSIGN.reshape(1,28,28,1) ASSIGN =model.predict(t) return res return",0,not_existent,"def testCNNModel(path,model):
     im1 =cv2.imread(path,0)
     im1 = cv2.resize(im1,(28,28))
     t = []
     t.append(im1.reshape(28,28))
     t = np.asarray(t)
     t = t.reshape(1,28,28,1)
     res =model.predict(t)
     return res
         
     return "
"testCNNModel('path',model)",0,execute_result,"testCNNModel('/kaggle/input/testdata2/fist1.jpg',model)"
label_binarizer.transform(np.asarray([0])),0,execute_result,label_binarizer.transform(np.asarray([0]))
"SETUP ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] ASSIGN = ['*.png', '*.jpg'] for typ in ASSIGN: for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(0) for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(1) for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(2)",1,not_existent,"import glob
 import cv2
 directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'
 directory_2 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/2'
 directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'
 
 
 
 
 X = []
 y = []
 types = ['*.png', '*.jpg']
 for typ in types:
         for filename in glob.glob(os.path.join(directory_2, typ)):
             im1 =cv2.imread(filename,0)
             im1 = cv2.resize(im1,(64,64))
             X.append(im1)
             y.append(0)
         for filename in glob.glob(os.path.join(directory_5, typ)):
             im1 =cv2.imread(filename,0)
             im1 = cv2.resize(im1,(64,64))
             X.append(im1)
             y.append(1)
         for filename in glob.glob(os.path.join(directory_unk, typ)):
             im1 =cv2.imread(filename,0)
             im1 = cv2.resize(im1,(64,64))
             X.append(im1)
             y.append(2)
             
 
"
ASSIGN = np.asarray(ASSIGN) ASSIGN = np.asarray(ASSIGN),1,not_existent,"X = np.asarray(X)
 y = np.asarray(y)
 
 
"
"CHECKPOINT ASSIGN = ASSIGN.reshape(-1,64,64,1) X.shape",1,execute_result,"X = X.reshape(-1,64,64,1)
 X.shape"
"SETUP SETUP ASSIGN = Xpath ASSIGN = Sequential() ASSIGN.add(Conv2D(16, (2,2), input_shape=ASSIGN.shape[1:], activation='relu')) ASSIGN.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')) ASSIGN.add(Conv2D(32, (3,3), activation='relu')) ASSIGN.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same')) ASSIGN.add(Conv2D(64, (5,5), activation='relu')) ASSIGN.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same')) ASSIGN.add(Flatten()) ASSIGN.add(Dense(128, activation='relu')) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Dense(3, activation='softmax')) ASSIGN = TensorBoard(log_dir=""path{}"".format(NAME)) ASSIGN.compile(loss='sparse_categorical_crossentropy', ASSIGN='adam', ASSIGN=['accuracy']) ASSIGN.fit(ASSIGN, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[ASSIGN])",0,stream,"IMG_SIZE=64
 import tensorflow as tf
 from tensorflow.keras.datasets import cifar10
 from tensorflow.keras.preprocessing.image import ImageDataGenerator
 from tensorflow.keras.models import Sequential
 from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
 from tensorflow.keras.layers import Conv2D, MaxPooling2D
 from tensorflow.keras.callbacks import TensorBoard
 import time
 
 import pickle
 
 NAME = ""Numbers-CNN-Model-{}"".format(str(time.ctime())) # Model Name
 
 
 X = X/255.0
 
 model = Sequential()
 
 model.add(Conv2D(16, (2,2), input_shape=X.shape[1:], activation='relu'))
 model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))
 model.add(Conv2D(32, (3,3), activation='relu'))
 model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))
 model.add(Conv2D(64, (5,5), activation='relu'))
 model.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))
 model.add(Flatten())
 model.add(Dense(128, activation='relu'))
 model.add(Dropout(0.2))
 model.add(Dense(3, activation='softmax')) # size must be equal to number of classes i.e. 11
 
 tensorboard = TensorBoard(log_dir=""/kaggle/working/logs/{}"".format(NAME))
 
 model.compile(loss='sparse_categorical_crossentropy',
               optimizer='adam',
               metrics=['accuracy'])
 
 model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[tensorboard])"
"CHECKPOINT def testCNNModel(path,model): ASSIGN =cv2.imread(path,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN = [] ASSIGN.append(ASSIGN.reshape(64,64)) ASSIGN = np.asarray(ASSIGN) ASSIGN = ASSIGN.reshape(1,64,64,1) ASSIGN =model.predict(t) return res return",0,not_existent,"def testCNNModel(path,model):
     im1 =cv2.imread(path,0)
     im1 = cv2.resize(im1,(64,64))
     t = []
     t.append(im1.reshape(64,64))
     t = np.asarray(t)
     t = t.reshape(1,64,64,1)
     res =model.predict(t)
     return res
         
     return "
"testCNNModel('path',model)",0,error,"testCNNModel('/kaggle/input/testdata2/two14.jpg',model)"
"SETUP ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] ASSIGN = ['*.png', '*.jpg'] for typ in ASSIGN: for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(0) for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(1) for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(2)",1,not_existent,"import glob
 import cv2
 directory_1 = '/kaggle/input/3shapesdataset/resized/1'
 directory_2 = '/kaggle/input/3shapesdataset/resized/2'
 directory_3 = '/kaggle/input/3shapesdataset/resized/3'
 #directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'
 
 
 
 
 X = []
 y = []
 types = ['*.png', '*.jpg']
 for typ in types:
         for filename in glob.glob(os.path.join(directory_1, typ)):
             im1 =cv2.imread(filename,0)
             im1 = cv2.resize(im1,(64,64))
             X.append(im1)
             y.append(0)
         for filename in glob.glob(os.path.join(directory_2, typ)):
             im1 =cv2.imread(filename,0)
             im1 = cv2.resize(im1,(64,64))
             X.append(im1)
             y.append(1)
         for filename in glob.glob(os.path.join(directory_3, typ)):
             im1 =cv2.imread(filename,0)
             im1 = cv2.resize(im1,(64,64))
             X.append(im1)
             y.append(2)
 #         for filename in glob.glob(os.path.join(directory_unk, typ)):
 #             im1 =cv2.imread(filename,0)
 #             im1 = cv2.resize(im1,(64,64))
 #             X.append(im1)
 #             y.append(3)
             
             
 
"
"testCNNModel('path',model)",0,execute_result,"testCNNModel('/kaggle/input/testdata2/fist5.jpg',model)"
CHECKPOINT print('asd'),0,stream,"print('asd')
"
"SETUP CHECKPOINT ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] ASSIGN = ['*.png', '*.jpg'] for typ in ASSIGN: for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(0) print('finished') for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(1) print('finished') for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(2) print('finished') for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(3) print('finished')",1,stream,"import glob
 import cv2
 directory_1 = '/kaggle/input/3shapesdatasetunk/All Data/1'
 directory_2 = '/kaggle/input/3shapesdatasetunk/All Data/2'
 directory_3 = '/kaggle/input/3shapesdatasetunk/All Data/3'
 # directory_4 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/4'
 # directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'
 directory_unk = '/kaggle/input/3shapesdatasetunk/All Data/unknown'
 #directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'
 
 
 
 
 X = []
 y = []
 types = ['*.png', '*.jpg']
 for typ in types:
         for filename in glob.glob(os.path.join(directory_1, typ)):
             im1 =cv2.imread(filename,0)
             im1 = cv2.resize(im1,(64,64))
             X.append(im1)
             y.append(0)
         print('finished')
         for filename in glob.glob(os.path.join(directory_2, typ)):
             im1 =cv2.imread(filename,0)
             im1 = cv2.resize(im1,(64,64))
             X.append(im1)
             y.append(1)
         print('finished')
         for filename in glob.glob(os.path.join(directory_3, typ)):
             im1 =cv2.imread(filename,0)
             im1 = cv2.resize(im1,(64,64))
             X.append(im1)
             y.append(2)
         print('finished')
         for filename in glob.glob(os.path.join(directory_unk, typ)):
             im1 =cv2.imread(filename,0)
             im1 = cv2.resize(im1,(64,64))
             X.append(im1)
             y.append(3)
         print('finished')
             
             
 
"
"SETUP SETUP ASSIGN = Xpath ASSIGN = Sequential() ASSIGN.add(Conv2D(16, (2,2), input_shape=ASSIGN.shape[1:], activation='relu')) ASSIGN.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')) ASSIGN.add(Conv2D(32, (3,3), activation='relu')) ASSIGN.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same')) ASSIGN.add(Conv2D(64, (5,5), activation='relu')) ASSIGN.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same')) ASSIGN.add(Flatten()) ASSIGN.add(Dense(128, activation='relu')) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Dense(4, activation='softmax')) ASSIGN = TensorBoard(log_dir=""path{}"".format(NAME)) ASSIGN.compile(loss='sparse_categorical_crossentropy', ASSIGN='adam', ASSIGN=['accuracy']) ASSIGN.fit(ASSIGN, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[ASSIGN])",0,stream,"IMG_SIZE=64
 import tensorflow as tf
 from tensorflow.keras.datasets import cifar10
 from tensorflow.keras.preprocessing.image import ImageDataGenerator
 from tensorflow.keras.models import Sequential
 from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten
 from tensorflow.keras.layers import Conv2D, MaxPooling2D
 from tensorflow.keras.callbacks import TensorBoard
 import time
 
 import pickle
 
 NAME = ""Numbers-CNN-Model-{}"".format(str(time.ctime())) # Model Name
 
 
 X = X/255.0
 
 model = Sequential()
 
 model.add(Conv2D(16, (2,2), input_shape=X.shape[1:], activation='relu'))
 model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))
 model.add(Conv2D(32, (3,3), activation='relu'))
 model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))
 model.add(Conv2D(64, (5,5), activation='relu'))
 model.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))
 model.add(Flatten())
 model.add(Dense(128, activation='relu'))
 model.add(Dropout(0.2))
 model.add(Dense(4, activation='softmax')) # size must be equal to number of classes i.e. 11
 
 tensorboard = TensorBoard(log_dir=""/kaggle/working/logs/{}"".format(NAME))
 
 model.compile(loss='sparse_categorical_crossentropy',
               optimizer='adam',
               metrics=['accuracy'])
 
 model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[tensorboard])"
"testCNNModel('path',model)",0,execute_result,"testCNNModel('/kaggle/input/testdata2/two4.jpg',model)"
"ASSIGN = [1, 2, 3, 4, 5, 6] for x in ASSIGN: print (x)",1,stream,"mylist = [1, 2, 3, 4, 5, 6]
 
 # Printing out each number one by one in a for-loop
 for x in mylist:
     print (x)"
CHECKPOINT ASSIGN = 0 for x in mylist: ASSIGN=ASSIGN+x print(ASSIGN),0,stream,"# Your notebook already knows about mylist. Sum its values by adding the code below this comment.
 total = 0
 
 # This is showing cumulative addition
 for x in mylist:
     total=total+x
     print(total)
"
"ASSIGN = ""This is a test string for HCDE 530"" ASSIGN=s.split() for x in ASSIGN: print (x)",1,stream,"s = ""This is a test string for HCDE 530""
 # Add your code below
 y=s.split()
 
 # For each loop, the word that comes out of the variable ""y"" goes on a new line
 for x in y:
     print (x)
"
"ASSIGN = [1, 2, 3, 4, 5, 6] ASSIGN[3]=""four"" print (ASSIGN)",1,stream,"# Add your code here
 mylist = [1, 2, 3, 4, 5, 6]
 
 # This will replace the fourth item in this list with ""four""
 mylist[3]=""four""
 print (mylist)
 
 
"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename)) ASSIGN = open('path', 'r') for f in ASSIGN: print (f.rstrip()) ASSIGN.close()",0,stream,"# The code below allows you to access your Kaggle data files
 import os
 for dirname, _, filenames in os.walk('/kaggle/input'):
     for filename in filenames:
         print(os.path.join(dirname, filename))
 
 # create a file handle called fname to open and hold the contents of the data file
 fname = open('/kaggle/input/testtext/test.txt', 'r')
 
 # Add your code below
 for f in fname:
     # This will print out each line
     print (f.rstrip())
 
 # It's good practice to close your file when you are finished. This is in the next line.
 fname.close()"
"CHECKPOINT ASSIGN = [""Akita"",""Alaskan Malamute"",""Australian shepherd"",""Basset hound"",""Beagle"",""Boston terrier"",""Bulldog"",""Chihuahua"",""Cocker Spaniel"",""Collie"",""French Bulldog"",""Golden Retriever"",""Great Dane"",""Poodle"",""Russell Terrier"",""Scottish Terrier"",""Siberian Husky"",""Skye terrier"",""Smooth Fox terrier"",""Terrier"",""Whippet""] for dogs in ASSIGN: if (dogs.find(""Terrier"") != -1): print (dogs.find(""Terrier"")) elif (dogs.find(""terrier"") != -1): print(dogs.find()) else: print(-1)",1,stream,"dogList = [""Akita"",""Alaskan Malamute"",""Australian shepherd"",""Basset hound"",""Beagle"",""Boston terrier"",""Bulldog"",""Chihuahua"",""Cocker Spaniel"",""Collie"",""French Bulldog"",""Golden Retriever"",""Great Dane"",""Poodle"",""Russell Terrier"",""Scottish Terrier"",""Siberian Husky"",""Skye terrier"",""Smooth Fox terrier"",""Terrier"",""Whippet""]
 
 # Add your code below
 for dogs in dogList:
     # If the word ""Terrier"" (uppercase) is in this item, print the character number of where it starts
     if (dogs.find(""Terrier"") != -1):
         print (dogs.find(""Terrier""))
      # If the word ""terrier"" (lowercase) is in this item, print the character number of where it starts
     elif (dogs.find(""terrier"") != -1):
         print(dogs.find(""terrier""))
     # If neither ""Terrier"" nor ""terrier"" are in the item, just print -1
     else:
         print(-1)
                                                
"
"CHECKPOINT ASSIGN = [0,1,1,0,1,1,0] for num in ASSIGN: ASSIGN==1: print()",1,stream,"binList = [0,1,1,0,1,1,0]
 
 # Add your code below
 for num in binList:
     # If the item in the list is ""1"" then print ""One""
     if num==1:
         print(""One"")
"
"CHECKPOINT for dogs in dogList: if (dogs.find(""Bulldog"") != -1): print(dogs)",0,stream,"for dogs in dogList:
     # If the item in dogList has ""Bulldog"" then print the name of the dog. Otherwise, don't print anything.
     if (dogs.find(""Bulldog"") != -1):
         print(dogs)"
"CHECKPOINT ASSIGN = 0 ASSIGN = 0 ASSIGN = 0 ASSIGN = open('path', 'r') for f in ASSIGN: ASSIGN=len(f)+ASSIGN ASSIGN=len(f[0])+ASSIGN ASSIGN=(len(f.split()))+ASSIGN print('%d characters'%ASSIGN) print('%d lines'%ASSIGN) print('%d words'%ASSIGN) ASSIGN.close()",1,stream,"numChars = 0
 numLines = 0
 numWords = 0
 
 # create a file handle called fname to open and hold the contents of the data file
 # make sure to upload your test.txt file first
 fname = open('/kaggle/input/testtext/test.txt', 'r')
 
 # Add your code below to read each line in the file, count the number of characters, lines, and words
 # updating the numChars, numLines, and numWords variables.
 for f in fname:
     
     # Cumulative addition of the length of each word
     numChars=len(f)+numChars
     # Cumulative addition of the first letter in each line
     numLines=len(f[0])+numLines
     # Cumulative addition of items in each line's list
     numWords=(len(f.split()))+numWords
     
 # output code below is provided for you; you should not edit this
 
 print('%d characters'%numChars)
 print('%d lines'%numLines)
 print('%d words'%numWords)
 
 # It's good practice to close your file when you are finished. This is in the next line.
 fname.close()"
"CHECKPOINT ASSIGN = 0 ASSIGN = 0 ASSIGN = 0 ASSIGN = open('path', 'r') for f in ASSIGN: ASSIGN=len(f)+ASSIGN ASSIGN=len(f[0])+ASSIGN ASSIGN=(len(f.split()))+ASSIGN print('%d characters'%ASSIGN) print('%d lines'%ASSIGN) print('%d words'%ASSIGN) ASSIGN.close()",1,stream,"# Add your code below
 numChars = 0
 numLines = 0
 numWords = 0
 
 # create a file handle called fname to open and hold the contents of the data file
 # make sure to upload your test.txt file first
 fname = open('/kaggle/input/reading/sherlock.txt', 'r')
 
 # Add your code below to read each line in the file, count the number of characters, lines, and words
 # updating the numChars, numLines, and numWords variables.
 for f in fname:
     
     # Cumulative addition of the length of each word
     numChars=len(f)+numChars
     # Cumulative addition of the first letter in each line
     numLines=len(f[0])+numLines
     # Cumulative addition of items in each line's list
     numWords=(len(f.split()))+numWords
     
 # output code below is provided for you; you should not edit this
 
 print('%d characters'%numChars)
 print('%d lines'%numLines)
 print('%d words'%numWords)
 
 # It's good practice to close your file when you are finished. This is in the next line.
 fname.close()"
SETUP,0,not_existent,import random as rd import matplotlib.pyplot as plt
"class Student: def __init__(self, id, shyness, attitude, cap, usageRate, year): self.id = id self.shyness = shyness ''' attitude towards drinking (normal distribution, mean is MeanAttitude and s.d 1) determines usagerate, whether student may drink alone and addiction ''' self.attitude = attitude ''' updated at the end of the school year if below a threshold, student can't continue next year (inSchool = 0) ''' self.cap = cap self.usageRate = usageRate self.inSchool = True self.year = year self.friends = [] self.host = [] self.guestList = [] self.whenAttend = [] def host_party(self, student_list): ASSIGN = self.whenAttend ASSIGN = (0.2 * self.shyness**2) - (len(attending))path ASSIGN = rd.random() if ASSIGN <= ASSIGN: ASSIGN = ['Sun','Mon','Tue','Wed','Thu','Fri','Sat'] for a in ASSIGN: if a in ASSIGN: ASSIGN.remove(a) ASSIGN = rd.choices(week, weights = [0.05,0.05,0.05,0.05,0.1,0.35,0.35]) self.host.append(ASSIGN) for others in student_list: if others.id in self.friends: others.willAttend(ASSIGN) if ASSIGN in others.whenAttend: self.guestList.append(others.id) def willAttend(self, ASSIGN): if ASSIGN not in self.whenAttend: ASSIGN = rd.random() ASSIGN = ['Sun','Sat'] if ASSIGN in ASSIGN: if ASSIGN <= 0.8: self.whenAttend.append(ASSIGN) else: if ASSIGN <= 0.4: self.whenAttend.append(ASSIGN) def partyDrink(self, peer_pressure): ASSIGN = (self.attitudepath) + (2*peer_pressure) - 0.1*(self.usageRatepath) ASSIGN = rd.random() if ASSIGN <= ASSIGN: self.usageRate += 1 self.experience() return 1 return 0 def drinkAlone(self): if self.attitude > 3: ASSIGN = self.attitudepath ASSIGN = rd.random() if ASSIGN <= ASSIGN: self.usageRate += 1 def experience(self): ASSIGN = rd.noramlvariate(meanExperience, stDevExperience) if ASSIGN < -6: ASSIGN = -6 elif ASSIGN > 3: ASSIGN = 3 ASSIGN = rd.random() if ASSIGN < probabilityofBust: ASSIGN -= 3 self.attitude += ASSIGN*0.1 def gradeExperience(self): if self.usageRate > usageRateGradeDrop: ASSIGN = -1*(self.usageRatepath) self.attitude += (ASSIGN * 0.2) def gradeUpdate(self): ASSIGN = 0.2path ASSIGN = rd.normalvariate(0.1,std_dev) if self.usageRate > usageRateGradeDrop: ASSIGN = self.usageRatepath(30*self.year) ASSIGN = 0.5path ASSIGN = rd.normalvariate(mean_drop,s_dev) ASSIGN += ASSIGN student.cap -= ASSIGN",1,not_existent,"### create Student Class class Student:    def __init__(self, id, shyness, attitude, cap, usageRate, year):     self.id = id      ##characteristics (integers)     #low score means more shy (normal distribution, mean 0 and s.d 1)     self.shyness = shyness     '''     attitude towards drinking      (normal distribution, mean is MeanAttitude and s.d 1)     determines usagerate, whether student may drink alone and addiction      '''     self.attitude = attitude     '''     updated at the end of the school year     if below a threshold, student can't continue next year (inSchool = 0)     '''     self.cap = cap     #number of times drinking (in the past 30 days?)>should update every 30 or just keep it for the whole sem?     self.usageRate = usageRate      self.inSchool = True         self.year = year      ##social life (lists)     self.friends = []     #list of days they are hosting a party in the next week, and a list of attendees     self.host = []     self.guestList = []     #list of days student will attend someone else's party     self.whenAttend = []     def host_party(self, student_list):        #determine if hosting that week,     attending = self.whenAttend          p_host = (0.2 * self.shyness**2) - (len(attending))/5     num = rd.random()     if num <= p_host:        #choose which day to host       week = ['Sun','Mon','Tue','Wed','Thu','Fri','Sat']       for a in attending:         if a in week:           week.remove(a)       #       day = rd.choices(week, weights = [0.05,0.05,0.05,0.05,0.1,0.35,0.35])       self.host.append(day)           #send out invites              for others in student_list:         #is there a better way to select the respective students based on their id?         #two nested for loops - might get hard over large student populations         if others.id in self.friends:           others.willAttend(day)            if day in others.whenAttend:             self.guestList.append(others.id)      #when student gets invited to a party   def willAttend(self, day):     if day not in self.whenAttend:       num = rd.random()       weekend = ['Sun','Sat']       if day in weekend:         if num <= 0.8:           self.whenAttend.append(day)       else:         if num <= 0.4:           self.whenAttend.append(day)           def partyDrink(self, peer_pressure):     # arbitrary coefficients      # limits - self.attitude = [2, 3]     #        - attitudeUse = [1,1]     #        - peer_pressure = [0, 1]     #        - usageRate = []     #        - usageRateUse = []     # take note of the limits     p_drink = (self.attitude/ attitudeUse) + (2*peer_pressure) - 0.1*(self.usageRate/usageRateUse)     num = rd.random()     if num <= p_drink:       self.usageRate += 1       self.experience()       return 1     return 0                 def drinkAlone(self):     if self.attitude > 3:       p_alone = self.attitude/15       num = rd.random()       if num <= p_alone:         self.usageRate += 1    #after a drink at the party     def experience(self):     num = rd.noramlvariate(meanExperience, stDevExperience)     if num < -6:       num = -6     elif num > 3:       num = 3      #if party gets busted     bust = rd.random()     if bust < probabilityofBust:       num -= 3      #update attitude towards drinking     # attitude update is arbitrary at 0.1     self.attitude += num*0.1 #    self.attitude += num*0.5         #every 3 weeks   def gradeExperience(self):     if self.usageRate > usageRateGradeDrop:       poor_exp = -1*(self.usageRate/15)       self.attitude += (poor_exp * 0.2)    #at the end of the semester   def gradeUpdate(self):     std_dev = 0.2/self.year     change = rd.normalvariate(0.1,std_dev)      #add on to grade drop due to drinking     if self.usageRate > usageRateGradeDrop:       mean_drop = self.usageRate/(30*self.year)       s_dev = 0.5/self.year       grade_drop = rd.normalvariate(mean_drop,s_dev)       change += grade_drop          student.cap -= change    "
"def make_friends(student1, student2): if student2.id not in student1.friends: shy1, shy2 = student1.shyness, student2.shyness fr1,fr2 = len(student1.friends), len(student2.friends) att1, att2 = student1.attitude, student2.attitude ASSIGN = ((wShy*(shy1 + shy2 + 4)) - (wFr*((fr1path(3+shy1)) + (fr2path(3+shy2)))) - (wAtt*(att1 - att2))) path(8*wShy) ASSIGN = rd.random() if ASSIGN <= ASSIGN: student1.friends.append(student2.id) student2.friends.append(student1.id) def party_friends(attendees): for guest in attendees: for others in attendees: if guest != others: make_friends(guest, others) def party_time(host, student_list, day): ASSIGN = host.guestList ASSIGN = [host] ASSIGN = 0 for others in student_list: if others.id in ASSIGN: ASSIGN.append(others) party_friends(ASSIGN) for j in range(3): for member in ASSIGN: ASSIGN = member.partyDrink(peer_pressure) ASSIGN += (xpath(ASSIGN)) host.guestList.remove(day)",1,not_existent,"### students interacting  def make_friends(student1, student2):   if student2.id not in student1.friends:     shy1, shy2 = student1.shyness, student2.shyness     fr1,fr2 = len(student1.friends), len(student2.friends)     att1, att2 = student1.attitude, student2.attitude     p = ((wShy*(shy1 + shy2 + 4)) - (wFr*((fr1/(3+shy1)) + (fr2/(3+shy2)))) - (wAtt*(att1 - att2))) / (8*wShy)     num = rd.random()     if num <= p:       student1.friends.append(student2.id)        student2.friends.append(student1.id)  #for all students at the party def party_friends(attendees):      for guest in attendees:     for others in attendees:        if guest != others:         make_friends(guest, others)   #will it be running the function twice for some of them? How to avoid?  #when there's a party host def party_time(host, student_list, day):   invited = host.guestList   attendees = [host]   peer_pressure = 0   for others in student_list:     if others.id in invited:       attendees.append(others)    party_friends(attendees)    #3 rounds of drinking or not   for j in range(3):     for member in attendees:       x = member.partyDrink(peer_pressure)       peer_pressure += (x/len(attendees))    host.guestList.remove(day)"
MeanAttitude = 3 NumberOfAgents = 1000 ASSIGN = 0.8 ASSIGN = 0.8 ASSIGN = 0.5 ASSIGN = 6 ASSIGN = 1 ASSIGN = 1.6 ASSIGN = 1.6 ASSIGN = 0.001 ASSIGN = 3 ASSIGN = 13,1,not_existent,## Input variables  MeanAttitude = 3 NumberOfAgents = 1000 wShy = 0.8  wFr = 0.8  wAtt = 0.5  usageRateUse = 6  attitudeUse = 1  meanExperience = 1.6  stdDevExperience = 1.6  probabilityOfBust = 0.001 maxGoodExperience = 3  usageRateGradeDrop = 13
"ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for student in range(NumberOfAgents): ASSIGN = rd.normalvariate(0,1) ASSIGN = rd.normalvariate(MeanAttitude,0.5) ASSIGN = rd.normalvariate(3.5, 0.5) if ASSIGN < 2: ASSIGN = 2 elif ASSIGN > 5: ASSIGN = 5 if ASSIGN < 0: ASSIGN = 0 elif ASSIGN <= 5: ASSIGN = round(attitude_) else: ASSIGN = round(2*attitude_) ASSIGN = rd.choice([1,2,3,4]) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN)",1,not_existent,"##making lists for the variables  attitude_list = [] shyness_list = [] cap_list = [] usagerate_list = [] year_list = []   for student in range(NumberOfAgents):   shyness_ = rd.normalvariate(0,1)   attitude_ = rd.normalvariate(MeanAttitude,0.5)   cap_ = rd.normalvariate(3.5, 0.5)   if cap_ < 2:     cap_ = 2   elif cap_ > 5:     cap_ = 5    if attitude_ < 0:     usagerate_ = 0   elif attitude_ <= 5:     usagerate_ = round(attitude_)   else:     usagerate_ = round(2*attitude_)    year_ = rd.choice([1,2,3,4])      shyness_list.append(shyness_)   attitude_list.append(attitude_)   cap_list.append(cap_)   year_list.append(year_)   usagerate_list.append(usagerate_)    #id, shyness, attitude, cap, usageRate, year"
"SETUP ASSIGN = rd.sample(range(1, 100000), NumberOfAgents) ASSIGN = pd.DataFrame({""id"": id_list, ""shyness"": shyness_list, ""attitude"": attitude_list, ""cap"": cap_list, ""usage"": usagerate_list, ""year"": year_list, }) ASSIGN.head()",1,execute_result,"## making a dataframe that represents student attributes  import pandas as pd  id_list = rd.sample(range(1, 100000), NumberOfAgents)  df = pd.DataFrame({""id"": id_list,                    ""shyness"": shyness_list,                    ""attitude"": attitude_list,                    ""cap"": cap_list,                    ""usage"": usagerate_list,                    ""year"": year_list,                    }) df.head()"
"ASSIGN = [] for index, row in df.iterrows(): ASSIGN = Student(row['id'], row['shyness'], row['attitude'], row['cap'], row['usage'], row['year']) ASSIGN.append(ASSIGN)",1,not_existent,"### making a list of student objects  student_list = []  for index, row in df.iterrows():   #print(index)   #print(row)   student = Student(row['id'],                      row['shyness'],                     row['attitude'],                     row['cap'],                     row['usage'],                     row['year'])   student_list.append(student)"
"CHECKPOINT ASSIGN = [] ASSIGN = 0 for student in student_list: ASSIGN.append(student.cap) ASSIGN += student.cap ASSIGN = total_start_cappath(student_list) print(ASSIGN) plt.hist(ASSIGN, color = 'purple') plt.title('Starting CAP') plt.show()",0,stream,"start_cap = [] total_start_cap = 0 for student in student_list:   start_cap.append(student.cap)   total_start_cap += student.cap  avg_start_cap = total_start_cap/len(student_list) print(avg_start_cap)  plt.hist(start_cap, color = 'purple') plt.title('Starting CAP') plt.show()"
ASSIGN = [] for student in student_list: ASSIGN.append(student.attitude) plt.hist(ASSIGN) plt.show(),0,display_data,now_attitude = [] for student in student_list:   now_attitude.append(student.attitude)  plt.hist(now_attitude) plt.show()
ASSIGN = [] for student in student_list: ASSIGN.append(student.usageRate) plt.hist(ASSIGN) plt.show(),0,display_data,start_usage = [] for student in student_list:   start_usage.append(student.usageRate)  plt.hist(start_usage) plt.show()
"ASSIGN = {} ASSIGN = {} for week in range(13): for student in student_list: student.host_party(student_list) ASSIGN == 0: for student in student_list: if student.year == 1: for i in range(20): if student != student_list[i]: make_friends(student, student_list[i]) else: for j in range(30): if student != student_list[j]: make_friends(student, student_list[j]) elif week %3 == 0 and week != 12: for student in student_list: student.gradeExperience() ASSIGN == 12: for student in student_list: student.gradeUpdate() if student.cap < 2: student.inSchool = False student_list.remove(student) ASSIGN = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun'] for day in ASSIGN: for student in student_list: if day in student.host: party_time(student, student_list, day) else: for loners in student_list: loners.drinkAlone() ASSIGN = 0 for student in student_list: ASSIGN += student.usageRate ASSIGN = total_usepath(student_list) ASSIGN[week] = ASSIGN ASSIGN = 0 for student in student_list: ASSIGN += student.attitude ASSIGN = total_attpath(student_list) ASSIGN[week] = ASSIGN",1,not_existent,"### The simulation usage_list = {} attitude_list = {}  for week in range(13):   for student in student_list:     student.host_party(student_list)   if week == 0:     #make some friends     for student in student_list:                   if student.year == 1:                      for i in range(20):           if student != student_list[i]:             make_friends(student, student_list[i])       else:                      for j in range(30):           if student != student_list[j]:             make_friends(student, student_list[j])    elif week %3 == 0 and week != 12:     #grade experience     for student in student_list:       student.gradeExperience()    elif week == 12:     #grade update     for student in student_list:       student.gradeUpdate()       if student.cap < 2:         student.inSchool = False         student_list.remove(student)    days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']   for day in days:     for student in student_list:       if day in student.host:         #party starts         party_time(student, student_list, day)                        #if there's no one hosting     else:       for loners in student_list:         loners.drinkAlone()    total_use = 0   for student in student_list:     total_use += student.usageRate   avg_use = total_use/len(student_list)     usage_list[week] = avg_use            total_att = 0   for student in student_list:     total_att += student.attitude   avg_att = total_att/len(student_list)   attitude_list[week] = avg_att"
CHECKPOINT attitude_list,0,execute_result,attitude_list
"ASSIGN = [] ASSIGN = [] for key, value in attitude_list.items(): ASSIGN.append(key) ASSIGN.append(value) plt.plot(ASSIGN, ASSIGN) plt.title('Average attitude over the semester') plt.xlabel('Week') plt.ylabel('Average attitude') plt.show()",0,display_data,"weeks = [] att = []  for key, value in attitude_list.items():   weeks.append(key)   att.append(value)  plt.plot(weeks, att) plt.title('Average attitude over the semester') plt.xlabel('Week') plt.ylabel('Average attitude') plt.show()"
"CHECKPOINT ASSIGN = [] ASSIGN = 0 for student in student_list: ASSIGN.append(student.cap) ASSIGN += student.cap ASSIGN = total_cappath(student_list) print(ASSIGN) print(max(start_cap)) print(max(ASSIGN)) plt.hist(start_cap, color = 'orange') plt.hist(ASSIGN, color = 'indigo') plt.legend(['Start', 'End']) plt.title('Average CAP Score') plt.show()",0,stream,"end_cap = [] total_cap = 0 for student in student_list:   end_cap.append(student.cap)   total_cap += student.cap  avg_cap = total_cap/len(student_list) print(avg_cap) print(max(start_cap)) print(max(end_cap))  plt.hist(start_cap, color = 'orange') plt.hist(end_cap, color = 'indigo') plt.legend(['Start', 'End']) plt.title('Average CAP Score') plt.show()"
"ASSIGN = [] for student in student_list: ASSIGN.append(student.usageRate) plt.hist(start_usage, color = 'blue') plt.hist(ASSIGN, color = 'red') plt.title('Number of drinks taken over the semester') plt.show()",0,display_data,"end_usage = [] for student in student_list:   end_usage.append(student.usageRate)  plt.hist(start_usage, color = 'blue') plt.hist(end_usage, color = 'red') plt.title('Number of drinks taken over the semester') plt.show()"
"ASSIGN = pd.DataFrame({'usage': end_usage, 'cap':end_cap}) ASSIGN = end_test[end_test['usage'] < 8] ASSIGN = end_test[end_test['usage'] > 8] plt.hist(ASSIGN['cap'], color = 'blue') plt.hist(ASSIGN['cap'], color = 'orange')",0,execute_result,"end_test = pd.DataFrame({'usage': end_usage, 'cap':end_cap})  g1 = end_test[end_test['usage'] < 8] g2 = end_test[end_test['usage'] > 8] plt.hist(g1['cap'], color = 'blue') plt.hist(g2['cap'], color = 'orange')"
"ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for student in range(NumberOfAgents): ASSIGN = rd.normalvariate(0,1) ASSIGN = rd.normalvariate(MeanAttitude,0.5) ASSIGN = rd.normalvariate(3.5, 0.5) if ASSIGN < 2: ASSIGN = 2 elif ASSIGN > 5: ASSIGN = 5 if ASSIGN < 0: ASSIGN = 0 elif ASSIGN <= 5: ASSIGN = round(attitude_) else: ASSIGN = round(2*attitude_) ASSIGN = rd.choice([1,2,3,4]) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN = rd.sample(range(1, 100000), NumberOfAgents) ASSIGN = pd.DataFrame({""id"": id_list, ""shyness"": ASSIGN, ""attitude"": ASSIGN, ""cap"": ASSIGN, ""usage"": ASSIGN, ""year"": ASSIGN, }) ASSIGN = [] for index, row in ASSIGN.iterrows(): ASSIGN = Student(row['id'], row['shyness'], row['attitude'], row['cap'], row['usage'], row['year']) ASSIGN.append(ASSIGN)",1,not_existent,"##making lists for the variables - second run  attitude_list = [] shyness_list = [] cap_list = [] usagerate_list = [] year_list = []   for student in range(NumberOfAgents):   shyness_ = rd.normalvariate(0,1)   attitude_ = rd.normalvariate(MeanAttitude,0.5)   cap_ = rd.normalvariate(3.5, 0.5)   if cap_ < 2:     cap_ = 2   elif cap_ > 5:     cap_ = 5    if attitude_ < 0:     usagerate_ = 0   elif attitude_ <= 5:     usagerate_ = round(attitude_)   else:     usagerate_ = round(2*attitude_)    year_ = rd.choice([1,2,3,4])      shyness_list.append(shyness_)   attitude_list.append(attitude_)   cap_list.append(cap_)   year_list.append(year_)   usagerate_list.append(usagerate_)    #id, shyness, attitude, cap, usageRate, year   id_list = rd.sample(range(1, 100000), NumberOfAgents)  df = pd.DataFrame({""id"": id_list,                    ""shyness"": shyness_list,                    ""attitude"": attitude_list,                    ""cap"": cap_list,                    ""usage"": usagerate_list,                    ""year"": year_list,                    })  ### making a list of student objects  student_list = []  for index, row in df.iterrows():   #print(index)   #print(row)   student = Student(row['id'],                      row['shyness'],                     row['attitude'],                     row['cap'],                     row['usage'],                     row['year'])   student_list.append(student)"
"ASSIGN = {} ASSIGN = {} for week in range(13): for student in student_list: student.host_party(student_list) ASSIGN == 0: for student in student_list: if student.year == 1: for i in range(20): if student != student_list[i]: make_friends(student, student_list[i]) else: for j in range(30): if student != student_list[j]: make_friends(student, student_list[j]) elif week %3 == 0 and week != 12: for student in student_list: student.gradeExperience() ASSIGN == 12: for student in student_list: student.gradeUpdate() if student.cap < 2: student.inSchool = False student_list.remove(student) ASSIGN = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun'] for day in ASSIGN: for student in student_list: if day in student.host: party_time(student, student_list, day) else: for loners in student_list: loners.drinkAlone() ASSIGN = 0 for student in student_list: ASSIGN += student.usageRate ASSIGN = total_usepath(student_list) ASSIGN[week] = ASSIGN ASSIGN = 0 for student in student_list: ASSIGN += student.attitude ASSIGN = total_attpath(student_list) ASSIGN[week] = ASSIGN",1,not_existent,"### The simulation - 2nd run usage_list_2 = {} attitude_list_2 = {}  for week in range(13):   for student in student_list:     student.host_party(student_list)   if week == 0:     #make some friends     for student in student_list:                   if student.year == 1:                      for i in range(20):           if student != student_list[i]:             make_friends(student, student_list[i])       else:                      for j in range(30):           if student != student_list[j]:             make_friends(student, student_list[j])    elif week %3 == 0 and week != 12:     #grade experience     for student in student_list:       student.gradeExperience()    elif week == 12:     #grade update     for student in student_list:       student.gradeUpdate()       if student.cap < 2:         student.inSchool = False         student_list.remove(student)    days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']   for day in days:     for student in student_list:       if day in student.host:         #party starts         party_time(student, student_list, day)                        #if there's no one hosting     else:       for loners in student_list:         loners.drinkAlone()    total_use = 0   for student in student_list:     total_use += student.usageRate   avg_use = total_use/len(student_list)     usage_list_2[week] = avg_use            total_att = 0   for student in student_list:     total_att += student.attitude   avg_att = total_att/len(student_list)   attitude_list_2[week] = avg_att"
"ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for student in range(NumberOfAgents): ASSIGN = rd.normalvariate(0,1) ASSIGN = rd.normalvariate(MeanAttitude,0.5) ASSIGN = rd.normalvariate(3.5, 0.5) if ASSIGN < 2: ASSIGN = 2 elif ASSIGN > 5: ASSIGN = 5 if ASSIGN < 0: ASSIGN = 0 elif ASSIGN <= 5: ASSIGN = round(attitude_) else: ASSIGN = round(2*attitude_) ASSIGN = rd.choice([1,2,3,4]) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN = rd.sample(range(1, 100000), NumberOfAgents) ASSIGN = pd.DataFrame({""id"": id_list, ""shyness"": ASSIGN, ""attitude"": ASSIGN, ""cap"": ASSIGN, ""usage"": ASSIGN, ""year"": ASSIGN, }) ASSIGN = [] for index, row in ASSIGN.iterrows(): ASSIGN = Student(row['id'], row['shyness'], row['attitude'], row['cap'], row['usage'], row['year']) ASSIGN.append(ASSIGN)",1,not_existent,"##making lists for the variables - third run  attitude_list = [] shyness_list = [] cap_list = [] usagerate_list = [] year_list = []   for student in range(NumberOfAgents):   shyness_ = rd.normalvariate(0,1)   attitude_ = rd.normalvariate(MeanAttitude,0.5)   cap_ = rd.normalvariate(3.5, 0.5)   if cap_ < 2:     cap_ = 2   elif cap_ > 5:     cap_ = 5    if attitude_ < 0:     usagerate_ = 0   elif attitude_ <= 5:     usagerate_ = round(attitude_)   else:     usagerate_ = round(2*attitude_)    year_ = rd.choice([1,2,3,4])      shyness_list.append(shyness_)   attitude_list.append(attitude_)   cap_list.append(cap_)   year_list.append(year_)   usagerate_list.append(usagerate_)    #id, shyness, attitude, cap, usageRate, year   id_list = rd.sample(range(1, 100000), NumberOfAgents)  df = pd.DataFrame({""id"": id_list,                    ""shyness"": shyness_list,                    ""attitude"": attitude_list,                    ""cap"": cap_list,                    ""usage"": usagerate_list,                    ""year"": year_list,                    })  ### making a list of student objects  student_list = []  for index, row in df.iterrows():   #print(index)   #print(row)   student = Student(row['id'],                      row['shyness'],                     row['attitude'],                     row['cap'],                     row['usage'],                     row['year'])   student_list.append(student)"
"ASSIGN = {} ASSIGN = {} for week in range(13): for student in student_list: student.host_party(student_list) ASSIGN == 0: for student in student_list: if student.year == 1: for i in range(20): if student != student_list[i]: make_friends(student, student_list[i]) else: for j in range(30): if student != student_list[j]: make_friends(student, student_list[j]) elif week %3 == 0 and week != 12: for student in student_list: student.gradeExperience() ASSIGN == 12: for student in student_list: student.gradeUpdate() if student.cap < 2: student.inSchool = False student_list.remove(student) ASSIGN = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun'] for day in ASSIGN: for student in student_list: if day in student.host: party_time(student, student_list, day) else: for loners in student_list: loners.drinkAlone() ASSIGN = 0 for student in student_list: ASSIGN += student.usageRate ASSIGN = total_usepath(student_list) ASSIGN[week] = ASSIGN ASSIGN = 0 for student in student_list: ASSIGN += student.attitude ASSIGN = total_attpath(student_list) ASSIGN[week] = ASSIGN",1,not_existent,"### The simulation - run 3 usage_list_3 = {} attitude_list_3 = {}  for week in range(13):   for student in student_list:     student.host_party(student_list)   if week == 0:     #make some friends     for student in student_list:                   if student.year == 1:                      for i in range(20):           if student != student_list[i]:             make_friends(student, student_list[i])       else:                      for j in range(30):           if student != student_list[j]:             make_friends(student, student_list[j])    elif week %3 == 0 and week != 12:     #grade experience     for student in student_list:       student.gradeExperience()    elif week == 12:     #grade update     for student in student_list:       student.gradeUpdate()       if student.cap < 2:         student.inSchool = False         student_list.remove(student)    days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']   for day in days:     for student in student_list:       if day in student.host:         #party starts         party_time(student, student_list, day)                        #if there's no one hosting     else:       for loners in student_list:         loners.drinkAlone()    total_use = 0   for student in student_list:     total_use += student.usageRate   avg_use = total_use/len(student_list)     usage_list_3[week] = avg_use            total_att = 0   for student in student_list:     total_att += student.attitude   avg_att = total_att/len(student_list)   attitude_list_3[week] = avg_att"
"ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for student in range(NumberOfAgents): ASSIGN = rd.normalvariate(0,1) ASSIGN = rd.normalvariate(MeanAttitude,0.5) ASSIGN = rd.normalvariate(3.5, 0.5) if ASSIGN < 2: ASSIGN = 2 elif ASSIGN > 5: ASSIGN = 5 if ASSIGN < 0: ASSIGN = 0 elif ASSIGN <= 5: ASSIGN = round(attitude_) else: ASSIGN = round(2*attitude_) ASSIGN = rd.choice([1,2,3,4]) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN = rd.sample(range(1, 100000), NumberOfAgents) ASSIGN = pd.DataFrame({""id"": id_list, ""shyness"": ASSIGN, ""attitude"": ASSIGN, ""cap"": ASSIGN, ""usage"": ASSIGN, ""year"": ASSIGN, }) ASSIGN = [] for index, row in ASSIGN.iterrows(): ASSIGN = Student(row['id'], row['shyness'], row['attitude'], row['cap'], row['usage'], row['year']) ASSIGN.append(ASSIGN)",1,not_existent,"##making lists for the variables - fourth run  attitude_list = [] shyness_list = [] cap_list = [] usagerate_list = [] year_list = []   for student in range(NumberOfAgents):   shyness_ = rd.normalvariate(0,1)   attitude_ = rd.normalvariate(MeanAttitude,0.5)   cap_ = rd.normalvariate(3.5, 0.5)   if cap_ < 2:     cap_ = 2   elif cap_ > 5:     cap_ = 5    if attitude_ < 0:     usagerate_ = 0   elif attitude_ <= 5:     usagerate_ = round(attitude_)   else:     usagerate_ = round(2*attitude_)    year_ = rd.choice([1,2,3,4])      shyness_list.append(shyness_)   attitude_list.append(attitude_)   cap_list.append(cap_)   year_list.append(year_)   usagerate_list.append(usagerate_)    #id, shyness, attitude, cap, usageRate, year   id_list = rd.sample(range(1, 100000), NumberOfAgents)  df = pd.DataFrame({""id"": id_list,                    ""shyness"": shyness_list,                    ""attitude"": attitude_list,                    ""cap"": cap_list,                    ""usage"": usagerate_list,                    ""year"": year_list,                    })  ### making a list of student objects  student_list = []  for index, row in df.iterrows():   #print(index)   #print(row)   student = Student(row['id'],                      row['shyness'],                     row['attitude'],                     row['cap'],                     row['usage'],                     row['year'])   student_list.append(student)"
"ASSIGN = {} ASSIGN = {} for week in range(13): for student in student_list: student.host_party(student_list) ASSIGN == 0: for student in student_list: if student.year == 1: for i in range(20): if student != student_list[i]: make_friends(student, student_list[i]) else: for j in range(30): if student != student_list[j]: make_friends(student, student_list[j]) elif week %3 == 0 and week != 12: for student in student_list: student.gradeExperience() ASSIGN == 12: for student in student_list: student.gradeUpdate() if student.cap < 2: student.inSchool = False student_list.remove(student) ASSIGN = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun'] for day in ASSIGN: for student in student_list: if day in student.host: party_time(student, student_list, day) else: for loners in student_list: loners.drinkAlone() ASSIGN = 0 for student in student_list: ASSIGN += student.usageRate ASSIGN = total_usepath(student_list) ASSIGN[week] = ASSIGN ASSIGN = 0 for student in student_list: ASSIGN += student.attitude ASSIGN = total_attpath(student_list) ASSIGN[week] = ASSIGN",1,not_existent,"### The simulation - run 4 usage_list_4 = {} attitude_list_4 = {}  for week in range(13):   for student in student_list:     student.host_party(student_list)   if week == 0:     #make some friends     for student in student_list:                   if student.year == 1:                      for i in range(20):           if student != student_list[i]:             make_friends(student, student_list[i])       else:                      for j in range(30):           if student != student_list[j]:             make_friends(student, student_list[j])    elif week %3 == 0 and week != 12:     #grade experience     for student in student_list:       student.gradeExperience()    elif week == 12:     #grade update     for student in student_list:       student.gradeUpdate()       if student.cap < 2:         student.inSchool = False         student_list.remove(student)    days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']   for day in days:     for student in student_list:       if day in student.host:         #party starts         party_time(student, student_list, day)                        #if there's no one hosting     else:       for loners in student_list:         loners.drinkAlone()    total_use = 0   for student in student_list:     total_use += student.usageRate   avg_use = total_use/len(student_list)     usage_list_4[week] = avg_use            total_att = 0   for student in student_list:     total_att += student.attitude   avg_att = total_att/len(student_list)   attitude_list_4[week] = avg_att"
"ASSIGN = [] ASSIGN = [] for key, value in usage_list_2.items(): ASSIGN.append(key) ASSIGN.append(value) ASSIGN = [] ASSIGN = [] for key, value in usage_list_3.items(): ASSIGN.append(key) ASSIGN.append(value) ASSIGN = [] ASSIGN = [] for key, value in usage_list_4.items(): ASSIGN.append(key) ASSIGN.append(value) plt.plot(ASSIGN, ASSIGN, label = 'sem 4') plt.plot(ASSIGN, ASSIGN, label = 'sem 3') plt.plot(ASSIGN, ASSIGN, label = 'sem 2') plt.title('Average usage over the sem') plt.xlabel('Week') plt.ylabel('Average usage rate') plt.legend()",0,execute_result,"week2 = [] usage2 = []  for key, value in usage_list_2.items():   week2.append(key)   usage2.append(value)  week3 = [] usage3 = []  for key, value in usage_list_3.items():   week3.append(key)   usage3.append(value)  week4 = [] usage4 = []  for key, value in usage_list_4.items():   week4.append(key)   usage4.append(value)  plt.plot(week4, usage4, label = 'sem 4') plt.plot(week3, usage3, label = 'sem 3') plt.plot(week2, usage2, label = 'sem 2') #plt.plot(weeks, usages, label = 'sem 1') plt.title('Average usage over the sem') plt.xlabel('Week') plt.ylabel('Average usage rate') plt.legend() #plt.show()"
"ASSIGN = [] ASSIGN = [] for key, value in attitude_list_2.items(): ASSIGN.append(key) ASSIGN.append(value) ASSIGN = [] ASSIGN = [] for key, value in attitude_list_3.items(): ASSIGN.append(key) ASSIGN.append(value) ASSIGN = [] ASSIGN = [] for key, value in attitude_list_4.items(): ASSIGN.append(key) ASSIGN.append(value) plt.plot(ASSIGN, ASSIGN, label = 'sem 4') plt.plot(ASSIGN, ASSIGN, label = 'sem 3') plt.plot(ASSIGN, ASSIGN, label = 'sem 2') plt.plot(weeks, att, label = 'sem 1') plt.title('Average attitude over the sem') plt.xlabel('Week') plt.ylabel('Average attitude level') plt.legend() plt.show()",0,display_data,"week2 = [] att2 = []  for key, value in attitude_list_2.items():   week2.append(key)   att2.append(value)  week3 = [] att3 = []  for key, value in attitude_list_3.items():   week3.append(key)   att3.append(value)  week4 = [] att4 = []  for key, value in attitude_list_4.items():   week4.append(key)   att4.append(value)  plt.plot(week4, att4, label = 'sem 4') plt.plot(week3, att3, label = 'sem 3') plt.plot(week2, att2, label = 'sem 2') plt.plot(weeks, att, label = 'sem 1') plt.title('Average attitude over the sem') plt.xlabel('Week') plt.ylabel('Average attitude level') plt.legend() plt.show()"
ASSIGN = [] ASSIGN = [] for student in student_list: ASSIGN.append(len(student.friends)) ASSIGN.append(student.shyness),1,not_existent,friendship_len = [] shyness_score = []  for student in student_list:   friendship_len.append(len(student.friends))   shyness_score.append(student.shyness)
"plt.scatter(shyness_score, friendship_len)",0,execute_result,"plt.scatter(shyness_score, friendship_len)"
ASSIGN = suicide_attacks['Province'].unique() ASSIGN.sort() suicide_attacks['Province'] = suicide_attacks['Province'].str.lower() suicide_attacks['Province'] = suicide_attacks['Province'].str.strip(),1,not_existent,"# Your turn! Take a look at all the unique values in the ""Province"" column.  provincia = suicide_attacks['Province'].unique() provincia.sort() suicide_attacks['Province'] = suicide_attacks['Province'].str.lower()   # Then convert the column to lowercase and remove any trailing white spaces suicide_attacks['Province'] = suicide_attacks['Province'].str.strip() "
"ASSIGN = fuzzywuzzy.process.extract(""kuram agency"", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio) replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=""kuram agency"")",1,not_existent,"# Your turn! It looks like 'kuram agency' and 'kurram agency' should matches = fuzzywuzzy.process.extract(""kuram agency"", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio) # be the same city. Correct the dataframe so that they are. replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=""kuram agency"")"
SETUP,0,not_existent,"import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
ASSIGN = pd.read_csv('path') ASSIGN = pd.read_csv('path'),0,not_existent,"train = pd.read_csv('/kaggle/input/integer-sequence-learning/train.csv.zip')
 test = pd.read_csv('/kaggle/input/integer-sequence-learning/test.csv.zip')"
"ASSIGN = train.Sequence.values[0].split(',')",1,not_existent,"val = train.Sequence.values[0].split(',')"
CHECKPOINT val,0,execute_result,val
"ASSIGN = np.reshape(np.array(val),(7, 2)) val_shaped ASSIGN = [int(i[0]) for i in val_shaped] ASSIGN = [int(i[1]) for i in val_shaped] ASSIGN = np.reshape(np.array(X),(7,1)) ASSIGN = np.reshape(np.array(y),(7,1))",1,not_existent,"val_shaped = np.reshape(np.array(val),(7, 2))
 val_shaped
 X = [int(i[0]) for i in val_shaped]
 y = [int(i[1]) for i in val_shaped]
 X_shaped = np.reshape(np.array(X),(7,1))
 y_shaped = np.reshape(np.array(y),(7,1))"
CHECKPOINT X_shaped,0,execute_result,X_shaped
CHECKPOINT y_shaped,0,execute_result,y_shaped
"SETUP ASSIGN = PolynomialFeatures(len(X)) ASSIGN = poly.fit_transform(X_shaped) ASSIGN = LinearRegression() ASSIGN.fit(ASSIGN, y_shaped)",1,execute_result,"from sklearn.preprocessing import PolynomialFeatures 
 from sklearn.linear_model import LinearRegression
   
 poly = PolynomialFeatures(len(X))
 X_poly = poly.fit_transform(X_shaped)
   
 # fit the transformed features to Linear Regression
 poly_model = LinearRegression()
 poly_model.fit(X_poly, y_shaped)
"
CHECKPOINT ASSIGN = poly_model.predict(X_poly) predicted,0,execute_result,"predicted = poly_model.predict(X_poly)
 predicted"
CHECKPOINT predicted,0,execute_result,predicted
y_shaped[6][0] -predicted[6][0],0,execute_result,y_shaped[6][0] -predicted[6][0]
"ASSIGN = pd.read_csv(""path"") ASSIGN = pd.read_csv(""path"")",0,not_existent,"train_data = pd.read_csv(""/kaggle/input/titanic/train.csv"")
 test_data = pd.read_csv(""/kaggle/input/titanic/test.csv"")"
CHECKPOINT ASSIGN = (train_data.dtypes == 'object') ASSIGN = list(s[s].index) print() print(ASSIGN),1,stream,"# Get list of categorical variables
 s = (train_data.dtypes == 'object')
 object_cols = list(s[s].index)
 
 print(""Categorical variables:"")
 print(object_cols)"
"ASSIGN=['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked'] X=train_data[ASSIGN] ASSIGN=train_data[""Survived""] ASSIGN=test_data[feature_name]",1,not_existent,"feature_name=['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']
 X=train_data[feature_name]
 y=train_data[""Survived""]
 X_test=test_data[feature_name]"
"SETUP X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)",1,not_existent,"from sklearn.model_selection import train_test_split
 X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
"SETUP ASSIGN=SimpleImputer(strategy=""most_frequent"") ASSIGN= pd.DataFrame(my_imputer.fit_transform(X_train)) ASSIGN=pd.DataFrame(my_imputer.transform(X_test)) ASSIGN=pd.DataFrame(my_imputer.transform(X_valid)) ASSIGN.index = X_train.index ASSIGN.index = X_valid.index ASSIGN.index = X_test.index ASSIGN.columns=X_train.columns ASSIGN.columns=X_valid.columns ASSIGN.columns=X_test.columns",1,not_existent,"from sklearn.impute import SimpleImputer
 my_imputer=SimpleImputer(strategy=""most_frequent"")
 imputed_X_train= pd.DataFrame(my_imputer.fit_transform(X_train))
 imputed_X_test=pd.DataFrame(my_imputer.transform(X_test))
 imputed_X_valid=pd.DataFrame(my_imputer.transform(X_valid))
 imputed_X_train.index = X_train.index
 imputed_X_valid.index = X_valid.index
 imputed_X_test.index = X_test.index
 imputed_X_train.columns=X_train.columns
 imputed_X_valid.columns=X_valid.columns
 imputed_X_test.columns=X_test.columns"
CHECKPOINT Col_with_missing_2 = [col for col in imputed_X_test.columns if imputed_X_test[col].isnull().any()] print(Col_with_missing_2),1,stream,"Col_with_missing_2 = [col for col in imputed_X_test.columns if imputed_X_test[col].isnull().any()]
 print(Col_with_missing_2)"
"ASSIGN = imputed_X_train['Sex'] + ""_"" + imputed_X_train['Embarked'] ASSIGN = imputed_X_valid['Sex'] + ""_"" + imputed_X_valid['Embarked'] ASSIGN = imputed_X_test['Sex'] + ""_"" + imputed_X_test['Embarked']",1,not_existent,"# Feature Generation
 New_feature_train = imputed_X_train['Sex'] + ""_"" + imputed_X_train['Embarked']
 New_feature_valid = imputed_X_valid['Sex'] + ""_"" + imputed_X_valid['Embarked']
 New_feature_test = imputed_X_test['Sex'] + ""_"" + imputed_X_test['Embarked']"
"imputed_X_train[""Sex_Embarked""]=New_feature_train imputed_X_valid[""Sex_Embarked""]=New_feature_valid imputed_X_test[""Sex_Embarked""]=New_feature_test",1,not_existent,"imputed_X_train[""Sex_Embarked""]=New_feature_train
 imputed_X_valid[""Sex_Embarked""]=New_feature_valid
 imputed_X_test[""Sex_Embarked""]=New_feature_test"
imputed_X_test.head(),0,execute_result,imputed_X_test.head()
"ASSIGN=['Sex','Embarked','Sex_Embarked']",1,not_existent,"Cat_cols=['Sex','Embarked','Sex_Embarked']"
"ASSIGN = ASSIGN.apply(pd.to_numeric) ASSIGN = ASSIGN.apply(pd.to_numeric) ASSIGN = ASSIGN.apply(pd.to_numeric) ASSIGN=ASSIGN.rename(columns={0:""Sex1"", 1:""Sex2""}) ASSIGN=ASSIGN.rename(columns={2:""C"", 3:""Q"",4:""S""}) ASSIGN=ASSIGN.rename(columns={0:""Sex1"", 1:""Sex2""}) ASSIGN=ASSIGN.rename(columns={2:""C"", 3:""Q"",4:""S""}) ASSIGN=ASSIGN.rename(columns={0:""Sex1"", 1:""Sex2""}) ASSIGN=ASSIGN.rename(columns={2:""C"", 3:""Q"",4:""S""})",1,not_existent,"OH_X_train = OH_X_train.apply(pd.to_numeric)
 OH_X_valid = OH_X_valid.apply(pd.to_numeric)
 OH_X_test = OH_X_test.apply(pd.to_numeric)
 OH_X_train=OH_X_train.rename(columns={0:""Sex1"", 1:""Sex2""})
 OH_X_train=OH_X_train.rename(columns={2:""C"", 3:""Q"",4:""S""})
 OH_X_valid=OH_X_valid.rename(columns={0:""Sex1"", 1:""Sex2""})
 OH_X_valid=OH_X_valid.rename(columns={2:""C"", 3:""Q"",4:""S""})
 OH_X_test=OH_X_test.rename(columns={0:""Sex1"", 1:""Sex2""})
 OH_X_test=OH_X_test.rename(columns={2:""C"", 3:""Q"",4:""S""})"
"SETUP CHECKPOINT ASSIGN = XGBClassifier(n_estimators=1000, learning_rate=0.001) ASSIGN.fit(OH_X_train, y_train, early_stopping_rounds=50, ASSIGN=[(OH_X_valid, y_valid)], verbose=False) ASSIGN.fit(OH_X_train, y_train) ASSIGN = my_model.predict(OH_X_valid) print(,metrics.accuracy_score(y_valid, ASSIGN))",0,stream,"from xgboost import XGBClassifier
 from sklearn import metrics
 my_model = XGBClassifier(n_estimators=1000, learning_rate=0.001)
 my_model.fit(OH_X_train, y_train, early_stopping_rounds=50, 
              eval_set=[(OH_X_valid, y_valid)], verbose=False)
 my_model.fit(OH_X_train, y_train)
 y_pred5 = my_model.predict(OH_X_valid)
 print(""Accuracy:"",metrics.accuracy_score(y_valid, y_pred5))"
"CHECKPOINT ASSIGN = my_model.predict(OH_X_test) ASSIGN = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions2}) ASSIGN.to_csv('my_submission_02_06.csv', index=False) print()",0,stream,"predictions2 = my_model.predict(OH_X_test)
 
 output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions2})
 output.to_csv('my_submission_02_06.csv', index=False)
 print(""Your submission was successfully saved!"")"
SETUP CHECKPOINT print(os.listdir()),0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed
 # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
 # For example, here's several helpful packages to load in 
 
 import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
 
 # Input data files are available in the ""../input/"" directory.
 # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
 
 import os
 import random
 import math
 print(os.listdir(""../input""))
 
 # Any results you write to the current directory are saved as output."
ASSIGN = [] ASSIGN = [],0,not_existent,"chromosome = []
 fitval = []
"
"def generateCromosome(): chromosome.clear() for i in range(4): ASSIGN = [] for j in range(6): ASSIGN.append(random.randint(0,1)) chromosome.append(ASSIGN) print (""Generated chromosome = "",chromosome)",1,not_existent,"def generateCromosome():
     chromosome.clear()
     for i in range(4):
         singleChromosome = []
         for j in range(6):
             singleChromosome.append(random.randint(0,1))
         chromosome.append(singleChromosome)
 
     print (""Generated chromosome = "",chromosome)
"
"def evaluateSolution(): fitval.clear() for i in range(4): ASSIGN = 0 for j in range(1,6): ASSIGN += math.pow(2,5-j)*chromosome[i][j] if chromosome[i][0] == 1: ASSIGN = - ASSIGN fitval.append(ASSIGN) print (""Fitval = "",fitval)",1,not_existent,"def evaluateSolution():
     fitval.clear()
     for i in range(4):
         val = 0
         for j in range(1,6):
             val += math.pow(2,5-j)*chromosome[i][j]
         if chromosome[i][0] == 1:
             val = - val
         fitval.append(val)
     print (""Fitval = "",fitval)
"
def func(x): return -(x*x)+5,0,not_existent,"def func(x):
     return -(x*x)+5"
"CHECKPOINT def selection(): ASSIGN = [[0 for i in range(2)] for j in range(4)] print (ASSIGN) ASSIGN=-1 ASSIGN=-1 for i in range(4): ASSIGN[i][0]=func(fitval[i]) ASSIGN[i][1]=i ASSIGN=sorted(ASSIGN,key=lambda l:l[0], reverse=True) ASSIGN=ftval2[0][1] ASSIGN=ftval2[1][1] ASSIGN = ftval2[0][0] print(,ASSIGN) return c1, c2, bstval",1,not_existent,"def selection():
     ftval2 = [[0 for i in range(2)] for j in range(4)]
     print (ftval2)
     c1=-1
     c2=-1
     for i in range(4):
         ftval2[i][0]=func(fitval[i])
         ftval2[i][1]=i
         
     ftval2=sorted(ftval2,key=lambda l:l[0], reverse=True)
     
     c1=ftval2[0][1]
     c2=ftval2[1][1]
     
     bstval = ftval2[0][0]
     print(""Bestval = "",bstval)
     return c1, c2, bstval
"
"CHECKPOINT def crossover(s1,s2): ASSIGN = random.randint(0,5) for i in range(ASSIGN, 6): chromosome[s1][i],chromosome[s2][i] = chromosome[s2][i],chromosome[s1][i] print(,ASSIGN,,c1,c2,,chromosome)",1,not_existent,"def crossover(s1,s2):
     select = random.randint(0,5)
     
     for i in range(select, 6):
         chromosome[s1][i],chromosome[s2][i] = chromosome[s2][i],chromosome[s1][i]
     print(""For crossover, Selected = "",select,"", Chromose no = "",c1,c2,"", After Crossover = "",chromosome)
"
"CHECKPOINT def mutation(): ASSIGN = random.randint(1,50) ASSIGN == 30: ASSIGN = random.randint(0,3) ASSIGN = random.randint(0,5) chromosome[ASSIGN][ASSIGN] = 1 - chromosome[ASSIGN][ASSIGN] print(,ASSIGN,,ASSIGN)",1,not_existent,"def mutation():
     select = random.randint(1,50)
     if select == 30:
         select2 = random.randint(0,3)
         select3 = random.randint(0,5)
         chromosome[select2][select3] = 1 - chromosome[select2][select3]
         print(""Mutation occured at, Chromosome = "",select2,"", Position = "",select3)
     "
"CHECKPOINT generateCromosome() for i in range(1000): print(,i+1) evaluateSolution() c1,c2,bstval=selection() ASSIGN==5.0: break crossover(c1,c2) mutation()",0,stream,"generateCromosome()
 for i in range(1000):
     print(""Iteration"",i+1)
     evaluateSolution()
     c1,c2,bstval=selection()
     if bstval==5.0:
         break
     crossover(c1,c2)
     mutation()
 
"
"github.head(""sample_commits"")",0,not_existent,"# print the first couple rows of the ""sample_commits"" table github.head(""sample_commits"")"
"ASSIGN = ("""""" WITH repolist AS ( SELECT repo_name FROM `bigquery-public-data.github_repos.sample_files` WHERE path LIKE '%.py' GROUP BY repo_name ) SELECT sf.repo_name, COUNT(sc.commit) AS number_of_commits FROM repolist as sf INNER JOIN `bigquery-public-data.github_repos.sample_commits` as sc ON sc.repo_name = sf.repo_name GROUP BY sf.repo_name ORDER BY number_of_commits DESC """""") github.estimate_query_size(ASSIGN)",1,not_existent,"# You can use two dashes (--) to add comments in SQL query_commit = (""""""         WITH repolist AS              (                 SELECT repo_name                 FROM  `bigquery-public-data.github_repos.sample_files`                 WHERE path LIKE '%.py'                 GROUP BY repo_name             )         SELECT sf.repo_name, COUNT(sc.commit) AS number_of_commits         FROM          repolist as sf          INNER JOIN `bigquery-public-data.github_repos.sample_commits` as sc             ON sc.repo_name = sf.repo_name          GROUP BY sf.repo_name         ORDER BY number_of_commits DESC         """""")  # check how big this query will be github.estimate_query_size(query_commit)"
"CHECKPOINT ASSIGN = github.query_to_pandas_safe(query_commit, max_gb_scanned=6) python_commits",0,not_existent,"python_commits = github.query_to_pandas_safe(query_commit, max_gb_scanned=6) python_commits"
SETUP CHECKPOINT binder.bind(globals()) print(),0,stream,"# Set up feedback system
 from learntools.core import binder
 binder.bind(globals())
 from learntools.sql.ex3 import *
 print(""Setup Complete"")"
"SETUP ASSIGN = bigquery.Client() ASSIGN = client.dataset(""hacker_news"", project=""bigquery-public-data"") ASSIGN = client.get_dataset(dataset_ref) ASSIGN = dataset_ref.table(""comments"") ASSIGN = client.get_table(table_ref) ASSIGN.list_rows(ASSIGN, max_results=5).to_dataframe()",1,stream,"from google.cloud import bigquery
 
 # Create a ""Client"" object
 client = bigquery.Client()
 
 # Construct a reference to the ""hacker_news"" dataset
 dataset_ref = client.dataset(""hacker_news"", project=""bigquery-public-data"")
 
 # API request - fetch the dataset
 dataset = client.get_dataset(dataset_ref)
 
 # Construct a reference to the ""comments"" table
 table_ref = dataset_ref.table(""comments"")
 
 # API request - fetch the table
 table = client.get_table(table_ref)
 
 # Preview the first five lines of the ""comments"" table
 client.list_rows(table, max_results=5).to_dataframe()"
SETUP,0,not_existent,"import matplotlib.pyplot as plt
 import seaborn as sns"
ASSIGN = pd.read_csv('..path') ASSIGN.head(),0,execute_result,"df = pd.read_csv('../input/attendancemarks/AttendanceMarksSA.csv')
 df.head()"
"ASSIGN = df['MSE'] ASSIGN = df['ESE'] sns.scatterplot(ASSIGN,ASSIGN)",0,execute_result,"x = df['MSE']
 y = df['ESE']
 sns.scatterplot(x,y)"
ASSIGN = 0 ASSIGN = 0 ASSIGN = 0.01 ASSIGN = 10000 ASSIGN = float(len(x)),0,not_existent,"b0 = 0
 b1 = 0
 alpha = 0.01
 count = 10000
 n = float(len(x))"
"CHECKPOINT for i in range(count): ASSIGN = b1*x + b0 ASSIGN = ASSIGN - (alphapath)*sum(x*(y_bar-y)) ASSIGN = ASSIGN - (alphapath)*sum(y_bar-y) print(ASSIGN,ASSIGN)",1,stream,"for i in range(count):
     y_bar = b1*x + b0
     b1 = b1 - (alpha/n)*sum(x*(y_bar-y))
     b0 = b0 - (alpha/n)*sum(y_bar-y)
         
 print(b0,b1)"
"ASSIGN = b1*x + b0 plt.scatter(x,y) plt.plot([min(x),max(x)],[min(ASSIGN),max(ASSIGN)],color='red') plt.show()",0,display_data,"y_bar = b1*x + b0
 
 plt.scatter(x,y)
 plt.plot([min(x),max(x)],[min(y_bar),max(y_bar)],color='red') #regression line
 plt.show()"
"SETUP SETUP CHECKPOINT def RSE(y_true,y_predict): ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = math.sqrt(RSSpath(len(y_true)-2)) return rse ASSIGN = RSE(df['ESE'],y_bar) print(ASSIGN)",1,stream,"import math
 def RSE(y_true,y_predict):
     y_true = np.array(y_true)
     y_predict = np.array(y_predict)
     RSS = np.sum(np.square(y_true-y_predict))
     
     rse = math.sqrt(RSS/(len(y_true)-2))
     return rse
 
 rse = RSE(df['ESE'],y_bar)
 print(rse)"
SETUP,0,not_existent,"from sklearn.model_selection import train_test_split
 from sklearn.linear_model import LinearRegression"
"CHECKPOINT ASSIGN = np.array(df['MSE']).reshape(-1,1) ASSIGN = np.array(df['ESE']).reshape(-1,1) ASSIGN = LinearRegression() ASSIGN.fit(ASSIGN,ASSIGN) print(ASSIGN.coef_) print(ASSIGN.intercept_) ASSIGN = lr.predict(X) ASSIGN = RSE(Y,yp) print(ASSIGN)",1,stream,"X = np.array(df['MSE']).reshape(-1,1)
 Y = np.array(df['ESE']).reshape(-1,1)
 
 lr = LinearRegression()
 lr.fit(X,Y)
 
 print(lr.coef_)
 print(lr.intercept_)
 
 yp = lr.predict(X)
 rse = RSE(Y,yp)
 
 print(rse)"
SETUP,0,stream,!pip install pycaret==2.0
SETUP,0,not_existent,import random import numpy as np import pandas as pd import matplotlib.pyplot as plt from pycaret.classification import * from sklearn.model_selection import train_test_split
ASSIGN=10000,0,not_existent,sampleNumber=10000
"ASSIGN = { 'x1': np.random.randint(0,3,sampleNumber) , 'x2': np.random.randint(0,3,sampleNumber) , 'x3': np.random.randint(0,2,sampleNumber) , 'x4': np.random.randint(0,3,sampleNumber) , 'x5': np.random.randint(0,2,sampleNumber) , 'x6': np.random.randint(0,2,sampleNumber) , 'y' : np.zeros(sampleNumber, dtype=bool) } ASSIGN = pd.DataFrame(ASSIGN=d) SLICE=(SLICE + SLICE + SLICE + SLICE + SLICE+ SLICE)>4 X=ASSIGN[['x1','x2','x3','x4','x5','x6']] ASSIGN=data[[""ASSIGN""]]",1,not_existent," d = {     'x1': np.random.randint(0,3,sampleNumber) ,#Make sacrifices     'x2': np.random.randint(0,3,sampleNumber) ,#Punctuality     'x3': np.random.randint(0,2,sampleNumber) ,#Not feeling bored while you are together     'x4': np.random.randint(0,3,sampleNumber) ,#You evaluate gifts     'x5': np.random.randint(0,2,sampleNumber) ,#Take care of my problems     'x6': np.random.randint(0,2,sampleNumber) ,#Rai respect      'y' : np.zeros(sampleNumber, dtype=bool)     } data  = pd.DataFrame(data=d)     data[""y""]=(data['x1'] +   data['x2'] +    data['x3'] +   data['x4'] +   data['x5']+    data['x6'])>4   X=data[['x1','x2','x3','x4','x5','x6']] y=data[[""y""]]  "
"X_train, X_test, y_train, y_test = train_test_split( ASSIGN=.20, random_state=42)",1,not_existent,"X_train, X_test, y_train, y_test = train_test_split(         X, y, test_size=.20, random_state=42)"
"ASSIGN=X_train ASSIGN[""y""]=y_train del X_train del y_train",1,stream,"train_Data=X_train train_Data[""y""]=y_train del X_train del y_train"
"ASSIGN= setup(data = train_Data, target = ""y"")",1,stream," clf= setup(data = train_Data, target = ""y"")"
compare_models(),0,display_data,compare_models()
ASSIGN = create_model('ASSIGN'),0,display_data,lr = create_model('lr')
"plot_model(lr,""confusion_matrix"")",0,display_data,"plot_model(lr,""confusion_matrix"")"
evaluate_model(lr),0,display_data,evaluate_model(lr)
"ASSIGN = predict_model(lr, data = X_test)",0,not_existent,"lr_pred = predict_model(lr, data = X_test) #new_data is pd dataframe"
CHECKPOINT lr_pred,0,execute_result,lr_pred
"y_test.reset_index(drop=True, inplace=True)",1,not_existent,"y_test.reset_index(drop=True, inplace=True)"
CHECKPOINT y_test,0,execute_result,y_test
"ASSIGN=ASSIGN.astype(""int"").values.T",1,not_existent,"y_test=y_test.astype(""int"").values.T"
"CHECKPOINT ASSIGN = np.nonzero(lr_pred[""Label""].values==y_test)[0] ASSIGN = np.nonzero(lr_pred[""Label""].values!=y_test)[0] print(len(ASSIGN),) print(len(ASSIGN),)",1,stream,"correct_predictions = np.nonzero(lr_pred[""Label""].values==y_test)[0] incorrect_predictions = np.nonzero(lr_pred[""Label""].values!=y_test)[0] print(len(correct_predictions),"" classified correctly"") print(len(incorrect_predictions),"" classified incorrectly"")"
SETUP,0,not_existent,import cv2 as cv import numpy as np import matplotlib.pyplot as plt import matplotlib.image as mpimg from matplotlib.patches import Rectangle 
"plt.figure(figsize=(20, 20)) plt.title(""Original"") plt.imshow(mpimg.imread('..path')) plt.show()",0,display_data,"plt.figure(figsize=(20, 20)) plt.title(""Original"") plt.imshow(mpimg.imread('../input/opencv-samples-images/WaldoBeach.jpg')) plt.show() "
"ASSIGN = cv.imread('..path',0) ASSIGN =img[500:650, 500:600] plt.imshow(ASSIGN,cmap = 'gray') plt.title('ASSIGN'), plt.xticks([]), plt.yticks([]) plt.show()",0,display_data,"  img = cv.imread('../input/opencv-samples-images/WaldoBeach.jpg',0)  template =img[500:650, 500:600] # template =img[500:650, 200:300] plt.imshow(template,cmap = 'gray') plt.title('template'), plt.xticks([]), plt.yticks([])  plt.show() "
"ASSIGN = img.copy() ASSIGN = template.shape[::-1] ASSIGN = ['cv.TM_CCOEFF', 'cv.TM_CCOEFF_NORMED', 'cv.TM_CCORR', 'cv.TM_CCORR_NORMED', 'cv.TM_SQDIFF', 'cv.TM_SQDIFF_NORMED'] plt.figure(figsize=(20, 20)) plt.imshow(mpimg.imread('..path')) plt.title('Detected Point') ASSIGN=[] for meth in ASSIGN: ASSIGN = img2.copy() ASSIGN = eval(meth) ASSIGN = cv.matchTemplate(img,template,method) ASSIGN = cv.minMaxLoc(res) if ASSIGN in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]: ASSIGN = min_loc else: ASSIGN = max_loc ASSIGN+=[(meth,ASSIGN,ASSIGN)] ASSIGN = plt.gca() ASSIGN.add_patch( Rectangle(ASSIGN, ASSIGN, ASSIGN ='none', ASSIGN ='b', ASSIGN = 4) ) plt.show()",0,display_data,"img2 = img.copy() w, h = template.shape[::-1] # All the 6 methods for comparison in a list methods = ['cv.TM_CCOEFF', 'cv.TM_CCOEFF_NORMED', 'cv.TM_CCORR',             'cv.TM_CCORR_NORMED', 'cv.TM_SQDIFF', 'cv.TM_SQDIFF_NORMED'] plt.figure(figsize=(20, 20)) plt.imshow(mpimg.imread('../input/opencv-samples-images/WaldoBeach.jpg')) plt.title('Detected Point') result=[]  for meth in methods:     img = img2.copy()     method = eval(meth)     # Apply template Matching     res = cv.matchTemplate(img,template,method)     min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)     # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum     if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:         top_left = min_loc     else:         top_left = max_loc      result+=[(meth,top_left,w,   h)]       ax = plt.gca()     ax.add_patch( Rectangle(top_left,                         w,   h,                         fc ='none',                           ec ='b',                          lw = 4) )        plt.show()     "
"ASSIGN=160 plt.figure(figsize=(20, 20)) for r in result: ASSIGN+=1 plt.subplot(ASSIGN),plt.imshow(res,cmap = 'gray') ASSIGN =img[ r[1][1]:r[1][1]+r[3], r[1][0]: r[1][0]+r[2]] plt.imshow(ASSIGN,cmap = 'gray') plt.title(r[0]), plt.xticks([]), plt.yticks([]) plt.show()",0,display_data,"index=160 plt.figure(figsize=(20, 20)) for r in result:     index+=1     plt.subplot(index),plt.imshow(res,cmap = 'gray')     template =img[ r[1][1]:r[1][1]+r[3], r[1][0]: r[1][0]+r[2]]     plt.imshow(template,cmap = 'gray')     plt.title(r[0]), plt.xticks([]), plt.yticks([])  plt.show()"
"SETUP CHECKPOINT ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'], ASSIGN.loc[:,'MSSubClass':'SaleCondition'])) ASSIGN = np.log1p(ASSIGN) ASSIGN = all_data.dtypes[all_data.dtypes != ""object""].index ASSIGN = train[numeric_feats].apply(lambda x: skew(x.dropna())) ASSIGN = ASSIGN[ASSIGN > 0.75] ASSIGN = ASSIGN.index ASSIGN[ASSIGN] = np.log1p(ASSIGN[ASSIGN]) ASSIGN = pd.get_dummies(ASSIGN) ASSIGN = ASSIGN.fillna(ASSIGN.mean()) ASSIGN = all_data[:train.shape[0]] ASSIGN = all_data[train.shape[0]:] ASSIGN = train.SalePrice def rmse_cv(model): ASSIGN= np.sqrt(-cross_val_score(model, X_train, y, scoring=""neg_mean_squared_error"", cv = 5)) return(ASSIGN) ASSIGN = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y) rmse_cv(ASSIGN).mean() ASSIGN = pd.Series(model_lasso.coef_, index = X_train.columns) print( + str(sum(ASSIGN != 0)) + + str(sum(ASSIGN == 0)) + ) ASSIGN = np.expm1(model_lasso.predict(X_test)) ASSIGN = pd.DataFrame({""id"":test.Id, ""SalePrice"":lasso_preds}) ASSIGN.to_csv(""ridge_sol.csv"", index = False)",0,stream,"import pandas as pd
 import numpy as np
 from scipy.stats import skew
 
 train = pd.read_csv(""../input/train.csv"")
 test = pd.read_csv(""../input/test.csv"")
 
 all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],
                       test.loc[:,'MSSubClass':'SaleCondition']))
 
 train[""SalePrice""] = np.log1p(train[""SalePrice""])
 numeric_feats = all_data.dtypes[all_data.dtypes != ""object""].index
 
 skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna()))
 skewed_feats = skewed_feats[skewed_feats > 0.75]
 skewed_feats = skewed_feats.index
 
 all_data[skewed_feats] = np.log1p(all_data[skewed_feats])
 all_data = pd.get_dummies(all_data)
 all_data = all_data.fillna(all_data.mean())
 
 X_train = all_data[:train.shape[0]]
 X_test = all_data[train.shape[0]:]
 y = train.SalePrice
 
 from sklearn.linear_model import LassoCV
 from sklearn.model_selection import cross_val_score
 
 def rmse_cv(model):
     rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=""neg_mean_squared_error"", cv = 5))
     return(rmse)
 
 model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)
 rmse_cv(model_lasso).mean()
 
 coef = pd.Series(model_lasso.coef_, index = X_train.columns)
 print(""Lasso picked "" + str(sum(coef != 0)) + "" variables and eliminated the other "" +  str(sum(coef == 0)) + "" variables"")
 
 lasso_preds = np.expm1(model_lasso.predict(X_test))
 
 solution = pd.DataFrame({""id"":test.Id, ""SalePrice"":lasso_preds})
 solution.to_csv(""ridge_sol.csv"", index = False)"
SETUP,0,not_existent,"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
"class SimpleLinearRegression: ASSIGN = 0 ASSIGN = 0 ASSIGN = 0 def fit(self, x_train, y_train): ASSIGN = sum(x_train) ASSIGN = sum(y_train) ASSIGN = np.sum(np.square(x_train)) ASSIGN = np.sum(np.square(y_train)) ASSIGN = np.dot(x_train,y_train) ASSIGN = len(x_train) ASSIGN = sum_of_x2 - sum_of_x * sum_of_xpath ASSIGN = sum_of_y2 - sum_of_y * sum_of_ypath ASSIGN = length * dotproduct - sum_of_x * sum_of_y ASSIGN = (length * sum_of_x2 - sum_of_x * sum_of_x) * (length * sum_of_y2 - (sum_of_y * sum_of_y)) ASSIGN = dotproduct - sum_of_x * sum_of_y path self.ASSIGN = np.square(ASSIGN path(ASSIGN)) self.ASSIGN = ASSIGN path((ASSIGN path) * sum_of_xpath) self.ASSIGN = ASSIGN path def predict(self,x_test): return x_test * self.coef + self.intercept",1,not_existent,"class SimpleLinearRegression:     coef = 0     intercept = 0     rsquared = 0     def fit(self, x_train, y_train):         sum_of_x = sum(x_train)         sum_of_y = sum(y_train)         sum_of_x2 = np.sum(np.square(x_train))         sum_of_y2 = np.sum(np.square(y_train))         dotproduct = np.dot(x_train,y_train)         length = len(x_train)         dif_x = sum_of_x2 - sum_of_x * sum_of_x/length         dif_y = sum_of_y2 - sum_of_y * sum_of_y/length         numerator = length * dotproduct - sum_of_x * sum_of_y         denom = (length * sum_of_x2 - sum_of_x * sum_of_x) * (length * sum_of_y2 - (sum_of_y * sum_of_y))         co = dotproduct - sum_of_x * sum_of_y / length         self.rsquared = np.square(numerator / np.sqrt(denom))         self.intercept = sum_of_y / length - ((co / dif_x) * sum_of_x/length)         self.coef = co / dif_x     def predict(self,x_test):         return x_test * self.coef + self.intercept         "
"ASSIGN = np.array([ 1, 2, 3, 4]) ASSIGN = np.array([ 2, 3, 4, 4])",1,not_existent,"x_train = np.array([ 1, 2, 3, 4]) y_train = np.array([ 2, 3, 4, 4])"
"ASSIGN = SimpleLinearRegression() ASSIGN.fit(x_train,y_train)",0,not_existent,"slr = SimpleLinearRegression() slr.fit(x_train,y_train)"
"CHECKPOINT print(, slr.coef) print('Y-Intercept:',slr.intercept) print('R-Squared:',slr.rsquared)",0,not_existent,"print(""Coefficient:"", slr.coef) print('Y-Intercept:',slr.intercept) print('R-Squared:',slr.rsquared)"
"SETUP CHECKPOINT ASSIGN = LinearRegression() ASSIGN.fit(x_train.reshape(-1,1), y_train.reshape(-1,1)) print(ASSIGN.coef_) print(ASSIGN.intercept_)",0,not_existent,"from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(x_train.reshape(-1,1), y_train.reshape(-1,1)) print(lr.coef_) print(lr.intercept_)"
SETUP CHECKPOINT print(os.listdir()),0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed
 # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
 # For example, here's several helpful packages to load in 
 
 import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
 
 # Input data files are available in the ""../input/"" directory.
 # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
 
 import os
 print(os.listdir(""../input""))
 
 # Any results you write to the current directory are saved as output."
"SETUP ASSIGN = pd.read_csv( ASSIGN='..path', ASSIGN=None, ASSIGN=',') ASSIGN.columns=['A', 'B', 'C', 'D', 'class'] ASSIGN.dropna(how=""all"", inplace=True) # drops the empty line at file-end ASSIGN.tail()",1,execute_result,"import pandas as pd
 
 df = pd.read_csv(
     filepath_or_buffer='../input/Seed_Data.csv',
     header=None,
     sep=',')
 
 df.columns=['A', 'B', 'C', 'D', 'class']
 df.dropna(how=""all"", inplace=True) # drops the empty line at file-end
 
 df.tail()"
"ASSIGN = df.ix[:,0:4].values ASSIGN = df.ix[:,4].values",1,stream,"# split data table into data X and class labels y
 
 X = df.ix[:,0:4].values
 y = df.ix[:,4].values
 
"
CHECKPOINT X,0,execute_result,X
CHECKPOINT y,0,execute_result,y
"SETUP ASSIGN = {1: 'type0', 2: 'type1', 3: 'type2'} ASSIGN = {0: 'A', 1: 'B', 2: 'C', 3: 'D'} with plt.style.context('seaborn-whitegrid'): plt.figure(figsize=(8, 6)) for cnt in range(4): plt.subplot(2, 2, cnt+1) for lab in ('type0', 'type1', 'type2'): plt.hist(X[y==lab, cnt], ASSIGN=lab, ASSIGN=10, ASSIGN=0.3,) plt.xlabel(ASSIGN[cnt]) plt.legend(loc='upper right', fancybox=True, fontsize=8) plt.tight_layout() plt.show()",0,display_data,"from matplotlib import pyplot as plt
 import numpy as np
 import math
 
 label_dict = {1: 'type0',
               2: 'type1',
               3: 'type2'}
 
 feature_dict = {0: 'A',
                 1: 'B',
                 2: 'C',
                 3: 'D'}
 
 with plt.style.context('seaborn-whitegrid'):
     plt.figure(figsize=(8, 6))
     for cnt in range(4):
         plt.subplot(2, 2, cnt+1)
         for lab in ('type0', 'type1', 'type2'):
             plt.hist(X[y==lab, cnt],
                      label=lab,
                      bins=10,
                      alpha=0.3,)
         plt.xlabel(feature_dict[cnt])
     plt.legend(loc='upper right', fancybox=True, fontsize=8)
 
     plt.tight_layout()
     plt.show()"
SETUP ASSIGN = StandardScaler().fit_transform(X),1,not_existent,"from sklearn.preprocessing import StandardScaler
 X_std = StandardScaler().fit_transform(X)"
CHECKPOINT X_std,0,execute_result,X_std
"SETUP CHECKPOINT ASSIGN = np.mean(X_std, axis=0) ASSIGN = (X_std - mean_vec).T.dot((X_std - mean_vec)) path(X_std.shape[0]-1) print('Covariance matrix \n%s' %ASSIGN)",1,stream,"import numpy as np
 mean_vec = np.mean(X_std, axis=0)
 cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)
 print('Covariance matrix \n%s' %cov_mat)"
CHECKPOINT ASSIGN = np.cov(X_std.T) ASSIGN = np.linalg.eig(cov_mat) print('Eigenvectors \n%s' %eig_vecs) print('\nEigenvalues \n%s' %eig_vals),1,stream,"cov_mat = np.cov(X_std.T)
 
 eig_vals, eig_vecs = np.linalg.eig(cov_mat)
 
 print('Eigenvectors \n%s' %eig_vecs)
 print('\nEigenvalues \n%s' %eig_vals)"
"CHECKPOINT ASSIGN = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))] ASSIGN.sort(key=lambda x: x[0], reverse=True) print('Eigenvalues in descending order:') for i in ASSIGN: print(i[0])",1,stream,"# Make a list of (eigenvalue, eigenvector) tuples
 eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]
 
 # Sort the (eigenvalue, eigenvector) tuples from high to low
 eig_pairs.sort(key=lambda x: x[0], reverse=True)
 
 # Visually confirm that the list is correctly sorted by decreasing eigenvalues
 print('Eigenvalues in descending order:')
 for i in eig_pairs:
     print(i[0])"
"ASSIGN = sum(eig_vals) ASSIGN = [(i path)*100 for i in sorted(eig_vals, reverse=True)] ASSIGN = np.cumsum(var_exp)",1,not_existent,"tot = sum(eig_vals)
 var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]
 cum_var_exp = np.cumsum(var_exp)"
"with plt.style.context('seaborn-whitegrid'): plt.figure(figsize=(6, 4)) plt.bar(range(4), var_exp, alpha=0.5, align='center', ASSIGN='individual explained variance') plt.step(range(4), cum_var_exp, where='mid', ASSIGN='cumulative explained variance') plt.ylabel('Explained variance ratio') plt.xlabel('Principal components') plt.legend(loc='best') plt.tight_layout()",0,display_data,"with plt.style.context('seaborn-whitegrid'):
     plt.figure(figsize=(6, 4))
 
     plt.bar(range(4), var_exp, alpha=0.5, align='center',
             label='individual explained variance')
     plt.step(range(4), cum_var_exp, where='mid',
              label='cumulative explained variance')
     plt.ylabel('Explained variance ratio')
     plt.xlabel('Principal components')
     plt.legend(loc='best')
     plt.tight_layout()"
"CHECKPOINT ASSIGN = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1))) print('Matrix W:\n', ASSIGN)",1,stream,"matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1),
                       eig_pairs[1][1].reshape(4,1)))
 
 print('Matrix W:\n', matrix_w)"
ASSIGN = X_std.dot(matrix_w),1,not_existent,Y = X_std.dot(matrix_w)
"with plt.style.context('seaborn-whitegrid'): plt.figure(figsize=(6, 4)) for lab, col in zip(('type0', 'type1', 'type2'), ('blue', 'red', 'green')): plt.scatter(Y[y==lab, 0], Y[y==lab, 1], ASSIGN=lab, ASSIGN=col) plt.xlabel('Principal Component 1') plt.ylabel('Principal Component 2') plt.legend(loc='lower center') plt.tight_layout() plt.show()",0,display_data,"with plt.style.context('seaborn-whitegrid'):
     plt.figure(figsize=(6, 4))
     for lab, col in zip(('type0', 'type1', 'type2'),
                         ('blue', 'red', 'green')):
         plt.scatter(Y[y==lab, 0],
                     Y[y==lab, 1],
                     label=lab,
                     c=col)
     plt.xlabel('Principal Component 1')
     plt.ylabel('Principal Component 2')
     plt.legend(loc='lower center')
     plt.tight_layout()
     plt.show()"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed
 # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
 # For example, here's several helpful packages to load in 
 
 import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
 
 # Input data files are available in the ""../input/"" directory.
 # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
 
 import os
 for dirname, _, filenames in os.walk('/kaggle/input/'):
     for filename in filenames:
         print(os.path.join(dirname, filename))"
SETUP ASSIGN = pd.read_csv('path') ASSIGN.head(),0,execute_result,"# Import pandas
 import pandas as pd
 
 # Read in the file content in a DataFrame called discoveries
 discoveries = pd.read_csv('/kaggle/input/week6dataset/discoveries.csv')
 
 # Display the first five lines of the DataFrame
 discoveries.head()"
CHECKPOINT print(discoveries.dtypes) ASSIGN = pd.to_datetime(ASSIGN) print(discoveries.dtypes),1,stream,"# Print the data type of each column in discoveries
 print(discoveries.dtypes)
 
 # Convert the date column to a datestamp type
 discoveries['date'] = pd.to_datetime(discoveries['date'])
 
 # Print the data type of each column in discoveries, again
 print(discoveries.dtypes)"
"SETUP discoveries.set_index('date', inplace = True) ASSIGN = discoveries.plot(color='blue') ASSIGN.set_xlabel('Date') ASSIGN.set_ylabel('Number of great discoveries') plt.show()",0,display_data,"import matplotlib.pyplot as plt
 %matplotlib inline
 import seaborn as sns
 
 # Set the date column as the index of your DataFrame discoveries
 discoveries.set_index('date', inplace = True)
 
 # Plot the time series in your DataFrame
 ax = discoveries.plot(color='blue')
 
 # Specify the x-axis label in your plot
 ax.set_xlabel('Date')
 
 # Specify the y-axis label in your plot
 ax.set_ylabel('Number of great discoveries')
 
 # Show plot
 plt.show()"
SETUP plt.style.use('ggplot') ASSIGN = discoveries.plot() ASSIGN.set_title('ggplot Style') plt.show(),0,display_data,"# Import the matplotlib.pyplot sub-module
 import matplotlib.pyplot as plt
 
 # Use the ggplot style
 plt.style.use('ggplot')
 ax2 = discoveries.plot()
 
 # Set the title
 ax2.set_title('ggplot Style')
 plt.show()"
SETUP plt.style.use('fivethirtyeight') ASSIGN = discoveries.plot() ASSIGN.set_title('FiveThirtyEight Style') plt.show(),0,display_data,"# Import the matplotlib.pyplot sub-module
 import matplotlib.pyplot as plt
 
 # Use the fivethirtyeight style
 plt.style.use('fivethirtyeight')
 
 # Plot the time series
 ax1 = discoveries.plot()
 ax1.set_title('FiveThirtyEight Style')
 plt.show()"
"ASSIGN = discoveries.plot(color='blue',figsize =(8, 3), linewidth=2, fontsize=6) ASSIGN.set_title('Number of great inventions and scientific discoveries from 1860 to 1959', fontsize=8) plt.show()",0,display_data,"# Plot a line chart of the discoveries DataFrame using the specified arguments
 ax = discoveries.plot(color='blue',figsize =(8, 3), linewidth=2, fontsize=6)
 
 # Specify the title in your plot
 ax.set_title('Number of great inventions and scientific discoveries from 1860 to 1959', fontsize=8)
 
 # Show plot
 plt.show()"
"ASSIGN = discoveries ['1939-01-01': '1958-01-01'] ASSIGN = discoveries_subset_2.plot(color='blue', fontsize=15) plt.show()",0,display_data,"# Select the subset of data between 1939 and 1958
 discoveries_subset_2 = discoveries ['1939-01-01': '1958-01-01']
 # Plot the time series in your DataFrame as a blue area chart
 ax = discoveries_subset_2.plot(color='blue', fontsize=15)
 
 # Show plot
 plt.show()"
"ASSIGN = discoveries.plot(color='blue', fontsize=6) ASSIGN.axvline('1939-01-01', color='red', linestyle='--') ASSIGN.axhline(4, color='green', linestyle='--') plt.show()",0,display_data,"# Plot your the discoveries time series
 ax = discoveries.plot(color='blue', fontsize=6)
 
 # Add a red vertical line
 ax.axvline('1939-01-01', color='red', linestyle='--')
 
 # Add a green horizontal line
 ax.axhline(4, color='green', linestyle='--')
 
 plt.show()"
"ASSIGN = discoveries.plot(color='blue', fontsize=6) ASSIGN.axvspan('1900-01-01', '1915-01-01', color='red', alpha=0.3) ASSIGN.axhspan(6, 8, color='green', alpha=0.3) plt.show()",0,display_data,"# Plot your the discoveries time series
 ax = discoveries.plot(color='blue', fontsize=6)
 
 # Add a vertical red shaded region
 ax.axvspan('1900-01-01', '1915-01-01', color='red', alpha=0.3)
 
 # Add a horizontal green shaded region
 ax.axhspan(6, 8, color='green', alpha=0.3)
 
 plt.show()"
"CHECKPOINT ASSIGN = pd.read_csv('path', parse_dates = ['datestamp']) print(ASSIGN)",0,stream,"co2_levels = pd.read_csv('/kaggle/input/week6dataset/co2_levels.csv', parse_dates = ['datestamp'])
 print(co2_levels)"
CHECKPOINT ASSIGN = ASSIGN.set_index('datestamp') print(ASSIGN.isnull().sum()),1,stream,"# Set datestamp column as index
 co2_levels = co2_levels.set_index('datestamp')
 
 # Print out the number of missing values
 print(co2_levels.isnull().sum())"
CHECKPOINT ASSIGN = ASSIGN.fillna(method='bfill') print(ASSIGN.isnull().sum()),1,stream,"# Impute missing values with the next valid observation
 co2_levels = co2_levels.fillna(method='bfill')
 
 # Print out the number of missing values
 print(co2_levels.isnull().sum())"
"ASSIGN = co2_levels.rolling(window=52).mean() ASSIGN = co2_levels.rolling(window=52).std() ASSIGN = ASSIGN + (ASSIGN * 2) ASSIGN = ASSIGN - (ASSIGN * 2) ASSIGN = ma.plot(linewidth=0.8, fontsize=6) ASSIGN.set_xlabel('Date', fontsize=10) ASSIGN.set_ylabel('CO2 levels in Mauai Hawaii', fontsize=10) ASSIGN.set_title('Rolling mean and variance of CO2 levels\nin Mauai Hawaii from 1958 to 2001', fontsize=10) plt.show()",0,display_data,"# Compute the 52 weeks rolling mean of the co2_levels DataFrame
 ma = co2_levels.rolling(window=52).mean()
 
 # Compute the 52 weeks rolling standard deviation of the co2_levels DataFrame
 mstd = co2_levels.rolling(window=52).std()
 
 # Add the upper bound column to the ma DataFrame
 ma['upper'] = ma['co2'] + (mstd['co2'] * 2)
 
 # Add the lower bound column to the ma DataFrame
 ma['lower'] = ma['co2'] - (mstd['co2'] * 2)
 
 # Plot the content of the ma DataFrame
 ax = ma.plot(linewidth=0.8, fontsize=6)
 
 # Specify labels, legend, and show the plot
 ax.set_xlabel('Date', fontsize=10)
 ax.set_ylabel('CO2 levels in Mauai Hawaii', fontsize=10)
 ax.set_title('Rolling mean and variance of CO2 levels\nin Mauai Hawaii from 1958 to 2001', fontsize=10)
 plt.show()"
ASSIGN = co2_levels.index.month ASSIGN = co2_levels.groupby(index_month).mean() ASSIGN.plot(fontsize = 6) plt.legend(fontsize=10) plt.show(),0,display_data,"# Get month for each dates in the index of co2_levels
 index_month = co2_levels.index.month
 
 # Compute the mean CO2 levels for each month of the year
 mean_co2_levels_by_month = co2_levels.groupby(index_month).mean()
 
 # Plot the mean CO2 levels for each month of the year
 mean_co2_levels_by_month.plot(fontsize = 6)
 
 # Specify the fontsize on the legend
 plt.legend(fontsize=10)
 
 # Show plot
 plt.show()"
CHECKPOINT print(co2_levels.describe()) print(co2_levels.co2.min()) print(co2_levels.co2.max()),0,stream,"# Print out summary statistics of the co2_levels DataFrame
 print(co2_levels.describe())
 
 # Print out the minima of the co2 column in the co2_levels DataFrame
 print(co2_levels.co2.min())
 
 # Print out the maxima of the co2 column in the co2_levels DataFrame
 print(co2_levels.co2.max())"
"ASSIGN = co2_levels.plot(kind = 'hist', bins = 50, fontsize=6) ASSIGN.set_xlabel('CO2', fontsize=10) ASSIGN.set_ylabel('Histogram of CO2 levels in Maui Hawaii', fontsize=10) plt.legend(fontsize=10) plt.show()",0,display_data,"# Generate a histogram
 ax = co2_levels.plot(kind = 'hist', bins = 50, fontsize=6)
 
 # Set the labels and display the plot
 ax.set_xlabel('CO2', fontsize=10)
 ax.set_ylabel('Histogram of CO2 levels in Maui Hawaii', fontsize=10)
 plt.legend(fontsize=10)
 plt.show()"
"ASSIGN = co2_levels.plot(kind = 'density', linewidth = 4, fontsize=6) ASSIGN.set_xlabel('CO2', fontsize=10) ASSIGN.set_ylabel('Density plot of CO2 levels in Maui Hawaii', fontsize=10) plt.show()",0,display_data,"# Display density plot of CO2 levels values
 ax = co2_levels.plot(kind = 'density', linewidth = 4, fontsize=6)
 
 # Annotate x-axis labels
 ax.set_xlabel('CO2', fontsize=10)
 
 # Annotate y-axis labels
 ax.set_ylabel('Density plot of CO2 levels in Maui Hawaii', fontsize=10)
 
 plt.show()"
"SETUP plt.style.use('fivethirtyeight') ASSIGN = tsaplots.plot_acf(co2_levels['co2'], lags=24) plt.show()",0,display_data,"# Import required libraries
 import matplotlib.pyplot as plt
 plt.style.use('fivethirtyeight')
 from statsmodels.graphics import tsaplots
 
 # Display the autocorrelation plot of your time series
 fig = tsaplots.plot_acf(co2_levels['co2'], lags=24)
 
 # Show plot
 plt.show()"
"SETUP plt.style.use('fivethirtyeight') ASSIGN = tsaplots.plot_pacf(co2_levels['co2'], lags=24) plt.show()",0,display_data,"# Import required libraries
 import matplotlib.pyplot as plt
 plt.style.use('fivethirtyeight')
 from statsmodels.graphics import tsaplots
 
 # Display the partial autocorrelation plot of your time series
 fig = tsaplots.plot_pacf(co2_levels['co2'], lags=24)
 
 # Show plot
 plt.show()"
SETUP CHECKPOINT ASSIGN = sm.tsa.seasonal_decompose(co2_levels) print(ASSIGN.seasonal),1,stream,"# Import statsmodels.api as sm
 import statsmodels.api as sm
 
 # Perform time series decompositon
 decomposition = sm.tsa.seasonal_decompose(co2_levels)
 
 # Print the seasonality component
 print(decomposition.seasonal)"
"ASSIGN = decomposition.ASSIGN ASSIGN = trend.plot(figsize=(12, 6), fontsize=6) ASSIGN.set_xlabel('Date', fontsize=10) ASSIGN.set_title('Seasonal component the CO2 time-series', fontsize=10) plt.show()",0,display_data,"# Extract the trend component
 trend = decomposition.trend
 
 # Plot the values of the trend
 ax = trend.plot(figsize=(12, 6), fontsize=6)
 
 # Specify axis labels
 ax.set_xlabel('Date', fontsize=10)
 ax.set_title('Seasonal component the CO2 time-series', fontsize=10)
 plt.show()"
"ASSIGN = pd.read_csv('path', parse_dates = ['Month'], index_col = 'Month') ASSIGN.head()",0,execute_result,"airline = pd.read_csv('/kaggle/input/week6dataset/airline_passengers.csv', parse_dates = ['Month'], index_col = 'Month')
 airline.head()"
"ASSIGN = airline.plot(color = 'blue', fontsize=12) ASSIGN.axvline('1955-12-01', color='red', linestyle='--') ASSIGN.set_xlabel('Date', fontsize=12) ASSIGN.set_title('Number of Monthly Airline Passengers', fontsize=12) plt.show()",0,display_data,"# Plot the time series in your dataframe
 ax = airline.plot(color = 'blue', fontsize=12)
 
 # Add a red vertical line at the date 1955-12-01
 ax.axvline('1955-12-01', color='red', linestyle='--')
 
 # Specify the labels in your plot
 ax.set_xlabel('Date', fontsize=12)
 ax.set_title('Number of Monthly Airline Passengers', fontsize=12)
 plt.show()"
CHECKPOINT print(airline.isnull().sum()) print(airline.describe()),0,stream,"# Print out the number of missing values
 print(airline.isnull().sum())
 
 # Print out summary statistics of the airline DataFrame
 print(airline.describe())"
"ASSIGN = airline.boxplot() ASSIGN.set_title('Boxplot of Monthly Airline\nPassengers Count', fontsize=20) plt.show()",0,display_data,"# Display boxplot of airline values
 ax = airline.boxplot()
 
 # Specify the title of your plot
 ax.set_title('Boxplot of Monthly Airline\nPassengers Count', fontsize=20)
 plt.show()"
ASSIGN = airline.index.month ASSIGN = airline.groupby(index_month).mean() ASSIGN.plot() plt.legend(fontsize=20) plt.show(),0,display_data,"# Get month for each dates from the index of airline
 index_month = airline.index.month
 
 # Compute the mean number of passengers for each month of the year
 mean_airline_by_month = airline.groupby(index_month).mean()
 
 # Plot the mean number of passengers for each month of the year
 mean_airline_by_month.plot()
 plt.legend(fontsize=20)
 plt.show()"
SETUP ASSIGN = sm.tsa.seasonal_decompose(airline) ASSIGN = decomposition.ASSIGN ASSIGN = decomposition.ASSIGN,0,not_existent,"# Import statsmodels.api as sm
 import statsmodels.api as sm
 
 # Perform time series decompositon
 decomposition = sm.tsa.seasonal_decompose(airline)
 
 # Extract the trend and seasonal components
 trend = decomposition.trend
 seasonal = decomposition.seasonal"
"ASSIGN = pd.concat([trend, seasonal], axis = 1) ASSIGN.columns = ['trend', 'seasonal']",1,not_existent,"airline_decomposed = pd.concat([trend, seasonal], axis = 1)
 airline_decomposed.columns = ['trend', 'seasonal']"
"CHECKPOINT print(airline_decomposed.head()) ASSIGN = airline_decomposed.plot(figsize=(12, 6), fontsize=15) ASSIGN.set_xlabel('Date', fontsize=15) plt.legend(fontsize=15) plt.show()",0,stream,"# Print the first 5 rows of airline_decomposed
 print(airline_decomposed.head())
 
 # Plot the values of the df_decomposed DataFrame
 ax = airline_decomposed.plot(figsize=(12, 6), fontsize=15)
 
 # Specify axis labels
 ax.set_xlabel('Date', fontsize=15)
 plt.legend(fontsize=15)
 plt.show()"
ASSIGN = pd.read_csv('path') display(ASSIGN.head(5)),0,display_data,"# Read in meat DataFrame
 meat = pd.read_csv('/kaggle/input/week6dataset/meat.csv')
 
 # Review the first five lines of the meat DataFrame
 display(meat.head(5))"
ASSIGN = pd.to_datetime(ASSIGN ) ASSIGN = ASSIGN.set_index('date') display(ASSIGN.describe()),0,display_data,"# Convert the date column to a datestamp type
 meat['date'] = pd.to_datetime(meat['date'] )
 
 # Set the date column as the index of your DataFrame meat
 meat = meat.set_index('date')
 
 # Print the summary statistics of the DataFrame
 display(meat.describe())"
"ASSIGN = meat.plot(linewidth = 2, fontsize = 12) ASSIGN.set_xlabel('Date') ASSIGN.legend(fontsize=15) plt.show()",0,display_data,"# Plot time series dataset
 ax = meat.plot(linewidth = 2, fontsize = 12)
 
 # Additional customizations
 ax.set_xlabel('Date')
 ax.legend(fontsize=15)
 
 # Show plot
 plt.show()"
ASSIGN = meat.plot.area(fontsize=12) ASSIGN.set_xlabel('Date') ASSIGN.legend(fontsize=15) plt.show(),0,display_data,"# Plot an area chart
 ax = meat.plot.area(fontsize=12)
 
 # Additional customizations
 ax.set_xlabel('Date')
 ax.legend(fontsize=15)
 
 # Show plot
 plt.show()"
"ASSIGN = meat.plot(colormap='cubehelix', fontsize=15) ASSIGN.set_xlabel('Date') ASSIGN.legend(fontsize=18) plt.show()",0,display_data,"# Plot time series dataset using the cubehelix color palette
 ax = meat.plot(colormap='cubehelix', fontsize=15)
 
 # Additional customizations
 ax.set_xlabel('Date')
 ax.legend(fontsize=18)
 
 # Show plot
 plt.show()"
"ASSIGN = meat.plot(colormap = 'PuOr', fontsize=15) ASSIGN.set_xlabel('Date') ASSIGN.legend(fontsize=18) plt.show()",0,display_data,"# Plot time series dataset using the cubehelix color palette
 ax = meat.plot(colormap = 'PuOr', fontsize=15)
 
 # Additional customizations
 ax.set_xlabel('Date')
 ax.legend(fontsize=18)
 
 # Show plot
 plt.show()"
CHECKPOINT ASSIGN = meat.mean(axis = 0) ASSIGN = pd.DataFrame(ASSIGN).transpose() ASSIGN.index = ['mean'] meat_mean,1,execute_result,"meat_mean = meat.mean(axis = 0)
 meat_mean = pd.DataFrame(meat_mean).transpose()
 meat_mean.index = ['mean']
 meat_mean"
"ASSIGN = meat.plot(fontsize=6, linewidth=1) ASSIGN.set_xlabel('Date', fontsize=6) ASSIGN.table(cellText=meat_mean.values, ASSIGN = [0.15]*len(meat_mean.columns), ASSIGN=meat_mean.index, ASSIGN=meat_mean.columns, ASSIGN='top') ASSIGN.legend(ASSIGN='upper center', bbox_to_anchor=(0.5, 0.95), ncol=3, fontsize=6) plt.show()",0,display_data,"# Plot the meat data
 ax = meat.plot(fontsize=6, linewidth=1)
 
 # Add x-axis labels
 ax.set_xlabel('Date', fontsize=6)
 
 # Add summary table information to the plot
 ax.table(cellText=meat_mean.values,
          colWidths = [0.15]*len(meat_mean.columns),
          rowLabels=meat_mean.index,
          colLabels=meat_mean.columns,
          loc='top')
 
 # Specify the fontsize and location of your legend
 ax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.95), ncol=3, fontsize=6)
 
 # Show plot
 plt.show()"
"meat.plot(subplots=True, ASSIGN=(2, 4), ASSIGN=False, ASSIGN=False, ASSIGN='viridis', ASSIGN=2, ASSIGN=False, ASSIGN=0.2) plt.show()",0,display_data,"# Create a facetted graph with 2 rows and 4 columns
 meat.plot(subplots=True, 
           layout=(2, 4), 
           sharex=False, 
           sharey=False, 
           colormap='viridis', 
           fontsize=2, 
           legend=False, 
           linewidth=0.2)
 
 plt.show()"
"CHECKPOINT print(meat[['beef', 'pork']].corr(method='spearman'))",0,stream,"# Print the correlation matrix between the beef and pork columns using the spearman method
 print(meat[['beef', 'pork']].corr(method='spearman'))"
"CHECKPOINT print(meat[['pork', 'veal', 'turkey']].corr(method='pearson'))",0,stream,"# Print the correlation matrix between the pork, veal and turkey columns using the pearson method
 print(meat[['pork', 'veal', 'turkey']].corr(method='pearson'))"
"SETUP ASSIGN = meat.corr(method='spearman') sns.heatmap(ASSIGN, ASSIGN=True, ASSIGN=0.4, ASSIGN={""size"": 10}) plt.xticks(rotation=90) plt.yticks(rotation=0) plt.show()",0,display_data,"# Import seaborn library
 import seaborn as sns
 
 # Get correlation matrix of the meat DataFrame
 corr_meat = meat.corr(method='spearman')
 
 
 # Customize the heatmap of the corr_meat correlation matrix
 sns.heatmap(corr_meat,
             annot=True,
             linewidths=0.4,
             annot_kws={""size"": 10})
 
 plt.xticks(rotation=90)
 plt.yticks(rotation=0) 
 plt.show()"
"SETUP ASSIGN = meat.corr(method = 'pearson') ASSIGN = sns.clustermap(corr_meat, ASSIGN=True, ASSIGN=True, ASSIGN=(10, 10)) plt.setp(ASSIGN.ax_heatmap.xaxis.get_majorticklabels(), rotation=90) plt.setp(ASSIGN.ax_heatmap.yaxis.get_majorticklabels(), rotation=0) plt.show()",0,display_data,"# Import seaborn library
 import seaborn as sns
 
 # Get correlation matrix of the meat DataFrame
 corr_meat = meat.corr(method = 'pearson')
 
 # Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels
 fig = sns.clustermap(corr_meat,
                      row_cluster=True,
                      col_cluster=True,
                      figsize=(10, 10))
 
 plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)
 plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)
 plt.show()"
"CHECKPOINT ASSIGN = pd.read_csv('path', parse_dates = ['datestamp']) display(ASSIGN.head(5)) print(ASSIGN.dtypes) ASSIGN = pd.to_datetime(ASSIGN) ASSIGN = ASSIGN.set_index('datestamp') display(ASSIGN.isnull().sum())",1,display_data,"# Read in jobs file
 jobs = pd.read_csv('/kaggle/input/week6dataset/employment.csv', parse_dates = ['datestamp'])
 
 # Review the first five lines of your DataFrame
 display(jobs.head(5))
 
 # Review the type of each column in your DataFrame
 print(jobs.dtypes)
 
 # Convert datestamp column to a datetime object
 jobs['datestamp'] = pd.to_datetime(jobs['datestamp'])
 
 # Set the datestamp columns as the index of your DataFrame
 jobs = jobs.set_index('datestamp')
 
 # Check the number of missing values in each column
 display(jobs.isnull().sum())"
"jobs.boxplot(fontsize=6, vert=False) plt.show() display(jobs.describe())",0,display_data,"# Generate a boxplot
 jobs.boxplot(fontsize=6, vert=False)
 plt.show()
 
 # Generate numerical summaries
 display(jobs.describe())"
"CHECKPOINT ASSIGN = jobs[['Finance', 'Information', 'Manufacturing', 'Construction']] print(ASSIGN.head()) ASSIGN = jobs_subset.plot(subplots=True, ASSIGN=(2,2), ASSIGN=False, ASSIGN=False, ASSIGN=0.7, ASSIGN=3, ASSIGN=False) plt.show()",0,stream,"# A subset of the jobs DataFrame
 jobs_subset = jobs[['Finance', 'Information', 'Manufacturing', 'Construction']]
 
 # Print the first 5 rows of jobs_subset
 print(jobs_subset.head())
 
 # Create a facetted graph with 2 rows and 2 columns
 ax = jobs_subset.plot(subplots=True,
                       layout=(2,2),
                       sharex=False,
                       sharey=False,
                       linewidth=0.7,
                       fontsize=3,
                       legend=False)
 
 plt.show()"
"ASSIGN = jobs.plot(colormap = 'Spectral', fontsize=6, linewidth=0.8) ASSIGN.set_xlabel('Date', fontsize=10) ASSIGN.set_ylabel('Unemployment Rate', fontsize=10) ASSIGN.set_title('Unemployment rate of U.S. workers by industry', fontsize=10) ASSIGN.legend(loc='center left', bbox_to_anchor=(1.0, 0.5)) ASSIGN.axvline('2001-07-01', color='blue', linestyle='--', linewidth=0.8) ASSIGN.axvline('2008-09-01', color='blue', linestyle='--', linewidth=0.8) plt.show()",0,display_data,"# Plot all time series in the jobs DataFrame
 ax = jobs.plot(colormap = 'Spectral', fontsize=6, linewidth=0.8)
 
 # Set labels and legend
 ax.set_xlabel('Date', fontsize=10)
 ax.set_ylabel('Unemployment Rate', fontsize=10)
 ax.set_title('Unemployment rate of U.S. workers by industry', fontsize=10)
 ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
 
 # Annotate your plots with vertical lines
 ax.axvline('2001-07-01', color='blue', linestyle='--', linewidth=0.8)
 ax.axvline('2008-09-01', color='blue', linestyle='--', linewidth=0.8)
 
 # Show plot
 plt.show()"
"ASSIGN = jobs.index.month ASSIGN = jobs.groupby(index_month).mean() ASSIGN = jobs_by_month.plot(fontsize=6, linewidth=1) ASSIGN.set_xlabel('Month', fontsize=10) ASSIGN.set_ylabel('Mean unemployment rate', fontsize=10) ASSIGN.legend(bbox_to_anchor=(0.8, 0.6), fontsize=10) plt.show()",0,display_data,"# Extract the month from the index of jobs
 index_month = jobs.index.month
 
 # Compute the mean unemployment rate for each month
 jobs_by_month = jobs.groupby(index_month).mean()
 
 # Plot the mean unemployment rate for each month
 ax = jobs_by_month.plot(fontsize=6, linewidth=1)
 
 # Set axis labels and legend
 ax.set_xlabel('Month', fontsize=10)
 ax.set_ylabel('Mean unemployment rate', fontsize=10)
 ax.legend(bbox_to_anchor=(0.8, 0.6), fontsize=10)
 plt.show()"
"ASSIGN = jobs.index.year ASSIGN = jobs.groupby(index_year).mean() ASSIGN = jobs_by_year.plot(fontsize=6, linewidth=1) ASSIGN.set_xlabel('Year', fontsize=10) ASSIGN.set_ylabel('Mean unemployment rate', fontsize=10) ASSIGN.legend(bbox_to_anchor=(0.1, 0.5), fontsize=10) plt.show()",0,display_data,"# Extract of the year in each date indices of the jobs DataFrame
 index_year = jobs.index.year
 
 # Compute the mean unemployment rate for each year
 jobs_by_year = jobs.groupby(index_year).mean()
 
 # Plot the mean unemployment rate for each year
 ax = jobs_by_year.plot(fontsize=6, linewidth=1)
 
 # Set axis labels and legend
 ax.set_xlabel('Year', fontsize=10)
 ax.set_ylabel('Mean unemployment rate', fontsize=10)
 ax.legend(bbox_to_anchor=(0.1, 0.5), fontsize=10)
 plt.show()"
ASSIGN = {} ASSIGN = jobs.columns for ts in ASSIGN: ASSIGN = sm.tsa.seasonal_decompose(jobs[ts]) ASSIGN[ts] = ASSIGN,1,not_existent,"# Initialize dictionary
 jobs_decomp = {}
 
 # Get the names of each time series in the DataFrame
 jobs_names = jobs.columns
 
 # Run time series decomposition on each time series of the DataFrame
 for ts in jobs_names:
     ts_decomposition = sm.tsa.seasonal_decompose(jobs[ts])
     jobs_decomp[ts] = ts_decomposition"
"ASSIGN = {} for ts in jobs_names: ASSIGN[ts] = jobs_decomp[ts].seasonal ASSIGN = pd.DataFrame(jobs_seasonal) ASSIGN.index.name = None ASSIGN.plot(subplots=True, ASSIGN=(4, 4), ASSIGN=False, ASSIGN=2, ASSIGN=0.3, ASSIGN=False) plt.show()",0,display_data,"jobs_seasonal = {}
 
 # Extract the seasonal values for the decomposition of each time series
 for ts in jobs_names:
     jobs_seasonal[ts] = jobs_decomp[ts].seasonal
     
 # Create a DataFrame from the jobs_seasonal dictionnary
 seasonality_df = pd.DataFrame(jobs_seasonal)
 
 # Remove the label for the index
 seasonality_df.index.name = None
 
 # Create a faceted plot of the seasonality_df DataFrame
 seasonality_df.plot(subplots=True,
                    layout=(4, 4),
                    sharey=False,
                    fontsize=2,
                    linewidth=0.3,
                    legend=False)
 
 # Show plot
 plt.show()"
"ASSIGN = seasonality_df.corr(method='spearman') ASSIGN = sns.clustermap(seasonality_corr, annot=True, annot_kws={""size"": 4}, linewidths=.4, figsize=(15, 10)) plt.setp(ASSIGN.ax_heatmap.yaxis.get_majorticklabels(), rotation=0) plt.setp(ASSIGN.ax_heatmap.xaxis.get_majorticklabels(), rotation=90) plt.show()",0,display_data,"# Get correlation matrix of the seasonality_df DataFrame
 seasonality_corr = seasonality_df.corr(method='spearman')
 
 # Customize the clustermap of the seasonality_corr correlation matrix
 fig = sns.clustermap(seasonality_corr, annot=True, annot_kws={""size"": 4}, linewidths=.4, figsize=(15, 10))
 plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)
 plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)
 plt.show()"
ASSIGN=pd.read_csv('path') ASSIGN=pd.read_csv('path'),0,not_existent,"train_data=pd.read_csv('/kaggle/input/digit-recognizer/train.csv')
 test_data=pd.read_csv('/kaggle/input/digit-recognizer/test.csv')"
CHECKPOINT ASSIGN = [col for col in train_data.columns if train_data[col].isnull().any()] print(ASSIGN),1,stream,"# No data for train_data is missing
 Col_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]
 print(Col_with_missing)"
CHECKPOINT ASSIGN = [col for col in test_data.columns if test_data[col].isnull().any()] print(ASSIGN),1,stream,"# No data for test_data is missing
 Col_with_missing = [col for col in test_data.columns if test_data[col].isnull().any()]
 print(Col_with_missing)"
"ASSIGN = train_data[""label""] ASSIGN = train_data.drop([""label""],axis = 1)",1,not_existent,"Y_train = train_data[""label""]
 
 # Drop 'label' column
 X_train = train_data.drop([""label""],axis = 1) "
Y_train.value_counts(),0,execute_result,Y_train.value_counts()
SETUP sns.countplot(Y_train),0,execute_result,"import seaborn as sns
 %matplotlib inline
 sns.countplot(Y_train)"
"ASSIGN=28 ASSIGN=28 def data_prep_X(X): ASSIGN=len(X) ASSIGN=X.values.reshape(num_img,img_row,img_col,1) ASSIGN=x_as_arraypath return X_out",1,not_existent,"img_row=28
 img_col=28
 def data_prep_X(X):
     num_img=len(X)
     x_as_array=X.values.reshape(num_img,img_row,img_col,1)
     X_out=x_as_array/255
     return X_out"
"SETUP ASSIGN=10 def data_prep_Y(Y): ASSIGN = to_categorical(Y, num_classes) return out_y",0,stream,"from keras.utils.np_utils import to_categorical
 num_classes=10
 def data_prep_Y(Y):
     out_y = to_categorical(Y, num_classes)
     return out_y"
ASSIGN = data_prep_X(ASSIGN) ASSIGN = data_prep_X(ASSIGN) ASSIGN = data_prep_Y(ASSIGN),1,not_existent,"X_train = data_prep_X(X_train)
 test_data = data_prep_X(test_data)
 Y_train = data_prep_Y(Y_train)"
"SETUP ASSIGN = plt.imshow(X_train[0][:,:,0])",0,display_data,"# Some examples images
 import matplotlib.pyplot as plt
 g = plt.imshow(X_train[0][:,:,0])"
"ASSIGN = plt.imshow(X_train[1][:,:,0])",0,display_data,"g = plt.imshow(X_train[1][:,:,0])"
"SETUP ASSIGN = Sequential() ASSIGN.add(Conv2D(20, kernel_size=(3, 3), ASSIGN='relu', ASSIGN=(img_row, img_col, 1))) ASSIGN.add(Conv2D(20, kernel_size=(3, 3), ASSIGN='relu')) ASSIGN.add(Flatten()) ASSIGN.add(Dense(128, ASSIGN='relu')) ASSIGN.add(Dense(num_classes, ASSIGN='softmax')) ASSIGN.compile(loss=keras.losses.categorical_crossentropy, ASSIGN='adam', ASSIGN=['accuracy']) ASSIGN.fit(X_train, Y_train, ASSIGN=128, ASSIGN=30, ASSIGN = 0.2)",0,stream,"from tensorflow.python import keras
 from tensorflow.python.keras.models import Sequential
 from tensorflow.python.keras.layers import Dense, Flatten, Conv2D
 
 model = Sequential()
 model.add(Conv2D(20, kernel_size=(3, 3),
                  activation='relu',
                  input_shape=(img_row, img_col, 1)))
 model.add(Conv2D(20, kernel_size=(3, 3), activation='relu'))
 model.add(Flatten())
 model.add(Dense(128, activation='relu'))
 model.add(Dense(num_classes, activation='softmax'))
 
 model.compile(loss=keras.losses.categorical_crossentropy,
               optimizer='adam',
               metrics=['accuracy'])
 model.fit(X_train, Y_train,
           batch_size=128,
           epochs=30,
           validation_split = 0.2)"
"ASSIGN = model.predict(test_data) ASSIGN = np.argmax(ASSIGN,axis = 1) ASSIGN = pd.Series(ASSIGN,name=""Label"")",1,not_existent,"# predict results
 results = model.predict(test_data)
 
 # select the indix with the maximum probability
 results = np.argmax(results,axis = 1)
 
 results = pd.Series(results,name=""Label"")"
"ASSIGN = pd.concat([pd.Series(range(1,28001),name = ""ImageId""),results],axis = 1) ASSIGN.to_csv(""mySubmission.csv"",index=False)",0,not_existent,"submission = pd.concat([pd.Series(range(1,28001),name = ""ImageId""),results],axis = 1)
 
 submission.to_csv(""mySubmission.csv"",index=False)"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"
 
 import numpy as np
 import pandas as pd 
 import os
 for dirname, _, filenames in os.walk('/kaggle/input'):
     for filename in filenames:
         print(os.path.join(dirname, filename))
 import matplotlib.pyplot as plt
 import tensorflow.keras as keras
 import seaborn as sns
 from keras.utils import to_categorical
 from keras.models import Sequential
 from keras.layers import Dense,BatchNormalization
 from sklearn.preprocessing import StandardScaler
 from sklearn.model_selection import train_test_split"
ASSIGN = pd.read_csv('path'),0,not_existent,df = pd.read_csv('/kaggle/input/league-of-legends-diamond-ranked-games-10-min/high_diamond_ranked_10min.csv')
df.head(5),0,execute_result,df.head(5)
"ASSIGN = df.iloc[:,1:40] ASSIGN.head(5)",1,execute_result,"dataset = df.iloc[:,1:40]
 dataset.head(5)"
ASSIGN = dataset.ASSIGN('pearson'),1,not_existent,corr = dataset.corr('pearson')
"sns.countplot(data = dataset,x='blueWins')",0,execute_result,"sns.countplot(data = dataset,x='blueWins')"
dataset.isnull().sum(),0,execute_result,dataset.isnull().sum()
"ASSIGN = dataset.iloc[:,0:19] ASSIGN.drop(columns = ['blueDeaths'],inplace = True) ASSIGN.head(5)",1,execute_result,"blue_df = dataset.iloc[:,0:19]
 blue_df.drop(columns = ['blueDeaths'],inplace = True)
 blue_df.head(5)"
"ASSIGN = blue_df.ASSIGN('pearson') plt.figure(figsize = (10,10)) sns.heatmap(ASSIGN,annot = True)",0,execute_result,"corr = blue_df.corr('pearson')
 plt.figure(figsize = (10,10))
 sns.heatmap(corr,annot = True)"
corr['blueWins'].sort_values(ascending=False),0,execute_result,corr['blueWins'].sort_values(ascending=False)
"ASSIGN = blue_df.iloc[:,1:] ASSIGN = blue_df.iloc[:,1]",1,not_existent,"X = blue_df.iloc[:,1:]
 y = blue_df.iloc[:,1]"
"ASSIGN = StandardScaler() ASSIGN.fit(X) ASSIGN = pd.DataFrame(scaler.transform(ASSIGN),columns=ASSIGN.columns) ASSIGN.head(5)",1,execute_result,"scaler = StandardScaler()
 scaler.fit(X)
 X = pd.DataFrame(scaler.transform(X),columns=X.columns)
 X.head(5)"
"CHECKPOINT ASSIGN = blue_df['blueWins'] ASSIGN = to_categorical(ASSIGN, 2) y",1,execute_result,"y = blue_df['blueWins']
 y = to_categorical(y, 2)
 y"
"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.20,random_state=0)",1,not_existent,"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.20,random_state=0)"
"ASSIGN = Sequential() ASSIGN.add(Dense(units=18,activation='relu',input_dim=len(X.columns))) ASSIGN.add(Dense(36,activation = 'relu')) ASSIGN.add(Dense(72,activation = 'relu')) ASSIGN.add(Dense(units=2,activation='softmax')) ASSIGN.summary()",0,stream,"model = Sequential()
 model.add(Dense(units=18,activation='relu',input_dim=len(X.columns)))
 model.add(Dense(36,activation = 'relu'))
 model.add(Dense(72,activation = 'relu'))
 model.add(Dense(units=2,activation='softmax'))
 model.summary()"
"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])",0,not_existent,"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
"ASSIGN = model.fit(X_train,y_train, ASSIGN=50, ASSIGN=(X_test,y_test))",0,stream,"history = model.fit(X_train,y_train,
                    epochs=50,
                    validation_data=(X_test,y_test))"
"plt.figure(figsize=(8,8)) plt.plot(history.history['val_accuracy']) plt.title('Accuracy curves') plt.xlabel('epochs') plt.ylabel('Accuracy') plt.show()",0,display_data,"plt.figure(figsize=(8,8))
 plt.plot(history.history['val_accuracy'])
 #plt.legend(['Training Accuracy','Validation Accuracy'])
 plt.title('Accuracy curves')
 plt.xlabel('epochs')
 plt.ylabel('Accuracy')
 plt.show()"
SETUP ASSIGN = 7 np.random.ASSIGN(ASSIGN),0,not_existent,import matplotlib.pyplot as plt import numpy as np import pandas as pd seed = 7 np.random.seed(seed)
"CHECKPOINT ASSIGN = np.load(""..path"") ASSIGN = np.load(""..path"") ASSIGN = np.load(""..path"") ASSIGN = np.load(""..path"") ASSIGN = pd.read_csv(""..path"") tlabel",0,not_existent,"tpure = np.load(""../input/train_images_pure.npy"") tnoisy = np.load(""../input/train_images_noisy.npy"") trotated = np.load(""../input/train_images_rotated.npy"") tboth = np.load(""../input/train_images_both.npy"") tlabel = pd.read_csv(""../input/train_labels.csv"") tlabel"
"plt.subplot(221) plt.imshow(tpure[0], cmap=plt.get_cmap('gray')) plt.subplot(222) plt.imshow(tpure[1], cmap=plt.get_cmap('gray')) plt.subplot(223) plt.imshow(tpure[2], cmap=plt.get_cmap('gray')) plt.subplot(224) plt.imshow(tpure[3], cmap=plt.get_cmap('gray')) plt.show()",0,not_existent,"plt.subplot(221) plt.imshow(tpure[0], cmap=plt.get_cmap('gray')) plt.subplot(222) plt.imshow(tpure[1], cmap=plt.get_cmap('gray')) plt.subplot(223) plt.imshow(tpure[2], cmap=plt.get_cmap('gray')) plt.subplot(224) plt.imshow(tpure[3], cmap=plt.get_cmap('gray')) # show the plot plt.show()"
"plt.subplot(221) plt.imshow(tnoisy[0], cmap=plt.get_cmap('gray')) plt.subplot(222) plt.imshow(tnoisy[1], cmap=plt.get_cmap('gray')) plt.subplot(223) plt.imshow(tnoisy[2], cmap=plt.get_cmap('gray')) plt.subplot(224) plt.imshow(tnoisy[3], cmap=plt.get_cmap('gray')) plt.show()",0,not_existent,"plt.subplot(221) plt.imshow(tnoisy[0], cmap=plt.get_cmap('gray')) plt.subplot(222) plt.imshow(tnoisy[1], cmap=plt.get_cmap('gray')) plt.subplot(223) plt.imshow(tnoisy[2], cmap=plt.get_cmap('gray')) plt.subplot(224) plt.imshow(tnoisy[3], cmap=plt.get_cmap('gray')) # show the plot plt.show()"
"for i in range(15,19): plt.subplot(221+(i%5)) plt.imshow(trotated[i], cmap=plt.get_cmap('gray')) plt.show()",0,not_existent,"for i in range(15,19):     plt.subplot(221+(i%5))     plt.imshow(trotated[i], cmap=plt.get_cmap('gray')) plt.show()"
"plt.subplot(221) plt.imshow(tboth[0], cmap=plt.get_cmap('gray')) plt.subplot(222) plt.imshow(tboth[1], cmap=plt.get_cmap('gray')) plt.subplot(223) plt.imshow(tboth[2], cmap=plt.get_cmap('gray')) plt.subplot(224) plt.imshow(tboth[3], cmap=plt.get_cmap('gray')) plt.show()",0,not_existent,"plt.subplot(221) plt.imshow(tboth[0], cmap=plt.get_cmap('gray')) plt.subplot(222) plt.imshow(tboth[1], cmap=plt.get_cmap('gray')) plt.subplot(223) plt.imshow(tboth[2], cmap=plt.get_cmap('gray')) plt.subplot(224) plt.imshow(tboth[3], cmap=plt.get_cmap('gray')) # show the plot plt.show()"
SETUP K.set_image_dim_ordering('th'),0,not_existent,from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout from keras.layers import Flatten from keras.layers.convolutional import Conv2D from keras.layers.convolutional import MaxPooling2D from keras.utils import np_utils from keras import backend as K from sklearn.model_selection import train_test_split from keras.callbacks import EarlyStopping K.set_image_dim_ordering('th')
"def DataPrep(db): ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], 1, 28, 28).astype('float32') ASSIGN = ASSIGN path ASSIGN = np_utils.to_categorical(ASSIGN) return db",1,not_existent,"def DataPrep(db):     db = db.reshape(db.shape[0], 1, 28, 28).astype('float32')     db = db / 255     db = np_utils.to_categorical(db)     return db"
ASSIGN = np_utils.to_categorical(ASSIGN['label']) ASSIGN = DataPrep(ASSIGN) ASSIGN = DataPrep(ASSIGN),1,not_existent,tlabel = np_utils.to_categorical(tlabel['label']) tpure = DataPrep(tpure) trotated = DataPrep(trotated)
"def deepCNN(): ASSIGN = Sequential() ASSIGN.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu')) ASSIGN.add(Conv2D(15, (3, 3), activation='relu')) ASSIGN.add(Dropout(0.2)) ASSIGN.add(Flatten()) ASSIGN.add(Dense(128, activation='relu')) ASSIGN.add(Dense(50, activation='relu')) ASSIGN.add(Dense(tlabel.shape[1], activation='softmax')) ASSIGN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) return model",0,not_existent,"def deepCNN():     # create model     model = Sequential()     model.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu'))     model.add(Conv2D(15, (3, 3), activation='relu'))     model.add(Dropout(0.2))     model.add(Flatten())     model.add(Dense(128, activation='relu'))     model.add(Dense(50, activation='relu'))     model.add(Dense(tlabel.shape[1], activation='softmax'))     # Compile model     model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])     return model"
ASSIGN = deepCNN(),0,not_existent,CNNmodel = deepCNN()
"ASSIGN = [EarlyStopping(monitor = 'val_loss', patience = 2)] Xtrain,Xvalidation,Ytrain,Yvalidation = train_test_split(tpure,tlabel, test_size = 0.2) CNNmodel.fit(Xtrain, Ytrain, validation_data=(Xvalidation,Yvalidation), epochs=20, ASSIGN=200, verbose=1, callbacks = callbacks)",1,not_existent,"callbacks = [EarlyStopping(monitor = 'val_loss', patience = 2)] Xtrain,Xvalidation,Ytrain,Yvalidation = train_test_split(tpure,tlabel, test_size = 0.2) CNNmodel.fit(Xtrain, Ytrain, validation_data=(Xvalidation,Yvalidation), epochs=20,            batch_size=200, verbose=1, callbacks = callbacks)"
SETUP CHECKPOINT binder.bind(globals()) print(),0,stream,"# Set up feedback system from learntools.core import binder binder.bind(globals()) from learntools.sql_advanced.ex1 import * print(""Setup Complete"")"
"SETUP ASSIGN = bigquery.Client() ASSIGN = client.dataset(""stackoverflow"", project=""bigquery-public-data"") ASSIGN = client.get_dataset(dataset_ref) ASSIGN = dataset_ref.table(""posts_questions"") ASSIGN = client.get_table(table_ref) ASSIGN.list_rows(ASSIGN, max_results=5).to_dataframe()",1,stream,"from google.cloud import bigquery  # Create a ""Client"" object client = bigquery.Client()  # Construct a reference to the ""stackoverflow"" dataset dataset_ref = client.dataset(""stackoverflow"", project=""bigquery-public-data"")  # API request - fetch the dataset dataset = client.get_dataset(dataset_ref)  # Construct a reference to the ""posts_questions"" table table_ref = dataset_ref.table(""posts_questions"")  # API request - fetch the table table = client.get_table(table_ref)  # Preview the first five lines of the table client.list_rows(table, max_results=5).to_dataframe()"
"ASSIGN = dataset_ref.table(""posts_answers"") ASSIGN = client.get_table(table_ref) client.list_rows(ASSIGN, max_results=5).to_dataframe()",1,execute_result,"# Construct a reference to the ""posts_answers"" table table_ref = dataset_ref.table(""posts_answers"")  # API request - fetch the table table = client.get_table(table_ref)  # Preview the first five lines of the table client.list_rows(table, max_results=5).to_dataframe()"
"CHECKPOINT ASSIGN = """""" SELECT q.id AS q_id, MIN(TIMESTAMP_DIFF(a.creation_date, q.creation_date, SECOND)) as time_to_answer FROM `bigquery-public-data.stackoverflow.posts_questions` AS q INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a ON q.id = a.parent_id WHERE q.creation_date >= '2018-01-01' and q.creation_date < '2018-02-01' GROUP BY q_id ORDER BY time_to_answer """""" ASSIGN = client.query(first_query).result().to_dataframe() print(""Percentage of answered questions: %s%%"" % \ (sum(ASSIGN[""time_to_answer""].notnull()) path(ASSIGN) * 100)) print(, len(ASSIGN)) ASSIGN.head()",1,stream,"first_query = """"""               SELECT q.id AS q_id,                   MIN(TIMESTAMP_DIFF(a.creation_date, q.creation_date, SECOND)) as time_to_answer               FROM `bigquery-public-data.stackoverflow.posts_questions` AS q                   INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a               ON q.id = a.parent_id               WHERE q.creation_date >= '2018-01-01' and q.creation_date < '2018-02-01'               GROUP BY q_id               ORDER BY time_to_answer               """"""  first_result = client.query(first_query).result().to_dataframe() print(""Percentage of answered questions: %s%%"" % \       (sum(first_result[""time_to_answer""].notnull()) / len(first_result) * 100)) print(""Number of questions:"", len(first_result)) first_result.head()"
SETUP,0,not_existent,from Levenshtein import distance import numpy as np 
"def get_distance_matrix(str_list): """""" Construct a levenshtein distance matrix for a list of strings"""""" ASSIGN = np.zeros(shape=(len(str_list), len(str_list))) print (""Starting to build distance matrix. This will iterate from 0 till "", len(str_list) ) for i in range(0, len(str_list)): print (i) for j in range(i+1, len(str_list)): ASSIGN[i][j] = distance(str_list[i], str_list[j]) for i in range(0, len(str_list)): for j in range(0, len(str_list)): ASSIGN == j: ASSIGN[i][j] = 0 elif i > j: ASSIGN[i][j] = ASSIGN[j][i] return dist_matrix",1,not_existent," def get_distance_matrix(str_list):     """""" Construct a levenshtein distance matrix for a list of strings""""""     dist_matrix = np.zeros(shape=(len(str_list), len(str_list)))      print (""Starting to build distance matrix. This will iterate from 0 till "", len(str_list) )     for i in range(0, len(str_list)):         print (i)         for j in range(i+1, len(str_list)):                 dist_matrix[i][j] = distance(str_list[i], str_list[j])      for i in range(0, len(str_list)):         for j in range(0, len(str_list)):             if i == j:                 dist_matrix[i][j] = 0              elif i > j:                 dist_matrix[i][j] = dist_matrix[j][i]      return dist_matrix  "
"ASSIGN = [ ""part"", ""spartan"" ] get_distance_matrix(ASSIGN)",1,stream,"str_list = [     ""part"", ""spartan""    ] get_distance_matrix(str_list)"
SETUP,0,not_existent,"import os
 
 import matplotlib.pyplot as plt
 import seaborn as sns
 %matplotlib inline
 
 import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
"SETUP sns.set(rc={'figure.figsize':(20, 15)})",0,not_existent,"# ML
 # for transformers creation
 from sklearn.base import BaseEstimator, TransformerMixin
 from sklearn.pipeline import Pipeline
 from sklearn.impute import SimpleImputer
 from sklearn.preprocessing import OneHotEncoder
 
 # models and metrics
 from sklearn.neighbors import KNeighborsClassifier
 from sklearn.svm import SVC
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV
 from sklearn.metrics import classification_report
 # distributions for random search
 from scipy.stats import randint, expon, reciprocal
 
 sns.set(rc={'figure.figsize':(20, 15)})"
"SETUP CHECKPOINT def save(model, cv_info, classification_report, name=""model"", cv_scores=None): ASSIGN = { ""cv_info"": cv_info, ""classification_report"": classification_report, ""model"": model, ""cv_scores"": cv_scores } joblib.dump(ASSIGN, ""path"" + name + "".pkl"") def load(name=""model"", verbose=True, with_metadata=False): ASSIGN = joblib.load(""path"" + name + "".pkl"") if verbose: print() [print(.format(key=key, val=val)) for key, val in ASSIGN[].items()] print() print(ASSIGN[]) if not with_metadata: return ASSIGN[""model""] else: return _model",0,not_existent,"import joblib  # for saving models from skikit-learn
 
 # some utils for saving and reading later
 def save(model, cv_info, classification_report, name=""model"", cv_scores=None):
     _model = {
         ""cv_info"": cv_info, ""classification_report"": classification_report, ""model"": model, ""cv_scores"": cv_scores
     }
     joblib.dump(_model, ""/kaggle/working/"" + name + "".pkl"")
 
 
 def load(name=""model"", verbose=True, with_metadata=False):
     _model = joblib.load(""/kaggle/working/"" + name + "".pkl"")
     if verbose:
         print(""\nLoading model with the following info:\n"")
         [print(""{key}: {val}"".format(key=key, val=val)) for key, val in _model[""cv_info""].items()]
         print(""\nClassification Report:\n"")
         print(_model[""classification_report""])
     if not with_metadata:
         return _model[""model""]
     else:
         return _model"
"CHECKPOINT for dirname, _, filenames in os.walk('..path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"for dirname, _, filenames in os.walk('../input/titanic'):
     for filename in filenames:
         print(os.path.join(dirname, filename))"
ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path'),0,not_existent,"train = pd.read_csv('../input/titanic/train.csv')
 test = pd.read_csv('../input/titanic/test.csv')"
train.head(10),0,execute_result,train.head(10)
"train[""Survived""].value_counts()",0,execute_result,"train[""Survived""].value_counts()"
"train[""Sex""].value_counts()",0,execute_result,"train[""Sex""].value_counts()"
"train[""Embarked""].value_counts()",0,execute_result,"train[""Embarked""].value_counts()"
"train[""Pclass""].value_counts()",0,execute_result,"# train[""Name""].value_counts() as imagined are uniques
 # train[""Ticket""].value_counts() is not unique, but has some few repetitions
 train[""Pclass""].value_counts()"
train.describe(),0,execute_result,train.describe()
SETUP train.profile_report(),0,display_data,"import pandas_profiling 
 
 train.profile_report()"
"train.hist(figsize=(20, 15)) plt.show()",0,display_data,"train.hist(figsize=(20, 15))
 plt.show()"
"ASSIGN = train.corr() ASSIGN[""Survived""]",1,execute_result,"corr_matrix = train.corr()
 corr_matrix[""Survived""]"
"sns.violinplot(x=""Sex"", y=""Age"", hue=""Survived"", ASSIGN=train, palette=""muted"", split=True) plt.show()",0,display_data,"# hypothesis for feature engineering ""women and children first""
 sns.violinplot(x=""Sex"", y=""Age"", hue=""Survived"",
                     data=train, palette=""muted"", split=True)
 plt.show()"
"ASSIGN = ASSIGN path* 15 train[[""Age"", ""Survived""]].groupby(['Age']).mean()",1,execute_result,"# Hypotesis: since the survived looks like ""bimodal"" near 15 years, we should try to see 
 # the correlation of categorizing if the passenger is less than 15 years
 
 train[""Age""] = train[""Age""] // 15 * 15
 train[[""Age"", ""Survived""]].groupby(['Age']).mean()"
"ASSIGN = ASSIGN + ASSIGN train[[""RelativesOnboard"", ""Survived""]].groupby([""RelativesOnboard""]).mean()",1,execute_result,"train[""RelativesOnboard""] = train[""SibSp""] + train[""Parch""]
 # train[[""RelativesOnboard"", ""Survived""]].groupby(['RelativesOnboard']).mean()
 train[[""RelativesOnboard"", ""Survived""]].groupby([""RelativesOnboard""]).mean()"
"train[[""SibSp"", ""Survived""]].groupby(['SibSp']).mean()",0,execute_result,"# count total family members look to have a better discrimination on survival rate
 # since number of siblings is nearer to the mean survival rate 38% 
 train[[""SibSp"", ""Survived""]].groupby(['SibSp']).mean()"
"train[[""Parch"", ""Survived""]].groupby(['Parch']).mean()",0,execute_result,"train[[""Parch"", ""Survived""]].groupby(['Parch']).mean()"
train.corr(),0,execute_result,"# Age grouped by 15 years have near 0 correlation, but some groups have more survival rate than others
 # this only means that the relation of age groups and survival rate are non-linear
 train.corr()"
"class DataFrameSelector(BaseEstimator, TransformerMixin): def __init__(self, attribute_names): self.attribute_names = attribute_names def fit(self, X, y=None): return self def transform(self, X): return X[self.attribute_names] class MostFrequentImputer(BaseEstimator, TransformerMixin): def fit(self, X, y=None): self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X], ASSIGN=X.columns) return self def transform(self, X, y=None): return X.fillna(self.most_frequent_) class AgeGrouper(BaseEstimator, TransformerMixin): def __init__(self, new_attribute=""AgeGrp"", attribute_name=""Age"", group_scale=15, del_originals=True): self.group_scale = group_scale self.attribute_name = attribute_name self.new_attribute = new_attribute self.del_originals = del_originals def fit(self, X, y=None): self.age_groups = X[self.attribute_name] path* self.group_scale return self def transform(self, X, y=None): X[self.new_attribute] = self.age_groups if self.del_originals: X.drop(columns=self.attribute_name, axis=1, inplace=True) return X class AtributesAdder(BaseEstimator, TransformerMixin): def __init__(self, new_attribute=""RelativesOnboard"", attribute_names=[""SibSp"", ""Parch""], del_originals=True): self.attribute_names = attribute_names self.final_attr = 0 self.new_attribute = new_attribute self.del_originals = del_originals def fit(self, X, y=None): for attr in self.attribute_names: self.final_attr += X[attr] return self def transform(self, X, y=None): X[self.new_attribute] = self.final_attr if self.del_originals: X.drop(columns=self.attribute_names, axis=1, inplace=True) return X",1,not_existent,"# Transformers created by https://github.com/ageron/handson-ml2
 
 # this transformers we will choose which attributes, late numerical and categorical
 # to use some input strategies
 class DataFrameSelector(BaseEstimator, TransformerMixin):
     def __init__(self, attribute_names):
         self.attribute_names = attribute_names
 
     def fit(self, X, y=None):
         return self
 
     def transform(self, X):
         return X[self.attribute_names]
 
 class MostFrequentImputer(BaseEstimator, TransformerMixin):
     def fit(self, X, y=None):
         self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],
                                         index=X.columns)
         return self
 
     def transform(self, X, y=None):
         return X.fillna(self.most_frequent_)
 
 
 class AgeGrouper(BaseEstimator, TransformerMixin):
     def __init__(self, new_attribute=""AgeGrp"", attribute_name=""Age"", group_scale=15, del_originals=True):
         self.group_scale = group_scale
         self.attribute_name = attribute_name
         self.new_attribute = new_attribute
         self.del_originals = del_originals
 
     def fit(self, X, y=None):
         self.age_groups = X[self.attribute_name] // self.group_scale * self.group_scale
         return self
 
     def transform(self, X, y=None):
         X[self.new_attribute] = self.age_groups
         if self.del_originals:
             X.drop(columns=self.attribute_name, axis=1, inplace=True)
         return X
 
 
 class AtributesAdder(BaseEstimator, TransformerMixin):
     def __init__(self, new_attribute=""RelativesOnboard"", attribute_names=[""SibSp"", ""Parch""], del_originals=True):
         self.attribute_names = attribute_names
         self.final_attr = 0
         self.new_attribute = new_attribute
         self.del_originals = del_originals
 
     def fit(self, X, y=None):
         for attr in self.attribute_names:
             self.final_attr += X[attr]
         return self
 
     def transform(self, X, y=None):
         X[self.new_attribute] = self.final_attr
         if self.del_originals:
             X.drop(columns=self.attribute_names, axis=1, inplace=True)
         return X"
"ASSIGN = Pipeline([ (""select_numeric"", DataFrameSelector([""Age"", ""Fare"", ""SibSp"", ""Parch""])), (""age_grouper"", AgeGrouper(attribute_name=""Age"", group_scale=15)), (""total_relatives"", AtributesAdder(attribute_names=[""SibSp"", ""Parch""], del_originals=True)), (""imputer"", SimpleImputer(strategy=""median"")), ])",1,not_existent,"# Numerical Pipeline
 num_pipeline = Pipeline([
         (""select_numeric"", DataFrameSelector([""Age"", ""Fare"", ""SibSp"", ""Parch""])),
         (""age_grouper"", AgeGrouper(attribute_name=""Age"", group_scale=15)),
         (""total_relatives"", AtributesAdder(attribute_names=[""SibSp"", ""Parch""], del_originals=True)),
         (""imputer"", SimpleImputer(strategy=""median"")),
     ])"
"ASSIGN = Pipeline([ (""select_cat"", DataFrameSelector([""Pclass"", ""Sex"", ""Embarked""])), (""imputer"", MostFrequentImputer()), (""cat_encoder"", OneHotEncoder(sparse=False)), ])",1,not_existent,"# Categorical Pipeline
 cat_pipeline = Pipeline([
         (""select_cat"", DataFrameSelector([""Pclass"", ""Sex"", ""Embarked""])),
         (""imputer"", MostFrequentImputer()),
         (""cat_encoder"", OneHotEncoder(sparse=False)),
     ])"
"SETUP ASSIGN = FeatureUnion(transformer_list=[ (""num_pipeline"", num_pipeline), (""cat_pipeline"", cat_pipeline), ])",1,not_existent,"from sklearn.pipeline import FeatureUnion
 preprocess_pipeline = FeatureUnion(transformer_list=[
         (""num_pipeline"", num_pipeline),
         (""cat_pipeline"", cat_pipeline),
     ])"
"ASSIGN = preprocess_pipeline.fit_transform(train) ASSIGN = train[""Survived""] X_train_val, X_test_val, y_train_val, y_test_val = train_test_split( ASSIGN=0.3, random_state=42)",1,stream,"X_train = preprocess_pipeline.fit_transform(train)
 y_train = train[""Survived""]
 
 X_train_val, X_test_val, y_train_val, y_test_val = train_test_split(
         X_train, y_train, test_size=0.3, random_state=42)"
"ASSIGN = np.zeros_like(corr_matrix) ASSIGN[np.triu_indices_from(ASSIGN)] = True with sns.axes_style(""white""): ASSIGN = plt.subplots(figsize=(7, 5)) ASSIGN = sns.heatmap(corr_matrix, mask=mask, vmax=.3, square=True, cmap=""YlGnBu"")",0,display_data,"mask = np.zeros_like(corr_matrix)
 mask[np.triu_indices_from(mask)] = True
 with sns.axes_style(""white""):
     f, ax = plt.subplots(figsize=(7, 5))
     ax = sns.heatmap(corr_matrix, mask=mask, vmax=.3, square=True, cmap=""YlGnBu"")"
"ASSIGN = { ""KNeighborsClassifier"": KNeighborsClassifier(), ""RandomForest"": RandomForestClassifier(), ""SVM"": SVC(), } ASSIGN = { ""KNeighborsClassifier"": { ""n_neighbors"": randint(low=1, high=30), }, ""RandomForest"": { ""n_estimators"": randint(low=1, high=200), ""max_features"": randint(low=1, high=8), }, ""SVM"": { ""kernel"": [""linear"", ""rbf""], ""C"": reciprocal(0.1, 200000), ""gamma"": expon(scale=1.0), } }",0,not_existent,"models = {
     ""KNeighborsClassifier"": KNeighborsClassifier(),
     ""RandomForest"": RandomForestClassifier(),
     ""SVM"": SVC(),
 }
 
 randomized_params = {
     ""KNeighborsClassifier"": {
         ""n_neighbors"": randint(low=1, high=30),
     },
     ""RandomForest"": {
         ""n_estimators"": randint(low=1, high=200),
         ""max_features"": randint(low=1, high=8),
     },
     ""SVM"": {
         ""kernel"": [""linear"", ""rbf""],
         ""C"": reciprocal(0.1, 200000),
         ""gamma"": expon(scale=1.0),
     }
 }"
"ASSIGN = ""accuracy"" for model_name in models.keys(): ASSIGN = RandomizedSearchCV(models[model_name], param_distributions=randomized_params[model_name], n_iter=100, ASSIGN=ASSIGN, cv=5, verbose=2, random_state=42, n_jobs=-1) ASSIGN.fit(X_train, y_train) ASSIGN = cross_val_score(grid.best_estimator_, X_train_val, y_train_val, cv=10, ASSIGN=ASSIGN, verbose=0, n_jobs=-1) ASSIGN = scores.mean() ASSIGN = scores.std() ASSIGN = grid.score(X_test_val, y_test_val) ASSIGN = {'Model_Name': model_name, 'Parameters': grid.best_params_, 'Test_Score': Test_scores, 'CV Mean': ASSIGN, 'CV STDEV': ASSIGN} ASSIGN = grid.best_estimator_.fit(X_train_val, y_train_val) ASSIGN.score(X_test_val, y_test_val) ASSIGN = clf.predict(X_test_val) ASSIGN = classification_report(y_test_val, y_pred) save(ASSIGN, ASSIGN, ASSIGN, name=""titanic_""+model_name+""_02"", cv_scores=ASSIGN)",0,stream,"scoring = ""accuracy""
 
 
 for model_name in models.keys():
     grid = RandomizedSearchCV(models[model_name], param_distributions=randomized_params[model_name], n_iter=100,
                                   scoring=scoring, cv=5, verbose=2, random_state=42,  n_jobs=-1)
     grid.fit(X_train, y_train)
 
     scores = cross_val_score(grid.best_estimator_, X_train_val, y_train_val, cv=10,
                              scoring=scoring, verbose=0, n_jobs=-1)
 
     CV_scores = scores.mean()
     STDev = scores.std()
     Test_scores = grid.score(X_test_val, y_test_val)
 
     cv_score = {'Model_Name': model_name, 'Parameters': grid.best_params_, 'Test_Score': Test_scores,
                 'CV Mean': CV_scores, 'CV STDEV': STDev}
 
     clf = grid.best_estimator_.fit(X_train_val, y_train_val)
     clf.score(X_test_val, y_test_val)
     y_pred = clf.predict(X_test_val)
     clf_report = classification_report(y_test_val, y_pred)
     save(grid, cv_score, clf_report, name=""titanic_""+model_name+""_02"", cv_scores=scores)"
"ASSIGN = load(""titanic_KNeighborsClassifier_02"", with_metadata=True)",0,stream,"knn_grid = load(""titanic_KNeighborsClassifier_02"", with_metadata=True)"
"ASSIGN = load(""titanic_SVM_02"", with_metadata=True)",0,stream,"svc_grid = load(""titanic_SVM_02"", with_metadata=True)"
"ASSIGN = load(""titanic_RandomForest_02"", with_metadata=True)",0,stream,"random_forest_grid = load(""titanic_RandomForest_02"", with_metadata=True)"
"plt.figure(figsize=(8, 4)) plt.plot([1]*10, knn_grid[""cv_scores""], ""."") plt.plot([2]*10, svc_grid[""cv_scores""], ""."") plt.plot([3]*10, random_forest_grid[""cv_scores""], ""."") plt.boxplot([knn_grid[""cv_scores""], svc_grid[""cv_scores""], random_forest_grid[""cv_scores""]], labels=(""KNN"", ""SVM"", ""Random Forest"")) plt.ylabel(""Accuracy"", fontsize=14) plt.show()",0,display_data,"plt.figure(figsize=(8, 4))
 plt.plot([1]*10, knn_grid[""cv_scores""], ""."")
 plt.plot([2]*10, svc_grid[""cv_scores""], ""."")
 plt.plot([3]*10, random_forest_grid[""cv_scores""], ""."")
 plt.boxplot([knn_grid[""cv_scores""], svc_grid[""cv_scores""], random_forest_grid[""cv_scores""]], labels=(""KNN"", ""SVM"", ""Random Forest""))
 plt.ylabel(""Accuracy"", fontsize=14)
 plt.show()"
"ASSIGN = random_forest_grid[""model""].best_estimator_.fit(X_train, y_train)",0,not_existent,"random_forest = random_forest_grid[""model""].best_estimator_.fit(X_train, y_train)"
ASSIGN = preprocess_pipeline.fit_transform(test),1,stream,X_test = preprocess_pipeline.fit_transform(test)
ASSIGN = random_forest.predict(X_test),0,not_existent,y_pred = random_forest.predict(X_test)
"ASSIGN = pd.DataFrame(columns=['PassengerId', 'Survived']) ASSIGN['PassengerId'] = test['PassengerId'] ASSIGN['Survived'] = y_pred",1,not_existent,"submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])
 submission_df['PassengerId'] = test['PassengerId']
 submission_df['Survived'] = y_pred"
"submission_df.to_csv(""path"", header=True, index=False) submission_df.head(10)",0,execute_result,"submission_df.to_csv(""/kaggle/working/titanic_02.csv"", header=True, index=False)
 submission_df.head(10)"
SETUP,0,not_existent,"# Importando as funes necessrias
 from __future__ import print_function
 
 import tensorflow as tf 
 import matplotlib.pyplot as plt
 import numpy as np
 import pandas as pd
 import seaborn as sns
 
 from tensorflow.keras.models import Sequential
 from tensorflow.keras.layers import Dense
 from tensorflow.keras.optimizers import RMSprop
 
 from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import LabelEncoder"
ASSIGN = pd.read_csv('..path') ASSIGN.head(),0,execute_result,"# Lendo o dataset e imprimindo uma amostra
 breast_cancer = pd.read_csv('../input/data.csv')
 breast_cancer.head()"
breast_cancer.info(),0,stream,"# Descrevendo as informaes dos tipos de dados do dataset
 breast_cancer.info()"
CHECKPOINT breast_cancer.shape,0,execute_result,"# Verificando o tamanho do dataset (569 linhas / 33 colunas)
 breast_cancer.shape"
breast_cancer.describe(),0,execute_result,"# Gerando estatsticas descritivas do dataset que apresentam um resumo dos dados, excluindo valores nulos
 breast_cancer.describe()"
breast_cancer.groupby('diagnosis').size(),0,execute_result,"# Contagem de registros agrupados pela coluna 'diagnosis', que representa o diagnstico (B = Benigno / M = Maligno)
 breast_cancer.groupby('diagnosis').size()"
breast_cancer.isnull().sum(),0,execute_result,"# Contagem de registros nulos por coluna
 breast_cancer.isnull().sum()"
ASSIGN = breast_cancer.columns[2:-1] ASSIGN = breast_cancer[feature_names] ASSIGN = breast_cancer.diagnosis,1,not_existent,"# As colunas 'id' e 'Unnamed: 32' no so teis para a anlise e sero descartadas 
 feature_names = breast_cancer.columns[2:-1]
 x = breast_cancer[feature_names]
 # A coluna 'diagnosis'  a caracterstica que vamos prever
 y = breast_cancer.diagnosis"
ASSIGN = LabelEncoder() ASSIGN = class_le.fit_transform(breast_cancer.diagnosis.values),1,not_existent,"# Transforma os dados da coluna 'diagnosis' para valores binrios (M = 1 / B = 0)
 class_le = LabelEncoder()
 y = class_le.fit_transform(breast_cancer.diagnosis.values)"
"sns.heatmap( ASSIGN=x.corr(), ASSIGN=True, ASSIGN='.2f', ASSIGN='RdYlGn' ) ASSIGN = plt.gcf() ASSIGN.set_size_inches(20, 16) plt.show()",0,display_data,"# Gera uma matriz de correlao (heatmap) que fornece informaes teis sobre a relao entre cada varivel do conjunto de dados
 sns.heatmap(
     data=x.corr(),
     annot=True,
     fmt='.2f',
     cmap='RdYlGn'
 )
 
 fig = plt.gcf()
 fig.set_size_inches(20, 16)
 
 plt.show()"
"CHECKPOINT ASSIGN = train_test_split( x, y, ASSIGN=42, ASSIGN=0.32 ) print(x_train.shape, y_train.shape) print(x_test.shape, y_test.shape)",1,stream,"# Obtendo os conjuntos de treino e teste, separando 32% do conjunto de dados para teste (test_size=0.32) e o restante para treino
 x_train, x_test, y_train, y_test = train_test_split(
     x,
     y,
     random_state=42,
     test_size=0.32
 )
 
 print(x_train.shape, y_train.shape)
 print(x_test.shape, y_test.shape)"
"CHECKPOINT ASSIGN = 64 ASSIGN = 2 ASSIGN = 200 ASSIGN = ASSIGN.astype('float32') ASSIGN = ASSIGN.astype('float32') ASSIGN = tf.keras.utils.to_categorical(ASSIGN, num_classes) ASSIGN = tf.keras.utils.to_categorical(ASSIGN, num_classes) ASSIGN = Sequential() ASSIGN.add(tf.keras.layers.Dense(100, input_dim=30, activation='sigmoid')) ASSIGN.add(tf.keras.layers.Dense(25, input_dim=30, activation='relu')) ASSIGN.add(tf.keras.layers.Dense(2, activation='softmax')) ASSIGN.summary() ASSIGN.compile(loss='categorical_crossentropy', ASSIGN=RMSprop(0.0001), ASSIGN=['accuracy']) ASSIGN = model.fit(x_train, y_train, ASSIGN=ASSIGN, ASSIGN=ASSIGN, ASSIGN=1, ASSIGN=(x_test, y_test)) ASSIGN = model.evaluate(x_test, y_test, verbose=1) print('Test loss:', ASSIGN[0]) print('Test accuracy:', ASSIGN[1]) plt.figure() plt.plot(np.arange(0,ASSIGN), ASSIGN.history[""loss""], label=""train_loss"") plt.plot(np.arange(0,ASSIGN), ASSIGN.history[""val_loss""], label=""val_loss"") plt.plot(np.arange(0,ASSIGN), ASSIGN.history[""acc""], label=""train_acc"") plt.plot(np.arange(0,ASSIGN), ASSIGN.history[""val_acc""], label=""val_acc"") plt.title(""Acurcia"") plt.xlabel(""pocas #"") plt.ylabel(""Losspath"") plt.legend() plt.show()",0,stream,"# Implementao da rede neural, utilizando 2 classes (diagnosis) e 200 pocas
 batch_size = 64
 num_classes = 2
 epochs = 200
 
 # Transformando os dados de entrada para float32
 x_train = x_train.astype('float32')
 x_test = x_test.astype('float32')
 
 # Convertendo os vetores das classes em matrizes de classificao binrias
 y_train = tf.keras.utils.to_categorical(y_train, num_classes)
 y_test = tf.keras.utils.to_categorical(y_test, num_classes)
 
 # Definio da arquitetura do modelo
 model = Sequential()
 # Camadas do modelo
 model.add(tf.keras.layers.Dense(100, input_dim=30, activation='sigmoid'))
 model.add(tf.keras.layers.Dense(25, input_dim=30, activation='relu'))
 model.add(tf.keras.layers.Dense(2, activation='softmax'))
 
 # Fim - Definio da arquitetura do modelo
 
 model.summary()
 
 model.compile(loss='categorical_crossentropy',
               optimizer=RMSprop(0.0001),
               metrics=['accuracy'])
 
 # Treinamento do modelo 
 H = model.fit(x_train, y_train,
                     batch_size=batch_size,
                     epochs=epochs,
                     verbose=1,
                     validation_data=(x_test, y_test))
 
 # Avaliao do modelo no conjunto de teste
 score = model.evaluate(x_test, y_test, verbose=1)
 
 print('Test loss:', score[0])
 print('Test accuracy:', score[1])
 
 # Plotando 'loss' e 'accuracy' para os datasets 'train' e 'test'
 plt.figure()
 plt.plot(np.arange(0,epochs), H.history[""loss""], label=""train_loss"")
 plt.plot(np.arange(0,epochs), H.history[""val_loss""], label=""val_loss"")
 plt.plot(np.arange(0,epochs), H.history[""acc""], label=""train_acc"")
 plt.plot(np.arange(0,epochs), H.history[""val_acc""], label=""val_acc"")
 plt.title(""Acurcia"")
 plt.xlabel(""pocas #"")
 plt.ylabel(""Loss/Accuracy"")
 plt.legend()
 plt.show()"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,not_existent,"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load  import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  # Input data files are available in the read-only ""../input/"" directory # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory  import os for dirname, _, filenames in os.walk('/kaggle/input'):     for filename in filenames:         print(os.path.join(dirname, filename))  # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All""  # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
"ASSIGN = pd.read_csv(""..path"", index_col=""ID"").drop(""MAC_CODE"", axis = 1)",0,not_existent,"train = pd.read_csv(""../input/cs-challenge/training_set.csv"", index_col=""ID"").drop(""MAC_CODE"", axis = 1)"
ASSIGN = ASSIGN.dropna(axis = 1),1,not_existent,"train = train.dropna(axis = 1)
"
"CHECKPOINT SETUP ASSIGN= StandardScaler() ASSIGN.fit(train) ASSIGN = pd.DataFrame(scaler.transform(ASSIGN), columns = ASSIGN.columns, index = ASSIGN.index) train",1,execute_result,"from sklearn.preprocessing import StandardScaler
 
 scaler= StandardScaler()
 scaler.fit(train)
 train = pd.DataFrame(scaler.transform(train), columns = train.columns, index = train.index)
 train"
"SETUP CHECKPOINT ASSIGN = linear_model.LassoCV(cv=5, random_state=0, max_iter=10000).fit(train.drop(""TARGET"",axis=1), train[""TARGET""]) print(ASSIGN.score(train.drop(,axis=1), train[]))",0,stream,"from sklearn import linear_model
 
 lasso_reg = linear_model.LassoCV(cv=5, random_state=0, max_iter=10000).fit(train.drop(""TARGET"",axis=1), train[""TARGET""])
 print(lasso_reg.score(train.drop(""TARGET"",axis=1), train[""TARGET""]))"
"SETUP plt.figure(figsize=(20,5)) plt.xticks(rotation = 'vertical') plt.bar(train.drop(""TARGET"", axis=1).columns, lasso_reg.coef_)",0,execute_result,"import matplotlib.pyplot as plt
 
 plt.figure(figsize=(20,5))
 plt.xticks(rotation = 'vertical')
 plt.bar(train.drop(""TARGET"", axis=1).columns, lasso_reg.coef_)"
ASSIGN = ASSIGN = train.columns.to_list() ASSIGN = [x for x in column_list if x.find('_max') == -1 and x.find('_min') == -1 and x.find('_c') == -1],1,not_existent,"column_list = column_list = train.columns.to_list()
 base_cols = [x for x in column_list if x.find('_max') == -1 and x.find('_min') == -1 and x.find('_c') == -1]"
CHECKPOINT ASSIGN = train[base_cols] train_base,1,execute_result,"train_base = train[base_cols]
 train_base"
"SETUP CHECKPOINT ASSIGN = linear_model.LassoCV(cv=5, random_state=0, max_iter=20000, fit_intercept=True,normalize = True).fit(train_base.drop(""TARGET"",axis=1), train_base[""TARGET""]) print(ASSIGN.score(train_base.drop(,axis=1), train_base[]))",0,stream,"from sklearn import linear_model
 
 lasso_reg_base = linear_model.LassoCV(cv=5, random_state=0, max_iter=20000, fit_intercept=True,normalize = True).fit(train_base.drop(""TARGET"",axis=1), train_base[""TARGET""])
 print(lasso_reg_base.score(train_base.drop(""TARGET"",axis=1), train_base[""TARGET""]))"
"SETUP plt.figure(figsize=(20,5)) plt.xticks(rotation = 'vertical') plt.bar(train_base.drop(""TARGET"", axis=1).columns, lasso_reg_base.coef_)",0,execute_result,"import matplotlib.pyplot as plt
 
 plt.figure(figsize=(20,5))
 plt.xticks(rotation = 'vertical')
 plt.bar(train_base.drop(""TARGET"", axis=1).columns, lasso_reg_base.coef_)"
"ASSIGN = pd.read_csv(""..path"", index_col = ""ID"").drop(""MAC_CODE"",axis=1) ASSIGN = np.ones(len(test.index)) ASSIGN = pd.DataFrame(scaler.transform(ASSIGN[train.columns.to_list()]), index=ASSIGN.index, columns=train.columns) ASSIGN = lasso_reg.predict(test[train.columns.to_list()].drop(""TARGET"", axis=1))",1,not_existent,"test = pd.read_csv(""../input/cs-challenge/test_set.csv"", index_col = ""ID"").drop(""MAC_CODE"",axis=1)
 test[""TARGET""] = np.ones(len(test.index))
 #test[train.columns.to_list()]
 test = pd.DataFrame(scaler.transform(test[train.columns.to_list()]), index=test.index, columns=train.columns)
 p1 = lasso_reg.predict(test[train.columns.to_list()].drop(""TARGET"", axis=1))"
"ASSIGN = p1 ASSIGN = pd.DataFrame(scaler.inverse_transform(test[train.columns.to_list()]), index=test.index, columns=test.columns)['TARGET'] ASSIGN.to_csv('ASSIGN.csv')",1,not_existent,"test['TARGET'] = p1
 a1 = pd.DataFrame(scaler.inverse_transform(test[train.columns.to_list()]), index=test.index, columns=test.columns)['TARGET']
 a1.to_csv('a1.csv')"
"ASSIGN = lasso_reg_base.predict(test[train_base.columns.to_list()].drop(""TARGET"", axis=1))",1,not_existent,"p2 = lasso_reg_base.predict(test[train_base.columns.to_list()].drop(""TARGET"", axis=1))"
"ASSIGN = p2 ASSIGN = pd.DataFrame(scaler.inverse_transform(test[train.columns.to_list()]), index=test.index, columns=test.columns)['TARGET'] ASSIGN.to_csv('ASSIGN.csv')",1,not_existent,"test[""TARGET""] = p2
 a2 = pd.DataFrame(scaler.inverse_transform(test[train.columns.to_list()]), index=test.index, columns=test.columns)['TARGET']
 a2.to_csv('a2.csv')"
SETUP,0,not_existent,import pandas as pd import matplotlib.pyplot as plt
"ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"")",0,not_existent,"train = pd.read_csv(""../input/train.csv"") test = pd.read_csv(""../input/test.csv"")"
"train[""idhogar""]",0,not_existent,"train[""idhogar""]"
"train[""parentesco1""].value_counts()",0,not_existent,"train[""parentesco1""].value_counts()"
"ASSIGN = train.drop(train[train[""parentesco1""] == 0].index)",1,not_existent,"train_only_heads = train.drop(train[train[""parentesco1""] == 0].index)"
"train_only_heads[""parentesco1""]",0,not_existent,"train_only_heads[""parentesco1""]"
"ASSIGN = train_only_heads.dropna(thresh=len(train_only_heads[""parentesco1""])path, axis=""columns"")",1,not_existent,"train_hna = train_only_heads.dropna(thresh=len(train_only_heads[""parentesco1""])/2, axis=""columns"")"
ASSIGN = ASSIGN.dropna(),1,not_existent,train_hna = train_hna.dropna()
"train_hna[(train_hna == 'yes') | (train_hna == 'no')].dropna(axis = 'columns', how='all')",0,not_existent,"train_hna[(train_hna == 'yes') | (train_hna == 'no')].dropna(axis = 'columns', how='all')"
CHECKPOINT train_hna,0,not_existent,train_hna
"ASSIGN = train_hna.drop(['Target','Id','idhogar','dependency','edjefe','edjefa'] ,axis = 'columns') ASSIGN = train_hna.Target",1,not_existent,"Xtrain_h = train_hna.drop(['Target','Id','idhogar','dependency','edjefe','edjefa'] ,axis = 'columns') Ytrain_h = train_hna.Target"
SETUP,0,not_existent,from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import cross_val_score
ASSIGN = KNeighborsClassifier(n_neighbors=10),0,not_existent,knn = KNeighborsClassifier(n_neighbors=10)
"ASSIGN = cross_val_score(knn, Xtrain_h, Ytrain_h, cv=10)",0,not_existent,"scores = cross_val_score(knn, Xtrain_h, Ytrain_h, cv=10)"
scores.mean(),0,not_existent,scores.mean()
"ASSIGN = [] for i in range(50): ASSIGN = KNeighborsClassifier(n_neighbors=i+1) ASSIGN = cross_val_score(knn, Xtrain_h, Ytrain_h, cv=5) ASSIGN.append(ASSIGN.mean())",0,not_existent,"score_array = [] for i in range(50):     knn = KNeighborsClassifier(n_neighbors=i+1)     scores = cross_val_score(knn, Xtrain_h, Ytrain_h, cv=5)     score_array.append(scores.mean())"
"plt.plot(score_array, 'ro')",0,not_existent,"plt.plot(score_array, 'ro')"
"ASSIGN = KNeighborsClassifier(n_neighbors=33) ASSIGN.fit(Xtrain_h, Ytrain_h)",0,not_existent,"knn = KNeighborsClassifier(n_neighbors=33) knn.fit(Xtrain_h, Ytrain_h)"
"CHECKPOINT ASSIGN = test.drop(['Id','idhogar','dependency','edjefe','edjefa','rez_esc', 'v18q1', 'v2a1'] ,axis = 'columns') Xtest",1,not_existent,"Xtest = test.drop(['Id','idhogar','dependency','edjefe','edjefa','rez_esc', 'v18q1', 'v2a1'] ,axis = 'columns') Xtest"
ASSIGN = ASSIGN.fillna(0),1,not_existent,Xtest = Xtest.fillna(0)
ASSIGN = knn.predict(Xtest),0,not_existent,pred = knn.predict(Xtest)
CHECKPOINT ASSIGN = pd.DataFrame(test.Id) ASSIGN = pred prediction,1,not_existent,prediction = pd.DataFrame(test.Id) prediction['Target'] = pred prediction
"prediction.to_csv(""submition.csv"",index = False)",0,not_existent,"prediction.to_csv(""submition.csv"",index = False)"
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'Mall_Customers.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,stream,"nRowsRead = 1000 # specify 'None' if want to read whole file
 df1 = pd.read_csv('../input/Mall_Customers.csv', delimiter=',', nrows = nRowsRead)
 df1.dataframeName = 'Mall_Customers.csv'
 nRow, nCol = df1.shape
 print(f'There are {nRow} rows and {nCol} columns')"
"SETUP def calc_logloss(targets, outputs, eps=1e-6): ASSIGN = [log_loss(np.floor(targets[:,i]), np.clip(outputs[:,i], eps, 1-eps)) for i in range(6)] return np.average(ASSIGN, weights=[2,1,1,1,1,1]) warnings.filterwarnings(""ignore"") ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"") ASSIGN = ASSIGN.merge(dup, on = 'SOPInstanceUID', how = 'left')",1,not_existent,"from sklearn.metrics import log_loss
 def calc_logloss(targets, outputs, eps=1e-6):
     logloss_classes = [log_loss(np.floor(targets[:,i]), np.clip(outputs[:,i], eps, 1-eps)) for i in range(6)]
     return np.average(logloss_classes, weights=[2,1,1,1,1,1])
 
 import pandas as pd
 import pickle
 import os
 import numpy as np
 import warnings
 warnings.filterwarnings(""ignore"")
 dup = pd.read_csv(""../input/stage1-test-gt/dup_s1_test.csv"")
 test = pd.read_csv(""../input/stage1-test-gt/s1_test_results.csv"")
 test = test.merge(dup, on = 'SOPInstanceUID', how = 'left')"
"def get_split_result(filename, test, eps, rm_dup=False): ASSIGN = pd.read_csv(filename) ASSIGN['type'] = ASSIGN['ID'].apply(lambda x: x.split('_')[2]) ASSIGN['name'] = ASSIGN['ID'].apply(lambda x: x.split('_')[1]) ASSIGN = f1[['ASSIGN']] ASSIGN = f1[['name','Label']][f1['type'] == 'epidural'] ASSIGN.columns = ['ASSIGN','epidural'] ASSIGN = f1[['name','Label']][f1['type'] == 'intraparenchymal'] ASSIGN.columns = ['ASSIGN','intraparenchymal'] ASSIGN = f1[['name','Label']][f1['type'] == 'intraventricular'] ASSIGN.columns = ['ASSIGN','intraventricular'] ASSIGN = f1[['name','Label']][f1['type'] == 'subarachnoid'] ASSIGN.columns = ['ASSIGN','subarachnoid'] ASSIGN = f1[['name','Label']][f1['type'] == 'subdural'] ASSIGN.columns = ['ASSIGN','subdural'] ASSIGN = f1[['name','Label']][f1['type'] == 'any'] ASSIGN.columns = ['ASSIGN','any'] ASSIGN = ASSIGN.merge(f1_any, on = 'ASSIGN', how = 'left') ASSIGN = ASSIGN.merge(f1_epidural, on = 'ASSIGN', how = 'left') ASSIGN = ASSIGN.merge(f1_intraparenchymal, on = 'ASSIGN', how = 'left') ASSIGN = ASSIGN.merge(f1_intraventricular, on = 'ASSIGN', how = 'left') ASSIGN = ASSIGN.merge(f1_subarachnoid, on = 'ASSIGN', how = 'left') ASSIGN = ASSIGN.merge(f1_subdural, on = 'ASSIGN', how = 'left') ASSIGN = ASSIGN.drop_duplicates() ASSIGN.rename(columns = {'ASSIGN': 'SOPInstanceUID'}, inplace=True) ASSIGN = 'ID_' + ASSIGN ASSIGN = ASSIGN.merge(test, on = 'SOPInstanceUID', how = 'left') if rm_dup: ASSIGN = name[name['dup'].isnull() == True] else: ASSIGN = name.copy() ASSIGN = name_use[['any_y', 'epidural_y', 'subdural_y', 'subarachnoid_y', 'intraventricular_y', 'intraparenchymal_y']].values ASSIGN = name_use[['any', 'epidural', 'subdural', 'subarachnoid', 'intraventricular', 'intraparenchymal']].values return calc_logloss(ASSIGN, ASSIGN, eps=eps)",1,not_existent,"def get_split_result(filename, test, eps, rm_dup=False):
     f1 = pd.read_csv(filename)
 
     f1['type'] = f1['ID'].apply(lambda x: x.split('_')[2])
     f1['name'] = f1['ID'].apply(lambda x: x.split('_')[1])
 
     name = f1[['name']]
 
     f1_epidural = f1[['name','Label']][f1['type'] == 'epidural']
     f1_epidural.columns = ['name','epidural']
     f1_intraparenchymal = f1[['name','Label']][f1['type'] == 'intraparenchymal']
     f1_intraparenchymal.columns = ['name','intraparenchymal']
     f1_intraventricular = f1[['name','Label']][f1['type'] == 'intraventricular']
     f1_intraventricular.columns = ['name','intraventricular']
     f1_subarachnoid = f1[['name','Label']][f1['type'] == 'subarachnoid']
     f1_subarachnoid.columns = ['name','subarachnoid']
     f1_subdural = f1[['name','Label']][f1['type'] == 'subdural']
     f1_subdural.columns = ['name','subdural']
     f1_any = f1[['name','Label']][f1['type'] == 'any']
     f1_any.columns = ['name','any']
 
     name = name.merge(f1_any, on = 'name', how = 'left')
     name = name.merge(f1_epidural, on = 'name', how = 'left')
     name = name.merge(f1_intraparenchymal, on = 'name', how = 'left')
     name = name.merge(f1_intraventricular, on = 'name', how = 'left')
     name = name.merge(f1_subarachnoid, on = 'name', how = 'left')
     name = name.merge(f1_subdural, on = 'name', how = 'left')
     name = name.drop_duplicates()
     name.rename(columns = {'name': 'SOPInstanceUID'}, inplace=True)
     name['SOPInstanceUID'] = 'ID_' + name['SOPInstanceUID']
     
     name = name.merge(test, on = 'SOPInstanceUID', how = 'left')
     
     if rm_dup:
         name_use = name[name['dup'].isnull() == True] #remove duplicate patientID
     else:
         name_use = name.copy()  #all test
     gt = name_use[['any_y',
            'epidural_y', 'subdural_y', 'subarachnoid_y', 'intraventricular_y',
            'intraparenchymal_y']].values
     pred = name_use[['any',
                'epidural', 'subdural', 'subarachnoid', 'intraventricular',
                'intraparenchymal']].values
     return calc_logloss(gt, pred, eps=eps)"
"get_split_result(""..path"", test, 1e-6)",1,execute_result,"#come from https://www.kaggle.com/krishnakatyal/keras-efficientnet-b3
 get_split_result(""../input/kernel-0076/submission.csv"", test, 1e-6)"
"get_split_result(""..path"", test, 1e-6, rm_dup=True)",1,execute_result,"get_split_result(""../input/kernel-0076/submission.csv"", test, 1e-6, rm_dup=True)"
SETUP,0,not_existent,import pandas as pd from pathlib import Path import matplotlib.pyplot as plt import seaborn as sns
ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path') ASSIGN.head(5),0,not_existent,# opening train/test csv files train = pd.read_csv('../input/train.csv') test = pd.read_csv('../input/test.csv') train.head(5)
CHECKPOINT print(f) print(f),0,not_existent,"# 10 for 1 ration print(f""Train shape : {train.shape}"") print(f""Test shape : {test.shape}"")"
train.nunique(),0,not_existent,# Dataset organization train.nunique()
"plt.figure(figsize = (8, 5)) plt.title('Category Distribuition') sns.distplot(train['landmark_id']) plt.show()",0,not_existent,"plt.figure(figsize = (8, 5)) plt.title('Category Distribuition') sns.distplot(train['landmark_id'])  plt.show()"
CHECKPOINT print(train['landmark_id'].value_counts().head(7)),0,not_existent,# Top categories print(train['landmark_id'].value_counts().head(7))
CHECKPOINT print(f) print(f),0,not_existent,"print(f""Median number : {train['landmark_id'].value_counts().median()}"") print(f""Mean number : {train['landmark_id'].value_counts().mean()}"")"
train['landmark_id'].value_counts().describe(),0,not_existent,# More exhaustive description train['landmark_id'].value_counts().describe()
"f""Number of classes under 10 occurences : {(train['landmark_id'].value_counts() <= 10).sum()}path{len(train['landmark_id'].unique())}""",0,not_existent,"f""Number of classes under 10 occurences : {(train['landmark_id'].value_counts() <= 10).sum()}/{len(train['landmark_id'].unique())}"""
"SETUP def display_category(urls, category_name): ASSIGN = ""width: 180px; margin: 0px; float: left; border: 1px solid black;"" ASSIGN = ''.join([f""<img style='{img_style}' src='{u}' path>"" for _, u in urls.head(12).iteritems()]) display(HTML(ASSIGN)) ASSIGN = train['landmark_id'].value_counts().keys()[0] ASSIGN = train[train['landmark_id'] == category]['url'] display_category(ASSIGN, """")",1,not_existent,"from IPython.display import Image from IPython.core.display import HTML   def display_category(urls, category_name):     img_style = ""width: 180px; margin: 0px; float: left; border: 1px solid black;""     images_list = ''.join([f""<img style='{img_style}' src='{u}' />"" for _, u in urls.head(12).iteritems()])      display(HTML(images_list))  category = train['landmark_id'].value_counts().keys()[0] urls = train[train['landmark_id'] == category]['url'] display_category(urls, """")      "
"ASSIGN = train['landmark_id'].value_counts().keys()[1] ASSIGN = train[train['landmark_id'] == category]['url'] display_category(ASSIGN, """")",1,not_existent,"category = train['landmark_id'].value_counts().keys()[1] urls = train[train['landmark_id'] == category]['url'] display_category(urls, """")"
"ASSIGN = train['landmark_id'].value_counts().keys()[2] ASSIGN = train[train['landmark_id'] == category]['url'] display_category(ASSIGN, """")",1,not_existent,"category = train['landmark_id'].value_counts().keys()[2] urls = train[train['landmark_id'] == category]['url'] display_category(urls, """")"
SETUP,0,not_existent,import os import json
"ASSIGN = {} ASSIGN = [] ASSIGN['people'].append({ 'name': 'Scott', 'website': 'stackabuse.com', 'from': 'Nebraska' }) ASSIGN['people'].append({ 'name': 'Larry', 'website': 'google.com', 'from': 'Michigan' }) ASSIGN['people'].append({ 'name': 'Tim', 'website': 'apple.com', 'from': 'Alabama' }) with open('ASSIGN.json', 'w') as outfile: json.dump(ASSIGN, outfile)",1,not_existent,"  data = {} data['people'] = [] data['people'].append({     'name': 'Scott',     'website': 'stackabuse.com',     'from': 'Nebraska' }) data['people'].append({     'name': 'Larry',     'website': 'google.com',     'from': 'Michigan' }) data['people'].append({     'name': 'Tim',     'website': 'apple.com',     'from': 'Alabama' })  with open('data.json', 'w') as outfile:     json.dump(data, outfile)"
"SETUP ASSIGN=""people""",0,not_existent,"dataset_name=""people""  API={""username"":""tareksherif"",""key"":""f4cf963ba526c529b3a9b0ea5058e6f0""}"
"os.environ['KAGGLE_USERNAME'] = API[""username""] os.environ['KAGGLE_KEY'] = API[""key""]",0,not_existent," os.environ['KAGGLE_USERNAME'] = API[""username""] os.environ['KAGGLE_KEY'] = API[""key""]"
"ASSIGN = { ""title"": dataset_name, ""id"": os.environ['KAGGLE_USERNAME']+""path""+dataset_name, ""licenses"": [ { ""name"": ""CC0-1.0"" } ] } with open('dataset-metadata.json', 'w') as outfile: json.dump(ASSIGN, outfile)",0,not_existent,"data = {   ""title"": dataset_name,   ""id"": os.environ['KAGGLE_USERNAME']+""/""+dataset_name,   ""licenses"": [     {       ""name"": ""CC0-1.0""     }   ] }   with open('dataset-metadata.json', 'w') as outfile:     json.dump(data, outfile)"
SETUP,0,stream,!kaggle datasets create -p .
SETUP,0,not_existent,"import numpy as np 
 import pandas as pd 
 import tensorflow as tf
 import matplotlib.pyplot as plt
 
 import math
 from sklearn import preprocessing, model_selection
 from sklearn.metrics import mean_squared_error
 import xgboost as xgb
 import scipy.stats as stats
"
"ASSIGN = pd.read_csv(""..path"") ASSIGN = pd.read_csv(""..path"")",0,not_existent,"df_belem = pd.read_csv(""../input/temperature-timeseries-for-some-brazilian-cities/station_belem.csv"")
 df_curitiba = pd.read_csv(""../input/temperature-timeseries-for-some-brazilian-cities/station_curitiba.csv"")"
"display(df_belem.shape, df_curitiba.shape)",0,display_data,"#Questo 1
 display(df_belem.shape, df_curitiba.shape)"
"df_belem.set_index('YEAR',inplace=True) df_curitiba.set_index('YEAR',inplace=True) display(df_belem.head()) display(df_curitiba.head())",1,display_data,"#Questo 2
 df_belem.set_index('YEAR',inplace=True)
 df_curitiba.set_index('YEAR',inplace=True)
 display(df_belem.head())
 display(df_curitiba.head())"
"plt.figure(figsize=(25,25)) df_belem.hist()",0,execute_result,"#exerccio 3
 #plota o histograma dos valores 
 #vemos que h vrios outliers prximos de 1000 que provavelmente no so valores de temperatura vlidos
 plt.figure(figsize=(25,25))
 #df_belem.boxplot()
 df_belem.hist()
 
"
display(df_belem['JAN'].value_counts()) display(df_curitiba['JAN'].value_counts()),0,display_data,"#mostra a quantidade de valores nicos no ms de janeiro
 #verificando os valores nicos confirmamos que o valor 999.90  o nico outlier
 display(df_belem['JAN'].value_counts())
 display(df_curitiba['JAN'].value_counts())"
"ASSIGN = df_belem.replace(999.90,np.nan) ASSIGN = ASSIGN.fillna(ASSIGN.mean()) display(ASSIGN) ASSIGN = df_curitiba.replace(999.90,np.nan) ASSIGN = ASSIGN.fillna(ASSIGN.mean()) display(ASSIGN)",1,display_data,"#exerccio 4
 #para tratar os outliers, podemos excluir os dados ausentes (999.90) ou substitu-lo pela mdia do ano anterior e posterior.
 #adotarei a soluo de substituir os nulos pela mdia.
 
 
 #cria um novo dataset transformando o outlier em nulo para aplicao das funes de tratamento
 df_belem_t = df_belem.replace(999.90,np.nan)
 #substitui os valores nulos restantes pela mdia do ano anterior e posterior
 df_belem_t = df_belem_t.fillna(df_belem_t.mean())
 display(df_belem_t)
 
 #cria um novo dataset transformando o outlier em nulo para aplicao das funes de tratamento
 df_curitiba_t = df_curitiba.replace(999.90,np.nan)
 #substitui os valores nulos restantes pela mdia do ano anterior e posterior
 df_curitiba_t = df_curitiba_t.fillna(df_curitiba_t.mean())
 display(df_curitiba_t)
"
"plt.figure(figsize=(10,10)) ASSIGN=plt.figure() ASSIGN=fig.add_axes([0,0,1,1]) ASSIGN.scatter(df_curitiba_t.index, df_curitiba_t.JUL, color='r') ASSIGN.scatter(df_belem_t.index, df_belem_t.JUL, color='b') ASSIGN.set_xlabel('Ano') ASSIGN.set_ylabel('Temperatura (C)') ASSIGN.legend([""Curitiba - Julho"", ""Belm - Julho""]) ASSIGN.set_title('scatter plot') plt.show()",0,display_data,"#exerccio 5
 plt.figure(figsize=(10,10))
 fig=plt.figure()
 ax=fig.add_axes([0,0,1,1])
 ax.scatter(df_curitiba_t.index, df_curitiba_t.JUL, color='r')
 ax.scatter(df_belem_t.index, df_belem_t.JUL, color='b')
 ax.set_xlabel('Ano')
 ax.set_ylabel('Temperatura (C)')
 ax.legend([""Curitiba - Julho"", ""Belm - Julho""])
 ax.set_title('scatter plot')
 plt.show()"
"display(df_curitiba_t['JUL'].describe()) display(df_belem_t['JUL'].describe()) stats.f_oneway(df_belem_t['JUL'], df_curitiba_t['JUL'])",0,display_data,"#questo 6
 display(df_curitiba_t['JUL'].describe())
 display(df_belem_t['JUL'].describe())
 stats.f_oneway(df_belem_t['JUL'], df_curitiba_t['JUL'])"
"ASSIGN = pd.DataFrame(df_curitiba_t['JAN'],columns=['JAN']) ASSIGN['A1'] = ASSIGN['JAN'].shift(1) ASSIGN['A2'] = ASSIGN['JAN'].shift(2) ASSIGN['A3'] = ASSIGN['JAN'].shift(3) ASSIGN = ASSIGN.dropna() display(ASSIGN.head()) X_train, X_test, y_train, y_test = model_selection.train_test_split(ASSIGN.drop(columns=['JAN']),ASSIGN['JAN'],test_size=0.25, random_state=33)",1,display_data,"#exerccio 7
 df_curitiba_jan = pd.DataFrame(df_curitiba_t['JAN'],columns=['JAN'])
 #cria o dataset de previso com os valores dos 3 anos anteriores
 df_curitiba_jan['A1'] = df_curitiba_jan['JAN'].shift(1)
 df_curitiba_jan['A2'] = df_curitiba_jan['JAN'].shift(2)
 df_curitiba_jan['A3'] = df_curitiba_jan['JAN'].shift(3)
 #dropa os primeiros anos (que no tem anos anteriores para montar o dataset)
 df_curitiba_jan = df_curitiba_jan.dropna()
 display(df_curitiba_jan.head())
 #separa em conjuntos de teste e treinamento
 X_train, X_test, y_train, y_test = model_selection.train_test_split(df_curitiba_jan.drop(columns=['JAN']),df_curitiba_jan['JAN'],test_size=0.25, random_state=33)
"
"ASSIGN = xgb.XGBRegressor() ASSIGN.fit(X_train,y_train) ASSIGN = model.predict(data=X_train) ASSIGN = model.predict(data=X_test)",0,stream,"#realiza regresso com o regressor de gradient boosting XGBoost
 #ele frequentemente apresenta resultados iniciais melhores que uma rede neural sem ajustes
 model = xgb.XGBRegressor()
 model.fit(X_train,y_train)
 p_train = model.predict(data=X_train)
 p_test = model.predict(data=X_test)"
"CHECKPOINT ASSIGN = math.sqrt(mean_squared_error(p_train, y_train)) print('Pontuao para o treinamento: %.2f RMSE' % (ASSIGN)) ASSIGN = math.sqrt(mean_squared_error(p_test, y_test)) print('Pontuao para o teste: %.2f RMSE' % (ASSIGN))",0,stream,"#calcula os erros de previso
 trainScore = math.sqrt(mean_squared_error(p_train, y_train))
 print('Pontuao para o treinamento: %.2f RMSE' % (trainScore))
 testScore = math.sqrt(mean_squared_error(p_test, y_test))
 print('Pontuao para o teste: %.2f RMSE' % (testScore))"
"ASSIGN = pd.DataFrame({'YEAR': X_test.index, 'PRED': p_test, 'REAL': y_test}).reset_index(drop=True) display(ASSIGN.sort_values(['YEAR']).set_index('YEAR')) plt.figure(figsize=(10,10)) ASSIGN=plt.figure() ASSIGN=fig.add_axes([0,0,1,1]) ASSIGN.scatter(ASSIGN['YEAR'],ASSIGN['PRED'] , color='r') ASSIGN.scatter(ASSIGN['YEAR'],ASSIGN['REAL'] , color='b') ASSIGN.set_xlabel('Ano') ASSIGN.set_ylabel('Temperatura (C)') ASSIGN.legend([""Curitiba - Janeiro - Previsto"", ""Curitiba - Janeiro - Real""]) ASSIGN.set_title('scatter plot') plt.show()",0,display_data,"#plota o resultado previsto em relao ao real
 df_plot = pd.DataFrame({'YEAR': X_test.index, 'PRED': p_test, 'REAL': y_test}).reset_index(drop=True)
 display(df_plot.sort_values(['YEAR']).set_index('YEAR'))
 plt.figure(figsize=(10,10))
 fig=plt.figure()
 ax=fig.add_axes([0,0,1,1])
 ax.scatter(df_plot['YEAR'],df_plot['PRED'] , color='r')
 ax.scatter(df_plot['YEAR'],df_plot['REAL'] , color='b')
 ax.set_xlabel('Ano')
 ax.set_ylabel('Temperatura (C)')
 ax.legend([""Curitiba - Janeiro - Previsto"", ""Curitiba - Janeiro - Real""])
 ax.set_title('scatter plot')
 plt.show()"
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed
 # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
 # For example, here's several helpful packages to load
 
 import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
 import matplotlib.pyplot as plt
 import seaborn as sns
 from sklearn import metrics 
 from sklearn.metrics import r2_score
 from sklearn.model_selection import train_test_split
 from sklearn.ensemble import RandomForestRegressor
 # Input data files are available in the read-only ""../input/"" directory
 # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
 
 import os
 for dirname, _, filenames in os.walk('/kaggle/input'):
     for filename in filenames:
         print(os.path.join(dirname, filename))
 
 # You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using ""Save & Run All"" 
 # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
"ASSIGN = pd.read_csv(""..path"")",0,not_existent,"sales_data = pd.read_csv(""../input/competitive-data-science-predict-future-sales/sales_train.csv"")"
sales_data.head(),0,execute_result,sales_data.head()
ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path'),0,not_existent,"itemcategories_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')
 items_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')
 shops_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')
 test_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')"
itemcategories_data.info() itemcategories_data.head(),0,stream,"itemcategories_data.info()
 itemcategories_data.head()"
items_data.info() items_data.head(),0,stream,"items_data.info()
 items_data.head()"
shops_data.info() shops_data.head(),0,stream,"shops_data.info()
 shops_data.head()"
test_data.info() test_data.head(),0,stream,"test_data.info()
 test_data.head()"
shops_data.isnull().sum(),0,execute_result,"shops_data.isnull().sum()
"
sales_data.isnull().sum(),0,execute_result,"sales_data.isnull().sum()
"
"plt.figure(figsize=(10,4)) sns.scatterplot(x=sales_data.item_cnt_day, y=sales_data.item_price, data=sales_data)",0,execute_result,"plt.figure(figsize=(10,4))
 sns.scatterplot(x=sales_data.item_cnt_day, y=sales_data.item_price, data=sales_data)"
ASSIGN = ASSIGN[ASSIGN.item_price<45000] ASSIGN = ASSIGN[ASSIGN.item_cnt_day<600],1,not_existent,"sales_data = sales_data[sales_data.item_price<45000]
 sales_data = sales_data[sales_data.item_cnt_day<600]"
ASSIGN = sales_data ASSIGN['month'] = pd.DatetimeIndex(ASSIGN['date']).month ASSIGN['year'] = pd.DatetimeIndex(ASSIGN['date']).year ASSIGN.head(10),1,execute_result,"sales_train_sub = sales_data
 sales_train_sub['month'] = pd.DatetimeIndex(sales_train_sub['date']).month
 sales_train_sub['year'] = pd.DatetimeIndex(sales_train_sub['date']).year
 sales_train_sub.head(10)"
"sats_grup = sales_train_sub.groupby([""date_block_num"",""shop_id"",""item_id""])[""item_cnt_day""].agg('sum').reset_index() ASSIGN=sats_grup.iloc[:,:-1] ASSIGN=sats_grup.iloc[:,-1:] ASSIGN = train_test_split(x,y,test_size=0.25, random_state=0)",1,not_existent,"sats_grup = sales_train_sub.groupby([""date_block_num"",""shop_id"",""item_id""])[""item_cnt_day""].agg('sum').reset_index()
 
 x=sats_grup.iloc[:,:-1]
 y=sats_grup.iloc[:,-1:]
 x_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.25, random_state=0)"
CHECKPOINT x,0,execute_result,x
"SETUP CHECKPOINT ASSIGN = ExtraTreesRegressor(n_estimators=25,random_state=16) ASSIGN.fit(x_train,y_train.values.ravel()) ASSIGN = etr.predict(x_test) print(,r2_score(y_test,ASSIGN)) print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, ASSIGN)) print('Root Mean Squared Error:', metrics.mean_squared_error(y_test, ASSIGN, squared=False))",0,stream,"from sklearn.ensemble import ExtraTreesRegressor
 etr = ExtraTreesRegressor(n_estimators=25,random_state=16)
 etr.fit(x_train,y_train.values.ravel())
 y_pred = etr.predict(x_test)
 
 
 print(""R2 Score:"",r2_score(y_test,y_pred))
 print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))
 print('Root Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred, squared=False))"
SETUP CHECKPOINT print(os.listdir()),0,not_existent,"# This Python 3 environment comes with many helpful analytics libraries installed # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python # For example, here's several helpful packages to load in   import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import matplotlib.pyplot as plt import seaborn as sns  # visualization tool  # Input data files are available in the ""../input/"" directory. # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory  import os print(os.listdir(""../input""))  # Any results you write to the current directory are saved as output."
"ASSIGN = pd.read_csv(""..path"")",0,not_existent,"forest_data = pd.read_csv(""../input/forest-area-of-land-area/forest_area.csv"")"
forest_data.head(20),0,not_existent,forest_data.head(20)
forest_data.info(),0,not_existent,forest_data.info()
"ASSIGN = pd.read_csv(""..path"")",0,not_existent,"happiness_data = pd.read_csv(""../input/world-happiness/2015.csv"")"
happiness_data.info() happiness_data.head(5),0,not_existent,happiness_data.info() happiness_data.head(5)
"ASSIGN = pd.concat([forest_data.iloc[:,:1],forest_data.iloc[:,-1]],axis=1)",1,not_existent,"new_forest_data = pd.concat([forest_data.iloc[:,:1],forest_data.iloc[:,-1]],axis=1) # getting country name and 2015 forest rate."
"ASSIGN = happiness_data.merge(new_forest_data, left_on='Country',right_on='CountryName') ASSIGN.rename(columns={'2015': 'ForestRatio'}, inplace=True) ASSIGN.drop([""CountryName""], axis= 1, inplace=True)",1,not_existent,"#merging happiness data and forest data, and then rename 2015 column to ForestRatio and Drop CountryName column which is dublicate because of merging. result = happiness_data.merge(new_forest_data, left_on='Country',right_on='CountryName')  result.rename(columns={'2015': 'ForestRatio'}, inplace=True) result.drop([""CountryName""], axis= 1, inplace=True)"
result.head(20),0,not_existent,result.head(20)
"ASSIGN = result[pd.notnull(result.ForestRatio) & result.ForestRatio > 0] ASSIGN.columns = [each.replace("" "","""").replace(""("",""_"").replace("")"","""") for each in ASSIGN.columns]",1,not_existent,"main_data = result[pd.notnull(result.ForestRatio) & result.ForestRatio > 0] #ForestRatio should be not null and greater than 0 main_data.columns = [each.replace("" "","""").replace(""("",""_"").replace("")"","""") for each in main_data.columns]"
main_data.corr(),0,not_existent,main_data.corr()
"plt.scatter(x = main_data.HappinessScore, y = main_data.ForestRatio) plt.xlabel(""Happiness Score"") plt.ylabel(""Forest Ratio"") plt.title(""Happiness Score vs Forest Ratio"")",0,not_existent,"plt.scatter(x = main_data.HappinessScore, y = main_data.ForestRatio) plt.xlabel(""Happiness Score"") plt.ylabel(""Forest Ratio"") plt.title(""Happiness Score vs Forest Ratio"")"
"ASSIGN = main_data.groupby([""Region""]).mean() ASSIGN.corr()",1,not_existent,"#ok lets start to change our perspective new_data = main_data.groupby([""Region""]).mean() new_data.corr()"
CHECKPOINT new_data,0,not_existent,new_data
"plt.scatter(x = new_data.ForestRatio, y = new_data.Freedom) plt.xlabel(""Forest Ratio"") plt.ylabel(""Freedom"") plt.title(""Forest Ratio vs Freedom"")",0,not_existent,"plt.scatter(x = new_data.ForestRatio, y = new_data.Freedom) plt.xlabel(""Forest Ratio"") plt.ylabel(""Freedom"") plt.title(""Forest Ratio vs Freedom"")"
"CHECKPOINT happiness_data.columns = [ each.replace("" "","""").replace(""("",""_"").replace("")"","""") for each in happiness_data.columns ] happiness_data.columns",1,not_existent,"happiness_data.columns = [ each.replace("" "","""").replace(""("",""_"").replace("")"","""") for each in happiness_data.columns ] happiness_data.columns"
happiness_data.corr(),0,not_existent,happiness_data.corr()
"ASSIGN = plt.subplots(figsize=(15, 15)) sns.heatmap(happiness_data.corr(), annot=True, linewidths=.5, fmt= '.2f',ax=ax) plt.show()",0,not_existent,"#correlation map f,ax = plt.subplots(figsize=(15, 15)) sns.heatmap(happiness_data.corr(), annot=True, linewidths=.5, fmt= '.2f',ax=ax) plt.show()"
"happiness_data.Family.plot(color = 'r',label = 'Family',linewidth=1, alpha = 0.9,grid = True) happiness_data.Economy_GDPperCapita.plot(color = 'orange',label = 'Economy_GDPperCapita',linewidth=1, alpha = 0.9,grid = True) happiness_data.Health_LifeExpectancy.plot(color = 'yellow',label = 'Health (Life Expectancy)',linewidth=1, alpha = 1.0,grid = True) happiness_data.Freedom.plot(color = 'g',label = 'Freedom',linewidth=1, alpha = 0.9,grid = True) happiness_data.Trust_GovernmentCorruption.plot(color = 'black',label = 'Trust (Government Corruption)',linewidth=1, alpha = 0.9,grid = True) happiness_data.Generosity.plot(color = 'b',label = 'Generosity',linewidth=1, alpha = 0.9,grid = True) happiness_data.DystopiaResidual.plot(color = 'gray',label = 'Dystopia Residual',linewidth=1, alpha = 0.9,grid = True) plt.legend() plt.xlabel('Happiness Rank') plt.ylabel('Score') plt.title('Happiness Factors Line Plot Graph') ASSIGN = plt.gcf() ASSIGN.set_size_inches(18.5, 10.5, forward=True) plt.show()",0,not_existent,"happiness_data.Family.plot(color = 'r',label = 'Family',linewidth=1, alpha = 0.9,grid = True) happiness_data.Economy_GDPperCapita.plot(color = 'orange',label = 'Economy_GDPperCapita',linewidth=1, alpha = 0.9,grid = True) happiness_data.Health_LifeExpectancy.plot(color = 'yellow',label = 'Health (Life Expectancy)',linewidth=1, alpha = 1.0,grid = True) happiness_data.Freedom.plot(color = 'g',label = 'Freedom',linewidth=1, alpha = 0.9,grid = True) happiness_data.Trust_GovernmentCorruption.plot(color = 'black',label = 'Trust (Government Corruption)',linewidth=1, alpha = 0.9,grid = True) happiness_data.Generosity.plot(color = 'b',label = 'Generosity',linewidth=1, alpha = 0.9,grid = True) happiness_data.DystopiaResidual.plot(color = 'gray',label = 'Dystopia Residual',linewidth=1, alpha = 0.9,grid = True)  plt.legend()  plt.xlabel('Happiness Rank') plt.ylabel('Score') plt.title('Happiness Factors Line Plot Graph') fig = plt.gcf() fig.set_size_inches(18.5, 10.5, forward=True) plt.show()"
"ASSIGN = plt.subplots(nrows = 2, ncols = 4, figsize=(30, 15)) happiness_data.plot(kind = ""scatter"", x= ""Family"", y = ""HappinessScore"", ax = axes[0][0]) happiness_data.plot(kind = ""scatter"", x= ""Economy_GDPperCapita"", y = ""HappinessScore"", ax = axes[0][1]) happiness_data.plot(kind = ""scatter"", x= ""Health_LifeExpectancy"", y = ""HappinessScore"", ax = axes[0][2]) happiness_data.plot(kind = ""scatter"", x= ""Freedom"", y = ""HappinessScore"", ax = axes[0][3]) happiness_data.plot(kind = ""scatter"", x= ""Trust_GovernmentCorruption"", y = ""HappinessScore"", ax = axes[1][0]) happiness_data.plot(kind = ""scatter"", x= ""Generosity"", y = ""HappinessScore"", ax = axes[1][1]) happiness_data.plot(kind = ""scatter"", x= ""DystopiaResidual"", y = ""HappinessScore"", ax = axes[1][2]) plt.show()",0,not_existent,"fig, axes = plt.subplots(nrows = 2, ncols = 4, figsize=(30, 15)) happiness_data.plot(kind = ""scatter"", x= ""Family"", y = ""HappinessScore"", ax = axes[0][0]) happiness_data.plot(kind = ""scatter"", x= ""Economy_GDPperCapita"", y = ""HappinessScore"", ax = axes[0][1]) happiness_data.plot(kind = ""scatter"", x= ""Health_LifeExpectancy"", y = ""HappinessScore"", ax = axes[0][2]) happiness_data.plot(kind = ""scatter"", x= ""Freedom"", y = ""HappinessScore"", ax = axes[0][3]) happiness_data.plot(kind = ""scatter"", x= ""Trust_GovernmentCorruption"", y = ""HappinessScore"", ax = axes[1][0]) happiness_data.plot(kind = ""scatter"", x= ""Generosity"", y = ""HappinessScore"", ax = axes[1][1]) happiness_data.plot(kind = ""scatter"", x= ""DystopiaResidual"", y = ""HappinessScore"", ax = axes[1][2]) plt.show()"
"ASSIGN = plt.subplots(nrows = 2, ncols = 4, figsize=(30, 15)) happiness_data.Family.plot(kind = 'hist',bins = 50, ax = axes[0][0]) happiness_data.Economy_GDPperCapita.plot(kind = ""hist"", ax = axes[0][1]) happiness_data.Health_LifeExpectancy.plot(kind = ""hist"", ax = axes[0][2]) happiness_data.Freedom.plot(kind = ""hist"", ax = axes[0][3]) happiness_data.Trust_GovernmentCorruption.plot(kind = ""hist"", ax = axes[1][0]) happiness_data.Generosity.plot(kind = ""hist"", ax = axes[1][1]) happiness_data.DystopiaResidual.plot(kind = ""hist"", ax = axes[1][2]) plt.show()",0,not_existent,"fig, axes = plt.subplots(nrows = 2, ncols = 4, figsize=(30, 15)) happiness_data.Family.plot(kind = 'hist',bins = 50, ax = axes[0][0]) happiness_data.Economy_GDPperCapita.plot(kind = ""hist"", ax = axes[0][1]) happiness_data.Health_LifeExpectancy.plot(kind = ""hist"", ax = axes[0][2]) happiness_data.Freedom.plot(kind = ""hist"", ax = axes[0][3]) happiness_data.Trust_GovernmentCorruption.plot(kind = ""hist"", ax = axes[1][0]) happiness_data.Generosity.plot(kind = ""hist"", ax = axes[1][1]) happiness_data.DystopiaResidual.plot(kind = ""hist"",  ax = axes[1][2]) plt.show()"
"happiness_data.boxplot(column='HappinessScore',by = 'Region', figsize=(30, 15))",0,not_existent,"happiness_data.boxplot(column='HappinessScore',by = 'Region', figsize=(30, 15))"
"happiness_data[""HappinessDegree""] = ['Happy' if each > 6 else 'Normal' if each > 5 else 'Unhappy' for each in happiness_data.HappinessScore ]",1,not_existent,"happiness_data[""HappinessDegree""] = ['Happy' if each > 6 else 'Normal' if each > 5 else 'Unhappy' for each in happiness_data.HappinessScore ]"
"ASSIGN = happiness_data.pivot_table( index=['Region'], columns = ""HappinessDegree"", values = ""HappinessRank"",aggfunc='count')",1,not_existent,"pivot_data = happiness_data.pivot_table( index=['Region'], columns = ""HappinessDegree"", values = ""HappinessRank"",aggfunc='count')"
CHECKPOINT pivot_data,0,not_existent,pivot_data
SETUP,0,not_existent,"import numpy as np
 import seaborn as sns
 import pandas as pd
 
 import sklearn
 import matplotlib.pyplot as plt
 
 from sklearn.neighbors import KNeighborsClassifier
 from sklearn.ensemble import RandomForestClassifier
 from sklearn.ensemble import AdaBoostClassifier
 
 from sklearn.model_selection import cross_val_score
 from sklearn import preprocessing
 
 %matplotlib inline"
"CHECKPOINT ASSIGN = pd.read_csv(""path"", na_values = ""?"") adult_train.shape",0,execute_result,"adult_train = pd.read_csv(""/kaggle/input/adult-pmr3508/train_data.csv"", na_values = ""?"")
 adult_train.shape
"
"CHECKPOINT ASSIGN = pd.read_csv(""path"", na_values = ""?"") adult_test.shape",0,execute_result,"adult_test = pd.read_csv(""/kaggle/input/adult-pmr3508/test_data.csv"", na_values = ""?"")
 adult_test.shape"
adult_train.head(),0,execute_result,adult_train.head()
adult_train.describe(),0,execute_result,adult_train.describe()
adult_train.describe(exclude = [np.number]),0,execute_result,adult_train.describe(exclude = [np.number])
CHECKPOINT ASSIGN = adult_train.dropna() n_adult.shape,1,execute_result,"n_adult = adult_train.dropna()
 n_adult.shape"
"ASSIGN = adult_train.describe(exclude = [np.number]).columns ASSIGN = adult_train[cat].apply(pd.Categorical) for col in ASSIGN: adult_train[col + ""_cat""] = ASSIGN[col].ASSIGN.codes ASSIGN = adult_test[cat[:-1]].apply(pd.Categorical) for col in ASSIGN[:-1]: adult_test[col + ""_cat""] = ASSIGN[col].ASSIGN.codes",1,not_existent,"cat = adult_train.describe(exclude = [np.number]).columns
 
 categoricAdult = adult_train[cat].apply(pd.Categorical)
 
 for col in cat:
     adult_train[col + ""_cat""] = categoricAdult[col].cat.codes
 categoricTestAdult = adult_test[cat[:-1]].apply(pd.Categorical)
 
 for col in cat[:-1]:
     adult_test[col + ""_cat""] = categoricTestAdult[col].cat.codes"
"sns.pairplot(adult_train, vars=[""age"", ""fnlwgt"", ""education.num"", ""capital.gain"", ""capital.loss"", ""hours.per.week""], hue=""income"", diag_kws={'bw':""1.0""}, corner=True) plt.show()",0,display_data,"sns.pairplot(adult_train, vars=[""age"", ""fnlwgt"", ""education.num"", ""capital.gain"", ""capital.loss"", 
                           ""hours.per.week""], hue=""income"", diag_kws={'bw':""1.0""}, corner=True)
 plt.show()"
"adult_train[""native.country""].value_counts().plot(kind=""pie"", figsize = (8,8)) plt.show()",0,display_data,"adult_train[""native.country""].value_counts().plot(kind=""pie"", figsize = (8,8))
 plt.show()"
"SETUP ASSIGN = adult_train.copy() ASSIGN = LabelEncoder() ASSIGN[""income""] = ASSIGN.fit_transform(ASSIGN['income']) plt.figure(figsize=(10,10)) ASSIGN = np.zeros_like(adult_copy.corr(), dtype=np.bool) ASSIGN[np.triu_indices_from(ASSIGN)] = True sns.heatmap(ASSIGN.corr(), square=True, vmin=-1, vmax=1, annot = True, linewidths=.5, ASSIGN=ASSIGN) plt.show()",1,display_data,"adult_copy = adult_train.copy()
 from sklearn.preprocessing import LabelEncoder
 le = LabelEncoder()
 adult_copy[""income""] = le.fit_transform(adult_copy['income'])
 
 #heat map:
 plt.figure(figsize=(10,10))
 mask = np.zeros_like(adult_copy.corr(), dtype=np.bool)
 mask[np.triu_indices_from(mask)] = True
 sns.heatmap(adult_copy.corr(), square=True, vmin=-1, vmax=1, annot = True, linewidths=.5, mask=mask)
 plt.show()"
"ASSIGN = plt.subplots(nrows = 2, ncols = 2) plt.tight_layout(pad = .4, w_pad = .5, h_pad = 1.) adult_train.groupby(['sex', 'income']).size().unstack().plot(kind = 'bar', stacked = True, ax = axes[0, 0], figsize = (20, 15)) ASSIGN = adult_train.groupby(['ASSIGN', 'income']).size().unstack() ASSIGN = adult_train.groupby('relationship').size() ASSIGN = ASSIGN.sort_values('sum', ascending = False)[['<=50K', '>50K']] ASSIGN.plot(kind = 'bar', stacked = True, ax = axes[0, 1]) ASSIGN = adult_train.groupby(['ASSIGN', 'income']).size().unstack() ASSIGN = adult_train.groupby('education').size() ASSIGN = ASSIGN.sort_values('sum', ascending = False)[['<=50K', '>50K']] ASSIGN.plot(kind = 'bar', stacked = True, ax = axes[1, 0]) ASSIGN = adult_train.groupby(['ASSIGN', 'income']).size().unstack() ASSIGN = adult_train.groupby('occupation').size() ASSIGN = ASSIGN.sort_values('sum', ascending = False)[['<=50K', '>50K']] ASSIGN.plot(kind = 'bar', stacked = True, ax = axes[1, 1])",0,execute_result,"fig, axes = plt.subplots(nrows = 2, ncols = 2)
 plt.tight_layout(pad = .4, w_pad = .5, h_pad = 1.)
 
 adult_train.groupby(['sex', 'income']).size().unstack().plot(kind = 'bar', stacked = True, ax = axes[0, 0], figsize = (20, 15))
 
 relationship = adult_train.groupby(['relationship', 'income']).size().unstack()
 relationship['sum'] = adult_train.groupby('relationship').size()
 relationship = relationship.sort_values('sum', ascending = False)[['<=50K', '>50K']]
 relationship.plot(kind = 'bar', stacked = True, ax = axes[0, 1])
 
 education = adult_train.groupby(['education', 'income']).size().unstack()
 education['sum'] = adult_train.groupby('education').size()
 education = education.sort_values('sum', ascending = False)[['<=50K', '>50K']]
 education.plot(kind = 'bar', stacked = True, ax = axes[1, 0])
 
 occupation = adult_train.groupby(['occupation', 'income']).size().unstack()
 occupation['sum'] = adult_train.groupby('occupation').size()
 occupation = occupation.sort_values('sum', ascending = False)[['<=50K', '>50K']]
 occupation.plot(kind = 'bar', stacked = True, ax = axes[1, 1])
 
"
"ASSIGN= ['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week'] ASSIGN= ['occupation', 'relationship', 'sex','education']",1,not_existent,"princ_num_colum= ['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']
 princ_cat_colum= ['occupation', 'relationship', 'sex','education']"
"ASSIGN = adult_train[princ_num_colum + princ_cat_colum] ASSIGN = adult_train[princ_num_colum + list(map(lambda x: x + ""_cat"", princ_cat_colum))] ASSIGN = adult_test[princ_num_colum + princ_cat_colum] ASSIGN = adult_test[princ_num_colum + list(map(lambda x: x + ""_cat"", princ_cat_colum))] ASSIGN = adult_train.income",1,not_existent,"X_train = adult_train[princ_num_colum + princ_cat_colum]
 numX_train = adult_train[princ_num_colum + list(map(lambda x: x + ""_cat"", princ_cat_colum))]
 
 X_test = adult_test[princ_num_colum + princ_cat_colum]
 numXadul_test = adult_test[princ_num_colum + list(map(lambda x: x + ""_cat"", princ_cat_colum))]
 
 Yadult = adult_train.income"
"CHECKPOINT ASSIGN = {} ASSIGN = 0.0 for k in range(30, 35): ASSIGN = KNeighborsClassifier(k, metric = 'manhattan') ASSIGN = np.mean(cross_val_score(knn, numX_train, Yadult, cv = 10)) if ASSIGN > ASSIGN: ASSIGN = k ASSIGN = score ASSIGN = knn ASSIGN['KNN'].fit(numX_train, Yadult) print(.format(ASSIGN, ASSIGN))",0,stream,"classifiers = {}
 scores = 0.0
 
 
 for k in range(30, 35):
     knn = KNeighborsClassifier(k, metric = 'manhattan')
     score = np.mean(cross_val_score(knn, numX_train, Yadult, cv = 10))
     
     if score > scores:
         bestK = k
         scores = score
         classifiers['KNN'] = knn
 
         
 classifiers['KNN'].fit(numX_train, Yadult)
         
 print(""Best acc: {}, K = {}"".format(scores, bestK))"
SETUP ASSIGN = classifiers['KNN'].predict(numXadul_test),0,stream,"%%time
 
 predictions = classifiers['KNN'].predict(numXadul_test)"
ASSIGN = pd.DataFrame({'Id' : list(range(len(predictions)))}) ASSIGN = pd.DataFrame({'ASSIGN' : predictions}) ASSIGN = income,1,not_existent,"id_index = pd.DataFrame({'Id' : list(range(len(predictions)))})
 income = pd.DataFrame({'income' : predictions})
 result = income"
"result.to_csv(""submission.csv"", index = True, index_label = 'Id')",0,not_existent,"result.to_csv(""submission.csv"", index = True, index_label = 'Id')"
SETUP CHECKPOINT print(),0,stream,"import pandas as pd
 import matplotlib.pyplot as plt
 %matplotlib inline
 import seaborn as sns
 print(""Setup Complete"")"
SETUP CHECKPOINT binder.bind(globals()) print(),0,stream,"# Set up code checking
 from learntools.core import binder
 binder.bind(globals())
 from learntools.data_viz_to_coder.ex2 import *
 print(""Setup Complete"")"
museum_data.head(),0,execute_result,"# Print the last five rows of the data 
 museum_data.head() # Your code here"
museum_data.tail(),0,execute_result,"# Last 5 rows of the data
 museum_data.tail()"
list(museum_data.columns),0,execute_result,list(museum_data.columns)
"SETUP CHECKPOINT ASSIGN = ' ASSIGN = ' ASSIGN = ' ASSIGN = ' register_matplotlib_converters() warnings.filterwarnings('ignore') for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename))",0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed
 # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
 # For example, here's several helpful packages to load in 
 
 #essential libraries
 import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
 import random
 from urllib.request import urlopen
 import matplotlib.pyplot as plt
 import seaborn as sb
 import plotly.express as ex
 import plotly.graph_objs as gp
 from plotly.subplots import make_subplots
 import plotly.figure_factory as ff
 import folium
 
 #Colour codes
 conf = '#393e46' 
 deth = '#ff2e63'  
 cure = '#21bf73'
 acti = '#fe9801'
 
 #Extra Libraries
 from pandas.plotting import register_matplotlib_converters
 register_matplotlib_converters()   
 
 #To remove the warnings
 import warnings
 warnings.filterwarnings('ignore')
 
 
 # Input data files are available in the ""../input/"" directory.
 # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
 
 import os
 for dirname, _, filenames in os.walk('/kaggle/input'):
     for filename in filenames:
         print(os.path.join(dirname, filename))
 
 # Any results you write to the current directory are saved as output."
"ASSIGN = pd.read_csv('path',parse_dates=['Date'], dayfirst=True) ASSIGN = pd.read_csv('path') ASSIGN = pd.read_excel('path',sheet_name='India', parse_dates=['Date']) ASSIGN = pd.read_excel('path',sheet_name='Italy', parse_dates=['Date']) ASSIGN = pd.read_excel('path',sheet_name='Korea', parse_dates=['Date'])",0,not_existent,"df_india = pd.read_csv('/kaggle/input/covid19-in-india/covid_19_india.csv',parse_dates=['Date'], dayfirst=True)
 df_coordinates = pd.read_csv('/kaggle/input/coronavirus-cases-in-india/Indian Coordinates.csv')
 df_India_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='India', parse_dates=['Date'])
 df_Italy_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='Italy', parse_dates=['Date'])
 df_Korea_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='Korea', parse_dates=['Date'])"
df_india.head(),0,execute_result,df_india.head()
"df_coordinates.dropna(axis = 1, inplace = True) df_coordinates.head()",1,execute_result,"df_coordinates.dropna(axis = 1, inplace = True)
 df_coordinates.head()"
"df_coordinates.rename(columns = {'Name of State path':'Statepath'}, inplace = True) df_coordinates.head()",1,execute_result,"df_coordinates.rename(columns = {'Name of State / UT':'State/UnionTerritory'}, inplace = True)
 df_coordinates.head()"
CHECKPOINT df_india.shape,0,execute_result,df_india.shape
df_india.isnull().sum(),0,execute_result,df_india.isnull().sum()
"df_india.dropna(axis = 0, inplace = True)",1,not_existent,"df_india.dropna(axis = 0, inplace = True)"
"df_india[""Statepath""].replace({'Chattisgarh': 'Chhattisgarh ', 'Chhattisgarh' :'Chhattisgarh ', 'Puducherry' : 'Pondicherry', 'Himachal Pradesh' : 'Himachal Pradesh ', 'Madhya Pradesh' : 'Madhya Pradesh ', 'Bihar':'Bihar ', 'Himachal Pradesh':'Himachal Pradesh ', 'Manipur':'Manipur ', 'West Bengal':'West Bengal ', 'Goa' : 'Goa '}, inplace=True)",1,not_existent,"df_india[""State/UnionTerritory""].replace({'Chattisgarh': 'Chhattisgarh ',
                                           'Chhattisgarh' :'Chhattisgarh ',
                                           'Puducherry' : 'Pondicherry',
                                           'Himachal Pradesh' : 'Himachal Pradesh ',
                                           'Madhya Pradesh' : 'Madhya Pradesh ',
                                           'Bihar':'Bihar ',
                                           'Himachal Pradesh':'Himachal Pradesh ',
                                           'Manipur':'Manipur ',
                                           'West Bengal':'West Bengal ',
                                           'Goa' : 'Goa '}, inplace=True)"
"ASSIGN = pd.merge(ASSIGN, df_coordinates, how='left', on='Statepath')",1,not_existent,"df_india = pd.merge(df_india, df_coordinates, how='left', on='State/UnionTerritory')"
df_india.isnull().sum(),0,execute_result,"df_india.isnull().sum()
"
"df_india[['Latitude','Longitude']] = df_india[['Latitude','Longitude']].fillna(0) df_india.isnull().sum()",1,execute_result,"df_india[['Latitude','Longitude']] = df_india[['Latitude','Longitude']].fillna(0)
 df_india.isnull().sum()"
"ASSIGN = ASSIGN.drop('Sno', axis = 1) ASSIGN.head()",1,execute_result,"df_india = df_india.drop('Sno', axis = 1) 
 df_india.head()"
df_india.to_csv('Processed_data.csv'),0,not_existent,df_india.to_csv('Processed_data.csv')
"ASSIGN = ASSIGN[['Date', 'Statepath', 'Latitude', 'Longitude','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths']]",1,not_existent,"#Rearranging the columns
 df_india = df_india[['Date', 'State/UnionTerritory', 'Latitude', 'Longitude','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths']]"
"ASSIGN = ['ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active'] df_india['Active'] = (df_india['ConfirmedIndianNational'] + df_india['ConfirmedForeignNational']) - df_india['Deaths'] - df_india['Cured']",1,not_existent,"Total_cases = ['ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']
 
 #Active = Confirmed - Deaths - Cured
 df_india['Active'] = (df_india['ConfirmedIndianNational'] + df_india['ConfirmedForeignNational']) - df_india['Deaths'] - df_india['Cured']"
df_india[Total_cases] = df_india[Total_cases].fillna(0),1,not_existent,"#Fill Null Values with Zeros
 df_india[Total_cases] = df_india[Total_cases].fillna(0)"
ASSIGN = df_india[df_india['Statepath'].str.contains('Maharashtra')] ASSIGN.head(),1,execute_result,"# cases in Maharashtra
 maha = df_india[df_india['State/UnionTerritory'].str.contains('Maharashtra')]
 maha.head()"
ASSIGN = df_india[df_india['Statepath'].str.contains('Kerala')] ASSIGN.head(),1,execute_result,"# cases in Kerala
 kerala = df_india[df_india['State/UnionTerritory'].str.contains('Kerala')]
 kerala.head()"
"ASSIGN = df_india.groupby('Date')['ConfirmedIndianNational','ConfirmedForeignNational','Deaths', 'Cured', 'Active'].sum().reset_index() ASSIGN.style.background_gradient(cmap='Reds')",1,execute_result,"df_india_latest = df_india.groupby('Date')['ConfirmedIndianNational','ConfirmedForeignNational','Deaths', 'Cured', 'Active'].sum().reset_index()
 df_india_latest.style.background_gradient(cmap='Reds')"
"Indian_National = df_india['ConfirmedIndianNational'].sum() ASSIGN = df_india['ConfirmedForeignNational'].sum() ASSIGN ={""Indian"": Indian_National,""Foriengners"":Foreigners} ASSIGN=['orange','blue'] plt.figure(figsize = (10,10)) plt.pie(ASSIGN.values(),labels=ASSIGN.keys(),ASSIGN=ASSIGN,shadow=True,explode=(0.1, 0.1), autopct='%1.2f%%') plt.axis('equal') plt.show()",0,display_data,"Indian_National = df_india['ConfirmedIndianNational'].sum()
 Foreigners = df_india['ConfirmedForeignNational'].sum()
 dct ={""Indian"": Indian_National,""Foriengners"":Foreigners}
 colors=['orange','blue']
 plt.figure(figsize = (10,10))
 plt.pie(dct.values(),labels=dct.keys(),colors=colors,shadow=True,explode=(0.1, 0.1), autopct='%1.2f%%')
 plt.axis('equal')
 plt.show()"
ASSIGN = df_india_latest[df_india_latest['Date']==max(df_india_latest['Date'])].reset_index(drop=True) ASSIGN.style.background_gradient(cmap='copper'),1,execute_result,"slate = df_india_latest[df_india_latest['Date']==max(df_india_latest['Date'])].reset_index(drop=True)
 slate.style.background_gradient(cmap='copper')"
"CHECKPOINT ASSIGN = slate.melt(id_vars=""Date"", value_vars=['Active','Cured','Deaths']) plot",0,execute_result,"plot = slate.melt(id_vars=""Date"", value_vars=['Active','Cured','Deaths'])
 plot"
"ASSIGN = ex.treemap(plot, path=['variable'], values=""value"", height=500, width=800, ASSIGN=[acti,cure,deth]) ASSIGN.show()",0,display_data,"matt = ex.treemap(plot, path=['variable'], values=""value"", height=500, width=800,
                 color_discrete_sequence=[acti,cure,deth])
 matt.show() "
"ASSIGN = df_india[df_india['Date']==max(df_india['Date'])].reset_index() ASSIGN = india_latest.groupby('Statepath')['ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active'].sum().reset_index() ASSIGN.style.background_gradient(cmap='OrRd')",1,execute_result,"india_latest = df_india[df_india['Date']==max(df_india['Date'])].reset_index()
 india_latest_groupby = india_latest.groupby('State/UnionTerritory')['ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active'].sum().reset_index()
 india_latest_groupby.style.background_gradient(cmap='OrRd')"
"ASSIGN = india_latest_groupby.sort_values(by='ConfirmedIndianNational', ascending=False) ASSIGN = ASSIGN.reset_index(drop=True) ASSIGN.style.background_gradient(cmap='OrRd')",1,execute_result,"#confirmed cases
 state_confirmed = india_latest_groupby.sort_values(by='ConfirmedIndianNational', ascending=False)
 state_confirmed = state_confirmed.reset_index(drop=True)
 state_confirmed.style.background_gradient(cmap='OrRd')"
"ASSIGN = state_confirmed[state_confirmed['Deaths']>0][['Statepath','Deaths']] ASSIGN.sort_values('Deaths',ascending=False).reset_index(drop=True).style.background_gradient(cmap='OrRd')",1,execute_result,"states_with_death = state_confirmed[state_confirmed['Deaths']>0][['State/UnionTerritory','Deaths']]
 states_with_death.sort_values('Deaths',ascending=False).reset_index(drop=True).style.background_gradient(cmap='OrRd')"
"ASSIGN = state_confirmed[state_confirmed['ConfirmedIndianNational']+ state_confirmed['ConfirmedForeignNational'] == state_confirmed['Deaths']+ state_confirmed['Cured']] ASSIGN = ASSIGN[['Statepath','ConfirmedIndianNational','ConfirmedForeignNational','Deaths','Cured']] ASSIGN = ASSIGN.sort_values('ConfirmedIndianNational', ascending=False) ASSIGN['Cured'].count()",1,execute_result,"no_recovery = state_confirmed[state_confirmed['ConfirmedIndianNational']+ state_confirmed['ConfirmedForeignNational'] == 
                               state_confirmed['Deaths']+ state_confirmed['Cured']]
 no_recovery = no_recovery[['State/UnionTerritory','ConfirmedIndianNational','ConfirmedForeignNational','Deaths','Cured']]
 no_recovery = no_recovery.sort_values('ConfirmedIndianNational', ascending=False)
 no_recovery['Cured'].count()"
"CHECKPOINT ASSIGN = folium.Map(location=[20.5937,78.9629], tiles='cartodbpositron', min_zoom=4, max_zoom=10, zoom_start=4) for i in range(0, len(df_india)): folium.Circle( ASSIGN=[df_india.iloc[i]['Latitude'],df_india.iloc[i]['Longitude']], ASSIGN='crimson', ASSIGN = '<li><bold>Statepath: '+str(df_india.iloc[i]['Statepath'])+ '<li><bold>ConfirmedIndianNational : '+str(df_india.iloc[i]['ConfirmedIndianNational'])+ '<li><bold>ConfirmedForeignNational : '+str(df_india.iloc[i]['ConfirmedForeignNational'])+ '<li><bold>Deaths : '+str(df_india.iloc[i]['Deaths'])+ '<li><bold>Cured : '+str(df_india.iloc[i]['Cured']), ASSIGN=int(df_india.iloc[i]['ConfirmedIndianNational'])**1.1).add_to(India) India",1,execute_result,"#India
 India = folium.Map(location=[20.5937,78.9629], tiles='cartodbpositron', min_zoom=4, max_zoom=10, zoom_start=4)
 for i in range(0, len(df_india)):
     folium.Circle(
         location=[df_india.iloc[i]['Latitude'],df_india.iloc[i]['Longitude']],
                   color='crimson',
                   tooltip = '<li><bold>State/UnionTerritory : '+str(df_india.iloc[i]['State/UnionTerritory'])+
                             '<li><bold>ConfirmedIndianNational : '+str(df_india.iloc[i]['ConfirmedIndianNational'])+
                             '<li><bold>ConfirmedForeignNational : '+str(df_india.iloc[i]['ConfirmedForeignNational'])+
                             '<li><bold>Deaths : '+str(df_india.iloc[i]['Deaths'])+
                             '<li><bold>Cured : '+str(df_india.iloc[i]['Cured']),
                   radius=int(df_india.iloc[i]['ConfirmedIndianNational'])**1.1).add_to(India)
 India
 
 #The output is not 100% correct as there was some issue with the cordinates."
"ASSIGN = df_india.groupby('Date')['Cured', 'Deaths', 'Active'].sum().reset_index() ASSIGN = ASSIGN.melt(id_vars='Date', value_vars=['Cured', 'Deaths', 'Active'], ASSIGN='Case', value_name='Count') ASSIGN.head() ASSIGN=ex.area(graph, x='Date', y='Count', color='Case', ASSIGN = 'Cases over time', color_discrete_sequence=[cure, deth, acti]) ASSIGN.show()",0,display_data,"graph = df_india.groupby('Date')['Cured', 'Deaths', 'Active'].sum().reset_index()
 graph = graph.melt(id_vars='Date', value_vars=['Cured', 'Deaths', 'Active'],
          var_name='Case', value_name='Count')
 graph.head()
 
 fig=ex.area(graph, x='Date', y='Count', color='Case',
            title = 'Cases over time', color_discrete_sequence=[cure, deth, acti])
 fig.show()"
"Cure_over_Death = df_india.groupby('Date').sum().reset_index() Cure_over_Death['No. of Deaths to 100 Confirmed Cases'] = round(Cure_over_Death['Deaths']path(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100 Cure_over_Death['No. of Recovered to 100 Confirmed Cases'] = round(Cure_over_Death['Cured']path(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100 Cure_over_Death = Cure_over_Death.melt(id_vars ='Date', ASSIGN=['No. of Deaths to 100 Confirmed Cases','No. of Recovered to 100 Confirmed Cases'], ASSIGN='Ratio', ASSIGN='Value') ASSIGN = ex.line(Cure_over_Death, x='Date', y='Value', color='Ratio', log_y=True, ASSIGN='Cure_over_Death', color_discrete_sequence=[deth,cure]) ASSIGN.show()",0,display_data,"Cure_over_Death = df_india.groupby('Date').sum().reset_index()
 
 Cure_over_Death['No. of Deaths to 100 Confirmed Cases'] = round(Cure_over_Death['Deaths']/(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100
 Cure_over_Death['No. of Recovered to 100 Confirmed Cases'] = round(Cure_over_Death['Cured']/(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100
 
 Cure_over_Death = Cure_over_Death.melt(id_vars ='Date',
                           value_vars=['No. of Deaths to 100 Confirmed Cases','No. of Recovered to 100 Confirmed Cases'],
                           var_name='Ratio',
                           value_name='Value')
 
 fig = ex.line(Cure_over_Death, x='Date', y='Value', color='Ratio', log_y=True,
              title='Cure_over_Death', color_discrete_sequence=[deth,cure])
 
 fig.show()"
"ASSIGN = df_india.drop(['Latitude', 'Longitude'], axis=1) ASSIGN['TotalConfirmed'] = ASSIGN['ConfirmedIndianNational'] + ASSIGN['ConfirmedForeignNational'] ASSIGN = ASSIGN[['Date', 'Statepath','TotalConfirmed','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']] ASSIGN.head()",1,execute_result,"df_india_data = df_india.drop(['Latitude', 'Longitude'], axis=1)
 df_india_data['TotalConfirmed'] = df_india_data['ConfirmedIndianNational'] + df_india_data['ConfirmedForeignNational']
 df_india_data = df_india_data[['Date', 'State/UnionTerritory','TotalConfirmed','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']]
 df_india_data.head()"
"ASSIGN = df_india_data[df_india_data['TotalConfirmed']!=0].groupby('Date')['Statepath'].unique().apply(len) ASSIGN = pd.DataFrame(ASSIGN).reset_index() ASSIGN = ex.line(spread, x='Date', y='Statepath', text='Statepath', ASSIGN='Number of Statepath', ASSIGN=[conf,deth, cure]) ASSIGN.update_traces(textposition='top center') ASSIGN.show()",0,display_data,"
 spread = df_india_data[df_india_data['TotalConfirmed']!=0].groupby('Date')['State/UnionTerritory'].unique().apply(len)
 spread = pd.DataFrame(spread).reset_index()
 
 spread_graph = ex.line(spread, x='Date', y='State/UnionTerritory', text='State/UnionTerritory',
               title='Number of State/UnionTerritory to which COVID-19 spread over the time',
               color_discrete_sequence=[conf,deth, cure])
 spread_graph.update_traces(textposition='top center')
 spread_graph.show()"
"ASSIGN = df_india_data.groupby(['Date', 'Statepath'])['TotalConfirmed'].sum().reset_index().sort_values('TotalConfirmed', ascending=False) ex.line(ASSIGN, x=""Date"", y=""TotalConfirmed"", color='Statepath', title='ASSIGN over time', height=600)",0,display_data,"Spread = df_india_data.groupby(['Date', 'State/UnionTerritory'])['TotalConfirmed'].sum().reset_index().sort_values('TotalConfirmed', ascending=False)
 
 ex.line(Spread, x=""Date"", y=""TotalConfirmed"", color='State/UnionTerritory', title='Spread over time', height=600)"
"ASSIGN = india_latest_groupby ASSIGN['TotalConfirmed'] = ASSIGN['ConfirmedIndianNational'] + ASSIGN['ConfirmedForeignNational'] ASSIGN = ASSIGN[['Statepath','TotalConfirmed','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']] ASSIGN.head()",1,execute_result,"latest_date = india_latest_groupby
 latest_date['TotalConfirmed'] = latest_date['ConfirmedIndianNational'] + latest_date['ConfirmedForeignNational']
 latest_date = latest_date[['State/UnionTerritory','TotalConfirmed','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']]
 latest_date.head()"
"ASSIGN = ex.bar(latest_date.sort_values('TotalConfirmed', ascending=False).head(30).sort_values('TotalConfirmed', ascending=True), ASSIGN=""TotalConfirmed"", y=""Statepath"", title='Confirmed Cases', text='TotalConfirmed', orientation='h', ASSIGN=900, height=700, range_x = [0, max(latest_date['TotalConfirmed'])+15]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"Confirmed_bar = ex.bar(latest_date.sort_values('TotalConfirmed', ascending=False).head(30).sort_values('TotalConfirmed', ascending=True), 
              x=""TotalConfirmed"", y=""State/UnionTerritory"", title='Confirmed Cases', text='TotalConfirmed', orientation='h', 
              width=900, height=700, range_x = [0, max(latest_date['TotalConfirmed'])+15])
 Confirmed_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')
 Confirmed_bar.show()"
"ASSIGN = ex.bar(latest_date.sort_values('Deaths', ascending=False).head(30).sort_values('Deaths', ascending=True), ASSIGN=""Deaths"", y=""Statepath"", title='Death in each state', text='Deaths', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Deaths'])+0.5]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"Death_rate_bar = ex.bar(latest_date.sort_values('Deaths', ascending=False).head(30).sort_values('Deaths', ascending=True), 
              x=""Deaths"", y=""State/UnionTerritory"", title='Death in each state', text='Deaths', orientation='h', 
              width=800, height=700, range_x = [0, max(latest_date['Deaths'])+0.5])
 Death_rate_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')
 Death_rate_bar.show()"
"ASSIGN = ex.bar(latest_date.sort_values('Cured', ascending=False).head(30).sort_values('Cured', ascending=True), ASSIGN=""Cured"", y=""Statepath"", title='Cured cases', text='Cured', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Cured'])+4]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"cure_bar = ex.bar(latest_date.sort_values('Cured', ascending=False).head(30).sort_values('Cured', ascending=True), 
              x=""Cured"", y=""State/UnionTerritory"", title='Cured cases', text='Cured', orientation='h', 
              width=800, height=700, range_x = [0, max(latest_date['Cured'])+4])
 cure_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')
 cure_bar.show()"
"ASSIGN = ex.bar(latest_date.sort_values('Active', ascending=False).head(30).sort_values('Active', ascending=True), ASSIGN=""Active"", y=""Statepath"", title='Active cases', text='Active', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Active'])+10]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"Active_cases = ex.bar(latest_date.sort_values('Active', ascending=False).head(30).sort_values('Active', ascending=True), 
              x=""Active"", y=""State/UnionTerritory"", title='Active cases', text='Active', orientation='h', 
              width=800, height=700, range_x = [0, max(latest_date['Active'])+10])
 Active_cases.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')
 Active_cases.show()"
"latest_date['Death Rate'] = round((latest_date['Deaths']path['TotalConfirmed'])*20,2) Top_50 = latest_date[latest_date['TotalConfirmed']>20] Top_50 = Top_50.sort_values('Death Rate', ascending=False) ASSIGN = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True), ASSIGN=""Death Rate"", y=""Statepath"", text='Death Rate', orientation='h', ASSIGN=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case') ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"latest_date['Death Rate'] = round((latest_date['Deaths']/latest_date['TotalConfirmed'])*20,2)
 Top_50 = latest_date[latest_date['TotalConfirmed']>20]
 Top_50 = Top_50.sort_values('Death Rate', ascending=False)
 
 Plot = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True), 
              x=""Death Rate"", y=""State/UnionTerritory"", text='Death Rate', orientation='h', 
              width=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case')
 Plot.update_traces(marker_color='#00a8cc', opacity=0.6, textposition='outside')
 Plot.show()"
"ASSIGN = df_india_data.groupby(['Statepath', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum() ASSIGN = ASSIGN.reset_index() ASSIGN = ex.bar(Date_vs_confirmed, x=""Date"", y=""TotalConfirmed"", color='Statepath', orientation='v', height=600, ASSIGN='Date vs Confirmed', color_discrete_sequence = ex.colors.cyclical.mygbm) ASSIGN.show()",0,display_data,"#Date vs Confirmed
 Date_vs_confirmed = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()
 Date_vs_confirmed = Date_vs_confirmed.reset_index()
 
 Date_vs_confirmed_fig = ex.bar(Date_vs_confirmed, x=""Date"", y=""TotalConfirmed"", color='State/UnionTerritory', orientation='v', height=600,
                         title='Date vs Confirmed', color_discrete_sequence = ex.colors.cyclical.mygbm)
 Date_vs_confirmed_fig.show()"
"ASSIGN = df_india_data.groupby(['Statepath', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum() ASSIGN = ASSIGN.reset_index() ASSIGN = ex.bar(Date_vs_cured, x=""Date"", y=""Cured"", color='Statepath', orientation='v', height=600, ASSIGN='Date vs Cured', color_discrete_sequence = ex.colors.cyclical.mygbm) ASSIGN.show()",0,display_data,"#Date vs Cured
 Date_vs_cured = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()
 Date_vs_cured = Date_vs_cured.reset_index()
 
 Date_vs_cured_fig = ex.bar(Date_vs_cured, x=""Date"", y=""Cured"", color='State/UnionTerritory', orientation='v', height=600,
                         title='Date vs Cured', color_discrete_sequence = ex.colors.cyclical.mygbm)
 Date_vs_cured_fig.show()"
"Date_vs_Deaths = df_india_data.groupby(['Statepath', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum() Date_vs_Deaths = Date_vs_Deaths.reset_index() Date_vs_Deaths_fig = ex.bar(Date_vs_Deaths, x=""Date"", y=""Deaths"", color='Statepath', orientation='v', height=600, ASSIGN='Date vs Active', color_discrete_sequence = ex.colors.cyclical.mygbm) Date_vs_Deaths_fig.show()",0,display_data,"#Date vs Active
 Date_vs_Deaths = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()
 Date_vs_Deaths = Date_vs_Deaths.reset_index()
 
 Date_vs_Deaths_fig = ex.bar(Date_vs_Deaths, x=""Date"", y=""Deaths"", color='State/UnionTerritory', orientation='v', height=600,
                         title='Date vs Active', color_discrete_sequence = ex.colors.cyclical.mygbm)
 Date_vs_Deaths_fig.show()"
"ASSIGN = df_india_data.groupby(['Statepath', 'Date', ])['TotalConfirmed', 'Deaths', 'Cured'] ASSIGN = ASSIGN.sum().diff().reset_index() ASSIGN = new_cases['Statepath'] != new_cases['Statepath'].shift(1) ASSIGN.loc[ASSIGN, 'TotalConfirmed'] = np.nan ASSIGN.loc[ASSIGN, 'Deaths'] = np.nan ASSIGN.loc[ASSIGN, 'Cured'] = np.nan ASSIGN = ex.bar(new_cases, x=""Date"", y=""TotalConfirmed"", color='Statepath',title='New cases') ASSIGN.show()",0,display_data,"new_cases = df_india_data.groupby(['State/UnionTerritory', 'Date', ])['TotalConfirmed', 'Deaths', 'Cured']
 new_cases = new_cases.sum().diff().reset_index()
 
 mat = new_cases['State/UnionTerritory'] != new_cases['State/UnionTerritory'].shift(1)
 
 new_cases.loc[mat, 'TotalConfirmed'] = np.nan
 new_cases.loc[mat, 'Deaths'] = np.nan
 new_cases.loc[mat, 'Cured'] = np.nan
 
 New_cases_plot = ex.bar(new_cases, x=""Date"", y=""TotalConfirmed"", color='State/UnionTerritory',title='New cases')
 New_cases_plot.show()"
"Death_vs_Conf = latest_date.sort_values('Deaths', ascending=False).iloc[:15, :] Death_vs_Conf_plot = ex.scatter(Death_vs_Conf, ASSIGN='TotalConfirmed', y='Deaths', color='Statepath', ASSIGN='Statepath', log_x=True, log_y=True, title='Deaths vs Confirmed') Death_vs_Conf_plot.update_traces(textposition='top center') Death_vs_Conf_plot.show()",0,display_data,"# Deaths vs Confirmed
 Death_vs_Conf = latest_date.sort_values('Deaths', ascending=False).iloc[:15, :]
 
 Death_vs_Conf_plot = ex.scatter(Death_vs_Conf, 
                  x='TotalConfirmed', y='Deaths', color='State/UnionTerritory',
                  text='State/UnionTerritory', log_x=True, log_y=True, title='Deaths vs Confirmed')
 Death_vs_Conf_plot.update_traces(textposition='top center')
 Death_vs_Conf_plot.show()"
"Cured_vs_Conf = latest_date.sort_values('Cured', ascending=False).iloc[:15, :] Cured_vs_Conf_plot = ex.scatter(Death_vs_Conf, ASSIGN='TotalConfirmed', y='Cured', color='Statepath', ASSIGN='Statepath', log_x=True, log_y=True, title='Cured vs Confirmed') Cured_vs_Conf_plot.update_traces(textposition='top center') Cured_vs_Conf_plot.show()",0,display_data,"#Cured vs Confirmed
 Cured_vs_Conf = latest_date.sort_values('Cured', ascending=False).iloc[:15, :]
 
 Cured_vs_Conf_plot = ex.scatter(Death_vs_Conf, 
                  x='TotalConfirmed', y='Cured', color='State/UnionTerritory',
                  text='State/UnionTerritory', log_x=True, log_y=True, title='Cured vs Confirmed')
 Cured_vs_Conf_plot.update_traces(textposition='top center')
 Cured_vs_Conf_plot.show()"
""""""" df_India_perday df_Italy_perday df_Korea_perday """""" ASSIGN = make_subplots( ASSIGN=2, cols=2, ASSIGN=[[{}, {}], [{""colspan"": 2}, None]], ASSIGN=(""S.Korea"",""Italy"", ""India"")) ASSIGN.add_trace(gp.Bar(x=df_Korea_perday['Date'], y=df_Korea_perday['Total Cases'], ASSIGN= dict(color=df_Korea_perday['Total Cases'], coloraxis=""coloraxis"")), 1, 1) ASSIGN.add_trace(gp.Bar(x=df_Italy_perday['Date'], y=df_Italy_perday['Total Cases'], ASSIGN= dict(color=df_Italy_perday['Total Cases'], coloraxis=""coloraxis"")), 1, 2) ASSIGN.add_trace(gp.Bar(x=df_india_data['Date'], y=df_india_data['TotalConfirmed'], ASSIGN= dict(color=df_india_data['TotalConfirmed'], coloraxis=""coloraxis"")), 2, 1) ASSIGN.update_layout(coloraxis=dict(colorscale='RdBu'), showlegend=False,title_text=""Total Confirmed cases(Cumulative)"") ASSIGN.update_layout(plot_bgcolor='rgb(250, 242, 242)') ASSIGN.show()",0,display_data,"""""""
 df_India_perday
 df_Italy_perday
 df_Korea_perday
 """"""
 
 Comparison = make_subplots(
     rows=2, cols=2,
     specs=[[{}, {}],
            [{""colspan"": 2}, None]],
     subplot_titles=(""S.Korea"",""Italy"", ""India""))
 
 Comparison.add_trace(gp.Bar(x=df_Korea_perday['Date'], y=df_Korea_perday['Total Cases'],
                     marker= dict(color=df_Korea_perday['Total Cases'], coloraxis=""coloraxis"")),
               1, 1)
 
 Comparison.add_trace(gp.Bar(x=df_Italy_perday['Date'], y=df_Italy_perday['Total Cases'],
                     marker= dict(color=df_Italy_perday['Total Cases'], coloraxis=""coloraxis"")),
               1, 2)
 
 Comparison.add_trace(gp.Bar(x=df_india_data['Date'], y=df_india_data['TotalConfirmed'],
                     marker= dict(color=df_india_data['TotalConfirmed'], coloraxis=""coloraxis"")),
               2, 1)
 
 Comparison.update_layout(coloraxis=dict(colorscale='RdBu'), showlegend=False,title_text=""Total Confirmed cases(Cumulative)"")
 
 Comparison.update_layout(plot_bgcolor='rgb(250, 242, 242)')
 Comparison.show()"
"ASSIGN = df_india_data['TotalConfirmed'].sum() ASSIGN = df_Italy_perday['Total Cases'].sum() South_Korea = df_Korea_perday['Total Cases'].sum() ASSIGN ={""India"": India,""Italy"":Italy, 'South Korea':South_Korea} ASSIGN=['red','blue', 'yellow'] plt.figure(figsize = (10,10)) plt.pie(ASSIGN.values(),labels=ASSIGN.keys(),ASSIGN=ASSIGN,shadow=True,explode=(0.1, 0.1, 0.1), autopct='%1.2f%%') plt.axis('equal') plt.show()",0,display_data,"India = df_india_data['TotalConfirmed'].sum()
 Italy = df_Italy_perday['Total Cases'].sum()
 South_Korea = df_Korea_perday['Total Cases'].sum()
 dict ={""India"": India,""Italy"":Italy, 'South Korea':South_Korea}
 colors=['red','blue', 'yellow']
 plt.figure(figsize = (10,10))
 plt.pie(dict.values(),labels=dict.keys(),colors=colors,shadow=True,explode=(0.1, 0.1, 0.1), autopct='%1.2f%%')
 plt.axis('equal')
 plt.show()"
"SETUP CHECKPOINT print(check_output([, ]).decode())",0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed
 # It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
 # For example, here's several helpful packages to load in 
 
 import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
 import seaborn as sns
 import matplotlib.pyplot as pl
 from matplotlib import cm as cm
 import plotly.plotly as py
 
 # Input data files are available in the ""../input/"" directory.
 # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory
 
 from subprocess import check_output
 print(check_output([""ls"", ""../input""]).decode(""utf8""))
 
 # Any results you write to the current directory are saved as output."
"ASSIGN=pd.read_csv(""..path"")",0,not_existent,"train=pd.read_csv(""../input/train.csv"")"
pd.options.display.max_rows = 999 pd.options.display.max_columns=999 train.describe(),1,execute_result,"pd.options.display.max_rows = 999
 pd.options.display.max_columns=999
 train.describe()"
train.count(),0,execute_result,train.count()
CHECKPOINT train.columns,0,execute_result,train.columns
CHECKPOINT train.dtypes,0,execute_result,train.dtypes
train.corr(),0,execute_result,train.corr()
"ASSIGN = pl.figure() ASSIGN = fig.add_subplot(111) ASSIGN = cm.get_cmap('jet', 80) ASSIGN = ax1.imshow(train.corr(), interpolation=""nearest"", cmap=cmap) ASSIGN.grid(True) pl.title('Abalone Feature Correlation') ASSIGN.colorbar(ASSIGN, ticks=[.75,.8,.85,.90,.95,1]) pl.show()",0,display_data,"fig = pl.figure()
 ax1 = fig.add_subplot(111)
 cmap = cm.get_cmap('jet', 80)
 cax = ax1.imshow(train.corr(), interpolation=""nearest"", cmap=cmap)
 ax1.grid(True)
 pl.title('Abalone Feature Correlation')
 #labels=['Id',	'MSSubClass',	'LotFrontage',	'LotArea',	'OverallQual',	'OverallCond',	'YearBuilt',	'YearRemodAdd',	'MasVnrArea',	'BsmtFinSF1',	'BsmtFinSF2',	'BsmtUnfSF',	'TotalBsmtSF',	'1stFlrSF',	'2ndFlrSF',	'LowQualFinSF',	'GrLivArea',	'BsmtFullBath',	'BsmtHalfBath',	'FullBath',	'HalfBath',	'BedroomAbvGr',	'KitchenAbvGr',	'TotRmsAbvGrd',	'Fireplaces',	'GarageYrBlt',	'GarageCars',	'GarageArea',	'WoodDeckSF',	'OpenPorchSF',	'EnclosedPorch',	'3SsnPorch',	'ScreenPorch',	'PoolArea',	'MiscVal',	'MoSold',	'YrSold',	'SalePrice',]
 #ax1.set_xticklabels(labels,fontsize=5)
 #ax1.set_yticklabels(labels,fontsize=5)
 # Add colorbar, make sure to specify tick locations to match desired ticklabels
 fig.colorbar(cax, ticks=[.75,.8,.85,.90,.95,1])
 pl.show()"
"pl.plot(train[""YearBuilt""], train[""GarageYrBlt""], ""o"")",0,execute_result,"
 pl.plot(train[""YearBuilt""], train[""GarageYrBlt""], ""o"")"
"pl.plot(train[""TotalBsmtSF""], train[""1stFlrSF""], ""o"")",0,execute_result,"pl.plot(train[""TotalBsmtSF""], train[""1stFlrSF""], ""o"")"
"pl.plot(train[""GarageCars""], train[""GarageArea""], ""o"")",0,execute_result,"
 pl.plot(train[""GarageCars""], train[""GarageArea""], ""o"")"
"pl.plot(train[""SalePrice""], train[""MiscVal""], ""o"") ASSIGN=train[[""SalePrice"",""MiscVal""]] ASSIGN.corr()",1,execute_result,"pl.plot(train[""SalePrice""], train[""MiscVal""], ""o"")
 traincor=train[[""SalePrice"",""MiscVal""]]
 traincor.corr()"
"Alley2=train['Alley'].fillna('NoAlley') train[""Alley2""]=Alley2",1,not_existent,"Alley2=train['Alley'].fillna('NoAlley')
 train[""Alley2""]=Alley2"
SETUP py.init_notebook_mode(connected=True),0,display_data,"import numpy as np # linear algebra
 import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
 
 import plotly.offline as py
 py.init_notebook_mode(connected=True)
 import plotly.graph_objs as go
 import plotly.tools as tls
 import seaborn as sns
 import matplotlib.image as mpimg
 import matplotlib.pyplot as plt
 import matplotlib
 %matplotlib inline
 
 # Import the 3 dimensionality reduction methods
 from sklearn.manifold import TSNE
 from sklearn.decomposition import PCA
 from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
ASSIGN = pd.read_csv('..path') ASSIGN.head(),0,execute_result,"train = pd.read_csv('../input/train.csv')
 train.head()"
CHECKPOINT print(train.shape),0,stream,print(train.shape)
"ASSIGN = train['label'] ASSIGN = ASSIGN.drop(""label"",axis=1)",1,not_existent,"# save the labels to a Pandas series target
 target = train['label']
 # Drop the label feature
 train = train.drop(""label"",axis=1)"
"SETUP ASSIGN = train.values ASSIGN = StandardScaler().fit_transform(X) ASSIGN = np.mean(X_std, axis=0) ASSIGN = np.cov(X_std.T) ASSIGN = np.linalg.eig(cov_mat) ASSIGN = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))] ASSIGN.sort(key = lambda x: x[0], reverse= True) ASSIGN = sum(eig_vals) ASSIGN = [(ipath)*100 for i in sorted(eig_vals, reverse=True)] ASSIGN = np.cumsum(var_exp)",1,stream,"# Standardize the data
 from sklearn.preprocessing import StandardScaler
 X = train.values
 X_std = StandardScaler().fit_transform(X)
 
 # Calculating Eigenvectors and eigenvalues of Cov matirx
 mean_vec = np.mean(X_std, axis=0)
 cov_mat = np.cov(X_std.T)
 eig_vals, eig_vecs = np.linalg.eig(cov_mat)
 # Create a list of (eigenvalue, eigenvector) tuples
 eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]
 
 # Sort the eigenvalue, eigenvector pair from high to low
 eig_pairs.sort(key = lambda x: x[0], reverse= True)
 
 # Calculation of Explained Variance from the eigenvalues
 tot = sum(eig_vals)
 var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance
 cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance"
"ASSIGN = go.Scatter( ASSIGN=list(range(784)), ASSIGN= cum_var_exp, ASSIGN='lines+markers', ASSIGN=""'Cumulative Explained Variance'"", ASSIGN=dict( ASSIGN='spline', ASSIGN = 'goldenrod' ) ) ASSIGN = go.Scatter( ASSIGN=list(range(784)), ASSIGN= var_exp, ASSIGN='lines+markers', ASSIGN=""'Individual Explained Variance'"", ASSIGN=dict( ASSIGN='linear', ASSIGN = 'black' ) ) ASSIGN = tls.make_subplots(insets=[{'cell': (1,1), 'l': 0.7, 'b': 0.5}], ASSIGN=True) ASSIGN.append_trace(ASSIGN, 1, 1) ASSIGN.append_trace(ASSIGN,1,1) ASSIGN.layout.title = 'Explained Variance plots - Full and Zoomed-in' ASSIGN.layout.xaxis = dict(range=[0, 80], title = 'Feature columns') ASSIGN.layout.yaxis = dict(range=[0, 60], title = 'Explained Variance')",0,stream,"trace1 = go.Scatter(
     x=list(range(784)),
     y= cum_var_exp,
     mode='lines+markers',
     name=""'Cumulative Explained Variance'"",
 #     hoverinfo= cum_var_exp,
     line=dict(
         shape='spline',
         color = 'goldenrod'
     )
 )
 trace2 = go.Scatter(
     x=list(range(784)),
     y= var_exp,
     mode='lines+markers',
     name=""'Individual Explained Variance'"",
 #     hoverinfo= var_exp,
     line=dict(
         shape='linear',
         color = 'black'
     )
 )
 fig = tls.make_subplots(insets=[{'cell': (1,1), 'l': 0.7, 'b': 0.5}],
                           print_grid=True)
 
 fig.append_trace(trace1, 1, 1)
 fig.append_trace(trace2,1,1)
 fig.layout.title = 'Explained Variance plots - Full and Zoomed-in'
 fig.layout.xaxis = dict(range=[0, 80], title = 'Feature columns')
 fig.layout.yaxis = dict(range=[0, 60], title = 'Explained Variance')
 # fig['data'] = []
 # fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))
 # fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))
 
 # fig['data'] = go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance')]
 # fig['data'] += [go.Scatter(x=list(range(784)), y=var_exp, xaxis='x2', yaxis='y2',name = 'Individual Explained Variance')]
 
 # # fig['data'] = data
 # # fig['layout'] = layout
 # # fig['data'] += data2
 # # fig['layout'] += layout2
 # py.iplot(fig, filename='inset example')"
"ASSIGN = 30 ASSIGN = PCA(n_components=n_components).fit(train.values) ASSIGN = pca.components_.reshape(n_components, 28, 28) ASSIGN = pca.components_",1,not_existent,"# Invoke SKlearn's PCA method
 n_components = 30
 pca = PCA(n_components=n_components).fit(train.values)
 
 eigenvalues = pca.components_.reshape(n_components, 28, 28)
 
 # Extracting the PCA components ( eignevalues )
 #eigenvalues = pca.components_.reshape(n_components, 28, 28)
 eigenvalues = pca.components_"
"ASSIGN = 4 ASSIGN = 7 plt.figure(figsize=(13,12)) for i in list(range(ASSIGN * ASSIGN)): ASSIGN =0 plt.subplot(ASSIGN, ASSIGN, i + 1) plt.imshow(eigenvalues[i].reshape(28,28), cmap='jet') ASSIGN = 'Eigenvalue ' + str(i + 1) plt.title(ASSIGN, size=6.5) plt.xticks(()) plt.yticks(()) plt.show()",0,display_data,"n_row = 4
 n_col = 7
 
 # Plot the first 8 eignenvalues
 plt.figure(figsize=(13,12))
 for i in list(range(n_row * n_col)):
     offset =0
     plt.subplot(n_row, n_col, i + 1)
     plt.imshow(eigenvalues[i].reshape(28,28), cmap='jet')
     title_text = 'Eigenvalue ' + str(i + 1)
     plt.title(title_text, size=6.5)
     plt.xticks(())
     plt.yticks(())
 plt.show()"
"plt.figure(figsize=(14,12)) for digit_num in range(0,70): plt.subplot(7,10,digit_num+1) ASSIGN = train.iloc[digit_num].as_matrix().reshape(28,28) plt.imshow(ASSIGN, interpolation = ""none"", cmap = ""afmhot"") plt.xticks([]) plt.yticks([]) plt.tight_layout()",0,stream,"# plot some of the numbers
 plt.figure(figsize=(14,12))
 for digit_num in range(0,70):
     plt.subplot(7,10,digit_num+1)
     grid_data = train.iloc[digit_num].as_matrix().reshape(28,28)  # reshape from 1d to 2d pixel array
     plt.imshow(grid_data, interpolation = ""none"", cmap = ""afmhot"")
     plt.xticks([])
     plt.yticks([])
 plt.tight_layout()"
del X X= train[:6000].values del train ASSIGN = StandardScaler().fit_transform(X) ASSIGN = PCA(n_components=5) ASSIGN.fit(ASSIGN) ASSIGN = pca.transform(X_std) ASSIGN = target[:6000],1,stream,"# Delete our earlier created X object
 del X
 # Taking only the first N rows to speed things up
 X= train[:6000].values
 del train
 # Standardising the values
 X_std = StandardScaler().fit_transform(X)
 
 # Call the PCA method with 5 components. 
 pca = PCA(n_components=5)
 pca.fit(X_std)
 X_5d = pca.transform(X_std)
 
 # For cluster coloring in our Plotly plots, remember to also restrict the target values 
 Target = target[:6000]"
"ASSIGN = go.Scatter( ASSIGN = X_5d[:,0], ASSIGN = X_5d[:,1], ASSIGN = 'markers', ASSIGN = Target, ASSIGN = False, ASSIGN = dict( ASSIGN = 8, ASSIGN = Target, ASSIGN ='Jet', ASSIGN = False, ASSIGN = dict( ASSIGN = 2, ASSIGN = 'rgb(255, 255, 255)' ), ASSIGN = 0.8 ) ) ASSIGN = [trace0] ASSIGN = go.Layout( ASSIGN= 'Principal Component Analysis (PCA)', ASSIGN= 'closest', ASSIGN= dict( ASSIGN= 'First Principal Component', ASSIGN= 5, ASSIGN= False, ASSIGN= 2, ), ASSIGN=dict( ASSIGN= 'Second Principal Component', ASSIGN= 5, ASSIGN= 2, ), ASSIGN= True ) ASSIGN = dict(data=data, layout=layout) py.iplot(ASSIGN, filename='styled-scatter')",0,display_data,"trace0 = go.Scatter(
     x = X_5d[:,0],
     y = X_5d[:,1],
 #     name = Target,
 #     hoveron = Target,
     mode = 'markers',
     text = Target,
     showlegend = False,
     marker = dict(
         size = 8,
         color = Target,
         colorscale ='Jet',
         showscale = False,
         line = dict(
             width = 2,
             color = 'rgb(255, 255, 255)'
         ),
         opacity = 0.8
     )
 )
 data = [trace0]
 
 layout = go.Layout(
     title= 'Principal Component Analysis (PCA)',
     hovermode= 'closest',
     xaxis= dict(
          title= 'First Principal Component',
         ticklen= 5,
         zeroline= False,
         gridwidth= 2,
     ),
     yaxis=dict(
         title= 'Second Principal Component',
         ticklen= 5,
         gridwidth= 2,
     ),
     showlegend= True
 )
 
 
 fig = dict(data=data, layout=layout)
 py.iplot(fig, filename='styled-scatter')"
"SETUP ASSIGN = KMeans(n_clusters=9) ASSIGN = kmeans.fit_predict(X_5d) ASSIGN = go.Scatter(x=X_5d[:, 0], y= X_5d[:, 1], mode=""markers"", ASSIGN=False, ASSIGN=dict( ASSIGN=8, ASSIGN = X_clustered, ASSIGN = 'Portland', ASSIGN=False, ASSIGN = dict( ASSIGN = 2, ASSIGN = 'rgb(255, 255, 255)' ) )) ASSIGN = go.Layout( ASSIGN= 'KMeans Clustering', ASSIGN= 'closest', ASSIGN= dict( ASSIGN= 'First Principal Component', ASSIGN= 5, ASSIGN= False, ASSIGN= 2, ), ASSIGN=dict( ASSIGN= 'Second Principal Component', ASSIGN= 5, ASSIGN= 2, ), ASSIGN= True ) ASSIGN = [trace_Kmeans] ASSIGN = dict(data=data, layout= layout) py.iplot(ASSIGN, filename=""svm"")",0,display_data,"from sklearn.cluster import KMeans # KMeans clustering 
 # Set a KMeans clustering with 9 components ( 9 chosen sneakily ;) as hopefully we get back our 9 class labels)
 kmeans = KMeans(n_clusters=9)
 # Compute cluster centers and predict cluster indices
 X_clustered = kmeans.fit_predict(X_5d)
 
 trace_Kmeans = go.Scatter(x=X_5d[:, 0], y= X_5d[:, 1], mode=""markers"",
                     showlegend=False,
                     marker=dict(
                             size=8,
                             color = X_clustered,
                             colorscale = 'Portland',
                             showscale=False, 
                             line = dict(
             width = 2,
             color = 'rgb(255, 255, 255)'
         )
                    ))
 
 layout = go.Layout(
     title= 'KMeans Clustering',
     hovermode= 'closest',
     xaxis= dict(
          title= 'First Principal Component',
         ticklen= 5,
         zeroline= False,
         gridwidth= 2,
     ),
     yaxis=dict(
         title= 'Second Principal Component',
         ticklen= 5,
         gridwidth= 2,
     ),
     showlegend= True
 )
 
 data = [trace_Kmeans]
 fig1 = dict(data=data, layout= layout)
 # fig1.append_trace(contour_list)
 py.iplot(fig1, filename=""svm"")"
SETUP,0,not_existent,"from IPython.display import display, Math, Latex"
"ASSIGN = LDA(n_components=5) ASSIGN = lda.fit_transform(X_std, Target.values )",1,stream,"lda = LDA(n_components=5)
 # Taking in as second argument the Target as labels
 X_LDA_2D = lda.fit_transform(X_std, Target.values )"
"8 ASSIGN = go.Scatter( ASSIGN = X_LDA_2D[:,0], ASSIGN = X_LDA_2D[:,1], ASSIGN = 'markers', ASSIGN = Target, ASSIGN = True, ASSIGN = dict( ASSIGN = 8, ASSIGN = Target, ASSIGN ='Jet', ASSIGN = False, ASSIGN = dict( ASSIGN = 2, ASSIGN = 'rgb(255, 255, 255)' ), ASSIGN = 0.8 ) ) ASSIGN = [traceLDA] ASSIGN = go.Layout( ASSIGN= 'Linear Discriminant Analysis (LDA)', ASSIGN= 'closest', ASSIGN= dict( ASSIGN= 'First Linear Discriminant', ASSIGN= 5, ASSIGN= False, ASSIGN= 2, ), ASSIGN=dict( ASSIGN= 'Second Linear Discriminant', ASSIGN= 5, ASSIGN= 2, ), ASSIGN= False ) ASSIGN = dict(data=data, layout=layout) py.iplot(ASSIGN, filename='styled-scatter')",0,display_data,"8# Using the Plotly library again
 traceLDA = go.Scatter(
     x = X_LDA_2D[:,0],
     y = X_LDA_2D[:,1],
 #     name = Target,
 #     hoveron = Target,
     mode = 'markers',
     text = Target,
     showlegend = True,
     marker = dict(
         size = 8,
         color = Target,
         colorscale ='Jet',
         showscale = False,
         line = dict(
             width = 2,
             color = 'rgb(255, 255, 255)'
         ),
         opacity = 0.8
     )
 )
 data = [traceLDA]
 
 layout = go.Layout(
     title= 'Linear Discriminant Analysis (LDA)',
     hovermode= 'closest',
     xaxis= dict(
          title= 'First Linear Discriminant',
         ticklen= 5,
         zeroline= False,
         gridwidth= 2,
     ),
     yaxis=dict(
         title= 'Second Linear Discriminant',
         ticklen= 5,
         gridwidth= 2,
     ),
     showlegend= False
 )
 
 fig = dict(data=data, layout=layout)
 py.iplot(fig, filename='styled-scatter')"
ASSIGN = TSNE(n_components=2) ASSIGN = tsne.fit_transform(X_std),1,not_existent,"# Invoking the t-SNE method
 tsne = TSNE(n_components=2)
 tsne_results = tsne.fit_transform(X_std) "
"ASSIGN = go.Scatter( ASSIGN = tsne_results[:,0], ASSIGN = tsne_results[:,1], ASSIGN = Target, ASSIGN = Target, ASSIGN = 'markers', ASSIGN = Target, ASSIGN = True, ASSIGN = dict( ASSIGN = 8, ASSIGN = Target, ASSIGN ='Jet', ASSIGN = False, ASSIGN = dict( ASSIGN = 2, ASSIGN = 'rgb(255, 255, 255)' ), ASSIGN = 0.8 ) ) ASSIGN = [traceTSNE] ASSIGN = dict(title = 'TSNE (T-Distributed Stochastic Neighbour Embedding)', ASSIGN= 'closest', ASSIGN = dict(zeroline = False), ASSIGN = dict(zeroline = False), ASSIGN= False, ) ASSIGN = dict(data=data, layout=layout) py.iplot(ASSIGN, filename='styled-scatter')",0,error,"traceTSNE = go.Scatter(
     x = tsne_results[:,0],
     y = tsne_results[:,1],
     name = Target,
      hoveron = Target,
     mode = 'markers',
     text = Target,
     showlegend = True,
     marker = dict(
         size = 8,
         color = Target,
         colorscale ='Jet',
         showscale = False,
         line = dict(
             width = 2,
             color = 'rgb(255, 255, 255)'
         ),
         opacity = 0.8
     )
 )
 data = [traceTSNE]
 
 layout = dict(title = 'TSNE (T-Distributed Stochastic Neighbour Embedding)',
               hovermode= 'closest',
               yaxis = dict(zeroline = False),
               xaxis = dict(zeroline = False),
               showlegend= False,
 
              )
 
 fig = dict(data=data, layout=layout)
 py.iplot(fig, filename='styled-scatter')"
SETUP,0,not_existent,import numpy as np import matplotlib.pyplot as plt import random %matplotlib inline 
"ASSIGN = range(1000) ASSIGN = np.asarray(ASSIGN) ASSIGN = 2 ASSIGN = 500 def line_function(ASSIGN): ASSIGN = w1 * X + b return y ASSIGN = line_function(X) plt.plot(ASSIGN,ASSIGN)",0,not_existent,"X = range(1000) #changing our list to numpy array to benifit from numpy's broadcasting X = np.asarray(X) w1 = 2 b  = 500 def line_function(X):     y = w1 * X + b     return y y = line_function(X)  plt.plot(X,y) "
"ASSIGN = range(1000) ASSIGN = np.asarray(ASSIGN) ASSIGN = 9path ASSIGN = 32 ASSIGN = line_function(C) ASSIGN = plt.figure(figsize=(4,3)) ASSIGN = fig.add_subplot(111) ASSIGN.set_title('change of ASSIGN with respect to ASSIGN') plt.plot(ASSIGN,ASSIGN) ASSIGN.set_xlabel('Celsius (ASSIGN)') ASSIGN.set_ylabel('Fahrenheit (ASSIGN)') plt.show()",0,not_existent,"C = range(1000) C = np.asarray(C) w1 = 9/2 b  = 32 F = line_function(C)  fig = plt.figure(figsize=(4,3)) ax = fig.add_subplot(111) ax.set_title('change of F with respect to C') # ax.scatter(x=data[:,0],y=data[:,1],label='Data') plt.plot(C,F) ax.set_xlabel('Celsius (C)') ax.set_ylabel('Fahrenheit (F)') plt.show()"
"ASSIGN = range(1000) ASSIGN = range(1000) ASSIGN = np.asarray(ASSIGN) ASSIGN = np.asarray(ASSIGN) ASSIGN = 8 ASSIGN = 6 ASSIGN = [w1,w2] ASSIGN = 500 def hyperplane(Xs): ASSIGN=b for (w,x) in zip(ASSIGN,Xs): ASSIGN+=w*x return y ASSIGN = plt.axes(projection='3d') ASSIGN = np.meshgrid(X1, X2) ASSIGN = [xv, yv] ASSIGN = hyperplane(Xs) ASSIGN.plot_surface(ASSIGN,ASSIGN);",0,not_existent,"X1 = range(1000) X2 = range(1000) X1 = np.asarray(X1) X2 = np.asarray(X2) w1 = 8 w2 = 6 Ws = [w1,w2] b  = 500 def hyperplane(Xs):     y=b     for (w,x) in zip(Ws,Xs):         y+=w*x     return y ax = plt.axes(projection='3d') xv, yv = np.meshgrid(X1, X2) Xs = [xv, yv] y = hyperplane(Xs) ax.plot_surface(xv, yv,y);  "
"SETUP ASSIGN = pd.read_csv('..path') plt.scatter(ASSIGN.x,ASSIGN.y,s = 4)",0,not_existent,"import pandas as pd df = pd.read_csv('../input/random-linear-regression/train.csv') plt.scatter(df.x,df.y,s = 4) "
"SETUP ASSIGN = pd.read_csv('..path') plt.scatter(ASSIGN.x,ASSIGN.y,s = 4) X= range(100) Y= X plt.plot(X,Y,c='red')",0,not_existent,"import pandas as pd df = pd.read_csv('../input/random-linear-regression/train.csv') plt.scatter(df.x,df.y,s = 4)  X= range(100) Y= X plt.plot(X,Y,c='red')"
"plt.scatter(df.x,df.y,s = 4) X= np.asarray((range(100))) Y= -X plt.plot(X,Y,c='red')",0,not_existent,"plt.scatter(df.x,df.y,s = 4)  X= np.asarray((range(100))) Y= -X plt.plot(X,Y,c='red')"
"SETUP ASSIGN = pd.read_csv('..path') ASSIGN = ASSIGN.dropna() plt.scatter(ASSIGN.x,ASSIGN.y,s = 4)",0,not_existent,"#loading data from the data set  import pandas as pd df = pd.read_csv('../input/random-linear-regression/train.csv') df = df.dropna() plt.scatter(df.x,df.y,s = 4) "
"def getXYfromDF(df): ASSIGN = [] for i in list(df.x): if(type(i)!=list): ASSIGN = [ASSIGN] ASSIGN.append(1) ASSIGN.append(ASSIGN) ASSIGN = np.asarray(ASSIGN) ASSIGN = np.asarray(df.ASSIGN) return X,y def randomWeights(m): ASSIGN= [] for ASSIGN in range(m): ASSIGN.append(random.randint(1,9)) ASSIGN = np.asarray(ASSIGN) return w ASSIGN = getXYfromDF(df) ASSIGN = X.shape[0] ASSIGN = X.shape[1] ASSIGN = randomWeights(m)",1,not_existent,"def getXYfromDF(df):     X = []     for i in list(df.x):         if(type(i)!=list):             i = [i]         i.append(1)         X.append(i)     X = np.asarray(X)     y = np.asarray(df.y)     return X,y def randomWeights(m):     w= []     for i in range(m):         w.append(random.randint(1,9))     w = np.asarray(w)     return w   X,y = getXYfromDF(df)    n = X.shape[0] m = X.shape[1]  w = randomWeights(m)"
"CHECKPOINT ASSIGN = np.dot(np.dot(np.linalg.inv(np.dot((X.T),X)),(X.T)),y) w",1,not_existent,"#finding the best wights analytically w = np.dot(np.dot(np.linalg.inv(np.dot((X.T),X)),(X.T)),y)  w"
"def plotTheLineWithData(X,w): plt.scatter(df.x,df.y,s = 4) X=[] for i in range(100): X.append([i,1]) ASSIGN = np.asarray(ASSIGN) ASSIGN = np.dot(X,w) plt.plot(ASSIGN[:,0],ASSIGN,c='red') plotTheLineWithData(ASSIGN,w)",0,not_existent,"def plotTheLineWithData(X,w):     plt.scatter(df.x,df.y,s = 4)      #this X is to generate test samples     X=[]     for i in range(100):         X.append([i,1])     X = np.asarray(X)     predicted_y = np.dot(X,w)      plt.plot(X[:,0],predicted_y,c='red') plotTheLineWithData(X,w)"
ASSIGN = getXYfromDF(df) ASSIGN = randomWeights(m),1,not_existent,"X,y = getXYfromDF(df)  w = randomWeights(m)"
"def MSE(y,y_predicted): return ((y- y_predicted)**2).mean()",0,not_existent,"def MSE(y,y_predicted):     return ((y- y_predicted)**2).mean() "
"def gradient_descent(X,y,w,max_iteration=1000,lr=0.00001): ASSIGN = [] ASSIGN = [] for iteration in range(max_iteration): ASSIGN = np.dot(X,w) ASSIGN = MSE(y,predicted_y) ASSIGN = round(ASSIGN,9) ASSIGN.append(w) ASSIGN.append(ASSIGN) ASSIGN = -(2path[0])* X.dot(loss).sum() ASSIGN = ASSIGN + lr * derivative return w_history,loss_hostory",0,not_existent,"def gradient_descent(X,y,w,max_iteration=1000,lr=0.00001):      w_history  = []     loss_hostory = []     for iteration in range(max_iteration):         predicted_y = np.dot(X,w)         loss =  MSE(y,predicted_y)         loss = round(loss,9)         w_history.append(w)         loss_hostory.append(loss)         derivative = -(2/y.shape[0])* X.dot(loss).sum()         w = w + lr * derivative     return w_history,loss_hostory "
"ASSIGN = gradient_descent(X,y,w,lr = 0.0000001)",0,not_existent,"w_history,loss_hostory = gradient_descent(X,y,w,lr = 0.0000001)"
ASSIGN = loss_hostory.index(min(loss_hostory)) ASSIGN = w_history[perfect_i] ASSIGN= perfect_w,1,not_existent,perfect_i = loss_hostory.index(min(loss_hostory))  perfect_w = w_history[perfect_i] w= perfect_w
"plotTheLineWithData(X,w)",0,not_existent,"plotTheLineWithData(X,w)"
"ASSIGN = pd.read_csv('..path') plt.scatter(ASSIGN.X1,ASSIGN.X2,c= ASSIGN.Y)",0,not_existent,"df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls') plt.scatter(df.X1,df.X2,c= df.Y)"
"ASSIGN = pd.read_csv('..path') plt.scatter(ASSIGN.X1,ASSIGN.X2,c= ASSIGN.Y) ASSIGN = range(1000) ASSIGN= X plt.plot(ASSIGN,ASSIGN,c='red')",0,not_existent,"df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls') plt.scatter(df.X1,df.X2,c= df.Y) X = range(1000) y= X plt.plot(X,y,c='red')"
"ASSIGN = np.asarray(range(1000)) ASSIGN = -(2path)*X plt.plot(ASSIGN,ASSIGN)",0,not_existent,"X = np.asarray(range(1000)) y = -(2/3)*X  plt.plot(X,y)"
"ASSIGN = np.asarray(range(1000)) ASSIGN = -(2path)*X plt.plot(ASSIGN,ASSIGN) plt.plot(800,-400,'+',c='green')",0,not_existent,"X = np.asarray(range(1000)) y = -(2/3)*X  plt.plot(X,y) plt.plot(800,-400,'+',c='green')"
"ASSIGN = np.asarray(range(1000)) ASSIGN = -(2path)*X plt.plot(ASSIGN,ASSIGN) plt.plot(400,-500,'_',c='red')",0,not_existent,"X = np.asarray(range(1000)) y = -(2/3)*X  plt.plot(X,y) plt.plot(400,-500,'_',c='red')"
ASSIGN = pd.read_csv('..path'),0,not_existent,df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')
"def getXYfromDF(df): ASSIGN = [] for i in df[['X1','X2']].values.tolist(): if(type(i)!=list): ASSIGN = [ASSIGN] ASSIGN.append(1) ASSIGN.append(ASSIGN) ASSIGN = np.asarray(ASSIGN) ASSIGN = np.asarray(df.Y) return X,y def randomWeights(m): ASSIGN= [] for ASSIGN in range(m): ASSIGN.append(random.randint(1,9)path) ASSIGN = np.asarray(ASSIGN) return w",1,not_existent,"def getXYfromDF(df):     X = []     for i in df[['X1','X2']].values.tolist():         if(type(i)!=list):             i = [i]         i.append(1)         X.append(i)     X = np.asarray(X)     y = np.asarray(df.Y)     return X,y def randomWeights(m):     w= []     for i in range(m):         w.append(random.randint(1,9)/100)     w = np.asarray(w)     return w "
CHECKPOINT ASSIGN = getXYfromDF(df) ASSIGN= randomWeights(3) print(ASSIGN),1,not_existent,"X,y = getXYfromDF(df) w= randomWeights(3) print(w)  "
"CHECKPOINT plt.scatter(df.X1,df.X2,c=df.Y) ASSIGN = np.column_stack((range(1000),np.ones(1000))) print(ASSIGN.shape) print(w[1:2].shape) ASSIGN = -np.divide(np.dot(X12,w[1:3]),w[0]) ASSIGN = [sub[0] for sub in X12] ASSIGN = res plt.plot(ASSIGN,ASSIGN)",0,not_existent,"plt.scatter(df.X1,df.X2,c=df.Y) X12 = np.column_stack((range(1000),np.ones(1000))) print(X12.shape) print(w[1:2].shape) y0 =  -np.divide(np.dot(X12,w[1:3]),w[0]) res = [sub[0] for sub in X12]  X1 = res plt.plot(X1,y0)"
"CHECKPOINT def equales(list1,list2): if(len(list1)!=len(list2)): return False else: for i in range(len(list1)): if(list1[i]!=list2[i]): return False return True def perceptron(X,y,w,learning_rate = 0.0001,max_iterations= 1000): for iteration in range(max_iterations): ASSIGN = w for i in range(w.shape[0]): if(np.dot(np.dot(X[i],w),y[i]) < 0 and y[i]<0): ASSIGN=ASSIGN- learning_rate * X[i] elif(np.dot(np.dot(X[i],ASSIGN),y[i]) < 0 and y[i]>0): ASSIGN=ASSIGN+ learning_rate * X[i] if(equales(ASSIGN,ASSIGN)): print('ASSIGN == ASSIGN in ',iteration) break return w",1,not_existent,"def equales(list1,list2):     if(len(list1)!=len(list2)):         return False     else:          for i in range(len(list1)):             if(list1[i]!=list2[i]):                 return False     return True def perceptron(X,y,w,learning_rate = 0.0001,max_iterations= 1000):     for iteration in range(max_iterations):         prev_w = w         for i in range(w.shape[0]):             if(np.dot(np.dot(X[i],w),y[i]) < 0 and y[i]<0):                 w=w- learning_rate * X[i]                              elif(np.dot(np.dot(X[i],w),y[i]) < 0 and y[i]>0):                 w=w+ learning_rate * X[i]         if(equales(prev_w,w)):             print('prev_w == w in ',iteration)             break                       return w"
"ASSIGN = perceptron(X,y,w,learning_rate=0.000001,max_iterations= 100000)",0,not_existent,"new_w = perceptron(X,y,w,learning_rate=0.000001,max_iterations= 100000)"
ASSIGN= new_w,1,not_existent,w= new_w
"def softmax(z): ASSIGN = np.exp(z) return ASSIGN path() ASSIGN = (3,12,-5,0,10) np.round(softmax(ASSIGN),1)",1,not_existent,"def softmax(z):     e_z = np.exp(z)     return e_z / e_z.sum() z = (3,12,-5,0,10)  np.round(softmax(z),1)"
ASSIGN = pd.read_csv('..path'),0,not_existent,#load the data  df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')
"def getOneHot(y): ASSIGN = [] for i in range(y.shape[0]): if(y[i]==-1): ASSIGN.append([1,0]) else: ASSIGN.append([0,1]) return np.asarray(ASSIGN) def getXYfromDF(df): ASSIGN = [] for i in df[['X1','X2']].values.tolist(): if(type(i)!=list): ASSIGN = [ASSIGN] ASSIGN.append(1) ASSIGN.append(ASSIGN) ASSIGN = np.asarray(ASSIGN) ASSIGN = np.asarray(df.Y) return X,y def randomWeights(m,k): ASSIGN= [] for ASSIGN in range(m): ASSIGN = [] for j in range(k): ASSIGN.append(random.randint(1,9)) ASSIGN.append(ASSIGN) ASSIGN = np.asarray(ASSIGN) return w ASSIGN = getXYfromDF(df) ASSIGN = getOneHot(ASSIGN)",1,not_existent,"#this function changes the output from numerical values to one hot vectors #example: #[1,-1,1] becomes:  #[[0,1],[1,0],[0,1] #this function is not general it's just for the case if the data has -1 and 1 cases only, #the generalized version will be coded next. def getOneHot(y):     newY = []     for i in range(y.shape[0]):         if(y[i]==-1):             newY.append([1,0])         else:             newY.append([0,1])     return np.asarray(newY) #this function loads the data to X and y vectors def getXYfromDF(df):     X = []     for i in df[['X1','X2']].values.tolist():         if(type(i)!=list):             i = [i]         i.append(1)         X.append(i)     X = np.asarray(X)     y = np.asarray(df.Y)     return X,y #this function generates random weights to initailize the weights(+ biases ofcourse) def randomWeights(m,k):     w= []     for i in range(m):         temp = []         for j in range(k):             temp.append(random.randint(1,9))         w.append(temp)     w = np.asarray(w)     return w   X,y = getXYfromDF(df)  y = getOneHot(y)"
"ASSIGN = X.shape[0] ASSIGN = X.shape[1] ASSIGN = 2 ASSIGN = randomWeights(m,k) ASSIGN=np.asarray(ASSIGN,'float64')",1,not_existent,"n = X.shape[0] #number of data samples m = X.shape[1] #number of features for each sample  k = 2 #number of classes w = randomWeights(m,k) w=np.asarray(w,'float64')"
CHECKPOINT w.shape,0,not_existent,w.shape
"CHECKPOINT CHECKPOINT plt.scatter(df.X1,df.X2,c=df.Y) ASSIGN = np.column_stack((range(1000),np.ones(1000))) ASSIGN = -np.divide(np.dot(X12,w[1:3]),w[0]) ASSIGN = [sub[0] for sub in X12] ASSIGN = np.asarray(res) print(ASSIGN.shape) print(ASSIGN.shape) plt.plot(ASSIGN,ASSIGN[:,0],c='blue') plt.plot(ASSIGN,ASSIGN[:,1],c='red') plt.show",0,not_existent,"#visualize the data that we are trying to fit, and plotting the current lines with the random weights plt.scatter(df.X1,df.X2,c=df.Y) X12 = np.column_stack((range(1000),np.ones(1000))) y0 =  -np.divide(np.dot(X12,w[1:3]),w[0]) res = [sub[0] for sub in X12]  X1 = np.asarray(res) print(X1.shape)  print(y0.shape) #plot the first line that represents the first linear model plt.plot(X1,y0[:,0],c='blue') #plot the second line that represents the second linear model plt.plot(X1,y0[:,1],c='red') plt.show"
def softmax(x): ASSIGN = np.exp(x - np.max(x)) return ASSIGN path(axis=0),1,not_existent,"#the softmax that we use previously was right, but numerically it wasn't stable #this 'edited softmax' is more numerically stable  def softmax(x):     temp = np.exp(x - np.max(x))  # for numerical stability     return temp / temp.sum(axis=0) "
"SETUP def cross_entropy(y, y_hat): ASSIGN = np.clip(ASSIGN, EPS, 1-EPS) return -np.sum(y * np.log(ASSIGN)path)",0,not_existent,"EPS = 1e-9 #same as in softmax,the first line in this function just gives numerical stability for cross entropy  def cross_entropy(y, y_hat):     y_hat = np.clip(y_hat, EPS, 1-EPS) # for numerical stability     return -np.sum(y * np.log(y_hat)/n) "
"ASSIGN = [] ASSIGN =1000 ASSIGN = 0.1 for _ in range(ASSIGN): ASSIGN = np.dot(X,w) ASSIGN = [] for i in range(n): ASSIGN.append(softmax(ASSIGN[i])) ASSIGN = np.asarray(ASSIGN) ASSIGN.append(cross_entropy(y,ASSIGN)) for j in range(k): ASSIGN=0 for i in range(n): ASSIGN += np.dot(X.T,(y-ASSIGN)) ASSIGN = - deltaTemppath ASSIGN = np.asarray(ASSIGN) w-=ASSIGN*ASSIGN",0,not_existent,"history = [] #loss history  numberOfRounds =1000 # max number of times the optimization algorithm will run learningRate = 0.1 for _ in range(numberOfRounds):     z = np.dot(X,w)     y_hat = []     for i in range(n):         y_hat.append(softmax(z[i]))     y_hat = np.asarray(y_hat)     history.append(cross_entropy(y,y_hat))      for j in range(k):         deltaTemp=0          #deltaTemp is the loss derivative , and we aggregate it from all n samples (in the simple form of gradient descent,         #and it works fine in case of offline training and smalle number of samples)          for i in range(n):             deltaTemp += np.dot(X.T,(y-y_hat))         deltaTemp  = - deltaTemp/n         deltaTemp = np.asarray(deltaTemp)         w-=learningRate*deltaTemp"
plt.plot(history) plt.title('the change of loss with iterations'),0,not_existent,plt.plot(history) plt.title('the change of loss with iterations')
SETUP ASSIGN = load_digits(),0,not_existent,from sklearn.datasets import load_digits digits = load_digits()
ASSIGN = digits.data ASSIGN = digits.target,1,not_existent,X = digits.data y = digits.target
"SETUP ASSIGN = plt.subplots(2, 3, sharex='col', sharey='row') ASSIGN = 0 for i in range(2): for j in range(3): ASSIGN = X[currNum,0:64] ASSIGN += 1 ASSIGN = np.array(ASSIGN, dtype='float') ASSIGN = img.reshape((8, 8)) ax[i, j].imshow(ASSIGN, cmap='gray')",0,not_existent,"import matplotlib.pyplot as plt fig, ax = plt.subplots(2, 3, sharex='col', sharey='row') currNum = 0 for i in range(2):     for j in range(3):         img = X[currNum,0:64]         currNum += 1         img = np.array(img, dtype='float')         pixels = img.reshape((8, 8))         ax[i, j].imshow(pixels, cmap='gray')"
"def add_bias(X): ASSIGN = [] for i in range(X.shape[0]): ASSIGN.append(np.append(X[i],1)) return np.asarray(ASSIGN) ASSIGN = add_bias(ASSIGN)",1,not_existent,"#simple functions to add 1 to each sample's vector for the bias def add_bias(X):     newX = []      for i in range(X.shape[0]):         newX.append(np.append(X[i],1))     return np.asarray(newX) X = add_bias(X)"
"ASSIGN=[0,1,2,3,4,5,6,7,8,9] def oneHot(y,ASSIGN): ASSIGN = [] for i in range(y.shape[0]): ASSIGN = [] for j in ASSIGN: if(y[i]==ASSIGN[j]): ASSIGN.append(1) else: ASSIGN.append(0) ASSIGN.append(ASSIGN) return np.asarray(ASSIGN) ASSIGN = oneHot(ASSIGN,targets)",1,not_existent,"#change the form of the target values from a single digit to a onehot so we can apply out algorithm targets=[0,1,2,3,4,5,6,7,8,9] def oneHot(y,targets):     newY = []     for i in range(y.shape[0]):          temp = []          for j in targets:             if(y[i]==targets[j]):                 temp.append(1)             else:                 temp.append(0)         newY.append(temp)     return np.asarray(newY) y = oneHot(y,targets)"
"ASSIGN = X.shape[0] ASSIGN = X.shape[1] ASSIGN = 10 ASSIGN = randomWeights(m,k) ASSIGN=np.asarray(ASSIGN,'float64')",1,not_existent,"n = X.shape[0] #number of data samples m = X.shape[1] #number of features for each sample  k = 10 #number of classes w = randomWeights(m,k) w=np.asarray(w,'float64')"
"CHECKPOINT ASSIGN = [] ASSIGN = 20 for iteration in range(ASSIGN): print('iteration: ',iteration) ASSIGN = np.dot(X,w) ASSIGN = [] for i in range(n): ASSIGN.append(softmax(ASSIGN[i])) ASSIGN = np.asarray(ASSIGN) ASSIGN.append(cross_entropy(y,ASSIGN)) for j in range(k): ASSIGN=0 for i in range(n): ASSIGN += np.dot(X.T,(y-ASSIGN)) ASSIGN = - deltaTemppath ASSIGN = np.asarray(ASSIGN) w-=0.1*ASSIGN",0,not_existent,"history = [] maxNumOfIterations = 20 for iteration in range(maxNumOfIterations):      print('iteration: ',iteration)     z = np.dot(X,w)     y_hat = []     for i in range(n):         y_hat.append(softmax(z[i]))     y_hat = np.asarray(y_hat)     history.append(cross_entropy(y,y_hat))     for j in range(k):         deltaTemp=0         for i in range(n):             deltaTemp += np.dot(X.T,(y-y_hat))         deltaTemp  = - deltaTemp/n         deltaTemp = np.asarray(deltaTemp)         w-=0.1*deltaTemp"
def giveMeValueFromOneHot(y_hat): return np.where(y_hat == 1)[0][0],1,not_existent,def giveMeValueFromOneHot(y_hat):     return np.where(y_hat == 1)[0][0]
"def predictDis(x): ASSIGN = np.array(x, dtype='float') ASSIGN = x[0:64].reshape((8, 8)) plt.imshow(ASSIGN, cmap='gray') ASSIGN = np.dot(x,w) print (""the class of this Image is: "",giveMeValueFromOneHot(softmax(ASSIGN)))",0,not_existent,"def predictDis(x):     img = np.array(x, dtype='float')     pixels = x[0:64].reshape((8, 8))     plt.imshow(pixels, cmap='gray')     z = np.dot(x,w)     print (""the class of this Image is: "",giveMeValueFromOneHot(softmax(z)))"
ASSIGN = X[0] predictDis(ASSIGN),1,not_existent,x = X[0] predictDis(x)
ASSIGN = X[1] predictDis(ASSIGN),1,not_existent,x = X[1] predictDis(x)
ASSIGN = X[3] predictDis(ASSIGN),1,not_existent,x = X[3] predictDis(x)
SETUP,0,not_existent,!pip install pydub
"SETUP ASSIGN="".path"" os.mkdir(ASSIGN) for dirname, _, filenames in os.walk('..path'): for filename in filenames: ASSIGN = ""..path""+filename ASSIGN = test_set+os.path.splitext(filename)[0]+"".wav"" ASSIGN = AudioSegment.from_mp3(src) ASSIGN = ASSIGN.set_frame_rate(8000) ASSIGN.export(ASSIGN, format=""wav"")",1,not_existent,"import os  from os import path from pydub import AudioSegment      test_set=""./test_set/""    os.mkdir(test_set)     for dirname, _, filenames in os.walk('../input/quran-asr-challenge/test_set'):     for filename in filenames:         # files                                                                                  src = ""../input/quran-asr-challenge/test_set/""+filename         dst = test_set+os.path.splitext(filename)[0]+"".wav""         # convert wav to mp3                                                                     sound = AudioSegment.from_mp3(src)         sound = sound.set_frame_rate(8000)         sound.export(dst, format=""wav"") "
SETUP,0,not_existent,import random import re 
"CHECKPOINT def getpatterns(symbol): return ""$$$.{6}|.{3}$$$.{3}|.{6}$$$|$.{2}$.{2}$.{2}|.$.{2}$.{2}$.|.{2}$.{2}$.{2}$|$.{3}$.{3}$|.{2}$.$.$.{2}"".replace(""$"",symbol) def checkPatterns(pattern,TicTecBoard): return len(re.findall(pattern,"""".join(TicTecBoard))) def printTicTecBoard(TicTecBoard): print() ASSIGN="""" ASSIGN="""" for i in range(0,9): ASSIGN+= 3*"" "" + str(i+1) + "" ""*3+""|"" ASSIGN+= 3*"" "" + TicTecBoard[i] + "" ""*3+""|"" if (i+1)%3==0: ASSIGN=ASSIGN[0:-1]+"" ""*25+strTicTecBoard[0:-1] +""\n""+""_""*25+"" ""*25+""_""*20+""\n"" ASSIGN="""" print(ASSIGN[0:-75],) def getValidPlace(TicTecBoard): return [str(i+1) for i,x in enumerate(TicTecBoard) if x==""-""]",1,not_existent,"def getpatterns(symbol):     return ""$$$.{6}|.{3}$$$.{3}|.{6}$$$|$.{2}$.{2}$.{2}|.$.{2}$.{2}$.|.{2}$.{2}$.{2}$|$.{3}$.{3}$|.{2}$.$.$.{2}"".replace(""$"",symbol)  def checkPatterns(pattern,TicTecBoard):     return len(re.findall(pattern,"""".join(TicTecBoard)))  def printTicTecBoard(TicTecBoard):     print(""\n"")     strTicTecBoard=""""     strTicTecBoardLearn=""""     for i in range(0,9):         strTicTecBoardLearn+= 3*"" "" +  str(i+1) + "" ""*3+""|""         strTicTecBoard+= 3*"" "" + TicTecBoard[i] + "" ""*3+""|""         if (i+1)%3==0:             strTicTecBoardLearn=strTicTecBoardLearn[0:-1]+"" ""*25+strTicTecBoard[0:-1] +""\n""+""_""*25+"" ""*25+""_""*20+""\n""             strTicTecBoard=""""          print(strTicTecBoardLearn[0:-75],""\n"")  def getValidPlace(TicTecBoard):     return [str(i+1) for i,x in enumerate(TicTecBoard) if x==""-""]"
"CHECKPOINT ASSIGN=getpatterns(""O"") ASSIGN=getpatterns(""X"") TicTecBoard=[""-"" for i in range(9)] ASSIGN=""O"" try: while True: ValidIndexList=getValidPlace(TicTecBoard) ASSIGN==""O"": while True: printTicTecBoard(TicTecBoard) ASSIGN=input("" \n Enter Cell Number from Valid Index List ""+str(ValidIndexList )+"" : \n"") if ASSIGN in ValidIndexList: ASSIGN=int(ASSIGN)-1 break else: print() else: ASSIGN=0 for place in ValidIndexList: ASSIGN=list(TicTecBoard) ASSIGN=int(place)-1 SLICE=ASSIGN if checkPatterns(ASSIGN,ASSIGN)>0: ASSIGN=testIndex ASSIGN=1 ASSIGN==0: for place in ValidIndexList: ASSIGN=list(TicTecBoard) ASSIGN=int(place)-1 SLICE=""O"" if checkPatterns(ASSIGN,ASSIGN)>0: ASSIGN=testIndex ASSIGN=-1 ASSIGN==0: ASSIGN=4 if ""5"" in ValidIndexList else int(random.choice(ValidIndexList))-1 SLICE=ASSIGN if checkPatterns(getpatterns(ASSIGN),TicTecBoard)>0: printTicTecBoard(TicTecBoard) print('\x1b[6;30;42m' +ASSIGN++ '\x1b[0m') break elif checkPatterns(""-"",TicTecBoard)==0: printTicTecBoard(TicTecBoard) print() break else: ASSIGN=""X"" if ASSIGN==""O"" else ""O"" except: print()",0,stream," patternO=getpatterns(""O"") patternX=getpatterns(""X"") TicTecBoard=[""-"" for i in range(9)] player=""O"" try:        while True:             ValidIndexList=getValidPlace(TicTecBoard)         if player==""O"":             while True:                   printTicTecBoard(TicTecBoard)                 index=input("" \n Enter Cell Number from Valid Index List  ""+str(ValidIndexList )+"" : \n"")                  if index in ValidIndexList:                     index=int(index)-1                     break                 else:                     print(""Plz Enter Valied Place"")           else:             machineWin=0             for place in ValidIndexList:                 testTicTecBoard=list(TicTecBoard)                 testIndex=int(place)-1                 testTicTecBoard[testIndex]=player                 if checkPatterns(patternX,testTicTecBoard)>0:                      index=testIndex                     machineWin=1             if machineWin==0:                 for place in ValidIndexList:                     testTicTecBoard=list(TicTecBoard)                     testIndex=int(place)-1                     testTicTecBoard[testIndex]=""O""                     if checkPatterns(patternO,testTicTecBoard)>0:                          index=testIndex                         machineWin=-1             if machineWin==0:                 index=4 if ""5"" in ValidIndexList else int(random.choice(ValidIndexList))-1            TicTecBoard[index]=player           if checkPatterns(getpatterns(player),TicTecBoard)>0:              printTicTecBoard(TicTecBoard)             print('\x1b[6;30;42m'  +player+"" is Win ""+ '\x1b[0m')               break         elif checkPatterns(""-"",TicTecBoard)==0:              printTicTecBoard(TicTecBoard)             print(""\033[93m Game is Draw  \033[0m"")             break          else:             player=""X"" if player==""O"" else ""O""    except:     print(""Plz Enter Valied Index"")    "
kickstarters_2017.sample(5),0,not_existent,#a little glimpse at the data  kickstarters_2017.sample(5)
"ASSIGN = kickstarters_2017.ASSIGN ASSIGN = minmax_scaling(goal, columns = [0]) ASSIGN=plt.subplots(1,2) sns.distplot(kickstarters_2017.ASSIGN, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Scaled data"")",0,not_existent,"# Your turn!   # We just scaled the ""usd_goal_real"" column. What about the ""goal"" column? goal = kickstarters_2017.goal  # scale the goals from 0 to 1 scaled_goal = minmax_scaling(goal, columns = [0])  # plot the original & scaled data together to compare fig, ax=plt.subplots(1,2) sns.distplot(kickstarters_2017.goal, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(scaled_goal, ax=ax[1]) ax[1].set_title(""Scaled data"")"
"ASSIGN = kickstarters_2017.pledged > 0 ASSIGN = kickstarters_2017.pledged.loc[index_of_positive_pledges] ASSIGN = stats.boxcox(positive_pledged)[0] ASSIGN=plt.subplots(1,2) sns.distplot(ASSIGN, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(ASSIGN, ax=ax[1]) ax[1].set_title(""Normalized data"")",0,not_existent,"# Your turn!  # We looked as the usd_pledged_real column. What about the ""pledged"" column? Does it have the same info? index_of_positive_pledged = kickstarters_2017.pledged > 0  # get only positive pledged (using their indexes) positive_pledged = kickstarters_2017.pledged.loc[index_of_positive_pledges]  # normalize the pledges (w/ Box-Cox) normalized_pledged = stats.boxcox(positive_pledged)[0]  # plot both together to compare fig, ax=plt.subplots(1,2) sns.distplot(positive_pledged, ax=ax[0]) ax[0].set_title(""Original Data"") sns.distplot(normalized_pledged, ax=ax[1]) ax[1].set_title(""Normalized data"")"
"SETUP init_notebook_mode(connected=True) ASSIGN = pd.read_csv(""path"") ASSIGN.head() ASSIGN = ASSIGN.rename(columns={'Countrypath':'Country'}) ASSIGN = ASSIGN.rename(columns={'ObservationDate':'Date'}) ASSIGN = df.groupby(['Country', 'Date']).sum().reset_index().sort_values('Date', ascending=False) ASSIGN = ASSIGN.drop_duplicates(subset = ['Country']) ASSIGN = ASSIGN[ASSIGN['Confirmed']>0] ASSIGN = df[df['Confirmed']>0] ASSIGN = ASSIGN.groupby(['Date','Country']).sum().reset_index() df_countrydate ASSIGN = px.choropleth(df_countrydate, ASSIGN=""Country"", ASSIGN = ""country names"", ASSIGN=""Confirmed"", ASSIGN=""Country"", ASSIGN=""Date"" ) ASSIGN.update_layout( ASSIGN = 'Global Spread of Coronavirus', ASSIGN = 0.5, ASSIGN=dict( ASSIGN = False, ASSIGN = False, )) ASSIGN.show()",1,display_data,"# Import libraries import numpy as np  import pandas as pd  import plotly as py import plotly.express as px import plotly.graph_objs as go from plotly.subplots import make_subplots from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot init_notebook_mode(connected=True) # Read Data df = pd.read_csv(""/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv"") df.head()  # Rename columns df = df.rename(columns={'Country/Region':'Country'}) df = df.rename(columns={'ObservationDate':'Date'}) # Manipulate Dataframe df_countries = df.groupby(['Country', 'Date']).sum().reset_index().sort_values('Date', ascending=False) df_countries = df_countries.drop_duplicates(subset = ['Country']) df_countries = df_countries[df_countries['Confirmed']>0] df_countrydate = df[df['Confirmed']>0] df_countrydate = df_countrydate.groupby(['Date','Country']).sum().reset_index() df_countrydate # Creating the visualization fig = px.choropleth(df_countrydate,                      locations=""Country"",                      locationmode = ""country names"",                     color=""Confirmed"",                      hover_name=""Country"",                      animation_frame=""Date""                    ) fig.update_layout(     title_text = 'Global Spread of Coronavirus',     title_x = 0.5,     geo=dict(         showframe = False,         showcoastlines = False,     ))      fig.show()"
"SETUP ASSIGN = pd.read_csv('..path', dtype=np.object) ASSIGN = multiple.iloc[0,:] ASSIGN = multiple.iloc[1:,:]",1,not_existent,"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import matplotlib.patches as patches  %matplotlib inline  multiple = pd.read_csv('../input/multipleChoiceResponses.csv', dtype=np.object) mulcQ = multiple.iloc[0,:] mulcA = multiple.iloc[1:,:] "
"ASSIGN = mulcA[round(mulcA.iloc[:,0].astype(int) path) <= 3].index ASSIGN = ASSIGN.drop(fast, axis=0) ASSIGN = mulcA.Q5 ASSIGN.value_counts(normalize=True).plot(kind='bar')",0,not_existent,"fast = mulcA[round(mulcA.iloc[:,0].astype(int) / 60) <= 3].index mulcA = mulcA.drop(fast, axis=0) rol = mulcA.Q5 rol.value_counts(normalize=True).plot(kind='bar')"
ASSIGN = mulcA.Q9 ASSIGN.value_counts().plot(kind='bar'),0,not_existent,rem = mulcA.Q9 rem.value_counts().plot(kind='bar')
"mulcA.replace({'Q9':{'0-10,000':1,'10-20,000':2,'20-30,000':3,'30-40,000':4,'40-50,000':5,'50-60,000':6, '60-70,000':7,'70-80,000':8,'80-90,000':9,'90-100,000':10,'100-125,000':11,'125-150,000':12, '150-200,000':13,'200-250,000':14,'250-300,000':15,'300-400,000':16,'400-500,000':17, '500,000+':18}},inplace = True) mulcA.replace({'Q3':{'United Kingdom of Great Britain and Northern Ireland':'Great B.','Iran, Islamic Republic of...':'Iran', 'United States of America':'USA','Hong Kong (S.A.R.)':'Hong Kong','Republic of Korea':'R. Korea', 'Czech Republic':'Czech R.'}},inplace = True) ASSIGN = multiple.filter(regex=""(Q{t}$|Q{t}_)"".format(t = 16))[1:] ASSIGN = {'Q16_Part_1':'Python','Q16_Part_2':'R','Q16_Part_3':'SQL','Q16_Part_4': 'Bash','Q16_Part_5':'Java', 'Q16_Part_6':'Javascript','Q16_Part_7':'VBA','Q16_Part_8':'Cpath++','Q16_Part_9':'MATLAB', 'Q16_Part_10':'Scala','Q16_Part_11':'Julia','Q16_Part_12':'Go','Q16_Part_13':'C 'Q16_Part_15':'Ruby','Q16_Part_16':'SASpath'} ASSIGN= q16.rename(columns=q16_col).fillna(0).replace('[^\\d]',1, regex=True) ASSIGN.pop('Q16_Part_17') ASSIGN.pop('Q16_Part_18') ASSIGN.pop('Q16_OTHER_TEXT') ASSIGN = list(q16_lim.iloc[:0]) for i in ASSIGN: SLICE= ASSIGN['{}'.format(i)] ASSIGN = mulcA[(mulcA.Q5 == 'Computer science (software engineering, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Engineering (non-computer focused)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Mathematics or statistics') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'A business discipline (accounting, economics, finance, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Physics or astronomy') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Information technology, networking, or system administration') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Medical or life sciences (biology, chemistry, medicine, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Social sciences (anthropology, psychology, sociology, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Humanities (history, literature, philosophy, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Environmental science or geology') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Fine arts or performing arts') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = [com_sci.Q9.mean(),eng_nco.Q9.mean(),mat_sta.Q9.mean(),biu_dis.Q9.mean(),phy_ast.Q9.mean(),inf_tec.Q9.mean(),med_sci.Q9.mean(), ASSIGN.Q9.mean(),ASSIGN.Q9.mean(),ASSIGN.Q9.mean(),ASSIGN.Q9.mean()] plt.figure(figsize=(20,10)) plt.bar(np.arange(11),ASSIGN,color=['dodgerblue','c','tomato','silver','midnightblue','tan']) plt.xticks(np.arange(11), ('Com. Science', 'Engieneering', 'Mathematics', 'Economics', 'Physics','Inf. Tecnologist','Medics','Social sciences','Humanities', 'Env. Sciense','Arts')) plt.yticks(np.arange(10),('$10000','$20000','$30000','$40000','$50000','$60000','$70000','$80000','$90000','$100000'))",0,not_existent,"# mulcA.replace({'Q9':{'0-10,000':1,'10-20,000':2,'20-30,000':3,'30-40,000':4,'40-50,000':5,'50-60,000':6,                        '60-70,000':7,'70-80,000':8,'80-90,000':9,'90-100,000':10,'100-125,000':11,'125-150,000':12,                        '150-200,000':13,'200-250,000':14,'250-300,000':15,'300-400,000':16,'400-500,000':17,                                  '500,000+':18}},inplace = True) mulcA.replace({'Q3':{'United Kingdom of Great Britain and Northern Ireland':'Great B.','Iran, Islamic Republic of...':'Iran',                        'United States of America':'USA','Hong Kong (S.A.R.)':'Hong Kong','Republic of Korea':'R. Korea',                       'Czech Republic':'Czech R.'}},inplace = True)  q16 = multiple.filter(regex=""(Q{t}$|Q{t}_)"".format(t = 16))[1:] q16_col = {'Q16_Part_1':'Python','Q16_Part_2':'R','Q16_Part_3':'SQL','Q16_Part_4': 'Bash','Q16_Part_5':'Java',            'Q16_Part_6':'Javascript','Q16_Part_7':'VBA','Q16_Part_8':'C/C++','Q16_Part_9':'MATLAB',            'Q16_Part_10':'Scala','Q16_Part_11':'Julia','Q16_Part_12':'Go','Q16_Part_13':'C#/.NET','Q16_Part_14':'PHP',            'Q16_Part_15':'Ruby','Q16_Part_16':'SAS/STATA'}  q16_lim= q16.rename(columns=q16_col).fillna(0).replace('[^\\d]',1, regex=True) q16_lim.pop('Q16_Part_17') q16_lim.pop('Q16_Part_18') q16_lim.pop('Q16_OTHER_TEXT') lab = list(q16_lim.iloc[:0]) for i in lab:     mulcA[i]= q16_lim['{}'.format(i)] # com_sci = mulcA[(mulcA.Q5 == 'Computer science (software engineering, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] eng_nco = mulcA[(mulcA.Q5 == 'Engineering (non-computer focused)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] mat_sta = mulcA[(mulcA.Q5 == 'Mathematics or statistics') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] biu_dis = mulcA[(mulcA.Q5 == 'A business discipline (accounting, economics, finance, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] phy_ast = mulcA[(mulcA.Q5 == 'Physics or astronomy') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] inf_tec = mulcA[(mulcA.Q5 == 'Information technology, networking, or system administration') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] med_sci = mulcA[(mulcA.Q5 == 'Medical or life sciences (biology, chemistry, medicine, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] soc_sci = mulcA[(mulcA.Q5 == 'Social sciences (anthropology, psychology, sociology, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] hum_tie = mulcA[(mulcA.Q5 == 'Humanities (history, literature, philosophy, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] env_sci = mulcA[(mulcA.Q5 == 'Environmental science or geology') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] fin_art = mulcA[(mulcA.Q5 == 'Fine arts or performing arts') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]   rem_prom = [com_sci.Q9.mean(),eng_nco.Q9.mean(),mat_sta.Q9.mean(),biu_dis.Q9.mean(),phy_ast.Q9.mean(),inf_tec.Q9.mean(),med_sci.Q9.mean(),             soc_sci.Q9.mean(),hum_tie.Q9.mean(),env_sci.Q9.mean(),fin_art.Q9.mean()] plt.figure(figsize=(20,10)) plt.bar(np.arange(11),rem_prom,color=['dodgerblue','c','tomato','silver','midnightblue','tan']) plt.xticks(np.arange(11), ('Com. Science', 'Engieneering', 'Mathematics', 'Economics', 'Physics','Inf. Tecnologist','Medics','Social sciences','Humanities',                             'Env. Sciense','Arts')) plt.yticks(np.arange(10),('$10000','$20000','$30000','$40000','$50000','$60000','$70000','$80000','$90000','$100000'))"
"ASSIGN = mulcA.Q5[(mulcA.Q2 == '18-21')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '22-24')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '25-29')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '30-34')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '35-39')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '40-44')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '45-49')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '50-54')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '55-59')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '60-69')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '70-79')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '80+')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = pd.DataFrame([age_18_21.value_counts(normalize=True),age_22_24.value_counts(normalize=True),age_25_29.value_counts(normalize=True),age_30_34.value_counts(normalize=True), ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True) ,ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True)], ASSIGN=['age18-21','age22-24','age25-29','age30-34','age35-39','age40-44','age45-49','age50-54','age55-59' ,'age60-69','age70-79','age80+']).T ASSIGN = df_arol.plot.barh(rot=0, subplots=True,figsize=(15,35))",0,not_existent," age_18_21 = mulcA.Q5[(mulcA.Q2 == '18-21')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_22_24 = mulcA.Q5[(mulcA.Q2 == '22-24')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_25_29 = mulcA.Q5[(mulcA.Q2 == '25-29')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_30_34 = mulcA.Q5[(mulcA.Q2 == '30-34')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_35_39 = mulcA.Q5[(mulcA.Q2 == '35-39')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_40_44 = mulcA.Q5[(mulcA.Q2 == '40-44')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_45_49 = mulcA.Q5[(mulcA.Q2 == '45-49')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_50_54 = mulcA.Q5[(mulcA.Q2 == '50-54')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_55_59 = mulcA.Q5[(mulcA.Q2 == '55-59')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_60_69 = mulcA.Q5[(mulcA.Q2 == '60-69')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_70_79 = mulcA.Q5[(mulcA.Q2 == '70-79')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_80m = mulcA.Q5[(mulcA.Q2 == '80+')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]  df_arol = pd.DataFrame([age_18_21.value_counts(normalize=True),age_22_24.value_counts(normalize=True),age_25_29.value_counts(normalize=True),age_30_34.value_counts(normalize=True),                         age_35_39.value_counts(normalize=True),age_40_44.value_counts(normalize=True),age_45_49.value_counts(normalize=True),age_50_54.value_counts(normalize=True)                         ,age_55_59.value_counts(normalize=True),age_60_69.value_counts(normalize=True),age_70_79.value_counts(normalize=True),age_80m.value_counts(normalize=True)],                        index=['age18-21','age22-24','age25-29','age30-34','age35-39','age40-44','age45-49','age50-54','age55-59'                               ,'age60-69','age70-79','age80+']).T  axes = df_arol.plot.barh(rot=0, subplots=True,figsize=(15,35)) "
"plt.figure(figsize=(15,3)) ASSIGN = ['Comp. Science','Engeneering','Mathematics','Inf. Technology','Business','Physics','Medical','Soc. Science'] ASSIGN = pd.DataFrame([com_sci.Q3.value_counts(),eng_nco.Q3.value_counts(),mat_sta.Q3.value_counts(),inf_tec.Q3.value_counts() ,biu_dis.Q3.value_counts(),phy_ast.Q3.value_counts(),med_sci.Q3.value_counts(),soc_sci.Q3.value_counts()] ,index=[names]) ASSIGN= ASSIGN.drop(['Other'],axis=1).T ASSIGN = plt.subplots(4, 2, sharey=True,figsize=(15,20)) axes[0, 0].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Comp. Science'].values[:10])),color='cadetblue') axes[0, 0].set_title('Comp. Science') axes[0, 1].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Engeneering'].values[:10])),color='slategray') axes[0, 1].set_title('Engeneering') axes[1, 0].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Mathematics'].values[:10])),color='seagreen') axes[1, 0].set_title('Mathematics') axes[1, 1].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Inf. Technology'].values[:10])),color='palevioletred') axes[1, 1].set_title('Inf. Technology') axes[2, 0].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Business'].values[:10])),color='darkblue') axes[2, 0].set_title('Business') axes[2, 1].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Physics'].values[:10])),color='khaki') axes[2, 1].set_title('Physics') axes[3, 0].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Medical'].values[:10])),color='k') axes[3, 0].set_title('Medical') axes[3, 1].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Soc. Science'].values[:10])),color='firebrick') axes[3, 1].set_title('Soc. Science')",0,not_existent,"plt.figure(figsize=(15,3)) names = ['Comp. Science','Engeneering','Mathematics','Inf. Technology','Business','Physics','Medical','Soc. Science'] df_coun = pd.DataFrame([com_sci.Q3.value_counts(),eng_nco.Q3.value_counts(),mat_sta.Q3.value_counts(),inf_tec.Q3.value_counts()                         ,biu_dis.Q3.value_counts(),phy_ast.Q3.value_counts(),med_sci.Q3.value_counts(),soc_sci.Q3.value_counts()]                        ,index=[names]) df_coun= df_coun.drop(['Other'],axis=1).T fig, axes = plt.subplots(4, 2,  sharey=True,figsize=(15,20)) axes[0, 0].barh(df_coun.index[:10],list(map(int,df_coun['Comp. Science'].values[:10])),color='cadetblue') axes[0, 0].set_title('Comp. Science') axes[0, 1].barh(df_coun.index[:10],list(map(int,df_coun['Engeneering'].values[:10])),color='slategray') axes[0, 1].set_title('Engeneering') axes[1, 0].barh(df_coun.index[:10],list(map(int,df_coun['Mathematics'].values[:10])),color='seagreen') axes[1, 0].set_title('Mathematics') axes[1, 1].barh(df_coun.index[:10],list(map(int,df_coun['Inf. Technology'].values[:10])),color='palevioletred') axes[1, 1].set_title('Inf. Technology') axes[2, 0].barh(df_coun.index[:10],list(map(int,df_coun['Business'].values[:10])),color='darkblue') axes[2, 0].set_title('Business') axes[2, 1].barh(df_coun.index[:10],list(map(int,df_coun['Physics'].values[:10])),color='khaki') axes[2, 1].set_title('Physics') axes[3, 0].barh(df_coun.index[:10],list(map(int,df_coun['Medical'].values[:10])),color='k') axes[3, 0].set_title('Medical') axes[3, 1].barh(df_coun.index[:10],list(map(int,df_coun['Soc. Science'].values[:10])),color='firebrick') axes[3, 1].set_title('Soc. Science')"
"ASSIGN = pd.DataFrame([mulcA['Python'].sum(),mulcA['R'].sum(),mulcA['SQL'].sum(),mulcA['Bash'].sum(),mulcA['Java'].sum(), mulcA['Javascript'].sum(),mulcA['VBA'].sum(),mulcA['Cpath++'].sum(),mulcA['MATLAB'].sum(), mulcA['Scala'].sum(),mulcA['Julia'].sum(),mulcA['Go'].sum(),mulcA['C mulcA['PHP'].sum(),mulcA['Ruby'].sum(),mulcA['SASpath'].sum()],index=lab) ASSIGN.plot(kind='bar',color='brown')",0,not_existent,"lengs = pd.DataFrame([mulcA['Python'].sum(),mulcA['R'].sum(),mulcA['SQL'].sum(),mulcA['Bash'].sum(),mulcA['Java'].sum(),                            mulcA['Javascript'].sum(),mulcA['VBA'].sum(),mulcA['C/C++'].sum(),mulcA['MATLAB'].sum(),                            mulcA['Scala'].sum(),mulcA['Julia'].sum(),mulcA['Go'].sum(),mulcA['C#/.NET'].sum(),                            mulcA['PHP'].sum(),mulcA['Ruby'].sum(),mulcA['SAS/STATA'].sum()],index=lab) lengs.plot(kind='bar',color='brown')"
"ASSIGN=[mulcA[i][mulcA.Q5=='Computer science (software engineering, etc.)'].sum() for i in lab ] ASSIGN=[mulcA[i][mulcA.Q5=='Engineering (non-computer focused)'].sum() for i in lab ] ASSIGN=[mulcA[i][mulcA.Q5=='Mathematics or statistics'].sum() for i in lab ] ASSIGN=[mulcA[i][mulcA.Q5=='A business discipline (accounting, economics, finance, etc.)'].sum() for i in lab ] ASSIGN=[mulcA[i][mulcA.Q5=='Physics or astronomy'].sum() for i in lab ] ASSIGN=[mulcA[i][mulcA.Q5=='Information technology, networking, or system administration'].sum() for i in lab ] ASSIGN=[mulcA[i][mulcA.Q5=='Medical or life sciences (biology, chemistry, medicine, etc.)'].sum() for i in lab ] ASSIGN=[mulcA[i][mulcA.Q5=='Social sciences (anthropology, psychology, sociology, etc.)'].sum() for i in lab ] ASSIGN = plt.subplots(4, 2, sharey=True,figsize=(15,20)) axes[0, 0].barh(lab,ASSIGN,color='c') axes[0, 0].set_title('Comp. Science') axes[0, 1].barh(lab,ASSIGN,color='r') axes[0, 1].set_title('Engeneering') axes[1, 0].barh(lab,ASSIGN,color='k') axes[1, 0].set_title('Mathematics') axes[1, 1].barh(lab,ASSIGN) axes[1, 1].set_title('Inf. Technology') axes[2, 0].barh(lab,ASSIGN,color = 'tomato') axes[2, 0].set_title('Business') axes[2, 1].barh(lab,ASSIGN,color='b') axes[2, 1].set_title('Physics') axes[3, 0].barh(lab,ASSIGN,color='y') axes[3, 0].set_title('Medical') axes[3, 1].barh(lab,ASSIGN,color='g') axes[3, 1].set_title('Soc. Science')",0,not_existent,"com_sci_lengs=[mulcA[i][mulcA.Q5=='Computer science (software engineering, etc.)'].sum() for i in lab ] eng_nco_lengs=[mulcA[i][mulcA.Q5=='Engineering (non-computer focused)'].sum() for i in lab ] mat_sta_lengs=[mulcA[i][mulcA.Q5=='Mathematics or statistics'].sum() for i in lab ] biu_dis_lengs=[mulcA[i][mulcA.Q5=='A business discipline (accounting, economics, finance, etc.)'].sum() for i in lab ] phy_ast_lengs=[mulcA[i][mulcA.Q5=='Physics or astronomy'].sum() for i in lab ] inf_tec_lengs=[mulcA[i][mulcA.Q5=='Information technology, networking, or system administration'].sum() for i in lab ] med_sci_lengs=[mulcA[i][mulcA.Q5=='Medical or life sciences (biology, chemistry, medicine, etc.)'].sum() for i in lab ] soc_sci_lengs=[mulcA[i][mulcA.Q5=='Social sciences (anthropology, psychology, sociology, etc.)'].sum() for i in lab ]  fig, axes = plt.subplots(4, 2,  sharey=True,figsize=(15,20)) axes[0, 0].barh(lab,com_sci_lengs,color='c') axes[0, 0].set_title('Comp. Science') axes[0, 1].barh(lab,eng_nco_lengs,color='r') axes[0, 1].set_title('Engeneering') axes[1, 0].barh(lab,mat_sta_lengs,color='k') axes[1, 0].set_title('Mathematics') axes[1, 1].barh(lab,inf_tec_lengs) axes[1, 1].set_title('Inf. Technology') axes[2, 0].barh(lab,biu_dis_lengs,color = 'tomato') axes[2, 0].set_title('Business') axes[2, 1].barh(lab,phy_ast_lengs,color='b') axes[2, 1].set_title('Physics') axes[3, 0].barh(lab,med_sci_lengs,color='y') axes[3, 0].set_title('Medical') axes[3, 1].barh(lab,soc_sci_lengs,color='g') axes[3, 1].set_title('Soc. Science')"
"ASSIGN = mulcA.Q5[mulcA.Q1 == ""Female""] ASSIGN = mulcA.Q5[mulcA.Q1 == ""Male""] ASSIGN = pd.DataFrame([fem_oc.value_counts(normalize=True),mal_oc.value_counts(normalize=True)],index=['Female','Male']).T ASSIGN = df_gen.plot.barh(rot=0, subplots=True,figsize=(15,15))",0,not_existent,"fem_oc = mulcA.Q5[mulcA.Q1 == ""Female""] mal_oc = mulcA.Q5[mulcA.Q1 == ""Male""] df_gen = pd.DataFrame([fem_oc.value_counts(normalize=True),mal_oc.value_counts(normalize=True)],index=['Female','Male']).T axes = df_gen.plot.barh(rot=0, subplots=True,figsize=(15,15))    "
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('path(6).csv', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'owid-covid-data (6).csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,stream,"nRowsRead = 1000 # specify 'None' if want to read whole file
 # owid-covid-data (6).csv may have more rows in reality, but we are only loading/previewing the first 1000 rows
 df1 = pd.read_csv('/kaggle/input/owid-covid-data (6).csv', delimiter=',', nrows = nRowsRead)
 df1.dataframeName = 'owid-covid-data (6).csv'
 nRow, nCol = df1.shape
 print(f'There are {nRow} rows and {nCol} columns')"
CHECKPOINT print(os.listdir('..path')) print(os.listdir('..path')),0,not_existent,print(os.listdir('../input')) print(os.listdir('../input/ida-csv-zip-350-kb-'))
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'IDAData.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,not_existent,"nRowsRead = 1000 # specify 'None' if want to read whole file # IDAData.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows df1 = pd.read_csv('../input/ida-csv-zip-350-kb-/IDAData.csv', delimiter=',', nrows = nRowsRead) df1.dataframeName = 'IDAData.csv' nRow, nCol = df1.shape print(f'There are {nRow} rows and {nCol} columns')"
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'IDACountry.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,not_existent,"nRowsRead = 1000 # specify 'None' if want to read whole file # IDACountry.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows df2 = pd.read_csv('../input/ida-csv-zip-350-kb-/IDACountry.csv', delimiter=',', nrows = nRowsRead) df2.dataframeName = 'IDACountry.csv' nRow, nCol = df2.shape print(f'There are {nRow} rows and {nCol} columns')"
df2.head(5),0,not_existent,df2.head(5)
"plotPerColumnDistribution(df2, 10, 5)",0,not_existent,"plotPerColumnDistribution(df2, 10, 5)"
"CHECKPOINT ASSIGN = 1000 ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead) ASSIGN.dataframeName = 'IDASeries.csv' nRow, nCol = ASSIGN.shape print(f'There are {nRow} rows and {nCol} columns')",0,not_existent,"nRowsRead = 1000 # specify 'None' if want to read whole file # IDASeries.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows df3 = pd.read_csv('../input/ida-csv-zip-350-kb-/IDASeries.csv', delimiter=',', nrows = nRowsRead) df3.dataframeName = 'IDASeries.csv' nRow, nCol = df3.shape print(f'There are {nRow} rows and {nCol} columns')"
df3.head(5),0,not_existent,df3.head(5)
"plotPerColumnDistribution(df3, 10, 5)",0,not_existent,"plotPerColumnDistribution(df3, 10, 5)"
"CHECKPOINT def count_unique_values(df) : ASSIGN = list(df.columns) print('Count unique values :') for i in ASSIGN : ASSIGN = len(df[i].unique()) print(i,':',ASSIGN) def check_missing_values(df) : ASSIGN = len(df) ASSIGN = list(df.columns) ASSIGN = [] ASSIGN = [] print('Variable with missing values :') for i in ASSIGN : ASSIGN = np.sum(df[i].isna()) ASSIGN = round(count*100path, 2) if ASSIGN > 0 : print(i,':',ASSIGN,'path',ASSIGN,'%') ASSIGN.append(i) ASSIGN.append(ASSIGN) return missing_var, missing_count def stepwise_selection(X, y, ASSIGN=[], ASSIGN=0.01, ASSIGN = 0.05, ASSIGN=True): ASSIGN = list(initial_list) while True: ASSIGN=False ASSIGN = list(set(X.columns)-set(included)) ASSIGN = pd.Series(index=excluded) for new_column in ASSIGN: ASSIGN = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included+[new_column]])) ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit() ASSIGN[new_column] = ASSIGN.pvalues[new_column] ASSIGN = new_pval.min() if ASSIGN < ASSIGN: ASSIGN = new_pval.argmin() ASSIGN.append(ASSIGN) ASSIGN=True if ASSIGN: print('Add {:30} with p-value {:.6}'.format(ASSIGN, ASSIGN)) ASSIGN = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included])) ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit() ASSIGN = model.ASSIGN.iloc[1:] ASSIGN = pvalues.max() if ASSIGN > ASSIGN: ASSIGN=True ASSIGN = pvalues.argmax() ASSIGN.remove(ASSIGN) if ASSIGN: print('Drop {:30} with p-value {:.6}'.format(ASSIGN, ASSIGN)) if not ASSIGN: break return included def dataset_ready(x_train, y_train) : ASSIGN = pd.get_dummies(x_train) ASSIGN = [1]*len(X) ASSIGN['gdp_pop'] = np.log(ASSIGN['gdp_per_capita']*ASSIGN['population']) ASSIGN = ['gdp_per_capita','population'] for i in ASSIGN : ASSIGN = np.log(ASSIGN) ASSIGN = pd.Series(X.columns) ASSIGN = list(X.filter(like='continent').columns) for i in ASSIGN : ASSIGN = i+'_gdp' ASSIGN = ASSIGN*ASSIGN for i in ASSIGN : ASSIGN = i+'_population' ASSIGN = ASSIGN*ASSIGN ASSIGN = y_train return X,Y",1,not_existent,"# Function used in this notebook def count_unique_values(df) :     var = list(df.columns)     print('Count unique values :')          for i in var :         count = len(df[i].unique())         print(i,':',count)  def check_missing_values(df) :     n = len(df)     var = list(df.columns)     missing_var = []     missing_count = []     print('Variable with missing values :')          for i in var :         count = np.sum(df[i].isna())         count_percentage = round(count*100/n, 2)         if count > 0 :             print(i,':',count,'//',count_percentage,'%')             missing_var.append(i)             missing_count.append(count_percentage)          return missing_var, missing_count  def stepwise_selection(X, y,                         initial_list=[],                         threshold_in=0.01,                         threshold_out = 0.05,                         verbose=True):       included = list(initial_list)     while True:         changed=False         # forward step         excluded = list(set(X.columns)-set(included))         new_pval = pd.Series(index=excluded)         for new_column in excluded:             model = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))                                 ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()             new_pval[new_column] = model.pvalues[new_column]         best_pval = new_pval.min()         if best_pval < threshold_in:             best_feature = new_pval.argmin()             included.append(best_feature)             changed=True             if verbose:                 print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))          # backward step         model = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included]))                             ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()         # use all coefs except intercept         pvalues = model.pvalues.iloc[1:]         worst_pval = pvalues.max() # null if pvalues is empty         if worst_pval > threshold_out:             changed=True             worst_feature = pvalues.argmax()             included.remove(worst_feature)             if verbose:                 print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))         if not changed:             break     return included  def dataset_ready(x_train, y_train) :     # Make dummy variable for categorical variable     X = pd.get_dummies(x_train)      # Make Intercept     X['Intercept'] = [1]*len(X)      # Make interaction between 'gdp_per_capita' and 'population'     X['gdp_pop'] = np.log(X['gdp_per_capita']*X['population'])      # Scale continuous variable with log function     cont_var = ['gdp_per_capita','population']     for i in cont_var :         X[i] = np.log(X[i])      # Make interaction between 'continent' and 'gdp'     col = pd.Series(X.columns)     var1 = list(X.filter(like='continent').columns)     for i in var1 :         string = i+'_gdp'         X[string] = X[i]*X['gdp_per_capita']         # Make interaction between 'continent' and 'population'     for i in var1 :         string = i+'_population'         X[string] = X[i]*X['population']        # Target variable     Y = y_train          return X,Y  # I use stepwise algorith from this link https://datascience.stackexchange.com/questions/24405/how-to-do-stepwise-regression-using-sklearn"
"SETUP rcParams['figure.figsize'] = [10,5] warnings.filterwarnings('ignore') pd.set_option('display.max_rows', 50) pd.set_option('display.max_columns', 50) sns.set() sns.set_style('whitegrid')",0,not_existent,"# Load and configure notebook settings import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import seaborn as sns import statsmodels.api as sm %matplotlib inline  from matplotlib.pylab import rcParams # For every plotting cell use this rcParams['figure.figsize'] = [10,5]  import warnings warnings.filterwarnings('ignore')  pd.set_option('display.max_rows', 50) pd.set_option('display.max_columns', 50)  sns.set() sns.set_style('whitegrid')"
ASSIGN = pd.read_csv('..path'),0,not_existent,# Load dataset df_train = pd.read_csv('../input/master.csv')
df_train.head(),0,not_existent,# Overview of the dataset df_train.head()
"df_train.rename(columns={' gdp_for_year ($) ':'gdp_for_year', 'gdp_per_capita ($)':'gdp_per_capita'}, inplace=True) ASSIGN = df_train.copy()",1,not_existent,"# Change some variable name df_train.rename(columns={' gdp_for_year ($) ':'gdp_for_year', 'gdp_per_capita ($)':'gdp_per_capita'}, inplace=True) df_train_v2 = df_train.copy()"
df_train.describe(),0,not_existent,# Summary of the dataset df_train.describe()
CHECKPOINT print('Data types of the dataset :') print(df_train.dtypes),0,not_existent,# Overview data types of the dataset print('Data types of the dataset :') print(df_train.dtypes)
"ASSIGN = [] for i in df_train['gdp_for_year'] : ASSIGN = i.ASSIGN(',') ASSIGN = '' for j in ASSIGN : ASSIGN = ASSIGN + j ASSIGN.append(int(ASSIGN)) df_train['gdp_for_year'] = ASSIGN",1,not_existent,"# Change 'gdp_for_year' data type change_var = [] for i in df_train['gdp_for_year'] :     split = i.split(',')     val = ''     for j in split :         val = val + j     change_var.append(int(val))      df_train['gdp_for_year'] = change_var"
"rcParams['figure.figsize'] = [10,5] sns.heatmap(df_train.corr(), annot=True, linewidths=0.2, cmap='coolwarm' ) plt.title('Correlation heatmap of the dataset', size=15, fontweight='bold') ; plt.xticks(rotation=45)",0,not_existent,"# See the correlation between the variables rcParams['figure.figsize'] = [10,5] sns.heatmap(df_train.corr(), annot=True, linewidths=0.2, cmap='coolwarm' ) plt.title('Correlation heatmap of the dataset', size=15, fontweight='bold') ; plt.xticks(rotation=45)"
ASSIGN = check_missing_values(df_train),1,not_existent,"# Check for missing values in the dataset missing_var, missing_count = check_missing_values(df_train)"
"df_train_v2.drop(columns=['country-year','HDI for year','gdp_for_year'], inplace=True)",1,not_existent,"# Remove unwanted variable df_train_v2.drop(columns=['country-year','HDI for year','gdp_for_year'], inplace=True)"
"ASSIGN = ['country','year','sex','age','generation'] count_unique_values(df_train[ASSIGN])",1,not_existent,"# Check how many unique values in categorical variable category_var = ['country','year','sex','age','generation'] count_unique_values(df_train[category_var])"
"rcParams['figure.figsize'] = [15,5] ASSIGN = gridspec.GridSpec(1,2) ASSIGN = plt.subplot(gs[0,0]) ASSIGN = plt.subplot(gs[0,1]) sns.distplot(df_train['suicides_no'], color=' ASSIGN.set_title('Distribution of the suicides_no', size=15, fontweight='bold') ; sns.distplot(np.log(df_train[df_train['suicides_no']>0]['suicides_no']), color=' ASSIGN.set_title('Distribution of the log of population', size=15, fontweight='bold') ;",0,not_existent,"# Check the distribution of target variable 'suicides_no' rcParams['figure.figsize'] = [15,5] gs = gridspec.GridSpec(1,2) ax1 = plt.subplot(gs[0,0]) ax2 = plt.subplot(gs[0,1])  # Plot 1 - Distribution of the target variable sns.distplot(df_train['suicides_no'], color='#7f181b', kde=True, hist=False, ax=ax1) ; ax1.set_title('Distribution of the suicides_no', size=15, fontweight='bold') ;  # Plot 2 - Distribution of the log(target variable) sns.distplot(np.log(df_train[df_train['suicides_no']>0]['suicides_no']), color='#7f181b', kde=True, hist=True, ax=ax2) ; ax2.set_title('Distribution of the log of population', size=15, fontweight='bold') ;"
"ASSIGN = df_train_v2['ASSIGN'].unique() ASSIGN = ['Europe','Central America','South America','Asia','Central America' ,'Australia','Europe','Asia','Central America','Asia' ,'Central America','Europe','Europe','Central America' ,'Europe','South America','Europe','Africa' ,'North America','South America','South America','Central America','Europe','Central America' ,'Asia','Europe','Europe','Central America','South America' ,'Central America','Europe','Oceania','Europe','Europe','Asia' ,'Europe','Europe','Central America','Central America','South America','Europe' ,'Europe','Europe','Asia','Europe','Central America','Asia' ,'Asia','Oceania','Asia','Asia','Europe' ,'Europe','Europe','Asia','Asia','Europe' ,'Africa','North America','Asia','Europe','Europe' ,'Oceania','Central America','Europe','Asia','Central America','South America' ,'Asia','Europe','Europe','Central America','Asia' ,'Asia','Europe','Europe' ,'Central America','Central America' ,'Central America','Europe','Europe' ,'Africa','Asia','Europe','Europe','Africa' ,'Europe','Asia','South America','Europe','Europe' ,'Asia','Central America','Asia','Asia' ,'Europe','Asia','Europe' ,'North America','South America','Asia'] ASSIGN = [] for i in range(len(ASSIGN)) : ASSIGN = len(df_train[df_train['country']==country[i]]) for j in range(ASSIGN) : ASSIGN.append(ASSIGN[i]) df_train_v2['continent'] = ASSIGN",1,not_existent,"# Make new variable 'continent' that represent continent of each country # Based on wikipedia.com country = df_train_v2['country'].unique() new_val = ['Europe','Central America','South America','Asia','Central America'           ,'Australia','Europe','Asia','Central America','Asia'           ,'Central America','Europe','Europe','Central America'           ,'Europe','South America','Europe','Africa'           ,'North America','South America','South America','Central America','Europe','Central America'           ,'Asia','Europe','Europe','Central America','South America'           ,'Central America','Europe','Oceania','Europe','Europe','Asia'           ,'Europe','Europe','Central America','Central America','South America','Europe'           ,'Europe','Europe','Asia','Europe','Central America','Asia'           ,'Asia','Oceania','Asia','Asia','Europe'           ,'Europe','Europe','Asia','Asia','Europe'           ,'Africa','North America','Asia','Europe','Europe'           ,'Oceania','Central America','Europe','Asia','Central America','South America'           ,'Asia','Europe','Europe','Central America','Asia'           ,'Asia','Europe','Europe'           ,'Central America','Central America'           ,'Central America','Europe','Europe'           ,'Africa','Asia','Europe','Europe','Africa'           ,'Europe','Asia','South America','Europe','Europe'           ,'Asia','Central America','Asia','Asia'           ,'Europe','Asia','Europe'           ,'North America','South America','Asia'] new_var = []  for i in range(len(country)) :     n = len(df_train[df_train['country']==country[i]])     for j in range(n) :         new_var.append(new_val[i])          df_train_v2['continent'] = new_var"
"CHECKPOINT ASSIGN = df_train_v2.groupby(by=['country','continent']).median()[['suicides_no','population','gdp_per_capita']].sort_values('suicides_no',ascending=False).reset_index() ASSIGN = list(df_check.head(10)['country']) print('Top 10 country with highest suicide median ') print(ASSIGN.head(10))",1,not_existent,"# Top 10 country with highest suicide median # We use median because the distribution is skewed to the right df_check = df_train_v2.groupby(by=['country','continent']).median()[['suicides_no','population','gdp_per_capita']].sort_values('suicides_no',ascending=False).reset_index() cat = list(df_check.head(10)['country']) print('Top 10 country with highest suicide median ') print(df_check.head(10))"
"ASSIGN = df_train.groupby(by=['country','year']).median()['suicides_no'].reset_index() rcParams['figure.figsize'] = [10,6] ASSIGN = gridspec.GridSpec(2,1) ASSIGN = plt.subplot(gs[0,0]) ASSIGN = plt.subplot(gs[1,0]) for i in cat[:3] : sns.lineplot(data=ASSIGN[ASSIGN['country']==i], x='year', y='suicides_no', ax=ASSIGN) ; ASSIGN.legend(cat[:3], loc=7, bbox_to_anchor=(1.3, 0.5)) ; ASSIGN.set_title('Number of suicide growth of top 3 country', size=15, fontweight='bold') ; for i in cat[3:] : sns.lineplot(data=ASSIGN[ASSIGN['country']==i], x='year', y='suicides_no', ax=ASSIGN) ; ASSIGN.legend(cat[3:], loc=7, bbox_to_anchor=(1.3, 0.5)) ; ASSIGN.set_xlabel('Number of suicide growth of reminding country', size=15, fontweight='bold') ;",0,not_existent,"# Top 10 country number of suicides growth year by year # We use median because the distribution is skewed to the right df_check = df_train.groupby(by=['country','year']).median()['suicides_no'].reset_index() rcParams['figure.figsize'] = [10,6] gs = gridspec.GridSpec(2,1) ax1 = plt.subplot(gs[0,0]) ax2 = plt.subplot(gs[1,0])  # Plot 1 - Line plot for top 3 country for i in cat[:3] :     sns.lineplot(data=df_check[df_check['country']==i], x='year', y='suicides_no', ax=ax1) ;      ax1.legend(cat[:3], loc=7,  bbox_to_anchor=(1.3, 0.5)) ; ax1.set_title('Number of suicide growth of top 3 country', size=15, fontweight='bold') ;  # Plot 2 - Line plot for reminding country for i in cat[3:] :     sns.lineplot(data=df_check[df_check['country']==i], x='year', y='suicides_no', ax=ax2) ;      ax2.legend(cat[3:], loc=7,  bbox_to_anchor=(1.3, 0.5)) ; ax2.set_xlabel('Number of suicide growth of reminding country', size=15, fontweight='bold') ;"
"CHECKPOINT ASSIGN = list(df_train_v2['ASSIGN'].unique()) ASSIGN = pd.Series(ASSIGN) ASSIGN = [] print('Count contry in each ASSIGN recorded in the dataset :') for i in ASSIGN : ASSIGN = len(new_val[new_val==i]) ASSIGN.append(ASSIGN) print(i,':',ASSIGN) ASSIGN = pd.DataFrame({'continent':continent, 'count':count}) ASSIGN.sort_values(by='ASSIGN', ascending=False, inplace=True) sns.catplot(data=ASSIGN, x='ASSIGN', y='ASSIGN') ; plt.xticks(rotation=45) plt.title('How many country in each ASSIGN', size=15, fontweight='bold') ;",0,not_existent,"# Count how many country in each continent recorded in the dataset continent = list(df_train_v2['continent'].unique()) new_val = pd.Series(new_val) count = []  print('Count contry in each continent recorded in the dataset :') for i in continent :     n = len(new_val[new_val==i])     count.append(n)     print(i,':',n)   #  Plot df = pd.DataFrame({'continent':continent, 'count':count}) df.sort_values(by='count', ascending=False, inplace=True) sns.catplot(data=df, x='continent', y='count') ; plt.xticks(rotation=45) plt.title('How many country in each continent', size=15, fontweight='bold') ;"
"ASSIGN = df_train_v2.groupby(by=['continent','year']).median()['suicides_no'].reset_index() rcParams['figure.figsize'] = [10,6] ASSIGN = gridspec.GridSpec(2,1) ASSIGN = plt.subplot(gs[0,0]) ASSIGN = plt.subplot(gs[1,0]) sns.lineplot(data=ASSIGN[ASSIGN['continent']=='North America'], x='year', y='suicides_no', ax=ASSIGN ) ; ASSIGN.legend(['North America'], loc=7, bbox_to_anchor=(1.3, 0.5)) ; ASSIGN.set_title('Number of suicide growth of North America', size=15, fontweight='bold') ; ASSIGN = pd.Series(ASSIGN) for i in ASSIGN[ASSIGN!='North America'] : sns.lineplot(data=ASSIGN[ASSIGN['ASSIGN']==i], x='year', y='suicides_no', ax=ASSIGN) ; ASSIGN.legend(ASSIGN[ASSIGN!='North America'], loc=7, bbox_to_anchor=(1.3, 0.5)) ; ASSIGN.set_xlabel('Number of suicide growth of reminding ASSIGN', size=15, fontweight='bold') ;",0,not_existent,"# Number of suicides growth in each continent df_check = df_train_v2.groupby(by=['continent','year']).median()['suicides_no'].reset_index() rcParams['figure.figsize'] = [10,6] gs = gridspec.GridSpec(2,1) ax1 = plt.subplot(gs[0,0]) ax2 = plt.subplot(gs[1,0])  # Plot 1 - Line plot for North America sns.lineplot(data=df_check[df_check['continent']=='North America'], x='year', y='suicides_no', ax=ax1 ) ; ax1.legend(['North America'], loc=7,  bbox_to_anchor=(1.3, 0.5)) ; ax1.set_title('Number of suicide growth of North America', size=15, fontweight='bold') ;  # Plot 2 - Line plot for reminding continent continent = pd.Series(continent) for i in continent[continent!='North America'] :     sns.lineplot(data=df_check[df_check['continent']==i], x='year', y='suicides_no', ax=ax2) ;      ax2.legend(continent[continent!='North America'], loc=7,  bbox_to_anchor=(1.3, 0.5)) ; ax2.set_xlabel('Number of suicide growth of reminding continent', size=15, fontweight='bold') ;"
"rcParams['figure.figsize'] = [15,5] ASSIGN = gridspec.GridSpec(1,2) ASSIGN = plt.subplot(gs[0,0]) ASSIGN = plt.subplot(gs[0,1]) sns.barplot(data=df_train_v2, x='sex', y='suicides_no', hue='age', ax=ASSIGN ,hue_order=['5-14 years','15-24 years','25-34 years','35-54 years','55-74 years','75+ years']) ; ASSIGN.set_title('Disrtibution of suicide count by sex', size=15, fontweight='bold') ; sns.barplot(data=df_train_v2, x='sex', y='suicidespath', hue='age', ax=ASSIGN ,hue_order=['5-14 years','15-24 years','25-34 years','35-54 years','55-74 years','75+ years']) ; ASSIGN.set_title('Disrtibution of suicide count (rescale) by sex ', size=15, fontweight='bold') ;",0,not_existent,"# Check the effect of gender and sex rcParams['figure.figsize'] = [15,5] gs = gridspec.GridSpec(1,2) ax1 = plt.subplot(gs[0,0]) ax2 = plt.subplot(gs[0,1])  # Plot 1 - based on sns.barplot(data=df_train_v2, x='sex', y='suicides_no', hue='age', ax=ax1            ,hue_order=['5-14 years','15-24 years','25-34 years','35-54 years','55-74 years','75+ years']) ; ax1.set_title('Disrtibution of suicide count by sex', size=15, fontweight='bold') ;  # Plot 2 sns.barplot(data=df_train_v2, x='sex', y='suicides/100k pop', hue='age', ax=ax2            ,hue_order=['5-14 years','15-24 years','25-34 years','35-54 years','55-74 years','75+ years']) ; ax2.set_title('Disrtibution of suicide count (rescale) by sex ', size=15, fontweight='bold') ;"
"rcParams['figure.figsize'] = [16,5] ASSIGN = gridspec.GridSpec(1,3, width_ratios=[2,8,6]) ASSIGN = plt.subplot(gs[0,0]) ASSIGN = plt.subplot(gs[0,1]) ASSIGN = plt.subplot(gs[0,2]) sns.barplot(data=df_train_v2[df_train_v2['continent']=='North America'], x='continent', y='suicides_no', ax=ASSIGN ,hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z'], hue='generation',) ; ASSIGN.get_legend().remove() ASSIGN.set_title('(NA)', size=15, fontweight='bold') ; sns.barplot(data=df_train_v2[df_train_v2['continent'].isin(['Europe','South America','Asia','Australia'])] , x='continent', y='suicides_no' ,hue='generation', ax=ax2 ,hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z']) ; ASSIGN.set_title('Suicide count by generation (E,SA,AS,AUS)', size=15, fontweight='bold') ; sns.barplot(data=df_train_v2[df_train_v2['continent'].isin(['Central America','Oceania','Africa'])] , x='continent', y='suicides_no' ,hue='generation', ax=ax3 ,hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z']) ; ASSIGN.get_legend().remove() ASSIGN.set_title('(CA,O,AF)', size=15, fontweight='bold') ;",0,not_existent,"# Check the effect of variable 'generation' rcParams['figure.figsize'] = [16,5] gs = gridspec.GridSpec(1,3, width_ratios=[2,8,6]) ax1 = plt.subplot(gs[0,0]) ax2 = plt.subplot(gs[0,1]) ax3 = plt.subplot(gs[0,2])  # Plot 1 - North America perspective sns.barplot(data=df_train_v2[df_train_v2['continent']=='North America'], x='continent', y='suicides_no', ax=ax1            ,hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z'], hue='generation',) ; ax1.get_legend().remove() ax1.set_title('(NA)', size=15, fontweight='bold') ;  # Plot 2 - Europe, South America, Asia, Australia perspective sns.barplot(data=df_train_v2[df_train_v2['continent'].isin(['Europe','South America','Asia','Australia'])]             , x='continent', y='suicides_no'             ,hue='generation', ax=ax2            ,hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z']) ; ax2.set_title('Suicide count by generation (E,SA,AS,AUS)', size=15, fontweight='bold') ;  # Plot 3 - Central America, Oceania, Africa perspective sns.barplot(data=df_train_v2[df_train_v2['continent'].isin(['Central America','Oceania','Africa'])]             , x='continent', y='suicides_no'             ,hue='generation', ax=ax3            ,hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z']) ; ax3.get_legend().remove() ax3.set_title('(CA,O,AF)', size=15, fontweight='bold') ;"
"SETUP ASSIGN = pd.Series(df_train_v2.columns) ASSIGN = df_train_v2[df_train_v2>0].dropna() ASSIGN = dummy[~dummy.isin(['country','suicides_no','suicidespath'])] ASSIGN = 'suicidespath' ASSIGN = train_test_split(df_train_v3[x], df_train_v3[y], test_size=0.2, random_state=11)",1,not_existent,"# Split train and validation set from sklearn.model_selection import train_test_split dummy = pd.Series(df_train_v2.columns) df_train_v3 = df_train_v2[df_train_v2>0].dropna()  x = dummy[~dummy.isin(['country','suicides_no','suicides/100k pop'])] y = 'suicides/100k pop'                        x_train, x_valid, y_train, y_valid = train_test_split(df_train_v3[x], df_train_v3[y], test_size=0.2, random_state=11)"
"X, Y = dataset_ready(x_train, y_train) X2, Y2 = dataset_ready(x_valid, y_valid) ASSIGN = stepwise_selection(X,Y, list(X.columns))",1,not_existent,"# Preparing dataset X, Y = dataset_ready(x_train, y_train) X2, Y2 = dataset_ready(x_valid, y_valid)  best_var = stepwise_selection(X,Y, list(X.columns))"
"SETUP CHECKPOINT ASSIGN = sm.genmod.GLM(endog=Y, exog=X[best_var] ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)) ASSIGN = GLM_gamma.fit() print(ASSIGN.summary()) print('Model AIC :',ASSIGN.aic) print('Model BIC :',ASSIGN.bic) print('Model deviance :',ASSIGN.deviance) print('Model RMSE :',rmse(ASSIGN.predict(X2[best_var]),Y2))",0,not_existent,"# Generalize Linear Model - Gamma distribution from statsmodels.tools.eval_measures import rmse import statsmodels.api as sm GLM_gamma = sm.genmod.GLM(endog=Y, exog=X[best_var]                             ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)) GLM_result = GLM_gamma.fit() print(GLM_result.summary()) print('Model AIC :',GLM_result.aic) print('Model BIC :',GLM_result.bic) print('Model deviance :',GLM_result.deviance) print('Model RMSE :',rmse(GLM_result.predict(X2[best_var]),Y2))"
"CHECKPOINT ASSIGN = pd.Series(df_train_v3.columns) ASSIGN = var[~var.isin(['country','suicides_no','suicidespath'])] ASSIGN = 'suicidespath' ASSIGN = df_train_v3[x] ASSIGN = df_train_v3[y] ASSIGN = len(X3) ASSIGN = len(Y3) ASSIGN.loc[ASSIGN+1] = [2016, 'male', '25-34 years', 21845000, 57588, 'Millenials', 'North America'] ASSIGN.loc[ASSIGN+2] = [2016, 'female', '25-34 years', 21917000, 57588, 'Millenials', 'North America'] ASSIGN.loc[ASSIGN+3] = [2016, 'male', '35-54 years', 40539000, 57588, 'Generation X', 'North America'] ASSIGN.loc[ASSIGN+4] = [2016, 'female', '35-54 years', 42031000, 57588, 'Generation X', 'North America'] ASSIGN.loc[ASSIGN+5] = [2016, 'male', '15-24 years', 21719000, 57588, 'Millenials', 'North America'] ASSIGN.loc[ASSIGN+6] = [2016, 'female', '15-24 years', 21169000, 57588, 'Millenials', 'North America'] ASSIGN.loc[ASSIGN+1] = 26.95 ASSIGN.loc[ASSIGN+2] = 6.75 ASSIGN.loc[ASSIGN+3] = 28.35 ASSIGN.loc[ASSIGN+4] = 9.46 ASSIGN.loc[ASSIGN+5] = 21.06 ASSIGN.loc[ASSIGN+6] = 5.42 ASSIGN,ASSIGN = dataset_ready(ASSIGN, ASSIGN) ASSIGN = ASSIGN.loc[nx+1:nx+6] ASSIGN = ASSIGN.loc[ny+1:nx+6] ASSIGN = GLM_result.ASSIGN(X3[best_var]) for i in range(len(ASSIGN)) : print('Option',i+1) print('Predicted suicide rates :',round(ASSIGN.iloc[i],2)) print('Actual suicide rates :',ASSIGN.iloc[i]) print('')",1,not_existent,"# Preparing prediction var = pd.Series(df_train_v3.columns) x = var[~var.isin(['country','suicides_no','suicides/100k pop'])] y = 'suicides/100k pop'  X3 = df_train_v3[x] Y3 = df_train_v3[y]  # Input data for prediction nx = len(X3) ny = len(Y3) X3.loc[nx+1] = [2016, 'male', '25-34 years', 21845000, 57588, 'Millenials', 'North America'] X3.loc[nx+2] = [2016, 'female', '25-34 years', 21917000, 57588, 'Millenials', 'North America'] X3.loc[nx+3] = [2016, 'male', '35-54 years', 40539000, 57588, 'Generation X', 'North America'] X3.loc[nx+4] = [2016, 'female', '35-54 years', 42031000, 57588, 'Generation X', 'North America'] X3.loc[nx+5] = [2016, 'male', '15-24 years', 21719000, 57588, 'Millenials', 'North America'] X3.loc[nx+6] = [2016, 'female', '15-24 years', 21169000, 57588, 'Millenials', 'North America'] Y3.loc[ny+1] = 26.95 Y3.loc[ny+2] = 6.75 Y3.loc[ny+3] = 28.35 Y3.loc[ny+4] = 9.46 Y3.loc[ny+5] = 21.06 Y3.loc[ny+6] = 5.42  # Tranform the data to be ready for precition X3,Y3 = dataset_ready(X3, Y3) X3 = X3.loc[nx+1:nx+6] Y3 = Y3.loc[ny+1:nx+6]  # Predict predict = GLM_result.predict(X3[best_var]) for i in range(len(predict)) :     print('Option',i+1)     print('Predicted suicide rates :',round(predict.iloc[i],2))     print('Actual suicide rates :',Y3.iloc[i])     print('')"
SETUP ASSIGN='path',0,not_existent,"import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import seaborn as sns import os import scipy.stats as stats from sklearn.model_selection import train_test_split  from sklearn.linear_model import LogisticRegression as LR import sklearn.metrics as m  input_folder='/kaggle/input/titanic/'"
ASSIGN = pd.read_csv(input_folder+'train.csv') ASSIGN = pd.read_csv(input_folder+'test.csv'),0,not_existent,train_data = pd.read_csv(input_folder+'train.csv') test_data = pd.read_csv(input_folder+'test.csv')
"CHECKPOINT print(train_data.isnull().sum()*100path().count()) sns.heatmap(train_data.isnull(), ASSIGN=True, ASSIGN=False, ASSIGN=False, ASSIGN='plasma')",0,stream,"print(train_data.isnull().sum()*100/train_data.isnull().count()) # About 20% of age is missing, about 77% of cabin is missing, 0.22% of embarked is missing  sns.heatmap(train_data.isnull(),             annot=True,             yticklabels=False,             cbar=False,             cmap='plasma')"
"train_data['Sex'] = train_data['Sex'].astype('category') train_data['Sex'] = train_data['Sex'].cat.codes sns.heatmap(train_data.corr(), ASSIGN=True, ASSIGN=False)",0,execute_result,"# We need to change sex to a numerical value, otherwise the correlation plot will not include sex train_data['Sex'] = train_data['Sex'].astype('category') train_data['Sex'] = train_data['Sex'].cat.codes   sns.heatmap(train_data.corr(),            annot=True,            cbar=False)"
"ASSIGN = train_data.filter(['Survived','Pclass','Sex','Age','Fare']) ASSIGN.dropna(how='any',axis='rows', inplace=True) ASSIGN = test_data.filter(['Survived','Pclass','Sex','Age','Fare','PassengerId']) ASSIGN.dropna(how='any',axis='rows', inplace=True)",1,not_existent,"train_filtered = train_data.filter(['Survived','Pclass','Sex','Age','Fare']) train_filtered.dropna(how='any',axis='rows', inplace=True)  test_filtered = test_data.filter(['Survived','Pclass','Sex','Age','Fare','PassengerId']) test_filtered.dropna(how='any',axis='rows', inplace=True)"
train_filtered['Sex'] = train_filtered['Sex'].astype('category') train_filtered['Sex'] = train_filtered['Sex'].cat.codes test_filtered['Sex'] = test_filtered['Sex'].astype('category') test_filtered['Sex'] = test_filtered['Sex'].cat.codes,1,not_existent,train_filtered['Sex'] = train_filtered['Sex'].astype('category') train_filtered['Sex'] = train_filtered['Sex'].cat.codes  test_filtered['Sex'] = test_filtered['Sex'].astype('category') test_filtered['Sex'] = test_filtered['Sex'].cat.codes
"ASSIGN = sns.jointplot(train_filtered.groupby(['Pclass']).sum().index, train_filtered.groupby(['Pclass']).sum()['Survived'] ) ASSIGN = sns.jointplot(train_filtered.groupby(['Sex']).sum().index, train_filtered.groupby(['Sex']).sum()['Survived'] ) ASSIGN = sns.jointplot(train_filtered.groupby(['Age']).sum().index, train_filtered.groupby(['Age']).sum()['Survived'] ) ASSIGN = sns.jointplot(train_filtered.groupby(['Fare']).sum().index, train_filtered.groupby(['Fare']).sum()['Survived'] )",0,not_existent,"# Survival by pclass g = sns.jointplot(train_filtered.groupby(['Pclass']).sum().index,                   train_filtered.groupby(['Pclass']).sum()['Survived']     )  # Survival by sex g = sns.jointplot(train_filtered.groupby(['Sex']).sum().index,                   train_filtered.groupby(['Sex']).sum()['Survived']     )  # Survival by age g = sns.jointplot(train_filtered.groupby(['Age']).sum().index,                   train_filtered.groupby(['Age']).sum()['Survived']     )  # Survival by fare g = sns.jointplot(train_filtered.groupby(['Fare']).sum().index,                   train_filtered.groupby(['Fare']).sum()['Survived']     )"
"CHECKPOINT X_train, X_test, y_train, y_test = train_test_split( train_filtered.drop('Survived', axis=1), train_filtered['Survived'], test_size=0.33) ASSIGN = LR().fit(y=y_train,X=X_train) ASSIGN = model.predict(X_test) ASSIGN = pd.DataFrame(data=y_test.tolist(),columns=['Survived actual']) ASSIGN['Survived predicted'] = ASSIGN ASSIGN=m.confusion_matrix(res['Survived actual'],res['Survived predicted']) sns.heatmap(ASSIGN,annot=True,fmt='d',cbar=0).set_title('Confusion Matrix') print('Accuracy: '+str(m.accuracy_score(ASSIGN['Survived actual'],ASSIGN['Survived predicted']))) print('Precision: '+str(m.precision_score(ASSIGN['Survived actual'],ASSIGN['Survived predicted']))) print('Recall: '+str(m.recall_score(ASSIGN['Survived actual'],ASSIGN['Survived predicted'])))",1,not_existent,"# Split data into test and train sets based on the train data set X_train, X_test, y_train, y_test = train_test_split(     train_filtered.drop('Survived', axis=1), train_filtered['Survived'], test_size=0.33)  # Train model model = LR().fit(y=y_train,X=X_train)  # Predict results results = model.predict(X_test)  # Add results to a data frame res = pd.DataFrame(data=y_test.tolist(),columns=['Survived actual']) res['Survived predicted'] = results  # Confusion matrix confmatrix=m.confusion_matrix(res['Survived actual'],res['Survived predicted']) sns.heatmap(confmatrix,annot=True,fmt='d',cbar=0).set_title('Confusion Matrix') #   True negatives (tn)     True positives (tp) #   False negatives (fn)    False positives (fp)  print('Accuracy: '+str(m.accuracy_score(res['Survived actual'],res['Survived predicted']))) # percent of accurate classification print('Precision: '+str(m.precision_score(res['Survived actual'],res['Survived predicted']))) # tp / (tp + fp), 0 is worst, 1 is best print('Recall: '+str(m.recall_score(res['Survived actual'],res['Survived predicted']))) # tp / (tp + fn), 0 is worst, 1 is best"
"CHECKPOINT ASSIGN = LR().fit(y=train_filtered['Survived'],X=train_filtered.drop('Survived', axis=1)) ASSIGN = model.predict(test_filtered.drop('PassengerId', axis=1)) ASSIGN = pd.DataFrame(data=results.tolist(), columns = ['Survived']) ASSIGN = ASSIGN.astype(int) print(ASSIGN)",1,not_existent,"# Train model model = LR().fit(y=train_filtered['Survived'],X=train_filtered.drop('Survived', axis=1))  # Predict survival for the test set results = model.predict(test_filtered.drop('PassengerId', axis=1))  res = pd.DataFrame(data=results.tolist(), columns = ['Survived']) res['PassengerId'] = test_filtered['PassengerId'].astype(int)  print(res)"
"res.to_csv('results.csv', index=False)",0,not_existent,"res.to_csv('results.csv', index=False)"
"CHECKPOINT ASSIGN = pd.read_csv(""..path"", index_col='Id') train",0,not_existent,"train = pd.read_csv(""../input/dataset/train_data.csv"", index_col='Id') train"
(train == '?').any(),0,not_existent,(train == '?').any()
train[train == '?'].count(),0,not_existent,train[train == '?'].count()
train['workclass'].value_counts().plot(kind = 'bar'),0,not_existent,train['workclass'].value_counts().plot(kind = 'bar')
train['occupation'].value_counts().plot(kind = 'bar'),0,not_existent,train['occupation'].value_counts().plot(kind = 'bar')
train['native.country'].value_counts().plot(kind = 'bar'),0,not_existent,train['native.country'].value_counts().plot(kind = 'bar')
"ASSIGN = ASSIGN.replace(to_replace = '?', value = 'Private')",1,not_existent,"train['workclass'] = train['workclass'].replace(to_replace = '?', value = 'Private')"
"train['native.country'] = train['native.country'].replace(to_replace = '?', value = 'United-States')",1,not_existent,"train['native.country'] = train['native.country'].replace(to_replace = '?', value = 'United-States')"
ASSIGN = ASSIGN.loc[ASSIGN.occupation != '?'],1,not_existent,train = train.loc[train.occupation != '?']
"ASSIGN = train.loc[:,'age':'native.country'] ASSIGN = train.income ASSIGN.head()",1,not_existent,"Xtrain = train.loc[:,'age':'native.country'] Ytrain = train.income Xtrain.head()"
Ytrain.head(),0,not_existent,Ytrain.head()
SETUP,0,not_existent,from sklearn import preprocessing
CHECKPOINT ASSIGN = pd.get_dummies(ASSIGN) Xtrain,1,not_existent,Xtrain = pd.get_dummies(Xtrain) Xtrain
"ASSIGN = KNeighborsClassifier(n_neighbors = 10) ASSIGN = cross_val_score(knn, Xtrain, Ytrain, cv=10)",0,not_existent,"knn = KNeighborsClassifier(n_neighbors = 10) scores = cross_val_score(knn, Xtrain, Ytrain, cv=10)"
"ASSIGN = [] for i in range(1,25): ASSIGN = KNeighborsClassifier(n_neighbors = i) ASSIGN = cross_val_score(knn, Xtrain, Ytrain, cv=10) ASSIGN.append(ASSIGN.mean())",0,not_existent,"scores_array = [] for i in range(1,25):     knn = KNeighborsClassifier(n_neighbors = i)     scores = cross_val_score(knn, Xtrain, Ytrain, cv=10)     scores_array.append(scores.mean())      "
"plt.plot(scores_array, 'ro')",0,not_existent,"plt.plot(scores_array, 'ro')"
"CHECKPOINT ASSIGN = pd.read_csv('..path', na_values='?', index_col='Id') test",0,not_existent,"test = pd.read_csv('../input/dataset/test_data.csv', na_values='?', index_col='Id') test"
ASSIGN = pd.get_dummies(test) ASSIGN.head(),1,not_existent,Xtest = pd.get_dummies(test) Xtest.head()
ASSIGN = set( Xtrain.columns ) - set( Xtest.columns ) for c in ASSIGN: ASSIGN = 0 ASSIGN = ASSIGN[Xtrain.columns],1,not_existent,##garantindo que ambas tenham a mesma dimenso missing_cols = set( Xtrain.columns ) - set( Xtest.columns ) for c in missing_cols:     Xtest[c] = 0 Xtest = Xtest[Xtrain.columns]
"ASSIGN = ASSIGN = KNeighborsClassifier(n_neighbors = 21) ASSIGN.fit(Xtrain,Ytrain)",0,not_existent,"knn = knn = KNeighborsClassifier(n_neighbors = 21) knn.fit(Xtrain,Ytrain)"
CHECKPOINT YtestPred = knn.predict(Xtest) YtestPred,0,not_existent,YtestPred = knn.predict(Xtest) YtestPred
ASSIGN = pd.DataFrame(index = test.index) ASSIGN = YtestPred,1,not_existent,prediction = pd.DataFrame(index = test.index) prediction['income'] = YtestPred
CHECKPOINT prediction,0,not_existent,prediction
"prediction.to_csv(""submition.csv"")",0,not_existent,"prediction.to_csv(""submition.csv"")"
SETUP SETUP,0,stream,"!pip install autokeras
 !pip install natsort"
SETUP,0,stream,"import autokeras as ak
 import numpy as np 
 import pandas as pd 
 from glob import glob
 from skimage.io import imread
 import skimage.io as sio
 import os
 from natsort import natsorted
 import matplotlib.pyplot as plt
 from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score
 from skimage.transform import resize, rotate
 import warnings; warnings.filterwarnings(""ignore"")"
"ASSIGN = ""..path"" ASSIGN = root_dir + ""path"" ASSIGN = root_dir + ""path"" ASSIGN = root_dir + ""path"" ASSIGN = root_dir + ""sample_submission.csv"" ASSIGN  = pd.read_csv(csv_path) ASSIGN  = np.array([ imread(train_dir+p)path]) ASSIGN  = df.has_cactus.values",1,not_existent,"# train test directories
 root_dir = ""../input""
 train_dir = root_dir + ""/train/train/""
 test_dir  = root_dir + ""/test/test/""
 csv_path  = root_dir + ""/train.csv""
 sub_path  = root_dir + ""sample_submission.csv""
 
 # loading images
 df   = pd.read_csv(csv_path)
 x    = np.array([ imread(train_dir+p)/255 for p in df.id.values])
 y    = df.has_cactus.values"
"SETUP ASSIGN = train_test_split(x, y, test_size=0.20,stratify=y)",1,not_existent,"# splitting training dataset into train/validation
 from sklearn.model_selection import train_test_split
 x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.20,stratify=y)"
"def display_images(imgs,y=None, y_pred=None): ASSIGN = imgs.shape[0] ASSIGN = 5 ASSIGN = n_imagespath ASSIGN = 1 plt.figure(figsize=(10,6),frameon=False) for i in range(ASSIGN): for j in range(ASSIGN): plt.subplot(ASSIGN, ASSIGN, ASSIGN) plt.imshow(imgs[ASSIGN-1]) plt.axis(""off"") if (y is not None) and (y_pred is not None): plt.title(""y=%d | pred=%0.1f""%(y[ASSIGN-1],y_pred[ASSIGN-1])) elif y is not None: plt.title(""y=%d""%y[ASSIGN-1]) ASSIGN+=1 plt.tight_layout() plt.show() def getProb(model, x): ASSIGN = model.preprocess(x) ASSIGN = model.data_transformer.transform_test(xprocessed) ASSIGN = model.cnn.predict(loader) ASSIGN  = np.exp(probs[:,1]) ASSIGN = num + np.exp(probs[:,0]) ASSIGN = num path return probs",0,not_existent,"# helper functions here
 def display_images(imgs,y=None, y_pred=None):
     n_images = imgs.shape[0]
     n_gridx  = 5
     n_gridy  = n_images//n_gridx
 #     n_grid   = int(np.sqrt(n_images))
     k = 1
     plt.figure(figsize=(10,6),frameon=False)
     for i in range(n_gridy):
         for j in range(n_gridx):
             plt.subplot(n_gridy, n_gridx, k)
             plt.imshow(imgs[k-1])
             plt.axis(""off"")
             if (y is not None) and (y_pred is not None):
                 plt.title(""y=%d | pred=%0.1f""%(y[k-1],y_pred[k-1]))
             elif y is not None:
                 plt.title(""y=%d""%y[k-1])
             k+=1
     plt.tight_layout()
     plt.show()
 
 
 def getProb(model, x):
     xprocessed = model.preprocess(x)
     loader = model.data_transformer.transform_test(xprocessed)
     probs  = model.cnn.predict(loader)
     num    = np.exp(probs[:,1])
     denom  = num + np.exp(probs[:,0])
     probs  = num / denom 
     return probs"
"ASSIGN = 20 ASSIGN = np.random.randint(0,len(x_train),n_samples) display_images(x_train[ASSIGN], y_train[ASSIGN])",1,display_data,"n_samples  = 20
 idx_sample = np.random.randint(0,len(x_train),n_samples)
 display_images(x_train[idx_sample], y_train[idx_sample])"
"ASSIGN = 5 ASSIGN = ak.ImageClassifier(verbose=True, augment=True ) ASSIGN.fit(x_train, y_train, time_limit=4*60*60)",0,stream,"runFor = 5 # time in hours
 model = ak.ImageClassifier(verbose=True, augment=True )
 model.fit(x_train, y_train, time_limit=4*60*60)"
"CHECKPOINT ASSIGN = model.predict(x_train) ASSIGN = getProb(model, x_train) print(, accuracy_score(y_train, ASSIGN)) print(, recall_score(y_train, ASSIGN)) print(, precision_score(y_train, ASSIGN)) print(, roc_auc_score(y_train, ASSIGN)) print(, f1_score(y_train, ASSIGN)) ASSIGN = model.predict(x_val) ASSIGN = getProb(model, x_val) print(, accuracy_score(y_val, ASSIGN)) print(, recall_score(y_val, ASSIGN)) print(, precision_score(y_val, ASSIGN)) print(,roc_auc_score(y_val, ASSIGN)) print(, f1_score(y_val, ASSIGN))",0,stream,"# model.final_fit(x_train, y_train, x_val, y_val, retrain=False)
 y_pred = model.predict(x_train)
 y_prob = getProb(model, x_train)
 print(""training   accuracy  = "", accuracy_score(y_train, y_pred))
 print(""training   recall    = "", recall_score(y_train, y_pred))
 print(""training   precision = "", precision_score(y_train, y_pred))
 print(""training   auc score = "", roc_auc_score(y_train, y_prob))
 print(""training   f1 score  = "", f1_score(y_train, y_pred))
 y_pred = model.predict(x_val)
 y_prob = getProb(model, x_val)
 print(""validation accuracy  = "", accuracy_score(y_val, y_pred))
 print(""validation recall    = "", recall_score(y_val, y_pred))
 print(""validation precision = "", precision_score(y_val, y_pred))
 print(""validation auc score = "",roc_auc_score(y_val, y_prob))
 print(""validation f1 score  = "", f1_score(y_val, y_pred))"
"ASSIGN = pd.read_csv('..path') ASSIGN = np.array([ imread(test_dir+p)path]) ASSIGN = np.array(ASSIGN) ASSIGN = getProb(model, x_test) ASSIGN['has_cactus'] = ASSIGN ASSIGN.to_csv('cactus_net_submission.csv', index=False)",1,not_existent,"df_test = pd.read_csv('../input/sample_submission.csv')
 x_test  = np.array([ imread(test_dir+p)/255 for p in df_test.id.values])
 x_test  = np.array(x_test)
 
 # test prediction
 y_prob_test = getProb(model, x_test)
 
 df_test['has_cactus'] = y_prob_test
 df_test.to_csv('cactus_net_submission.csv', index=False)"
"CHECKPOINT ASSIGN = pd.read_csv(""..path"") print(ASSIGN.isnull().sum()) ASSIGN.dropna(inplace=True) ASSIGN = ASSIGN[ASSIGN['country']!='World']",1,stream,"df = pd.read_csv(""../input/ecological-footprint/EcologicalFootPrint.csv"")
 print(df.isnull().sum())
 df.dropna(inplace=True)
 df = df[df['country']!='World']"
df['record'].value_counts(),0,execute_result,df['record'].value_counts()
"ASSIGN = ['crop_land','grazing_land','forest_land','fishing_ground','built_up_land','carbon','total'] for columns in ASSIGN: plt.figure(figsize=(15,10)) ASSIGN = df.groupby('year')[columns].mean() sns.barplot(ASSIGN.index,ASSIGN.values).set_xticklabels(sns.barplot(ASSIGN.index,ASSIGN.values).get_xticklabels(),rotation=""90"") plt.title(""Year Comparation with ""+columns) plt.xlabel(columns) plt.ylabel(""Count"")",0,display_data,"l = ['crop_land','grazing_land','forest_land','fishing_ground','built_up_land','carbon','total']
 for columns in l:
     plt.figure(figsize=(15,10))
     every_year = df.groupby('year')[columns].mean()
     sns.barplot(every_year.index,every_year.values).set_xticklabels(sns.barplot(every_year.index,every_year.values).get_xticklabels(),rotation=""90"")
     plt.title(""Year Comparation with ""+columns)
     plt.xlabel(columns)
     plt.ylabel(""Count"")"
"plt.figure(figsize=(10,10)) plt.title(""Total 10, most Carbon Producing Countring"") plt.xlabel(""Country"") plt.ylabel(""Rank"") df.groupby(['country']).mean()['carbon'].sort_values(ascending=False)[:10].plot.bar()",0,execute_result,"plt.figure(figsize=(10,10))
 plt.title(""Total 10, most Carbon Producing Countring"")
 plt.xlabel(""Country"")
 plt.ylabel(""Rank"")
 df.groupby(['country']).mean()['carbon'].sort_values(ascending=False)[:10].plot.bar()"
"plt.figure(figsize=(10,10)) plt.title(""Total 10, lowest Carbon Producing Countring"") plt.xlabel(""Country"") plt.ylabel(""Rank"") df.groupby(['country']).mean()['carbon'].sort_values(ascending=True)[:10].plot.bar()",0,execute_result,"plt.figure(figsize=(10,10))
 plt.title(""Total 10, lowest Carbon Producing Countring"")
 plt.xlabel(""Country"")
 plt.ylabel(""Rank"")
 df.groupby(['country']).mean()['carbon'].sort_values(ascending=True)[:10].plot.bar()"
"plt.figure(figsize=(10,10)) plt.title(""Total 10, Lowest Country with Forst Land"") plt.xlabel(""Country"") plt.ylabel(""Rank"") df.groupby(['country']).mean()['forest_land'].sort_values(ascending=True)[:10].plot.bar()",0,execute_result,"plt.figure(figsize=(10,10))
 plt.title(""Total 10, Lowest Country with Forst Land"")
 plt.xlabel(""Country"")
 plt.ylabel(""Rank"")
 df.groupby(['country']).mean()['forest_land'].sort_values(ascending=True)[:10].plot.bar()"
"plt.figure(figsize=(10,10)) plt.title(""Total 10, Highest Country with Forst Land"") plt.xlabel(""Country"") plt.ylabel(""Rank"") df.groupby(['country']).mean()['forest_land'].sort_values(ascending=False)[:10].plot.bar()",0,execute_result,"plt.figure(figsize=(10,10))
 plt.title(""Total 10, Highest Country with Forst Land"")
 plt.xlabel(""Country"")
 plt.ylabel(""Rank"")
 df.groupby(['country']).mean()['forest_land'].sort_values(ascending=False)[:10].plot.bar()"
"plt.figure(figsize=(10,10)) plt.title(""Total 10, Highest Country with Build Up Land"") plt.xlabel(""Country"") plt.ylabel(""Rank"") df.groupby(['country']).mean()['built_up_land'].sort_values(ascending=False)[:10].plot.bar()",0,execute_result,"plt.figure(figsize=(10,10))
 plt.title(""Total 10, Highest Country with Build Up Land"")
 plt.xlabel(""Country"")
 plt.ylabel(""Rank"")
 df.groupby(['country']).mean()['built_up_land'].sort_values(ascending=False)[:10].plot.bar()"
"plt.figure(figsize=(10,10)) plt.title(""Total 10, Lowest Country with Build Up Land"") plt.xlabel(""Country"") plt.ylabel(""Rank"") df.groupby(['country']).mean()['built_up_land'].sort_values(ascending=True)[:10].plot.bar()",0,execute_result,"plt.figure(figsize=(10,10))
 plt.title(""Total 10, Lowest Country with Build Up Land"")
 plt.xlabel(""Country"")
 plt.ylabel(""Rank"")
 df.groupby(['country']).mean()['built_up_land'].sort_values(ascending=True)[:10].plot.bar()"
SETUP,0,stream,!pip install imageio-ffmpeg
"SETUP warnings.filterwarnings(""ignore"")",0,not_existent,"import imageio
 import numpy as np
 import matplotlib.pyplot as plt
 import matplotlib.animation as animation
 from skimage.transform import resize
 from IPython.display import HTML
 import warnings
 warnings.filterwarnings(""ignore"")
"
"def display(driving ): ASSIGN = plt.figure(figsize=(10, 6)) ASSIGN = [] for i in range(len(driving)): ASSIGN = plt.imshow(driving[i], animated=True) plt.axis('off') ASSIGN.append([ASSIGN]) ASSIGN = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000) plt.close() return ani",0,not_existent,"
  
 def display(driving ):
     fig = plt.figure(figsize=(10, 6))
 
     ims = []
     for i in range(len(driving)):
         im = plt.imshow(driving[i], animated=True)
         plt.axis('off')
         ims.append([im])
 
     ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)
     plt.close()
     return ani
     
"
"ASSIGN = imageio.get_reader('..path') ASSIGN = reader.get_meta_data()['ASSIGN'] ASSIGN = [] try: for im in ASSIGN: ASSIGN.append(im) except RuntimeError: pass ASSIGN.close() ASSIGN = [resize(frame, (256, 256))[..., :3] for frame in ASSIGN] HTML(display(ASSIGN).to_html5_video())",1,execute_result,"reader = imageio.get_reader('../input/digitsinnoise-video/Test.mp4')
 fps = reader.get_meta_data()['fps']
 driving_video = []
 try:
     for im in reader:
         driving_video.append(im)
 except RuntimeError:
     pass
 reader.close()
 
 driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]
 
 
 
 HTML(display(driving_video).to_html5_video())"
"SETUP CHECKPOINT sys.path.append('..path') binder.bind(globals()) print() ASSIGN = pd.read_csv('..path', nrows=50000) ASSIGN = ASSIGN.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' + 'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' + 'pickup_longitude > -74 and pickup_longitude < -73.9 and ' + 'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' + 'fare_amount > 0' ) ASSIGN = data.fare_amount ASSIGN = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude'] ASSIGN = data[base_features] train_X, val_X, train_y, val_y = train_test_split(ASSIGN, ASSIGN, random_state=1) ASSIGN = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y) print() ASSIGN.head()",1,not_existent,"import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split  # Environment Set-Up for feedback system. import sys sys.path.append('../input/ml-insights-tools') from learntools.core import binder binder.bind(globals()) from ex3 import * print(""Setup Complete"")  # Data manipulation code below here data = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows=50000)  # Remove data with extreme outlier coordinates or negative fares data = data.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +                   'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +                   'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +                   'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +                   'fare_amount > 0'                   )  y = data.fare_amount  base_features = ['pickup_longitude',                  'pickup_latitude',                  'dropoff_longitude',                  'dropoff_latitude']  X = data[base_features]   train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) first_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y) print(""Data sample:"") data.head()"
data.describe(),0,not_existent,data.describe()
"SETUP ASSIGN = 'pickup_longitude' ASSIGN = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name) pdp.pdp_plot(ASSIGN, ASSIGN) plt.show()",0,not_existent,"from matplotlib import pyplot as plt from pdpbox import pdp, get_dataset, info_plots  feat_name = 'pickup_longitude' pdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)  pdp.pdp_plot(pdp_dist, feat_name) plt.show()"
"for feat_name in base_features: ASSIGN = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name) pdp.pdp_plot(ASSIGN, feat_name) plt.show()",0,not_existent,"for feat_name in base_features:     pdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)     pdp.pdp_plot(pdp_dist, feat_name)     plt.show()"
"ASSIGN = ['pickup_longitude', 'dropoff_longitude'] ASSIGN = pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats) pdp.pdp_interact_plot(pdp_interact_out=ASSIGN, feature_names=ASSIGN, plot_type='contour') plt.show()",0,not_existent,"# Add your code here feats = ['pickup_longitude', 'dropoff_longitude'] inter1  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)  pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=feats, plot_type='contour') plt.show()"
"ASSIGN = ['pickup_latitude', 'dropoff_latitude'] ASSIGN = pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats) pdp.pdp_interact_plot(pdp_interact_out=ASSIGN, feature_names=ASSIGN, plot_type='contour') plt.show()",0,not_existent,"feats = ['pickup_latitude', 'dropoff_latitude'] inter1  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)  pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=feats, plot_type='contour') plt.show()"
"ASSIGN = ['pickup_latitude', 'dropoff_longitude'] ASSIGN = ['pickup_longitude', 'dropoff_longitude'] ASSIGN = pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats) pdp.pdp_interact_plot(pdp_interact_out=ASSIGN, feature_names=ASSIGN, plot_type='contour') plt.show()",0,not_existent,"feats = ['pickup_latitude', 'dropoff_longitude'] feats = ['pickup_longitude', 'dropoff_longitude'] inter1  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)  pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=feats, plot_type='contour') plt.show()"
"(np.random.rand(10) < .5).astype(float) np.random.choice([1, -1], 10)",1,not_existent,"(np.random.rand(10) < .5).astype(float) np.random.choice([1, -1], 10)"
pip install face_recognition,0,stream,pip install face_recognition
SETUP,0,not_existent,"import os
 import cv2
 import face_recognition"
SETUP ASSIGN = Image.open('path') display(ASSIGN),0,display_data,"from PIL import Image, ImageDraw
 from IPython.display import display
 
 # Sample Image
 virat_img = Image.open('/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/unknown_faces/Kohli-Williamson.jpg')
 display(virat_img)"
SETUP,0,not_existent,"KNOWN_FACES_DIR = '/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/known_faces'
 UNKNOWN_FACES_DIR = '/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/unknown_faces'
 TOLERANCE = 0.6"
CHECKPOINT print('Loading known faces...') ASSIGN = [] ASSIGN = [],0,stream,"print('Loading known faces...')
 known_faces = []
 known_names = []"
for name in os.listdir(KNOWN_FACES_DIR): for filename in os.listdir(f'{KNOWN_FACES_DIR}path{name}'): ASSIGN = face_recognition.load_image_file(f'{KNOWN_FACES_DIR}path{name}path{filename}') ASSIGN = face_recognition.face_encodings(image)[0] known_faces.append(ASSIGN) known_names.append(name),1,not_existent,"for name in os.listdir(KNOWN_FACES_DIR):
 
     # Next we load every file of faces of known person
     for filename in os.listdir(f'{KNOWN_FACES_DIR}/{name}'):
 
         # Load an image
         image = face_recognition.load_image_file(f'{KNOWN_FACES_DIR}/{name}/{filename}')
 
         # Get 128-dimension face encoding
         # Always returns a list of found faces, for this purpose we take first face only (assuming one face per image as you can't be twice on one image)
         encoding = face_recognition.face_encodings(image)[0]
 
         # Append encodings and name
         known_faces.append(encoding)
         known_names.append(name)"
CHECKPOINT print(known_names),0,stream,print(known_names)
"CHECKPOINT for filename in os.listdir(UNKNOWN_FACES_DIR): print(f'Filename {filename}', end='') ASSIGN = face_recognition.load_image_file(f'{UNKNOWN_FACES_DIR}path{filename}') ASSIGN = face_recognition.ASSIGN(unknown_image) ASSIGN = face_recognition.ASSIGN(unknown_image, face_locations) ASSIGN = Image.fromarray(unknown_image) ASSIGN = ImageDraw.Draw(pil_image) for (top, right, bottom, left), face_encoding in zip(ASSIGN, ASSIGN): ASSIGN = face_recognition.compare_faces(known_faces, face_encoding,TOLERANCE) ASSIGN = ""Unknown"" ASSIGN = face_recognition.face_distance(known_faces, face_encoding) ASSIGN = np.argmin(face_distances) if ASSIGN[ASSIGN]: ASSIGN = known_names[best_match_index] ASSIGN.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255)) ASSIGN = draw.textsize(name) ASSIGN.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255)) ASSIGN.text((left + 6, bottom - text_height - 5), ASSIGN, fill=(255, 255, 255, 255)) del draw display(ASSIGN)",0,stream,"for filename in os.listdir(UNKNOWN_FACES_DIR):
 
     # Load image
     print(f'Filename {filename}', end='')
     unknown_image = face_recognition.load_image_file(f'{UNKNOWN_FACES_DIR}/{filename}')
     # Load an image with an unknown face
     #unknown_image = face_recognition.load_image_file(""/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/known_faces/Virat_Kohli/gettyimages-463104486-2048x2048.jpg"")
 
     # Find all the faces and face encodings in the unknown image
     face_locations = face_recognition.face_locations(unknown_image)
     face_encodings = face_recognition.face_encodings(unknown_image, face_locations)
 
     # Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library
     # See http://pillow.readthedocs.io/ for more about PIL/Pillow
     pil_image = Image.fromarray(unknown_image)
     # Create a Pillow ImageDraw Draw instance to draw with
     draw = ImageDraw.Draw(pil_image)
 
     # Loop through each face found in the unknown image
     for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):
         # See if the face is a match for the known face(s)
         matches = face_recognition.compare_faces(known_faces, face_encoding,TOLERANCE)
 
         name = ""Unknown""
 
         # Or instead, use the known face with the smallest distance to the new face
         face_distances = face_recognition.face_distance(known_faces, face_encoding)
         best_match_index = np.argmin(face_distances)
         if matches[best_match_index]:
             name = known_names[best_match_index]
 
         # Draw a box around the face using the Pillow module
         draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))
 
         # Draw a label with a name below the face
         text_width, text_height = draw.textsize(name)
         draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))
         draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))
 
     # Remove the drawing library from memory as per the Pillow docs
     del draw
 
     # Display the resulting image
     display(pil_image)"
