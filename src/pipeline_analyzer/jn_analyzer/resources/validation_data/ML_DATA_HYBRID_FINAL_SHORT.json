{
    "source": [
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd \n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow.keras as keras\n",
                "import seaborn as sns\n",
                "from keras.utils import to_categorical\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Dense,BatchNormalization\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import train_test_split"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "df = pd.read_csv('/kaggle/input/league-of-legends-diamond-ranked-games-10-min/high_diamond_ranked_10min.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.head(5)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "dataset = df.iloc[:,1:40]\n",
                "dataset.head(5)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "corr = dataset.corr('pearson')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(data = dataset,x='blueWins')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "dataset.isnull().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "blue_df = dataset.iloc[:,0:19]\n",
                "blue_df.drop(columns = ['blueDeaths'],inplace = True)\n",
                "blue_df.head(5)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "corr = blue_df.corr('pearson')\n",
                "plt.figure(figsize = (10,10))\n",
                "sns.heatmap(corr,annot = True)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "corr['blueWins'].sort_values(ascending=False)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X = blue_df.iloc[:,1:]\n",
                "y = blue_df.iloc[:,1]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "scaler = StandardScaler()\n",
                "scaler.fit(X)\n",
                "X = pd.DataFrame(scaler.transform(X),columns=X.columns)\n",
                "X.head(5)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "y = blue_df['blueWins']\n",
                "y = to_categorical(y, 2)\n",
                "y"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.20,random_state=0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "model = Sequential()\n",
                "model.add(Dense(units=18,activation='relu',input_dim=len(X.columns)))\n",
                "model.add(Dense(36,activation = 'relu'))\n",
                "model.add(Dense(72,activation = 'relu'))\n",
                "model.add(Dense(units=2,activation='softmax'))\n",
                "model.summary()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "history = model.fit(X_train,y_train,\n",
                "                   epochs=50,\n",
                "                   validation_data=(X_test,y_test))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(8,8))\n",
                "plt.plot(history.history['val_accuracy'])\n",
                "#plt.legend(['Training Accuracy','Validation Accuracy'])\n",
                "plt.title('Accuracy curves')\n",
                "plt.xlabel('epochs')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import xgboost as xgb\n",
                "import matplotlib.pyplot as plt"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "df_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\n",
                "df_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\n",
                "df_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])\n",
                "\n",
                "df_train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df_train['price_doc'].hist(bins=50)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "y_train = df_train['price_doc'].values\n",
                "id_test = df_test['id']\n",
                "\n",
                "df_train.drop(['id', 'price_doc'], axis=1, inplace=True)\n",
                "df_test.drop(['id'], axis=1, inplace=True)\n",
                "\n",
                "# Build df_all = (df_train+df_test).join(df_macro)\n",
                "num_train = len(df_train)\n",
                "df_all = pd.concat([df_train, df_test])\n",
                "df_all = pd.merge_ordered(df_all, df_macro, on='timestamp', how='left')\n",
                "print(df_all.shape)\n",
                "\n",
                "# Add month-year\n",
                "month_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)\n",
                "month_year_cnt_map = month_year.value_counts().to_dict()\n",
                "df_all['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
                "\n",
                "# Add week-year count\n",
                "week_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)\n",
                "week_year_cnt_map = week_year.value_counts().to_dict()\n",
                "df_all['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
                "\n",
                "# Add month and day-of-week\n",
                "df_all['month'] = df_all.timestamp.dt.month\n",
                "df_all['dow'] = df_all.timestamp.dt.dayofweek\n",
                "\n",
                "# Other feature engineering\n",
                "df_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\n",
                "df_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n",
                "\n",
                "# Remove timestamp column (may overfit the model in train)\n",
                "df_all.drop(['timestamp'], axis=1, inplace=True)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Deal with categorical values\n",
                "df_numeric = df_all.select_dtypes(exclude=['object'])\n",
                "df_obj = df_all.select_dtypes(include=['object']).copy()\n",
                "\n",
                "for c in df_obj:\n",
                "    df_obj[c] = pd.factorize(df_obj[c])[0]\n",
                "\n",
                "df_values = pd.concat([df_numeric, df_obj], axis=1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Convert to numpy values\n",
                "X_all = df_values.values\n",
                "print(X_all.shape)\n",
                "\n",
                "X_train = X_all[:num_train]\n",
                "X_test = X_all[num_train:]\n",
                "\n",
                "df_columns = df_values.columns"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "xgb_params = {\n",
                "    'eta': 0.05,\n",
                "    'max_depth': 5,\n",
                "    'subsample': 0.7,\n",
                "    'colsample_bytree': 0.7,\n",
                "    'objective': 'reg:linear',\n",
                "    'eval_metric': 'rmse',\n",
                "    'silent': 1\n",
                "}\n",
                "\n",
                "dtrain = xgb.DMatrix(X_train, y_train, feature_names=df_columns)\n",
                "dtest = xgb.DMatrix(X_test, feature_names=df_columns)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Uncomment to tune XGB `num_boost_rounds`\n",
                "\n",
                "#cv_result = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n",
                "#    verbose_eval=True, show_stdv=False)\n",
                "#cv_result[['train-rmse-mean', 'test-rmse-mean']].plot()\n",
                "#num_boost_rounds = len(cv_result)\n",
                "\n",
                "num_boost_round = 383"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(1, 1, figsize=(8, 16))\n",
                "xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results"
            ],
            "content": [
                "y_pred = model.predict(dtest)\n",
                "\n",
                "df_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n",
                "\n",
                "df_sub.to_csv('sub.csv', index=False)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# import libraries",
                "import torch",
                "import numpy as np"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "content": [
                "from torchvision import datasets",
                "import torchvision.transforms as transforms",
                "from torch.utils.data.sampler import SubsetRandomSampler",
                "",
                "# number of subprocesses to use for data loading",
                "num_workers = 0",
                "# how many samples per batch to load",
                "batch_size = 20",
                "# percentage of training set to use as validation",
                "valid_size = 0.2",
                "",
                "# convert data to torch.FloatTensor",
                "transform = transforms.ToTensor()",
                "",
                "# choose the training and test datasets",
                "train_data = datasets.MNIST(root='data', train=True,",
                "                                   download=True, transform=transform)",
                "test_data = datasets.MNIST(root='data', train=False,",
                "                                  download=True, transform=transform)",
                "",
                "# obtain training indices that will be used for validation",
                "num_train = len(train_data)",
                "indices = list(range(num_train))",
                "np.random.shuffle(indices)",
                "split = int(np.floor(valid_size * num_train))",
                "train_idx, valid_idx = indices[split:], indices[:split]",
                "",
                "# define samplers for obtaining training and validation batches",
                "train_sampler = SubsetRandomSampler(train_idx)",
                "valid_sampler = SubsetRandomSampler(valid_idx)",
                "",
                "# prepare data loaders",
                "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,",
                "    sampler=train_sampler, num_workers=num_workers)",
                "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, ",
                "    sampler=valid_sampler, num_workers=num_workers)",
                "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, ",
                "    num_workers=num_workers)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "import matplotlib.pyplot as plt",
                "%matplotlib inline",
                "    ",
                "# obtain one batch of training images",
                "dataiter = iter(train_loader)",
                "images, labels = dataiter.next()",
                "images = images.numpy()",
                "",
                "# plot the images in the batch, along with the corresponding labels",
                "fig = plt.figure(figsize=(25, 4))",
                "for idx in np.arange(20):",
                "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])",
                "    ax.imshow(np.squeeze(images[idx]), cmap='gray')",
                "    # print out the correct label for each image",
                "    # .item() gets the value contained in a Tensor",
                "    ax.set_title(str(labels[idx].item()))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "img = np.squeeze(images[1])",
                "",
                "fig = plt.figure(figsize = (12,12)) ",
                "ax = fig.add_subplot(111)",
                "ax.imshow(img, cmap='gray')",
                "width, height = img.shape",
                "thresh = img.max()/2.5",
                "for x in range(width):",
                "    for y in range(height):",
                "        val = round(img[x][y],2) if img[x][y] !=0 else 0",
                "        ax.annotate(str(val), xy=(y,x),",
                "                    horizontalalignment='center',",
                "                    verticalalignment='center',",
                "                    color='white' if img[x][y]<thresh else 'black')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "validate_data"
            ],
            "content": [
                "import torch.nn as nn",
                "import torch.nn.functional as F",
                "",
                "# define the NN architecture",
                "class Net(nn.Module):",
                "    def __init__(self):",
                "        super(Net, self).__init__()",
                "        # number of hidden nodes in each layer (512)",
                "        hidden_1 = 512",
                "        hidden_2 = 512",
                "        # linear layer (784 -> hidden_1)",
                "        self.fc1 = nn.Linear(28 * 28, hidden_1)",
                "        # linear layer (n_hidden -> hidden_2)",
                "        self.fc2 = nn.Linear(hidden_1, hidden_2)",
                "        # linear layer (n_hidden -> 10)",
                "        self.fc3 = nn.Linear(hidden_2, 10)",
                "        # dropout layer (p=0.2)",
                "        # dropout prevents overfitting of data",
                "        self.dropout = nn.Dropout(0.2)",
                "",
                "    def forward(self, x):",
                "        # flatten image input",
                "        x = x.view(-1, 28 * 28)",
                "        # add hidden layer, with relu activation function",
                "        x = F.relu(self.fc1(x))",
                "        # add dropout layer",
                "        x = self.dropout(x)",
                "        # add hidden layer, with relu activation function",
                "        x = F.relu(self.fc2(x))",
                "        # add dropout layer",
                "        x = self.dropout(x)",
                "        # add output layer",
                "        x = self.fc3(x)",
                "        return x",
                "",
                "# initialize the NN",
                "model = Net()",
                "print(model)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "# specify loss function (categorical cross-entropy)",
                "criterion = nn.CrossEntropyLoss()",
                "",
                "# specify optimizer (stochastic gradient descent) and learning rate = 0.01",
                "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results",
                "validate_data",
                "train_model"
            ],
            "content": [
                "# number of epochs to train the model",
                "n_epochs = 50",
                "",
                "# initialize tracker for minimum validation loss",
                "valid_loss_min = np.Inf # set initial \"min\" to infinity",
                "",
                "for epoch in range(n_epochs):",
                "    # monitor training loss",
                "    train_loss = 0.0",
                "    valid_loss = 0.0",
                "    ",
                "    ###################",
                "    # train the model #",
                "    ###################",
                "    model.train() # prep model for training",
                "    for data, target in train_loader:",
                "        # clear the gradients of all optimized variables",
                "        optimizer.zero_grad()",
                "        # forward pass: compute predicted outputs by passing inputs to the model",
                "        output = model(data)",
                "        # calculate the loss",
                "        loss = criterion(output, target)",
                "        # backward pass: compute gradient of the loss with respect to model parameters",
                "        loss.backward()",
                "        # perform a single optimization step (parameter update)",
                "        optimizer.step()",
                "        # update running training loss",
                "        train_loss += loss.item()*data.size(0)",
                "        ",
                "    ######################    ",
                "    # validate the model #",
                "    ######################",
                "    model.eval() # prep model for evaluation",
                "    for data, target in valid_loader:",
                "        # forward pass: compute predicted outputs by passing inputs to the model",
                "        output = model(data)",
                "        # calculate the loss",
                "        loss = criterion(output, target)",
                "        # update running validation loss ",
                "        valid_loss += loss.item()*data.size(0)",
                "        ",
                "    # print training/validation statistics ",
                "    # calculate average loss over an epoch",
                "    train_loss = train_loss/len(train_loader.sampler)",
                "    valid_loss = valid_loss/len(valid_loader.sampler)",
                "    ",
                "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(",
                "        epoch+1, ",
                "        train_loss,",
                "        valid_loss",
                "        ))",
                "    ",
                "    # save model if validation loss has decreased",
                "    if valid_loss <= valid_loss_min:",
                "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(",
                "        valid_loss_min,",
                "        valid_loss))",
                "        torch.save(model.state_dict(), 'model.pt')",
                "        valid_loss_min = valid_loss"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "model.load_state_dict(torch.load('model.pt'))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "# initialize lists to monitor test loss and accuracy",
                "test_loss = 0.0",
                "class_correct = list(0. for i in range(10))",
                "class_total = list(0. for i in range(10))",
                "",
                "model.eval() # prep model for evaluation",
                "",
                "for data, target in test_loader:",
                "    # forward pass: compute predicted outputs by passing inputs to the model",
                "    output = model(data)",
                "    # calculate the loss",
                "    loss = criterion(output, target)",
                "    # update test loss ",
                "    test_loss += loss.item()*data.size(0)",
                "    # convert output probabilities to predicted class",
                "    _, pred = torch.max(output, 1)",
                "    # compare predictions to true label",
                "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))",
                "    # calculate test accuracy for each object class",
                "    for i in range(len(target)):",
                "        label = target.data[i]",
                "        class_correct[label] += correct[i].item()",
                "        class_total[label] += 1",
                "",
                "# calculate and print avg test loss",
                "test_loss = test_loss/len(test_loader.sampler)",
                "print('Test Loss: {:.6f}\\n'.format(test_loss))",
                "",
                "for i in range(10):",
                "    if class_total[i] > 0:",
                "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (",
                "            str(i), 100 * class_correct[i] / class_total[i],",
                "            np.sum(class_correct[i]), np.sum(class_total[i])))",
                "    else:",
                "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))",
                "",
                "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (",
                "    100. * np.sum(class_correct) / np.sum(class_total),",
                "    np.sum(class_correct), np.sum(class_total)))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# obtain one batch of test images",
                "dataiter = iter(test_loader)",
                "images, labels = dataiter.next()",
                "",
                "# get sample outputs",
                "output = model(images)",
                "# convert output probabilities to predicted class",
                "_, preds = torch.max(output, 1)",
                "# prep images for display",
                "images = images.numpy()",
                "",
                "# plot the images in the batch, along with predicted and true labels",
                "fig = plt.figure(figsize=(25, 4))",
                "for idx in np.arange(20):",
                "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])",
                "    ax.imshow(np.squeeze(images[idx]), cmap='gray')",
                "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),",
                "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import pandas as pd"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "df = pd.read_csv(\"../input/titanic/titanic.csv\")",
                "df.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "inputs = df.drop('Survived',axis='columns')",
                "target = df.Survived"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "inputs.Age[:10]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "inputs.Age = inputs.Age.fillna(inputs.Age.mean())",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.preprocessing import LabelEncoder",
                "le_Pclass = LabelEncoder()",
                "le_Sex = LabelEncoder()",
                "le_Age = LabelEncoder()",
                "le_Fare=LabelEncoder()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "inputs['Pclass_n'] = le_Pclass.fit_transform(inputs['Pclass'])",
                "inputs['Sex_n'] = le_Sex.fit_transform(inputs['Sex'])",
                "inputs['Age_n'] = le_Age.fit_transform(inputs['Age'])",
                "inputs['Fare_n'] = le_Fare.fit_transform(inputs['Fare'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "inputs"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "inputs_n = inputs.drop(['Age','Sex','Fare','Pclass'],axis='columns')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "inputs_n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "from sklearn import tree",
                "model = tree.DecisionTreeClassifier()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "model.fit(inputs_n,target)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "model.score(inputs_n,target)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "model.predict([[0,30,2,80]])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Import libraries",
                "import numpy as np ",
                "import pandas as pd ",
                "import plotly as py",
                "import plotly.express as px",
                "import plotly.graph_objs as go",
                "from plotly.subplots import make_subplots",
                "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot",
                "init_notebook_mode(connected=True)",
                "# Read Data",
                "df = pd.read_csv(\"/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv\")",
                "df.head()",
                "",
                "# Rename columns",
                "df = df.rename(columns={'Country/Region':'Country'})",
                "df = df.rename(columns={'ObservationDate':'Date'})",
                "# Manipulate Dataframe",
                "df_countries = df.groupby(['Country', 'Date']).sum().reset_index().sort_values('Date', ascending=False)",
                "df_countries = df_countries.drop_duplicates(subset = ['Country'])",
                "df_countries = df_countries[df_countries['Confirmed']>0]",
                "df_countrydate = df[df['Confirmed']>0]",
                "df_countrydate = df_countrydate.groupby(['Date','Country']).sum().reset_index()",
                "df_countrydate",
                "# Creating the visualization",
                "fig = px.choropleth(df_countrydate, ",
                "                    locations=\"Country\", ",
                "                    locationmode = \"country names\",",
                "                    color=\"Confirmed\", ",
                "                    hover_name=\"Country\", ",
                "                    animation_frame=\"Date\"",
                "                   )",
                "fig.update_layout(",
                "    title_text = 'Global Spread of Coronavirus',",
                "    title_x = 0.5,",
                "    geo=dict(",
                "        showframe = False,",
                "        showcoastlines = False,",
                "    ))",
                "    ",
                "fig.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "",
                "# Input data files are available in the read-only \"../input/\" directory",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                "",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" ",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as pl\n",
                "from matplotlib import cm as cm\n",
                "import plotly.plotly as py\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train=pd.read_csv(\"../input/train.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "pd.options.display.max_rows = 999\n",
                "pd.options.display.max_columns=999\n",
                "train.describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.count()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.columns"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.dtypes"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.corr()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig = pl.figure()\n",
                "ax1 = fig.add_subplot(111)\n",
                "cmap = cm.get_cmap('jet', 80)\n",
                "cax = ax1.imshow(train.corr(), interpolation=\"nearest\", cmap=cmap)\n",
                "ax1.grid(True)\n",
                "pl.title('Abalone Feature Correlation')\n",
                "#labels=['Id',\t'MSSubClass',\t'LotFrontage',\t'LotArea',\t'OverallQual',\t'OverallCond',\t'YearBuilt',\t'YearRemodAdd',\t'MasVnrArea',\t'BsmtFinSF1',\t'BsmtFinSF2',\t'BsmtUnfSF',\t'TotalBsmtSF',\t'1stFlrSF',\t'2ndFlrSF',\t'LowQualFinSF',\t'GrLivArea',\t'BsmtFullBath',\t'BsmtHalfBath',\t'FullBath',\t'HalfBath',\t'BedroomAbvGr',\t'KitchenAbvGr',\t'TotRmsAbvGrd',\t'Fireplaces',\t'GarageYrBlt',\t'GarageCars',\t'GarageArea',\t'WoodDeckSF',\t'OpenPorchSF',\t'EnclosedPorch',\t'3SsnPorch',\t'ScreenPorch',\t'PoolArea',\t'MiscVal',\t'MoSold',\t'YrSold',\t'SalePrice',]\n",
                "#ax1.set_xticklabels(labels,fontsize=5)\n",
                "#ax1.set_yticklabels(labels,fontsize=5)\n",
                "# Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
                "fig.colorbar(cax, ticks=[.75,.8,.85,.90,.95,1])\n",
                "pl.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "\n",
                "pl.plot(train[\"YearBuilt\"], train[\"GarageYrBlt\"], \"o\")"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "pl.plot(train[\"TotalBsmtSF\"], train[\"1stFlrSF\"], \"o\")"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "\n",
                "pl.plot(train[\"GarageCars\"], train[\"GarageArea\"], \"o\")"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "visualize_data",
                "validate_data"
            ],
            "content": [
                "pl.plot(train[\"SalePrice\"], train[\"MiscVal\"], \"o\")\n",
                "traincor=train[[\"SalePrice\",\"MiscVal\"]]\n",
                "traincor.corr()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Alley2=train['Alley'].fillna('NoAlley')\n",
                "train[\"Alley2\"]=Alley2"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.count()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train = pd.read_csv('/kaggle/input/integer-sequence-learning/train.csv.zip')\n",
                "test = pd.read_csv('/kaggle/input/integer-sequence-learning/test.csv.zip')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "val = train.Sequence.values[0].split(',')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "val"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "val_shaped = np.reshape(np.array(val),(7, 2))\n",
                "val_shaped\n",
                "X = [int(i[0]) for i in val_shaped]\n",
                "y = [int(i[1]) for i in val_shaped]\n",
                "X_shaped = np.reshape(np.array(X),(7,1))\n",
                "y_shaped = np.reshape(np.array(y),(7,1))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_shaped"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y_shaped"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "from sklearn.preprocessing import PolynomialFeatures \n",
                "from sklearn.linear_model import LinearRegression\n",
                "  \n",
                "poly = PolynomialFeatures(len(X))\n",
                "X_poly = poly.fit_transform(X_shaped)\n",
                "  \n",
                "# fit the transformed features to Linear Regression\n",
                "poly_model = LinearRegression()\n",
                "poly_model.fit(X_poly, y_shaped)\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "predicted = poly_model.predict(X_poly)\n",
                "predicted"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y_shaped"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "predicted"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y_shaped[6][0] -predicted[6][0]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "print(os.listdir(\"../input\"))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "Rest = pd.read_csv('../input/zomato.csv', encoding='latin-1')\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Rest.head()\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Rest.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "Rest.to_csv(\"Rest.csv\")\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "import pandas as pd",
                "from sklearn.ensemble import RandomForestRegressor",
                "from sklearn.linear_model import LinearRegression",
                "from sklearn.model_selection import train_test_split",
                "",
                "# Environment Set-Up for feedback system.",
                "import sys",
                "sys.path.append('../input/ml-insights-tools')",
                "from learntools.core import binder",
                "binder.bind(globals())",
                "from ex3 import *",
                "print(\"Setup Complete\")",
                "",
                "# Data manipulation code below here",
                "data = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows=50000)",
                "",
                "# Remove data with extreme outlier coordinates or negative fares",
                "data = data.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +",
                "                  'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +",
                "                  'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +",
                "                  'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +",
                "                  'fare_amount > 0'",
                "                  )",
                "",
                "y = data.fare_amount",
                "",
                "base_features = ['pickup_longitude',",
                "                 'pickup_latitude',",
                "                 'dropoff_longitude',",
                "                 'dropoff_latitude']",
                "",
                "X = data[base_features]",
                "",
                "",
                "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)",
                "first_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y)",
                "print(\"Data sample:\")",
                "data.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "data.describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "from matplotlib import pyplot as plt",
                "from pdpbox import pdp, get_dataset, info_plots",
                "",
                "feat_name = 'pickup_longitude'",
                "pdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)",
                "",
                "pdp.pdp_plot(pdp_dist, feat_name)",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "for feat_name in base_features:",
                "    pdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)",
                "    pdp.pdp_plot(pdp_dist, feat_name)",
                "    plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Add your code here",
                "feats = ['pickup_longitude', 'dropoff_longitude']",
                "inter1  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)",
                "",
                "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=feats, plot_type='contour')",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "feats = ['pickup_latitude', 'dropoff_latitude']",
                "inter1  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)",
                "",
                "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=feats, plot_type='contour')",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "feats = ['pickup_latitude', 'dropoff_longitude']",
                "feats = ['pickup_longitude', 'dropoff_longitude']",
                "inter1  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)",
                "",
                "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=feats, plot_type='contour')",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "(np.random.rand(10) < .5).astype(float)",
                "np.random.choice([1, -1], 10)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# Set up feedback system",
                "from learntools.core import binder",
                "binder.bind(globals())",
                "from learntools.sql_advanced.ex1 import *",
                "print(\"Setup Complete\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "content": [
                "from google.cloud import bigquery",
                "",
                "# Create a \"Client\" object",
                "client = bigquery.Client()",
                "",
                "# Construct a reference to the \"stackoverflow\" dataset",
                "dataset_ref = client.dataset(\"stackoverflow\", project=\"bigquery-public-data\")",
                "",
                "# API request - fetch the dataset",
                "dataset = client.get_dataset(dataset_ref)",
                "",
                "# Construct a reference to the \"posts_questions\" table",
                "table_ref = dataset_ref.table(\"posts_questions\")",
                "",
                "# API request - fetch the table",
                "table = client.get_table(table_ref)",
                "",
                "# Preview the first five lines of the table",
                "client.list_rows(table, max_results=5).to_dataframe()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Construct a reference to the \"posts_answers\" table",
                "table_ref = dataset_ref.table(\"posts_answers\")",
                "",
                "# API request - fetch the table",
                "table = client.get_table(table_ref)",
                "",
                "# Preview the first five lines of the table",
                "client.list_rows(table, max_results=5).to_dataframe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "first_query = \"\"\"",
                "              SELECT q.id AS q_id,",
                "                  MIN(TIMESTAMP_DIFF(a.creation_date, q.creation_date, SECOND)) as time_to_answer",
                "              FROM `bigquery-public-data.stackoverflow.posts_questions` AS q",
                "                  INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a",
                "              ON q.id = a.parent_id",
                "              WHERE q.creation_date >= '2018-01-01' and q.creation_date < '2018-02-01'",
                "              GROUP BY q_id",
                "              ORDER BY time_to_answer",
                "              \"\"\"",
                "",
                "first_result = client.query(first_query).result().to_dataframe()",
                "print(\"Percentage of answered questions: %s%%\" % \\",
                "      (sum(first_result[\"time_to_answer\"].notnull()) / len(first_result) * 100))",
                "print(\"Number of questions:\", len(first_result))",
                "first_result.head()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "!pip install pycaret --quiet"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import numpy as np",
                "import pandas as pd",
                "from sklearn.model_selection import KFold",
                "",
                "#import regression module",
                "from pycaret.regression import *"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "BASE_PATH = '../input/trends-assessment-prediction'",
                "",
                "fnc_df = pd.read_csv(f\"{BASE_PATH}/fnc.csv\")",
                "loading_df = pd.read_csv(f\"{BASE_PATH}/loading.csv\")",
                "labels_df = pd.read_csv(f\"{BASE_PATH}/train_scores.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])",
                "df = fnc_df.merge(loading_df, on=\"Id\")",
                "labels_df[\"is_train\"] = True",
                "df = df.merge(labels_df, on=\"Id\", how=\"left\")",
                "",
                "test_df = df[df[\"is_train\"] != True].copy()",
                "df = df[df[\"is_train\"] == True].copy()",
                "print(f'Shape of train data: {df.shape}, Shape of test data: {test_df.shape}')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "target_cols = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']",
                "df.drop(['is_train'], axis=1, inplace=True)",
                "test_df = test_df.drop(target_cols + ['is_train'], axis=1)",
                "",
                "",
                "# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.",
                "FNC_SCALE = 1/500",
                "df[fnc_features] *= FNC_SCALE",
                "test_df[fnc_features] *= FNC_SCALE"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def get_train_data(target):",
                "    other_targets = [tar for tar in target_cols if tar != target]",
                "    train_df = df.drop( other_targets, axis=1)",
                "    return train_df"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "target = 'age'",
                "",
                "train_df = get_train_data(target)",
                "",
                "setup_reg = setup(",
                "    data = train_df,",
                "    target = target,",
                "    train_size=0.8,",
                "    numeric_imputation = 'mean',",
                "    silent = True",
                ")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "blacklist_models = ['tr']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "compare_models(",
                "    blacklist = blacklist_models,",
                "    fold = 10,",
                "    sort = 'MAE', ## competition metric",
                "    turbo = True",
                ")",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "br_age = create_model(",
                "    estimator='br',",
                "    fold=10",
                ")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "# here we are tuning the above created model",
                "tuned_br_age = tune_model(",
                "    estimator='br',",
                "    fold=10,",
                "    optimize = 'mae',",
                "    n_iter=50",
                ")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# plot_model(estimator = None, plot = residuals)",
                "plot_model(estimator = tuned_br_age, plot = 'learning')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_model(estimator = tuned_br_age, plot = 'residuals')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_model(estimator = tuned_br_age, plot = 'feature')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "evaluate_model(estimator=tuned_br_age)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "predictions =  predict_model(tuned_br_age, data=test_df)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "predictions.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "target = target_cols[0]",
                "train_df = get_train_data(target)",
                "",
                "setup_reg = setup(",
                "    data = train_df,",
                "    target = target,",
                "    train_size=0.8,",
                "    numeric_imputation = 'mean',",
                "    silent = True",
                ")",
                "",
                "compare_models(",
                "    blacklist = blacklist_models,",
                "    fold = 10,",
                "    sort = 'MAE',",
                "    turbo = True",
                ")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "target = target_cols[1]",
                "train_df = get_train_data(target)",
                "",
                "setup_reg = setup(",
                "    data = train_df,",
                "    target = target,",
                "    train_size=0.8,",
                "    numeric_imputation = 'mean',",
                "    silent = True",
                ")",
                "",
                "compare_models(",
                "    blacklist = blacklist_models,",
                "    fold = 10,",
                "    sort = 'MAE',",
                "    turbo = True",
                ")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "target = target_cols[2]",
                "train_df = get_train_data(target)",
                "",
                "setup_reg = setup(",
                "    data = train_df,",
                "    target = target,",
                "    train_size=0.8,",
                "    numeric_imputation = 'mean',",
                "    silent = True",
                ")",
                "",
                "compare_models(",
                "    blacklist = blacklist_models,",
                "    fold = 10,",
                "    sort = 'MAE',",
                "    turbo = True",
                ")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "target = target_cols[3]",
                "train_df = get_train_data(target)",
                "",
                "setup_reg = setup(",
                "    data = train_df,",
                "    target = target,",
                "    train_size=0.8,",
                "    numeric_imputation = 'mean',",
                "    silent = True",
                ")",
                "",
                "compare_models(",
                "    blacklist = blacklist_models,",
                "    fold = 10,",
                "    sort = 'MAE',",
                "    turbo = True",
                ")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "target = target_cols[4]",
                "train_df = get_train_data(target)",
                "",
                "setup_reg = setup(",
                "    data = train_df,",
                "    target = target,",
                "    train_size=0.8,",
                "    numeric_imputation = 'mean',",
                "    silent = True",
                ")",
                "",
                "compare_models(",
                "    blacklist = blacklist_models,",
                "    fold = 10,",
                "    sort = 'MAE',",
                "    turbo = True",
                ")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "# mapping targets to their corresponding models",
                "",
                "models = []",
                "",
                "target_models_dict = {",
                "    'age': 'br',",
                "    'domain1_var1':'catboost',",
                "    'domain1_var2':'svm',",
                "    'domain2_var1':'catboost',",
                "    'domain2_var2':'catboost',",
                "}",
                "",
                "def tune_and_ensemble(target):",
                "    train_df = get_train_data(target)    ",
                "    exp_reg = setup(",
                "        data = train_df,",
                "        target = target,",
                "        train_size=0.8,",
                "        numeric_imputation = 'mean',",
                "        silent = True",
                "    )",
                "    ",
                "    model_name = target_models_dict[target]",
                "    tuned_model = tune_model(model_name, fold=10)",
                "    model = ensemble_model(tuned_model, fold=10)",
                "    return model",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "target = target_cols[0]",
                "model = tune_and_ensemble(target)",
                "models.append(model)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "target = target_cols[1]",
                "model = tune_and_ensemble(target)",
                "models.append(model)",
                "%tb"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "target"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "target = target_cols[2]",
                "model = tune_and_ensemble(target)",
                "models.append(model)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "target = target_cols[3]",
                "model = tune_and_ensemble(target)",
                "models.append(model)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "target = target_cols[4]",
                "model = tune_and_ensemble(target)",
                "models.append(model)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results"
            ],
            "content": [
                "### create a pipeline or function to run for all targets",
                "",
                "def finalize_model_pipeline(model, target):",
                "    # this will train the model on holdout data",
                "    finalize_model(model)",
                "    save_model(model, f'{target}_{target_models_dict[target]}', verbose=True)",
                "    # making predictions on test data",
                "    predictions = predict_model(model, data=test_df)",
                "    test_df[target] = predictions['Label'].values"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for index, target in enumerate(target_cols):",
                "    model = models[index]",
                "    finalize_model_pipeline(model,target)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "sub_df = pd.melt(test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")",
                "sub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")",
                "",
                "sub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")",
                "assert sub_df.shape[0] == test_df.shape[0]*5",
                "",
                "sub_df.to_csv(\"submission1.csv\", index=False)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "sub_df.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "import pandas as pd\n",
                "\n",
                "reviews = pd.read_csv(\"../input/wine-reviews/winemag-data-130k-v2.csv\", index_col=0)\n",
                "pd.set_option(\"display.max_rows\", 5)\n",
                "\n",
                "from learntools.core import binder; binder.bind(globals())\n",
                "from learntools.pandas.indexing_selecting_and_assigning import *\n",
                "print(\"Setup complete.\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "reviews.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "\n",
                "\n",
                "import numpy as np \n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "from collections import Counter\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "data = pd.read_csv(\"/kaggle/input/fish-market/Fish.csv\")\n",
                "data.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "x = data.isnull().sum().sum()\n",
                "data.columns"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "species_list = data.Species\n",
                "Counter(species_list)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "labels = \"Perch\" , \"Bream\" , \"Roach\" , \"Pike\" , \"Smelt\" , \"Parkki\" , \"Whitefish\"\n",
                "sizes = [56,35,20,17,14,11,6]\n",
                "explode = (0,0,0,0,0,0,0)\n",
                "fig1,ax1 = plt.subplots()\n",
                "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\n",
                "ax1.axis('equal')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Weight[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Weight[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Weight[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Weight[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Weight[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Weight[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Weight[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)\n",
                "plt.xlabel(\"index of Spices\")\n",
                "plt.ylabel(\"weight of fish in Gram g\")\n",
                "plt.grid()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Length1[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Length1[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Length1[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Length1[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Length1[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Length1[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Length1[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)\n",
                "plt.xlabel(\"index of Spices\")\n",
                "plt.ylabel(\"vertical length in cm\")\n",
                "plt.grid()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Length2[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Length2[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Length2[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Length2[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Length2[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Length2[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Length2[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)\n",
                "plt.xlabel(\"index of Spices\")\n",
                "plt.ylabel(\"diagonal length in cm\")\n",
                "plt.grid()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Length3[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Length3[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Length3[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Length3[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Length3[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Length3[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Length3[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)\n",
                "plt.xlabel(\"index of Spices\")\n",
                "plt.ylabel(\"cross length in cm\")\n",
                "plt.grid()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Height[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Height[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Height[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Height[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Height[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Height[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Height[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)\n",
                "plt.xlabel(\"index of Spices\")\n",
                "plt.ylabel(\"height in cm\")\n",
                "plt.grid()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Width[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Width[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Width[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Width[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Width[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Width[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Width[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)\n",
                "plt.xlabel(\"index of Spices\")\n",
                "plt.ylabel(\"diagonal width in cm\")\n",
                "plt.grid()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "y = data.Species\n",
                "x = data.drop([\"Species\"],axis = 1)\n",
                "from sklearn.model_selection import train_test_split\n",
                "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "visualize_data"
            ],
            "content": [
                "knn_score = 0.0\n",
                "highest_indx = 1\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "score_list = []\n",
                "for each in range(1,100):\n",
                "    knn = KNeighborsClassifier(n_neighbors = each)\n",
                "    knn.fit(x_train,y_train)\n",
                "    score_list.append(knn.score(x_test,y_test))\n",
                "    if (knn_score < knn.score(x_test,y_test) ):\n",
                "        knn_score = knn.score(x_test,y_test)\n",
                "        highest_indx = highest_indx+1\n",
                "        \n",
                "plt.plot(score_list,color = \"purple\" , alpha = 1 )    \n",
                "plt.grid()         "
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(\"KNN Max Accuracy : \",knn_score)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.linear_model import LogisticRegression\n",
                "lr = LogisticRegression()\n",
                "lr.fit(x_train,y_train)\n",
                "lr_score = lr.score(x_test,y_test)\n",
                "print(\"Logistic Regression Accuracy : \",lr_score)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.naive_bayes import GaussianNB\n",
                "naive_bayes = GaussianNB()\n",
                "naive_bayes.fit(x_train,y_train)\n",
                "nb_score = naive_bayes.score(x_test,y_test)\n",
                "print(\"Naive Bayes Accuracy : \",nb_score)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "rfc = RandomForestClassifier(n_estimators = 100)\n",
                "rfc.fit(x_train,y_train)\n",
                "rf_score = rfc.score(x_test,y_test)\n",
                "print(\"Random Forest Accuracy : \",rf_score)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "dict1 = {\"Logistic Regression\" : lr_score,\"Random Forest\" : rf_score,\"K-Nearest Neighbour\" : knn_score ,\"Naive Bayes\": nb_score ,\"K-Nearest Neighbour\" : knn_score }\n",
                "dict1"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "#essential libraries\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import random\n",
                "from urllib.request import urlopen\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sb\n",
                "import plotly.express as ex\n",
                "import plotly.graph_objs as gp\n",
                "from plotly.subplots import make_subplots\n",
                "import plotly.figure_factory as ff\n",
                "import folium\n",
                "\n",
                "#Colour codes\n",
                "conf = '#393e46' \n",
                "deth = '#ff2e63'  \n",
                "cure = '#21bf73'\n",
                "acti = '#fe9801'\n",
                "\n",
                "#Extra Libraries\n",
                "from pandas.plotting import register_matplotlib_converters\n",
                "register_matplotlib_converters()   \n",
                "\n",
                "#To remove the warnings\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "df_india = pd.read_csv('/kaggle/input/covid19-in-india/covid_19_india.csv',parse_dates=['Date'], dayfirst=True)\n",
                "df_coordinates = pd.read_csv('/kaggle/input/coronavirus-cases-in-india/Indian Coordinates.csv')\n",
                "df_India_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='India', parse_dates=['Date'])\n",
                "df_Italy_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='Italy', parse_dates=['Date'])\n",
                "df_Korea_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='Korea', parse_dates=['Date'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_india.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_coordinates.dropna(axis = 1, inplace = True)\n",
                "df_coordinates.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_coordinates.rename(columns = {'Name of State / UT':'State/UnionTerritory'}, inplace = True)\n",
                "df_coordinates.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_india.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_india.isnull().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_india.dropna(axis = 0, inplace = True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_india.isnull().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_india[\"State/UnionTerritory\"].replace({'Chattisgarh': 'Chhattisgarh ',\n",
                "                                          'Chhattisgarh' :'Chhattisgarh ',\n",
                "                                          'Puducherry' : 'Pondicherry',\n",
                "                                          'Himachal Pradesh' : 'Himachal Pradesh ',\n",
                "                                          'Madhya Pradesh' : 'Madhya Pradesh ',\n",
                "                                          'Bihar':'Bihar ',\n",
                "                                          'Himachal Pradesh':'Himachal Pradesh ',\n",
                "                                          'Manipur':'Manipur ',\n",
                "                                          'West Bengal':'West Bengal ',\n",
                "                                          'Goa' : 'Goa '}, inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_india = pd.merge(df_india, df_coordinates, how='left', on='State/UnionTerritory')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_india.isnull().sum()\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_india[['Latitude','Longitude']] = df_india[['Latitude','Longitude']].fillna(0)\n",
                "df_india.isnull().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_india = df_india.drop('Sno', axis = 1) \n",
                "df_india.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_india.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "df_india.to_csv('Processed_data.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Rearranging the columns\n",
                "df_india = df_india[['Date', 'State/UnionTerritory', 'Latitude', 'Longitude','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths']]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Total_cases = ['ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']\n",
                "\n",
                "#Active = Confirmed - Deaths - Cured\n",
                "df_india['Active'] = (df_india['ConfirmedIndianNational'] + df_india['ConfirmedForeignNational']) - df_india['Deaths'] - df_india['Cured']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_india.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Fill Null Values with Zeros\n",
                "df_india[Total_cases] = df_india[Total_cases].fillna(0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# cases in Maharashtra\n",
                "maha = df_india[df_india['State/UnionTerritory'].str.contains('Maharashtra')]\n",
                "maha.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# cases in Kerala\n",
                "kerala = df_india[df_india['State/UnionTerritory'].str.contains('Kerala')]\n",
                "kerala.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_india_latest = df_india.groupby('Date')['ConfirmedIndianNational','ConfirmedForeignNational','Deaths', 'Cured', 'Active'].sum().reset_index()\n",
                "df_india_latest.style.background_gradient(cmap='Reds')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Indian_National = df_india['ConfirmedIndianNational'].sum()\n",
                "Foreigners = df_india['ConfirmedForeignNational'].sum()\n",
                "dct ={\"Indian\": Indian_National,\"Foriengners\":Foreigners}\n",
                "colors=['orange','blue']\n",
                "plt.figure(figsize = (10,10))\n",
                "plt.pie(dct.values(),labels=dct.keys(),colors=colors,shadow=True,explode=(0.1, 0.1), autopct='%1.2f%%')\n",
                "plt.axis('equal')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "slate = df_india_latest[df_india_latest['Date']==max(df_india_latest['Date'])].reset_index(drop=True)\n",
                "slate.style.background_gradient(cmap='copper')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "plot = slate.melt(id_vars=\"Date\", value_vars=['Active','Cured','Deaths'])\n",
                "plot"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "matt = ex.treemap(plot, path=['variable'], values=\"value\", height=500, width=800,\n",
                "                color_discrete_sequence=[acti,cure,deth])\n",
                "matt.show() "
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "india_latest = df_india[df_india['Date']==max(df_india['Date'])].reset_index()\n",
                "india_latest_groupby = india_latest.groupby('State/UnionTerritory')['ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active'].sum().reset_index()\n",
                "india_latest_groupby.style.background_gradient(cmap='OrRd')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#confirmed cases\n",
                "state_confirmed = india_latest_groupby.sort_values(by='ConfirmedIndianNational', ascending=False)\n",
                "state_confirmed = state_confirmed.reset_index(drop=True)\n",
                "state_confirmed.style.background_gradient(cmap='OrRd')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "states_with_death = state_confirmed[state_confirmed['Deaths']>0][['State/UnionTerritory','Deaths']]\n",
                "states_with_death.sort_values('Deaths',ascending=False).reset_index(drop=True).style.background_gradient(cmap='OrRd')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "no_recovery = state_confirmed[state_confirmed['ConfirmedIndianNational']+ state_confirmed['ConfirmedForeignNational'] == \n",
                "                              state_confirmed['Deaths']+ state_confirmed['Cured']]\n",
                "no_recovery = no_recovery[['State/UnionTerritory','ConfirmedIndianNational','ConfirmedForeignNational','Deaths','Cured']]\n",
                "no_recovery = no_recovery.sort_values('ConfirmedIndianNational', ascending=False)\n",
                "no_recovery['Cured'].count()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "#India\n",
                "India = folium.Map(location=[20.5937,78.9629], tiles='cartodbpositron', min_zoom=4, max_zoom=10, zoom_start=4)\n",
                "for i in range(0, len(df_india)):\n",
                "    folium.Circle(\n",
                "        location=[df_india.iloc[i]['Latitude'],df_india.iloc[i]['Longitude']],\n",
                "                  color='crimson',\n",
                "                  tooltip = '<li><bold>State/UnionTerritory : '+str(df_india.iloc[i]['State/UnionTerritory'])+\n",
                "                            '<li><bold>ConfirmedIndianNational : '+str(df_india.iloc[i]['ConfirmedIndianNational'])+\n",
                "                            '<li><bold>ConfirmedForeignNational : '+str(df_india.iloc[i]['ConfirmedForeignNational'])+\n",
                "                            '<li><bold>Deaths : '+str(df_india.iloc[i]['Deaths'])+\n",
                "                            '<li><bold>Cured : '+str(df_india.iloc[i]['Cured']),\n",
                "                  radius=int(df_india.iloc[i]['ConfirmedIndianNational'])**1.1).add_to(India)\n",
                "India\n",
                "\n",
                "#The output is not 100% correct as there was some issue with the cordinates."
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "graph = df_india.groupby('Date')['Cured', 'Deaths', 'Active'].sum().reset_index()\n",
                "graph = graph.melt(id_vars='Date', value_vars=['Cured', 'Deaths', 'Active'],\n",
                "         var_name='Case', value_name='Count')\n",
                "graph.head()\n",
                "\n",
                "fig=ex.area(graph, x='Date', y='Count', color='Case',\n",
                "           title = 'Cases over time', color_discrete_sequence=[cure, deth, acti])\n",
                "fig.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Cure_over_Death = df_india.groupby('Date').sum().reset_index()\n",
                "\n",
                "Cure_over_Death['No. of Deaths to 100 Confirmed Cases'] = round(Cure_over_Death['Deaths']/(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100\n",
                "Cure_over_Death['No. of Recovered to 100 Confirmed Cases'] = round(Cure_over_Death['Cured']/(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100\n",
                "\n",
                "Cure_over_Death = Cure_over_Death.melt(id_vars ='Date',\n",
                "                          value_vars=['No. of Deaths to 100 Confirmed Cases','No. of Recovered to 100 Confirmed Cases'],\n",
                "                          var_name='Ratio',\n",
                "                          value_name='Value')\n",
                "\n",
                "fig = ex.line(Cure_over_Death, x='Date', y='Value', color='Ratio', log_y=True,\n",
                "             title='Cure_over_Death', color_discrete_sequence=[deth,cure])\n",
                "\n",
                "fig.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_india_data = df_india.drop(['Latitude', 'Longitude'], axis=1)\n",
                "df_india_data['TotalConfirmed'] = df_india_data['ConfirmedIndianNational'] + df_india_data['ConfirmedForeignNational']\n",
                "df_india_data = df_india_data[['Date', 'State/UnionTerritory','TotalConfirmed','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']]\n",
                "df_india_data.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "\n",
                "spread = df_india_data[df_india_data['TotalConfirmed']!=0].groupby('Date')['State/UnionTerritory'].unique().apply(len)\n",
                "spread = pd.DataFrame(spread).reset_index()\n",
                "\n",
                "spread_graph = ex.line(spread, x='Date', y='State/UnionTerritory', text='State/UnionTerritory',\n",
                "              title='Number of State/UnionTerritory to which COVID-19 spread over the time',\n",
                "              color_discrete_sequence=[conf,deth, cure])\n",
                "spread_graph.update_traces(textposition='top center')\n",
                "spread_graph.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Spread = df_india_data.groupby(['Date', 'State/UnionTerritory'])['TotalConfirmed'].sum().reset_index().sort_values('TotalConfirmed', ascending=False)\n",
                "\n",
                "ex.line(Spread, x=\"Date\", y=\"TotalConfirmed\", color='State/UnionTerritory', title='Spread over time', height=600)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "latest_date = india_latest_groupby\n",
                "latest_date['TotalConfirmed'] = latest_date['ConfirmedIndianNational'] + latest_date['ConfirmedForeignNational']\n",
                "latest_date = latest_date[['State/UnionTerritory','TotalConfirmed','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']]\n",
                "latest_date.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Confirmed_bar = ex.bar(latest_date.sort_values('TotalConfirmed', ascending=False).head(30).sort_values('TotalConfirmed', ascending=True), \n",
                "             x=\"TotalConfirmed\", y=\"State/UnionTerritory\", title='Confirmed Cases', text='TotalConfirmed', orientation='h', \n",
                "             width=900, height=700, range_x = [0, max(latest_date['TotalConfirmed'])+15])\n",
                "Confirmed_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')\n",
                "Confirmed_bar.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Death_rate_bar = ex.bar(latest_date.sort_values('Deaths', ascending=False).head(30).sort_values('Deaths', ascending=True), \n",
                "             x=\"Deaths\", y=\"State/UnionTerritory\", title='Death in each state', text='Deaths', orientation='h', \n",
                "             width=800, height=700, range_x = [0, max(latest_date['Deaths'])+0.5])\n",
                "Death_rate_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')\n",
                "Death_rate_bar.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "cure_bar = ex.bar(latest_date.sort_values('Cured', ascending=False).head(30).sort_values('Cured', ascending=True), \n",
                "             x=\"Cured\", y=\"State/UnionTerritory\", title='Cured cases', text='Cured', orientation='h', \n",
                "             width=800, height=700, range_x = [0, max(latest_date['Cured'])+4])\n",
                "cure_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')\n",
                "cure_bar.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Active_cases = ex.bar(latest_date.sort_values('Active', ascending=False).head(30).sort_values('Active', ascending=True), \n",
                "             x=\"Active\", y=\"State/UnionTerritory\", title='Active cases', text='Active', orientation='h', \n",
                "             width=800, height=700, range_x = [0, max(latest_date['Active'])+10])\n",
                "Active_cases.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')\n",
                "Active_cases.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "latest_date['Death Rate'] = round((latest_date['Deaths']/latest_date['TotalConfirmed'])*20,2)\n",
                "Top_50 = latest_date[latest_date['TotalConfirmed']>20]\n",
                "Top_50 = Top_50.sort_values('Death Rate', ascending=False)\n",
                "\n",
                "Plot = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True), \n",
                "             x=\"Death Rate\", y=\"State/UnionTerritory\", text='Death Rate', orientation='h', \n",
                "             width=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case')\n",
                "Plot.update_traces(marker_color='#00a8cc', opacity=0.6, textposition='outside')\n",
                "Plot.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#Date vs Confirmed\n",
                "Date_vs_confirmed = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()\n",
                "Date_vs_confirmed = Date_vs_confirmed.reset_index()\n",
                "\n",
                "Date_vs_confirmed_fig = ex.bar(Date_vs_confirmed, x=\"Date\", y=\"TotalConfirmed\", color='State/UnionTerritory', orientation='v', height=600,\n",
                "                        title='Date vs Confirmed', color_discrete_sequence = ex.colors.cyclical.mygbm)\n",
                "Date_vs_confirmed_fig.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#Date vs Cured\n",
                "Date_vs_cured = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()\n",
                "Date_vs_cured = Date_vs_cured.reset_index()\n",
                "\n",
                "Date_vs_cured_fig = ex.bar(Date_vs_cured, x=\"Date\", y=\"Cured\", color='State/UnionTerritory', orientation='v', height=600,\n",
                "                        title='Date vs Cured', color_discrete_sequence = ex.colors.cyclical.mygbm)\n",
                "Date_vs_cured_fig.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#Date vs Active\n",
                "Date_vs_Deaths = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()\n",
                "Date_vs_Deaths = Date_vs_Deaths.reset_index()\n",
                "\n",
                "Date_vs_Deaths_fig = ex.bar(Date_vs_Deaths, x=\"Date\", y=\"Deaths\", color='State/UnionTerritory', orientation='v', height=600,\n",
                "                        title='Date vs Active', color_discrete_sequence = ex.colors.cyclical.mygbm)\n",
                "Date_vs_Deaths_fig.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "new_cases = df_india_data.groupby(['State/UnionTerritory', 'Date', ])['TotalConfirmed', 'Deaths', 'Cured']\n",
                "new_cases = new_cases.sum().diff().reset_index()\n",
                "\n",
                "mat = new_cases['State/UnionTerritory'] != new_cases['State/UnionTerritory'].shift(1)\n",
                "\n",
                "new_cases.loc[mat, 'TotalConfirmed'] = np.nan\n",
                "new_cases.loc[mat, 'Deaths'] = np.nan\n",
                "new_cases.loc[mat, 'Cured'] = np.nan\n",
                "\n",
                "New_cases_plot = ex.bar(new_cases, x=\"Date\", y=\"TotalConfirmed\", color='State/UnionTerritory',title='New cases')\n",
                "New_cases_plot.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Deaths vs Confirmed\n",
                "Death_vs_Conf = latest_date.sort_values('Deaths', ascending=False).iloc[:15, :]\n",
                "\n",
                "Death_vs_Conf_plot = ex.scatter(Death_vs_Conf, \n",
                "                 x='TotalConfirmed', y='Deaths', color='State/UnionTerritory',\n",
                "                 text='State/UnionTerritory', log_x=True, log_y=True, title='Deaths vs Confirmed')\n",
                "Death_vs_Conf_plot.update_traces(textposition='top center')\n",
                "Death_vs_Conf_plot.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#Cured vs Confirmed\n",
                "Cured_vs_Conf = latest_date.sort_values('Cured', ascending=False).iloc[:15, :]\n",
                "\n",
                "Cured_vs_Conf_plot = ex.scatter(Death_vs_Conf, \n",
                "                 x='TotalConfirmed', y='Cured', color='State/UnionTerritory',\n",
                "                 text='State/UnionTerritory', log_x=True, log_y=True, title='Cured vs Confirmed')\n",
                "Cured_vs_Conf_plot.update_traces(textposition='top center')\n",
                "Cured_vs_Conf_plot.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "\"\"\"\n",
                "df_India_perday\n",
                "df_Italy_perday\n",
                "df_Korea_perday\n",
                "\"\"\"\n",
                "\n",
                "Comparison = make_subplots(\n",
                "    rows=2, cols=2,\n",
                "    specs=[[{}, {}],\n",
                "           [{\"colspan\": 2}, None]],\n",
                "    subplot_titles=(\"S.Korea\",\"Italy\", \"India\"))\n",
                "\n",
                "Comparison.add_trace(gp.Bar(x=df_Korea_perday['Date'], y=df_Korea_perday['Total Cases'],\n",
                "                    marker= dict(color=df_Korea_perday['Total Cases'], coloraxis=\"coloraxis\")),\n",
                "              1, 1)\n",
                "\n",
                "Comparison.add_trace(gp.Bar(x=df_Italy_perday['Date'], y=df_Italy_perday['Total Cases'],\n",
                "                    marker= dict(color=df_Italy_perday['Total Cases'], coloraxis=\"coloraxis\")),\n",
                "              1, 2)\n",
                "\n",
                "Comparison.add_trace(gp.Bar(x=df_india_data['Date'], y=df_india_data['TotalConfirmed'],\n",
                "                    marker= dict(color=df_india_data['TotalConfirmed'], coloraxis=\"coloraxis\")),\n",
                "              2, 1)\n",
                "\n",
                "Comparison.update_layout(coloraxis=dict(colorscale='RdBu'), showlegend=False,title_text=\"Total Confirmed cases(Cumulative)\")\n",
                "\n",
                "Comparison.update_layout(plot_bgcolor='rgb(250, 242, 242)')\n",
                "Comparison.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "India = df_india_data['TotalConfirmed'].sum()\n",
                "Italy = df_Italy_perday['Total Cases'].sum()\n",
                "South_Korea = df_Korea_perday['Total Cases'].sum()\n",
                "dict ={\"India\": India,\"Italy\":Italy, 'South Korea':South_Korea}\n",
                "colors=['red','blue', 'yellow']\n",
                "plt.figure(figsize = (10,10))\n",
                "plt.pie(dict.values(),labels=dict.keys(),colors=colors,shadow=True,explode=(0.1, 0.1, 0.1), autopct='%1.2f%%')\n",
                "plt.axis('equal')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(os.listdir('../input'))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "df1 = pd.read_csv('../input/Mall_Customers.csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'Mall_Customers.csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df1.head(5)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotCorrelationMatrix(df1, 8)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotScatterMatrix(df1, 12, 10)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# import package with helper functions ",
                "import bq_helper",
                "",
                "# create a helper object for this dataset",
                "github = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\",",
                "                                              dataset_name=\"github_repos\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# You can use two dashes (--) to add comments in SQL",
                "query = (\"\"\"",
                "        -- Select all the columns we want in our joined table",
                "        SELECT L.license, COUNT(sf.path) AS number_of_files",
                "        FROM `bigquery-public-data.github_repos.sample_files` as sf",
                "        -- Table to merge into sample_files",
                "        INNER JOIN `bigquery-public-data.github_repos.licenses` as L ",
                "            ON sf.repo_name = L.repo_name -- what columns should we join on?",
                "        GROUP BY L.license",
                "        ORDER BY number_of_files DESC",
                "        \"\"\")",
                "",
                "file_count_by_license = github.query_to_pandas_safe(query, max_gb_scanned=6)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# print out all the returned results",
                "print(file_count_by_license)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# print the first couple rows of the \"sample_commits\" table",
                "github.head(\"sample_commits\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# You can use two dashes (--) to add comments in SQL",
                "query_commit = (\"\"\"",
                "        WITH repolist AS ",
                "            (",
                "                SELECT repo_name",
                "                FROM  `bigquery-public-data.github_repos.sample_files`",
                "                WHERE path LIKE '%.py'",
                "                GROUP BY repo_name",
                "            )",
                "        SELECT sf.repo_name, COUNT(sc.commit) AS number_of_commits",
                "        FROM ",
                "        repolist as sf ",
                "        INNER JOIN `bigquery-public-data.github_repos.sample_commits` as sc",
                "            ON sc.repo_name = sf.repo_name ",
                "        GROUP BY sf.repo_name",
                "        ORDER BY number_of_commits DESC",
                "        \"\"\")",
                "",
                "# check how big this query will be",
                "github.estimate_query_size(query_commit)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "python_commits = github.query_to_pandas_safe(query_commit, max_gb_scanned=6)",
                "python_commits"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "\n",
                "\n",
                "#Importing relevant libraries\n",
                "import numpy as np \n",
                "import pandas as pd \n",
                "import plotly as py\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "import plotly.graph_objs as go\n",
                "from plotly.subplots import make_subplots\n",
                "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
                "init_notebook_mode(connected=True)\n",
                "\n",
                "##Importing data into notebook\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "#Reading the data by pandas..Trying this you may have to change location according to local location\n",
                "\n",
                "corona_data=pd.read_csv('/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv')\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Viewing first three rows of data for quick insight\n",
                "corona_data.head(3)\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Viewing last two rows of data\n",
                "corona_data.tail(2)\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "choro_map=px.choropleth(corona_data, \n",
                "                    locations=\"Country/Region\", \n",
                "                    locationmode = \"country names\",\n",
                "                    color=\"Confirmed\", \n",
                "                    hover_name=\"Country/Region\", \n",
                "                    animation_frame=\"ObservationDate\"\n",
                "                   )\n",
                "\n",
                "choro_map.update_layout(\n",
                "    title_text = 'Global Spread of Coronavirus',\n",
                "    title_x = 0.5,\n",
                "    geo=dict(\n",
                "        showframe = False,\n",
                "        showcoastlines = False,\n",
                "    ))\n",
                "    \n",
                "choro_map.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "pie_chart = px.pie(corona_data, values = 'Confirmed',names='Country/Region', height=600)\n",
                "pie_chart.update_traces(textposition='inside', textinfo='percent+label')\n",
                "\n",
                "pie_chart.update_layout(\n",
                "    title_x = 0.5,\n",
                "    geo=dict(\n",
                "        showframe = False,\n",
                "        showcoastlines = False,\n",
                "    ))\n",
                "\n",
                "pie_chart.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Manipulating the dataframe\n",
                "top10 = corona_data.groupby(['Country/Region', 'ObservationDate']).sum().reset_index().sort_values('Confirmed', ascending=False)\n",
                "top10  = top10.drop_duplicates(subset = ['Country/Region'])\n",
                "top10 = top10.iloc[0:10]\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "pie_chart_top10 = px.pie(top10, values = 'Confirmed',names='Country/Region', height=600)\n",
                "pie_chart_top10.update_traces(textposition='inside', textinfo='percent+label')\n",
                "\n",
                "pie_chart_top10.update_layout(\n",
                "    title_x = 0.5,\n",
                "    geo=dict(\n",
                "        showframe = False,\n",
                "        showcoastlines = False,\n",
                "    ))\n",
                "\n",
                "pie_chart_top10.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "#Manipulating the dataframe\n",
                "last20 = corona_data.groupby(['Country/Region', 'ObservationDate']).sum().reset_index().sort_values('Confirmed', ascending=False)\n",
                "last20  = last20.drop_duplicates(subset = ['Country/Region'])\n",
                "last20 = last20.iloc[-20:-1]\n",
                "last20"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "pie_chart_last20 = px.pie(last20, values = 'Confirmed',names='Country/Region', height=600)\n",
                "pie_chart_last20.update_traces(textposition='inside', textinfo='percent+label')\n",
                "\n",
                "pie_chart_last20.update_layout(\n",
                "    title_x = 0.5,\n",
                "    geo=dict(\n",
                "        showframe = False,\n",
                "        showcoastlines = False,\n",
                "    ))\n",
                "\n",
                "pie_chart_last20.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "bar_data = corona_data.groupby(['Country/Region', 'ObservationDate'])['Confirmed', 'Deaths', 'Recovered'].sum().reset_index().sort_values('ObservationDate', ascending=True)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "bar_fig = px.bar(bar_data, x=\"ObservationDate\", y=\"Confirmed\", color='Country/Region', text = 'Confirmed', orientation='v', height=1300,width=1000,\n",
                "             title='Increase in COVID-19 Cases')\n",
                "bar_fig.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "bar_fig2 = px.bar(bar_data, x=\"ObservationDate\", y=\"Deaths\", color='Country/Region', text = 'Deaths', orientation='v', height=1000,width=900,\n",
                "             title='COVID-19 Deaths since February to April 11th')\n",
                "bar_fig2.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "bar_fig3 = px.bar(bar_data, x=\"ObservationDate\", y=\"Recovered\", color='Country/Region', text = 'Recovered', orientation='v', height=1000,width=900,\n",
                "             title='COVID-19 Recovered Cases since February to April 11th')\n",
                "bar_fig3.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "line_data = corona_data.groupby('ObservationDate').sum().reset_index()\n",
                "\n",
                "line_data = line_data.melt(id_vars='ObservationDate', \n",
                "                 value_vars=['Confirmed', \n",
                "                             'Recovered', \n",
                "                             'Deaths'], \n",
                "                 var_name='Ratio', \n",
                "                 value_name='Value')\n",
                "\n",
                "line_fig = px.line(line_data, x=\"ObservationDate\", y=\"Value\", line_shape=\"spline\",color='Ratio', \n",
                "              title='Confirmed cases, Recovered cases, and Death Over Time')\n",
                "line_fig.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "content": [
                "import pandas as pd",
                "import numpy as np",
                "import seaborn as sns",
                "import matplotlib.pyplot as plt",
                "import matplotlib.patches as patches",
                "",
                "%matplotlib inline",
                "",
                "multiple = pd.read_csv('../input/multipleChoiceResponses.csv', dtype=np.object)",
                "mulcQ = multiple.iloc[0,:]",
                "mulcA = multiple.iloc[1:,:]",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fast = mulcA[round(mulcA.iloc[:,0].astype(int) / 60) <= 3].index",
                "mulcA = mulcA.drop(fast, axis=0)",
                "rol = mulcA.Q5",
                "rol.value_counts(normalize=True).plot(kind='bar')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "rem = mulcA.Q9",
                "rem.value_counts().plot(kind='bar')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#",
                "mulcA.replace({'Q9':{'0-10,000':1,'10-20,000':2,'20-30,000':3,'30-40,000':4,'40-50,000':5,'50-60,000':6,",
                "                       '60-70,000':7,'70-80,000':8,'80-90,000':9,'90-100,000':10,'100-125,000':11,'125-150,000':12,",
                "                       '150-200,000':13,'200-250,000':14,'250-300,000':15,'300-400,000':16,'400-500,000':17,",
                "                                 '500,000+':18}},inplace = True)",
                "mulcA.replace({'Q3':{'United Kingdom of Great Britain and Northern Ireland':'Great B.','Iran, Islamic Republic of...':'Iran',",
                "                       'United States of America':'USA','Hong Kong (S.A.R.)':'Hong Kong','Republic of Korea':'R. Korea',",
                "                      'Czech Republic':'Czech R.'}},inplace = True)",
                "",
                "q16 = multiple.filter(regex=\"(Q{t}$|Q{t}_)\".format(t = 16))[1:]",
                "q16_col = {'Q16_Part_1':'Python','Q16_Part_2':'R','Q16_Part_3':'SQL','Q16_Part_4': 'Bash','Q16_Part_5':'Java',",
                "           'Q16_Part_6':'Javascript','Q16_Part_7':'VBA','Q16_Part_8':'C/C++','Q16_Part_9':'MATLAB',",
                "           'Q16_Part_10':'Scala','Q16_Part_11':'Julia','Q16_Part_12':'Go','Q16_Part_13':'C#/.NET','Q16_Part_14':'PHP',",
                "           'Q16_Part_15':'Ruby','Q16_Part_16':'SAS/STATA'}",
                "",
                "q16_lim= q16.rename(columns=q16_col).fillna(0).replace('[^\\\\d]',1, regex=True)",
                "q16_lim.pop('Q16_Part_17')",
                "q16_lim.pop('Q16_Part_18')",
                "q16_lim.pop('Q16_OTHER_TEXT')",
                "lab = list(q16_lim.iloc[:0])",
                "for i in lab:",
                "    mulcA[i]= q16_lim['{}'.format(i)]",
                "#",
                "com_sci = mulcA[(mulcA.Q5 == 'Computer science (software engineering, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "eng_nco = mulcA[(mulcA.Q5 == 'Engineering (non-computer focused)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "mat_sta = mulcA[(mulcA.Q5 == 'Mathematics or statistics') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "biu_dis = mulcA[(mulcA.Q5 == 'A business discipline (accounting, economics, finance, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "phy_ast = mulcA[(mulcA.Q5 == 'Physics or astronomy') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "inf_tec = mulcA[(mulcA.Q5 == 'Information technology, networking, or system administration') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "med_sci = mulcA[(mulcA.Q5 == 'Medical or life sciences (biology, chemistry, medicine, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "soc_sci = mulcA[(mulcA.Q5 == 'Social sciences (anthropology, psychology, sociology, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "hum_tie = mulcA[(mulcA.Q5 == 'Humanities (history, literature, philosophy, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "env_sci = mulcA[(mulcA.Q5 == 'Environmental science or geology') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "fin_art = mulcA[(mulcA.Q5 == 'Fine arts or performing arts') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "",
                "",
                "rem_prom = [com_sci.Q9.mean(),eng_nco.Q9.mean(),mat_sta.Q9.mean(),biu_dis.Q9.mean(),phy_ast.Q9.mean(),inf_tec.Q9.mean(),med_sci.Q9.mean(),",
                "            soc_sci.Q9.mean(),hum_tie.Q9.mean(),env_sci.Q9.mean(),fin_art.Q9.mean()]",
                "plt.figure(figsize=(20,10))",
                "plt.bar(np.arange(11),rem_prom,color=['dodgerblue','c','tomato','silver','midnightblue','tan'])",
                "plt.xticks(np.arange(11), ('Com. Science', 'Engieneering', 'Mathematics', 'Economics', 'Physics','Inf. Tecnologist','Medics','Social sciences','Humanities',",
                "                            'Env. Sciense','Arts'))",
                "plt.yticks(np.arange(10),('$10000','$20000','$30000','$40000','$50000','$60000','$70000','$80000','$90000','$100000'))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "",
                "age_18_21 = mulcA.Q5[(mulcA.Q2 == '18-21')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_22_24 = mulcA.Q5[(mulcA.Q2 == '22-24')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_25_29 = mulcA.Q5[(mulcA.Q2 == '25-29')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_30_34 = mulcA.Q5[(mulcA.Q2 == '30-34')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_35_39 = mulcA.Q5[(mulcA.Q2 == '35-39')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_40_44 = mulcA.Q5[(mulcA.Q2 == '40-44')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_45_49 = mulcA.Q5[(mulcA.Q2 == '45-49')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_50_54 = mulcA.Q5[(mulcA.Q2 == '50-54')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_55_59 = mulcA.Q5[(mulcA.Q2 == '55-59')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_60_69 = mulcA.Q5[(mulcA.Q2 == '60-69')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_70_79 = mulcA.Q5[(mulcA.Q2 == '70-79')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_80m = mulcA.Q5[(mulcA.Q2 == '80+')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "",
                "df_arol = pd.DataFrame([age_18_21.value_counts(normalize=True),age_22_24.value_counts(normalize=True),age_25_29.value_counts(normalize=True),age_30_34.value_counts(normalize=True),",
                "                        age_35_39.value_counts(normalize=True),age_40_44.value_counts(normalize=True),age_45_49.value_counts(normalize=True),age_50_54.value_counts(normalize=True)",
                "                        ,age_55_59.value_counts(normalize=True),age_60_69.value_counts(normalize=True),age_70_79.value_counts(normalize=True),age_80m.value_counts(normalize=True)],",
                "                       index=['age18-21','age22-24','age25-29','age30-34','age35-39','age40-44','age45-49','age50-54','age55-59'",
                "                              ,'age60-69','age70-79','age80+']).T",
                "",
                "axes = df_arol.plot.barh(rot=0, subplots=True,figsize=(15,35))",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,3))",
                "names = ['Comp. Science','Engeneering','Mathematics','Inf. Technology','Business','Physics','Medical','Soc. Science']",
                "df_coun = pd.DataFrame([com_sci.Q3.value_counts(),eng_nco.Q3.value_counts(),mat_sta.Q3.value_counts(),inf_tec.Q3.value_counts()",
                "                        ,biu_dis.Q3.value_counts(),phy_ast.Q3.value_counts(),med_sci.Q3.value_counts(),soc_sci.Q3.value_counts()]",
                "                       ,index=[names])",
                "df_coun= df_coun.drop(['Other'],axis=1).T",
                "fig, axes = plt.subplots(4, 2,  sharey=True,figsize=(15,20))",
                "axes[0, 0].barh(df_coun.index[:10],list(map(int,df_coun['Comp. Science'].values[:10])),color='cadetblue')",
                "axes[0, 0].set_title('Comp. Science')",
                "axes[0, 1].barh(df_coun.index[:10],list(map(int,df_coun['Engeneering'].values[:10])),color='slategray')",
                "axes[0, 1].set_title('Engeneering')",
                "axes[1, 0].barh(df_coun.index[:10],list(map(int,df_coun['Mathematics'].values[:10])),color='seagreen')",
                "axes[1, 0].set_title('Mathematics')",
                "axes[1, 1].barh(df_coun.index[:10],list(map(int,df_coun['Inf. Technology'].values[:10])),color='palevioletred')",
                "axes[1, 1].set_title('Inf. Technology')",
                "axes[2, 0].barh(df_coun.index[:10],list(map(int,df_coun['Business'].values[:10])),color='darkblue')",
                "axes[2, 0].set_title('Business')",
                "axes[2, 1].barh(df_coun.index[:10],list(map(int,df_coun['Physics'].values[:10])),color='khaki')",
                "axes[2, 1].set_title('Physics')",
                "axes[3, 0].barh(df_coun.index[:10],list(map(int,df_coun['Medical'].values[:10])),color='k')",
                "axes[3, 0].set_title('Medical')",
                "axes[3, 1].barh(df_coun.index[:10],list(map(int,df_coun['Soc. Science'].values[:10])),color='firebrick')",
                "axes[3, 1].set_title('Soc. Science')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "lengs = pd.DataFrame([mulcA['Python'].sum(),mulcA['R'].sum(),mulcA['SQL'].sum(),mulcA['Bash'].sum(),mulcA['Java'].sum(),",
                "                           mulcA['Javascript'].sum(),mulcA['VBA'].sum(),mulcA['C/C++'].sum(),mulcA['MATLAB'].sum(),",
                "                           mulcA['Scala'].sum(),mulcA['Julia'].sum(),mulcA['Go'].sum(),mulcA['C#/.NET'].sum(),",
                "                           mulcA['PHP'].sum(),mulcA['Ruby'].sum(),mulcA['SAS/STATA'].sum()],index=lab)",
                "lengs.plot(kind='bar',color='brown')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "com_sci_lengs=[mulcA[i][mulcA.Q5=='Computer science (software engineering, etc.)'].sum() for i in lab ]",
                "eng_nco_lengs=[mulcA[i][mulcA.Q5=='Engineering (non-computer focused)'].sum() for i in lab ]",
                "mat_sta_lengs=[mulcA[i][mulcA.Q5=='Mathematics or statistics'].sum() for i in lab ]",
                "biu_dis_lengs=[mulcA[i][mulcA.Q5=='A business discipline (accounting, economics, finance, etc.)'].sum() for i in lab ]",
                "phy_ast_lengs=[mulcA[i][mulcA.Q5=='Physics or astronomy'].sum() for i in lab ]",
                "inf_tec_lengs=[mulcA[i][mulcA.Q5=='Information technology, networking, or system administration'].sum() for i in lab ]",
                "med_sci_lengs=[mulcA[i][mulcA.Q5=='Medical or life sciences (biology, chemistry, medicine, etc.)'].sum() for i in lab ]",
                "soc_sci_lengs=[mulcA[i][mulcA.Q5=='Social sciences (anthropology, psychology, sociology, etc.)'].sum() for i in lab ]",
                "",
                "fig, axes = plt.subplots(4, 2,  sharey=True,figsize=(15,20))",
                "axes[0, 0].barh(lab,com_sci_lengs,color='c')",
                "axes[0, 0].set_title('Comp. Science')",
                "axes[0, 1].barh(lab,eng_nco_lengs,color='r')",
                "axes[0, 1].set_title('Engeneering')",
                "axes[1, 0].barh(lab,mat_sta_lengs,color='k')",
                "axes[1, 0].set_title('Mathematics')",
                "axes[1, 1].barh(lab,inf_tec_lengs)",
                "axes[1, 1].set_title('Inf. Technology')",
                "axes[2, 0].barh(lab,biu_dis_lengs,color = 'tomato')",
                "axes[2, 0].set_title('Business')",
                "axes[2, 1].barh(lab,phy_ast_lengs,color='b')",
                "axes[2, 1].set_title('Physics')",
                "axes[3, 0].barh(lab,med_sci_lengs,color='y')",
                "axes[3, 0].set_title('Medical')",
                "axes[3, 1].barh(lab,soc_sci_lengs,color='g')",
                "axes[3, 1].set_title('Soc. Science')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fem_oc = mulcA.Q5[mulcA.Q1 == \"Female\"]",
                "mal_oc = mulcA.Q5[mulcA.Q1 == \"Male\"]",
                "df_gen = pd.DataFrame([fem_oc.value_counts(normalize=True),mal_oc.value_counts(normalize=True)],index=['Female','Male']).T",
                "axes = df_gen.plot.barh(rot=0, subplots=True,figsize=(15,15))    "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input/'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "# Import pandas\n",
                "import pandas as pd\n",
                "\n",
                "# Read in the file content in a DataFrame called discoveries\n",
                "discoveries = pd.read_csv('/kaggle/input/week6dataset/discoveries.csv')\n",
                "\n",
                "# Display the first five lines of the DataFrame\n",
                "discoveries.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Print the data type of each column in discoveries\n",
                "print(discoveries.dtypes)\n",
                "\n",
                "# Convert the date column to a datestamp type\n",
                "discoveries['date'] = pd.to_datetime(discoveries['date'])\n",
                "\n",
                "# Print the data type of each column in discoveries, again\n",
                "print(discoveries.dtypes)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "\n",
                "# Set the date column as the index of your DataFrame discoveries\n",
                "discoveries.set_index('date', inplace = True)\n",
                "\n",
                "# Plot the time series in your DataFrame\n",
                "ax = discoveries.plot(color='blue')\n",
                "\n",
                "# Specify the x-axis label in your plot\n",
                "ax.set_xlabel('Date')\n",
                "\n",
                "# Specify the y-axis label in your plot\n",
                "ax.set_ylabel('Number of great discoveries')\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "# Import the matplotlib.pyplot sub-module\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Use the ggplot style\n",
                "plt.style.use('ggplot')\n",
                "ax2 = discoveries.plot()\n",
                "\n",
                "# Set the title\n",
                "ax2.set_title('ggplot Style')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "# Import the matplotlib.pyplot sub-module\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Use the fivethirtyeight style\n",
                "plt.style.use('fivethirtyeight')\n",
                "\n",
                "# Plot the time series\n",
                "ax1 = discoveries.plot()\n",
                "ax1.set_title('FiveThirtyEight Style')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Plot a line chart of the discoveries DataFrame using the specified arguments\n",
                "ax = discoveries.plot(color='blue',figsize =(8, 3), linewidth=2, fontsize=6)\n",
                "\n",
                "# Specify the title in your plot\n",
                "ax.set_title('Number of great inventions and scientific discoveries from 1860 to 1959', fontsize=8)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Select the subset of data between 1939 and 1958\n",
                "discoveries_subset_2 = discoveries ['1939-01-01': '1958-01-01']\n",
                "# Plot the time series in your DataFrame as a blue area chart\n",
                "ax = discoveries_subset_2.plot(color='blue', fontsize=15)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Plot your the discoveries time series\n",
                "ax = discoveries.plot(color='blue', fontsize=6)\n",
                "\n",
                "# Add a red vertical line\n",
                "ax.axvline('1939-01-01', color='red', linestyle='--')\n",
                "\n",
                "# Add a green horizontal line\n",
                "ax.axhline(4, color='green', linestyle='--')\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Plot your the discoveries time series\n",
                "ax = discoveries.plot(color='blue', fontsize=6)\n",
                "\n",
                "# Add a vertical red shaded region\n",
                "ax.axvspan('1900-01-01', '1915-01-01', color='red', alpha=0.3)\n",
                "\n",
                "# Add a horizontal green shaded region\n",
                "ax.axhspan(6, 8, color='green', alpha=0.3)\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "co2_levels = pd.read_csv('/kaggle/input/week6dataset/co2_levels.csv', parse_dates = ['datestamp'])\n",
                "print(co2_levels)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Set datestamp column as index\n",
                "co2_levels = co2_levels.set_index('datestamp')\n",
                "\n",
                "# Print out the number of missing values\n",
                "print(co2_levels.isnull().sum())"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Impute missing values with the next valid observation\n",
                "co2_levels = co2_levels.fillna(method='bfill')\n",
                "\n",
                "# Print out the number of missing values\n",
                "print(co2_levels.isnull().sum())"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Compute the 52 weeks rolling mean of the co2_levels DataFrame\n",
                "ma = co2_levels.rolling(window=52).mean()\n",
                "\n",
                "# Compute the 52 weeks rolling standard deviation of the co2_levels DataFrame\n",
                "mstd = co2_levels.rolling(window=52).std()\n",
                "\n",
                "# Add the upper bound column to the ma DataFrame\n",
                "ma['upper'] = ma['co2'] + (mstd['co2'] * 2)\n",
                "\n",
                "# Add the lower bound column to the ma DataFrame\n",
                "ma['lower'] = ma['co2'] - (mstd['co2'] * 2)\n",
                "\n",
                "# Plot the content of the ma DataFrame\n",
                "ax = ma.plot(linewidth=0.8, fontsize=6)\n",
                "\n",
                "# Specify labels, legend, and show the plot\n",
                "ax.set_xlabel('Date', fontsize=10)\n",
                "ax.set_ylabel('CO2 levels in Mauai Hawaii', fontsize=10)\n",
                "ax.set_title('Rolling mean and variance of CO2 levels\\nin Mauai Hawaii from 1958 to 2001', fontsize=10)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Get month for each dates in the index of co2_levels\n",
                "index_month = co2_levels.index.month\n",
                "\n",
                "# Compute the mean CO2 levels for each month of the year\n",
                "mean_co2_levels_by_month = co2_levels.groupby(index_month).mean()\n",
                "\n",
                "# Plot the mean CO2 levels for each month of the year\n",
                "mean_co2_levels_by_month.plot(fontsize = 6)\n",
                "\n",
                "# Specify the fontsize on the legend\n",
                "plt.legend(fontsize=10)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Print out summary statistics of the co2_levels DataFrame\n",
                "print(co2_levels.describe())\n",
                "\n",
                "# Print out the minima of the co2 column in the co2_levels DataFrame\n",
                "print(co2_levels.co2.min())\n",
                "\n",
                "# Print out the maxima of the co2 column in the co2_levels DataFrame\n",
                "print(co2_levels.co2.max())"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Generate a histogram\n",
                "ax = co2_levels.plot(kind = 'hist', bins = 50, fontsize=6)\n",
                "\n",
                "# Set the labels and display the plot\n",
                "ax.set_xlabel('CO2', fontsize=10)\n",
                "ax.set_ylabel('Histogram of CO2 levels in Maui Hawaii', fontsize=10)\n",
                "plt.legend(fontsize=10)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Display density plot of CO2 levels values\n",
                "ax = co2_levels.plot(kind = 'density', linewidth = 4, fontsize=6)\n",
                "\n",
                "# Annotate x-axis labels\n",
                "ax.set_xlabel('CO2', fontsize=10)\n",
                "\n",
                "# Annotate y-axis labels\n",
                "ax.set_ylabel('Density plot of CO2 levels in Maui Hawaii', fontsize=10)\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "# Import required libraries\n",
                "import matplotlib.pyplot as plt\n",
                "plt.style.use('fivethirtyeight')\n",
                "from statsmodels.graphics import tsaplots\n",
                "\n",
                "# Display the autocorrelation plot of your time series\n",
                "fig = tsaplots.plot_acf(co2_levels['co2'], lags=24)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "# Import required libraries\n",
                "import matplotlib.pyplot as plt\n",
                "plt.style.use('fivethirtyeight')\n",
                "from statsmodels.graphics import tsaplots\n",
                "\n",
                "# Display the partial autocorrelation plot of your time series\n",
                "fig = tsaplots.plot_pacf(co2_levels['co2'], lags=24)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "process_data"
            ],
            "content": [
                "# Import statsmodels.api as sm\n",
                "import statsmodels.api as sm\n",
                "\n",
                "# Perform time series decompositon\n",
                "decomposition = sm.tsa.seasonal_decompose(co2_levels)\n",
                "\n",
                "# Print the seasonality component\n",
                "print(decomposition.seasonal)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Extract the trend component\n",
                "trend = decomposition.trend\n",
                "\n",
                "# Plot the values of the trend\n",
                "ax = trend.plot(figsize=(12, 6), fontsize=6)\n",
                "\n",
                "# Specify axis labels\n",
                "ax.set_xlabel('Date', fontsize=10)\n",
                "ax.set_title('Seasonal component the CO2 time-series', fontsize=10)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "airline = pd.read_csv('/kaggle/input/week6dataset/airline_passengers.csv', parse_dates = ['Month'], index_col = 'Month')\n",
                "airline.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Plot the time series in your dataframe\n",
                "ax = airline.plot(color = 'blue', fontsize=12)\n",
                "\n",
                "# Add a red vertical line at the date 1955-12-01\n",
                "ax.axvline('1955-12-01', color='red', linestyle='--')\n",
                "\n",
                "# Specify the labels in your plot\n",
                "ax.set_xlabel('Date', fontsize=12)\n",
                "ax.set_title('Number of Monthly Airline Passengers', fontsize=12)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Print out the number of missing values\n",
                "print(airline.isnull().sum())\n",
                "\n",
                "# Print out summary statistics of the airline DataFrame\n",
                "print(airline.describe())"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Display boxplot of airline values\n",
                "ax = airline.boxplot()\n",
                "\n",
                "# Specify the title of your plot\n",
                "ax.set_title('Boxplot of Monthly Airline\\nPassengers Count', fontsize=20)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Get month for each dates from the index of airline\n",
                "index_month = airline.index.month\n",
                "\n",
                "# Compute the mean number of passengers for each month of the year\n",
                "mean_airline_by_month = airline.groupby(index_month).mean()\n",
                "\n",
                "# Plot the mean number of passengers for each month of the year\n",
                "mean_airline_by_month.plot()\n",
                "plt.legend(fontsize=20)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# Import statsmodels.api as sm\n",
                "import statsmodels.api as sm\n",
                "\n",
                "# Perform time series decompositon\n",
                "decomposition = sm.tsa.seasonal_decompose(airline)\n",
                "\n",
                "# Extract the trend and seasonal components\n",
                "trend = decomposition.trend\n",
                "seasonal = decomposition.seasonal"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "airline_decomposed = pd.concat([trend, seasonal], axis = 1)\n",
                "airline_decomposed.columns = ['trend', 'seasonal']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Print the first 5 rows of airline_decomposed\n",
                "print(airline_decomposed.head())\n",
                "\n",
                "# Plot the values of the df_decomposed DataFrame\n",
                "ax = airline_decomposed.plot(figsize=(12, 6), fontsize=15)\n",
                "\n",
                "# Specify axis labels\n",
                "ax.set_xlabel('Date', fontsize=15)\n",
                "plt.legend(fontsize=15)\n",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "visualize_data",
                "validate_data"
            ],
            "content": [
                "# Read in meat DataFrame\n",
                "meat = pd.read_csv('/kaggle/input/week6dataset/meat.csv')\n",
                "\n",
                "# Review the first five lines of the meat DataFrame\n",
                "display(meat.head(5))"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Convert the date column to a datestamp type\n",
                "meat['date'] = pd.to_datetime(meat['date'] )\n",
                "\n",
                "# Set the date column as the index of your DataFrame meat\n",
                "meat = meat.set_index('date')\n",
                "\n",
                "# Print the summary statistics of the DataFrame\n",
                "display(meat.describe())"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Plot time series dataset\n",
                "ax = meat.plot(linewidth = 2, fontsize = 12)\n",
                "\n",
                "# Additional customizations\n",
                "ax.set_xlabel('Date')\n",
                "ax.legend(fontsize=15)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Plot an area chart\n",
                "ax = meat.plot.area(fontsize=12)\n",
                "\n",
                "# Additional customizations\n",
                "ax.set_xlabel('Date')\n",
                "ax.legend(fontsize=15)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Plot time series dataset using the cubehelix color palette\n",
                "ax = meat.plot(colormap='cubehelix', fontsize=15)\n",
                "\n",
                "# Additional customizations\n",
                "ax.set_xlabel('Date')\n",
                "ax.legend(fontsize=18)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Plot time series dataset using the cubehelix color palette\n",
                "ax = meat.plot(colormap = 'PuOr', fontsize=15)\n",
                "\n",
                "# Additional customizations\n",
                "ax.set_xlabel('Date')\n",
                "ax.legend(fontsize=18)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "meat_mean = meat.mean(axis = 0)\n",
                "meat_mean = pd.DataFrame(meat_mean).transpose()\n",
                "meat_mean.index = ['mean']\n",
                "meat_mean"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Plot the meat data\n",
                "ax = meat.plot(fontsize=6, linewidth=1)\n",
                "\n",
                "# Add x-axis labels\n",
                "ax.set_xlabel('Date', fontsize=6)\n",
                "\n",
                "# Add summary table information to the plot\n",
                "ax.table(cellText=meat_mean.values,\n",
                "         colWidths = [0.15]*len(meat_mean.columns),\n",
                "         rowLabels=meat_mean.index,\n",
                "         colLabels=meat_mean.columns,\n",
                "         loc='top')\n",
                "\n",
                "# Specify the fontsize and location of your legend\n",
                "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.95), ncol=3, fontsize=6)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Create a facetted graph with 2 rows and 4 columns\n",
                "meat.plot(subplots=True, \n",
                "          layout=(2, 4), \n",
                "          sharex=False, \n",
                "          sharey=False, \n",
                "          colormap='viridis', \n",
                "          fontsize=2, \n",
                "          legend=False, \n",
                "          linewidth=0.2)\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Print the correlation matrix between the beef and pork columns using the spearman method\n",
                "print(meat[['beef', 'pork']].corr(method='spearman'))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Print the correlation matrix between the pork, veal and turkey columns using the pearson method\n",
                "print(meat[['pork', 'veal', 'turkey']].corr(method='pearson'))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "# Import seaborn library\n",
                "import seaborn as sns\n",
                "\n",
                "# Get correlation matrix of the meat DataFrame\n",
                "corr_meat = meat.corr(method='spearman')\n",
                "\n",
                "\n",
                "# Customize the heatmap of the corr_meat correlation matrix\n",
                "sns.heatmap(corr_meat,\n",
                "            annot=True,\n",
                "            linewidths=0.4,\n",
                "            annot_kws={\"size\": 10})\n",
                "\n",
                "plt.xticks(rotation=90)\n",
                "plt.yticks(rotation=0) \n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "# Import seaborn library\n",
                "import seaborn as sns\n",
                "\n",
                "# Get correlation matrix of the meat DataFrame\n",
                "corr_meat = meat.corr(method = 'pearson')\n",
                "\n",
                "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
                "fig = sns.clustermap(corr_meat,\n",
                "                     row_cluster=True,\n",
                "                     col_cluster=True,\n",
                "                     figsize=(10, 10))\n",
                "\n",
                "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
                "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Read in jobs file\n",
                "jobs = pd.read_csv('/kaggle/input/week6dataset/employment.csv', parse_dates = ['datestamp'])\n",
                "\n",
                "# Review the first five lines of your DataFrame\n",
                "display(jobs.head(5))\n",
                "\n",
                "# Review the type of each column in your DataFrame\n",
                "print(jobs.dtypes)\n",
                "\n",
                "# Convert datestamp column to a datetime object\n",
                "jobs['datestamp'] = pd.to_datetime(jobs['datestamp'])\n",
                "\n",
                "# Set the datestamp columns as the index of your DataFrame\n",
                "jobs = jobs.set_index('datestamp')\n",
                "\n",
                "# Check the number of missing values in each column\n",
                "display(jobs.isnull().sum())"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Generate a boxplot\n",
                "jobs.boxplot(fontsize=6, vert=False)\n",
                "plt.show()\n",
                "\n",
                "# Generate numerical summaries\n",
                "display(jobs.describe())"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# A subset of the jobs DataFrame\n",
                "jobs_subset = jobs[['Finance', 'Information', 'Manufacturing', 'Construction']]\n",
                "\n",
                "# Print the first 5 rows of jobs_subset\n",
                "print(jobs_subset.head())\n",
                "\n",
                "# Create a facetted graph with 2 rows and 2 columns\n",
                "ax = jobs_subset.plot(subplots=True,\n",
                "                      layout=(2,2),\n",
                "                      sharex=False,\n",
                "                      sharey=False,\n",
                "                      linewidth=0.7,\n",
                "                      fontsize=3,\n",
                "                      legend=False)\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Plot all time series in the jobs DataFrame\n",
                "ax = jobs.plot(colormap = 'Spectral', fontsize=6, linewidth=0.8)\n",
                "\n",
                "# Set labels and legend\n",
                "ax.set_xlabel('Date', fontsize=10)\n",
                "ax.set_ylabel('Unemployment Rate', fontsize=10)\n",
                "ax.set_title('Unemployment rate of U.S. workers by industry', fontsize=10)\n",
                "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
                "\n",
                "# Annotate your plots with vertical lines\n",
                "ax.axvline('2001-07-01', color='blue', linestyle='--', linewidth=0.8)\n",
                "ax.axvline('2008-09-01', color='blue', linestyle='--', linewidth=0.8)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Extract the month from the index of jobs\n",
                "index_month = jobs.index.month\n",
                "\n",
                "# Compute the mean unemployment rate for each month\n",
                "jobs_by_month = jobs.groupby(index_month).mean()\n",
                "\n",
                "# Plot the mean unemployment rate for each month\n",
                "ax = jobs_by_month.plot(fontsize=6, linewidth=1)\n",
                "\n",
                "# Set axis labels and legend\n",
                "ax.set_xlabel('Month', fontsize=10)\n",
                "ax.set_ylabel('Mean unemployment rate', fontsize=10)\n",
                "ax.legend(bbox_to_anchor=(0.8, 0.6), fontsize=10)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Extract of the year in each date indices of the jobs DataFrame\n",
                "index_year = jobs.index.year\n",
                "\n",
                "# Compute the mean unemployment rate for each year\n",
                "jobs_by_year = jobs.groupby(index_year).mean()\n",
                "\n",
                "# Plot the mean unemployment rate for each year\n",
                "ax = jobs_by_year.plot(fontsize=6, linewidth=1)\n",
                "\n",
                "# Set axis labels and legend\n",
                "ax.set_xlabel('Year', fontsize=10)\n",
                "ax.set_ylabel('Mean unemployment rate', fontsize=10)\n",
                "ax.legend(bbox_to_anchor=(0.1, 0.5), fontsize=10)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Initialize dictionary\n",
                "jobs_decomp = {}\n",
                "\n",
                "# Get the names of each time series in the DataFrame\n",
                "jobs_names = jobs.columns\n",
                "\n",
                "# Run time series decomposition on each time series of the DataFrame\n",
                "for ts in jobs_names:\n",
                "    ts_decomposition = sm.tsa.seasonal_decompose(jobs[ts])\n",
                "    jobs_decomp[ts] = ts_decomposition"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "jobs_seasonal = {}\n",
                "\n",
                "# Extract the seasonal values for the decomposition of each time series\n",
                "for ts in jobs_names:\n",
                "    jobs_seasonal[ts] = jobs_decomp[ts].seasonal\n",
                "    \n",
                "# Create a DataFrame from the jobs_seasonal dictionnary\n",
                "seasonality_df = pd.DataFrame(jobs_seasonal)\n",
                "\n",
                "# Remove the label for the index\n",
                "seasonality_df.index.name = None\n",
                "\n",
                "# Create a faceted plot of the seasonality_df DataFrame\n",
                "seasonality_df.plot(subplots=True,\n",
                "                   layout=(4, 4),\n",
                "                   sharey=False,\n",
                "                   fontsize=2,\n",
                "                   linewidth=0.3,\n",
                "                   legend=False)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Get correlation matrix of the seasonality_df DataFrame\n",
                "seasonality_corr = seasonality_df.corr(method='spearman')\n",
                "\n",
                "# Customize the clustermap of the seasonality_corr correlation matrix\n",
                "fig = sns.clustermap(seasonality_corr, annot=True, annot_kws={\"size\": 4}, linewidths=.4, figsize=(15, 10))\n",
                "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
                "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\r",
                "from sklearn.preprocessing import StandardScaler\r",
                "import matplotlib.pyplot as plt # plotting\r",
                "import numpy as np # linear algebra\r",
                "import os # accessing directory structure\r",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(os.listdir('../input'))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\r",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "    nunique = df.nunique()\r",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\r",
                "    nRow, nCol = df.shape\r",
                "    columnNames = list(df)\r",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\r",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\r",
                "    for i in range(min(nCol, nGraphShown)):\r",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\r",
                "        columnDf = df.iloc[:, i]\r",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\r",
                "            valueCounts = columnDf.value_counts()\r",
                "            valueCounts.plot.bar()\r",
                "        else:\r",
                "            columnDf.hist()\r",
                "        plt.ylabel('counts')\r",
                "        plt.xticks(rotation = 90)\r",
                "        plt.title(f'{columnNames[i]} (column {i})')\r",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\r",
                "    plt.show()\r",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\r",
                "def plotCorrelationMatrix(df, graphWidth):",
                "    filename = df.dataframeName\r",
                "    df = df.dropna('columns') # drop columns with NaN\r",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\r",
                "    if df.shape[1] < 2:\r",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\r",
                "        return\r",
                "    corr = df.corr()\r",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\r",
                "    corrMat = plt.matshow(corr, fignum = 1)\r",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\r",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\r",
                "    plt.gca().xaxis.tick_bottom()\r",
                "    plt.colorbar(corrMat)\r",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\r",
                "    plt.show()\r",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\r",
                "def plotScatterMatrix(df, plotSize, textSize):",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\r",
                "    # Remove rows and columns that would lead to df being singular\r",
                "    df = df.dropna('columns')\r",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\r",
                "    columnNames = list(df)\r",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots",
                "        columnNames = columnNames[:10]",
                "    df = df[columnNames]\r",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\r",
                "    corrs = df.corr().values\r",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\r",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\r",
                "    plt.suptitle('Scatter and Density Plot')\r",
                "    plt.show()\r",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file",
                "# bank.csv has 11162 rows in reality, but we are only loading/previewing the first 1000 rows",
                "df1 = pd.read_csv('../input/bank.csv', delimiter=',', nrows = nRowsRead)",
                "df1.dataframeName = 'bank.csv'",
                "nRow, nCol = df1.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df1.head(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotCorrelationMatrix(df1, 8)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotScatterMatrix(df1, 20, 10)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# owid-covid-data (6).csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df1 = pd.read_csv('/kaggle/input/owid-covid-data (6).csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'owid-covid-data (6).csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df1.head(5)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotCorrelationMatrix(df1, 8)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotScatterMatrix(df1, 20, 10)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\r",
                "from sklearn.preprocessing import StandardScaler\r",
                "import matplotlib.pyplot as plt # plotting\r",
                "import numpy as np # linear algebra\r",
                "import os # accessing directory structure\r",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(os.listdir('../input'))",
                "print(os.listdir('../input/ida-csv-zip-350-kb-'))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\r",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "    nunique = df.nunique()\r",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\r",
                "    nRow, nCol = df.shape\r",
                "    columnNames = list(df)\r",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\r",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\r",
                "    for i in range(min(nCol, nGraphShown)):\r",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\r",
                "        columnDf = df.iloc[:, i]\r",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\r",
                "            valueCounts = columnDf.value_counts()\r",
                "            valueCounts.plot.bar()\r",
                "        else:\r",
                "            columnDf.hist()\r",
                "        plt.ylabel('counts')\r",
                "        plt.xticks(rotation = 90)\r",
                "        plt.title(f'{columnNames[i]} (column {i})')\r",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\r",
                "    plt.show()\r",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\r",
                "def plotCorrelationMatrix(df, graphWidth):",
                "    filename = df.dataframeName\r",
                "    df = df.dropna('columns') # drop columns with NaN\r",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\r",
                "    if df.shape[1] < 2:\r",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\r",
                "        return\r",
                "    corr = df.corr()\r",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\r",
                "    corrMat = plt.matshow(corr, fignum = 1)\r",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\r",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\r",
                "    plt.gca().xaxis.tick_bottom()\r",
                "    plt.colorbar(corrMat)\r",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\r",
                "    plt.show()\r",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\r",
                "def plotScatterMatrix(df, plotSize, textSize):",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\r",
                "    # Remove rows and columns that would lead to df being singular\r",
                "    df = df.dropna('columns')\r",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\r",
                "    columnNames = list(df)\r",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots",
                "        columnNames = columnNames[:10]",
                "    df = df[columnNames]\r",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\r",
                "    corrs = df.corr().values\r",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\r",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\r",
                "    plt.suptitle('Scatter and Density Plot')\r",
                "    plt.show()\r",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file",
                "# IDAData.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows",
                "df1 = pd.read_csv('../input/ida-csv-zip-350-kb-/IDAData.csv', delimiter=',', nrows = nRowsRead)",
                "df1.dataframeName = 'IDAData.csv'",
                "nRow, nCol = df1.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df1.head(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file",
                "# IDACountry.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows",
                "df2 = pd.read_csv('../input/ida-csv-zip-350-kb-/IDACountry.csv', delimiter=',', nrows = nRowsRead)",
                "df2.dataframeName = 'IDACountry.csv'",
                "nRow, nCol = df2.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df2.head(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df2, 10, 5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file",
                "# IDASeries.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows",
                "df3 = pd.read_csv('../input/ida-csv-zip-350-kb-/IDASeries.csv', delimiter=',', nrows = nRowsRead)",
                "df3.dataframeName = 'IDASeries.csv'",
                "nRow, nCol = df3.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df3.head(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df3, 10, 5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# IRIS_TYPE_CLF.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df1 = pd.read_csv('/kaggle/input/IRIS_TYPE_CLF.csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'IRIS_TYPE_CLF.csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df1.head(5)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotCorrelationMatrix(df1, 8)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotScatterMatrix(df1, 12, 10)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(os.listdir('../input'))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "print(\"Setup Complete\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# Set up code checking\n",
                "from learntools.core import binder\n",
                "binder.bind(globals())\n",
                "from learntools.data_viz_to_coder.ex2 import *\n",
                "print(\"Setup Complete\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Print the last five rows of the data \n",
                "museum_data.head() # Your code here"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Last 5 rows of the data\n",
                "museum_data.tail()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "list(museum_data.columns)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Function used in this notebook",
                "def count_unique_values(df) :",
                "    var = list(df.columns)",
                "    print('Count unique values :')",
                "    ",
                "    for i in var :",
                "        count = len(df[i].unique())",
                "        print(i,':',count)",
                "",
                "def check_missing_values(df) :",
                "    n = len(df)",
                "    var = list(df.columns)",
                "    missing_var = []",
                "    missing_count = []",
                "    print('Variable with missing values :')",
                "    ",
                "    for i in var :",
                "        count = np.sum(df[i].isna())",
                "        count_percentage = round(count*100/n, 2)",
                "        if count > 0 :",
                "            print(i,':',count,'//',count_percentage,'%')",
                "            missing_var.append(i)",
                "            missing_count.append(count_percentage)",
                "    ",
                "    return missing_var, missing_count",
                "",
                "def stepwise_selection(X, y, ",
                "                       initial_list=[], ",
                "                       threshold_in=0.01, ",
                "                       threshold_out = 0.05, ",
                "                       verbose=True):",
                " ",
                "    included = list(initial_list)",
                "    while True:",
                "        changed=False",
                "        # forward step",
                "        excluded = list(set(X.columns)-set(included))",
                "        new_pval = pd.Series(index=excluded)",
                "        for new_column in excluded:",
                "            model = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))",
                "                                ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()",
                "            new_pval[new_column] = model.pvalues[new_column]",
                "        best_pval = new_pval.min()",
                "        if best_pval < threshold_in:",
                "            best_feature = new_pval.argmin()",
                "            included.append(best_feature)",
                "            changed=True",
                "            if verbose:",
                "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))",
                "",
                "        # backward step",
                "        model = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included]))",
                "                            ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()",
                "        # use all coefs except intercept",
                "        pvalues = model.pvalues.iloc[1:]",
                "        worst_pval = pvalues.max() # null if pvalues is empty",
                "        if worst_pval > threshold_out:",
                "            changed=True",
                "            worst_feature = pvalues.argmax()",
                "            included.remove(worst_feature)",
                "            if verbose:",
                "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))",
                "        if not changed:",
                "            break",
                "    return included",
                "",
                "def dataset_ready(x_train, y_train) :",
                "    # Make dummy variable for categorical variable",
                "    X = pd.get_dummies(x_train)",
                "",
                "    # Make Intercept",
                "    X['Intercept'] = [1]*len(X)",
                "",
                "    # Make interaction between 'gdp_per_capita' and 'population'",
                "    X['gdp_pop'] = np.log(X['gdp_per_capita']*X['population'])",
                "",
                "    # Scale continuous variable with log function",
                "    cont_var = ['gdp_per_capita','population']",
                "    for i in cont_var :",
                "        X[i] = np.log(X[i])",
                "",
                "    # Make interaction between 'continent' and 'gdp'",
                "    col = pd.Series(X.columns)",
                "    var1 = list(X.filter(like='continent').columns)",
                "    for i in var1 :",
                "        string = i+'_gdp'",
                "        X[string] = X[i]*X['gdp_per_capita']   ",
                "",
                "    # Make interaction between 'continent' and 'population'",
                "    for i in var1 :",
                "        string = i+'_population'",
                "        X[string] = X[i]*X['population']  ",
                "",
                "    # Target variable",
                "    Y = y_train",
                "    ",
                "    return X,Y",
                "",
                "# I use stepwise algorith from this link https://datascience.stackexchange.com/questions/24405/how-to-do-stepwise-regression-using-sklearn"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# Load and configure notebook settings",
                "import pandas as pd",
                "import numpy as np",
                "import matplotlib.pyplot as plt",
                "import matplotlib.gridspec as gridspec",
                "import seaborn as sns",
                "import statsmodels.api as sm",
                "%matplotlib inline",
                "",
                "from matplotlib.pylab import rcParams",
                "# For every plotting cell use this",
                "rcParams['figure.figsize'] = [10,5]",
                "",
                "import warnings",
                "warnings.filterwarnings('ignore')",
                "",
                "pd.set_option('display.max_rows', 50)",
                "pd.set_option('display.max_columns', 50)",
                "",
                "sns.set()",
                "sns.set_style('whitegrid')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "# Load dataset",
                "df_train = pd.read_csv('../input/master.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Overview of the dataset",
                "df_train.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Change some variable name",
                "df_train.rename(columns={' gdp_for_year ($) ':'gdp_for_year', 'gdp_per_capita ($)':'gdp_per_capita'}, inplace=True)",
                "df_train_v2 = df_train.copy()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Summary of the dataset",
                "df_train.describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Overview data types of the dataset",
                "print('Data types of the dataset :')",
                "print(df_train.dtypes)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Change 'gdp_for_year' data type",
                "change_var = []",
                "for i in df_train['gdp_for_year'] :",
                "    split = i.split(',')",
                "    val = ''",
                "    for j in split :",
                "        val = val + j",
                "    change_var.append(int(val))",
                "    ",
                "df_train['gdp_for_year'] = change_var"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# See the correlation between the variables",
                "rcParams['figure.figsize'] = [10,5]",
                "sns.heatmap(df_train.corr(), annot=True, linewidths=0.2, cmap='coolwarm' )",
                "plt.title('Correlation heatmap of the dataset', size=15, fontweight='bold') ;",
                "plt.xticks(rotation=45)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Check for missing values in the dataset",
                "missing_var, missing_count = check_missing_values(df_train)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Remove unwanted variable",
                "df_train_v2.drop(columns=['country-year','HDI for year','gdp_for_year'], inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Check how many unique values in categorical variable",
                "category_var = ['country','year','sex','age','generation']",
                "count_unique_values(df_train[category_var])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Check the distribution of target variable 'suicides_no'",
                "rcParams['figure.figsize'] = [15,5]",
                "gs = gridspec.GridSpec(1,2)",
                "ax1 = plt.subplot(gs[0,0])",
                "ax2 = plt.subplot(gs[0,1])",
                "",
                "# Plot 1 - Distribution of the target variable",
                "sns.distplot(df_train['suicides_no'], color='#7f181b', kde=True, hist=False, ax=ax1) ;",
                "ax1.set_title('Distribution of the suicides_no', size=15, fontweight='bold') ;",
                "",
                "# Plot 2 - Distribution of the log(target variable)",
                "sns.distplot(np.log(df_train[df_train['suicides_no']>0]['suicides_no']), color='#7f181b', kde=True, hist=True, ax=ax2) ;",
                "ax2.set_title('Distribution of the log of population', size=15, fontweight='bold') ;"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Make new variable 'continent' that represent continent of each country",
                "# Based on wikipedia.com",
                "country = df_train_v2['country'].unique()",
                "new_val = ['Europe','Central America','South America','Asia','Central America'",
                "          ,'Australia','Europe','Asia','Central America','Asia'",
                "          ,'Central America','Europe','Europe','Central America'",
                "          ,'Europe','South America','Europe','Africa'",
                "          ,'North America','South America','South America','Central America','Europe','Central America'",
                "          ,'Asia','Europe','Europe','Central America','South America'",
                "          ,'Central America','Europe','Oceania','Europe','Europe','Asia'",
                "          ,'Europe','Europe','Central America','Central America','South America','Europe'",
                "          ,'Europe','Europe','Asia','Europe','Central America','Asia'",
                "          ,'Asia','Oceania','Asia','Asia','Europe'",
                "          ,'Europe','Europe','Asia','Asia','Europe'",
                "          ,'Africa','North America','Asia','Europe','Europe'",
                "          ,'Oceania','Central America','Europe','Asia','Central America','South America'",
                "          ,'Asia','Europe','Europe','Central America','Asia'",
                "          ,'Asia','Europe','Europe'",
                "          ,'Central America','Central America'",
                "          ,'Central America','Europe','Europe'",
                "          ,'Africa','Asia','Europe','Europe','Africa'",
                "          ,'Europe','Asia','South America','Europe','Europe'",
                "          ,'Asia','Central America','Asia','Asia'",
                "          ,'Europe','Asia','Europe'",
                "          ,'North America','South America','Asia']",
                "new_var = []",
                "",
                "for i in range(len(country)) :",
                "    n = len(df_train[df_train['country']==country[i]])",
                "    for j in range(n) :",
                "        new_var.append(new_val[i])",
                "        ",
                "df_train_v2['continent'] = new_var"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Top 10 country with highest suicide median",
                "# We use median because the distribution is skewed to the right",
                "df_check = df_train_v2.groupby(by=['country','continent']).median()[['suicides_no','population','gdp_per_capita']].sort_values('suicides_no',ascending=False).reset_index()",
                "cat = list(df_check.head(10)['country'])",
                "print('Top 10 country with highest suicide median ')",
                "print(df_check.head(10))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Top 10 country number of suicides growth year by year",
                "# We use median because the distribution is skewed to the right",
                "df_check = df_train.groupby(by=['country','year']).median()['suicides_no'].reset_index()",
                "rcParams['figure.figsize'] = [10,6]",
                "gs = gridspec.GridSpec(2,1)",
                "ax1 = plt.subplot(gs[0,0])",
                "ax2 = plt.subplot(gs[1,0])",
                "",
                "# Plot 1 - Line plot for top 3 country",
                "for i in cat[:3] :",
                "    sns.lineplot(data=df_check[df_check['country']==i], x='year', y='suicides_no', ax=ax1) ;",
                "    ",
                "ax1.legend(cat[:3], loc=7,  bbox_to_anchor=(1.3, 0.5)) ;",
                "ax1.set_title('Number of suicide growth of top 3 country', size=15, fontweight='bold') ;",
                "",
                "# Plot 2 - Line plot for reminding country",
                "for i in cat[3:] :",
                "    sns.lineplot(data=df_check[df_check['country']==i], x='year', y='suicides_no', ax=ax2) ;",
                "    ",
                "ax2.legend(cat[3:], loc=7,  bbox_to_anchor=(1.3, 0.5)) ;",
                "ax2.set_xlabel('Number of suicide growth of reminding country', size=15, fontweight='bold') ;"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Count how many country in each continent recorded in the dataset",
                "continent = list(df_train_v2['continent'].unique())",
                "new_val = pd.Series(new_val)",
                "count = []",
                "",
                "print('Count contry in each continent recorded in the dataset :')",
                "for i in continent :",
                "    n = len(new_val[new_val==i])",
                "    count.append(n)",
                "    print(i,':',n)",
                " ",
                "#  Plot",
                "df = pd.DataFrame({'continent':continent, 'count':count})",
                "df.sort_values(by='count', ascending=False, inplace=True)",
                "sns.catplot(data=df, x='continent', y='count') ;",
                "plt.xticks(rotation=45)",
                "plt.title('How many country in each continent', size=15, fontweight='bold') ;"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Number of suicides growth in each continent",
                "df_check = df_train_v2.groupby(by=['continent','year']).median()['suicides_no'].reset_index()",
                "rcParams['figure.figsize'] = [10,6]",
                "gs = gridspec.GridSpec(2,1)",
                "ax1 = plt.subplot(gs[0,0])",
                "ax2 = plt.subplot(gs[1,0])",
                "",
                "# Plot 1 - Line plot for North America",
                "sns.lineplot(data=df_check[df_check['continent']=='North America'], x='year', y='suicides_no', ax=ax1 ) ;",
                "ax1.legend(['North America'], loc=7,  bbox_to_anchor=(1.3, 0.5)) ;",
                "ax1.set_title('Number of suicide growth of North America', size=15, fontweight='bold') ;",
                "",
                "# Plot 2 - Line plot for reminding continent",
                "continent = pd.Series(continent)",
                "for i in continent[continent!='North America'] :",
                "    sns.lineplot(data=df_check[df_check['continent']==i], x='year', y='suicides_no', ax=ax2) ;",
                "    ",
                "ax2.legend(continent[continent!='North America'], loc=7,  bbox_to_anchor=(1.3, 0.5)) ;",
                "ax2.set_xlabel('Number of suicide growth of reminding continent', size=15, fontweight='bold') ;"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Check the effect of gender and sex",
                "rcParams['figure.figsize'] = [15,5]",
                "gs = gridspec.GridSpec(1,2)",
                "ax1 = plt.subplot(gs[0,0])",
                "ax2 = plt.subplot(gs[0,1])",
                "",
                "# Plot 1 - based on",
                "sns.barplot(data=df_train_v2, x='sex', y='suicides_no', hue='age', ax=ax1",
                "           ,hue_order=['5-14 years','15-24 years','25-34 years','35-54 years','55-74 years','75+ years']) ;",
                "ax1.set_title('Disrtibution of suicide count by sex', size=15, fontweight='bold') ;",
                "",
                "# Plot 2",
                "sns.barplot(data=df_train_v2, x='sex', y='suicides/100k pop', hue='age', ax=ax2",
                "           ,hue_order=['5-14 years','15-24 years','25-34 years','35-54 years','55-74 years','75+ years']) ;",
                "ax2.set_title('Disrtibution of suicide count (rescale) by sex ', size=15, fontweight='bold') ;"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Check the effect of variable 'generation'",
                "rcParams['figure.figsize'] = [16,5]",
                "gs = gridspec.GridSpec(1,3, width_ratios=[2,8,6])",
                "ax1 = plt.subplot(gs[0,0])",
                "ax2 = plt.subplot(gs[0,1])",
                "ax3 = plt.subplot(gs[0,2])",
                "",
                "# Plot 1 - North America perspective",
                "sns.barplot(data=df_train_v2[df_train_v2['continent']=='North America'], x='continent', y='suicides_no', ax=ax1",
                "           ,hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z'], hue='generation',) ;",
                "ax1.get_legend().remove()",
                "ax1.set_title('(NA)', size=15, fontweight='bold') ;",
                "",
                "# Plot 2 - Europe, South America, Asia, Australia perspective",
                "sns.barplot(data=df_train_v2[df_train_v2['continent'].isin(['Europe','South America','Asia','Australia'])]",
                "            , x='continent', y='suicides_no'",
                "            ,hue='generation', ax=ax2",
                "           ,hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z']) ;",
                "ax2.set_title('Suicide count by generation (E,SA,AS,AUS)', size=15, fontweight='bold') ;",
                "",
                "# Plot 3 - Central America, Oceania, Africa perspective",
                "sns.barplot(data=df_train_v2[df_train_v2['continent'].isin(['Central America','Oceania','Africa'])]",
                "            , x='continent', y='suicides_no'",
                "            ,hue='generation', ax=ax3",
                "           ,hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z']) ;",
                "ax3.get_legend().remove()",
                "ax3.set_title('(CA,O,AF)', size=15, fontweight='bold') ;"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "# Split train and validation set",
                "from sklearn.model_selection import train_test_split",
                "dummy = pd.Series(df_train_v2.columns)",
                "df_train_v3 = df_train_v2[df_train_v2>0].dropna()",
                "",
                "x = dummy[~dummy.isin(['country','suicides_no','suicides/100k pop'])]",
                "y = 'suicides/100k pop'",
                "                      ",
                "x_train, x_valid, y_train, y_valid = train_test_split(df_train_v3[x], df_train_v3[y], test_size=0.2, random_state=11)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Preparing dataset",
                "X, Y = dataset_ready(x_train, y_train)",
                "X2, Y2 = dataset_ready(x_valid, y_valid)",
                "",
                "best_var = stepwise_selection(X,Y, list(X.columns))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "# Generalize Linear Model - Gamma distribution",
                "from statsmodels.tools.eval_measures import rmse",
                "import statsmodels.api as sm",
                "GLM_gamma = sm.genmod.GLM(endog=Y, exog=X[best_var]",
                "                            ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log))",
                "GLM_result = GLM_gamma.fit()",
                "print(GLM_result.summary())",
                "print('Model AIC :',GLM_result.aic)",
                "print('Model BIC :',GLM_result.bic)",
                "print('Model deviance :',GLM_result.deviance)",
                "print('Model RMSE :',rmse(GLM_result.predict(X2[best_var]),Y2))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "# Preparing prediction",
                "var = pd.Series(df_train_v3.columns)",
                "x = var[~var.isin(['country','suicides_no','suicides/100k pop'])]",
                "y = 'suicides/100k pop'",
                "",
                "X3 = df_train_v3[x]",
                "Y3 = df_train_v3[y]",
                "",
                "# Input data for prediction",
                "nx = len(X3)",
                "ny = len(Y3)",
                "X3.loc[nx+1] = [2016, 'male', '25-34 years', 21845000, 57588, 'Millenials', 'North America']",
                "X3.loc[nx+2] = [2016, 'female', '25-34 years', 21917000, 57588, 'Millenials', 'North America']",
                "X3.loc[nx+3] = [2016, 'male', '35-54 years', 40539000, 57588, 'Generation X', 'North America']",
                "X3.loc[nx+4] = [2016, 'female', '35-54 years', 42031000, 57588, 'Generation X', 'North America']",
                "X3.loc[nx+5] = [2016, 'male', '15-24 years', 21719000, 57588, 'Millenials', 'North America']",
                "X3.loc[nx+6] = [2016, 'female', '15-24 years', 21169000, 57588, 'Millenials', 'North America']",
                "Y3.loc[ny+1] = 26.95",
                "Y3.loc[ny+2] = 6.75",
                "Y3.loc[ny+3] = 28.35",
                "Y3.loc[ny+4] = 9.46",
                "Y3.loc[ny+5] = 21.06",
                "Y3.loc[ny+6] = 5.42",
                "",
                "# Tranform the data to be ready for precition",
                "X3,Y3 = dataset_ready(X3, Y3)",
                "X3 = X3.loc[nx+1:nx+6]",
                "Y3 = Y3.loc[ny+1:nx+6]",
                "",
                "# Predict",
                "predict = GLM_result.predict(X3[best_var])",
                "for i in range(len(predict)) :",
                "    print('Option',i+1)",
                "    print('Predicted suicide rates :',round(predict.iloc[i],2))",
                "    print('Actual suicide rates :',Y3.iloc[i])",
                "    print('')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Setting package umum ",
                "import pandas as pd",
                "import pandas_profiling as pp",
                "import numpy as np",
                "import matplotlib.pyplot as plt",
                "import matplotlib.gridspec as gridspec",
                "import seaborn as sns",
                "from tqdm import tqdm_notebook as tqdm",
                "import time",
                "import tensorflow as tf",
                "%matplotlib inline",
                "",
                "from matplotlib.pylab import rcParams",
                "# For every plotting cell use this",
                "# grid = gridspec.GridSpec(n_row,n_col)",
                "# ax = plt.subplot(grid[i])",
                "# fig, axes = plt.subplots()",
                "rcParams['figure.figsize'] = [10,5]",
                "plt.style.use('fivethirtyeight') ",
                "sns.set_style('whitegrid')",
                "",
                "import warnings",
                "warnings.filterwarnings('ignore')",
                "from tqdm import tqdm",
                "",
                "pd.set_option('display.max_rows', 50)",
                "pd.set_option('display.max_columns', 50)",
                "pd.options.display.float_format = '{:.5f}'.format",
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Try Keras",
                "from keras import Sequential",
                "from keras.layers import Dense",
                "",
                "# Define model",
                "model = Sequential([Dense(units=1, input_shape=[1])])",
                "",
                "# Compile model",
                "model.compile(optimizer='sgd', loss='mean_squared_error')",
                "",
                "# Define data",
                "xs = np.array([-1, 0, 1, 2, 3, 4], dtype=float)",
                "ys = np.array([-3, -1, 1, 3, 5, 7], dtype=float)",
                "",
                "# Train model",
                "model.fit(xs, ys, epochs=500)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "### Predict",
                "model.predict([6])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "process_data"
            ],
            "content": [
                "### House price",
                "from keras import Sequential",
                "from keras.layers import Dense",
                "",
                "# Define model",
                "model = Sequential()",
                "model.add(Dense(units=1, input_shape=[1]))",
                "",
                "# Compile model",
                "model.compile(optimizer='sgd', loss='mean_squared_error')",
                "",
                "# Define data",
                "xs = np.array([0,1,2,3,4,5,6], dtype=float)",
                "ys = np.array([0.5,1,1.5,2,2.5,3,3.5], dtype=float)",
                "",
                "# Train model",
                "model.fit(xs, ys, epochs=500)",
                "",
                "# Predict",
                "model.predict([7])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "### Load data",
                "from keras.datasets import fashion_mnist",
                "(train_img, train_lab), (test_img, test_lab) = fashion_mnist.load_data()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "### Plot one image",
                "plt.imshow(train_img[11]) ;"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### Normalize data",
                "train_img = train_img / 225",
                "test_img = test_img / 225"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "validate_data"
            ],
            "content": [
                "### Make keras model",
                "import tensorflow as tf",
                "from keras import Sequential",
                "from keras.layers import Flatten,Dense",
                "import time",
                "",
                "start = time.time()",
                "",
                "# Set early stopping",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "    ",
                "    def __init__(self, threshold) :",
                "        self.thres = threshold",
                "        ",
                "    def on_epoch_end(self, epoch, logs={}) :",
                "        if (logs.get('accuracy')>self.thres) :",
                "            print('\\nReached',self.thres*100,'Accuracy so stop train!')",
                "            self.model.stop_training=True",
                "            ",
                "callbacks = EarlyStop(0.9)",
                "",
                "# Define model",
                "model = Sequential()",
                "model.add(Flatten(input_shape=(28,28)))",
                "model.add(Dense(512, activation=tf.nn.relu))",
                "model.add(Dense(512, activation=tf.nn.relu))",
                "model.add(Dense(10, activation=tf.nn.softmax))",
                "",
                "# Compile model",
                "model.compile(optimizer=tf.optimizers.Adam(),",
                "              loss='sparse_categorical_crossentropy',",
                "              metrics=['accuracy'])",
                "",
                "# Train model",
                "model.fit(train_img, train_lab, epochs=10, callbacks=[callbacks])",
                "",
                "end = time.time()",
                "print('Time Used :',(end-start)/60)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "### Evaluate model",
                "model.evaluate(test_img, test_lab)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "### Predict ",
                "pred = model.predict(test_img)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "### See the predicted label",
                "np.argmax(pred[0])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "### Load data",
                "from keras.datasets import mnist",
                "(train_img, train_lab), (test_img, test_lab) = mnist.load_data()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### Normalize data",
                "train_img = train_img / 225",
                "test_img = test_img / 225"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "### See the shape of the data",
                "print(train_img.shape)",
                "print(len(set(train_lab)))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "validate_data"
            ],
            "content": [
                "### Make keras model",
                "import tensorflow as tf",
                "from keras import Sequential",
                "from keras.layers import Flatten,Dense",
                "import time",
                "",
                "start = time.time()",
                "",
                "# Set early stopping",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "    ",
                "    def __init__(self, threshold) :",
                "        self.thres = threshold",
                "        ",
                "    def on_epoch_end(self, epoch, logs={}) :",
                "        if (logs.get('accuracy')>self.thres) :",
                "            print('\\nReached',self.thres*100,'Accuracy so stop train!')",
                "            self.model.stop_training=True",
                "            ",
                "callbacks = EarlyStop(0.99)",
                "",
                "# Define model",
                "model = Sequential()",
                "model.add(Flatten(input_shape=(28,28)))",
                "model.add(Dense(512, activation=tf.nn.relu))",
                "model.add(Dense(512, activation=tf.nn.relu))",
                "model.add(Dense(10, activation=tf.nn.softmax))",
                "",
                "# Compile model",
                "model.compile(optimizer=tf.optimizers.Adam(),",
                "              loss='sparse_categorical_crossentropy',",
                "              metrics=['accuracy'])",
                "",
                "# Train model",
                "model.fit(train_img, train_lab, epochs=10, callbacks=[callbacks])",
                "",
                "end = time.time()",
                "print('Time Used :',(end-start)/60)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### Download the data",
                "!wget --no-check-certificate \\",
                "  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\",
                "  -O /tmp/cats_and_dogs_filtered.zip"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "### Unzip the data",
                "import zipfile",
                "import time",
                "",
                "start = time.time()",
                "",
                "data_zip = '/tmp/cats_and_dogs_filtered.zip'",
                "data_ref = zipfile.ZipFile(data_zip, 'r')",
                "data_ref.extractall()",
                "data_ref.close()",
                "print('Unzip Data Completed')",
                "",
                "end = time.time()",
                "print('Time Used :',(end-start)/60)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### Set directory path for data train and validation",
                "import os",
                "base_dir = '/kaggle/working/cats_and_dogs_filtered'",
                "train_dir = os.path.join(base_dir, 'train')",
                "val_dir = os.path.join(base_dir, 'validation')",
                "",
                "# Directory with our training cat/dog pictures",
                "train_cats_dir = os.path.join(train_dir, 'cats')",
                "train_dogs_dir = os.path.join(train_dir, 'dogs')",
                "",
                "# Directory with our validation cat/dog pictures",
                "val_cats_dir = os.path.join(val_dir, 'cats')",
                "val_dogs_dir = os.path.join(val_dir, 'dogs')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### Get the name of the file for train and validation",
                "train_cat_fn = os.listdir(train_cats_dir)",
                "train_dog_fn = os.listdir(train_dogs_dir)",
                "",
                "val_cat_fn = os.listdir(val_cats_dir)",
                "val_dog_fn = os.listdir(val_dogs_dir)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "ingest_data"
            ],
            "content": [
                "### Image example",
                "import matplotlib.image as mpimg",
                "rcParams['figure.figsize'] = [10,5]",
                "plt.style.use('fivethirtyeight') ",
                "sns.set_style('whitegrid')",
                "grid = gridspec.GridSpec(1,2)",
                "",
                "# Cat image",
                "ax = plt.subplot(grid[0])",
                "cat_img = mpimg.imread(os.path.join(train_cats_dir, train_cat_fn[0]))",
                "ax.imshow(cat_img)",
                "ax.axis('off') ;",
                "",
                "# Dog image",
                "ax = plt.subplot(grid[1])",
                "dog_img = mpimg.imread(os.path.join(train_dogs_dir, train_dog_fn[0]))",
                "ax.imshow(dog_img)",
                "ax.axis('off') ;"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Preprocess the image",
                "from keras.preprocessing.image import ImageDataGenerator",
                "",
                "# Define generator",
                "train_datagen = ImageDataGenerator(rescale=1/255)",
                "val_datagen = ImageDataGenerator(rescale=1/255)",
                "",
                "# Define flow for train gen",
                "train_gen = train_datagen.flow_from_directory(train_dir,",
                "                                              batch_size=20,",
                "                                              class_mode='binary',",
                "                                              target_size=(150,150))",
                "",
                "# Define flow for validation gen",
                "val_gen = val_datagen.flow_from_directory(val_dir,",
                "                                          batch_size=20,",
                "                                          class_mode='binary',",
                "                                          target_size=(150,150))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "validate_data"
            ],
            "content": [
                "### Make keras model",
                "import tensorflow as tf",
                "import keras",
                "from keras import Sequential",
                "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense",
                "import time",
                "",
                "start = time.time()",
                "",
                "# Set early stopping",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "    ",
                "    def __init__(self, threshold) :",
                "        self.thres = threshold",
                "        ",
                "    def on_epoch_end(self, epoch, logs={}) :",
                "        if (logs.get('val_accuracy')>self.thres) :",
                "            print('\\nReached',self.thres*100,' Validation Accuracy so stop train!')",
                "            self.model.stop_training=True",
                "            ",
                "callbacks = EarlyStop(0.72)",
                "",
                "# Define model",
                "model = Sequential()",
                "model.add(Conv2D(16, (3,3), activation='relu', input_shape=(150,150,3)))",
                "model.add(MaxPooling2D(2,2))",
                "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))",
                "model.add(MaxPooling2D(2,2))",
                "model.add(Conv2D(64, (3,3), activation='relu', input_shape=(150,150,3)))",
                "model.add(MaxPooling2D(2,2))",
                "",
                "model.add(Flatten())",
                "model.add(Dense(512, activation='relu'))",
                "model.add(Dense(1, activation='sigmoid'))",
                "",
                "# Compile model",
                "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),",
                "              loss='binary_crossentropy',",
                "              metrics=['accuracy'])",
                "",
                "# Train model",
                "history = model.fit_generator(train_gen, ",
                "                              validation_data=val_gen,",
                "                              steps_per_epoch=100,",
                "                              epochs=20,",
                "                              validation_steps=50,",
                "                              verbose=1,",
                "                              callbacks=[callbacks])",
                "",
                "end = time.time()",
                "print('Time Used :',(end-start)/60)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "### Try predict some image",
                "from keras.preprocessing.image import load_img, img_to_array",
                "from random import sample",
                "list_cat_fn = [os.path.join(val_cats_dir, fn) for fn in sample(val_cat_fn, 5)]",
                "list_dog_fn = [os.path.join(val_dogs_dir, fn) for fn in sample(val_dog_fn, 5)]",
                "list_fn = list_cat_fn + list_dog_fn",
                "",
                "# Iteration to predict",
                "for fn in list_fn :",
                "    ",
                "    # Load and preprocess image",
                "    img = load_img(fn, target_size=(150,150))",
                "    vect_img = img_to_array(img)",
                "    vect_img = np.expand_dims(vect_img, axis=0)",
                "    ready_img = np.vstack([vect_img])",
                "    ",
                "    # Predict",
                "    classes = model.predict(ready_img, batch_size=10)",
                "    if classes == 0 :",
                "        print(fn,'is a cat')",
                "    else :",
                "        print(fn,'is a dog')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "### Representation of the image based on model",
                "from keras.preprocessing.image import load_img, img_to_array",
                "import keras",
                "from random import choice",
                "list_cat_fn = [os.path.join(val_cats_dir, fn) for fn in sample(val_cat_fn, 5)]",
                "list_dog_fn = [os.path.join(val_dogs_dir, fn) for fn in sample(val_dog_fn, 5)]",
                "list_fn = list_cat_fn + list_dog_fn",
                "succ_out = [layer.output for layer in model.layers[1:]]",
                "",
                "# Define viz model",
                "viz_model = keras.models.Model(inputs=model.input, outputs=succ_out)",
                "",
                "# Pick one image randomly",
                "img = load_img(choice(list_fn), target_size=(150,150))",
                "x = img_to_array(img)",
                "x   = x.reshape((1,) + x.shape)  ",
                "x = x / 255",
                "",
                "# Predicting",
                "successive_feature_maps = viz_model.predict(x)",
                "",
                "# List of layer name",
                "layer_names = [layer.name for layer in model.layers]",
                "",
                "# Plot the visualization on each layer",
                "for layer_name, feature_map in zip(layer_names, successive_feature_maps):",
                "  ",
                "  if len(feature_map.shape) == 4:",
                "    ",
                "    #-------------------------------------------",
                "    # Just do this for the conv / maxpool layers, not the fully-connected layers",
                "    #-------------------------------------------",
                "    n_features = feature_map.shape[-1]  # number of features in the feature map",
                "    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)",
                "    ",
                "    # We will tile our images in this matrix",
                "    display_grid = np.zeros((size, size * n_features))",
                "    ",
                "    #-------------------------------------------------",
                "    # Postprocess the feature to be visually palatable",
                "    #-------------------------------------------------",
                "    for i in range(n_features):",
                "      x  = feature_map[0, :, :, i]",
                "      x -= x.mean()",
                "      x /= x.std ()",
                "      x *=  64",
                "      x += 128",
                "      x  = np.clip(x, 0, 255).astype('uint8')",
                "      display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid",
                "",
                "    #-----------------",
                "    # Display the grid",
                "    #-----------------",
                "",
                "    scale = 20. / n_features",
                "    plt.figure( figsize=(scale * n_features, scale) )",
                "    plt.title ( layer_name )",
                "    plt.grid  ( False )",
                "    plt.imshow( display_grid, aspect='auto', cmap='viridis' ) "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "### Plot the performance of the model",
                "rcParams['figure.figsize'] = [10,8]",
                "plt.style.use('fivethirtyeight') ",
                "sns.set_style('whitegrid')",
                "grid = gridspec.GridSpec(2,1)",
                "",
                "# Set the variable ",
                "acc      = history.history[     'accuracy' ]",
                "val_acc  = history.history[ 'val_accuracy' ]",
                "loss     = history.history[    'loss' ]",
                "val_loss = history.history['val_loss' ]",
                "epo      = list(range(len(acc)))",
                "",
                "# Plot the loss",
                "ax = plt.subplot(grid[0])",
                "ax.plot(epo, loss, label='Train Loss')",
                "ax.plot(epo, val_loss, label='Validation Loss')",
                "ax.set_title('Training and Validation Loss')",
                "ax.legend() ;",
                "",
                "# Plot the loss",
                "ax = plt.subplot(grid[1])",
                "ax.plot(epo, acc, label='Train Acc')",
                "ax.plot(epo, val_acc, label='Validation Acc')",
                "ax.set_title('Training and Validation Accuracy')",
                "ax.legend() ;",
                "",
                "plt.tight_layout()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#-----------------------------------------------------------",
                "# Retrieve a list of list results on training and test data",
                "# sets for each training epoch",
                "#-----------------------------------------------------------",
                "acc      = history.history[     'accuracy' ]",
                "val_acc  = history.history[ 'val_accuracy' ]",
                "loss     = history.history[    'loss' ]",
                "val_loss = history.history['val_loss' ]",
                "",
                "epochs   = range(len(acc)) # Get number of epochs",
                "",
                "#------------------------------------------------",
                "# Plot training and validation accuracy per epoch",
                "#------------------------------------------------",
                "plt.plot  ( epochs,     acc )",
                "plt.plot  ( epochs, val_acc )",
                "plt.title ('Training and validation accuracy')",
                "plt.figure()",
                "",
                "#------------------------------------------------",
                "# Plot training and validation loss per epoch",
                "#------------------------------------------------",
                "plt.plot  ( epochs,     loss )",
                "plt.plot  ( epochs, val_loss )",
                "plt.title ('Training and validation loss'   )"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "### Load full data",
                "import zipfile",
                "import time",
                "",
                "# Unzip train data",
                "start = time.time()",
                "",
                "train_zip = '/kaggle/input/dogs-vs-cats/train.zip'",
                "train_ref = zipfile.ZipFile(train_zip, 'r')",
                "train_ref.extractall()",
                "train_ref.close()",
                "print('Unzip Train Completed')",
                "",
                "end = time.time()",
                "print('Time Used :',(end-start)/60)",
                "",
                "# Unzip test data",
                "start = time.time()",
                "",
                "test_zip = '/kaggle/input/dogs-vs-cats/test1.zip'",
                "test_ref = zipfile.ZipFile(test_zip, 'r')",
                "test_ref.extractall()",
                "test_ref.close()",
                "print('Unzip Test Completed')",
                "",
                "end = time.time()",
                "print('Time Used :',(end-start)/60)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "transfer_results"
            ],
            "content": [
                "### Make directory for ImageGenerator",
                "import os",
                "TRAINING_DIR = '/kaggle/training/'",
                "TRAINING_CAT_DIR = '/kaggle/training/cat/'",
                "TRAINING_DOG_DIR = '/kaggle/training/dog/'",
                "os.mkdir(TRAINING_DIR)",
                "os.mkdir(TRAINING_CAT_DIR)",
                "os.mkdir(TRAINING_DOG_DIR)",
                "",
                "VALIDATION_DIR = '/kaggle/validation/'",
                "VALIDATION_CAT_DIR = '/kaggle/validation/cat/'",
                "VALIDATION_DOG_DIR = '/kaggle/validation/dog/'",
                "os.mkdir(VALIDATION_DIR)",
                "os.mkdir(VALIDATION_CAT_DIR)",
                "os.mkdir(VALIDATION_DOG_DIR)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### List file name in train dataset",
                "SOURCE_DIR = '/kaggle/working/train/'",
                "list_fn = os.listdir(SOURCE_DIR)",
                "list_cat_fn = [fn for fn in list_fn if 'cat' in fn]",
                "list_dog_fn = [fn for fn in list_fn if 'dog' in fn]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "### Split data",
                "import random",
                "from shutil import copyfile",
                "SPLIT_PROP = 0.8",
                "list_class = ['cat','dog']",
                "list_name_fn = [list_cat_fn, list_dog_fn]",
                "",
                "for i,c in enumerate(tqdm(list_class)) :",
                "            ",
                "    # Splitting",
                "    list_class_fn = list_name_fn[i]",
                "    pure_random = random.sample(list_class_fn, len(list_class_fn))",
                "    random_train = pure_random[:int(SPLIT_PROP*len(pure_random))]",
                "    random_val = pure_random[int(SPLIT_PROP*len(pure_random)):]",
                "    ",
                "    # Insert into new train dir",
                "    TRAIN_CLASS_DIR = os.path.join(TRAINING_DIR,c)",
                "    for f in random_train :",
                "        copyfile(os.path.join(SOURCE_DIR,f), os.path.join(TRAIN_CLASS_DIR,f))",
                "        ",
                "    # Insert into new valid dir",
                "    VALID_CLASS_DIR = os.path.join(VALIDATION_DIR,c)",
                "    for f in random_val :",
                "        copyfile(os.path.join(SOURCE_DIR,f), os.path.join(VALID_CLASS_DIR,f))",
                "        ",
                "    del pure_random, random_train, random_val"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "# Make pre train model",
                "from tensorflow.keras.applications.resnet50 import ResNet50",
                "from tensorflow.keras.applications.resnet50 import preprocess_input,decode_predictions",
                "",
                "HEIGHT = 150",
                "WIDTH = 150",
                "base_model = ResNet50(weights='imagenet', ",
                "                 include_top=False, ",
                "                 input_shape=(HEIGHT, WIDTH, 3))",
                "",
                "prec_input = tf.keras.applications.resnet50.preprocess_input",
                "decode = tf.keras.applications.resnet50.decode_predictions"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### Define parameter",
                "BATCH_SIZE=10",
                "EPOCHS = 10"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Preprocess the image",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator",
                "",
                "# Define generator",
                "train_datagen = ImageDataGenerator(rescale=1/255, preprocessing_function=prec_input)",
                "val_datagen = ImageDataGenerator(rescale=1/255, preprocessing_function=prec_input)",
                "",
                "# Define flow for train gen",
                "train_gen = train_datagen.flow_from_directory(TRAINING_DIR,",
                "                                              batch_size=BATCH_SIZE,",
                "                                              interpolation='bicubic',",
                "                                              class_mode='categorical',",
                "                                              shuffle=True,",
                "                                              target_size=(HEIGHT, WIDTH))",
                "",
                "# Define flow for validation gen",
                "val_gen = val_datagen.flow_from_directory(VALIDATION_DIR,",
                "                                          batch_size=BATCH_SIZE,",
                "                                          interpolation='bicubic',",
                "                                          class_mode='categorical',",
                "                                          shuffle=False,",
                "                                          target_size=(HEIGHT, WIDTH))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Add layer and dont train the layer before",
                "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "from tensorflow.keras.activations import swish",
                "fc_layers = []",
                "num_classes = 2",
                "dropout = 0.2",
                "",
                "def make_model() :",
                "    ",
                "    # Freeze layer",
                "    for l in base_model.layers :",
                "        l.trainable = False",
                "",
                "    # Make new model",
                "    model = Sequential()",
                "    model.add(base_model)",
                "    model.add(GlobalAveragePooling2D())",
                "    model.add(Flatten())",
                "    for fc in fc_layers:",
                "",
                "        # New FC layer, random init",
                "        model.add(Dense(fc, activation=swish))",
                "        model.add(Dropout(dropout))",
                "",
                "    model.add(Dense(num_classes, activation='softmax'))",
                "",
                "    # Compile model",
                "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['acc'])",
                "    ",
                "    return model",
                "",
                "",
                "model = make_model()",
                "model.summary()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "history = model.fit_generator(train_gen,",
                "                              epochs=EPOCHS,",
                "                              verbose=1,",
                "                              validation_data=val_gen)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "# DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS",
                "# USE AT LEAST 3 CONVOLUTION LAYERS",
                "model = tf.keras.models.Sequential([",
                "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),",
                "    tf.keras.layers.MaxPooling2D(2,2),",
                "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),",
                "    tf.keras.layers.MaxPooling2D(2,2), ",
                "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), ",
                "    tf.keras.layers.MaxPooling2D(2,2),",
                "    tf.keras.layers.Flatten(), ",
                "    tf.keras.layers.Dense(512, activation='relu'), ",
                "    tf.keras.layers.Dense(2, activation='softmax')  ",
                "",
                "# YOUR CODE HERE",
                "])",
                "",
                "model.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['acc'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "history = model.fit_generator(train_gen,",
                "                              epochs=EPOCHS,",
                "                              verbose=1,",
                "                              validation_data=val_gen)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### Get data",
                "!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip -O /tmp/horse-or-human.zip",
                "!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip -O /tmp/validation-horse-or-human.zip "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "### Extract the zip file",
                "import zipfile",
                "import time",
                "",
                "# Unzip train data",
                "data_zip = '/tmp/horse-or-human.zip'",
                "data_ref = zipfile.ZipFile(data_zip, 'r')",
                "data_ref.extractall('/training')",
                "data_ref.close()",
                "print('Unzip Train Data Completed')",
                "",
                "# Unzip valid data",
                "valid_zip = '/tmp/validation-horse-or-human.zip'",
                "valid_ref = zipfile.ZipFile(valid_zip, 'r')",
                "valid_ref.extractall('/validating')",
                "valid_ref.close()",
                "print('Unzip Valid Data Completed')",
                "",
                "%time"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "### Check the proportion of classes in train and valid dataset",
                "TRAIN_HORSE_DIR = '/training/horses'",
                "TRAIN_HUMAN_DIR = '/training/humans'",
                "VAL_HORSE_DIR = '/validating/horses'",
                "VAL_HUMAN_DIR = '/validating/humans'",
                "",
                "print('Total obs on horse class in train :',len(os.listdir(TRAIN_HORSE_DIR)))",
                "print('Total obs on human class in train :',len(os.listdir(TRAIN_HUMAN_DIR)))",
                "print('Total obs on horse class in validation :',len(os.listdir(VAL_HORSE_DIR)))",
                "print('Total obs on human class in validation :',len(os.listdir(VAL_HUMAN_DIR)))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "### Define pretrained model",
                "from tensorflow.keras.applications.inception_v3 import InceptionV3",
                "from tensorflow.keras.applications.inception_v3 import preprocess_input,decode_predictions",
                "BATCH_SIZE = 20",
                "INPUT_SIZE = (150,150)",
                "",
                "# Get local weight",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\",
                "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5",
                "    ",
                "# Define inception model",
                "base_model = InceptionV3(weights=None, ",
                "                         include_top=False, ",
                "                         input_shape=(INPUT_SIZE[0], INPUT_SIZE[1], 3))",
                "",
                "# Load local weight",
                "local_weight_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'",
                "base_model.load_weights(local_weight_file)",
                "",
                "# Freeze all layer",
                "base_model.trainable = False",
                "",
                "# Summary of the model",
                "base_model.summary()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "### Get specific layer from pre-trained model ",
                "last_layer = base_model.get_layer('mixed7')",
                "last_output = last_layer.output",
                "",
                "print('Last layer shape :',last_layer.output_shape)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Set the image generator",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator",
                "TRAIN_DIR = '/training'",
                "VALID_DIR = '/validating'",
                "",
                "# Make image generator for training dataset",
                "train_datagen = ImageDataGenerator(rescale = 1./255.,",
                "                                   rotation_range = 40,",
                "                                   width_shift_range = 0.2,",
                "                                   height_shift_range = 0.2,",
                "                                   shear_range = 0.2,",
                "                                   zoom_range = 0.2,",
                "                                   horizontal_flip = True)",
                "",
                "train_gen = train_datagen.flow_from_directory(TRAIN_DIR,",
                "                                              batch_size = BATCH_SIZE,",
                "                                              class_mode = 'binary', ",
                "                                              target_size = INPUT_SIZE)  ",
                "",
                "# Make image generator for validation dataset",
                "val_datagen = ImageDataGenerator(rescale = 1/255. )",
                "",
                "val_gen =  val_datagen.flow_from_directory(VALID_DIR,",
                "                                            batch_size  = BATCH_SIZE,",
                "                                            class_mode  = 'binary', ",
                "                                            target_size = INPUT_SIZE)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Define new model",
                "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "from tensorflow.keras.activations import swish\\",
                "",
                "# model = Sequential()",
                "# model.add(base_model)",
                "# model.add(Flatten())",
                "# model.add(Dense(1024, activation='relu'))",
                "# model.add(Dropout(0.2))",
                "# model.add(Dense(1, activation='sigmoid'))",
                "",
                "# Use this when you not using the whole pre trained model",
                "x = Flatten()(last_output)",
                "x = Dense(1024, activation='relu')(x)",
                "x = Dropout(0.2)(x)",
                "x = Dense(1, activation='sigmoid')(x)",
                "model = Model(base_model.input, x)",
                "",
                "# Compile model",
                "model.compile(optimizer = RMSprop(lr=0.0001),",
                "              loss = 'binary_crossentropy',",
                "              metrics = 'accuracy')",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "### Define early callback based on metrics",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "    ",
                "    def __init__(self, threshold) :",
                "        self.thres = threshold",
                "        ",
                "    def on_epoch_end(self, epoch, logs={}) :",
                "        if (logs.get('accuracy')>self.thres) :",
                "            print('\\nReached',self.thres*100,'Accuracy so stop train!')",
                "            self.model.stop_training=True",
                "            ",
                "callbacks = EarlyStop(0.99)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Train model",
                "EPOCHS = 99",
                "history = model.fit_generator(train_gen,",
                "                              validation_data=val_gen,",
                "                              steps_per_epoch=100,",
                "                              validation_steps=50,",
                "                              epochs=EPOCHS,",
                "                              callbacks=callbacks,",
                "                              verbose=1)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data",
                "transfer_results"
            ],
            "content": [
                "### Function to read the data",
                "def get_data(path) :",
                "    # Read the csv",
                "    df = pd.read_csv(path)",
                "    ",
                "    # Get all the label",
                "    from tensorflow.keras.utils import to_categorical",
                "    label = to_categorical(np.array(df['label']))",
                "    ",
                "    # Get array of image",
                "    image = []",
                "    for i in range(len(df)) :",
                "        split_img = np.array(np.split(df.iloc[i,1:].values, 28))",
                "        image.append(split_img)",
                "    ",
                "    return np.array(image), label",
                "    "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "### Get data",
                "train_img, train_label = get_data('../input/sign-language-mnist/sign_mnist_train.csv')",
                "val_img, val_label = get_data('../input/sign-language-mnist/sign_mnist_test.csv')",
                "",
                "%time"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "### Shape of the data",
                "print('Train image shape :',train_img.shape)",
                "print('Train label shape :',train_label.shape)",
                "print('Valid image shape :',val_img.shape)",
                "print('Valid label shape :',val_label.shape)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### Make the image generator",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator",
                "BATCH_SIZE = 32",
                "INPUT_SIZE = (28,28)",
                "TRAIN_DIR = '/training'",
                "VALID_DIR = '/validating'",
                "",
                "# Make image generator for training dataset",
                "train_img = np.expand_dims(train_img, 3)",
                "train_datagen = ImageDataGenerator(rescale = 1./255.,",
                "                                   rotation_range = 40,",
                "                                   width_shift_range = 0.2,",
                "                                   height_shift_range = 0.2,",
                "                                   shear_range = 0.2,",
                "                                   zoom_range = 0.2,",
                "                                   horizontal_flip = True)",
                "",
                "train_gen = train_datagen.flow(x=train_img, y=train_label,",
                "                              batch_size = BATCH_SIZE)  ",
                "",
                "# Make image generator for validation dataset",
                "val_img = np.expand_dims(val_img, 3)",
                "val_datagen = ImageDataGenerator(rescale = 1/255. )",
                "",
                "val_gen = val_datagen.flow(x=val_img, y=val_label,",
                "                              batch_size = BATCH_SIZE) "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Define new model",
                "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, MaxPooling2D, Conv2D",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "from tensorflow.keras.activations import swish\\",
                "",
                "model = Sequential()",
                "model.add(Conv2D(64, (3,3), activation='relu', input_shape=(INPUT_SIZE[0], INPUT_SIZE[1], 1)))",
                "model.add(MaxPooling2D(2,2))",
                "model.add(Conv2D(128, (3,3), activation='relu'))",
                "model.add(MaxPooling2D(2,2))",
                "model.add(Flatten())",
                "model.add(Dense(256, activation='relu'))",
                "model.add(Dropout(0.2))",
                "model.add(Dense(25, activation='softmax'))",
                "",
                "# Compile model",
                "model.compile(optimizer = Adam(),",
                "              loss = 'categorical_crossentropy',",
                "              metrics = 'accuracy')",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "### Define early callback based on metrics",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "    ",
                "    def __init__(self, threshold) :",
                "        self.thres = threshold",
                "        ",
                "    def on_epoch_end(self, epoch, logs={}) :",
                "        if (logs.get('val_accuracy')>self.thres) :",
                "            print('\\nReached',self.thres*100,'Accuracy so stop train!')",
                "            self.model.stop_training=True",
                "            ",
                "callbacks = EarlyStop(0.9)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Train model",
                "EPOCHS = 50",
                "history = model.fit_generator(train_gen,",
                "                              validation_data=val_gen,",
                "                              steps_per_epoch=train_img.shape[0] // BATCH_SIZE,",
                "                              validation_steps=val_img.shape[0] // BATCH_SIZE,",
                "                              epochs=EPOCHS,",
                "                              callbacks=callbacks,",
                "                              verbose=1)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "### Evaluate model",
                "model.evaluate(val_img, val_label)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "### Plot the performance of the model",
                "rcParams['figure.figsize'] = [10,8]",
                "plt.style.use('fivethirtyeight') ",
                "sns.set_style('whitegrid')",
                "grid = gridspec.GridSpec(2,1)",
                "",
                "# Get the metrics and loss",
                "acc      = history.history[     'accuracy' ]",
                "val_acc  = history.history[ 'val_accuracy' ]",
                "loss     = history.history[    'loss' ]",
                "val_loss = history.history['val_loss' ]",
                "epo   = range(len(acc)) # Get number of epochs",
                "",
                "# Plot the loss",
                "ax = plt.subplot(grid[0])",
                "ax.plot(epo, loss, label='Train Loss')",
                "ax.plot(epo, val_loss, label='Validation Loss')",
                "ax.set_title('Training and Validation Loss')",
                "ax.legend() ;",
                "",
                "# Plot the acccuracy",
                "ax = plt.subplot(grid[1])",
                "ax.plot(epo, acc, label='Train Acc')",
                "ax.plot(epo, val_acc, label='Validation Acc')",
                "ax.set_title('Training and Validation Accuracy')",
                "ax.legend() ;"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### Get data",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv \\",
                "    -O /tmp/bbc-text.csv",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "### Load data",
                "data = pd.read_csv('/tmp/bbc-text.csv')",
                "label = list(data['category'])",
                "sentences_raw = list(data['text'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "### Print the first expected output",
                "print(len(sentences_raw))",
                "print(sentences_raw[0])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "### Remove stopwords from data",
                "from nltk.tokenize import word_tokenize",
                "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]",
                "sentences = []",
                "",
                "for t in tqdm(sentences_raw) :",
                "    tokenize_text = word_tokenize(t)",
                "    list_text = [i for i in tokenize_text if i not in stopwords]",
                "    sentences.append((\" \").join(list_text))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "validate_data"
            ],
            "content": [
                "### Tokenize the text data",
                "from tensorflow.keras.preprocessing.text import Tokenizer",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences",
                "tokenizer = Tokenizer(oov_token='<OOV>',",
                "                      #num_words to specify maximum number of token based on frequency",
                "                     )",
                "",
                "# Get the token dict from data",
                "tokenizer.fit_on_texts(sentences)",
                "word_index = tokenizer.word_index",
                "print(len(word_index))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### Pad the text data",
                "sequences = tokenizer.texts_to_sequences(sentences)",
                "padded = pad_sequences(sequences, padding='post', truncating='pre', maxlen=None)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "### Print the second output",
                "print(padded[0])",
                "print(padded.shape)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "### Tokenize the label data",
                "from tensorflow.keras.preprocessing.text import Tokenizer",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences",
                "tokenizer_label = Tokenizer()",
                "",
                "# Get the token dict from data",
                "tokenizer_label.fit_on_texts(label)",
                "label_word_index = tokenizer_label.word_index",
                "label_seq = tokenizer_label.texts_to_sequences(label)",
                "",
                "print(label_seq)",
                "print(label_word_index)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### Get data",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv \\",
                "    -O /tmp/bbc-text.csv",
                ""
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "### Load data",
                "data = pd.read_csv('/tmp/bbc-text.csv')",
                "label = list(data['category'])",
                "sentences_raw = list(data['text'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "### Remove stopwords from data",
                "from nltk.tokenize import word_tokenize",
                "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]",
                "sentences = []",
                "",
                "for t in tqdm(sentences_raw) :",
                "    tokenize_text = word_tokenize(t)",
                "    list_text = [i for i in tokenize_text if i not in stopwords]",
                "    sentences.append((\" \").join(list_text))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "lab_tokenizer.word_index"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### Define variable for tokenizing and modelling",
                "# Reduce this parameter to reduce overfitting",
                "VOCAB_SIZE = 1000",
                "EMBEDDING_DIM = 16",
                "MAX_LENGTH = 120",
                "",
                "TRUNC_TYPE = 'post'",
                "PADDING_TYPE = 'post'",
                "OOV_TOK = '<OOV>'",
                "TRAIN_PROP = .8"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "### Split dataset",
                "train_size = int(TRAIN_PROP * len(sentences))",
                "train_sentences = sentences[:train_size]",
                "train_label = label[:train_size]",
                "val_sentences = sentences[train_size:]",
                "val_label = label[train_size:]",
                "",
                "print(train_size)",
                "print(len(train_sentences))",
                "print(len(train_label))",
                "print(len(val_sentences))",
                "print(len(val_label))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "### Tokenize the train sentences",
                "from tensorflow.keras.preprocessing.text import Tokenizer",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences",
                "text_tokenizer = Tokenizer(oov_token=OOV_TOK, num_words=VOCAB_SIZE)",
                "",
                "# Get the token dict from data",
                "text_tokenizer.fit_on_texts(train_sentences)",
                "word_index = text_tokenizer.word_index",
                "",
                "# Pad the data",
                "train_sequences = text_tokenizer.texts_to_sequences(train_sentences)",
                "train_padded = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### Pad the validation data",
                "val_sequences = text_tokenizer.texts_to_sequences(val_sentences)",
                "val_padded = pad_sequences(val_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "### Tokenize the label",
                "from tensorflow.keras.preprocessing.text import Tokenizer",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences",
                "lab_tokenizer = Tokenizer()",
                "",
                "# Get the token dict from data",
                "lab_tokenizer.fit_on_texts(train_label)",
                "",
                "# Get label sequences",
                "train_label_seq = np.array(lab_tokenizer.texts_to_sequences(train_label))",
                "val_label_seq = np.array(lab_tokenizer.texts_to_sequences(val_label))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Make simple Embedding MLP model",
                "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "",
                "model = Sequential()",
                "model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH))",
                "model.add(GlobalAveragePooling1D())",
                "model.add(Dense(24, activation='relu'))",
                "model.add(Dense(6, activation='softmax'))",
                "",
                "# Compile model",
                "model.compile(optimizer = Adam(),",
                "              loss = 'sparse_categorical_crossentropy',",
                "              metrics = 'accuracy')",
                "",
                "# Summary of the model",
                "model.summary()",
                ""
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "### Train the model",
                "num_epochs = 30",
                "history = model.fit(train_padded,",
                "                    train_label_seq,",
                "                    epochs=num_epochs,",
                "                    validation_data=(val_padded, val_label_seq),",
                "                    verbose=1)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "### Plot the performance of the model",
                "rcParams['figure.figsize'] = [10,8]",
                "plt.style.use('fivethirtyeight') ",
                "sns.set_style('whitegrid')",
                "grid = gridspec.GridSpec(2,1)",
                "",
                "# Get the metrics and loss",
                "acc      = history.history[     'accuracy' ]",
                "val_acc  = history.history[ 'val_accuracy' ]",
                "loss     = history.history[    'loss' ]",
                "val_loss = history.history['val_loss' ]",
                "epo   = range(len(acc)) # Get number of epochs",
                "",
                "# Plot the loss",
                "ax = plt.subplot(grid[0])",
                "ax.plot(epo, loss, label='Train Loss')",
                "ax.plot(epo, val_loss, label='Validation Loss')",
                "ax.set_title('Training and Validation Loss')",
                "ax.legend() ;",
                "",
                "# Plot the acccuracy",
                "ax = plt.subplot(grid[1])",
                "ax.plot(epo, acc, label='Train Acc')",
                "ax.plot(epo, val_acc, label='Validation Acc')",
                "ax.set_title('Training and Validation Accuracy')",
                "ax.legend() ;"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### Make dictionary to reverse the number from tokenizing to text",
                "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])",
                "",
                "def decode_sentence(text):",
                "    return ' '.join([reverse_word_index.get(i, '?') for i in text])",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### Get the embedding weight for visualization",
                "e = model.layers[0]",
                "weights = e.get_weights()[0]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "### Save the weight",
                "import io",
                "",
                "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')",
                "out_m = io.open('meta.tsv', 'w', encoding='utf-8')",
                "for word_num in range(1, VOCAB_SIZE):",
                "    word = reverse_word_index[word_num]",
                "    embeddings = weights[word_num]",
                "    out_m.write(word + \"\\n\")",
                "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")",
                "out_v.close()",
                "out_m.close()",
                "",
                "# Use the file to make embedding visualization at https://projector.tensorflow.org/"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### Get data",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv \\",
                "    -O /tmp/training_cleaned.csv"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "### Load the data",
                "data = pd.read_csv('/tmp/training_cleaned.csv', names=['label','id','time','query','handle','text'])",
                "data = data.sample(frac=1).reset_index(drop=True)",
                "sentences = list(data['text'])",
                "label = list(data['label'].map({0:0, 4:1}))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### Define variable for tokenizing and modelling",
                "EMBEDDING_DIM = 100",
                "MAX_LENGTH = 16",
                "TRUNC_TYPE = 'post'",
                "PADDING_TYPE = 'post'",
                "OOV_TOK = '<OOV>'",
                "TRAIN_PROP = .8"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "### Split dataset",
                "train_size = int(TRAIN_PROP * len(sentences))",
                "train_sentences = sentences[:train_size]",
                "train_label = label[:train_size]",
                "val_sentences = sentences[train_size:]",
                "val_label = label[train_size:]",
                "",
                "print(train_size)",
                "print(len(train_sentences))",
                "print(len(train_label))",
                "print(len(val_sentences))",
                "print(len(val_label))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "### Tokenize the train sentences",
                "from tensorflow.keras.preprocessing.text import Tokenizer",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences",
                "text_tokenizer = Tokenizer(oov_token=OOV_TOK)",
                "",
                "# Get the token dict from data",
                "text_tokenizer.fit_on_texts(train_sentences)",
                "word_index = text_tokenizer.word_index",
                "",
                "# Pad the data",
                "train_sequences = text_tokenizer.texts_to_sequences(train_sentences)",
                "train_padded = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### Pad the validation data",
                "val_sequences = text_tokenizer.texts_to_sequences(val_sentences)",
                "val_padded = pad_sequences(val_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### Define vocab size",
                "VOCAB_SIZE = len(word_index) + 1"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "### Get weight for embedding matrix in model",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\",
                "    -O /tmp/glove.6B.100d.txt",
                "    ",
                "# Load the weight",
                "embeddings_index = {};",
                "with open('/tmp/glove.6B.100d.txt') as f:",
                "    for line in f:",
                "        values = line.split();",
                "        word = values[0];",
                "        coefs = np.asarray(values[1:], dtype='float32');",
                "        embeddings_index[word] = coefs;",
                "",
                "# Make weight matrix",
                "embeddings_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM));",
                "for word, i in word_index.items():",
                "    embedding_vector = embeddings_index.get(word);",
                "    if embedding_vector is not None:",
                "        embeddings_matrix[i] = embedding_vector;"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Make model",
                "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, LSTM, Bidirectional, GRU",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "",
                "# Make simple embedding model",
                "model_simple = Sequential()",
                "model_simple.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "                    weights=[embeddings_matrix], trainable=False))",
                "model_simple.add(GlobalAveragePooling1D())",
                "model_simple.add(Dense(128, activation='relu'))",
                "model_simple.add(Dense(1, activation='sigmoid'))",
                "",
                "model_simple.compile(optimizer = Adam(),",
                "              loss = 'binary_crossentropy',",
                "              metrics = 'accuracy')",
                "",
                "# Make single LSTM model",
                "model_single_lstm = Sequential()",
                "model_single_lstm.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "                    weights=[embeddings_matrix], trainable=False))",
                "model_single_lstm.add(Bidirectional(LSTM(64)))",
                "model_single_lstm.add(Dense(128, activation='relu'))",
                "model_single_lstm.add(Dense(1, activation='sigmoid'))",
                "",
                "model_single_lstm.compile(optimizer = Adam(),",
                "              loss = 'binary_crossentropy',",
                "              metrics = 'accuracy')",
                "",
                "# Make single GRU",
                "model_single_gru = Sequential()",
                "model_single_gru.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "                    weights=[embeddings_matrix], trainable=False))",
                "model_single_gru.add(Bidirectional(GRU(64)))",
                "model_single_gru.add(Dense(128, activation='relu'))",
                "model_single_gru.add(Dense(1, activation='sigmoid'))",
                "",
                "model_single_gru.compile(optimizer = Adam(),",
                "              loss = 'binary_crossentropy',",
                "              metrics = 'accuracy')",
                "",
                "# Make single Conv",
                "model_single_conv = Sequential()",
                "model_single_conv.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "                    weights=[embeddings_matrix], trainable=False))",
                "model_single_conv.add(Conv1D(64, 5, activation='relu'))",
                "model_single_conv.add(GlobalMaxPooling1D())",
                "model_single_conv.add(Dense(128, activation='relu'))",
                "model_single_conv.add(Dense(1, activation='sigmoid'))",
                "",
                "model_single_conv.compile(optimizer = Adam(),",
                "              loss = 'binary_crossentropy',",
                "              metrics = 'accuracy')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "### Plot the performance of the model",
                "def plot_history(history) :",
                "    rcParams['figure.figsize'] = [10,8]",
                "    plt.style.use('fivethirtyeight') ",
                "    sns.set_style('whitegrid')",
                "    grid = gridspec.GridSpec(2,1)",
                "",
                "    # Get the metrics and loss",
                "    acc      = history.history[     'accuracy' ]",
                "    val_acc  = history.history[ 'val_accuracy' ]",
                "    loss     = history.history[    'loss' ]",
                "    val_loss = history.history['val_loss' ]",
                "    epo   = range(len(acc)) # Get number of epochs",
                "",
                "    # Plot the loss",
                "    ax = plt.subplot(grid[0])",
                "    ax.plot(epo, loss, label='Train Loss')",
                "    ax.plot(epo, val_loss, label='Validation Loss')",
                "    ax.set_title('Training and Validation Loss')",
                "    ax.legend() ;",
                "",
                "    # Plot the acccuracy",
                "    ax = plt.subplot(grid[1])",
                "    ax.plot(epo, acc, label='Train Acc')",
                "    ax.plot(epo, val_acc, label='Validation Acc')",
                "    ax.set_title('Training and Validation Accuracy')",
                "    ax.legend() ;"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Train simple model",
                "num_epochs = 5",
                "history_simple = model_simple.fit(train_padded,",
                "                    np.array(train_label),",
                "                    epochs=num_epochs,",
                "                    validation_data=(val_padded, np.array(val_label)),",
                "                    verbose=1)",
                "",
                "%time"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "### Plot simple model performance",
                "plot_history(history_simple)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Train single lstm model",
                "num_epochs = 10",
                "history_single_lstm = model_single_lstm.fit(train_padded,",
                "                    np.array(train_label),",
                "                    epochs=num_epochs,",
                "                    validation_data=(val_padded, np.array(val_label)),",
                "                    verbose=1)",
                "",
                "%time"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "### Plot simple model performance",
                "plot_history(history_single_lstm)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Train single gru model",
                "num_epochs = 5",
                "history_single_gru = model_single_gru.fit(train_padded,",
                "                    np.array(train_label),",
                "                    epochs=num_epochs,",
                "                    validation_data=(val_padded, np.array(val_label)),",
                "                    verbose=1)",
                "",
                "%time"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "### Plot simple model performance",
                "plot_history(history_single_gru)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Train single conv model",
                "num_epochs = 5",
                "history_single_conv = model_single_conv.fit(train_padded,",
                "                    np.array(train_label),",
                "                    epochs=num_epochs,",
                "                    validation_data=(val_padded, np.array(val_label)),",
                "                    verbose=1)",
                "",
                "%time"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "### Plot simple model performance",
                "plot_history(history_single_conv)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Make complicated model",
                "from tensorflow.keras.layers import Dense, Embedding, Dropout, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, LSTM, Bidirectional, GRU",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "",
                "# Make simple embedding model",
                "model = Sequential()",
                "model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "                    weights=[embeddings_matrix], trainable=False))",
                "model.add(Dropout(0.2))",
                "model.add(Conv1D(64, 5, activation='relu'))",
                "model.add(MaxPooling1D(4))",
                "model.add(LSTM(64))",
                "model.add(Dense(1, activation='sigmoid'))",
                "",
                "model.compile(optimizer = Adam(),",
                "              loss = 'binary_crossentropy',",
                "              metrics = 'accuracy')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Train single conv model",
                "num_epochs = 5",
                "history = model.fit(train_padded,",
                "                    np.array(train_label),",
                "                    epochs=num_epochs,",
                "                    validation_data=(val_padded, np.array(val_label)),",
                "                    verbose=1)",
                "",
                "%time"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "### Plot simple model performance",
                "plot_history(history)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### Get data",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\",
                "    -O /tmp/sonnets.txt"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "### Load the data",
                "data = open('/tmp/sonnets.txt').read()",
                "corpus = data.lower().split('\\n')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "### Define variable for tokenizing and modelling",
                "EMBEDDING_DIM = 100",
                "TRUNC_TYPE = 'post'",
                "PADDING_TYPE = 'pre'",
                "OOV_TOK = '<OOV>'",
                "TRAIN_PROP = .8"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "### Make the input sequences for prediciton word",
                "from tensorflow.keras.preprocessing.text import Tokenizer",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences",
                "tokenizer = Tokenizer()",
                "",
                "# Tokenize the corpus",
                "tokenizer.fit_on_texts(corpus)",
                "word_index = tokenizer.word_index",
                "VOCAB_SIZE = len(word_index) + 1",
                "",
                "# Make input sequences",
                "train_seq = []",
                "for text in corpus :",
                "    token_list = tokenizer.texts_to_sequences([text])[0]",
                "    ",
                "    for i in range(1, len(token_list)) :",
                "        gram = token_list[:i+1]",
                "        train_seq.append(gram)",
                "        ",
                "# Pad the sequences",
                "MAX_LENGTH = np.max([len(seq) for seq in train_seq])",
                "train_padded = pad_sequences(train_seq, padding=PADDING_TYPE, maxlen=MAX_LENGTH)",
                "",
                "# Split to train data and label",
                "train_data, train_label = train_padded[:,:-1], train_padded[:,-1]",
                "",
                "# One hot label",
                "from tensorflow.keras.utils import to_categorical",
                "train_label = to_categorical(train_label, num_classes=VOCAB_SIZE)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "### Get weight for embedding matrix in model",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\",
                "    -O /tmp/glove.6B.100d.txt",
                "    ",
                "# Load the weight",
                "embeddings_index = {};",
                "with open('/tmp/glove.6B.100d.txt') as f:",
                "    for line in f:",
                "        values = line.split();",
                "        word = values[0];",
                "        coefs = np.asarray(values[1:], dtype='float32');",
                "        embeddings_index[word] = coefs;",
                "",
                "# Make weight matrix",
                "embeddings_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM));",
                "for word, i in word_index.items():",
                "    embedding_vector = embeddings_index.get(word);",
                "    if embedding_vector is not None:",
                "        embeddings_matrix[i] = embedding_vector;"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Make model",
                "from tensorflow.keras.layers import Dense, Embedding, TimeDistributed, Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, LSTM, Bidirectional, GRU",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.regularizers import l1, l2",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "",
                "# Make simple embedding model",
                "model = Sequential()",
                "model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH-1,",
                "                    weights=[embeddings_matrix], trainable=True))",
                "model.add(Bidirectional(LSTM(256, return_sequences=True)))",
                "model.add(Dropout(0.2))",
                "model.add(Bidirectional(LSTM(128)))",
                "model.add(Dense(VOCAB_SIZE // 2, activation='relu', kernel_regularizer=l2(0.01)))",
                "model.add(Dense(VOCAB_SIZE, activation='softmax'))",
                "",
                "model.compile(optimizer = Adam(),",
                "              loss = 'categorical_crossentropy',",
                "              metrics = 'accuracy')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "### Train model",
                "num_epochs = 100",
                "history = model.fit(train_data,",
                "                    train_label,",
                "                    epochs=num_epochs,",
                "                    verbose=1)",
                "",
                "%time"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "### Plot the performance of the model",
                "def plot_history(history) :",
                "    rcParams['figure.figsize'] = [10,8]",
                "    plt.style.use('fivethirtyeight') ",
                "    sns.set_style('whitegrid')",
                "    grid = gridspec.GridSpec(2,1)",
                "",
                "    # Get the metrics and loss",
                "    acc      = history.history[     'accuracy' ]",
                "    loss     = history.history[    'loss' ]",
                "    epo   = range(len(acc)) # Get number of epochs",
                "",
                "    # Plot the loss",
                "    ax = plt.subplot(grid[0])",
                "    ax.plot(epo, loss, label='Train Loss')",
                "    ax.set_title('Training Loss')",
                "    ax.legend() ;",
                "",
                "    # Plot the acccuracy",
                "    ax = plt.subplot(grid[1])",
                "    ax.plot(epo, acc, label='Train Acc')",
                "    ax.set_title('Training Accuracy')",
                "    ax.legend() ;"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "### Plot model performance - LSTM no Bidirection",
                "plot_history(history)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"",
                "next_words = 100",
                "  ",
                "for _ in range(next_words):",
                "    token_list = tokenizer.texts_to_sequences([seed_text])[0]",
                "    token_list = pad_sequences([token_list], maxlen=MAX_LENGTH-1, padding='pre')",
                "    predicted = model.predict_classes(token_list, verbose=0)",
                "    output_word = \"\"",
                "    for word, index in tokenizer.word_index.items():",
                "        if index == predicted:",
                "            output_word = word",
                "            break",
                "    seed_text += \" \" + output_word",
                "print(seed_text)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import pandas as pd",
                "import matplotlib.pyplot as plt"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "train = pd.read_csv(\"../input/dataset/train_data.csv\", index_col='Id')",
                "train"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "(train == '?').any()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[train == '?'].count()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "train['workclass'].value_counts().plot(kind = 'bar')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "train['occupation'].value_counts().plot(kind = 'bar')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "train['native.country'].value_counts().plot(kind = 'bar')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train['workclass'] = train['workclass'].replace(to_replace = '?', value = 'Private')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train['native.country'] = train['native.country'].replace(to_replace = '?', value = 'United-States')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[train == '?'].count()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train = train.loc[train.occupation != '?']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Xtrain = train.loc[:,'age':'native.country']",
                "Ytrain = train.income",
                "Xtrain.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Ytrain.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.neighbors import KNeighborsClassifier",
                "from sklearn.model_selection import cross_val_score"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn import preprocessing"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Xtrain = pd.get_dummies(Xtrain)",
                "Xtrain"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "content": [
                "knn = KNeighborsClassifier(n_neighbors = 10)",
                "scores = cross_val_score(knn, Xtrain, Ytrain, cv=10)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "scores.mean()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "content": [
                "scores_array = []",
                "for i in range(1,25):",
                "    knn = KNeighborsClassifier(n_neighbors = i)",
                "    scores = cross_val_score(knn, Xtrain, Ytrain, cv=10)",
                "    scores_array.append(scores.mean())",
                "    ",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.plot(scores_array, 'ro')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "test = pd.read_csv('../input/dataset/test_data.csv', na_values='?', index_col='Id')",
                "test"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Xtest = pd.get_dummies(test)",
                "Xtest.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "##garantindo que ambas tenham a mesma dimenso",
                "missing_cols = set( Xtrain.columns ) - set( Xtest.columns )",
                "for c in missing_cols:",
                "    Xtest[c] = 0",
                "Xtest = Xtest[Xtrain.columns]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "knn = knn = KNeighborsClassifier(n_neighbors = 21)",
                "knn.fit(Xtrain,Ytrain)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "YtestPred = knn.predict(Xtest)",
                "YtestPred"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "prediction = pd.DataFrame(index = test.index)",
                "prediction['income'] = YtestPred"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "prediction"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "prediction.to_csv(\"submition.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import pandas as pd",
                "import numpy as np",
                "from sklearn.model_selection import cross_val_score"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train = pd.read_csv(\"../input/train.csv\", index_col = 'Id')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.neighbors import KNeighborsRegressor"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "knn = KNeighborsRegressor(38)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "knn_score = cross_val_score(knn, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "knn_score.mean()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.linear_model import LassoCV"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "lasso = LassoCV(cv = 10)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "lasso_score = cross_val_score(lasso, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "lasso_score.mean()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.linear_model import RidgeCV"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ridge = RidgeCV(cv = 10)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "ridge_score = cross_val_score(ridge, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "ridge_score.mean()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import numpy as np",
                "import pandas as pd",
                "import matplotlib.pyplot as plt"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "train = pd.read_csv(\"../input/featdataset/train_data.csv\", index_col = 'Id')",
                "train.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data",
                "validate_data"
            ],
            "content": [
                "plt.figure(figsize=(15,10))",
                "(train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[:54]].plot(kind = 'bar')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "(train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[54:57]].plot(kind = 'bar')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import sklearn",
                "from sklearn.naive_bayes import GaussianNB",
                "from sklearn.model_selection import cross_val_score"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Xtrain = train[train.columns[0:57]]",
                "Xtrain.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Ytrain = train['ham']",
                "Ytrain.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "NB = GaussianNB()",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "scores = cross_val_score(NB, Xtrain,Ytrain, cv=10)",
                "scores"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "test = pd.read_csv('../input/featdataset/test_features.csv', index_col = 'Id')",
                "test.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "NB.fit(Xtrain,Ytrain)",
                "Ytest = NB.predict(test)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "pred = pd.DataFrame(index = test.index)",
                "pred['ham'] = Ytest",
                "pred.to_csv('submission.csv',index = True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import matplotlib.pyplot as plt",
                "import numpy as np",
                "import pandas as pd",
                "seed = 7",
                "np.random.seed(seed)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "tpure = np.load(\"../input/train_images_pure.npy\")",
                "tnoisy = np.load(\"../input/train_images_noisy.npy\")",
                "trotated = np.load(\"../input/train_images_rotated.npy\")",
                "tboth = np.load(\"../input/train_images_both.npy\")",
                "tlabel = pd.read_csv(\"../input/train_labels.csv\")",
                "tlabel"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.subplot(221)",
                "plt.imshow(tpure[0], cmap=plt.get_cmap('gray'))",
                "plt.subplot(222)",
                "plt.imshow(tpure[1], cmap=plt.get_cmap('gray'))",
                "plt.subplot(223)",
                "plt.imshow(tpure[2], cmap=plt.get_cmap('gray'))",
                "plt.subplot(224)",
                "plt.imshow(tpure[3], cmap=plt.get_cmap('gray'))",
                "# show the plot",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.subplot(221)",
                "plt.imshow(tnoisy[0], cmap=plt.get_cmap('gray'))",
                "plt.subplot(222)",
                "plt.imshow(tnoisy[1], cmap=plt.get_cmap('gray'))",
                "plt.subplot(223)",
                "plt.imshow(tnoisy[2], cmap=plt.get_cmap('gray'))",
                "plt.subplot(224)",
                "plt.imshow(tnoisy[3], cmap=plt.get_cmap('gray'))",
                "# show the plot",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "for i in range(15,19):",
                "    plt.subplot(221+(i%5))",
                "    plt.imshow(trotated[i], cmap=plt.get_cmap('gray'))",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.subplot(221)",
                "plt.imshow(tboth[0], cmap=plt.get_cmap('gray'))",
                "plt.subplot(222)",
                "plt.imshow(tboth[1], cmap=plt.get_cmap('gray'))",
                "plt.subplot(223)",
                "plt.imshow(tboth[2], cmap=plt.get_cmap('gray'))",
                "plt.subplot(224)",
                "plt.imshow(tboth[3], cmap=plt.get_cmap('gray'))",
                "# show the plot",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from keras.models import Sequential",
                "from keras.layers import Dense",
                "from keras.layers import Dropout",
                "from keras.layers import Flatten",
                "from keras.layers.convolutional import Conv2D",
                "from keras.layers.convolutional import MaxPooling2D",
                "from keras.utils import np_utils",
                "from keras import backend as K",
                "from sklearn.model_selection import train_test_split",
                "from keras.callbacks import EarlyStopping",
                "K.set_image_dim_ordering('th')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def DataPrep(db):",
                "    db = db.reshape(db.shape[0], 1, 28, 28).astype('float32')",
                "    db = db / 255",
                "    db = np_utils.to_categorical(db)",
                "    return db"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "tlabel = np_utils.to_categorical(tlabel['label'])",
                "tpure = DataPrep(tpure)",
                "trotated = DataPrep(trotated)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "def deepCNN():",
                "    # create model",
                "    model = Sequential()",
                "    model.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu'))",
                "    model.add(Conv2D(15, (3, 3), activation='relu'))",
                "    model.add(Dropout(0.2))",
                "    model.add(Flatten())",
                "    model.add(Dense(128, activation='relu'))",
                "    model.add(Dense(50, activation='relu'))",
                "    model.add(Dense(tlabel.shape[1], activation='softmax'))",
                "    # Compile model",
                "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])",
                "    return model"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "CNNmodel = deepCNN()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "callbacks = [EarlyStopping(monitor = 'val_loss', patience = 2)]",
                "Xtrain,Xvalidation,Ytrain,Yvalidation = train_test_split(tpure,tlabel, test_size = 0.2)",
                "CNNmodel.fit(Xtrain, Ytrain, validation_data=(Xvalidation,Yvalidation), epochs=20, ",
                "          batch_size=200, verbose=1, callbacks = callbacks)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import pandas as pd",
                "import matplotlib.pyplot as plt"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train = pd.read_csv(\"../input/train.csv\")",
                "test = pd.read_csv(\"../input/test.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[\"idhogar\"]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[\"parentesco1\"].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train_only_heads = train.drop(train[train[\"parentesco1\"] == 0].index)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_only_heads[\"parentesco1\"]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train_hna = train_only_heads.dropna(thresh=len(train_only_heads[\"parentesco1\"])/2, axis=\"columns\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train_hna = train_hna.dropna()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_hna[(train_hna == 'yes') | (train_hna == 'no')].dropna(axis = 'columns', how='all')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_hna"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Xtrain_h = train_hna.drop(['Target','Id','idhogar','dependency','edjefe','edjefa'] ,axis = 'columns')",
                "Ytrain_h = train_hna.Target"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.neighbors import KNeighborsClassifier",
                "from sklearn.model_selection import cross_val_score"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "knn = KNeighborsClassifier(n_neighbors=10)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "scores = cross_val_score(knn, Xtrain_h, Ytrain_h, cv=10)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "scores.mean()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "content": [
                "score_array = []",
                "for i in range(50):",
                "    knn = KNeighborsClassifier(n_neighbors=i+1)",
                "    scores = cross_val_score(knn, Xtrain_h, Ytrain_h, cv=5)",
                "    score_array.append(scores.mean())"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.plot(score_array, 'ro')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "knn = KNeighborsClassifier(n_neighbors=33)",
                "knn.fit(Xtrain_h, Ytrain_h)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Xtest = test.drop(['Id','idhogar','dependency','edjefe','edjefa','rez_esc', 'v18q1', 'v2a1'] ,axis = 'columns')",
                "Xtest"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Xtest = Xtest.fillna(0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "pred = knn.predict(Xtest)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "prediction = pd.DataFrame(test.Id)",
                "prediction['Target'] = pred",
                "prediction"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "prediction.to_csv(\"submition.csv\",index = False)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train = pd.read_csv('/kaggle/input/cs-challenge/training_set.csv', index_col=\"ID\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train = train.dropna(axis=1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "column_list = column_list = train.columns.to_list()\n",
                "non_redundant_cols = [x for x in column_list if x.find('_max') == -1 and x.find('_min') == -1 and x.find('_c') == -1]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "non_redundant_cols"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#train_non_re = train[non_redundant_cols]\n",
                "train_non_re=train"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def ind_max(l):\n",
                "    M=l[0]\n",
                "    ind=0\n",
                "    for i in range(1,len(l)):\n",
                "        if l[i]>M:\n",
                "            M=l[i]\n",
                "            ind=i\n",
                "    return ind"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "#squared_cols = []\n",
                "pows=[-5+i/2 for i in range(21)]\n",
                "pows.remove(0.0)\n",
                "res=[]\n",
                "#for col in non_redundant_cols:\n",
                "for col in column_list:\n",
                "    if col != 'MAC_CODE':\n",
                "        corr=[]\n",
                "        for p in pows:\n",
                "            if (p%1==0 or not any(train[col]<0)) and (p>0 or not any(train[col]==0)):                \n",
                "                    corr.append(abs(train_non_re['TARGET'].corr(train_non_re[col]**p)))\n",
                "            else:\n",
                "                corr.append(0)\n",
                "        p=pows[ind_max(corr)]\n",
                "        res.append(p)\n",
                "        train_non_re[col] = np.power(train_non_re[col], p)\n",
                "res\n",
                "            \n",
                "            \n",
                "            #cor1 = abs(train_non_re['TARGET'].corr(train_non_re[col]))\n",
                "            #cor2 = abs(train_non_re['TARGET'].corr(train_non_re[col]**2))\n",
                "            #if(cor2 > cor1):\n",
                "                #train_non_re[col] = np.power(train_non_re[col], 2)\n",
                "                #squared_cols.append(col)\n",
                "        "
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
                "from sklearn.linear_model import RidgeCV\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.compose import ColumnTransformer\n",
                "\n",
                "\n",
                "col_transformer = ColumnTransformer([\n",
                "    ('MAC_CODE', OneHotEncoder(dtype='int'),['MAC_CODE'])],\n",
                "    remainder = StandardScaler())\n",
                "\n",
                "reg = RidgeCV(cv=5)\n",
                "\n",
                "pipe = Pipeline([('col_transformer', col_transformer),('reg', reg)], verbose=True)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_non_re"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "Xtr, Xte, ytr,  yte = train_test_split(train_non_re.drop('TARGET', axis=1), train_non_re['TARGET'], test_size=0.2)\n",
                "\n",
                "pipe.fit(Xtr,ytr)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import mean_absolute_error\n",
                "\n",
                "mean_absolute_error(yte, pipe.predict(Xte))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "pipe.fit(train_non_re.drop('TARGET', axis=1),train_non_re['TARGET'])"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "test = pd.read_csv('/kaggle/input/cs-challenge/test_set.csv', index_col=\"ID\")\n",
                "#test = test[[x for x in non_redundant_cols if x != 'TARGET']]\n",
                "test = test[[x for x in column_list if x != 'TARGET']]\n",
                "\n",
                "for col in squared_cols:\n",
                "    test[col] = np.power(test[col],2)\n",
                "\n",
                "predict = pipe.predict(test)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "test['TARGET'] = predict\n",
                "test['TARGET'].to_csv(\"squared_ridge.csv\")"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train = pd.read_csv(\"../input/cs-challenge/training_set.csv\", index_col=\"ID\").drop(\"MAC_CODE\", axis = 1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train = train.dropna(axis = 1)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "scaler= StandardScaler()\n",
                "scaler.fit(train)\n",
                "train = pd.DataFrame(scaler.transform(train), columns = train.columns, index = train.index)\n",
                "train"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn import linear_model\n",
                "\n",
                "lasso_reg = linear_model.LassoCV(cv=5, random_state=0, max_iter=10000).fit(train.drop(\"TARGET\",axis=1), train[\"TARGET\"])\n",
                "print(lasso_reg.score(train.drop(\"TARGET\",axis=1), train[\"TARGET\"]))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.figure(figsize=(20,5))\n",
                "plt.xticks(rotation = 'vertical')\n",
                "plt.bar(train.drop(\"TARGET\", axis=1).columns, lasso_reg.coef_)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "column_list = column_list = train.columns.to_list()\n",
                "base_cols = [x for x in column_list if x.find('_max') == -1 and x.find('_min') == -1 and x.find('_c') == -1]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "train_base = train[base_cols]\n",
                "train_base"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn import linear_model\n",
                "\n",
                "lasso_reg_base = linear_model.LassoCV(cv=5, random_state=0, max_iter=20000, fit_intercept=True,normalize = True).fit(train_base.drop(\"TARGET\",axis=1), train_base[\"TARGET\"])\n",
                "print(lasso_reg_base.score(train_base.drop(\"TARGET\",axis=1), train_base[\"TARGET\"]))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.figure(figsize=(20,5))\n",
                "plt.xticks(rotation = 'vertical')\n",
                "plt.bar(train_base.drop(\"TARGET\", axis=1).columns, lasso_reg_base.coef_)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "test = pd.read_csv(\"../input/cs-challenge/test_set.csv\", index_col = \"ID\").drop(\"MAC_CODE\",axis=1)\n",
                "test[\"TARGET\"] = np.ones(len(test.index))\n",
                "#test[train.columns.to_list()]\n",
                "test = pd.DataFrame(scaler.transform(test[train.columns.to_list()]), index=test.index, columns=train.columns)\n",
                "p1 = lasso_reg.predict(test[train.columns.to_list()].drop(\"TARGET\", axis=1))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "test['TARGET'] = p1\n",
                "a1 = pd.DataFrame(scaler.inverse_transform(test[train.columns.to_list()]), index=test.index, columns=test.columns)['TARGET']\n",
                "a1.to_csv('a1.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "p2 = lasso_reg_base.predict(test[train_base.columns.to_list()].drop(\"TARGET\", axis=1))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "test[\"TARGET\"] = p2\n",
                "a2 = pd.DataFrame(scaler.inverse_transform(test[train.columns.to_list()]), index=test.index, columns=test.columns)['TARGET']\n",
                "a2.to_csv('a2.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "",
                "# helpful modules",
                "import fuzzywuzzy",
                "from fuzzywuzzy import process",
                "import chardet",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "# look at the first ten thousand bytes to guess the character encoding",
                "with open(\"../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", 'rb') as rawdata:",
                "    result = chardet.detect(rawdata.read(100000))",
                "",
                "# check what the character encoding might be",
                "print(result)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "# read in our dat",
                "suicide_attacks = pd.read_csv(\"../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", ",
                "                              encoding='Windows-1252')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# convert to lower case",
                "suicide_attacks['City'] = suicide_attacks['City'].str.lower()",
                "# remove trailing white spaces",
                "suicide_attacks['City'] = suicide_attacks['City'].str.strip()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Your turn! Take a look at all the unique values in the \"Province\" column. ",
                "provincia = suicide_attacks['Province'].unique()",
                "provincia.sort()",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.lower()",
                "",
                "",
                "# Then convert the column to lowercase and remove any trailing white spaces",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.strip()",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# get the top 10 closest matches to \"d.i khan\"",
                "matches = fuzzywuzzy.process.extract(\"d.i khan\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "",
                "# take a look at them",
                "matches"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# function to replace rows in the provided column of the provided dataframe",
                "# that match the provided string above the provided ratio with the provided string",
                "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):",
                "    # get a list of unique strings",
                "    strings = df[column].unique()",
                "    ",
                "    # get the top 10 closest matches to our input string",
                "    matches = fuzzywuzzy.process.extract(string_to_match, strings, ",
                "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "",
                "    # only get matches with a ratio > 90",
                "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]",
                "",
                "    # get the rows of all the close matches in our dataframe",
                "    rows_with_matches = df[column].isin(close_matches)",
                "",
                "    # replace all rows with close matches with the input matches ",
                "    df.loc[rows_with_matches, column] = string_to_match",
                "    ",
                "    # let us know the function's done",
                "    print(\"All done!\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# use the function we just wrote to replace close matches to \"d.i khan\" with \"d.i khan\"",
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"d.i khan\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Your turn! It looks like 'kuram agency' and 'kurram agency' should",
                "matches = fuzzywuzzy.process.extract(\"kuram agency\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "# be the same city. Correct the dataframe so that they are.",
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"kuram agency\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "",
                "# for Box-Cox Transformation",
                "from scipy import stats",
                "",
                "# for min_max scaling",
                "from mlxtend.preprocessing import minmax_scaling",
                "",
                "# plotting modules",
                "import seaborn as sns",
                "import matplotlib.pyplot as plt",
                "",
                "# read in all our data",
                "kickstarters_2017 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201801.csv\")",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# generate 1000 data points randomly drawn from an exponential distribution",
                "original_data = np.random.exponential(size = 1000)",
                "",
                "# mix-max scale the data between 0 and 1",
                "scaled_data = minmax_scaling(original_data, columns = [0])",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(original_data, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(scaled_data, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# normalize the exponential data with boxcox",
                "normalized_data = stats.boxcox(original_data)",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(original_data, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(normalized_data[0], ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# select the usd_goal_real column",
                "usd_goal = kickstarters_2017.usd_goal_real",
                "",
                "# scale the goals from 0 to 1",
                "scaled_data = minmax_scaling(usd_goal, columns = [0])",
                "",
                "# plot the original & scaled data together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(scaled_data, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Your turn! ",
                "",
                "# We just scaled the \"usd_goal_real\" column. What about the \"goal\" column?",
                "goal = kickstarters_2017.goal",
                "goal_scaled = minmax_scaling(goal, columns = [0])",
                "fig_goal, ax_goal=plt.subplots(1,2)",
                "",
                "sns.distplot(kickstarters_2017.goal, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(goal_scaled, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# get the index of all positive pledges (Box-Cox only takes postive values)",
                "index_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0",
                "",
                "# get only positive pledges (using their indexes)",
                "positive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]",
                "",
                "# normalize the pledges (w/ Box-Cox)",
                "normalized_pledges = stats.boxcox(positive_pledges)[0]",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(positive_pledges, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(normalized_pledges, ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Your turn! ",
                "# We looked as the usd_pledged_real column. What about the \"pledged\" column? Does it have the same info?",
                "",
                "index_of_positive = kickstarters_2017.pledged > 0",
                "",
                "# get only positive pledges (using their indexes)",
                "positive_pledges_x = kickstarters_2017.pledged.loc[index_of_positive]",
                "",
                "# normalize the pledges (w/ Box-Cox)",
                "normalized_pledges_x = stats.boxcox(positive_pledges_x)[0]",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(positive_pledges_x, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(normalized_pledges_x, ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "",
                "# Input data files are available in the read-only \"../input/\" directory",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                "",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" ",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "# Code you have previously used to load data\n",
                "import pandas as pd\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "\n",
                "\n",
                "# Path of the file to read\n",
                "iowa_file_path = '../input/home-data-for-ml-course/train.csv'\n",
                "\n",
                "home_data = pd.read_csv(iowa_file_path)\n",
                "# Create target object and call it y\n",
                "y = home_data.SalePrice\n",
                "# Create X\n",
                "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
                "X = home_data[features]\n",
                "\n",
                "# Split into validation and training data\n",
                "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
                "\n",
                "# Specify Model\n",
                "iowa_model = DecisionTreeRegressor(random_state=1)\n",
                "# Fit Model\n",
                "iowa_model.fit(train_X, train_y)\n",
                "\n",
                "# Make validation predictions and calculate mean absolute error\n",
                "val_predictions = iowa_model.predict(val_X)\n",
                "val_mae = mean_absolute_error(val_predictions, val_y)\n",
                "print(\"Validation MAE: {:,.0f}\".format(val_mae))\n",
                "\n",
                "# Set up code checking\n",
                "from learntools.core import binder\n",
                "binder.bind(globals())\n",
                "from learntools.machine_learning.ex5 import *\n",
                "print(\"\\nSetup complete\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
                "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
                "    model.fit(train_X, train_y)\n",
                "    preds_val = model.predict(val_X)\n",
                "    mae = mean_absolute_error(val_y, preds_val)\n",
                "    return(mae)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "#importando bibliotecas\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing\n",
                "import matplotlib.pyplot as plt # visualization\n",
                "import seaborn as sns # visualization\n",
                "from scipy import stats\n",
                "from scipy.stats import norm \n",
                "import warnings \n",
                "warnings.filterwarnings('ignore') #ignore warnings\n",
                "\n",
                "%matplotlib inline\n",
                "import gc"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "stores=pd.read_csv(\"C:/Users/sony/OneDrive/Documentos/teste_ds_ze/stores.csv\")"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "stores"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "features = pd.read_csv(\"C:\\\\Users\\\\sony\\\\OneDrive\\\\Documentos\\\\features.csv\")\n"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "features"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "test = pd.read_csv(\"C:\\\\Users\\\\sony\\\\OneDrive\\\\Documentos\\\\test.csv\")"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train = pd.read_csv(\"C:\\\\Users\\\\sony\\\\OneDrive\\\\Documentos\\\\train.csv\")"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(\"the structure of train data is \", train.shape)\n",
                "print(\"the structure of test  data is \", test.shape)\n",
                "print(\"the ratio of train data : test data is \", (round(train.shape[0]*100/(train.shape[0]+test.shape[0])),100-round(train.shape[0]*100/(train.shape[0]+test.shape[0]))))"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "train=train.merge(stores, on='Store', how='left')\n",
                "train.head()"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "dataset  =  pd.merge(stores, features) \n",
                "dataset"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.pairplot(dataset)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def scatter(dataset, column):\n",
                "    plt.figure()\n",
                "    plt.scatter(dataset[column] , dataset['weeklySales'])\n",
                "    plt.ylabel('weeklySales')\n",
                "    plt.xlabel(column)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def scatter(dataset, column):\n",
                "    plt.figure()\n",
                "    plt.scatter(dataset[column] , dataset['weeklySales'])\n",
                "    plt.ylabel('weeklySales')\n",
                "    plt.xlabel(column)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig = plt.figure(figsize=(18, 14))\n",
                "corr = dataset.corr()\n",
                "c = plt.pcolor(corr)\n",
                "plt.yticks(np.arange(0.5, len(corr.index), 1), corr.index)\n",
                "plt.xticks(np.arange(0.5, len(corr.columns), 1), corr.columns)\n",
                "fig.colorbar(c)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.pairplot(dataset, vars=['weeklySales', 'Fuel_Price', 'Size', 'CPI', 'Dept', 'Temperature', 'Unemployment'])"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.pairplot(dataset, vars=[ 'Fuel_Price', 'Size', 'CPI', 'Dept', 'Temperature', 'Unemployment'])"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Year=pd.Series(train['Year'].unique())\n",
                "Week=pd.Series(train['Week'].unique())\n",
                "Month=pd.Series(train['Month'].unique())\n",
                "Day=pd.Series(train['Day'].unique())\n",
                "n_days=pd.Series(train['n_days'].unique())"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(\"the shape of stores data set is\", stores.shape)\n",
                "print(\"the unique value of store is\", stores['Store'].unique())\n",
                "print(\"the unique value of Type is\", stores['Type'].unique())"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "print(stores.head())\n",
                "grouped=stores.groupby('Type')\n",
                "print(grouped.describe()['Size'].round(2))"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.style.use('ggplot')\n",
                "labels=['A store','B store','C store']\n",
                "sizes=grouped.describe()['Size'].round(1)\n",
                "sizes=[(22/(17+6+22))*100,(17/(17+6+22))*100,(6/(17+6+22))*100] # convert to the proportion\n",
                "\n",
                "\n",
                "fig, axes = plt.subplots(1,1, figsize=(10,10))\n",
                "\n",
                "wprops={'edgecolor':'black',\n",
                "      'linewidth':2}\n",
                "\n",
                "tprops = {'fontsize':30}\n",
                "\n",
                "\n",
                "axes.pie(sizes,\n",
                "        labels=labels,\n",
                "        explode=(0.02,0,0),\n",
                "        autopct='%1.1f%%',\n",
                "        pctdistance=0.6,\n",
                "        labeldistance=1.2,\n",
                "        wedgeprops=wprops,\n",
                "        textprops=tprops,\n",
                "        radius=0.8,\n",
                "        center=(0.5,0.5))\n",
                "plt.show()\n"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "data = pd.concat([stores['Type'], stores['Size']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "fig = sns.boxplot(x='Type', y='Size', data=data)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "data = pd.concat([train['Type'], train['Weekly_Sales']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "fig = sns.boxplot(x='Type', y='Weekly_Sales', data=data, showfliers=False)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.style.use('ggplot')\n",
                "\n",
                "fig=plt.figure()\n",
                "ax=fig.add_subplot(111)\n",
                "\n",
                "ax.scatter(train['Size'],train['Weekly_Sales'], alpha=0.5)\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "types=stores['Type'].unique()\n",
                "\n",
                "plt.style.use('ggplot')\n",
                "\n",
                "fig=plt.figure(figsize=(10,5))\n",
                "ax=fig.add_subplot(111)\n",
                "\n",
                "for t in types:\n",
                "    x=train.loc[train['Type']==t, 'Size']\n",
                "    y=train.loc[train['Type']==t, 'Weekly_Sales']\n",
                "    \n",
                "    ax.scatter(x,y,alpha=0.5, label=t)\n",
                "\n",
                "ax.set_title('Scatter plot size and sales by store type')\n",
                "ax.set_xlabel('Size')\n",
                "ax.set_ylabel('Weekly_Sales')\n",
                "\n",
                "ax.legend(loc='higher right',fontsize=12)\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "data = pd.concat([train['Store'], train['Weekly_Sales'], train['Type']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(25, 8))\n",
                "fig = sns.boxplot(x='Store', y='Weekly_Sales', data=data, showfliers=False, hue=\"Type\")"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "data = pd.concat([train['Store'], train['Weekly_Sales'], train['IsHoliday']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(25, 8))\n",
                "fig = sns.boxplot(x='Store', y='Weekly_Sales', data=data, showfliers=False, hue=\"IsHoliday\")"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "data = pd.concat([train['Dept'], train['Weekly_Sales'], train['Type']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(25, 10))\n",
                "fig = sns.boxplot(x='Dept', y='Weekly_Sales', data=data, showfliers=False)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "data = pd.concat([train['Dept'], train['Weekly_Sales'], train['Type']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(10, 50))\n",
                "fig = sns.boxplot(y='Dept', x='Weekly_Sales', data=data, showfliers=False, hue=\"Type\",orient=\"h\") "
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "data = pd.concat([train['Dept'], train['Weekly_Sales'], train['IsHoliday']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(25, 10))\n",
                "fig = sns.boxplot(x='Dept', y='Weekly_Sales', data=data, showfliers=False, hue=\"IsHoliday\")"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.style.use('ggplot')\n",
                "fig, axes = plt.subplots(1,2, figsize = (20,5))\n",
                "fig.subplots_adjust(wspace=1, hspace=1)\n",
                "fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)\n",
                "\n",
                "sales_holiday=train[['IsHoliday','Weekly_Sales']]\n",
                "target=[sales_holiday['Weekly_Sales'].loc[sales_holiday['IsHoliday']==True],sales_holiday['Weekly_Sales'].loc[sales_holiday['IsHoliday']==False]]\n",
                "labels=['Holiday','Not Holiday']\n",
                "\n",
                "#median\n",
                "medianprop={'color':'#2196F3',\n",
                "            'linewidth': 2,\n",
                "            'linestyle':'-'}\n",
                "\n",
                "# outliers\n",
                "\n",
                "flierprop={'color' : '#EC407A',\n",
                "          'marker' : 'o',\n",
                "          'markerfacecolor': '#2196F3',\n",
                "          'markeredgecolor':'white',\n",
                "          'markersize' : 3,\n",
                "          'linestyle' : 'None',\n",
                "          'linewidth' : 0.1}\n",
                "\n",
                "\n",
                "\n",
                "axes[0].boxplot(target,labels=labels, patch_artist = 'Patch',\n",
                "                  showmeans=True,\n",
                "                  flierprops=flierprop,\n",
                "                  medianprops=medianprop)\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "axes[1].boxplot(target,labels=labels, patch_artist = 'Patch',\n",
                "                  showmeans=True,\n",
                "                  flierprops=flierprop,\n",
                "                  medianprops=medianprop)\n",
                "\n",
                "axes[1].set_ylim(-6000,80000)\n",
                "\n",
                "plt.show()\n",
                "\n",
                "\n"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(train[train['IsHoliday']==True]['Weekly_Sales'].describe().round(1))\n",
                "print(train[train['IsHoliday']==False]['Weekly_Sales'].describe().round(1))"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "data = pd.concat([train['Month'], train['Weekly_Sales']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "fig = sns.boxplot(x='Month', y=\"Weekly_Sales\", data=data, showfliers=False)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "data = pd.concat([train['Month'], train['Weekly_Sales'],train['IsHoliday']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "fig = sns.boxplot(x='Month', y=\"Weekly_Sales\", data=data, showfliers=False, hue='IsHoliday')"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "data = pd.concat([train['Month'], train['Weekly_Sales'],train['Type']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "fig = sns.boxplot(x='Month', y=\"Weekly_Sales\", data=data, showfliers=False, hue='Type')"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "data = pd.concat([train['Year'], train['Weekly_Sales']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "fig = sns.boxplot(x='Year', y=\"Weekly_Sales\", data=data, showfliers=False)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "data = pd.concat([train['Week'], train['Weekly_Sales']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(20, 6))\n",
                "fig = sns.boxplot(x='Week', y=\"Weekly_Sales\", data=data, showfliers=False)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "sns.distplot(train['Weekly_Sales'])"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(\"Skewness: \", train['Weekly_Sales'].skew()) #skewness\n",
                "print(\"Kurtosis: \", train['Weekly_Sales'].kurt()) #kurtosis"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train['Weekly_Sales'].min()"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig = plt.figure(figsize = (10,5))\n",
                "\n",
                "fig.add_subplot(1,2,1)\n",
                "res = stats.probplot(train.loc[train['Weekly_Sales']>0,'Weekly_Sales'], plot=plt)\n",
                "\n",
                "fig.add_subplot(1,2,2)\n",
                "res = stats.probplot(np.log1p(train.loc[train['Weekly_Sales']>0,'Weekly_Sales']), plot=plt)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.describe()['Weekly_Sales']"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "train_over_zero=train[train['Weekly_Sales']>0]\n",
                "train_below_zero=train[train['Weekly_Sales']<=0]\n",
                "sales_over_zero = np.log1p(train_over_zero['Weekly_Sales'])\n",
                "#histogram\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "sns.distplot(sales_over_zero)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(\"Skewness: \", sales_over_zero.skew()) #skewness\n",
                "print(\"Kurtosis: \", sales_over_zero.kurt()) #kurtosis"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "grouped=train.groupby(['Dept','Date']).mean().round(0).reset_index()\n",
                "print(grouped.shape)\n",
                "print(grouped.head())\n",
                "data=grouped[['Dept','Date','Weekly_Sales']]\n",
                "\n",
                "\n",
                "dept=train['Dept'].unique()\n",
                "dept.sort()\n",
                "dept_1=dept[0:20]\n",
                "dept_2=dept[20:40]\n",
                "dept_3=dept[40:60]\n",
                "dept_4=dept[60:]\n",
                "\n",
                "fig, ax = plt.subplots(2,2,figsize=(20,10))\n",
                "fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
                "fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)\n",
                "\n",
                "for i in dept_1 :\n",
                "    data_1=data[data['Dept']==i]\n",
                "    ax[0,0].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')\n",
                "\n",
                "for i in dept_2 :\n",
                "    data_1=data[data['Dept']==i]\n",
                "    ax[0,1].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')\n",
                "    \n",
                "for i in dept_3 :\n",
                "    data_1=data[data['Dept']==i]\n",
                "    ax[1,0].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')    \n",
                "\n",
                "for i in dept_4 :\n",
                "    data_1=data[data['Dept']==i]\n",
                "    ax[1,1].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')        \n",
                "    \n",
                "ax[0,0].set_title('Mean sales record by department(0~19)')\n",
                "ax[0,1].set_title('Mean sales record by department(20~39)')\n",
                "ax[1,0].set_title('Mean sales record by department(40~59)')\n",
                "ax[1,1].set_title('Mean sales record by department(60~)')\n",
                "\n",
                "\n",
                "ax[0,0].set_ylabel('Mean sales')\n",
                "ax[0,0].set_xlabel('Date')\n",
                "ax[0,1].set_ylabel('Mean sales')\n",
                "ax[0,1].set_xlabel('Date')\n",
                "ax[1,0].set_ylabel('Mean sales')\n",
                "ax[1,0].set_xlabel('Date')\n",
                "ax[1,1].set_ylabel('Mean sales')\n",
                "ax[1,1].set_xlabel('Date')\n",
                "\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "grouped=train.groupby(['Store','Date']).mean().round(0).reset_index()\n",
                "grouped.shape\n",
                "grouped.head()\n",
                "\n",
                "data=grouped[['Store','Date','Weekly_Sales']]\n",
                "type(data)\n",
                "\n",
                "\n",
                "store=train['Store'].unique()\n",
                "store.sort()\n",
                "store_1=store[0:5]\n",
                "store_2=store[5:10]\n",
                "store_3=store[10:15]\n",
                "store_4=store[15:20]\n",
                "store_5=store[20:25]\n",
                "store_6=store[25:30]\n",
                "store_7=store[30:35]\n",
                "store_8=store[35:40]\n",
                "store_9=store[40:]\n",
                "\n",
                "fig, ax = plt.subplots(5,2,figsize=(20,15))\n",
                "\n",
                "fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
                "fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)\n",
                "\n",
                "for i in store_1 :\n",
                "    data_1=data[data['Store']==i]\n",
                "    ax[0,0].plot(data_1['Date'], data_1['Weekly_Sales'])\n",
                "    \n",
                "for i in store_2 :\n",
                "    data_2=data[data['Store']==i]\n",
                "    ax[0,1].plot(data_2['Date'], data_2['Weekly_Sales'])\n",
                "    \n",
                "for i in store_3 :\n",
                "    data_3=data[data['Store']==i]\n",
                "    ax[1,0].plot(data_3['Date'], data_3['Weekly_Sales'])\n",
                "\n",
                "for i in store_4 :\n",
                "    data_4=data[data['Store']==i]\n",
                "    ax[1,1].plot(data_4['Date'], data_4['Weekly_Sales'])\n",
                "    \n",
                "for i in store_5 :\n",
                "    data_5=data[data['Store']==i]\n",
                "    ax[2,0].plot(data_5['Date'], data_5['Weekly_Sales'])  \n",
                "\n",
                "for i in store_6 :\n",
                "    data_6=data[data['Store']==i]\n",
                "    ax[2,1].plot(data_6['Date'], data_6['Weekly_Sales'])  \n",
                "\n",
                "for i in store_7 :\n",
                "    data_7=data[data['Store']==i]\n",
                "    ax[3,0].plot(data_7['Date'], data_7['Weekly_Sales'])      \n",
                "\n",
                "for i in store_8 :\n",
                "    data_8=data[data['Store']==i]\n",
                "    ax[3,1].plot(data_8['Date'], data_8['Weekly_Sales'])     \n",
                "    \n",
                "for i in store_9 :\n",
                "    data_9=data[data['Store']==i]\n",
                "    ax[4,0].plot(data_9['Date'], data_9['Weekly_Sales'])     \n",
                "\n",
                "    \n",
                "ax[0,0].set_title('Mean sales record by store(0~4)')\n",
                "ax[0,1].set_title('Mean sales record by store(5~9)')\n",
                "ax[1,0].set_title('Mean sales record by store(10~14)')\n",
                "ax[1,1].set_title('Mean sales record by store(15~19)')\n",
                "ax[2,0].set_title('Mean sales record by store(20~24)')\n",
                "ax[2,1].set_title('Mean sales record by store(25~29)')\n",
                "ax[3,0].set_title('Mean sales record by store(30~34)')\n",
                "ax[3,1].set_title('Mean sales record by store(35~39)')\n",
                "ax[4,0].set_title('Mean sales record by store(40~)')\n",
                "\n",
                "\n",
                "\n",
                "ax[0,0].set_ylabel('Mean sales')\n",
                "ax[0,0].set_xlabel('Date')\n",
                "ax[0,1].set_ylabel('Mean sales')\n",
                "ax[0,1].set_xlabel('Date')\n",
                "ax[1,0].set_ylabel('Mean sales')\n",
                "ax[1,0].set_xlabel('Date')\n",
                "ax[1,1].set_ylabel('Mean sales')\n",
                "ax[1,1].set_xlabel('Date')\n",
                "ax[2,0].set_ylabel('Mean sales')\n",
                "ax[2,0].set_xlabel('Date')\n",
                "ax[2,1].set_ylabel('Mean sales')\n",
                "ax[2,1].set_xlabel('Date')\n",
                "ax[3,0].set_ylabel('Mean sales')\n",
                "ax[3,0].set_xlabel('Date')\n",
                "ax[3,1].set_ylabel('Mean sales')\n",
                "ax[3,1].set_xlabel('Date')\n",
                "ax[4,0].set_ylabel('Mean sales')\n",
                "ax[4,0].set_xlabel('Date')\n",
                "\n",
                "\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "import pandas as pd\n",
                "features = pd.read_csv(\"../input/features.csv\")"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "import pandas as pd\n",
                "test = pd.read_csv(\"../input/test.csv\")"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "import pandas as pd\n",
                "train = pd.read_csv(\"../input/train.csv\")"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# Importando as funes necessrias\n",
                "from __future__ import print_function\n",
                "\n",
                "import tensorflow as tf \n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense\n",
                "from tensorflow.keras.optimizers import RMSprop\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "# Lendo o dataset e imprimindo uma amostra\n",
                "breast_cancer = pd.read_csv('../input/data.csv')\n",
                "breast_cancer.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Descrevendo as informaes dos tipos de dados do dataset\n",
                "breast_cancer.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Verificando o tamanho do dataset (569 linhas / 33 colunas)\n",
                "breast_cancer.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Gerando estatsticas descritivas do dataset que apresentam um resumo dos dados, excluindo valores nulos\n",
                "breast_cancer.describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Contagem de registros agrupados pela coluna 'diagnosis', que representa o diagnstico (B = Benigno / M = Maligno)\n",
                "breast_cancer.groupby('diagnosis').size()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Contagem de registros nulos por coluna\n",
                "breast_cancer.isnull().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# As colunas 'id' e 'Unnamed: 32' no so teis para a anlise e sero descartadas \n",
                "feature_names = breast_cancer.columns[2:-1]\n",
                "x = breast_cancer[feature_names]\n",
                "# A coluna 'diagnosis'  a caracterstica que vamos prever\n",
                "y = breast_cancer.diagnosis"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Transforma os dados da coluna 'diagnosis' para valores binrios (M = 1 / B = 0)\n",
                "class_le = LabelEncoder()\n",
                "y = class_le.fit_transform(breast_cancer.diagnosis.values)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Gera uma matriz de correlao (heatmap) que fornece informaes teis sobre a relao entre cada varivel do conjunto de dados\n",
                "sns.heatmap(\n",
                "    data=x.corr(),\n",
                "    annot=True,\n",
                "    fmt='.2f',\n",
                "    cmap='RdYlGn'\n",
                ")\n",
                "\n",
                "fig = plt.gcf()\n",
                "fig.set_size_inches(20, 16)\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Obtendo os conjuntos de treino e teste, separando 32% do conjunto de dados para teste (test_size=0.32) e o restante para treino\n",
                "x_train, x_test, y_train, y_test = train_test_split(\n",
                "    x,\n",
                "    y,\n",
                "    random_state=42,\n",
                "    test_size=0.32\n",
                ")\n",
                "\n",
                "print(x_train.shape, y_train.shape)\n",
                "print(x_test.shape, y_test.shape)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Implementao da rede neural, utilizando 2 classes (diagnosis) e 200 pocas\n",
                "batch_size = 64\n",
                "num_classes = 2\n",
                "epochs = 200\n",
                "\n",
                "# Transformando os dados de entrada para float32\n",
                "x_train = x_train.astype('float32')\n",
                "x_test = x_test.astype('float32')\n",
                "\n",
                "# Convertendo os vetores das classes em matrizes de classificao binrias\n",
                "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
                "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
                "\n",
                "# Definio da arquitetura do modelo\n",
                "model = Sequential()\n",
                "# Camadas do modelo\n",
                "model.add(tf.keras.layers.Dense(100, input_dim=30, activation='sigmoid'))\n",
                "model.add(tf.keras.layers.Dense(25, input_dim=30, activation='relu'))\n",
                "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
                "\n",
                "# Fim - Definio da arquitetura do modelo\n",
                "\n",
                "model.summary()\n",
                "\n",
                "model.compile(loss='categorical_crossentropy',\n",
                "              optimizer=RMSprop(0.0001),\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "# Treinamento do modelo \n",
                "H = model.fit(x_train, y_train,\n",
                "                    batch_size=batch_size,\n",
                "                    epochs=epochs,\n",
                "                    verbose=1,\n",
                "                    validation_data=(x_test, y_test))\n",
                "\n",
                "# Avaliao do modelo no conjunto de teste\n",
                "score = model.evaluate(x_test, y_test, verbose=1)\n",
                "\n",
                "print('Test loss:', score[0])\n",
                "print('Test accuracy:', score[1])\n",
                "\n",
                "# Plotando 'loss' e 'accuracy' para os datasets 'train' e 'test'\n",
                "plt.figure()\n",
                "plt.plot(np.arange(0,epochs), H.history[\"loss\"], label=\"train_loss\")\n",
                "plt.plot(np.arange(0,epochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
                "plt.plot(np.arange(0,epochs), H.history[\"acc\"], label=\"train_acc\")\n",
                "plt.plot(np.arange(0,epochs), H.history[\"val_acc\"], label=\"val_acc\")\n",
                "plt.title(\"Acurcia\")\n",
                "plt.xlabel(\"pocas #\")\n",
                "plt.ylabel(\"Loss/Accuracy\")\n",
                "plt.legend()\n",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "import seaborn as sns",
                "import os",
                "import scipy.stats as stats",
                "from sklearn.model_selection import train_test_split ",
                "from sklearn.linear_model import LogisticRegression as LR",
                "import sklearn.metrics as m",
                "",
                "input_folder='/kaggle/input/titanic/'"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train_data = pd.read_csv(input_folder+'train.csv')",
                "test_data = pd.read_csv(input_folder+'test.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "print(train_data.isnull().sum()*100/train_data.isnull().count())",
                "# About 20% of age is missing, about 77% of cabin is missing, 0.22% of embarked is missing",
                "",
                "sns.heatmap(train_data.isnull(),",
                "            annot=True,",
                "            yticklabels=False,",
                "            cbar=False,",
                "            cmap='plasma')"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# We need to change sex to a numerical value, otherwise the correlation plot will not include sex",
                "train_data['Sex'] = train_data['Sex'].astype('category')",
                "train_data['Sex'] = train_data['Sex'].cat.codes",
                "",
                "",
                "sns.heatmap(train_data.corr(),",
                "           annot=True,",
                "           cbar=False)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train_filtered = train_data.filter(['Survived','Pclass','Sex','Age','Fare'])",
                "train_filtered.dropna(how='any',axis='rows', inplace=True)",
                "",
                "test_filtered = test_data.filter(['Survived','Pclass','Sex','Age','Fare','PassengerId'])",
                "test_filtered.dropna(how='any',axis='rows', inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train_filtered['Sex'] = train_filtered['Sex'].astype('category')",
                "train_filtered['Sex'] = train_filtered['Sex'].cat.codes",
                "",
                "test_filtered['Sex'] = test_filtered['Sex'].astype('category')",
                "test_filtered['Sex'] = test_filtered['Sex'].cat.codes"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Survival by pclass",
                "g = sns.jointplot(train_filtered.groupby(['Pclass']).sum().index,",
                "                  train_filtered.groupby(['Pclass']).sum()['Survived']",
                "    )",
                "",
                "# Survival by sex",
                "g = sns.jointplot(train_filtered.groupby(['Sex']).sum().index,",
                "                  train_filtered.groupby(['Sex']).sum()['Survived']",
                "    )",
                "",
                "# Survival by age",
                "g = sns.jointplot(train_filtered.groupby(['Age']).sum().index,",
                "                  train_filtered.groupby(['Age']).sum()['Survived']",
                "    )",
                "",
                "# Survival by fare",
                "g = sns.jointplot(train_filtered.groupby(['Fare']).sum().index,",
                "                  train_filtered.groupby(['Fare']).sum()['Survived']",
                "    )"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Split data into test and train sets based on the train data set",
                "X_train, X_test, y_train, y_test = train_test_split(",
                "    train_filtered.drop('Survived', axis=1), train_filtered['Survived'], test_size=0.33)",
                "",
                "# Train model",
                "model = LR().fit(y=y_train,X=X_train)",
                "",
                "# Predict results",
                "results = model.predict(X_test)",
                "",
                "# Add results to a data frame",
                "res = pd.DataFrame(data=y_test.tolist(),columns=['Survived actual'])",
                "res['Survived predicted'] = results",
                "",
                "# Confusion matrix",
                "confmatrix=m.confusion_matrix(res['Survived actual'],res['Survived predicted'])",
                "sns.heatmap(confmatrix,annot=True,fmt='d',cbar=0).set_title('Confusion Matrix')",
                "#   True negatives (tn)     True positives (tp)",
                "#   False negatives (fn)    False positives (fp)",
                "",
                "print('Accuracy: '+str(m.accuracy_score(res['Survived actual'],res['Survived predicted']))) # percent of accurate classification",
                "print('Precision: '+str(m.precision_score(res['Survived actual'],res['Survived predicted']))) # tp / (tp + fp), 0 is worst, 1 is best",
                "print('Recall: '+str(m.recall_score(res['Survived actual'],res['Survived predicted']))) # tp / (tp + fn), 0 is worst, 1 is best"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "# Train model",
                "model = LR().fit(y=train_filtered['Survived'],X=train_filtered.drop('Survived', axis=1))",
                "",
                "# Predict survival for the test set",
                "results = model.predict(test_filtered.drop('PassengerId', axis=1))",
                "",
                "res = pd.DataFrame(data=results.tolist(), columns = ['Survived'])",
                "res['PassengerId'] = test_filtered['PassengerId'].astype(int)",
                "",
                "print(res)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "res.to_csv('results.csv', index=False)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                ""
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "import seaborn as sb",
                "import matplotlib.pyplot as pl",
                "",
                "%matplotlib inline",
                "",
                "train = pd.read_csv('/kaggle/input/titanic/train.csv')",
                "test  = pd.read_csv('/kaggle/input/titanic/test.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.isnull().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.isnull().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def grafico(feature):",
                "    survived = train[train['Survived']==1][feature].value_counts()",
                "    dead = train[train['Survived']==0][feature].value_counts()",
                "    df = pd.DataFrame([survived,dead])",
                "    df.index = ['Survived','Dead']",
                "    df.plot(kind='bar', stacked=True, figsize=(10,5))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('Sex')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('Pclass')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('SibSp')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('Parch')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('Embarked')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train_test = [train, test]",
                "for dataset in train_test:",
                "    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train['Title'].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test['Title'].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "title_map = {\"Mr\": 0,",
                "            \"Miss\": 1,",
                "            \"Mrs\": 2,",
                "            \"Master\": 3,",
                "            \"Dr\": 3,",
                "            \"Rev\": 3,",
                "            \"Col\": 3,",
                "            \"Major\": 3,",
                "            \"Mlle\": 3,",
                "            \"Ms\": 3,",
                "            \"Don\": 3,",
                "            \"Lady\": 3,",
                "            \"Jonkheer\": 3,",
                "            \"Countess\": 3,",
                "            \"Mme\": 3,",
                "            \"Sir\": 3,",
                "            \"Capt\": 3}",
                "for dataset in train_test:",
                "    dataset['Title'] = dataset['Title'].map(title_map)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "test[\"Title\"].fillna(0, inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.isnull().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "grafico('Title')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train.drop('Name', axis=1, inplace=True)",
                "test.drop('Name', axis=1, inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "sex_map = {\"male\": 0, \"female\": 1}",
                "for dataset in train_test:",
                "    dataset['Sex'] = dataset['Sex'].map(sex_map)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('Sex')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head(100)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)",
                "test[\"Age\"].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "facet = sb.FacetGrid(train, hue=\"Survived\", aspect=4)",
                "facet.map(sb.kdeplot, 'Age', shade=True)",
                "facet.set(xlim=(0, train['Age'].max()))",
                "facet.add_legend()",
                "pl.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for dataset in train_test:",
                "    dataset.loc[ dataset['Age'] <= 16, 'Age'] =0",
                "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1",
                "    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2",
                "    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3",
                "    dataset.loc[(dataset['Age'] > 62), 'Age'] = 4"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('Age')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()",
                "Pclass2 = train[train['Pclass']==2]['Embarked'].value_counts()",
                "Pclass3 = train[train['Pclass']==3]['Embarked'].value_counts()",
                "df = pd.DataFrame([Pclass1,Pclass2,Pclass3])",
                "df.index = ['1st class', '2nd class', '3rd class']",
                "df.plot(kind='bar', stacked=True, figsize=(10,5))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for dataset in train_test:",
                "    dataset['Embarked'] =  dataset['Embarked'].fillna('S')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "emb_map = {\"S\": 0,",
                "           \"C\": 1,",
                "           \"Q\": 2}",
                "for dataset in train_test:",
                "    dataset['Embarked'] = dataset['Embarked'].map(emb_map)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)",
                "test[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for dataset in train_test:",
                "    dataset.loc[ dataset['Fare'] <= 17, 'Fare'] =0",
                "    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1",
                "    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2",
                "    dataset.loc[(dataset['Fare'] > 100), 'Fare'] = 3"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.Cabin.value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for dataset in train_test:",
                "    dataset['Cabin'] = dataset['Cabin'].str[:1]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()",
                "Pclass2 = train[train['Pclass']==2]['Cabin'].value_counts()",
                "Pclass3 = train[train['Pclass']==3]['Cabin'].value_counts()",
                "df = pd.DataFrame([Pclass1,Pclass2,Pclass3])",
                "df.index = ['1st class', '2nd class', '3rd class']",
                "df.plot(kind='bar', stacked=True, figsize=(10,5))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "cab_map = {\"A\": 0,",
                "           \"B\": 0.4,",
                "           \"C\": 0.8,",
                "           \"D\": 1.2,",
                "           \"E\": 1.6,",
                "           \"F\": 2.0,",
                "           \"G\": 2.4,",
                "           \"T\": 2.8}",
                "for dataset in train_test:",
                "    dataset['Cabin'] = dataset['Cabin'].map(cab_map)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)",
                "test[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"]  + 1",
                "test[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"]  + 1"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "facet = sb.FacetGrid(train, hue=\"Survived\", aspect=4)",
                "facet.map(sb.kdeplot, 'FamilySize', shade= True)",
                "facet.set(xlim=(0, train['FamilySize'].max()))",
                "facet.add_legend()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "family_map = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}",
                "for dataset in train_test:",
                "    dataset['FamilySize'] = dataset['FamilySize'].map(family_map)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "features_drop = ['Ticket', 'SibSp', 'Parch']",
                "train = train.drop(features_drop, axis=1)",
                "test = test.drop(features_drop, axis=1)",
                "train = train.drop(['PassengerId'], axis=1)",
                "train_data = train.drop('Survived', axis=1)",
                "target = train['Survived']",
                "",
                "train_data.shape, target.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_data.head(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.neighbors import KNeighborsClassifier",
                "from sklearn.tree import DecisionTreeClassifier",
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.naive_bayes import GaussianNB",
                "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis",
                "from sklearn import linear_model",
                "from sklearn.svm import SVC",
                "",
                "import numpy as np"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.model_selection import KFold",
                "from sklearn.model_selection import cross_val_score",
                "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "clf = KNeighborsClassifier(n_neighbors = 13)",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(score)",
                "round(np.mean(score)*100, 2)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "clf = DecisionTreeClassifier()",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(score)",
                "round(np.mean(score)*100, 2)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "clf = RandomForestClassifier(n_estimators=13)",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(score)",
                "round(np.mean(score)*100, 2)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "clf = GaussianNB()",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(score)",
                "round(np.mean(score)*100, 2)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "clf = SVC()",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(score)",
                "round(np.mean(score)*100,2)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "clf = QuadraticDiscriminantAnalysis()",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(score)",
                "round(np.mean(score)*100,2)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "clf = linear_model.LinearRegression()",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1)",
                "print(score)",
                "round(np.mean(score)*100,2)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "clf = SVC()",
                "",
                "clf.fit(train_data, target)",
                "",
                "test_data = test.drop(\"PassengerId\", axis=1).copy()",
                "",
                "prediction = clf.predict(test_data)",
                "",
                "test_data2 = pd.read_csv('/kaggle/input/testes/teste.csv')",
                "prediction2 = clf.predict(test_data2)",
                "",
                "print(prediction2)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "submission = pd.DataFrame({",
                "        \"PassengerId\": test[\"PassengerId\"],",
                "        \"Survived\": prediction",
                "    })",
                "",
                "submission.to_csv('/kaggle/working/submission.csv', index=False)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "submission = pd.read_csv('/kaggle/working/submission.csv')",
                "submission.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import numpy as np \n",
                "import pandas as pd \n",
                "import tensorflow as tf\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import math\n",
                "from sklearn import preprocessing, model_selection\n",
                "from sklearn.metrics import mean_squared_error\n",
                "import xgboost as xgb\n",
                "import scipy.stats as stats\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "df_belem = pd.read_csv(\"../input/temperature-timeseries-for-some-brazilian-cities/station_belem.csv\")\n",
                "df_curitiba = pd.read_csv(\"../input/temperature-timeseries-for-some-brazilian-cities/station_curitiba.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#Questo 1\n",
                "display(df_belem.shape, df_curitiba.shape)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "#Questo 2\n",
                "df_belem.set_index('YEAR',inplace=True)\n",
                "df_curitiba.set_index('YEAR',inplace=True)\n",
                "display(df_belem.head())\n",
                "display(df_curitiba.head())"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#exerccio 3\n",
                "#plota o histograma dos valores \n",
                "#vemos que h vrios outliers prximos de 1000 que provavelmente no so valores de temperatura vlidos\n",
                "plt.figure(figsize=(25,25))\n",
                "#df_belem.boxplot()\n",
                "df_belem.hist()\n",
                "\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#mostra a quantidade de valores nicos no ms de janeiro\n",
                "#verificando os valores nicos confirmamos que o valor 999.90  o nico outlier\n",
                "display(df_belem['JAN'].value_counts())\n",
                "display(df_curitiba['JAN'].value_counts())"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "#exerccio 4\n",
                "#para tratar os outliers, podemos excluir os dados ausentes (999.90) ou substitu-lo pela mdia do ano anterior e posterior.\n",
                "#adotarei a soluo de substituir os nulos pela mdia.\n",
                "\n",
                "\n",
                "#cria um novo dataset transformando o outlier em nulo para aplicao das funes de tratamento\n",
                "df_belem_t = df_belem.replace(999.90,np.nan)\n",
                "#substitui os valores nulos restantes pela mdia do ano anterior e posterior\n",
                "df_belem_t = df_belem_t.fillna(df_belem_t.mean())\n",
                "display(df_belem_t)\n",
                "\n",
                "#cria um novo dataset transformando o outlier em nulo para aplicao das funes de tratamento\n",
                "df_curitiba_t = df_curitiba.replace(999.90,np.nan)\n",
                "#substitui os valores nulos restantes pela mdia do ano anterior e posterior\n",
                "df_curitiba_t = df_curitiba_t.fillna(df_curitiba_t.mean())\n",
                "display(df_curitiba_t)\n"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#exerccio 5\n",
                "plt.figure(figsize=(10,10))\n",
                "fig=plt.figure()\n",
                "ax=fig.add_axes([0,0,1,1])\n",
                "ax.scatter(df_curitiba_t.index, df_curitiba_t.JUL, color='r')\n",
                "ax.scatter(df_belem_t.index, df_belem_t.JUL, color='b')\n",
                "ax.set_xlabel('Ano')\n",
                "ax.set_ylabel('Temperatura (C)')\n",
                "ax.legend([\"Curitiba - Julho\", \"Belm - Julho\"])\n",
                "ax.set_title('scatter plot')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "#questo 6\n",
                "display(df_curitiba_t['JUL'].describe())\n",
                "display(df_belem_t['JUL'].describe())\n",
                "stats.f_oneway(df_belem_t['JUL'], df_curitiba_t['JUL'])"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "#exerccio 7\n",
                "df_curitiba_jan = pd.DataFrame(df_curitiba_t['JAN'],columns=['JAN'])\n",
                "#cria o dataset de previso com os valores dos 3 anos anteriores\n",
                "df_curitiba_jan['A1'] = df_curitiba_jan['JAN'].shift(1)\n",
                "df_curitiba_jan['A2'] = df_curitiba_jan['JAN'].shift(2)\n",
                "df_curitiba_jan['A3'] = df_curitiba_jan['JAN'].shift(3)\n",
                "#dropa os primeiros anos (que no tem anos anteriores para montar o dataset)\n",
                "df_curitiba_jan = df_curitiba_jan.dropna()\n",
                "display(df_curitiba_jan.head())\n",
                "#separa em conjuntos de teste e treinamento\n",
                "X_train, X_test, y_train, y_test = model_selection.train_test_split(df_curitiba_jan.drop(columns=['JAN']),df_curitiba_jan['JAN'],test_size=0.25, random_state=33)\n"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "#realiza regresso com o regressor de gradient boosting XGBoost\n",
                "#ele frequentemente apresenta resultados iniciais melhores que uma rede neural sem ajustes\n",
                "model = xgb.XGBRegressor()\n",
                "model.fit(X_train,y_train)\n",
                "p_train = model.predict(data=X_train)\n",
                "p_test = model.predict(data=X_test)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "#calcula os erros de previso\n",
                "trainScore = math.sqrt(mean_squared_error(p_train, y_train))\n",
                "print('Pontuao para o treinamento: %.2f RMSE' % (trainScore))\n",
                "testScore = math.sqrt(mean_squared_error(p_test, y_test))\n",
                "print('Pontuao para o teste: %.2f RMSE' % (testScore))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#plota o resultado previsto em relao ao real\n",
                "df_plot = pd.DataFrame({'YEAR': X_test.index, 'PRED': p_test, 'REAL': y_test}).reset_index(drop=True)\n",
                "display(df_plot.sort_values(['YEAR']).set_index('YEAR'))\n",
                "plt.figure(figsize=(10,10))\n",
                "fig=plt.figure()\n",
                "ax=fig.add_axes([0,0,1,1])\n",
                "ax.scatter(df_plot['YEAR'],df_plot['PRED'] , color='r')\n",
                "ax.scatter(df_plot['YEAR'],df_plot['REAL'] , color='b')\n",
                "ax.set_xlabel('Ano')\n",
                "ax.set_ylabel('Temperatura (C)')\n",
                "ax.legend([\"Curitiba - Janeiro - Previsto\", \"Curitiba - Janeiro - Real\"])\n",
                "ax.set_title('scatter plot')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# Imports for Exploratory Data Analysis\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Imports for Machine Learning Models\n",
                "import string\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.feature_extraction.text import TfidfTransformer\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.pipeline import Pipeline\n",
                "\n",
                "# Validation\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "\n",
                "# Set the style of the plots' background\n",
                "sns.set_style(\"darkgrid\")\n",
                "\n",
                "# Show the plot in the same window as the notebook\n",
                "%matplotlib inline"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "jobs = pd.read_csv(\"/kaggle/input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv\")\n",
                "jobs.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(12,8))\n",
                "sns.heatmap(jobs.isnull(), cmap=\"coolwarm\", yticklabels=False, cbar=False)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "jobs.drop(columns=[\"department\", \"salary_range\", \"benefits\"], inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "jobs.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "jobs.describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "jobs.isnull().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# List with the columns to check the length of the text\n",
                "feature_lst = [\"company_profile\", \"description\", \"requirements\"]\n",
                "\n",
                "# For loop to treat the missing values in the columns of the feature_lst.\n",
                "for col in feature_lst:\n",
                "    # If the job post is real, change the missing values to \"none\"\n",
                "    jobs.loc[(jobs[col].isnull()) & (jobs[\"fraudulent\"] == 0), col] = \"none\"\n",
                "    \n",
                "    # If the job post is fake, change the missing values to \"missing\"\n",
                "    jobs.loc[(jobs[col].isnull()) & (jobs[\"fraudulent\"] == 1), col] = \"missing\""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# For loop to create new columns with the lengths of the ones in the feature_lst\n",
                "for num,col in enumerate(feature_lst):\n",
                "    jobs[str(num)] = jobs[col].apply(len)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Rename the new columns created above\n",
                "jobs = jobs.rename({\"0\": \"profile_length\", \"1\": \"description_length\", \"2\": \"requirements_length\"}, axis=1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "jobs.isnull().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "jobs[\"fraudulent\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.pairplot(data=jobs[[\"fraudulent\", \"profile_length\", \"description_length\", \"requirements_length\"]],\n",
                "             hue=\"fraudulent\", height=2, aspect=2);"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "profile_grid = sns.FacetGrid(jobs, col=\"fraudulent\", aspect=1.5, height=4, sharey=False)\n",
                "profile_grid = profile_grid.map(plt.hist, \"profile_length\", bins=40)\n",
                "\n",
                "# Flatten the axes. Create an iterator\n",
                "axes = profile_grid.axes.flatten()\n",
                "\n",
                "# Title\n",
                "axes[0].set_title(\"Non-Fraudulent (0)\", fontsize=14)\n",
                "axes[1].set_title(\"Fraudulent (1)\", fontsize=14)\n",
                "\n",
                "# Labels\n",
                "axes[0].set_ylabel(\"Count\", fontsize=14)\n",
                "for ax in axes:\n",
                "    ax.set_xlabel(\"Profile Text Length\", fontsize=14)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "description_grid = sns.FacetGrid(jobs, col=\"fraudulent\", aspect=1.5, height=4, sharey=False)\n",
                "description_grid = description_grid.map(plt.hist, \"description_length\", bins=40)\n",
                "\n",
                "# Flatten the axes. Create an iterator\n",
                "axes = description_grid.axes.flatten()\n",
                "\n",
                "# Title\n",
                "axes[0].set_title(\"Non-Fraudulent (0)\", fontsize=14)\n",
                "axes[1].set_title(\"Fraudulent (1)\", fontsize=14)\n",
                "\n",
                "# Labels\n",
                "axes[0].set_ylabel(\"Count\", fontsize=14)\n",
                "for ax in axes:\n",
                "    ax.set_xlabel(\"Description Text Length\", fontsize=14)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "requirements_grid = sns.FacetGrid(jobs, col=\"fraudulent\", aspect=1.5, height=4, sharey=False)\n",
                "requirements_grid = requirements_grid.map(plt.hist, \"requirements_length\", bins=40)\n",
                "\n",
                "# Another option. Makes less obviuos which axes is to be labelled\n",
                "#requirements_grid.set_axis_labels(\"Requirement Length\", \"Count\")\n",
                "\n",
                "# Flatten the axes. Create an iterator\n",
                "axes = requirements_grid.axes.flatten()\n",
                "\n",
                "# Title\n",
                "axes[0].set_title(\"Non Fraudulent (0)\", fontsize=14)\n",
                "axes[1].set_title(\"Fraudulent (1)\", fontsize=14)\n",
                "\n",
                "# Labels\n",
                "axes[0].set_ylabel(\"Count\", fontsize=14)\n",
                "for ax in axes:\n",
                "    ax.set_xlabel(\"Requirements Text Length\", fontsize=14)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.catplot(x=\"has_company_logo\", hue=\"fraudulent\", data=jobs, kind=\"count\", aspect=2, height=4);\n",
                "\n",
                "plt.xlabel(\"Company Logo\", fontsize=14)\n",
                "plt.xticks([0, 1], (\"Has\", \"Doesn't have\"), fontsize=12)\n",
                "plt.ylabel(\"Count\", fontsize=14);"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Create a 1 by 2 figure and axes\n",
                "fig, axes = plt.subplots(1, 2, figsize=(18,8))\n",
                "\n",
                "# Plot a countplot on the first axes\n",
                "employ = sns.countplot(x=jobs[\"employment_type\"].dropna(), hue=jobs[\"fraudulent\"], palette=\"Set1\", ax=axes[0])\n",
                "axes[0].set_xlabel(\"Employment Type\", fontsize=15)\n",
                "axes[0].set_ylabel(\"Count\", fontsize=15)\n",
                "axes[0].set_title(\"Employment Type Count\", fontsize=15)\n",
                "axes[0].legend(\"\")\n",
                "\n",
                "# Write the height of the bars on top\n",
                "for p in employ.patches:\n",
                "    employ.annotate(\"{:.0f}\".format(p.get_height()), \n",
                "                        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
                "                        ha='center', va='center', fontsize=14, color='black', xytext=(0, 12),\n",
                "                        textcoords='offset points')\n",
                "\n",
                "#############################################################\n",
                "\n",
                "# Plot a countplot on the second axes\n",
                "employ_zoom = sns.countplot(x=jobs[\"employment_type\"].dropna(), hue=jobs[\"fraudulent\"], palette=\"Set1\", ax=axes[1])\n",
                "axes[1].set_xlabel(\"Employment Type\", fontsize=15)\n",
                "axes[1].set_ylim((0, 1500))\n",
                "axes[1].set_ylabel(\"\")\n",
                "axes[1].set_title(\"Employment Type Count Zoom\", fontsize=15)\n",
                "axes[1].legend(title=\"Fraudulent\", title_fontsize=14, fontsize=12, bbox_to_anchor=(1.2, 0.6));"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "jobs.columns"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X1_profile = jobs[\"company_profile\"]\n",
                "y1 = jobs[\"fraudulent\"]\n",
                "X1_profile_train, X1_profile_test, y1_train, y1_test = train_test_split(X1_profile, y1, test_size=0.2, random_state=42)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def text_process(text):\n",
                "    # Remove the punctuation\n",
                "    nopunc = [char for char in text if char not in string.punctuation]\n",
                "    \n",
                "    # Join the list of characters to form strings\n",
                "    nopunc = \"\".join(nopunc)\n",
                "    \n",
                "    # Remove stopwords\n",
                "    return [word for word in nopunc.split() if word.lower() not in stopwords.words(\"english\")]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "NB_pipeline = Pipeline([(\"bow no func\", CountVectorizer()),\n",
                "                       (\"NB_classifier\", MultinomialNB())])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "NB_pipeline.fit(X1_profile_train, y1_train)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "NB_pred = NB_pipeline.predict(X1_profile_test)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(classification_report(y1_test, NB_pred))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "print(confusion_matrix(y1_test, NB_pred))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "NB_func_pipeline = Pipeline([(\"bow with func\", CountVectorizer(analyzer=text_process)),\n",
                "                            (\"NB_classifier\", MultinomialNB())])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X2_profile = jobs[\"company_profile\"]\n",
                "y2 = jobs[\"fraudulent\"]\n",
                "X2_profile_train, X2_profile_test, y2_train, y2_test = train_test_split(X2_profile, y2, test_size=0.2, random_state=42)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "NB_func_pipeline.fit(X2_profile_train, y2_train)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "NB_func_pred = NB_func_pipeline.predict(X2_profile_test)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(classification_report(y2_test, NB_func_pred))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "print(confusion_matrix(y2_test, NB_func_pred))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "NB_tfidf_pipeline = Pipeline([(\"bow no func\", CountVectorizer()),\n",
                "                              (\"tfidf\", TfidfTransformer()),\n",
                "                              (\"NB_classifier\", MultinomialNB())])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X3_profile = jobs[\"company_profile\"]\n",
                "y3 = jobs[\"fraudulent\"]\n",
                "X3_profile_train, X3_profile_test, y3_train, y3_test = train_test_split(X3_profile, y3, test_size=0.2, random_state=42)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "NB_tfidf_pipeline.fit(X3_profile_train, y3_train)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "NB_tfidf_pred = NB_tfidf_pipeline.predict(X3_profile_test)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(classification_report(y3_test, NB_tfidf_pred))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "print(confusion_matrix(y3_test, NB_tfidf_pred))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "NB_func_tfidf_pipeline = Pipeline([(\"bow with func\", CountVectorizer(analyzer=text_process)),\n",
                "                              (\"tfidf\", TfidfTransformer()),\n",
                "                              (\"NB_classifier\", MultinomialNB())])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X4_profile = jobs[\"company_profile\"]\n",
                "y4 = jobs[\"fraudulent\"]\n",
                "X4_profile_train, X4_profile_test, y4_train, y4_test = train_test_split(X4_profile, y4, test_size=0.2, random_state=42)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "NB_func_tfidf_pipeline.fit(X4_profile_train, y4_train)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "NB_func_tfidf_pred = NB_func_tfidf_pipeline.predict(X4_profile_test)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(classification_report(y4_test, NB_func_tfidf_pred))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "print(confusion_matrix(y4_test, NB_func_tfidf_pred))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "# Important imports for the analysis of the dataset\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "sns.set_style(\"darkgrid\")\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "# Create the dataframe and check the first 8 rows\n",
                "app_df = pd.read_csv(\"/kaggle/input/17k-apple-app-store-strategy-games/appstore_games.csv\")\n",
                "app_df.head(8)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Dropping columns that I will not use for this analysis\n",
                "app_df_cut = app_df.drop(columns=['URL', 'Subtitle', 'Icon URL'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "app_df_cut.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Most reviewed app\n",
                "#app_df_cut.iloc[app_df_cut[\"User Rating Count\"].idxmax()]\n",
                "\n",
                "# A better way of seeing the most reviwed apps \n",
                "app_df_cut = app_df_cut.sort_values(by=\"User Rating Count\", ascending=False)\n",
                "app_df_cut.head(5)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Get the columns \"User Rating Count\" and \"Average User Rating\" where they are both equal to NaN and set the\n",
                "# values to 0.\n",
                "app_df_cut.loc[(app_df_cut[\"User Rating Count\"].isnull()) | (app_df_cut[\"Average User Rating\"].isnull()),\n",
                "               [\"Average User Rating\", \"User Rating Count\"]] = 0"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Check if there are any other missing values in those columns\n",
                "app_df_cut.loc[(app_df_cut[\"User Rating Count\"].isnull()) | (app_df_cut[\"Average User Rating\"].isnull())]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Get the column \"In-app Purchases\" where the value is NaN and set it to zero\n",
                "app_df_cut.loc[app_df_cut[\"In-app Purchases\"].isnull(),\n",
                "               \"In-app Purchases\"] = 0"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Check if there are any NaN value in the \"In-app Purchases\" column\n",
                "app_df_cut.loc[app_df_cut[\"In-app Purchases\"].isnull()]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Check if there are missing or 0 ID's\n",
                "app_df_cut.loc[(app_df_cut[\"ID\"] == 0) | (app_df_cut[\"ID\"].isnull()),\n",
                "              \"ID\"]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Check for duplicates in the ID column\n",
                "len(app_df_cut[\"ID\"]) - len(app_df_cut[\"ID\"].unique())\n",
                "\n",
                "# The number of unique values is lower than the total amount of ID's, therefore there are duplicates among them."
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Drop every duplicate ID row\n",
                "app_df_cut.drop_duplicates(subset=\"ID\", inplace=True)\n",
                "app_df_cut.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Check if there are null values in the Size column\n",
                "app_df_cut[(app_df_cut[\"Size\"].isnull()) | (app_df_cut['Size'] == 0)]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Drop the only row in which the game has no size\n",
                "app_df_cut.drop([16782], axis=0, inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Convert the size to MB\n",
                "app_df_cut[\"Size\"] = round(app_df_cut[\"Size\"]/1000000)\n",
                "app_df_cut.head(5)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Drop the row with NaN values in the \"Price\" column\n",
                "app_df_cut = app_df_cut.drop(app_df_cut.loc[app_df_cut[\"Price\"].isnull()].index)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Check if there are any null values on the price column\n",
                "app_df_cut.loc[app_df_cut[\"Price\"].isnull()]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Drop the rows with NaN values in the \"Languages\" column\n",
                "app_df_cut = app_df_cut.drop(app_df_cut.loc[app_df_cut[\"Languages\"].isnull()].index)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Check if there are any null values on the \"Languages\" column\n",
                "app_df_cut.loc[app_df_cut[\"Languages\"].isnull()]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "app_df_cut.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "app_df_cut.to_csv(\"app_df_clean.csv\", index=False)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "app_df_clean = pd.read_csv(\"app_df_clean.csv\")\n",
                "app_df_clean.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Transform the the string dates into datetime objects\n",
                "app_df_clean[\"Original Release Date\"] = pd.to_datetime(app_df_clean[\"Original Release Date\"])\n",
                "app_df_clean[\"Current Version Release Date\"] = pd.to_datetime(app_df_clean[\"Current Version Release Date\"])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "app_df_clean.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "# Variables\n",
                "years = app_df_clean[\"Original Release Date\"].apply(lambda date: date.year)\n",
                "size = app_df_clean[\"Size\"]\n",
                "\n",
                "# Plot a swarmplot\n",
                "palette = sns.color_palette(\"muted\")\n",
                "size = sns.swarmplot(x=years, y=size, palette=palette)\n",
                "size.set_ylabel(\"Size (in MB)\", fontsize=16)\n",
                "size.set_xlabel(\"Original Release Date\", fontsize=16)\n",
                "size.set_title(\"Time Evolution of the Apps' Sizes\", fontsize=20)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "# Plot a countplot\n",
                "palette1 = sns.color_palette(\"inferno_r\")\n",
                "apps_per_year = sns.countplot(x=years, data=app_df_clean, palette=palette1)\n",
                "apps_per_year.set_xlabel(\"Year of Release\", fontsize=16)\n",
                "apps_per_year.set_ylabel(\"Amount\", fontsize=16)\n",
                "apps_per_year.set_title(\"Quantity of Apps per Year\", fontsize=20)\n",
                "\n",
                "# Write the height of each bar on top of them\n",
                "for p in apps_per_year.patches:\n",
                "    apps_per_year.annotate(\"{}\".format(p.get_height()),\n",
                "                          (p.get_x() + p.get_width() / 2, p.get_height() + 40),\n",
                "                          va=\"center\", ha=\"center\", fontsize=16)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "#Make a list of years from 2014 to 2018\n",
                "years_lst = [year for year in range(2014,2019)]\n",
                "\n",
                "#For loop to get a picture of the amount of games produced from August to December\n",
                "for year in years_lst:\n",
                "    from_August = app_df_clean[\"Original Release Date\"].apply(lambda date: (date.year == year) & (date.month >= 8)).sum()\n",
                "    total = app_df_clean[\"Original Release Date\"].apply(lambda date: date.year == year).sum()\n",
                "    print(\"In {year}, {percentage}% games were produced from August to December.\"\n",
                "          .format(year=year,\n",
                "                  percentage=round((from_August/total)*100, 1)))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "# Variables\n",
                "price = app_df_clean[\"Price\"]\n",
                "\n",
                "# Plot a Countplot\n",
                "palette2 = sns.light_palette(\"green\", reverse=True)\n",
                "price_vis = sns.countplot(x=price, palette=palette2)\n",
                "price_vis.set_xlabel(\"Price (in US dollars)\", fontsize=16)\n",
                "price_vis.set_xticklabels(price_vis.get_xticklabels(), fontsize=12, rotation=45)\n",
                "price_vis.set_ylabel(\"Amount\", fontsize=16)\n",
                "price_vis.set_title(\"Quantity of Each App per Price\", fontsize=20)\n",
                "\n",
                "# Write the height of the bars on top\n",
                "for p in price_vis.patches:\n",
                "    price_vis.annotate(\"{:.0f}\".format(p.get_height()), # Text that will appear on the screen\n",
                "                       (p.get_x() + p.get_width() / 2 + 0.1, p.get_height()), # (x, y) has to be a tuple\n",
                "                       ha='center', va='center', fontsize=14, color='black', xytext=(0, 10), # Customizations\n",
                "                       textcoords='offset points')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "# Variables\n",
                "in_app_purchases = app_df_clean[\"In-app Purchases\"].str.split(\",\").apply(lambda lst: len(lst))\n",
                "\n",
                "# Plot a stripplot\n",
                "palette3 = sns.color_palette(\"BuGn_r\", 23)\n",
                "in_app_purchases_vis = sns.stripplot(x=price, y=in_app_purchases, palette=palette3)\n",
                "in_app_purchases_vis.set_xlabel(\"Game Price (in US dollars)\", fontsize=16)\n",
                "in_app_purchases_vis.set_xticklabels(in_app_purchases_vis.get_xticklabels(), fontsize=12, rotation=45)\n",
                "in_app_purchases_vis.set_ylabel(\"In-app Purchases Available\", fontsize=16)\n",
                "in_app_purchases_vis.set_title(\"Quantity of In-app Purchases per Game Price\", fontsize=20)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Plot a distribution of the top 200 apps by their price\n",
                "\n",
                "# Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "# Plot a Countplot\n",
                "palette4 = sns.color_palette(\"BuPu_r\")\n",
                "top_prices = sns.countplot(app_df_clean.iloc[:200][\"Price\"], palette=palette4)\n",
                "top_prices.set_xlabel(\"Price (in US dollars)\", fontsize=16)\n",
                "top_prices.set_xticklabels(top_prices.get_xticklabels(), fontsize=12)\n",
                "top_prices.set_ylabel(\"Amount\", fontsize=16)\n",
                "top_prices.set_title(\"Quantity of Each App per Price\", fontsize=20)\n",
                "\n",
                "# Write the height of the bars on top\n",
                "for p in top_prices.patches:\n",
                "    top_prices.annotate(\"{:.0f}\".format(p.get_height()), \n",
                "                        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
                "                        ha='center', va='center', fontsize=14, color='black', xytext=(0, 8),\n",
                "                        textcoords='offset points')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Create the DataFrames needed\n",
                "paid = app_df_clean[app_df_clean[\"Price\"] > 0]\n",
                "total_paid = len(paid)\n",
                "free = app_df_clean[app_df_clean[\"Price\"] == 0]\n",
                "total_free = len(free)\n",
                "\n",
                "# Make the figure and the axes (1 row, 2 columns)\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16,10))\n",
                "\n",
                "# Free apps countplot\n",
                "free_vis = sns.countplot(x=\"Average User Rating\", data=free, ax=axes[0])\n",
                "free_vis.set_xlabel(\"Average User Rating\", fontsize=16)\n",
                "free_vis.set_ylabel(\"Amount\", fontsize=16)\n",
                "free_vis.set_title(\"Free Apps\", fontsize=20)\n",
                "\n",
                "# Display the percentages on top of the bars\n",
                "for p in free_vis.patches:\n",
                "     free_vis.annotate(\"{:.1f}%\".format(100 * (p.get_height()/total_free)),\n",
                "                       (p.get_x() + p.get_width() / 2 + 0.1, p.get_height()),\n",
                "                        ha='center', va='center', fontsize=14, color='black', xytext=(0, 8),\n",
                "                        textcoords='offset points')\n",
                "    \n",
                "# Paid apps countplot\n",
                "paid_vis = sns.countplot(x=\"Average User Rating\", data=paid, ax=axes[1])\n",
                "paid_vis.set_xlabel(\"Average User Rating\", fontsize=16)\n",
                "paid_vis.set_ylabel(\" \", fontsize=16)\n",
                "paid_vis.set_title(\"Paid Apps\", fontsize=20)\n",
                "\n",
                "# Display the percentages on top of the bars\n",
                "for p in paid_vis.patches:\n",
                "    paid_vis.annotate(\"{:.1f}%\".format(100 * (p.get_height()/total_paid)),\n",
                "                      (p.get_x() + p.get_width() / 2 + 0.1, p.get_height()),\n",
                "                       ha='center', va='center', fontsize=14, color='black', xytext=(0, 8),\n",
                "                       textcoords='offset points')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "# Make a countplot\n",
                "palette5 = sns.color_palette(\"BuGn_r\")\n",
                "age_vis = sns.countplot(x=app_df_clean[\"Age Rating\"], order=[\"4+\", \"9+\", \"12+\", \"17+\"], palette=palette5)\n",
                "age_vis.set_xlabel(\"Age Rating\", fontsize=16)\n",
                "age_vis.set_ylabel(\"Amount\", fontsize=16)\n",
                "age_vis.set_title(\"Amount of Games per Age Restriction\", fontsize=20)\n",
                "\n",
                "# Write the height of the bars on top\n",
                "for p in age_vis.patches:\n",
                "    age_vis.annotate(\"{:.0f}\".format(p.get_height()), \n",
                "                        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
                "                        ha='center', va='center', fontsize=14, color='black', xytext=(0, 8),\n",
                "                        textcoords='offset points')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Create a new column that contains the amount of languages that app has available\n",
                "app_df_clean[\"numLang\"] = app_df_clean[\"Languages\"].apply(lambda x: len(x.split(\",\")))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "#Variables\n",
                "lang = app_df_clean.loc[app_df_clean[\"numLang\"] <= 25, \"numLang\"]\n",
                "\n",
                "#Plot a countplot\n",
                "palette6 = sns.color_palette(\"PuBuGn_r\")\n",
                "numLang_vis = sns.countplot(x=lang, data=app_df_clean, palette=palette6)\n",
                "numLang_vis.set_xlabel(\"Quantity of Languages\", fontsize=16)\n",
                "numLang_vis.set_ylabel(\"Amount of Games\", fontsize=16)\n",
                "numLang_vis.set_title(\"Quantity of Languages Available per Game\", fontsize=20)\n",
                "\n",
                "# Write the height of the bars on top\n",
                "for p in numLang_vis.patches:\n",
                "    numLang_vis.annotate(\"{:.0f}\".format(p.get_height()), \n",
                "                        (p.get_x() + p.get_width() / 2. + .1, p.get_height()),\n",
                "                        ha='center', va='center', fontsize=12, color='black', xytext=(0, 12),\n",
                "                        textcoords='offset points')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Amount of games that have only the English language\n",
                "len(app_df_clean[(app_df_clean[\"numLang\"] == 1) & (app_df_clean[\"Languages\"] == \"EN\")])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Amount of games that have only one language and is not English\n",
                "len(app_df_clean[(app_df_clean[\"numLang\"] == 1) & (app_df_clean[\"Languages\"] != \"EN\")])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "",
                "# Input data files are available in the \"../input/\" directory.",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory",
                "",
                "import os",
                "print(os.listdir(\"../input\"))",
                "",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train_file_path = '../input/train.csv' ",
                "test_file_path = '../input/test.csv' ",
                "submission_file_path = 'submission.csv' ",
                "",
                "train = pd.read_csv(train_file_path)",
                "test = pd.read_csv(test_file_path)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.fillna(0)",
                "test.fillna(0)",
                "",
                "train.dtypes"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "data_predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare']",
                "",
                "train_y = train['Survived']",
                "train_X = train[data_predictors]",
                "test_X = test[data_predictors]",
                "",
                "print(test_X.head())"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "train_x = pd.get_dummies(train_X)",
                "test_x = pd.get_dummies(test_X)",
                "train_x = train_x.fillna(0)",
                "train_x"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "visualize_data"
            ],
            "content": [
                "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence",
                "from sklearn.ensemble import GradientBoostingRegressor",
                "",
                "plot_model = GradientBoostingRegressor()",
                "plot_model.fit(train_x, train_y)",
                "dep_plots = plot_partial_dependence(plot_model,       ",
                "                                   features=[1, 3], ",
                "                                   X=train_x,            # raw predictors data.",
                "                                   feature_names=['Pclass', 'Age', 'SibSp', 'Fare'], # labels on graphs",
                "                                   grid_resolution=10) # number of values to plot on x axis"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from xgboost import XGBRegressor",
                "from sklearn.preprocessing import Imputer",
                "from sklearn.pipeline import make_pipeline",
                "from sklearn.model_selection import cross_val_score",
                "",
                "data_model = make_pipeline(Imputer(), XGBRegressor())",
                "data_model.fit(train_x, train_y)",
                "",
                "scores = cross_val_score(data_model, train_x, train_y, scoring='neg_mean_absolute_error')",
                "print(scores)",
                "print('Mean Absolute Error %2f' %(-1 * scores.mean()))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "prediction = data_model.predict(test_x)",
                "print('Estimated survivors: ' + str(prediction.astype(int).sum()) + ' passengers')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "result = test.assign(Survived=prediction.astype(int))",
                "result.to_csv(submission_file_path,sep=',',columns=['PassengerId', 'Survived'], index=False)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "\n",
                "import sklearn\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.ensemble import AdaBoostClassifier\n",
                "\n",
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn import preprocessing\n",
                "\n",
                "%matplotlib inline"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "adult_train = pd.read_csv(\"/kaggle/input/adult-pmr3508/train_data.csv\", na_values = \"?\")\n",
                "adult_train.shape\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "adult_test = pd.read_csv(\"/kaggle/input/adult-pmr3508/test_data.csv\", na_values = \"?\")\n",
                "adult_test.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "adult_train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "adult_train.describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "adult_train.describe(exclude = [np.number])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "n_adult = adult_train.dropna()\n",
                "n_adult.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "cat = adult_train.describe(exclude = [np.number]).columns\n",
                "\n",
                "categoricAdult = adult_train[cat].apply(pd.Categorical)\n",
                "\n",
                "for col in cat:\n",
                "    adult_train[col + \"_cat\"] = categoricAdult[col].cat.codes\n",
                "categoricTestAdult = adult_test[cat[:-1]].apply(pd.Categorical)\n",
                "\n",
                "for col in cat[:-1]:\n",
                "    adult_test[col + \"_cat\"] = categoricTestAdult[col].cat.codes"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "adult_train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.pairplot(adult_train, vars=[\"age\", \"fnlwgt\", \"education.num\", \"capital.gain\", \"capital.loss\", \n",
                "                          \"hours.per.week\"], hue=\"income\", diag_kws={'bw':\"1.0\"}, corner=True)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "adult_train[\"native.country\"].value_counts().plot(kind=\"pie\", figsize = (8,8))\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "process_data"
            ],
            "content": [
                "adult_copy = adult_train.copy()\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "le = LabelEncoder()\n",
                "adult_copy[\"income\"] = le.fit_transform(adult_copy['income'])\n",
                "\n",
                "#heat map:\n",
                "plt.figure(figsize=(10,10))\n",
                "mask = np.zeros_like(adult_copy.corr(), dtype=np.bool)\n",
                "mask[np.triu_indices_from(mask)] = True\n",
                "sns.heatmap(adult_copy.corr(), square=True, vmin=-1, vmax=1, annot = True, linewidths=.5, mask=mask)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, axes = plt.subplots(nrows = 2, ncols = 2)\n",
                "plt.tight_layout(pad = .4, w_pad = .5, h_pad = 1.)\n",
                "\n",
                "adult_train.groupby(['sex', 'income']).size().unstack().plot(kind = 'bar', stacked = True, ax = axes[0, 0], figsize = (20, 15))\n",
                "\n",
                "relationship = adult_train.groupby(['relationship', 'income']).size().unstack()\n",
                "relationship['sum'] = adult_train.groupby('relationship').size()\n",
                "relationship = relationship.sort_values('sum', ascending = False)[['<=50K', '>50K']]\n",
                "relationship.plot(kind = 'bar', stacked = True, ax = axes[0, 1])\n",
                "\n",
                "education = adult_train.groupby(['education', 'income']).size().unstack()\n",
                "education['sum'] = adult_train.groupby('education').size()\n",
                "education = education.sort_values('sum', ascending = False)[['<=50K', '>50K']]\n",
                "education.plot(kind = 'bar', stacked = True, ax = axes[1, 0])\n",
                "\n",
                "occupation = adult_train.groupby(['occupation', 'income']).size().unstack()\n",
                "occupation['sum'] = adult_train.groupby('occupation').size()\n",
                "occupation = occupation.sort_values('sum', ascending = False)[['<=50K', '>50K']]\n",
                "occupation.plot(kind = 'bar', stacked = True, ax = axes[1, 1])\n",
                "\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "princ_num_colum= ['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']\n",
                "princ_cat_colum= ['occupation', 'relationship', 'sex','education']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_train = adult_train[princ_num_colum + princ_cat_colum]\n",
                "numX_train = adult_train[princ_num_colum + list(map(lambda x: x + \"_cat\", princ_cat_colum))]\n",
                "\n",
                "X_test = adult_test[princ_num_colum + princ_cat_colum]\n",
                "numXadul_test = adult_test[princ_num_colum + list(map(lambda x: x + \"_cat\", princ_cat_colum))]\n",
                "\n",
                "Yadult = adult_train.income"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "classifiers = {}\n",
                "scores = 0.0\n",
                "\n",
                "\n",
                "for k in range(30, 35):\n",
                "    knn = KNeighborsClassifier(k, metric = 'manhattan')\n",
                "    score = np.mean(cross_val_score(knn, numX_train, Yadult, cv = 10))\n",
                "    \n",
                "    if score > scores:\n",
                "        bestK = k\n",
                "        scores = score\n",
                "        classifiers['KNN'] = knn\n",
                "\n",
                "        \n",
                "classifiers['KNN'].fit(numX_train, Yadult)\n",
                "        \n",
                "print(\"Best acc: {}, K = {}\".format(scores, bestK))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "content": [
                "%%time\n",
                "\n",
                "predictions = classifiers['KNN'].predict(numXadul_test)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "id_index = pd.DataFrame({'Id' : list(range(len(predictions)))})\n",
                "income = pd.DataFrame({'income' : predictions})\n",
                "result = income"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "result.to_csv(\"submission.csv\", index = True, index_label = 'Id')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "",
                "# helpful character encoding module",
                "import chardet",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# start with a string",
                "before = \"This is the euro symbol: \"",
                "",
                "# check to see what datatype it is",
                "type(before)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# encode it to a different encoding, replacing characters that raise errors",
                "after = before.encode(\"utf-8\", errors = \"replace\")",
                "",
                "# check the type",
                "type(after)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# take a look at what the bytes look like",
                "after"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# convert it back to utf-8",
                "print(after.decode(\"utf-8\"))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# try to decode our bytes with the ascii encoding",
                "print(after.decode(\"ascii\"))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# start with a string",
                "before = \"This is the euro symbol: \"",
                "",
                "# encode it to a different encoding, replacing characters that raise errors",
                "after = before.encode(\"ascii\", errors = \"replace\")",
                "",
                "# convert it back to utf-8",
                "print(after.decode(\"ascii\"))",
                "",
                "# We've lost the original underlying byte string! It's been ",
                "# replaced with the underlying byte string for the unknown character :("
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Your turn! Try encoding and decoding different symbols to ASCII and",
                "# see what happens. I'd recommend $, #,  and  but feel free to",
                "# try other characters. What happens? When would this cause problems?",
                "",
                "# start with a string",
                "my_text = \"i'll try the recommended $, #,  and   and see what happens.\"",
                "",
                "# encode it to a different encoding, replacing characters that raise errors",
                "my_text_encoded = my_text.encode(\"ascii\", errors = \"replace\")",
                "",
                "# convert it back to utf-8",
                "print(my_text_encoded.decode(\"ascii\"))",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "# try to read in a file not in UTF-8",
                "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "# look at the first ten thousand bytes to guess the character encoding",
                "with open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:",
                "    result = chardet.detect(rawdata.read(10000))",
                "",
                "# check what the character encoding might be",
                "print(result)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "# read in the file with the encoding detected by chardet",
                "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')",
                "",
                "# look at the first few lines",
                "kickstarter_2016.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "# Your Turn! Trying to read in this file gives you an error. Figure out",
                "# what the correct encoding should be and read in the file. :)",
                "",
                "read_times=[1,10,100,1000,10000,100000,1000000]",
                "with open(\"../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv\", 'rb') as rawdata1:",
                "    for i in read_times:",
                "        result1 = chardet.detect(rawdata1.read(i))",
                "        print (i, result1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "police_killings = pd.read_csv(\"../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv\", encoding='Windows-1252')",
                "",
                "# look at the first few lines",
                "police_killings.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "# save our file (will be saved as UTF-8 by default!)",
                "kickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "# Your turn! Save out a version of the police_killings dataset with UTF-8 encoding ",
                "police_killings.to_csv(\"PoliceKillingsUS-utf8.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "",
                "# read in all our data",
                "nfl_data = pd.read_csv(\"../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2017 (v4).csv\")",
                "sf_permits = pd.read_csv(\"../input/building-permit-applications-data/Building_Permits.csv\")",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0) "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# look at a few rows of the nfl_data file. I can see a handful of missing data already!",
                "nfl_data.sample(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# your turn! Look at a couple of rows from the sf_permits dataset. Do you notice any missing data?",
                "",
                "# your code goes here :)",
                "",
                "sf_permits.sample(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# get the number of missing data points per column",
                "missing_values_count = nfl_data.isnull().sum()",
                "",
                "# look at the # of missing points in the first ten columns",
                "missing_values_count[0:10]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# how many total missing values do we have?",
                "total_cells = np.product(nfl_data.shape)",
                "total_missing = missing_values_count.sum()",
                "",
                "# percent of data that is missing",
                "(total_missing/total_cells) * 100"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# your turn! Find out what percent of the sf_permits dataset is missing",
                "",
                "# get the number of missing data points per column for the sf_permits",
                "missing_values_count_sf_permits = sf_permits.isnull().sum()",
                "",
                "# count the total cells of sf_permits and its missing data",
                "total_cells_sf_permits = np.product(sf_permits.shape)",
                "total_missing_sf_permits = missing_values_count_sf_permits.sum()",
                "",
                "# percent of data that is missing on sf_permits",
                "(total_missing_sf_permits/total_cells_sf_permits) * 100"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# look at the # of missing points in the first ten columns",
                "missing_values_count[0:10]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# look for the percentage of missing data in order to have some idea of it.",
                "print(\"Percentage of missinf field 'Street Number Suffix': {0:.2f} %\".format((sf_permits['Street Number Suffix'].isnull().sum()/sf_permits['Street Number Suffix'].shape[0])*100))",
                "print(\"Percentageof missinf field 'Zipcode': {0:.2f} %\".format((sf_permits['Zipcode'].isnull().sum()/sf_permits['Zipcode'].shape[0])*100))",
                "#sf_permits['Zipcode']",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# remove all the rows that contain a missing value",
                "nfl_data.dropna()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# remove all columns with at least one missing value",
                "columns_with_na_dropped = nfl_data.dropna(axis=1)",
                "columns_with_na_dropped.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# just how much data did we lose?",
                "print(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])",
                "print(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Your turn! Try removing all the rows from the sf_permits dataset that contain missing values. How many are left?",
                "# remove all the rows that contain a missing value",
                "sf_permits.dropna()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Now try removing all the columns with empty values. Now how much of your data is left?",
                "columns_with_na_dropped_sf = sf_permits.dropna(axis=1)",
                "",
                "# just how much data did we lose?",
                "print(\"Columns in original dataset: %d \\n\" % sf_permits.shape[1])",
                "print(\"Columns with na's dropped: %d\" % columns_with_na_dropped_sf.shape[1])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# get a small subset of the NFL dataset",
                "subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()",
                "subset_nfl_data"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# replace all NA's with 0",
                "subset_nfl_data.fillna(0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# replace all NA's the value that comes directly after it in the same column, ",
                "# then replace all the reamining na's with 0",
                "subset_nfl_data.fillna(method = 'bfill', axis=0).fillna(\"0\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Your turn! Try replacing all the NaN's in the sf_permits data with the one that",
                "# comes directly after it and then ",
                "",
                "sf_permits_sub = sf_permits.fillna(method=\"bfill\", axis=0).fillna(0)",
                "",
                "sf_permits_sub.sample(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "",
                "# helpful modules",
                "import fuzzywuzzy",
                "from fuzzywuzzy import process",
                "import chardet",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "# look at the first ten thousand bytes to guess the character encoding",
                "with open(\"../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", 'rb') as rawdata:",
                "    result = chardet.detect(rawdata.read(100000))",
                "",
                "# check what the character encoding might be",
                "print(result)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "# read in our dat",
                "suicide_attacks = pd.read_csv(\"../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", ",
                "                              encoding='Windows-1252')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "suicide_attacks.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# convert to lower case",
                "suicide_attacks['City'] = suicide_attacks['City'].str.lower()",
                "# remove trailing white spaces",
                "suicide_attacks['City'] = suicide_attacks['City'].str.strip()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Your turn! Take a look at all the unique values in the \"Province\" column. ",
                "# Then convert the column to lowercase and remove any trailing white spaces",
                "",
                "# get all the unique values in the 'Province' column",
                "provinces = suicide_attacks['Province'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "provinces.sort()",
                "provinces"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# convert to lower case",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.lower()",
                "# remove trailing white spaces",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.strip()",
                "",
                "# get all the unique values in the 'City' column",
                "provinces = suicide_attacks['Province'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "provinces.sort()",
                "provinces"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# get the top 10 closest matches to \"d.i khan\"",
                "matches = fuzzywuzzy.process.extract(\"d.i khan\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "",
                "# take a look at them",
                "matches"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# function to replace rows in the provided column of the provided dataframe",
                "# that match the provided string above the provided ratio with the provided string",
                "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):",
                "    # get a list of unique strings",
                "    strings = df[column].unique()",
                "    ",
                "    # get the top 10 closest matches to our input string",
                "    matches = fuzzywuzzy.process.extract(string_to_match, strings, ",
                "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "",
                "    # only get matches with a ratio > 90",
                "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]",
                "",
                "    # get the rows of all the close matches in our dataframe",
                "    rows_with_matches = df[column].isin(close_matches)",
                "",
                "    # replace all rows with close matches with the input matches ",
                "    df.loc[rows_with_matches, column] = string_to_match",
                "    ",
                "    # let us know the function's done",
                "    print(\"All done!\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# use the function we just wrote to replace close matches to \"d.i khan\" with \"d.i khan\"",
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"d.i khan\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Your turn! It looks like 'kuram agency' and 'kurram agency' should",
                "# be the same city. Correct the dataframe so that they are.",
                "",
                "# get the top 10 closest matches to \"d.i khan\"",
                "matches = fuzzywuzzy.process.extract(\"kuram agency\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "",
                "# take a look at them",
                "matches"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"kuram agency\", min_ratio = 94)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "import seaborn as sns",
                "import datetime",
                "",
                "# read in our data",
                "earthquakes = pd.read_csv(\"../input/earthquake-database/database.csv\")",
                "landslides = pd.read_csv(\"../input/landslide-events/catalog.csv\")",
                "volcanos = pd.read_csv(\"../input/volcanic-eruptions/database.csv\")",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# print the first few rows of the date column",
                "print(landslides['date'].head())"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# check the data type of our date column",
                "landslides['date'].dtype"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Your turn! Check the data type of the Date column in the earthquakes dataframe",
                "# (note the capital 'D' in date!)",
                "earthquakes['Date'].dtype"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# create a new column, date_parsed, with the parsed dates",
                "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format = \"%m/%d/%y\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# print the first few rows",
                "landslides['date_parsed'].head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Your turn! Create a new column, date_parsed, in the earthquakes",
                "# dataset that has correctly parsed dates in it. (Don't forget to ",
                "# double-check that the dtype is correct!)",
                "print(\"the data type is '{}' \".format(earthquakes['Date'].dtype))",
                "print(earthquakes['Date'].head())",
                "",
                "#since the date format is supposebly american i will assume it is month/day/year",
                "earthquakes['date_parsed']=pd.to_datetime(earthquakes['Date'], format = \"%m/%d/%Y\")",
                "earthquakes['date_parsed'].head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#check larger range-found it!",
                "earthquakes.iloc[3370:3390]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "earthquakes['date_parsed']=pd.to_datetime(earthquakes['Date'], infer_datetime_format=True)",
                "earthquakes['date_parsed'].head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# try to get the day of the month from the date column",
                "day_of_month_landslides = landslides['date'].dt.day"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# get the day of the month from the date_parsed column",
                "day_of_month_landslides = landslides['date_parsed'].dt.day"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Your turn! get the day of the month from the date_parsed column",
                "",
                "# Didn'understand much, i will do for earthquakes",
                "day_of_month_earthquakes = earthquakes['date_parsed'].dt.day"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# remove na's",
                "day_of_month_landslides = day_of_month_landslides.dropna()",
                "",
                "# plot the day of the month",
                "sns.distplot(day_of_month_landslides, kde=False, bins=31)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Your turn! Plot the days of the month from your",
                "# earthquake dataset and make sure they make sense.",
                "",
                "day_of_month_earthquakes = day_of_month_earthquakes.dropna()",
                "sns.distplot(day_of_month_earthquakes, kde=False, bins=31)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "volcanos['Last Known Eruption'].sample(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "",
                "# for Box-Cox Transformation",
                "from scipy import stats",
                "",
                "# for min_max scaling",
                "from mlxtend.preprocessing import minmax_scaling",
                "",
                "# plotting modules",
                "import seaborn as sns",
                "import matplotlib.pyplot as plt",
                "",
                "# read in all our data",
                "kickstarters_2017 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201801.csv\")",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# generate 1000 data points randomly drawn from an exponential distribution",
                "original_data = np.random.exponential(size = 1000)",
                "",
                "# mix-max scale the data between 0 and 1",
                "scaled_data = minmax_scaling(original_data, columns = [0])",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(original_data, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(scaled_data, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# normalize the exponential data with boxcox",
                "normalized_data = stats.boxcox(original_data)",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(original_data, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(normalized_data[0], ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#a little glimpse at the data",
                "",
                "kickstarters_2017.sample(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# select the usd_goal_real column",
                "usd_goal = kickstarters_2017.usd_goal_real",
                "",
                "# scale the goals from 0 to 1",
                "scaled_data = minmax_scaling(usd_goal, columns = [0])",
                "",
                "# plot the original & scaled data together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(scaled_data, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Your turn! ",
                "",
                "# We just scaled the \"usd_goal_real\" column. What about the \"goal\" column?",
                "goal = kickstarters_2017.goal",
                "",
                "# scale the goals from 0 to 1",
                "scaled_goal = minmax_scaling(goal, columns = [0])",
                "",
                "# plot the original & scaled data together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(kickstarters_2017.goal, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(scaled_goal, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# get the index of all positive pledges (Box-Cox only takes postive values)",
                "index_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0",
                "",
                "# get only positive pledges (using their indexes)",
                "positive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]",
                "",
                "# normalize the pledges (w/ Box-Cox)",
                "normalized_pledges = stats.boxcox(positive_pledges)[0]",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(positive_pledges, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(normalized_pledges, ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Your turn! ",
                "# We looked as the usd_pledged_real column. What about the \"pledged\" column? Does it have the same info?",
                "index_of_positive_pledged = kickstarters_2017.pledged > 0",
                "",
                "# get only positive pledged (using their indexes)",
                "positive_pledged = kickstarters_2017.pledged.loc[index_of_positive_pledges]",
                "",
                "# normalize the pledges (w/ Box-Cox)",
                "normalized_pledged = stats.boxcox(positive_pledged)[0]",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(positive_pledged, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(normalized_pledged, ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import os\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "%matplotlib inline\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "# ML\n",
                "# for transformers creation\n",
                "from sklearn.base import BaseEstimator, TransformerMixin\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "\n",
                "# models and metrics\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
                "from sklearn.metrics import classification_report\n",
                "# distributions for random search\n",
                "from scipy.stats import randint, expon, reciprocal\n",
                "\n",
                "sns.set(rc={'figure.figsize':(20, 15)})"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "transfer_results",
                "validate_data"
            ],
            "content": [
                "import joblib  # for saving models from skikit-learn\n",
                "\n",
                "# some utils for saving and reading later\n",
                "def save(model, cv_info, classification_report, name=\"model\", cv_scores=None):\n",
                "    _model = {\n",
                "        \"cv_info\": cv_info, \"classification_report\": classification_report, \"model\": model, \"cv_scores\": cv_scores\n",
                "    }\n",
                "    joblib.dump(_model, \"/kaggle/working/\" + name + \".pkl\")\n",
                "\n",
                "\n",
                "def load(name=\"model\", verbose=True, with_metadata=False):\n",
                "    _model = joblib.load(\"/kaggle/working/\" + name + \".pkl\")\n",
                "    if verbose:\n",
                "        print(\"\\nLoading model with the following info:\\n\")\n",
                "        [print(\"{key}: {val}\".format(key=key, val=val)) for key, val in _model[\"cv_info\"].items()]\n",
                "        print(\"\\nClassification Report:\\n\")\n",
                "        print(_model[\"classification_report\"])\n",
                "    if not with_metadata:\n",
                "        return _model[\"model\"]\n",
                "    else:\n",
                "        return _model"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for dirname, _, filenames in os.walk('../input/titanic'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train = pd.read_csv('../input/titanic/train.csv')\n",
                "test = pd.read_csv('../input/titanic/test.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[\"Survived\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[\"Sex\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[\"Embarked\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# train[\"Name\"].value_counts() as imagined are uniques\n",
                "# train[\"Ticket\"].value_counts() is not unique, but has some few repetitions\n",
                "train[\"Pclass\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "import pandas_profiling \n",
                "\n",
                "train.profile_report()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "train.hist(figsize=(20, 15))\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "corr_matrix = train.corr()\n",
                "corr_matrix[\"Survived\"]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# hypothesis for feature engineering \"women and children first\"\n",
                "sns.violinplot(x=\"Sex\", y=\"Age\", hue=\"Survived\",\n",
                "                    data=train, palette=\"muted\", split=True)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Hypotesis: since the survived looks like \"bimodal\" near 15 years, we should try to see \n",
                "# the correlation of categorizing if the passenger is less than 15 years\n",
                "\n",
                "train[\"Age\"] = train[\"Age\"] // 15 * 15\n",
                "train[[\"Age\", \"Survived\"]].groupby(['Age']).mean()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "train[\"RelativesOnboard\"] = train[\"SibSp\"] + train[\"Parch\"]\n",
                "# train[[\"RelativesOnboard\", \"Survived\"]].groupby(['RelativesOnboard']).mean()\n",
                "train[[\"RelativesOnboard\", \"Survived\"]].groupby([\"RelativesOnboard\"]).mean()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# count total family members look to have a better discrimination on survival rate\n",
                "# since number of siblings is nearer to the mean survival rate 38% \n",
                "train[[\"SibSp\", \"Survived\"]].groupby(['SibSp']).mean()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[[\"Parch\", \"Survived\"]].groupby(['Parch']).mean()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Age grouped by 15 years have near 0 correlation, but some groups have more survival rate than others\n",
                "# this only means that the relation of age groups and survival rate are non-linear\n",
                "train.corr()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Transformers created by https://github.com/ageron/handson-ml2\n",
                "\n",
                "# this transformers we will choose which attributes, late numerical and categorical\n",
                "# to use some input strategies\n",
                "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
                "    def __init__(self, attribute_names):\n",
                "        self.attribute_names = attribute_names\n",
                "\n",
                "    def fit(self, X, y=None):\n",
                "        return self\n",
                "\n",
                "    def transform(self, X):\n",
                "        return X[self.attribute_names]\n",
                "\n",
                "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
                "    def fit(self, X, y=None):\n",
                "        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n",
                "                                        index=X.columns)\n",
                "        return self\n",
                "\n",
                "    def transform(self, X, y=None):\n",
                "        return X.fillna(self.most_frequent_)\n",
                "\n",
                "\n",
                "class AgeGrouper(BaseEstimator, TransformerMixin):\n",
                "    def __init__(self, new_attribute=\"AgeGrp\", attribute_name=\"Age\", group_scale=15, del_originals=True):\n",
                "        self.group_scale = group_scale\n",
                "        self.attribute_name = attribute_name\n",
                "        self.new_attribute = new_attribute\n",
                "        self.del_originals = del_originals\n",
                "\n",
                "    def fit(self, X, y=None):\n",
                "        self.age_groups = X[self.attribute_name] // self.group_scale * self.group_scale\n",
                "        return self\n",
                "\n",
                "    def transform(self, X, y=None):\n",
                "        X[self.new_attribute] = self.age_groups\n",
                "        if self.del_originals:\n",
                "            X.drop(columns=self.attribute_name, axis=1, inplace=True)\n",
                "        return X\n",
                "\n",
                "\n",
                "class AtributesAdder(BaseEstimator, TransformerMixin):\n",
                "    def __init__(self, new_attribute=\"RelativesOnboard\", attribute_names=[\"SibSp\", \"Parch\"], del_originals=True):\n",
                "        self.attribute_names = attribute_names\n",
                "        self.final_attr = 0\n",
                "        self.new_attribute = new_attribute\n",
                "        self.del_originals = del_originals\n",
                "\n",
                "    def fit(self, X, y=None):\n",
                "        for attr in self.attribute_names:\n",
                "            self.final_attr += X[attr]\n",
                "        return self\n",
                "\n",
                "    def transform(self, X, y=None):\n",
                "        X[self.new_attribute] = self.final_attr\n",
                "        if self.del_originals:\n",
                "            X.drop(columns=self.attribute_names, axis=1, inplace=True)\n",
                "        return X"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Numerical Pipeline\n",
                "num_pipeline = Pipeline([\n",
                "        (\"select_numeric\", DataFrameSelector([\"Age\", \"Fare\", \"SibSp\", \"Parch\"])),\n",
                "        (\"age_grouper\", AgeGrouper(attribute_name=\"Age\", group_scale=15)),\n",
                "        (\"total_relatives\", AtributesAdder(attribute_names=[\"SibSp\", \"Parch\"], del_originals=True)),\n",
                "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
                "    ])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Categorical Pipeline\n",
                "cat_pipeline = Pipeline([\n",
                "        (\"select_cat\", DataFrameSelector([\"Pclass\", \"Sex\", \"Embarked\"])),\n",
                "        (\"imputer\", MostFrequentImputer()),\n",
                "        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n",
                "    ])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.pipeline import FeatureUnion\n",
                "preprocess_pipeline = FeatureUnion(transformer_list=[\n",
                "        (\"num_pipeline\", num_pipeline),\n",
                "        (\"cat_pipeline\", cat_pipeline),\n",
                "    ])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_train = preprocess_pipeline.fit_transform(train)\n",
                "y_train = train[\"Survived\"]\n",
                "\n",
                "X_train_val, X_test_val, y_train_val, y_test_val = train_test_split(\n",
                "        X_train, y_train, test_size=0.3, random_state=42)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "mask = np.zeros_like(corr_matrix)\n",
                "mask[np.triu_indices_from(mask)] = True\n",
                "with sns.axes_style(\"white\"):\n",
                "    f, ax = plt.subplots(figsize=(7, 5))\n",
                "    ax = sns.heatmap(corr_matrix, mask=mask, vmax=.3, square=True, cmap=\"YlGnBu\")"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "models = {\n",
                "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
                "    \"RandomForest\": RandomForestClassifier(),\n",
                "    \"SVM\": SVC(),\n",
                "}\n",
                "\n",
                "randomized_params = {\n",
                "    \"KNeighborsClassifier\": {\n",
                "        \"n_neighbors\": randint(low=1, high=30),\n",
                "    },\n",
                "    \"RandomForest\": {\n",
                "        \"n_estimators\": randint(low=1, high=200),\n",
                "        \"max_features\": randint(low=1, high=8),\n",
                "    },\n",
                "    \"SVM\": {\n",
                "        \"kernel\": [\"linear\", \"rbf\"],\n",
                "        \"C\": reciprocal(0.1, 200000),\n",
                "        \"gamma\": expon(scale=1.0),\n",
                "    }\n",
                "}"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "content": [
                "scoring = \"accuracy\"\n",
                "\n",
                "\n",
                "for model_name in models.keys():\n",
                "    grid = RandomizedSearchCV(models[model_name], param_distributions=randomized_params[model_name], n_iter=100,\n",
                "                                  scoring=scoring, cv=5, verbose=2, random_state=42,  n_jobs=-1)\n",
                "    grid.fit(X_train, y_train)\n",
                "\n",
                "    scores = cross_val_score(grid.best_estimator_, X_train_val, y_train_val, cv=10,\n",
                "                             scoring=scoring, verbose=0, n_jobs=-1)\n",
                "\n",
                "    CV_scores = scores.mean()\n",
                "    STDev = scores.std()\n",
                "    Test_scores = grid.score(X_test_val, y_test_val)\n",
                "\n",
                "    cv_score = {'Model_Name': model_name, 'Parameters': grid.best_params_, 'Test_Score': Test_scores,\n",
                "                'CV Mean': CV_scores, 'CV STDEV': STDev}\n",
                "\n",
                "    clf = grid.best_estimator_.fit(X_train_val, y_train_val)\n",
                "    clf.score(X_test_val, y_test_val)\n",
                "    y_pred = clf.predict(X_test_val)\n",
                "    clf_report = classification_report(y_test_val, y_pred)\n",
                "    save(grid, cv_score, clf_report, name=\"titanic_\"+model_name+\"_02\", cv_scores=scores)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "knn_grid = load(\"titanic_KNeighborsClassifier_02\", with_metadata=True)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "svc_grid = load(\"titanic_SVM_02\", with_metadata=True)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "random_forest_grid = load(\"titanic_RandomForest_02\", with_metadata=True)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(8, 4))\n",
                "plt.plot([1]*10, knn_grid[\"cv_scores\"], \".\")\n",
                "plt.plot([2]*10, svc_grid[\"cv_scores\"], \".\")\n",
                "plt.plot([3]*10, random_forest_grid[\"cv_scores\"], \".\")\n",
                "plt.boxplot([knn_grid[\"cv_scores\"], svc_grid[\"cv_scores\"], random_forest_grid[\"cv_scores\"]], labels=(\"KNN\", \"SVM\", \"Random Forest\"))\n",
                "plt.ylabel(\"Accuracy\", fontsize=14)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "random_forest = random_forest_grid[\"model\"].best_estimator_.fit(X_train, y_train)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_test = preprocess_pipeline.fit_transform(test)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "y_pred = random_forest.predict(X_test)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
                "submission_df['PassengerId'] = test['PassengerId']\n",
                "submission_df['Survived'] = y_pred"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "transfer_results",
                "validate_data"
            ],
            "content": [
                "submission_df.to_csv(\"/kaggle/working/titanic_02.csv\", header=True, index=False)\n",
                "submission_df.head(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "from sklearn.impute import SimpleImputer",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "from sklearn.ensemble import RandomForestRegressor",
                "from sklearn.metrics import mean_squared_error",
                "from sklearn.model_selection import train_test_split",
                "import math",
                "import os",
                "print(os.listdir(\"../input\"))",
                "df = pd.read_csv(\"../input/train.csv\")",
                "test_final = pd.read_csv(\"../input/test.csv\")",
                "test_final_id = test_final"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def Filterdataset (dataset):    ",
                "    dataset = dataset.copy()",
                "    ",
                "    dataset['has_alley'] = df['Alley'].fillna(0).apply(lambda _: 0 if _ == 0 else 1)",
                "    dataset = dataset.fillna(value= {'Alley':'No alley access'})",
                "    dataset['has_BsmtQual'] = df['BsmtQual'].fillna(0).apply(lambda _: 0 if _ == 0 else 1)",
                "    dataset = dataset.fillna(value= {'BsmtQual':'No Basement'})",
                "    dataset['has_BsmtCond'] = df['BsmtCond'].fillna(0).apply(lambda _: 0 if _ == 0 else 1)",
                "    dataset = dataset.fillna(value= {'BsmtCond':'No Basement'})",
                "    dataset['Age'] = dataset['YrSold'] - dataset['YearBuilt']",
                "    dataset['AgeSinceRemode'] = dataset['YrSold'] - dataset['YearRemodAdd']",
                "    dataset['WholeArea'] = (dataset['GrLivArea'] + dataset['GarageArea'] + dataset['1stFlrSF'] + dataset['2ndFlrSF']).astype('float32')",
                "    #dataset = dataset.select_dtypes(include=['float64','int'])",
                "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='most_frequent')",
                "    dataset = pd.get_dummies(dataset, drop_first=True)",
                "    datasetc = dataset",
                "    dataset = imp_mean.fit_transform(dataset)",
                "    dataset = pd.DataFrame(data = dataset, index = datasetc.index, columns = datasetc.columns)",
                "    if 'Id' in dataset.columns:",
                "        dataset = dataset.drop(['Id'], axis=1)",
                "    if 'SalePrice' in dataset.columns:",
                "        dataset = dataset.drop(['SalePrice'], axis=1)",
                "    return dataset"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "y = df['SalePrice']",
                "X = df.drop('SalePrice',axis=1)",
                "",
                "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.10, random_state=0)",
                "",
                "X_train = Filterdataset(X_train)",
                "X_test = Filterdataset(X_test)",
                "test_final = Filterdataset(test_final)",
                "columns = []",
                "for c in X_train.columns:",
                "    if c in X_test.columns:",
                "        if c in test_final.columns:",
                "            columns.append(c)",
                "X_train = X_train[columns]",
                "test_final = test_final[columns]",
                "X_test = X_test[columns]",
                "regr = RandomForestRegressor(max_depth=50, random_state=0, n_estimators=100)",
                "regr.fit(X_train, np.log(y_train))",
                "print(len(X_train.columns), len(X_test.columns), len(test_final.columns))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "y_pred_train = regr.predict(X_train)",
                "y_pred_test = regr.predict(X_test)",
                "y_pred_final = np.exp(regr.predict(test_final))",
                "print(y_pred_final)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "def Mrmse(y_true,y_pred):",
                "    y_true = np.log(y_true)",
                "    #y_pred = np.log(y_pred)",
                "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))",
                "    return rmse"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(Mrmse(y_train,y_pred_train))",
                "print(Mrmse(y_test,y_pred_test))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "my_submission = pd.DataFrame({'Id': test_final_id['Id'], 'SalePrice': y_pred_final})",
                "# you could use any filename. We choose submission here",
                "my_submission.to_csv('submission.csv', index=False)",
                "#my_submission"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "df = pd.read_csv('../input/attendancemarks/AttendanceMarksSA.csv')\n",
                "df.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "x = df['MSE']\n",
                "y = df['ESE']\n",
                "sns.scatterplot(x,y)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "b0 = 0\n",
                "b1 = 0\n",
                "alpha = 0.01\n",
                "count = 10000\n",
                "n = float(len(x))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "for i in range(count):\n",
                "    y_bar = b1*x + b0\n",
                "    b1 = b1 - (alpha/n)*sum(x*(y_bar-y))\n",
                "    b0 = b0 - (alpha/n)*sum(y_bar-y)\n",
                "        \n",
                "print(b0,b1)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "y_bar = b1*x + b0\n",
                "\n",
                "plt.scatter(x,y)\n",
                "plt.plot([min(x),max(x)],[min(y_bar),max(y_bar)],color='red') #regression line\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "import math\n",
                "def RSE(y_true,y_predict):\n",
                "    y_true = np.array(y_true)\n",
                "    y_predict = np.array(y_predict)\n",
                "    RSS = np.sum(np.square(y_true-y_predict))\n",
                "    \n",
                "    rse = math.sqrt(RSS/(len(y_true)-2))\n",
                "    return rse\n",
                "\n",
                "rse = RSE(df['ESE'],y_bar)\n",
                "print(rse)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LinearRegression"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "X = np.array(df['MSE']).reshape(-1,1)\n",
                "Y = np.array(df['ESE']).reshape(-1,1)\n",
                "\n",
                "lr = LinearRegression()\n",
                "lr.fit(X,Y)\n",
                "\n",
                "print(lr.coef_)\n",
                "print(lr.intercept_)\n",
                "\n",
                "yp = lr.predict(X)\n",
                "rse = RSE(Y,yp)\n",
                "\n",
                "print(rse)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "import time"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "#Borrowed from Faron's Road to 4 kernel\n",
                "DATA_DIR = \"../input\"\n",
                "\n",
                "ID_COLUMN = 'Id'\n",
                "TARGET_COLUMN = 'Response'\n",
                "\n",
                "SEED = 0\n",
                "CHUNKSIZE = 10000\n",
                "NROWS = 50000\n",
                "\n",
                "TRAIN_NUMERIC = \"{0}/train_numeric.csv\".format(DATA_DIR)\n",
                "TRAIN_DATE = \"{0}/train_date.csv\".format(DATA_DIR)\n",
                "\n",
                "TEST_NUMERIC = \"{0}/test_numeric.csv\".format(DATA_DIR)\n",
                "TEST_DATE = \"{0}/test_date.csv\".format(DATA_DIR)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "#Borrowed from Faron's Road to 4 kernel\n",
                "\n",
                "train = pd.read_csv(TRAIN_NUMERIC, usecols=[ID_COLUMN, TARGET_COLUMN], nrows=NROWS)\n",
                "test = pd.read_csv(TEST_NUMERIC, usecols=[ID_COLUMN], nrows=NROWS)\n",
                "\n",
                "train[\"StartTime\"] = -1\n",
                "test[\"StartTime\"] = -1\n",
                "train[\"EndTime\"] = -1\n",
                "test[\"EndTime\"] = -1"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "#Borrowed from Faron's Road to 4 kernel\n",
                "\n",
                "nrows = 0\n",
                "print ('nrows',)\n",
                "\n",
                "train_reader = pd.read_csv(TRAIN_DATE, chunksize=CHUNKSIZE, iterator=True)\n",
                "test_reader = pd.read_csv(TEST_DATE, chunksize=CHUNKSIZE, iterator=True)\n",
                "for i in range(int(NROWS/CHUNKSIZE) + 1):\n",
                "    tr = train_reader.get_chunk()\n",
                "    te = test_reader.get_chunk()\n",
                "\n",
                "#for tr, te in zip(pd.read_csv(TRAIN_DATE, chunksize=CHUNKSIZE), pd.read_csv(TEST_DATE, chunksize=CHUNKSIZE)):\n",
                "    feats = np.setdiff1d(tr.columns, [ID_COLUMN])    \n",
                "\n",
                "    stime_tr = tr[feats].min(axis=1).values\n",
                "    stime_te = te[feats].min(axis=1).values\n",
                "\n",
                "    etime_tr = tr[feats].max(axis=1).values\n",
                "    etime_te = te[feats].max(axis=1).values\n",
                "    \n",
                "    train.loc[train.Id.isin(tr.Id), 'StartTime'] = stime_tr\n",
                "    test.loc[test.Id.isin(te.Id), 'StartTime'] = stime_te\n",
                "\n",
                "    train.loc[train.Id.isin(tr.Id), 'EndTime'] = etime_tr\n",
                "    test.loc[test.Id.isin(te.Id), 'EndTime'] = etime_te\n",
                "    \n",
                "    nrows += CHUNKSIZE\n",
                "    print (nrows,)\n",
                "    if nrows >= NROWS:\n",
                "        break"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Borrowed from Faron's Road to 4 kernel\n",
                "#HAD to change the names so they are easy to type. what can I say \\_()_/\n",
                "\n",
                "ntrain = train.shape[0]\n",
                "train_test = pd.concat((train, test)).reset_index(drop=True).reset_index(drop=False)\n",
                "\n",
                "train_test['Duration'] = train_test['EndTime'] - train_test['StartTime']\n",
                "\n",
                "train_test['magic1'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\n",
                "train_test['magic2'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n",
                "\n",
                "train_test = train_test.sort_values(by=['StartTime', 'Id'], ascending=True)\n",
                "\n",
                "train_test['magic3'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\n",
                "train_test['magic4'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n",
                "\n",
                "train_test = train_test.sort_values(by=['index']).drop(['index'], axis=1)\n",
                "train = train_test.iloc[:ntrain, :]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "def twoplot(df, col, xaxis=None):\n",
                "    ''' scatter plot a feature split into response values as two subgraphs '''\n",
                "    if col not in df.columns.values:\n",
                "        print('ERROR: %s not a column' % col)\n",
                "    ndf = pd.DataFrame(index = df.index)\n",
                "    ndf[col] = df[col]\n",
                "    ndf[xaxis] = df[xaxis] if xaxis else df.index\n",
                "    ndf['Response'] = df['Response']\n",
                "    \n",
                "    g = sns.FacetGrid(ndf, col=\"Response\", hue=\"Response\")\n",
                "    g.map(plt.scatter, xaxis, col, alpha=.7, s=1)\n",
                "    g.add_legend();\n",
                "    \n",
                "    del ndf"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'magic1')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'magic2')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'magic3')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'magic4')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'Duration')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'StartTime')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'EndTime')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "!pip install autokeras\n",
                "!pip install natsort"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import autokeras as ak\n",
                "import numpy as np \n",
                "import pandas as pd \n",
                "from glob import glob\n",
                "from skimage.io import imread\n",
                "import skimage.io as sio\n",
                "import os\n",
                "from natsort import natsorted\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
                "from skimage.transform import resize, rotate\n",
                "import warnings; warnings.filterwarnings(\"ignore\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "# train test directories\n",
                "root_dir = \"../input\"\n",
                "train_dir = root_dir + \"/train/train/\"\n",
                "test_dir  = root_dir + \"/test/test/\"\n",
                "csv_path  = root_dir + \"/train.csv\"\n",
                "sub_path  = root_dir + \"sample_submission.csv\"\n",
                "\n",
                "# loading images\n",
                "df   = pd.read_csv(csv_path)\n",
                "x    = np.array([ imread(train_dir+p)/255 for p in df.id.values])\n",
                "y    = df.has_cactus.values"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "# splitting training dataset into train/validation\n",
                "from sklearn.model_selection import train_test_split\n",
                "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.20,stratify=y)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "content": [
                "# helper functions here\n",
                "def display_images(imgs,y=None, y_pred=None):\n",
                "    n_images = imgs.shape[0]\n",
                "    n_gridx  = 5\n",
                "    n_gridy  = n_images//n_gridx\n",
                "#     n_grid   = int(np.sqrt(n_images))\n",
                "    k = 1\n",
                "    plt.figure(figsize=(10,6),frameon=False)\n",
                "    for i in range(n_gridy):\n",
                "        for j in range(n_gridx):\n",
                "            plt.subplot(n_gridy, n_gridx, k)\n",
                "            plt.imshow(imgs[k-1])\n",
                "            plt.axis(\"off\")\n",
                "            if (y is not None) and (y_pred is not None):\n",
                "                plt.title(\"y=%d | pred=%0.1f\"%(y[k-1],y_pred[k-1]))\n",
                "            elif y is not None:\n",
                "                plt.title(\"y=%d\"%y[k-1])\n",
                "            k+=1\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "def getProb(model, x):\n",
                "    xprocessed = model.preprocess(x)\n",
                "    loader = model.data_transformer.transform_test(xprocessed)\n",
                "    probs  = model.cnn.predict(loader)\n",
                "    num    = np.exp(probs[:,1])\n",
                "    denom  = num + np.exp(probs[:,0])\n",
                "    probs  = num / denom \n",
                "    return probs"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "n_samples  = 20\n",
                "idx_sample = np.random.randint(0,len(x_train),n_samples)\n",
                "display_images(x_train[idx_sample], y_train[idx_sample])"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "runFor = 5 # time in hours\n",
                "model = ak.ImageClassifier(verbose=True, augment=True )\n",
                "model.fit(x_train, y_train, time_limit=4*60*60)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "# model.final_fit(x_train, y_train, x_val, y_val, retrain=False)\n",
                "y_pred = model.predict(x_train)\n",
                "y_prob = getProb(model, x_train)\n",
                "print(\"training   accuracy  = \", accuracy_score(y_train, y_pred))\n",
                "print(\"training   recall    = \", recall_score(y_train, y_pred))\n",
                "print(\"training   precision = \", precision_score(y_train, y_pred))\n",
                "print(\"training   auc score = \", roc_auc_score(y_train, y_prob))\n",
                "print(\"training   f1 score  = \", f1_score(y_train, y_pred))\n",
                "y_pred = model.predict(x_val)\n",
                "y_prob = getProb(model, x_val)\n",
                "print(\"validation accuracy  = \", accuracy_score(y_val, y_pred))\n",
                "print(\"validation recall    = \", recall_score(y_val, y_pred))\n",
                "print(\"validation precision = \", precision_score(y_val, y_pred))\n",
                "print(\"validation auc score = \",roc_auc_score(y_val, y_prob))\n",
                "print(\"validation f1 score  = \", f1_score(y_val, y_pred))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "transfer_results"
            ],
            "content": [
                "df_test = pd.read_csv('../input/sample_submission.csv')\n",
                "x_test  = np.array([ imread(test_dir+p)/255 for p in df_test.id.values])\n",
                "x_test  = np.array(x_test)\n",
                "\n",
                "# test prediction\n",
                "y_prob_test = getProb(model, x_test)\n",
                "\n",
                "df_test['has_cactus'] = y_prob_test\n",
                "df_test.to_csv('cactus_net_submission.csv', index=False)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "df = pd.read_csv(\"../input/ecological-footprint/EcologicalFootPrint.csv\")\n",
                "print(df.isnull().sum())\n",
                "df.dropna(inplace=True)\n",
                "df = df[df['country']!='World']"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['record'].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "l = ['crop_land','grazing_land','forest_land','fishing_ground','built_up_land','carbon','total']\n",
                "for columns in l:\n",
                "    plt.figure(figsize=(15,10))\n",
                "    every_year = df.groupby('year')[columns].mean()\n",
                "    sns.barplot(every_year.index,every_year.values).set_xticklabels(sns.barplot(every_year.index,every_year.values).get_xticklabels(),rotation=\"90\")\n",
                "    plt.title(\"Year Comparation with \"+columns)\n",
                "    plt.xlabel(columns)\n",
                "    plt.ylabel(\"Count\")"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,10))\n",
                "plt.title(\"Total 10, most Carbon Producing Countring\")\n",
                "plt.xlabel(\"Country\")\n",
                "plt.ylabel(\"Rank\")\n",
                "df.groupby(['country']).mean()['carbon'].sort_values(ascending=False)[:10].plot.bar()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,10))\n",
                "plt.title(\"Total 10, lowest Carbon Producing Countring\")\n",
                "plt.xlabel(\"Country\")\n",
                "plt.ylabel(\"Rank\")\n",
                "df.groupby(['country']).mean()['carbon'].sort_values(ascending=True)[:10].plot.bar()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,10))\n",
                "plt.title(\"Total 10, Lowest Country with Forst Land\")\n",
                "plt.xlabel(\"Country\")\n",
                "plt.ylabel(\"Rank\")\n",
                "df.groupby(['country']).mean()['forest_land'].sort_values(ascending=True)[:10].plot.bar()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,10))\n",
                "plt.title(\"Total 10, Highest Country with Forst Land\")\n",
                "plt.xlabel(\"Country\")\n",
                "plt.ylabel(\"Rank\")\n",
                "df.groupby(['country']).mean()['forest_land'].sort_values(ascending=False)[:10].plot.bar()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,10))\n",
                "plt.title(\"Total 10, Highest Country with Build Up Land\")\n",
                "plt.xlabel(\"Country\")\n",
                "plt.ylabel(\"Rank\")\n",
                "df.groupby(['country']).mean()['built_up_land'].sort_values(ascending=False)[:10].plot.bar()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,10))\n",
                "plt.title(\"Total 10, Lowest Country with Build Up Land\")\n",
                "plt.xlabel(\"Country\")\n",
                "plt.ylabel(\"Rank\")\n",
                "df.groupby(['country']).mean()['built_up_land'].sort_values(ascending=True)[:10].plot.bar()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import pandas as pd",
                "from pathlib import Path",
                "import matplotlib.pyplot as plt",
                "import seaborn as sns"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "# opening train/test csv files",
                "train = pd.read_csv('../input/train.csv')",
                "test = pd.read_csv('../input/test.csv')",
                "train.head(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# 10 for 1 ration",
                "print(f\"Train shape : {train.shape}\")",
                "print(f\"Test shape : {test.shape}\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Dataset organization",
                "train.nunique()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize = (8, 5))",
                "plt.title('Category Distribuition')",
                "sns.distplot(train['landmark_id'])",
                "",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Top categories",
                "print(train['landmark_id'].value_counts().head(7))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(f\"Median number : {train['landmark_id'].value_counts().median()}\")",
                "print(f\"Mean number : {train['landmark_id'].value_counts().mean()}\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# More exhaustive description",
                "train['landmark_id'].value_counts().describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "f\"Number of classes under 10 occurences : {(train['landmark_id'].value_counts() <= 10).sum()}/{len(train['landmark_id'].unique())}\""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "from IPython.display import Image",
                "from IPython.core.display import HTML ",
                "",
                "def display_category(urls, category_name):",
                "    img_style = \"width: 180px; margin: 0px; float: left; border: 1px solid black;\"",
                "    images_list = ''.join([f\"<img style='{img_style}' src='{u}' />\" for _, u in urls.head(12).iteritems()])",
                "",
                "    display(HTML(images_list))",
                "",
                "category = train['landmark_id'].value_counts().keys()[0]",
                "urls = train[train['landmark_id'] == category]['url']",
                "display_category(urls, \"\")",
                "    ",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "category = train['landmark_id'].value_counts().keys()[1]",
                "urls = train[train['landmark_id'] == category]['url']",
                "display_category(urls, \"\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "category = train['landmark_id'].value_counts().keys()[2]",
                "urls = train[train['landmark_id'] == category]['url']",
                "display_category(urls, \"\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "H = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n",
                "H_T = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "H.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[H.duplicated()]\n",
                "#No duplicates"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[H.loc[:,~H.columns.isin(['SalePrice'])].duplicated()]\n",
                "#No duplicates"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Unique_data = H.apply(pd.Series.nunique)\n",
                "Unique_data[Unique_data == 1]\n",
                "#No single unique values in the dataset"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Sum = H.isnull().sum().sort_values(ascending = False)\n",
                "Percent = ((H.isnull().sum()*100)/H.count()[0]).sort_values(ascending = False)\n",
                "NullValues = pd.concat([Sum, Percent], axis = 1, keys = [\"Sum\", \"Percent\"])\n",
                "NullValues[NullValues.Sum > 0]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Deleting columns that have more than 20% missing values and Id column\n",
                "H.drop(['Id','Alley', 'PoolQC', 'Fence','MiscFeature','MiscVal','FireplaceQu','LotFrontage'], axis = 1, inplace = True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Sum = H.isnull().sum().sort_values(ascending = False)\n",
                "Percent = ((H.isnull().sum()*100)/H.count()[0]).sort_values(ascending = False)\n",
                "NullValues = pd.concat([Sum, Percent], axis = 1, keys = [\"Sum\", \"Percent\"])\n",
                "NullValues[NullValues.Sum > 0]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H[H[\"GarageArea\"] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]\n",
                "\n",
                "# Missing values exist as there is no garage for these homes"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.fillna({'GarageType': 'NoGarage', 'GarageYrBlt': 0, 'GarageFinish': 'NoGarage', 'GarageQual': 'NoGarage','GarageCond': 'NoGarage'} , inplace = True)\n",
                "\n",
                "#Filling appropriate values for nulls "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[H[\"GarageArea\"] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "H[H['TotalBsmtSF'] == 0][['BsmtQual', 'BsmtCond','BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].isnull().sum()\n",
                "\n",
                "# Missing values exist as there is no basement for these homes"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[H['TotalBsmtSF'] > 0][['BsmtQual', 'BsmtCond','BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].isnull().sum()\n",
                "\n",
                "# BsmtExposure and BsmtFinType2 have missing values though these homes have a basement"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "mode = H['BsmtExposure'].mode()[0]\n",
                "H.loc[(H['TotalBsmtSF'] > 0) & (H['BsmtExposure'].isnull()), 'BsmtExposure'] = mode\n",
                "\n",
                "mode = H['BsmtFinType2'].mode()[0]\n",
                "H.loc[(H['TotalBsmtSF'] > 0) & (H['BsmtFinType2'].isnull()), 'BsmtFinType2'] = mode\n",
                "\n",
                "#Filling these nulls with mode"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.fillna({'BsmtQual': 'NoBasement', 'BsmtCond': 'NoBasement','BsmtExposure': 'NoBasement', 'BsmtFinType1': 'NoBasement', 'BsmtFinType2': 'NoBasement'} , inplace = True)\n",
                "\n",
                "\n",
                "#Filling appropriate values for other nulls "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "mode = H['Electrical'].mode()[0]\n",
                "H['Electrical'].fillna(mode, inplace = True)\n",
                "mode = H['MasVnrType'].mode()[0]\n",
                "H['MasVnrType'].fillna(mode, inplace = True)\n",
                "median = H['MasVnrArea'].median()\n",
                "H['MasVnrArea'].fillna(median, inplace = True)\n",
                "\n",
                "# Filling missing values in MasVnrArea,MasVnrType,Electrical with mode"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Sum = H.isnull().sum().sort_values(ascending = False)\n",
                "Percent = ((H.isnull().sum()*100)/H.count()[0]).sort_values(ascending = False)\n",
                "NullValues = pd.concat([Sum, Percent], axis = 1, keys = [\"Sum\", \"Percent\"])\n",
                "NullValues[NullValues.Sum > 0]\n",
                "\n",
                "#No null values"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H[\"AgeOfHouse\"] = 2011 - H[\"YearBuilt\"]\n",
                "H[\"AgeOfRemod\"] = 2011 - H[\"YearRemodAdd\"]\n",
                "H['AgeOfSell'] = 2011 - H['YrSold']\n",
                "H['AgeOfGarage'] = 2011 - H['GarageYrBlt']\n",
                "H.loc[H['AgeOfGarage'] > 100 , 'AgeOfGarage'] = 0\n",
                "H.drop(['YearBuilt','YearRemodAdd','YrSold','GarageYrBlt'], axis = 1, inplace = True)\n",
                "\n",
                "#Using age instead of year for better intution and ease"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H['BsmtBath'] = H['BsmtFullBath'] + (0.5 * H['BsmtHalfBath'])\n",
                "H['Bath'] = H['FullBath'] + (0.5 * H['HalfBath'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "H['TotalPorchArea'] = H['OpenPorchSF'] + H['EnclosedPorch'] + H['3SsnPorch'] + H['ScreenPorch']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "numerical_columns = ['SalePrice','LotArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','GarageArea','WoodDeckSF','TotalPorchArea','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MasVnrArea','AgeOfGarage', 'AgeOfHouse', 'AgeOfRemod','AgeOfSell']\n",
                "categorical_columns = ['BsmtBath','Bath','BedroomAbvGr','BldgType','BsmtHalfBath','BsmtFullBath','Condition1','Condition2','Electrical','Exterior1st','Exterior2nd','Fireplaces','Foundation','FullBath','Functional','GarageCars','GarageFinish','GarageType','HalfBath','Heating','HouseStyle','KitchenAbvGr','LandContour','LandSlope','LotConfig','LotShape','MSSubClass','MSZoning','MasVnrType','MoSold','Neighborhood','PavedDrive','RoofMatl','RoofStyle','SaleCondition','SaleType','Street','TotRmsAbvGrd','Utilities']\n",
                "ordinal_columns = [ \"OverallQual\",\"OverallCond\",\"ExterQual\",\"ExterCond\",\"BsmtQual\",'BsmtCond',\"BsmtExposure\",\"HeatingQC\",\"KitchenQual\",\"GarageQual\",\"GarageCond\", 'BsmtFinType1', 'BsmtFinType2','CentralAir']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[ordinal_columns]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H['ExterQual'] = H['ExterQual'].map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['ExterCond'] = H['ExterCond'].map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['BsmtQual'] = H['BsmtQual'].map({'NoBasement' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['BsmtCond'] = H['BsmtCond'].map({'NoBasement' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['BsmtExposure'] = H['BsmtExposure'].map({'Gd' : 4, 'Av' : 3, 'Mn' : 2, 'No' : 1, 'NoBasement' : 0})\n",
                "H['HeatingQC'] = H['HeatingQC'].map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['KitchenQual'] = H['KitchenQual'].map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['GarageQual'] = H['GarageQual'].map({'NoGarage' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['GarageCond'] = H['GarageCond'].map({'NoGarage' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['BsmtFinType1'] = H['BsmtFinType1'].map({'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'NoBasement' : 0})\n",
                "H['BsmtFinType2'] = H['BsmtFinType2'].map({'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'NoBasement' : 0})\n",
                "H['CentralAir'] = H['CentralAir'].map({'N' : 0, 'Y' : 1})"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[ordinal_columns].head(5)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[ordinal_columns].isnull().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[categorical_columns].dtypes"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for i in range(0, len(categorical_columns)):\n",
                "    if (H[categorical_columns[i]].dtype == 'int64') | (H[categorical_columns[i]].dtype == 'float64'):\n",
                "        H[categorical_columns[i]] = H[categorical_columns[i]].apply(str)\n",
                "        \n",
                "#Changing data type to string/object"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[categorical_columns].head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"BsmtBath\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"BsmtBath\"] == '3.0', 'BsmtBath'] = '2.0'\n",
                "\n",
                "#Merging 3 baths to 2 as there is only 1 record"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"BedroomAbvGr\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"BedroomAbvGr\"] == '8', 'BedroomAbvGr'] = '6'\n",
                "#Merging 8 to closer one 6"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"BsmtFullBath\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"BsmtFullBath\"] == '3', 'BsmtFullBath'] = '2'\n",
                "#Merging 3 to closer one 2"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Condition1\"].value_counts()\n",
                "# Not merging as they seem to have an importance"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Condition2\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"Condition2\"].isin(['PosA','RRAn','RRAe']), 'Condition2'] = 'PosA_RRAn_RRAe'\n",
                "#Merging 'PosA','RRAn','RRAe' to one field"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Electrical\"].value_counts()\n",
                "# Not merging as they seem to have some importance"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Exterior1st\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"Exterior1st\"].isin(['Stone','BrkComm','CBlock','AsphShn','ImStucc']), 'Exterior1st'] = 'Other'\n",
                "#Renaming 'Stone','BrkComm','CBlock','AsphShn','ImStucc' to other"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Exterior2nd\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"Exterior2nd\"].isin(['Stone','Brk Cmn','CBlock','AsphShn','ImStucc','Other']), 'Exterior2nd'] = 'Other'\n",
                "# Renaming 'Stone','BrkComm','CBlock','AsphShn','ImStucc' to Other"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Utilities\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.drop(['Utilities'], axis = 1, inplace = True)\n",
                "categorical_columns.remove('Utilities')\n",
                "\n",
                "#Droping Utilities as it has 99% data as 1 unique value"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Heating\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"Heating\"] == 'Floor', 'Heating'] = 'OthW'\n",
                "#Merging 'Floor' to 'OthW'"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"RoofMatl\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"RoofMatl\"].isin(['Roll','Membran','Metal','ClyTile']), 'RoofMatl'] = 'Other'\n",
                "#Clubbing 'Roll','Membran','Metal','ClyTile' to other"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"TotRmsAbvGrd\"].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"TotRmsAbvGrd\"] == '2', 'TotRmsAbvGrd'] = '3'\n",
                "H.loc[H[\"TotRmsAbvGrd\"] == '14', 'TotRmsAbvGrd'] = '12'\n",
                "#Merging outliers to closer values"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "numerical_columns_1 = ['SalePrice','LotArea','BsmtFinSF1','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','GrLivArea','GarageArea','WoodDeckSF','TotalPorchArea','MasVnrArea','AgeOfGarage', 'AgeOfHouse', 'AgeOfRemod','AgeOfSell']\n",
                "plt.figure(figsize=(15,60))\n",
                "for i in range(0, len(numerical_columns_1)):\n",
                "    plt.subplot(12,2,(i+1))\n",
                "    sns.distplot(H[numerical_columns_1[i]])"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from scipy import stats\n",
                "\n",
                "# correcting target variable\n",
                "H['SalePrice'], fitted_lambda = stats.boxcox(H['SalePrice'])\n",
                "\n",
                "#Correcting some normally distributed but skewed numerical data\n",
                "H['LotArea'], fitted_lambda = stats.boxcox(H['LotArea'])\n",
                "H['1stFlrSF'], fitted_lambda = stats.boxcox(H['1stFlrSF'])\n",
                "H['GrLivArea'], fitted_lambda = stats.boxcox(H['GrLivArea'])\n",
                "\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H['TotalBsmtSF'].describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.distplot(H['1stFlrSF'])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.distplot(H['GrLivArea'])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,15))\n",
                "correlation = H[numerical_columns].corr()\n",
                "sns.heatmap(correlation, annot = True)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,60))\n",
                "for i in range(0, len(categorical_columns)):\n",
                "    plt.subplot(20,2,(i+1))\n",
                "    sns.boxplot(data = H, x = categorical_columns[i], y = 'SalePrice'  )\n",
                "\n",
                "#Plotting all categorical with box plot to ponder and check for any obvious issues with data    "
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,60))\n",
                "for i in range(0, len(numerical_columns)):\n",
                "    plt.subplot(12,2,(i+1))\n",
                "    sns.scatterplot(data = H, x = numerical_columns[i], y = 'SalePrice'  )\n",
                "    \n",
                "#Ploting all numerical data in scatter plots to ponder"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,35))\n",
                "for i in range(0, len(ordinal_columns)):\n",
                "    plt.subplot(7,2,(i+1))\n",
                "    sns.barplot(data = H, x = ordinal_columns[i], y = 'SalePrice'  )\n",
                "    \n",
                "#Ploting all ordinal values with Sala price in a bar graph \n",
                "# Sale price doesnt seem to change much with any ordinal data"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "from sklearn import linear_model, metrics\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.linear_model import Lasso\n",
                "from sklearn.linear_model import ElasticNet\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.feature_selection import RFE\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "y = H['SalePrice']\n",
                "\n",
                "X = H.drop(['SalePrice'], axis = 1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "House_Dummies = pd.get_dummies(H[categorical_columns], drop_first = True)\n",
                "House_Dummies.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(categorical_columns)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X = X.drop(categorical_columns, axis = 1)\n",
                "X = pd.concat([X, House_Dummies], axis=1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.7, test_size = 0.3, random_state = 100)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "scaler = StandardScaler()\n",
                "numerical_columns.remove('SalePrice')\n",
                "X_train[numerical_columns+ordinal_columns] = scaler.fit_transform(X_train[numerical_columns+ordinal_columns])\n",
                "X_test[numerical_columns+ordinal_columns] = scaler.transform(X_test[numerical_columns+ordinal_columns])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_train.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_test.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "params = {'alpha': [0.00001,0.00005,0.0001, 0.0005,0.001,0.01, 0.02]}\n",
                "#arams = {'alpha': [0.1, 1,10,100,200,300,500,1000]}\n",
                "\n",
                "\n",
                "lasso = Lasso()\n",
                "\n",
                "\n",
                "folds = 5\n",
                "#Taking 5 folds for Cross validation\n",
                "\n",
                "model_cv = GridSearchCV(estimator = lasso, \n",
                "                        param_grid = params, \n",
                "                        scoring= 'neg_mean_absolute_error', \n",
                "                        cv = folds, \n",
                "                        return_train_score=True,\n",
                "                        verbose = 1)            \n",
                "\n",
                "model_cv.fit(X_train, y_train)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "content": [
                "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
                "cv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n",
                "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
                "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
                "plt.xlabel('alpha')\n",
                "plt.ylabel('Negative Mean Absolute Error')\n",
                "\n",
                "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
                "plt.legend(['train score', 'test score'], loc='upper left')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "alpha =0.001\n",
                "lasso = Lasso(alpha=alpha)  \n",
                "lasso.fit(X_train, y_train) "
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Lasso_coef = pd.DataFrame({\"Feature\":X_train.columns.tolist(),\"Coefficients\":lasso.coef_})\n",
                "Lasso_coef[Lasso_coef['Coefficients'] != 0 ].sort_values(by = \"Coefficients\" , ascending = False).count()\n",
                "\n",
                "#Feature selection is done by Lasso and features narrowed down to 46"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "y_test_lasso_predict = lasso.predict(X_test)\n",
                "y_train_lasso_predict = lasso.predict(X_train)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(metrics.r2_score(y_true=y_test, y_pred=y_test_lasso_predict))\n",
                "print(metrics.r2_score(y_true=y_train, y_pred=y_train_lasso_predict))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "\n",
                "params = {'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 1.5, 2, 10, 100, 1000]}\n",
                "\n",
                "\n",
                "ridge = Ridge()\n",
                "\n",
                "# cross validation with 5 folds\n",
                "folds = 5\n",
                "model_cv = GridSearchCV(estimator = ridge, \n",
                "                        param_grid = params, \n",
                "                        scoring= 'neg_mean_absolute_error', \n",
                "                        cv = folds, \n",
                "                        return_train_score=True,\n",
                "                        verbose = 1)            \n",
                "model_cv.fit(X_train, y_train) "
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "content": [
                "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
                "cv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n",
                "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
                "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
                "plt.xlabel('alpha')\n",
                "plt.ylabel('Negative Mean Absolute Error')\n",
                "\n",
                "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
                "plt.legend(['train score', 'test score'], loc='upper left')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "alpha =100\n",
                "ridge = Ridge(alpha=alpha)  \n",
                "ridge.fit(X_train, y_train) "
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "y_test_ridge_predict = ridge.predict(X_test)\n",
                "y_train_ridge_predict = ridge.predict(X_train)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(metrics.r2_score(y_true=y_test, y_pred=y_test_ridge_predict))\n",
                "print(metrics.r2_score(y_true=y_train, y_pred=y_train_ridge_predict))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Ridge_coef = pd.DataFrame({\"Feature\":X_train.columns.tolist(),\"Coefficients\":ridge.coef_})\n",
                "Ridge_coef[Ridge_coef['Coefficients'] != 0 ].sort_values(by = \"Coefficients\" , ascending = False).count()\n",
                "#No feature selection happened"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "params = {'alpha': [0,0.0001, 0.0005, 0.001, 0.01]}\n",
                "\n",
                "elasticnet = ElasticNet()\n",
                "\n",
                "# cross validation\n",
                "model_cv = GridSearchCV(estimator = elasticnet, \n",
                "                        param_grid = params, \n",
                "                        scoring= 'neg_mean_absolute_error', \n",
                "                        cv = folds, \n",
                "                        return_train_score=True,\n",
                "                        verbose = 1)            \n",
                "\n",
                "model_cv.fit(X_train, y_train) "
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "content": [
                "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
                "cv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n",
                "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
                "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
                "plt.xlabel('alpha')\n",
                "plt.ylabel('Negative Mean Absolute Error')\n",
                "\n",
                "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
                "plt.legend(['train score', 'test score'], loc='upper left')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "alpha =0.001\n",
                "elasticnet = ElasticNet(alpha=alpha)  \n",
                "elasticnet.fit(X_train, y_train) "
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "y_test_elasticnet_predict = elasticnet.predict(X_test)\n",
                "y_train_elasticnet_predict = elasticnet.predict(X_train)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(metrics.r2_score(y_true=y_test, y_pred=y_test_elasticnet_predict))\n",
                "print(metrics.r2_score(y_true=y_train, y_pred=y_train_elasticnet_predict))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "elasticnet_coef = pd.DataFrame({\"Feature\":X_train.columns.tolist(),\"Coefficients\":elasticnet.coef_})\n",
                "elasticnet_coef[elasticnet_coef['Coefficients'] != 0 ].sort_values(by = \"Coefficients\" , ascending = False).count()\n",
                "#Feature selection happened but not as good as Lasso"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Lasso_coef = Lasso_coef[Lasso_coef['Coefficients'] != 0 ].sort_values(by = \"Coefficients\" , ascending = False).reset_index()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Lasso_coef.drop(['index'], axis = 1, inplace = True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Lasso_coef.head(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Lasso_coef.tail(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Lasso_coef['Feature'].to_list()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "\n",
                "plt.figure(figsize=(15,15))\n",
                "sns.barplot(x=\"Coefficients\", y=\"Feature\", data=Lasso_coef, palette=\"vlag\")\n",
                "plt.xlabel(\"Feature Importance\")\n",
                "plt.tight_layout()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "",
                "# Input data files are available in the \"../input/\" directory.",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory",
                "",
                "import os",
                "print(os.listdir(\"../input\"))",
                "print(os.listdir(\"../input/A_Z Handwritten Data\"))",
                "",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "data_train_file= \"../input/A_Z Handwritten Data/A_Z Handwritten Data.csv\"",
                "data_test_file= \"../input/A_Z Handwritten Data/A_Z Handwritten Data.csv\"",
                "",
                "df_train=pd.read_csv(data_train_file)",
                "df_test= pd.read_csv(data_test_file)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train.describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "import matplotlib.pyplot as plt ",
                "%matplotlib inline",
                "for i in range(5000,5005):",
                "    sample =np.reshape(df_test[df_test.columns[1:]].iloc[i].values/255,(28,28))",
                "    plt.figure()",
                "    plt.title(\"labelled cs {}\".format(df_test[\"0\"].iloc[i]))",
                "    plt.imshow(sample,'gray')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "import matplotlib.pyplot as plt",
                "import seaborn as sns",
                "from scipy.stats import norm",
                "from scipy.stats import skew",
                "from scipy import stats",
                "from sklearn.metrics import mean_squared_error, make_scorer",
                "%matplotlib inline",
                "# Input data files are available in the \"../input/\" directory.",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory",
                "",
                "import os",
                "print(os.listdir(\"../input\"))",
                "",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "train_df = pd.read_csv('../input/train.csv', header=0, sep=',')",
                "train_df = train_df.loc[:,'MSSubClass':]",
                "#print('Training data\\n', train_df.head())",
                "print('Training data columns:', train_df.columns)",
                "print('Training data shape', train_df.shape)",
                "",
                "test_df = pd.read_csv('../input/test.csv', header=0, sep=',')",
                "test_df = test_df.loc[:,'MSSubClass':]",
                "#print('Test data\\n', test_df.head())",
                "print('Training data columns:', test_df.columns)",
                "print('Test data shape\\n', test_df.shape)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Missing data",
                "print('Missing data points in every features')",
                "total = train_df.isnull().sum().sort_values(ascending=False)",
                "percent = (train_df.isnull().sum()/train_df.isnull().count()).sort_values(ascending=False)",
                "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])",
                "print(missing_data[:20])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Findout how many of the columns are caregorical or numerical",
                "nan_columns = missing_data[missing_data['Percent'] > 0.0].index",
                "filr_train_df = train_df[missing_data[missing_data['Percent'] <= 0.0].index]",
                "numr_cols = filr_train_df.dtypes[filr_train_df.dtypes != \"object\"].index # Numerical columns",
                "catg_cols = filr_train_df.dtypes[filr_train_df.dtypes == \"object\"].index # Categorical columns",
                "print(catg_cols)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#dealing with missing data",
                "for ind, col in filr_train_df[catg_cols].iteritems(): # What kind of data in categorical columns",
                "    print(ind, set(list(col)))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Corrilation of numerical features with sales prize",
                "print('Number of features:', len(filr_train_df.columns))",
                "filr_train_df_corr = filr_train_df.corr()",
                "highly_corr = filr_train_df_corr[(filr_train_df_corr['SalePrice'] > 0.1)].index # No negative correlation in data",
                "corr_sale = filr_train_df.loc[:,highly_corr].corr()",
                "print('Number of features with corr:', len(corr_sale))",
                "plt.figure(figsize=(10,10))",
                "sns.heatmap(corr_sale, annot=True, square=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Now check for skewnwss of numerical features",
                "skewed_feats = filr_train_df[highly_corr].apply(lambda x: skew(x.dropna())) #compute skewness",
                "print('Skewness in feature data\\n',skewed_feats)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Correct skewness for features with positive skewness > 0.5 with log1p transformation",
                "LT_columns = []",
                "numLT_train_df = pd.DataFrame()",
                "for ind, skew in skewed_feats.iteritems():",
                "    if (skew > 0.5):",
                "        numLT_train_df = pd.concat([numLT_train_df, np.log1p(filr_train_df[ind])], axis=1)",
                "        LT_columns.append(ind)",
                "    else:",
                "        numLT_train_df = pd.concat([numLT_train_df, filr_train_df[ind]], axis=1)",
                "        ",
                "# Example of skewness",
                "print('Example feature:', LT_columns[0])",
                "skew_ex = pd.DataFrame({'Not_trsf': filr_train_df[LT_columns[0]], 'log_trsf':numLT_train_df[LT_columns[0]]})",
                "skew_ex.hist()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "content": [
                "# Can also look at normal prob plot",
                "#histogram and normal probability plot",
                "sns.set()",
                "sns.distplot(numLT_train_df[LT_columns[0]], fit=norm);",
                "fig = plt.figure()",
                "res = stats.probplot(numLT_train_df[LT_columns[0]], plot=plt)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "print(numLT_train_df.shape)",
                "sns.pairplot(numLT_train_df.iloc[:,:5])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# What features we want to take",
                "catg_cols # these are our categorical features",
                "highly_corr # numeric features which we have selected based on correlation",
                "LT_columns # numeric features which needs to be transformed"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Concat test and training for preprocessing",
                "tot_data = pd.concat([train_df.loc[:,'MSSubClass':], test_df.loc[:,'MSSubClass':]], ignore_index=True)",
                "print(tot_data.shape)",
                "print(tot_data.head())"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Create dummy variable for categorical features",
                "tot_data_cat = pd.get_dummies(tot_data[catg_cols])",
                "tot_data_cat.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Log transform numerical data",
                "tot_data_num = tot_data[highly_corr]",
                "print(tot_data_num.shape)",
                "for cols in LT_columns:",
                "    tot_data_num.loc[:,cols] = np.log1p(tot_data_num.loc[:,cols])",
                "print(tot_data_num.shape)",
                "tot_data_num.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Combining preprocessed data",
                "tot_data_pro = pd.concat([tot_data_cat, tot_data_num],axis=1)",
                "tot_data_pro.head()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Creating matrix for sklern ",
                "pr_trainData = tot_data_pro[:1000]",
                "pr_testData = tot_data_pro[1000:1400]",
                "pr_testData = pr_testData.fillna(pr_testData.mean())",
                "print(pr_trainData.shape, pr_testData.shape)",
                "Y = pr_trainData['SalePrice']",
                "X_trainData = pr_trainData.drop('SalePrice', axis=1)",
                "X_testData = pr_testData.drop('SalePrice', axis=1)",
                "print(X_trainData.shape, X_testData.shape)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV",
                "from sklearn.model_selection import cross_val_score, train_test_split",
                "",
                "# Define error measure for official scoring : RMSE",
                "scorer = make_scorer(mean_squared_error, greater_is_better = False)",
                "",
                "def rmse_cv_train(model):",
                "    rmse= np.sqrt(-cross_val_score(model, X_trainData, Y, scoring = scorer, cv = 5))",
                "    return(rmse)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])",
                "ridge.fit(X_trainData, Y)",
                "alpha = ridge.alpha_",
                "print(\"Best alpha :\", alpha)",
                "",
                "print(\"Try again for more precision with alphas centered around \" + str(alpha))",
                "ridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, ",
                "                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,",
                "                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], ",
                "                cv = 5)",
                "ridge.fit(X_trainData, Y)",
                "alpha = ridge.alpha_",
                "print(\"Best alpha :\", alpha)",
                "",
                "print(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())",
                "y_train_rdg = ridge.predict(X_trainData)",
                "y_test_rdg = ridge.predict(X_testData)",
                "# Plot residuals",
                "sns.set()",
                "plt.scatter(y_train_rdg, y_train_rdg - Y, c = \"blue\", marker = \"o\", label = \"Training data\", alpha=0.7)",
                "plt.scatter(y_test_rdg, y_test_rdg - pr_testData['SalePrice'], c = \"green\", marker = \"o\", label = \"Validation data\", alpha=0.7)",
                "plt.title(\"Linear regression with Ridge regularization\")",
                "plt.xlabel(\"Predicted values\")",
                "plt.ylabel(\"Residuals\")",
                "plt.legend(loc = \"upper left\")",
                "plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")",
                "plt.show()",
                "",
                "# Plot predictions",
                "plt.scatter(y_train_rdg, Y, c = \"blue\", marker = \"o\", label = \"Training data\", alpha=0.7)",
                "plt.scatter(y_test_rdg, pr_testData['SalePrice'], c = \"green\", marker = \"o\", label = \"Validation data\", alpha=0.7)",
                "plt.title(\"Linear regression with Ridge regularization\")",
                "plt.xlabel(\"Predicted values\")",
                "plt.ylabel(\"Real values\")",
                "plt.legend(loc = \"upper left\")",
                "plt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")",
                "plt.show()",
                "",
                "# Plot important coefficients",
                "coefs = pd.Series(ridge.coef_, index = X_trainData.columns)",
                "print(\"Ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\",
                "      str(sum(coefs == 0)) + \" features\")",
                "imp_coefs = pd.concat([coefs.sort_values().head(10),",
                "                     coefs.sort_values().tail(10)])",
                "imp_coefs.plot(kind = \"barh\")",
                "plt.title(\"Coefficients in the Ridge Model\")",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1], ",
                "                max_iter = 50000, cv = 10)",
                "lasso.fit(X_trainData, Y)",
                "alpha = lasso.alpha_",
                "print(\"Best alpha :\", alpha)",
                "",
                "print(\"Try again for more precision with alphas centered around \" + str(alpha))",
                "lasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, ",
                "                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, ",
                "                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, ",
                "                          alpha * 1.4], ",
                "                max_iter = 50000, cv = 10)",
                "lasso.fit(X_trainData, Y)",
                "alpha = lasso.alpha_",
                "print(\"Best alpha :\", alpha)",
                "",
                "print(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())",
                "y_train_las = lasso.predict(X_trainData)",
                "y_test_las = lasso.predict(X_testData)",
                "",
                "# Plot residuals",
                "plt.scatter(y_train_las, y_train_las - Y, c = \"blue\", marker = \"s\", label = \"Training data\", alpha=0.7)",
                "plt.scatter(y_test_las, y_test_las - pr_testData['SalePrice'], c = \"green\", marker = \"s\", label = \"Validation data\", alpha=0.7)",
                "plt.title(\"Linear regression with Lasso regularization\")",
                "plt.xlabel(\"Predicted values\")",
                "plt.ylabel(\"Residuals\")",
                "plt.legend(loc = \"upper left\")",
                "plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")",
                "plt.show()",
                "",
                "# Plot predictions",
                "plt.scatter(y_train_las, Y, c = \"blue\", marker = \"s\", label = \"Training data\", alpha=0.7)",
                "plt.scatter(y_test_las, pr_testData['SalePrice'], c = \"green\", marker = \"s\", label = \"Validation data\", alpha=0.7)",
                "plt.title(\"Linear regression with Lasso regularization\")",
                "plt.xlabel(\"Predicted values\")",
                "plt.ylabel(\"Real values\")",
                "plt.legend(loc = \"upper left\")",
                "plt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")",
                "plt.show()",
                "",
                "# Plot important coefficients",
                "coefs = pd.Series(lasso.coef_, index = X_trainData.columns)",
                "print(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\",
                "      str(sum(coefs == 0)) + \" features\")",
                "imp_coefs = pd.concat([coefs.sort_values().head(10),",
                "                     coefs.sort_values().tail(10)])",
                "imp_coefs.plot(kind = \"barh\")",
                "plt.title(\"Coefficients in the Lasso Model\")",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "from __future__ import print_function\n",
                "import keras\n",
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Dense, Dropout, Activation, Flatten\n",
                "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, MaxPool2D, AveragePooling2D\n",
                "import os\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "import seaborn as sns\n",
                "import matplotlib\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "import itertools\n",
                "\n",
                "%matplotlib inline\n",
                "# TensorFlow and tf.keras\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout,Flatten\n",
                "from tensorflow.keras.optimizers import Adam, SGD\n",
                "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
                "from tensorflow.keras.callbacks import TensorBoard,EarlyStopping\n",
                "%load_ext tensorboard\n",
                "\n",
                "# Helper libraries\n",
                "import numpy as np\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "import datetime\n",
                "\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout\n",
                "from keras.regularizers import l2\n",
                "from keras.regularizers import l1\n",
                "from tensorflow.keras import regularizers\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.callbacks import TensorBoard,EarlyStopping\n",
                "\n",
                "kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
                "\n",
                "print(tf.__version__)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "import numpy as np\n",
                "from keras import backend as K\n",
                "from keras.models import Sequential\n",
                "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
                "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "\n",
                "#Start\n",
                "train_data_path = '../input/animal1209/animal_dataset_intermediate_new/train_split/train'\n",
                "test_data_path = '../input/animal1209/animal_dataset_intermediate_new/train_split/val'\n",
                "img_rows = 150\n",
                "img_cols = 150\n",
                "epochs = 100\n",
                "batch_size = 32\n",
                "num_of_train_samples = 5736\n",
                "num_of_test_samples = 2460\n",
                "\n",
                "#Image Generator\n",
                "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
                "                                   rotation_range=40,\n",
                "                                   width_shift_range=0.2,\n",
                "                                   height_shift_range=0.2,\n",
                "                                   shear_range=0.2,\n",
                "                                   zoom_range=0.2,\n",
                "                                   horizontal_flip=True,\n",
                "                                   fill_mode='nearest')\n",
                "\n",
                "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
                "\n",
                "train_generator = train_datagen.flow_from_directory(train_data_path,\n",
                "                                                    target_size=(img_rows, img_cols),\n",
                "                                                    batch_size=batch_size,\n",
                "                                                    class_mode='categorical')\n",
                "\n",
                "validation_generator = test_datagen.flow_from_directory(test_data_path,\n",
                "                                                        target_size=(img_rows, img_cols),\n",
                "                                                        batch_size=batch_size,\n",
                "                                                        class_mode='categorical')\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "ingest_data"
            ],
            "content": [
                "\n",
                "train_generator_2 = image_dataset_from_directory(\n",
                "    directory=r\"../input/animal1209/animal_dataset_intermediate_new/train_split/train\",\n",
                "    labels = \"inferred\", label_mode = 'int',\n",
                "    validation_split = 0.2,\n",
                "    subset = \"training\",\n",
                "    seed = 1337,\n",
                "    image_size=(224, 224),\n",
                "    batch_size=32\n",
                ")\n",
                "\n",
                "#visualizing the data\n",
                "import matplotlib.pyplot as plt\n",
                "plt.figure(figsize=(10, 10))\n",
                "for images, labels in train_generator_2.take(1):\n",
                "    for i in range(9):\n",
                "        ax = plt.subplot(3, 3, i + 1)\n",
                "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
                "        plt.title(int(labels[i]))\n",
                "        plt.axis(\"off\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "#Define optimizer \n",
                "\n",
                "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
                "steps_per_epoch=STEP_SIZE_TRAIN \n",
                "\n",
                "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
                "  0.005, decay_steps=steps_per_epoch*1000,decay_rate=1,staircase=False)\n",
                "\n",
                "optimizer_2 = SGD(lr_schedule)\n",
                "optimizer= SGD(lr = 0.01)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "# Define  Callbacks \n",
                "\n",
                "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
                "                              patience=5, min_lr=0.001)\n",
                "callbacks=[reduce_lr]\n",
                "\n",
                "earlystopping_callback = EarlyStopping(\n",
                "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto',\n",
                "    baseline=None, restore_best_weights=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "#MLP model 62%/42%\n",
                "\n",
                "model3 = Sequential()\n",
                "#input layer size is 784 after flattening\n",
                "model3.add(Flatten(input_shape=(224, 224, 3)))\n",
                "  \n",
                "#hidden layer with 512 neurons\n",
                "model3.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l1(0.01)))\n",
                "model3.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l1(0.01)))\n",
                "model3.add(Dropout(0.5))\n",
                "model3.add(BatchNormalization())\n",
                "\n",
                "model3.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l1(0.01)))\n",
                "model3.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l1(0.01)))\n",
                "model3.add(Dropout(0.5))\n",
                "model3.add(BatchNormalization())\n",
                "model3.add(Dense(5, activation='softmax'))\n",
                "\n",
                "model3.summary()\n",
                "\n",
                "\n",
                "# compile model\n",
                "model3.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics=['accuracy'])"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "# CNN model\n",
                "\n",
                "model = Sequential()\n",
                "model.add(Convolution2D(32, (3, 3), input_shape=(img_rows, img_cols, 3), padding='valid'))\n",
                "model.add(Activation('relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
                "\n",
                "model.add(Convolution2D(32, (3, 3), padding='valid'))\n",
                "model.add(Activation('relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
                "\n",
                "model.add(Convolution2D(64, (3, 3), padding='valid'))\n",
                "model.add(Activation('relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
                "\n",
                "model.add(Flatten())\n",
                "model.add(Dense(64))\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(0.5))\n",
                "model.add(Dense(5))\n",
                "model.add(Activation('softmax'))\n",
                "\n",
                "model.summary()\n",
                "\n",
                "model.compile(loss='categorical_crossentropy',\n",
                "              optimizer='rmsprop',\n",
                "              metrics=['accuracy'])"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "#Train\n",
                "model.fit_generator(train_generator,\n",
                "                    steps_per_epoch=num_of_train_samples // batch_size,\n",
                "                    epochs=epochs,\n",
                "                    validation_data=validation_generator,\n",
                "                    validation_steps=num_of_test_samples // batch_size)\n",
                "\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "model.save_weights('model.h5')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "\n",
                "# TensorFlow and tf.keras\n",
                "\n",
                "%load_ext tensorboard\n",
                "import datetime\n",
                "print(tf.__version__)\n",
                "\n",
                "# run the tensorboard command to view the visualizations.\n",
                "\n",
                "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
                "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
                "%tensorboard --logdir logs/fit"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "#Evaluate the model \n",
                "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
                "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
                "model.evaluate(validation_generator,\n",
                "steps=STEP_SIZE_VALID)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "\n",
                "Y_pred = model.predict_generator(validation_generator, num_of_test_samples)\n",
                "y_pred = np.argmax(Y_pred, axis=1)\n",
                "\n",
                "y_pred[200]\n",
                "\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y_pred.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "predictions = [labels[i] for i in y_pred]\n",
                "predictions"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "\n",
                "target_names = ['elefante_train', 'farfalla_train', 'mucca_train','pecora_train','scoiattolo_train']\n",
                "print(classification_report(validation_generator.labels, y_pred, target_names=target_names))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "#Extract the test data => I didnt find a way without creating a new folder on colab\n",
                "\n",
                "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.0/255.)\n",
                "\n",
                "test_generator = test_datagen.flow_from_directory(\"../input/animal-ori/animal_dataset_intermediate\",\n",
                "                                                    \n",
                "                                                    class_mode = 'categorical', \n",
                "                                                    target_size = (150, 150))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "#prediction on test data \n",
                "\n",
                "num = 9106\n",
                "\n",
                "Y_pred_test = model.predict_generator(test_generator)\n",
                "y_pred_test = np.argmax(Y_pred_test, axis=1)\n",
                "y_pred_test\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Y_pred_test"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "y_final = y_pred_test[0:910]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(*y_final, sep = \", \")  "
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "res = pd.DataFrame(y_final) #preditcions are nothing but the final predictions of your model on input features of your new unseen test data\n",
                " # its important for comparison. Here \"test_new\" is your new test dataset\n",
                "res.columns = [\"prediction\"]\n",
                "res.to_csv(\"prediction_results.csv\")      # the csv file will be saved locally on the same location where this notebook is located."
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "res"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "im = pd.read_csv('../input/animal-ori/animal_dataset_intermediate/Testing_set_animals.csv')\n",
                "im\n",
                "\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "rei = pd.concat([im,res],  axis = 1) "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "rei.drop('target', axis = 1)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "rei['animal'] = rei['prediction'] \n",
                "rei"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "rei['animal'] =rei['animal'].replace(to_replace=2,value = \"mucca\") "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "rei.head(100)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "rei['animal'] =rei['animal'].replace(to_replace=3,value = \"pecora\") "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "\n",
                "rei['animal'] =rei['animal'].replace(to_replace=4,value = \"scoiattolo\") "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "rei['animal'] =rei['animal'].replace(to_replace=0,value = \"elefante\") "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "rei"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "rei['animal'] =rei['animal'].replace(to_replace=1,value = \"farfalla\") "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "rei.head(200)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "rei.to_csv('final')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y_final.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# Set up feedback system\n",
                "from learntools.core import binder\n",
                "binder.bind(globals())\n",
                "from learntools.sql.ex3 import *\n",
                "print(\"Setup Complete\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "content": [
                "from google.cloud import bigquery\n",
                "\n",
                "# Create a \"Client\" object\n",
                "client = bigquery.Client()\n",
                "\n",
                "# Construct a reference to the \"hacker_news\" dataset\n",
                "dataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n",
                "\n",
                "# API request - fetch the dataset\n",
                "dataset = client.get_dataset(dataset_ref)\n",
                "\n",
                "# Construct a reference to the \"comments\" table\n",
                "table_ref = dataset_ref.table(\"comments\")\n",
                "\n",
                "# API request - fetch the table\n",
                "table = client.get_table(table_ref)\n",
                "\n",
                "# Preview the first five lines of the \"comments\" table\n",
                "client.list_rows(table, max_results=5).to_dataframe()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "from numpy import sort",
                "",
                "# Input data files are available in the \"../input/\" directory.",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory",
                "",
                "import os",
                "print(os.listdir(\"../input\"))",
                "",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import warnings",
                "warnings.filterwarnings('ignore')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "import multiprocessing",
                "",
                "n_jobs = multiprocessing.cpu_count()",
                "n_jobs"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "#prediction and Classification Report",
                "from sklearn.metrics import classification_report",
                "",
                "# select features using threshold",
                "from sklearn.feature_selection import SelectFromModel",
                "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score, cross_val_predict",
                "from sklearn.metrics import accuracy_score, recall_score, f1_score",
                "from sklearn.metrics.scorer import make_scorer",
                "",
                "# plot tree, importance",
                "from xgboost import plot_tree, plot_importance",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# load xgboost, test train split",
                "import xgboost as xgb",
                "from sklearn.model_selection import train_test_split, cross_validate",
                "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif",
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.metrics import confusion_matrix"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import pandas as pd",
                "import seaborn as sns",
                "import matplotlib.pyplot as plt",
                "",
                "%matplotlib inline"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "df = pd.read_csv('../input/train.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "num_of_cols = len(list(df.columns))",
                "num_of_cols"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "pd.options.display.max_columns = num_of_cols"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(df)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# columns with null values",
                "df_isna = pd.DataFrame(df.isnull().sum())",
                "df_isna.loc[(df_isna.loc[:, df_isna.dtypes != object] != 0).any(1)]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "nan_cols = list(df_isna.loc[(df_isna.loc[:, df_isna.dtypes != object] != 0).any(1)].T.columns)",
                "nan_cols"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[nan_cols].describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.describe(include='all')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[nan_cols].sample(3000).describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['parentesco1'].loc[df.parentesco1 == 1].describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "(df['parentesco1'].loc[df.parentesco1 == 1].describe()['count']/len(df))*100"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# find number of households",
                "",
                "df['idhogar'].describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "house_ids = list(df['idhogar'].unique())"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['parentesco1','idhogar']].loc[df.parentesco1 == 1].head(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "hid_heads = df.groupby(['idhogar'])['parentesco1'].apply(lambda x: pd.unique(x.values.ravel()).tolist()).reset_index()",
                "len(hid_heads)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_hid = pd.DataFrame(hid_heads, index=None, columns=['idhogar','parentesco1'])",
                "df_hid.sample(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_hid['parentesco1'] = df_hid['parentesco1'].apply(lambda x: ''.join(map(str, x)))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_hid.sample(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_hid.loc[df_hid.parentesco1 == '0']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# id's without head!",
                "hid_wo_heads = list(df_hid['idhogar'].loc[df_hid.parentesco1 == '0'])",
                "len(hid_wo_heads)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_hwoh = df[df['idhogar'].isin(hid_wo_heads)]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_hwoh[['idhogar', 'parentesco1','v2a1']]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['v2a1'].hist()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['v2a1'].loc[-df['idhogar'].isin(hid_wo_heads)].hist()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df_hwoh['v2a1'].hist()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(df_hwoh)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# these 15 households (23 rows) doesn't have a head..",
                "# we should exclude these from analysis and scoring perhaps...",
                "df_hwoh['idhogar'].unique()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '09b195e7a'])",
                "print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == 'f2bfa75c4'])",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# required dataframe - without households without a head!!",
                "print(\"before removal: \", len(df))",
                "df = df.loc[-df['idhogar'].isin(hid_wo_heads)]",
                "print(\"after removal: \", len(df))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['v2a1'].describe().plot()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df[['v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned']].describe().plot()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import gc",
                "",
                "gc.collect()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(df['v2a1'].unique())"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['v2a1'].unique()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['v2a1'].max()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 > 1000000]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 >= 1000000]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# remove these two rows...",
                "df[['v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '563cc81b7']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "print(\"before removal: \", len(df))",
                "df.drop(df[df.idhogar == '563cc81b7'].index, inplace=True)",
                "print(\"after removal: \", len(df))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['v2a1'].hist()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['v2a1'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['v18q1'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['rez_esc'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['meaneduc'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['SQBmeaned'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "cols = list(df.columns)",
                "cols"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.sample(10)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "set(df.dtypes)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "col_types = {}",
                "",
                "for col in cols:",
                "    col_types[col] = df[col].dtype",
                "    # print(col, df[col].dtype)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(len(col_types))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for key in sorted(col_types):",
                "    print(key, col_types[key])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "cat_cols = []",
                "num_cols = []",
                "for col in cols:",
                "    if df[col].dtype == 'O':",
                "        cat_cols.append(col)",
                "        print(col, df[col].dtype)",
                "    else:",
                "        num_cols.append(col)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# categorical columns",
                "cat_cols"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[cat_cols].sample(10)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(num_cols)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# numerical columns",
                "sorted(num_cols)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "g = sns.PairGrid(df[nan_cols])",
                "g = g.map_offdiag(plt.scatter)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "cols_electronics = ['refrig','mobilephone','television','qmobilephone','computer', 'v18q', 'v18q1', ]",
                "cols_house_details = ['v2a1', 'area1', 'area2', 'bedrooms','rooms', 'cielorazo', 'v14a', ",
                "                    'tamhog', 'hacdor', 'hacapo', 'r4t3', ]",
                "cols_person_details = ['age', 'agesq', 'female', 'male',]",
                "cols_SQ = ['SQBage', 'SQBdependency', 'SQBedjefe', 'SQBescolari', 'SQBhogar_nin', ",
                "           'SQBhogar_total', 'SQBmeaned', 'SQBovercrowding',]",
                "cols_water = ['abastaguadentro', 'abastaguafuera', 'abastaguano',]",
                "",
                "cols_h = [ 'hhsize', 'hogar_adul', 'hogar_mayor', 'hogar_nin', 'hogar_total',]",
                "cols_r = ['r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3',]",
                "cols_tip = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',]",
                "cols_roof = ['techocane', 'techoentrepiso', 'techootro', 'techozinc',]",
                "cols_floor = ['pisocemento', 'pisomadera', 'pisomoscer', 'pisonatur', 'pisonotiene', 'pisoother',]",
                "cols_sanitary = [ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6',]",
                "cols_parents = [ 'parentesco1', 'parentesco10', 'parentesco11', 'parentesco12', 'parentesco2', 'parentesco3',",
                "                'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9',]",
                "cols_outside_wall = [ 'paredblolad', 'pareddes', 'paredfibras', 'paredmad', 'paredother', ",
                "              'paredpreb', 'paredzinc', 'paredzocalo',]",
                "cols_instlevel = [ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6',",
                "                  'instlevel7', 'instlevel8', 'instlevel9',]",
                "cols_lugar = [ 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6',]",
                "cols_estadoc = [ 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', ",
                "                'estadocivil5', 'estadocivil6', 'estadocivil7',]",
                "cols_elim = ['elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6',]",
                "cols_energ = ['energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4',]",
                "cols_eviv = [ 'eviv1', 'eviv2', 'eviv3',]",
                "cols_etech = [ 'etecho1', 'etecho2', 'etecho3',]",
                "cols_pared = [ 'epared1', 'epared2', 'epared3',]",
                "cols_unknown = [ 'dis', 'escolari', 'meaneduc', ",
                "                'overcrowding', 'rez_esc', 'tamhog', 'tamviv', ]",
                "cols_elec = ['coopele', 'noelec', 'planpri', 'public',]",
                "",
                "total_features = cols_electronics+cols_house_details+cols_person_details+\\",
                "cols_SQ+cols_water+cols_h+cols_r+cols_tip+cols_roof+\\",
                "cols_floor+cols_sanitary+cols_parents+cols_outside_wall+\\",
                "cols_instlevel+cols_lugar+cols_estadoc+cols_elim+cols_energ+\\",
                "cols_eviv+cols_etech+cols_pared+cols_unknown+cols_elec",
                "",
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df[cols_electronics].plot.area()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['Target'].unique()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "cols_electronics_target = cols_electronics.append('Target')",
                "df[cols_electronics].corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "cols_electronics.remove('Target')",
                "cols_electronics"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.groupby('Target')[cols_electronics].sum()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['tamhog'].unique()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# high correlation between ",
                "# no. of persons in the household,",
                "# persons living in the household ",
                "# and size of the household",
                "# we can use any one...!!",
                "df[['tamhog','r4t3', 'tamviv']].corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['r4t3','tamviv']].corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "total_features.remove('r4t3')",
                "total_features.remove('tamhog')",
                "total_features.remove('tamviv')",
                "",
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['escolari'].unique()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['escolari'].hist()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['escolari'].describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['escolari'].plot.line()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df.escolari)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "correlations = df[num_cols].corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# correlation heatmap masking",
                "mask = np.zeros_like(correlations, dtype=np.bool)",
                "mask[np.triu_indices_from(mask)] = True",
                "",
                "f, ax = plt.subplots(figsize=(17, 13))",
                "cmap = sns.diverging_palette(220, 10, as_cmap=True)",
                "",
                "sns.heatmap(correlations, mask=mask, cmap=cmap, vmax=.3, center=0,",
                "square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# difficult to look into the above one",
                "es_corr = df[num_cols].corrwith(df.escolari, axis=0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for x,y in zip(num_cols, list(es_corr)):",
                "    if (y >= 0.75) or (y < -0.6):",
                "        print(x,y)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "sqbes_corr = df[num_cols].corrwith(df.SQBescolari, axis=0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for x,y in zip(num_cols, list(sqbes_corr)):",
                "    if (y >= 0.5) or (y < -0.6):",
                "        print(x,y)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "total_features.remove('escolari')",
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df.loc[df.Target == 1].groupby('overcrowding').SQBescolari.value_counts().unstack().plot.bar()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['overcrowding'].hist()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['overcrowding'].unique()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df.plot.scatter(x='Target', y='overcrowding')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df.groupby('Target').overcrowding.value_counts().unstack().plot.bar()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['Target'].describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['Target'].unique()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['Target'].hist()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "nan_cols"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# filling missing values",
                "",
                "df[nan_cols].corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for col in nan_cols:",
                "    if col != 'v2a1':",
                "        print(col, df[col].unique())"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# there's a clear quadratic relation between meaneduc and SQBmeaned",
                "# hence, we can ignore either one of these..say, meaneduc",
                "sns.regplot(df['meaneduc'],df['SQBmeaned'], order=2)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# filling na values in meaneduc and SQBmeaned",
                "df['meaneduc'].fillna(0, inplace=True)",
                "df['SQBmeaned'].fillna(0, inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "total_features.remove('meaneduc')",
                "",
                "total_features"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# we can fill v18q1 (household tablets) with 0 as individual tablet count is 0 for all such columns",
                "df[['v18q','v18q1','idhogar']].loc[df.v18q1.isna()].describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df['v18q1'] = df['v18q'].groupby(df['idhogar']).transform('sum')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.sample(7)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ff = pd.DataFrame(df.isnull().sum())",
                "ff.loc[(ff.loc[:, ff.dtypes != object] != 0).any(1)]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# rez_esc - years behind in school",
                "df['rez_esc'].describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['rez_esc'].isnull().sum()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# only these many rows has values for years behind school",
                "len(df) - df['rez_esc'].isnull().sum()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['v2a1'].isnull().sum()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# only these many rows has values for income",
                "len(df) - df['v2a1'].isnull().sum()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# number of rows where income and rez_esc has values",
                "len(df.loc[(df.v2a1 >= 0)]), len(df.loc[(df.rez_esc >= 0)])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# how many rows with nan values for both income and rez_esc ",
                "len(df.loc[(df.v2a1 >= 0) & (df.rez_esc >= 0)])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# how many rows with nan values for either income or rez_esc ",
                "len(df.loc[(df.v2a1 >= 0) | (df.rez_esc >= 0)])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['rez_esc'].hist()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['rez_esc','v2a1']].corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['Target','rez_esc']].corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['Target','rez_esc']].fillna(0).corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','Target']].corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','Target']].fillna(0).corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['rez_esc'].unique()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(13,7))",
                "sns.kdeplot(df['rez_esc'])",
                "sns.kdeplot(df['rez_esc'].fillna(0))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(13,7))",
                "sns.kdeplot(df['v2a1'])",
                "sns.kdeplot(df['v2a1'].fillna(0))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "x, y = df['rez_esc'], df['v2a1']",
                "plt.figure(figsize=(23,17))",
                "g = sns.jointplot(x, y, data=df, kind=\"kde\", color=\"m\")",
                "g.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")",
                "g.ax_joint.collections[0].set_alpha(0)",
                "g.set_axis_labels(\"$X$\", \"$Y$\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "x, y = df['rez_esc'].fillna(0), df['v2a1'].fillna(0)",
                "sns.jointplot(x, y, data=df, kind=\"kde\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df['rez_esc'].fillna(0, inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ff = pd.DataFrame(df.isnull().sum())",
                "ff.loc[(ff.loc[:, ff.dtypes != object] != 0).any(1)]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "x, y = df['Target'], df['v2a1']",
                "plt.figure(figsize=(23,17))",
                "g = sns.jointplot(x, y, data=df, kind=\"kde\", color=\"m\")",
                "g.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")",
                "g.ax_joint.collections[0].set_alpha(0)",
                "g.set_axis_labels(\"$X$\", \"$Y$\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "x, y = df['Target'], df['v2a1'].fillna(0)",
                "plt.figure(figsize=(23,17))",
                "g = sns.jointplot(x, y, data=df, kind=\"kde\", color=\"m\")",
                "g.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")",
                "g.ax_joint.collections[0].set_alpha(0)",
                "g.set_axis_labels(\"$X$\", \"$Y$\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['Target'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.groupby('Target').count()['v2a1']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(figsize=(15,7))",
                "df.groupby('Target').count()['v2a1'].plot(ax=ax)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(figsize=(15,7))",
                "df.fillna(0).groupby('Target').count()['v2a1'].plot(ax=ax)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(figsize=(15,7))",
                "df.groupby(['Target','hhsize']).count()['v2a1'].unstack().plot(ax=ax)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(figsize=(15,7))",
                "df.fillna(0).groupby(['Target','hhsize']).count()['v2a1'].unstack().plot(ax=ax)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['hhsize'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['hogar_total'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# use hhsize, ignore 'hogar_total',",
                "total_features.remove('hogar_total')",
                "",
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['hhsize','hogar_adul']].corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['hhsize','Target']].corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['Target','hogar_adul']].corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['hogar_adul'])",
                "sns.kdeplot(df['hhsize'])",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['hogar_total'])",
                "sns.kdeplot(df['hogar_adul'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "max(df['hogar_adul']), max(df['hogar_total'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df.groupby('idhogar').sum()[['hogar_adul','hogar_total']].sample(10).plot.bar()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['hogar_total'])",
                "sns.kdeplot(df['hogar_nin'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['male'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['female'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# removing female",
                "total_features.remove('female')",
                "",
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['r4t3'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['tamhog'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['tamviv'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(17,13))",
                "sns.kdeplot(df['tamviv'])",
                "sns.kdeplot(df['tamhog'])",
                "sns.kdeplot(df['r4t3'])",
                "sns.kdeplot(df['hhsize'])",
                "sns.kdeplot(df['hogar_total'])",
                "#sns.kdeplot(df['hogar_adul'])",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# removing 'r4t3', as 'hhsize' is of almost same distribution",
                "total_features.remove('r4t3')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['dependency'].describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['dependency'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['SQBdependency'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['SQBdependency'].describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "cat_cols"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['edjefe'].describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['edjefa'].describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['edjefe'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['edjefa'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.loc[df.edjefa == 'yes', 'edjefa'] = 1",
                "df.loc[df.edjefa == 'no', 'edjefa'] = 0",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.loc[df.edjefe == 'yes', 'edjefe'] = 1",
                "df.loc[df.edjefe == 'no', 'edjefe'] = 0",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['edjefa','edjefe']].describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df[['edjefa','edjefe']] = df[['edjefa','edjefe']].apply(pd.to_numeric)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['edjefa','edjefe']].dtypes"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "total_features.append('edjefa')",
                "total_features.append('edjefe')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "cols_water"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[cols_water].describe()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[cols_water].corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['abastaguadentro'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['abastaguafuera'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['abastaguano'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_water_target = df.groupby('Target')[cols_water].sum().reset_index()",
                "df_water_target"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "722+1496+1133+5844"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_water_target.corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "total_features.remove('abastaguano')",
                "total_features.remove('abastaguafuera')",
                "",
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# cols_floor",
                "# ",
                "# ['pisocemento', 'pisomadera', 'pisomoscer', 'pisonatur', 'pisonotiene', 'pisoother',]",
                "",
                "df['pisocemento'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_floor_target = df.groupby('Target')[cols_floor].sum().reset_index()",
                "df_floor_target"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# removing these features -> inc by 0.002",
                "total_features.remove('pisonatur')",
                "total_features.remove('pisonotiene')",
                "total_features.remove('pisoother')",
                "",
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_wall_target = df.groupby('Target')[cols_outside_wall].sum().reset_index()",
                "df_wall_target"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['paredblolad'])",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['paredpreb'])",
                "sns.kdeplot(df['paredmad'])",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['paredmad'])",
                "sns.kdeplot(df['paredzocalo'])",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['paredpreb'])",
                "sns.kdeplot(df['paredmad'])",
                "sns.kdeplot(df['paredzocalo'])",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_roof_target = df.groupby('Target')[cols_roof].sum().reset_index()",
                "df_roof_target"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['techozinc'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['techozinc'])",
                "sns.kdeplot(df['techoentrepiso'])",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['techoentrepiso'])",
                "sns.kdeplot(df['techocane'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# [ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6',]",
                "",
                "df_sani_target = df.groupby('Target')[cols_sanitary].sum().reset_index()",
                "df_sani_target"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['sanitario1'])",
                "sns.kdeplot(df['sanitario6'])",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['sanitario3'])",
                "sns.kdeplot(df['sanitario2'])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# cols_tip ",
                "# ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',]",
                "",
                "df_tipo_target = df.groupby('Target')[cols_tip].sum().reset_index()",
                "df_tipo_target"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['tipovivi2'])",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['tipovivi1'])",
                "sns.kdeplot(df['tipovivi3'])",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['tipovivi5'])",
                "sns.kdeplot(df['tipovivi4'])",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['v2a1'].isna().sum()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['tipovivi3'].value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.loc[(df['v2a1'].isna()) & (df.tipovivi3 == 1)]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['v2a1'].loc[df.parentesco1 == 1].plot.line()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['v2a1'].loc[df.parentesco1 == 1].plot.hist()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['v2a1'].loc[df.parentesco1 == 1].mean(), df['v2a1'].loc[df.parentesco1 == 1].max(), df['v2a1'].loc[df.parentesco1 == 1].min()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (df.v2a1.isna())].describe(include='all')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 != 1].describe(include='all')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 == 1].describe(include='all')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df['v2a1'].fillna(120000, inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['v2a1'].isna().sum()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "'Target' in total_features"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X, y = df[total_features], df['Target']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Split the dataset into train and Test",
                "seed = 42",
                "test_size = 0.3",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "#Train the XGboost Model for Classification",
                "model1 = xgb.XGBClassifier(n_jobs=n_jobs)",
                "model1"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "train_model1 = model1.fit(X_train, y_train)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "model2 = xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, subsample=0.5, n_jobs=n_jobs)",
                "model2"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "train_model2 = model2.fit(X_train, y_train)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "# predictions",
                "pred1 = train_model1.predict(X_test)",
                "pred2 = train_model2.predict(X_test)",
                "",
                "print('Model 1 XGboost Report %r' % (classification_report(y_test, pred1)))",
                "print('Model 2 XGboost Report %r' % (classification_report(y_test, pred2)))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(\"Accuracy for model 1: %.2f\" % (accuracy_score(y_test, pred1) * 100))",
                "print(\"Accuracy for model 2: %.2f\" % (accuracy_score(y_test, pred2) * 100))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "#Let's do a little Gridsearch, Hyperparameter Tunning",
                "model3 = xgb.XGBClassifier(",
                " learning_rate =0.1,",
                " n_estimators=1000,",
                " max_depth=5,",
                " min_child_weight=1,",
                " gamma=0,",
                " subsample=0.8,",
                " colsample_bytree=0.8,",
                " objective= 'binary:logistic',",
                " n_jobs=n_jobs,",
                " scale_pos_weight=1,",
                " seed=27)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "train_model3 = model3.fit(X_train, y_train)",
                "pred3 = train_model3.predict(X_test)",
                "print(\"Accuracy for model 3: %.2f\" % (accuracy_score(y_test, pred3) * 100))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print('Model 3 XGboost Report %r' % (classification_report(y_test, pred3)))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "gc.collect()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "parameters = {",
                "    'n_estimators': [100],",
                "    'max_depth': [6, 9],",
                "    'subsample': [0.9, 1.0],",
                "    'colsample_bytree': [0.9, 1.0],",
                "}",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "grid = GridSearchCV(model3,",
                "                    parameters, n_jobs=n_jobs,",
                "                    scoring=\"neg_log_loss\",",
                "                    cv=3)",
                "grid"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "gc.collect()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(figsize=(23, 17))",
                "plot_importance(model3, ax=ax)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "less_imp_features = ['estadocivil1','instlevel9','techocane','parentesco10','v14a',",
                "                     'parentesco11','parentesco5','paredother','parentesco7','noelec',",
                "                     'elimbasu4','elimbasu6']",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# before removing less important features",
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "for f in less_imp_features:",
                "    if f in total_features:",
                "        total_features.remove(f)",
                "",
                "len(total_features)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X, y = df[total_features], df['Target']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Split the dataset into train and Test",
                "seed = 43",
                "test_size = 0.3",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "train_model5 = model3.fit(X_train, y_train)",
                "pred5 = train_model5.predict(X_test)",
                "print(\"Accuracy for model 5: %.2f\" % (accuracy_score(y_test, pred5) * 100))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print('Model 5 XGboost Report %r' % (classification_report(y_test, pred5)))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "train_model6 = train_model5.fit(X, y)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_model6"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "thresholds = sort(model3.feature_importances_)",
                "thresholds"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "thresholds.shape"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "np.unique(thresholds).shape"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "model4 = RandomForestClassifier(n_jobs=n_jobs)",
                "model4"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "gc.collect()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "train_model4 = model4.fit(X_train, y_train)",
                "pred4 = train_model4.predict(X_test)",
                "print(\"Accuracy for model 4: %.2f\" % (accuracy_score(y_test, pred4) * 100))",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print('Model 4 XGboost Report %r' % (classification_report(y_test, pred4)))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "confusion_matrix(y_test, pred4)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "df_test = pd.read_csv('../input/test.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(df_test)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_test.sample(10)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(df_test)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_test.loc[df_test.edjefa == 'yes', 'edjefa'] = 1",
                "df_test.loc[df_test.edjefa == 'no', 'edjefa'] = 0",
                "",
                "df_test.loc[df_test.edjefe == 'yes', 'edjefe'] = 1",
                "df_test.loc[df_test.edjefe == 'no', 'edjefe'] = 0",
                "df_test[['edjefa','edjefe']] = df_test[['edjefa','edjefe']].apply(pd.to_numeric)",
                "df_test[['edjefa','edjefe']].dtypes"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_actual_test = df_test[total_features]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_actual_test.shape"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "pred_actual = train_model6.predict(X_actual_test)",
                "pred_actual"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "pred_actual.shape"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_final = pd.DataFrame(df['Id'], pred_actual).reset_index()",
                "df_final.columns = ['Target','Id']",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "cols = df_final.columns.tolist()",
                "cols"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "cols = cols[-1:] + cols[:-1]",
                "cols",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_final = df_final[cols]",
                "df_final.head(7)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "df_final.index.name = None",
                "df_final.head(7)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_final['Target'].value_counts()",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_final[cols].sample(4)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "df_final[cols].to_csv('sample_submission.csv', index=False)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "os.listdir('../input/')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# import package with helper functions ",
                "import bq_helper",
                "",
                "# create a helper object for this dataset",
                "github = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\",",
                "                                              dataset_name=\"github_repos\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# You can use two dashes (--) to add comments in SQL",
                "query = (\"\"\"",
                "        -- Select all the columns we want in our joined table",
                "        SELECT L.license, COUNT(sf.path) AS number_of_files",
                "        FROM `bigquery-public-data.github_repos.sample_files` as sf",
                "        -- Table to merge into sample_files",
                "        INNER JOIN `bigquery-public-data.github_repos.licenses` as L ",
                "            ON sf.repo_name = L.repo_name -- what columns should we join on?",
                "        GROUP BY L.license",
                "        ORDER BY number_of_files DESC",
                "        \"\"\")",
                "",
                "file_count_by_license = github.query_to_pandas_safe(query, max_gb_scanned=6)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# print out all the returned results",
                "print(file_count_by_license)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#view the list of tables inside the big query",
                "github.list_tables()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#lets view the a couple of lines from the table using head",
                "github.head(\"sample_commits\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#lets view the a couple of lines from the table using head",
                "github.head(\"sample_files\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "#How many commits (recorded in the \"sample_commits\" table) have been made in repos written in the Python programming language? (I'm looking for the number of commits per repo for all the repos written in Python.",
                "",
                "query = (\"\"\"",
                "        -- Select all the columns we want in our joined table",
                "        SELECT sf.repo_name, count(sc.commit) number_of_commits_in_python",
                "        FROM `bigquery-public-data.github_repos.sample_commits` as sc",
                "        -- Table to merge into sample_files",
                "        INNER JOIN `bigquery-public-data.github_repos.sample_files` as sf ",
                "            ON sc.repo_name =  sf.repo_name -- what columns should we join on?",
                "        WHERE sf.path LIKE '%.py'",
                "        GROUP BY sf.repo_name",
                "        ORDER BY number_of_commits_in_python DESC",
                "        \"\"\")",
                "",
                "print(github.estimate_query_size(query))",
                "file_count_by_python_files = github.query_to_pandas_safe(query, max_gb_scanned=6)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "file_count_by_python_files"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import os",
                "import torch",
                "import torchvision",
                "import tarfile",
                "import torch.nn as nn",
                "import numpy as np",
                "import torch.nn.functional as F",
                "from torchvision.datasets.utils import download_url",
                "from torchvision.datasets import ImageFolder",
                "from torch.utils.data import DataLoader",
                "import torchvision.transforms as tt",
                "from torch.utils.data import random_split",
                "from torchvision.utils import make_grid",
                "import matplotlib.pyplot as plt",
                "%matplotlib inline"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "project_name='course_project'"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "data_dir = '../input/russian-handwritten-letters'",
                "",
                "print(os.listdir(data_dir))",
                ""
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from torchvision.datasets import ImageFolder",
                "from torchvision.transforms import ToTensor"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "dataset = ImageFolder(data_dir, transform=ToTensor())"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "img, label = dataset[0]",
                "print(img.shape, label)",
                "img"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(dataset.classes)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "import matplotlib.pyplot as plt",
                "",
                "def show_example(img, label):",
                "    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")",
                "    plt.imshow(img.permute(1, 2, 0))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "show_example(*dataset[0])"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Data transforms (normalization & data augmentation)",
                "stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))",
                "train_tfms = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'), ",
                "                         tt.RandomHorizontalFlip(), ",
                "                         tt.ToTensor(), ",
                "                         tt.Normalize(*stats,inplace=True)])",
                "valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "batch_size = 400"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "val_size = 5000",
                "train_size = len(dataset) - val_size",
                "",
                "train_ds, val_ds = random_split(dataset, [train_size, val_size])",
                "len(train_ds), len(val_ds)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "# PyTorch data loaders",
                "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)",
                "valid_dl = DataLoader(val_ds, batch_size*2, num_workers=3, pin_memory=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "random_seed = 42",
                "torch.manual_seed(random_seed);"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "image_size = 784",
                "hidden_size = 256",
                "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)",
                "val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "from torchvision.utils import make_grid",
                "",
                "def show_batch(dl):",
                "    for images, labels in dl:",
                "        fig, ax = plt.subplots(figsize=(12, 12))",
                "        ax.set_xticks([]); ax.set_yticks([])",
                "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))",
                "        break"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "show_batch(train_dl)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "def get_default_device():",
                "    \"\"\"Pick GPU if available, else CPU\"\"\"",
                "    if torch.cuda.is_available():",
                "        return torch.device('cuda')",
                "    else:",
                "        return torch.device('cpu')",
                "    ",
                "def to_device(data, device):",
                "    \"\"\"Move tensor(s) to chosen device\"\"\"",
                "    if isinstance(data, (list,tuple)):",
                "        return [to_device(x, device) for x in data]",
                "    return data.to(device, non_blocking=True)",
                "",
                "class DeviceDataLoader():",
                "    \"\"\"Wrap a dataloader to move data to a device\"\"\"",
                "    def __init__(self, dl, device):",
                "        self.dl = dl",
                "        self.device = device",
                "        ",
                "    def __iter__(self):",
                "        \"\"\"Yield a batch of data after moving it to device\"\"\"",
                "        for b in self.dl: ",
                "            yield to_device(b, self.device)",
                "",
                "    def __len__(self):",
                "        \"\"\"Number of batches\"\"\"",
                "        return len(self.dl)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "device = get_default_device()",
                "device"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train_dl = DeviceDataLoader(train_dl, device)",
                "valid_dl = DeviceDataLoader(valid_dl, device)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "class SimpleResidualBlock(nn.Module):",
                "    def __init__(self):",
                "        super().__init__()",
                "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)",
                "        self.relu1 = nn.ReLU()",
                "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)",
                "        self.relu2 = nn.ReLU()",
                "        ",
                "    def forward(self, x):",
                "        out = self.conv1(x)",
                "        out = self.relu1(out)",
                "        out = self.conv2(out)",
                "        return self.relu2(out) + x # ReLU can be applied before or after adding the input"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "simple_resnet = to_device(SimpleResidualBlock(), device)",
                "",
                "for images, labels in train_dl:",
                "    out = simple_resnet(images)",
                "    print(out.shape)",
                "    break",
                "    ",
                "del simple_resnet, images, labels",
                "torch.cuda.empty_cache()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "def accuracy(outputs, labels):",
                "    _, preds = torch.max(outputs, dim=1)",
                "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))",
                "",
                "class ImageClassificationBase(nn.Module):",
                "    def training_step(self, batch):",
                "        images, labels = batch ",
                "        out = self(images)                  # Generate predictions",
                "        loss = F.cross_entropy(out, labels) # Calculate loss",
                "        return loss",
                "    ",
                "    def validation_step(self, batch):",
                "        images, labels = batch ",
                "        out = self(images)                    # Generate predictions",
                "        loss = F.cross_entropy(out, labels)   # Calculate loss",
                "        acc = accuracy(out, labels)           # Calculate accuracy",
                "        return {'val_loss': loss.detach(), 'val_acc': acc}",
                "        ",
                "    def validation_epoch_end(self, outputs):",
                "        batch_losses = [x['val_loss'] for x in outputs]",
                "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses",
                "        batch_accs = [x['val_acc'] for x in outputs]",
                "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies",
                "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}",
                "    ",
                "    def epoch_end(self, epoch, result):",
                "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(",
                "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "def conv_block(in_channels, out_channels, pool=False):",
                "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), ",
                "              nn.BatchNorm2d(out_channels), ",
                "              nn.ReLU(inplace=True)]",
                "    if pool: layers.append(nn.MaxPool2d(2))",
                "    return nn.Sequential(*layers)",
                "",
                "class ResNet9(ImageClassificationBase):",
                "    def __init__(self, in_channels, num_classes):",
                "        super().__init__()",
                "        ",
                "        self.conv1 = conv_block(in_channels, 64)",
                "        self.conv2 = conv_block(64, 128, pool=True)",
                "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))",
                "        ",
                "        self.conv3 = conv_block(128, 256, pool=True)",
                "        self.conv4 = conv_block(256, 512, pool=True)",
                "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))",
                "        ",
                "        self.classifier = nn.Sequential(nn.MaxPool2d(4), ",
                "                                        nn.Flatten(), ",
                "                                        nn.Linear(512, num_classes))",
                "        ",
                "    def forward(self, xb):",
                "        out = self.conv1(xb)",
                "        out = self.conv2(out)",
                "        out = self.res1(out) + out",
                "        out = self.conv3(out)",
                "        out = self.conv4(out)",
                "        out = self.res2(out) + out",
                "        out = self.classifier(out)",
                "        return out"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "model = to_device(ResNet9(3, 10), device)",
                "model"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "@torch.no_grad()",
                "def evaluate(model, val_loader):",
                "    model.eval()",
                "    outputs = [model.validation_step(batch) for batch in val_loader]",
                "    return model.validation_epoch_end(outputs)",
                "",
                "def get_lr(optimizer):",
                "    for param_group in optimizer.param_groups:",
                "        return param_group['lr']",
                "",
                "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, ",
                "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):",
                "    torch.cuda.empty_cache()",
                "    history = []",
                "    ",
                "    # Set up cutom optimizer with weight decay",
                "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)",
                "    # Set up one-cycle learning rate scheduler",
                "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, ",
                "                                                steps_per_epoch=len(train_loader))",
                "    ",
                "    for epoch in range(epochs):",
                "        # Training Phase ",
                "        model.train()",
                "        train_losses = []",
                "        lrs = []",
                "        for batch in train_loader:",
                "            loss = model.training_step(batch)",
                "            train_losses.append(loss)",
                "            loss.backward()",
                "            ",
                "            # Gradient clipping",
                "            if grad_clip: ",
                "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)",
                "            ",
                "            optimizer.step()",
                "            optimizer.zero_grad()",
                "            ",
                "            # Record & update learning rate",
                "            lrs.append(get_lr(optimizer))",
                "            sched.step()",
                "        ",
                "        # Validation phase",
                "        result = evaluate(model, val_loader)",
                "        result['train_loss'] = torch.stack(train_losses).mean().item()",
                "        result['lrs'] = lrs",
                "        model.epoch_end(epoch, result)",
                "        history.append(result)",
                "    return history"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "epochs = 8",
                "max_lr = 0.01",
                "grad_clip = 0.1",
                "weight_decay = 1e-4",
                "opt_func = torch.optim.Adam"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "%%time",
                "history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plot_accuracies(history):",
                "    accuracies = [x['val_acc'] for x in history]",
                "    plt.plot(accuracies, '-x')",
                "    plt.xlabel('epoch')",
                "    plt.ylabel('accuracy')",
                "    plt.title('Accuracy vs. No. of epochs');"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "history = [evaluate(model, valid_dl)]",
                "history"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_accuracies(history)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plot_losses(history):",
                "    train_losses = [x.get('train_loss') for x in history]",
                "    val_losses = [x['val_loss'] for x in history]",
                "    plt.plot(train_losses, '-bx')",
                "    plt.plot(val_losses, '-rx')",
                "    plt.xlabel('epoch')",
                "    plt.ylabel('loss')",
                "    plt.legend(['Training', 'Validation'])",
                "    plt.title('Loss vs. No. of epochs');"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_losses(history)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plot_lrs(history):",
                "    lrs = np.concatenate([x.get('lrs', []) for x in history])",
                "    plt.plot(lrs)",
                "    plt.xlabel('Batch no.')",
                "    plt.ylabel('Learning rate')",
                "    plt.title('Learning Rate vs. Batch no.');"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_lrs(history)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "!pip install jovian --upgrade --quiet",
                "import jovian",
                "jovian.commit(project=project_name)",
                "dataset_url='../input/russian-handwritten-letters'",
                "jovian.log_dataset(dataset_url=dataset_url, val_size=val_size, random_seed=random_seed)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings    #warnings to ignore any kind of warnings that we may recieve.\n",
                "warnings.filterwarnings('ignore')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def display_all(df):\n",
                "    '''\n",
                "    input: dataframe\n",
                "    description: it takes a dataframe and allows use to show a mentioned no. of rows and columns in the screen\n",
                "    '''\n",
                "    with pd.option_context(\"display.max_rows\",10,\"display.max_columns\",9):  #you might want to change these numbers.\n",
                "        display(df)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "df=pd.read_csv('../input/diabetes.csv')\n",
                "df.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "display_all(df)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def missing_values_table(df):\n",
                "        # Total missing values\n",
                "        mis_val = df.isnull().sum()\n",
                "        \n",
                "        # Percentage of missing values\n",
                "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
                "        \n",
                "        # Make a table with the results\n",
                "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
                "        \n",
                "        # Rename the columns\n",
                "        mis_val_table_ren_columns = mis_val_table.rename(\n",
                "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
                "        \n",
                "        # Sort the table by percentage of missing descending\n",
                "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
                "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
                "        '% of Total Values', ascending=False).round(1)\n",
                "        \n",
                "        # Print some summary information\n",
                "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
                "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
                "              \" columns that have missing values.\")\n",
                "        \n",
                "        \n",
                "        return mis_val_table_ren_columns"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "missing_values_table(df)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "features_with_missing_values=['BMI','SkinThickness','BloodPressure','Insulin','Glucose']\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for i in features_with_missing_values:\n",
                "    df[i]=df[i].replace(0,np.median(df[i].values))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "target=df['Outcome'].values\n",
                "df.drop(['Outcome'],inplace=True,axis=1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "#from sklearn importing standard scalar that will convert the provided dataframe into standardised one.\n",
                "from sklearn.preprocessing import StandardScaler                                              \n",
                "sta=StandardScaler()\n",
                "input=sta.fit_transform(df)    #will give numpy array as output"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train,X_test,y_train,y_test=train_test_split(input,target,test_size=0.1,random_state=0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.neighbors import KNeighborsClassifier"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "knn=KNeighborsClassifier(n_neighbors=7)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "knn.fit(X_train,y_train)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "knn.score(X_test,y_test)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "train_model",
                "evaluate_model",
                "transfer_results",
                "validate_data"
            ],
            "content": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from scipy.stats import skew\n",
                "\n",
                "train = pd.read_csv(\"../input/train.csv\")\n",
                "test = pd.read_csv(\"../input/test.csv\")\n",
                "\n",
                "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
                "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
                "\n",
                "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
                "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
                "\n",
                "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
                "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
                "skewed_feats = skewed_feats.index\n",
                "\n",
                "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
                "all_data = pd.get_dummies(all_data)\n",
                "all_data = all_data.fillna(all_data.mean())\n",
                "\n",
                "X_train = all_data[:train.shape[0]]\n",
                "X_test = all_data[train.shape[0]:]\n",
                "y = train.SalePrice\n",
                "\n",
                "from sklearn.linear_model import LassoCV\n",
                "from sklearn.model_selection import cross_val_score\n",
                "\n",
                "def rmse_cv(model):\n",
                "    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
                "    return(rmse)\n",
                "\n",
                "model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\n",
                "rmse_cv(model_lasso).mean()\n",
                "\n",
                "coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n",
                "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
                "\n",
                "lasso_preds = np.expm1(model_lasso.predict(X_test))\n",
                "\n",
                "solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":lasso_preds})\n",
                "solution.to_csv(\"ridge_sol.csv\", index = False)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "import pandas as pd \n",
                "trd=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n",
                "trd.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "import pandas as pd \n",
                "tsd=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "tsd.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "w=trd.loc[trd.YrSold<2008][trd.SaleCondition=='Normal']\n",
                "#print(w)\n",
                "rw= len(w)/len(trd)\n",
                "print(\"percentage of houses are for sale before 2008 are normal is : \",rw)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "x=trd.loc[trd.LotArea<10000][trd.SaleCondition=='Normal']\n",
                "#print(w)\n",
                "rw= len(x)/len(trd)\n",
                "print(\"percentage of houses are having area less than 10000 m^2 are normal is : \",rw)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "value=trd.loc[trd.MSSubClass>60]\n",
                "print(value)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "train_model"
            ],
            "content": [
                "\"\"\"author    s_agnik1511\"\"\"\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "trd = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
                "train_y = trd.SalePrice\n",
                "predictor_cols = ['LotArea']\n",
                "train_X = trd[predictor_cols]\n",
                "my_model = RandomForestRegressor()\n",
                "my_model.fit(train_X, train_y)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "trd.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "transfer_results"
            ],
            "content": [
                "\"\"\"author s_agnik1511\"\"\"\n",
                "import pandas as pd\n",
                "my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n",
                "my_submission.to_csv('submission_sagnik.csv', index=False)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "import pandas as pd\n",
                "k=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "k.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "transfer_results"
            ],
            "content": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "m=tsd.predict(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "Submission[\"taregt\"]=m\n",
                "Submission.to_csv(\"submission_sagnik123\",index=False)\n"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "ingest_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "\"\"\"author s_agnik1511\"\"\"\n",
                "test=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
                "test_X=test[predictor_cols]\n",
                "predicted_prices=my_model.predict(test_X)\n",
                "print(predicted_prices)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.linear_model import LinearRegression\n",
                "import pandas as pd\n",
                "Model=LinearRegression()\n",
                "x_test=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n",
                "Model.fit(x_test)\n",
                "Model.predict(x_test)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "\n",
                "import pandas as pd # for reading the data frame\n",
                "import numpy as np # for numerical calculation\n",
                "import matplotlib.pyplot as plt # use for visualization\n",
                "import seaborn as sns   # mostly used for statistical visualization \n",
                "%matplotlib inline      # used for inline ploting\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n",
                "test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "\n",
                "print(\"Shape of train: \", train.shape) #  rows : 1459 columns : 81\n",
                "print(\"Shape of test: \", test.shape)  # rows : 1459 columns : 80"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head(20)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.head(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df = pd.concat((train, test)) # here we concat the test and train data set\n",
                "temp_df = df\n",
                "print(\"Shape of df: \", df.shape)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "temp_df.head() # by default its selected 5 rows and all the columns"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "temp_df.tail() # for vewig the last five rows"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# To show the all columns\n",
                "pd.set_option(\"display.max_columns\",2000)  # used for viewing all the columns at onces\n",
                "pd.set_option(\"display.max_rows\",85)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.head() "
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.info()   # lets view the information about our data set like find the data types of our columns"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.describe() # used for finding the describtion about the data set like,  mean , standard deviation given below"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.select_dtypes(include=['int64', 'float64']).columns  # extracrt the columns whose dtype is intege and float"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.select_dtypes(include=['object']).columns  # find the columns whose dtype is object"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Set index as Id column\n",
                "df = df.set_index(\"Id\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Show the null values using heatmap\n",
                "plt.figure(figsize=(16,9))\n",
                "sns.heatmap(df.isnull())       \n",
                "\n",
                "# useing heat map we can see the missing values ... the white stripes indicates the missing values \n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.isnull().sum()   # from this we can see which columns has how many missig values  like LotFrontsge has 486 missing vales"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Get the percentages of null value\n",
                "null_percent = df.isnull().sum()/df.shape[0]*100\n",
                "null_percent\n",
                "\n",
                "\n",
                "# from this we can say LotFrontage has 16 % and Alley has 93 % missing vslues"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "col_for_drop = null_percent[null_percent > 20].keys() # if the null value % 20 or > 20 so need to drop it"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# drop columns\n",
                "df = df.drop(col_for_drop, \"columns\")\n",
                "df.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "null_percent = df.isnull().sum()/df.shape[0]*100\n",
                "null_percent # shows values which has less than 20 % missing values"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# find the unique value count\n",
                "for i in df.columns:\n",
                "    print(i + \"\\t\" + str(len(df[i].unique())))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# find unique values of each column\n",
                "for i in df.columns:\n",
                "    print(\"Unique value of:>>> {} ({})\\n{}\\n\".format(i, len(df[i].unique()), df[i].unique()))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Describe the target \n",
                "train[\"SalePrice\"].describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Plot the distplot of target\n",
                "plt.figure(figsize=(10,8))\n",
                "bar = sns.distplot(train[\"SalePrice\"])"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# correlation heatmap\n",
                "plt.figure(figsize=(25,25))\n",
                "ax = sns.heatmap(train.corr(), cmap = \"coolwarm\", annot=True, linewidth=2)\n",
                "\n",
                "# here we use piearson corelation"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# correlation heatmap of higly correlated features with SalePrice\n",
                "hig_corr = train.corr()\n",
                "hig_corr_features = hig_corr.index[abs(hig_corr[\"SalePrice\"]) >= 0.5]\n",
                "hig_corr_features\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,8))\n",
                "bar = sns.distplot(train[\"LotFrontage\"])"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,8))\n",
                "bar = sns.distplot(train[\"MSSubClass\"])"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "mylist = [1, 2, 3, 4, 5, 6]\n",
                "\n",
                "# Printing out each number one by one in a for-loop\n",
                "for x in mylist:\n",
                "    print (x)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Your notebook already knows about mylist. Sum its values by adding the code below this comment.\n",
                "total = 0\n",
                "\n",
                "# This is showing cumulative addition\n",
                "for x in mylist:\n",
                "    total=total+x\n",
                "    print(total)\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "s = \"This is a test string for HCDE 530\"\n",
                "# Add your code below\n",
                "y=s.split()\n",
                "\n",
                "# For each loop, the word that comes out of the variable \"y\" goes on a new line\n",
                "for x in y:\n",
                "    print (x)\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Add your code here\n",
                "mylist = [1, 2, 3, 4, 5, 6]\n",
                "\n",
                "# This will replace the fourth item in this list with \"four\"\n",
                "mylist[3]=\"four\"\n",
                "print (mylist)\n",
                "\n",
                "\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "# The code below allows you to access your Kaggle data files\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# create a file handle called fname to open and hold the contents of the data file\n",
                "fname = open('/kaggle/input/testtext/test.txt', 'r')\n",
                "\n",
                "# Add your code below\n",
                "for f in fname:\n",
                "    # This will print out each line\n",
                "    print (f.rstrip())\n",
                "\n",
                "# It's good practice to close your file when you are finished. This is in the next line.\n",
                "fname.close()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "dogList = [\"Akita\",\"Alaskan Malamute\",\"Australian shepherd\",\"Basset hound\",\"Beagle\",\"Boston terrier\",\"Bulldog\",\"Chihuahua\",\"Cocker Spaniel\",\"Collie\",\"French Bulldog\",\"Golden Retriever\",\"Great Dane\",\"Poodle\",\"Russell Terrier\",\"Scottish Terrier\",\"Siberian Husky\",\"Skye terrier\",\"Smooth Fox terrier\",\"Terrier\",\"Whippet\"]\n",
                "\n",
                "# Add your code below\n",
                "for dogs in dogList:\n",
                "    # If the word \"Terrier\" (uppercase) is in this item, print the character number of where it starts\n",
                "    if (dogs.find(\"Terrier\") != -1):\n",
                "        print (dogs.find(\"Terrier\"))\n",
                "     # If the word \"terrier\" (lowercase) is in this item, print the character number of where it starts\n",
                "    elif (dogs.find(\"terrier\") != -1):\n",
                "        print(dogs.find(\"terrier\"))\n",
                "    # If neither \"Terrier\" nor \"terrier\" are in the item, just print -1\n",
                "    else:\n",
                "        print(-1)\n",
                "                                               \n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "binList = [0,1,1,0,1,1,0]\n",
                "\n",
                "# Add your code below\n",
                "for num in binList:\n",
                "    # If the item in the list is \"1\" then print \"One\"\n",
                "    if num==1:\n",
                "        print(\"One\")\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for dogs in dogList:\n",
                "    # If the item in dogList has \"Bulldog\" then print the name of the dog. Otherwise, don't print anything.\n",
                "    if (dogs.find(\"Bulldog\") != -1):\n",
                "        print(dogs)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "numChars = 0\n",
                "numLines = 0\n",
                "numWords = 0\n",
                "\n",
                "# create a file handle called fname to open and hold the contents of the data file\n",
                "# make sure to upload your test.txt file first\n",
                "fname = open('/kaggle/input/testtext/test.txt', 'r')\n",
                "\n",
                "# Add your code below to read each line in the file, count the number of characters, lines, and words\n",
                "# updating the numChars, numLines, and numWords variables.\n",
                "for f in fname:\n",
                "    \n",
                "    # Cumulative addition of the length of each word\n",
                "    numChars=len(f)+numChars\n",
                "    # Cumulative addition of the first letter in each line\n",
                "    numLines=len(f[0])+numLines\n",
                "    # Cumulative addition of items in each line's list\n",
                "    numWords=(len(f.split()))+numWords\n",
                "    \n",
                "# output code below is provided for you; you should not edit this\n",
                "\n",
                "print('%d characters'%numChars)\n",
                "print('%d lines'%numLines)\n",
                "print('%d words'%numWords)\n",
                "\n",
                "# It's good practice to close your file when you are finished. This is in the next line.\n",
                "fname.close()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "# Add your code below\n",
                "numChars = 0\n",
                "numLines = 0\n",
                "numWords = 0\n",
                "\n",
                "# create a file handle called fname to open and hold the contents of the data file\n",
                "# make sure to upload your test.txt file first\n",
                "fname = open('/kaggle/input/reading/sherlock.txt', 'r')\n",
                "\n",
                "# Add your code below to read each line in the file, count the number of characters, lines, and words\n",
                "# updating the numChars, numLines, and numWords variables.\n",
                "for f in fname:\n",
                "    \n",
                "    # Cumulative addition of the length of each word\n",
                "    numChars=len(f)+numChars\n",
                "    # Cumulative addition of the first letter in each line\n",
                "    numLines=len(f[0])+numLines\n",
                "    # Cumulative addition of items in each line's list\n",
                "    numWords=(len(f.split()))+numWords\n",
                "    \n",
                "# output code below is provided for you; you should not edit this\n",
                "\n",
                "print('%d characters'%numChars)\n",
                "print('%d lines'%numLines)\n",
                "print('%d words'%numWords)\n",
                "\n",
                "# It's good practice to close your file when you are finished. This is in the next line.\n",
                "fname.close()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# Importing the relevant libraries\n",
                "import IPython.display\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "import plotly.offline as py\n",
                "py.init_notebook_mode(connected=True)\n",
                "import plotly.graph_objs as go\n",
                "from matplotlib import pyplot as plt"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "items = pd.read_csv(\"../input/items.csv\")\n",
                "holiday_events = pd.read_csv(\"../input/holidays_events.csv\")\n",
                "stores = pd.read_csv(\"../input/stores.csv\")\n",
                "oil = pd.read_csv(\"../input/oil.csv\")\n",
                "transactions = pd.read_csv(\"../input/transactions.csv\",parse_dates=['date'])\n",
                "train = pd.read_csv(\"../input/train.csv\", parse_dates=['date'])"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(train.shape)\n",
                "train.head()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Check for NULL values in all files\n",
                "print(\"Nulls in train: {0} => {1}\".format(train.columns.values, train.isnull().any().values))\n",
                "print('---')\n",
                "print(\"Nulls in oil: {0} => {1}\".format(oil.columns.values,oil.isnull().any().values))\n",
                "print('---')\n",
                "print(\"Nulls in holiday_events: {0} => {1}\".format(holiday_events.columns.values,holiday_events.isnull().any().values))\n",
                "print('---')\n",
                "print(\"Nulls in stores: {0} => {1}\".format(stores.columns.values,stores.isnull().any().values))\n",
                "print('---')\n",
                "print(\"Nulls in transactions: {0} => {1}\".format(transactions.columns.values,transactions.isnull().any().values))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# EDA for oil.csv begins here #\n",
                "oil.head(3)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "# Plotting graph for oil price trends\n",
                "trace = go.Scatter(\n",
                "    name='Oil prices',\n",
                "    x=oil['date'],\n",
                "    y=oil['dcoilwtico'].dropna(),\n",
                "    mode='lines',\n",
                "   )\n",
                "\n",
                "data = [trace]\n",
                "\n",
                "layout = go.Layout(\n",
                "    yaxis = dict(title = 'Daily Oil price'),\n",
                "    showlegend = True)\n",
                "fig = go.Figure(data = data, layout = layout)\n",
                "py.iplot(fig, filename='pandas-time-series-error-bars')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# EDA for holiday_events.csv begins here #\n",
                "holiday_events.head(3)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "holiday_events.type.unique()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.style.use('seaborn-white')\n",
                "holiday_local_type = holiday_events.groupby(['locale_name', 'type']).size()\n",
                "holiday_local_type.unstack().plot(kind='bar',stacked=True, colormap= 'magma_r', figsize=(12,10),  grid=False)\n",
                "plt.ylabel('Count of entries')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# EDA for items.csv begins here\n",
                "items.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# BAR PLOT FOR ITEMS V/S FAMILY TYPE\n",
                "x, y = (list(x) for x in zip(*sorted(zip(items.family.value_counts().index, \n",
                "                                         items.family.value_counts().values), \n",
                "                                        reverse = False)))\n",
                "trace2 = go.Bar(\n",
                "    y = items.family.value_counts().values,\n",
                "    x = items.family.value_counts().index\n",
                ")\n",
                "\n",
                "layout = dict(\n",
                "    title='Counts of items per family category',\n",
                "     width = 900, height = 600,\n",
                "    yaxis=dict(\n",
                "        showgrid = True,\n",
                "        showline = True,\n",
                "        showticklabels = True\n",
                "    ))\n",
                "\n",
                "fig1 = go.Figure(data=[trace2])\n",
                "fig1['layout'].update(layout)\n",
                "py.iplot(fig1, filename='plots')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Persihable or not \n",
                "plt.style.use('seaborn-white')\n",
                "fam_perishable = items.groupby(['family', 'perishable']).size()\n",
                "fam_perishable.unstack().plot(kind='bar',stacked=True, colormap = 'coolwarm', figsize=(12,10),  grid = True)\n",
                "plt.ylabel('count')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# EDA for stores.csv begins here #\n",
                "stores.head(3)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# store distribution across states\n",
                "fig, ax = plt.subplots()\n",
                "fig.set_size_inches(8, 8)\n",
                "ax = sns.countplot(y = stores['state'], data = stores) "
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# store distribution across cities\n",
                "fig, ax = plt.subplots()\n",
                "fig.set_size_inches(8, 8)\n",
                "ax = sns.countplot(y = stores['city'], data = stores) "
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Unique state names\n",
                "stores.state.unique()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Unique state names\n",
                "stores.city.unique()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Various types of stores and their count\n",
                "fig, ax = plt.subplots()\n",
                "fig.set_size_inches(8, 5)\n",
                "ax = sns.countplot(x = \"type\", data = stores, palette=\"Paired\")"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ct = pd.crosstab(stores.state, stores.type)\n",
                "ct.plot.bar(figsize = (12, 6), stacked=True)\n",
                "plt.legend(title='type')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ct = pd.crosstab(stores.city, stores.type)\n",
                "\n",
                "ct.plot.bar(figsize = (12, 6), stacked=True)\n",
                "plt.legend(title='type')\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# total no. of unique stores \n",
                "stores.store_nbr.nunique()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# total no. of stores (including non-unique)\n",
                "stores.cluster.sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots()\n",
                "fig.set_size_inches(12, 7)\n",
                "ax = sns.countplot(x = \"cluster\", data = stores)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# EDA for transactions.csv begins here #\n",
                "transactions.head(3)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Finding out the no.of transactions (rows)\n",
                "print(\"There are {0} transactions\".\n",
                "      format(transactions.shape[0], transactions.shape[1]))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Time series plot for transaction #\n",
                "plt.style.use('seaborn-white')\n",
                "plt.figure(figsize=(13,11))\n",
                "plt.plot(transactions.date.values, transactions.transactions.values, color='grey')\n",
                "plt.axvline(x='2015-12-23',color='red',alpha=0.3)\n",
                "plt.axvline(x='2016-12-23',color='red',alpha=0.3)\n",
                "plt.axvline(x='2014-12-23',color='red',alpha=0.3)\n",
                "plt.axvline(x='2013-12-23',color='red',alpha=0.3)\n",
                "plt.axvline(x='2013-05-12',color='green',alpha=0.2, linestyle= '--')\n",
                "plt.axvline(x='2015-05-10',color='green',alpha=0.2, linestyle= '--')\n",
                "plt.axvline(x='2016-05-08',color='green',alpha=0.2, linestyle= '--')\n",
                "plt.axvline(x='2014-05-11',color='green',alpha=0.2, linestyle= '--')\n",
                "plt.axvline(x='2017-05-14',color='green',alpha=0.2, linestyle= '--')\n",
                "plt.ylabel('Transactions per day', fontsize= 16)\n",
                "plt.xlabel('Date', fontsize= 16)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "import pandas as pd",
                "import numpy as np",
                "import matplotlib.pyplot as plt",
                "%matplotlib inline",
                "import seaborn as sns",
                "sns.set_style(\"whitegrid\")",
                "plt.style.use(\"fivethirtyeight\")",
                "",
                "#Showing full path of datasets",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                "        ",
                "",
                "#Disable warnings",
                "import warnings",
                "warnings.filterwarnings('ignore')"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "df = pd.read_csv(\"/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Number of rows and columns in our dataset",
                "df.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#The 24 columns ",
                "df.columns"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#As mentioned in the dataset description , ",
                "#we should exclude the variable Risk-MM when training a binary classification model.",
                "#Not excluding it will leak the answers to your model and reduce its predictability.",
                "",
                "df.drop(['RISK_MM'],axis=1,inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Basic Information of dataset",
                "",
                "df.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Before looking at the description of the data",
                "#We can see that there are few columns with very less data",
                "#Evaporation,Sunshine,Cloud9am,Cloud3pm",
                "#It is better to remove these four columns as it will affect our prediction even if we",
                "#fill the na values...",
                "",
                "#Date and Location is also not required",
                "#As we are predicting rain in australia and not when and where in australia",
                "",
                "",
                "drop_cols = ['Evaporation','Sunshine','Cloud9am','Cloud3pm','Date','Location']",
                "",
                "df.drop(columns=drop_cols,inplace=True,axis=1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Basic description of our data",
                "#Numerical features first",
                "df.describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Including Categorical features with include object",
                "df.describe(include='object')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Now including all the features",
                "df.describe(include='all')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Our dataset consists of 142193 rows and the count for many features is less than 142193.",
                "#This shows presence of Null values.",
                "#Let's look at the null values..",
                "",
                "df.isna().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.skew()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Filling missing values",
                "",
                "#We can see that there are outliers in our data",
                "#So the best way to fill the na values in our numerical features is with median",
                "#Because median deals the best with outliers",
                "",
                "#Let's separate numerical and categorical",
                "#data type of numerical features is equal to float64",
                "#With the help of following list comprehension we separate the numerical features...",
                "",
                "num = [col for col in df.columns if df[col].dtype==\"float64\"]",
                "",
                "for col in num:",
                "    df[col].fillna(df[col].median(),inplace=True)",
                "    ",
                "cat = [col for col in df.columns if df[col].dtype==\"O\"]",
                "for col in cat:",
                "    df[col].fillna(df[col].mode()[0],inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Check missing values",
                "df.isna().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.corr().style.background_gradient(cmap=\"Reds\")"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#With the use of heatmap",
                "corr = df.corr()",
                "",
                "fig = plt.figure(figsize=(12,12))",
                "sns.heatmap(corr,annot=True,fmt=\".1f\",linewidths=\"0.1\")"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(\"Numerical features :: {}\\n\".format(num))",
                "print(\"No of Numerical features :: {}\".format(len(num)))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,15))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "i=1",
                "colors = ['Red','Blue','Green','Cyan',",
                "         'Red','Blue','Green','Cyan',",
                "         'Red','Blue','Green','Cyan']",
                "j=0",
                "for col in num:",
                "    plt.subplot(3,4,i)",
                "    a1 = sns.distplot(df[col],color=colors[j])",
                "    i+=1",
                "    j+=1"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,30))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "i=1",
                "for col in num:",
                "    plt.subplot(6,2,i)",
                "    a1 = sns.boxplot(data=df,x=\"RainTomorrow\",y=col)",
                "    i+=1"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "process_data"
            ],
            "content": [
                "#Create a loop that finds the outliers in train and test  and removes it",
                "features_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']",
                "",
                "for col in features_to_examine:",
                "    IQR = df[col].quantile(0.75) - df[col].quantile(0.25) ",
                "    Lower_Bound = df[col].quantile(0.25) - (IQR*3)",
                "    Upper_Bound = df[col].quantile(0.75) + (IQR*3)",
                "    ",
                "    print(\"The outliers in {} feature are values <<< {} and >>> {}\".format(col,Lower_Bound,Upper_Bound))",
                "    ",
                "    minimum = df[col].min()",
                "    maximum = df[col].max()",
                "    print(\"The minimum value in {} is {} and maximum value is {}\".format(col,minimum,maximum))",
                "    ",
                "    if maximum>Upper_Bound:",
                "          print(\"The outliers for {} are value greater than {}\\n\".format(col,Upper_Bound))",
                "    elif minimum<Lower_Bound:",
                "          print(\"The outliers for {} are value smaller than {}\\n\".format(col,Lower_Bound))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,30))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "i=1",
                "for col in num:",
                "    plt.subplot(6,2,i)",
                "    a1 = sns.barplot(data=df,x=\"RainTomorrow\",y=col)",
                "    i+=1"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,5))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "i=1",
                "features_list = [\"MaxTemp\",\"Temp9am\",\"Temp3pm\"]",
                "for feature in features_list:",
                "    plt.subplot(1,3,i)",
                "    sns.scatterplot(data=df,x=\"MinTemp\",y=feature,hue=\"RainTomorrow\")",
                "    i+=1"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,8))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "plt.subplot(3,2,1)",
                "sns.scatterplot(data=df,x=\"WindSpeed9am\",y=\"WindGustSpeed\",hue=\"RainTomorrow\")",
                "",
                "plt.subplot(3,2,2)",
                "sns.scatterplot(data=df,x=\"WindSpeed3pm\",y=\"WindGustSpeed\",hue=\"RainTomorrow\")",
                "",
                "plt.subplot(3,2,3)",
                "sns.scatterplot(data=df,x=\"Humidity9am\",y=\"Humidity3pm\",hue=\"RainTomorrow\")",
                "",
                "plt.subplot(3,2,4)",
                "sns.scatterplot(data=df,x=\"Temp9am\",y=\"Temp3pm\",hue=\"RainTomorrow\")",
                "",
                "plt.subplot(3,2,5)",
                "sns.scatterplot(data=df,x=\"MaxTemp\",y=\"Temp9am\",hue=\"RainTomorrow\")",
                "",
                "plt.subplot(3,2,6)",
                "sns.scatterplot(data=df,x=\"Humidity3pm\",y=\"Temp3pm\",hue=\"RainTomorrow\")"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "cat"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['WindGustDir'].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig = plt.figure(figsize=(15,5))",
                "sns.countplot(data=df,x=\"WindGustDir\",hue=\"RainTomorrow\");"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['WindDir9am'].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig = plt.figure(figsize=(15,5))",
                "sns.countplot(data=df,x=\"WindDir9am\",hue=\"RainTomorrow\");"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['WindDir3pm'].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig = plt.figure(figsize=(15,5))",
                "sns.countplot(data=df,x=\"WindDir3pm\",hue=\"RainTomorrow\");"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['RainTomorrow'].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(data=df,x=\"RainTomorrow\")"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split as tts",
                "y=df[['RainTomorrow']]",
                "X=df.drop(['RainTomorrow'],axis=1)",
                "",
                "X_train,X_test,y_train,y_test = tts(X,y,test_size=0.3,random_state=0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_train"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_test"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#We'll plot these four as subplots ",
                "",
                "plt.figure(figsize=(15,30))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "features_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']",
                "i=1",
                "for col in features_to_examine:",
                "    plt.subplot(6,2,i)",
                "    fig = df[col].hist(bins=10)",
                "    fig.set_xlabel(col)",
                "    fig.set_ylabel('RainTomorrow')",
                "    i+=1"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def remove_outliers(df,col,Lower_Bound,Upper_Bound):    ",
                "    minimum = df[col].min()",
                "    maximum = df[col].max()",
                "    ",
                "    if maximum>Upper_Bound:",
                "        return np.where(df[col]>Upper_Bound,Upper_Bound,df[col])",
                "          ",
                "    elif minimum<Lower_Bound:",
                "        return np.where(df[col]<Lower_Bound,Lower_Bound,df[col])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for df1 in [X_train,X_test]:",
                "    df1['Rainfall'] = remove_outliers(df1,'Rainfall',-1.799,2.4)",
                "    df1['WindGustSpeed'] = remove_outliers(df1,'WindGustSpeed',-14.0,91.0)",
                "    df1['WindSpeed9am'] = remove_outliers(df1,'WindSpeed9am',-29.0,55.0)",
                "    df1['WindSpeed3pm'] = remove_outliers(df1,'WindSpeed3pm',-20.0,57.0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#If we look at their boxplots we can see that the outliers are now capped...",
                "plt.figure(figsize=(15,30))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "features_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']",
                "i=1",
                "for col in features_to_examine:",
                "    plt.subplot(6,2,i)",
                "    fig = sns.boxplot(data=X_train,y=col)",
                "    fig.set_xlabel(col)",
                "    fig.set_ylabel('RainTomorrow')",
                "    i+=1"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Describe helps us understand more about the mean and max values",
                "",
                "X_train[features_to_examine].describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_test[features_to_examine].describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Our next step is to encode all the categorical variables.",
                "#first we will convert our target variable",
                "",
                "for df2 in [y_train,y_test]:",
                "    df2['RainTomorrow'] = df2['RainTomorrow'].replace({\"Yes\":1,",
                "                                                    \"No\":0})",
                "",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "import category_encoders as ce",
                "",
                "encoder = ce.BinaryEncoder(cols=['RainToday'])",
                "",
                "X_train = encoder.fit_transform(X_train)",
                "",
                "X_test = encoder.transform(X_test)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Now we will make our training dataset",
                "",
                "X_train = pd.concat([X_train[num],X_train[['RainToday_0','RainToday_1']],",
                "                    pd.get_dummies(X_train['WindGustDir']),",
                "                    pd.get_dummies(X_train['WindDir9am']),",
                "                    pd.get_dummies(X_train['WindDir3pm'])],axis=1)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Same for testing set",
                "",
                "X_test = pd.concat([X_test[num],X_test[['RainToday_0','RainToday_1']],",
                "                    pd.get_dummies(X_test['WindGustDir']),",
                "                    pd.get_dummies(X_test['WindDir9am']),",
                "                    pd.get_dummies(X_test['WindDir3pm'])],axis=1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_test.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "#our training and testing set is ready for our model",
                "#But ,before that we need to bring all the features to same scale with feature scaling",
                "#For this we will use MinMaxScaler",
                "#As there our negative values in our dataset and MinMaxScaler scales our data in range -1 to 1.",
                "",
                "cols = X_train.columns",
                "",
                "from sklearn.preprocessing import MinMaxScaler",
                "",
                "scaler = MinMaxScaler()",
                "X_train = scaler.fit_transform(X_train)",
                "X_test = scaler.transform(X_test)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_train = pd.DataFrame(X_train,columns=cols)",
                "X_test = pd.DataFrame(X_test,columns=cols)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "from sklearn.linear_model import LogisticRegression",
                "",
                "# instantiate the model",
                "logreg = LogisticRegression(solver='liblinear', random_state=0)",
                "",
                "",
                "# fit the model",
                "logreg.fit(X_train, y_train)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "#Prediction on Xtest",
                "",
                "y_pred_test = logreg.predict(X_test)",
                "",
                "y_pred_test"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "#using predict_proba gives the probability value for the target feature",
                "",
                "logreg.predict_proba(X_test)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "#probability of getting no rain (0)",
                "",
                "logreg.predict_proba(X_test)[:,0]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "#probability of getting rain (1)",
                "",
                "logreg.predict_proba(X_test)[:,1]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "#Check accuracy with accuracy_score",
                "",
                "from sklearn.metrics import accuracy_score",
                "",
                "predict_test = accuracy_score(y_test,y_pred_test)",
                "",
                "print(\"Accuracy of model on test set :: {}\".format(predict_test))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "#Creating confusion matrix",
                "",
                "from sklearn.metrics import confusion_matrix",
                "",
                "confusion_matrix = confusion_matrix(y_test, y_pred_test)",
                "print(confusion_matrix)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "#Classification report",
                "from sklearn.metrics import classification_report",
                "",
                "print(classification_report(y_test, y_pred_test))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "#Comparing train and test accuracy",
                "",
                "y_pred_train = logreg.predict(X_train)",
                "y_pred_train"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "#Check accuracy of our model with train set",
                "",
                "predict_train = accuracy_score(y_train,y_pred_train)",
                "print(\"Accuracy of our model on train set :: {}\".format(predict_train))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Overall Accuracy",
                "",
                "print(\"Accuracy of our model :: {}\".format(logreg.score(X_test,y_test)))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "#C=100",
                "",
                "# instantiate the model",
                "logreg100 = LogisticRegression(solver='liblinear',C=100, random_state=0)",
                "",
                "",
                "# fit the model",
                "logreg100.fit(X_train, y_train)",
                "",
                "#Prediction on Xtest",
                "",
                "y_pred_test = logreg100.predict(X_test)",
                "",
                "y_pred_test"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "predict_test = accuracy_score(y_test,y_pred_test)",
                "",
                "print(\"Accuracy of model on test set :: {}\".format(predict_test))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Overall Accuracy",
                "",
                "print(\"Accuracy of our model :: {}\".format(logreg100.score(X_test,y_test)))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "#Confusion matrix",
                "from sklearn.metrics import confusion_matrix",
                "",
                "confusion_matrix = confusion_matrix(y_test, y_pred_test)",
                "print(confusion_matrix)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Classification report",
                "print(classification_report(y_test, y_pred_test))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "#Let's increase the regularization strength",
                "",
                "#C=0.01",
                "",
                "# instantiate the model",
                "logreg001 = LogisticRegression(solver='liblinear',C=0.01, random_state=0)",
                "",
                "",
                "# fit the model",
                "logreg001.fit(X_train, y_train)",
                "",
                "#Prediction on Xtest",
                "",
                "y_pred_test = logreg001.predict(X_test)",
                "",
                "y_pred_test"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "predict_test = accuracy_score(y_test,y_pred_test)",
                "",
                "print(\"Accuracy of model on test set :: {}\".format(predict_test))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "#Overall Accuracy",
                "",
                "print(\"Accuracy of our model :: {}\".format(logreg001.score(X_test,y_test)))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "# store the predicted probabilities for class 1 - Probability of rain",
                "",
                "y_pred1 = logreg100.predict_proba(X_test)[:, 1]",
                "y_pred0 = logreg100.predict_proba(X_test)[:, 0]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# plot histogram of predicted probabilities",
                "",
                "",
                "# adjust the font size ",
                "plt.rcParams['font.size'] = 12",
                "",
                "",
                "# plot histogram with 10 bins",
                "plt.hist(y_pred1, bins = 10)",
                "plt.hist(y_pred0, bins = 10)",
                "",
                "# set the title of predicted probabilities",
                "plt.title('Histogram of predicted probabilities')",
                "",
                "",
                "# set the x-axis limit",
                "plt.xlim(0,1)",
                "",
                "#Set legend",
                "plt.legend('upper left' , labels = ['Rain','No Rain'])",
                "",
                "# set the title",
                "plt.xlabel('Predicted probabilities')",
                "plt.ylabel('Frequency')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import gc",
                "import warnings",
                "warnings.filterwarnings(\"ignore\")",
                "",
                "import pandas as pd",
                "import numpy as np",
                "from IPython.display import display",
                "from IPython.core.display import HTML",
                "import plotly.plotly as py",
                "from plotly.offline import init_notebook_mode, iplot",
                "init_notebook_mode(connected=True)",
                "import plotly.graph_objs as go",
                "import matplotlib.pyplot as plt",
                "import matplotlib",
                "#matplotlib.rc['font.size'] = 9.0",
                "matplotlib.rc('font', size=20)",
                "matplotlib.rc('axes', titlesize=20)",
                "matplotlib.rc('axes', labelsize=20)",
                "matplotlib.rc('xtick', labelsize=20)",
                "matplotlib.rc('ytick', labelsize=20)",
                "matplotlib.rc('legend', fontsize=20)",
                "matplotlib.rc('figure', titlesize=20)",
                "import seaborn as sns",
                "",
                "%matplotlib inline"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "import subprocess",
                "#from https://stackoverflow.com/questions/845058/how-to-get-line-count-cheaply-in-python , Olafur's answer",
                "def file_len(fname):",
                "    p = subprocess.Popen(['wc', '-l', fname], stdout=subprocess.PIPE, ",
                "                                              stderr=subprocess.PIPE)",
                "    result, err = p.communicate()",
                "    if p.returncode != 0:",
                "        raise IOError(err)",
                "    return int(result.strip().split()[0])",
                "",
                "lines = file_len('../input/data.csv')",
                "print('Number of lines in \"train.csv\" is:', lines)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "skiplines = np.random.choice(np.arange(1, lines), size=lines-1-1000000, replace=False)",
                "skiplines=np.sort(skiplines)",
                "print('lines to skip:', len(skiplines))",
                "",
                "data = pd.read_csv(\"../input/data.csv\", skiprows=skiplines)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "data.sample(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "data.isnull().sum(0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Just a helper module to make visualizations more intuitive",
                "num_to_month={",
                "    1:\"Jan\",",
                "    2:\"Feb\",",
                "    3:\"Mar\",",
                "    4:\"Apr\",",
                "    5:\"May\",",
                "    6:\"June\",",
                "    7:\"July\",",
                "    8:\"Aug\",",
                "    9:\"Sept\",",
                "    10:\"Oct\",",
                "    11:\"Nov\",",
                "    12:\"Dec\"",
                "}",
                "data['month'] = data.month.apply(lambda x: num_to_month[x])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "gc.collect()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "pivot = data.pivot_table(index='year', columns='month', values='day', aggfunc=len)",
                "colors = [\"#8B8B00\", \"#8B7E66\", \"#EE82EE\", \"#00C78C\", ",
                "          \"#00E5EE\", \"#FF6347\", \"#EED2EE\", ",
                "          \"#63B8FF\", \"#00FF7F\", \"#B9D3EE\", ",
                "          \"#836FFF\", \"#7D26CD\"]",
                "pivot.loc[:,['Jan','Feb', 'Mar',",
                "            'Apr','May','June',",
                "            'July','Aug','Sept',",
                "            'Oct','Nov','Dec']].plot.bar(stacked=True, figsize=(20,10), color=colors)",
                "plt.xlabel(\"Years\")",
                "plt.ylabel(\"Ridership\")",
                "plt.legend(loc=10)",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "f, ax = plt.subplots(1,2, figsize=(20,7))",
                "colors = ['#66b3ff','#ff9999']",
                "pie = ax[0].pie(list(data['gender'].value_counts()), ",
                "                   labels=list(data.gender.unique()),",
                "                  autopct='%1.1f%%', shadow=True, startangle=90, colors=colors)",
                "count = sns.countplot(x='usertype', data=data, ax=ax[1], color='g', alpha=0.75)",
                "ax[0].set_title(\"Gender Distribution in Ridership\")",
                "ax[1].set_xlabel(\"Type of Rider\")",
                "ax[1].set_ylabel(\"Ridership\")",
                "ax[1].set_title(\"Type of Customers\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "data.usertype.value_counts()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "station_info = data[['from_station_name','latitude_start','longitude_start']].drop_duplicates(subset='from_station_name')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "station_info.sample(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "lat_list = list(station_info.latitude_start)",
                "lat_list = [str(i) for i in lat_list]",
                "lon_list = list(station_info.longitude_start)",
                "lon_list = [str(i) for i in lon_list]",
                "names = list(station_info.from_station_name)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "display(HTML(\"\"\"",
                "<div>",
                "    <a href=\"https://plot.ly/~sominw/6/?share_key=y6irxkKqSVolnuF0l4w420\" target=\"_blank\" title=\"Chicago Cycle Sharing Stations\" style=\"display: block; text-align: center;\"><img src=\"https://plot.ly/~sominw/6.png?share_key=y6irxkKqSVolnuF0l4w420\" alt=\"Chicago Cycle Sharing Stations\" style=\"max-width: 100%;width: 600px;\"  width=\"600\" onerror=\"this.onerror=null;this.src='https://plot.ly/404.png';\" /></a>",
                "    <script data-plotly=\"sominw:6\" sharekey-plotly=\"y6irxkKqSVolnuF0l4w420\" src=\"https://plot.ly/embed.js\" async></script>",
                "</div>\"\"\"))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "!pip install imageio-ffmpeg"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import imageio\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.animation as animation\n",
                "from skimage.transform import resize\n",
                "from IPython.display import HTML\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "\n",
                " \n",
                "def display(driving ):\n",
                "    fig = plt.figure(figsize=(10, 6))\n",
                "\n",
                "    ims = []\n",
                "    for i in range(len(driving)):\n",
                "        im = plt.imshow(driving[i], animated=True)\n",
                "        plt.axis('off')\n",
                "        ims.append([im])\n",
                "\n",
                "    ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)\n",
                "    plt.close()\n",
                "    return ani\n",
                "    \n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "visualize_data"
            ],
            "content": [
                "reader = imageio.get_reader('../input/digitsinnoise-video/Test.mp4')\n",
                "fps = reader.get_meta_data()['fps']\n",
                "driving_video = []\n",
                "try:\n",
                "    for im in reader:\n",
                "        driving_video.append(im)\n",
                "except RuntimeError:\n",
                "    pass\n",
                "reader.close()\n",
                "\n",
                "driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n",
                "\n",
                "\n",
                "\n",
                "HTML(display(driving_video).to_html5_video())"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import os",
                "import cv2",
                "import numpy as np"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "def DisplayImage(image,title,cols=1):",
                "  ",
                "    import matplotlib.pyplot as plt",
                "    ",
                "    image_no=len(image)+1",
                "    postion=0",
                "",
                "    plt.figure(figsize=(8, 8))",
                "    for i in range(1,image_no):",
                "        postion+=1",
                "        plt.subplot(1,cols,postion),plt.imshow(image[i-1],cmap = 'gray'), plt.title(title[i-1]), plt.axis('off')",
                "       ",
                "        if ( i%cols==0):",
                "            plt.show(),plt.figure(figsize=(8, 8))",
                "            postion=0"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "images=[]",
                "titles=[]",
                "for dirname, _, filenames in os.walk('../input/digits-in-noise/Test'):",
                "    for filename in filenames[0:9]:",
                "       ",
                "        images.append( cv2.imread(dirname+\"/\"+ filename,0))",
                "        titles.append(\"1\")",
                "        ",
                "        ",
                "DisplayImage(images,titles,3)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "!python -m pip install dtw",
                "",
                "import numpy as np",
                "from dtw import dtw"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "",
                " ",
                "s1 = np.array([1,2,3,4,3,2,1,1,1,2]) ",
                "s2 = np.array([0,1,1,2,3,4,3,2,1,1]) ",
                "",
                "",
                "",
                "manhattan_distance = lambda s1,s2: np.abs(s1 - s2)",
                "",
                "d, cost_matrix, acc_cost_matrix, path = dtw( s1,s2, dist=manhattan_distance)",
                "",
                "print(d)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "!pip install dtaidistance",
                "from dtaidistance import dtw",
                "from dtaidistance import dtw_visualisation as dtwvis",
                "import random",
                "import numpy as np"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "",
                "x = np.arange(0, 20, .5)",
                "s1 = np.sin(x)",
                "s2 = np.sin(x - 1)",
                "random.seed(1)",
                "for idx in range(len(s2)):",
                "    if random.random() < 0.05:",
                "        s2[idx] += (random.random() - 0.5) / 2",
                "d, paths = dtw.warping_paths(s1, s2, window=25, psi=2)",
                "best_path = dtw.best_path(paths)",
                "dtwvis.plot_warpingpaths(s1, s2, paths, best_path)",
                ""
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "distance = dtw.distance(s1, s2)",
                "print(distance)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "!pip install pydub"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data",
                "transfer_results"
            ],
            "content": [
                "import os",
                "",
                "from os import path",
                "from pydub import AudioSegment",
                "",
                "",
                " ",
                "",
                "test_set=\"./test_set/\"",
                " ",
                "",
                "os.mkdir(test_set) ",
                " ",
                "",
                "for dirname, _, filenames in os.walk('../input/quran-asr-challenge/test_set'):",
                "    for filename in filenames:",
                "        # files                                                                         ",
                "        src = \"../input/quran-asr-challenge/test_set/\"+filename",
                "        dst = test_set+os.path.splitext(filename)[0]+\".wav\"",
                "        # convert wav to mp3                                                            ",
                "        sound = AudioSegment.from_mp3(src)",
                "        sound = sound.set_frame_rate(8000)",
                "        sound.export(dst, format=\"wav\")",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from Levenshtein import distance",
                "import numpy as np",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "",
                "def get_distance_matrix(str_list):",
                "    \"\"\" Construct a levenshtein distance matrix for a list of strings\"\"\"",
                "    dist_matrix = np.zeros(shape=(len(str_list), len(str_list)))",
                "",
                "    print (\"Starting to build distance matrix. This will iterate from 0 till \", len(str_list) )",
                "    for i in range(0, len(str_list)):",
                "        print (i)",
                "        for j in range(i+1, len(str_list)):",
                "                dist_matrix[i][j] = distance(str_list[i], str_list[j]) ",
                "    for i in range(0, len(str_list)):",
                "        for j in range(0, len(str_list)):",
                "            if i == j:",
                "                dist_matrix[i][j] = 0 ",
                "            elif i > j:",
                "                dist_matrix[i][j] = dist_matrix[j][i]",
                "",
                "    return dist_matrix",
                "",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "str_list = [",
                "    \"part\", \"spartan\"",
                "  ",
                "]",
                "get_distance_matrix(str_list)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import numpy as np",
                "",
                "import pandas as pd",
                "import matplotlib",
                "import matplotlib.pyplot as plt",
                "from matplotlib import image",
                "import seaborn as sns",
                "%matplotlib inline",
                "",
                "",
                "",
                "from scipy.spatial import distance",
                "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding",
                "from keras.models import Sequential",
                "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D",
                "from keras.optimizers import RMSprop",
                "from tensorflow.keras.utils  import plot_model, model_to_dot",
                "from keras.preprocessing.image import ImageDataGenerator",
                "from keras.callbacks import EarlyStopping,LearningRateScheduler,ReduceLROnPlateau",
                "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "DSPATH=\"../input/olivetti-faces/\"",
                "X = np.load(DSPATH+\"olivetti_faces.npy\")",
                "y = np.load(DSPATH+\"olivetti_faces_target.npy\")",
                "",
                " ",
                "ThiefImage={}",
                "ThiefImage[\"False\"]= image.imread(\"../input/thief-images/False.jpg\")",
                "ThiefImage[\"True\"]=image.imread(\"../input/thief-images/True.jpg\")",
                " ",
                "",
                "",
                "",
                " "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ClASSES=np.unique(y)",
                "# N_CLASSES=len(np.unique(labels))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Split into train/val",
                "",
                "StratifiedSplit = StratifiedShuffleSplit( test_size=0.4, random_state=0)",
                "StratifiedSplit.get_n_splits(X, y)",
                "for train_index, test_index in StratifiedSplit.split(X, y):",
                "    X_train, X_test, y_train, y_test= X[train_index], X[test_index], y[train_index], y[test_index]",
                "    ",
                "# X_train, X_test, y_train, y_test = train_test_split(    ",
                "#     X, y, test_size=.40, random_state=42)",
                " "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "import seaborn as sns",
                "g = sns.countplot(y_train)",
                "",
                ""
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# calculate class mean ",
                "class_mean=[]",
                "for i in range(len(X_train)):",
                "    class_mean.append(X_train[y_train==i].mean(axis = 0))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#Show Data",
                "",
                "def ShowTrainingData(showNclasses=5):",
                "    if showNclasses>=40:",
                "        showNclasses=ClASSES",
                "    rows,cols=2,4",
                "    ",
                "    for i in range(showNclasses+1):",
                "        fig,ax =  plt.subplots(rows,cols )",
                "        j=0",
                "        for face in X_train[y_train==i]:",
                "            j+=1",
                "            if j==cols:",
                "                j=5",
                "            ax=plt.subplot(rows,cols,j)",
                "            ax.imshow(face ,'gray' )",
                "",
                "        ax = plt.subplot(1,cols,cols)",
                "       ",
                "        ax.imshow( class_mean[i], 'gray' )",
                "        plt.xlabel(\"Class \"+str(i)+\" mean \" )",
                "        fig.tight_layout(pad=1.0)",
                "        plt.show()",
                "    "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#Show Data",
                "",
                "def ShowTrainingData2(showNclasses):",
                "    if showNclasses>=40:",
                "        showNclasses=ClASSES",
                "    rows,cols=2,4",
                "    ",
                "    for i in range(showNclasses+1):",
                "        fig = plt.figure(figsize=(8, 4))",
                "    ",
                "        j=0",
                "        for face in X_train[y_train==i]:",
                "            j+=1",
                "            if j==cols:",
                "                j=5",
                "            fig.add_subplot(rows, cols, j)",
                "            plt.imshow(face, cmap = plt.get_cmap('gray'))",
                " ",
                "            plt.axis('off')",
                "",
                "        ",
                "        fig.add_subplot(1,cols,cols)",
                "        plt.imshow(class_mean[i], cmap = plt.get_cmap('gray'))",
                "        plt.title(\"class_mean {}\".format(i), fontsize=16)",
                "        plt.axis('off')",
                " #         fig.tight_layout(pad=1.0)",
                "",
                "",
                "        plt.suptitle(\"There are 6 image for class {}\".format(i), fontsize=15)",
                "        plt.show()",
                "",
                "    "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def ShowPredictions(predic_Model,ShowNPredictions=5):",
                "    ",
                "    if ShowNPredictions>=len(y_predictions):",
                "        ShowNPredictions=len(y_predictions)",
                "    rows,cols=1,3",
                " ",
                "    for index, row in y_predictions.iterrows():",
                "        if (index>ShowNPredictions):",
                "            break",
                "            ",
                "        x=int(row[\"x\"])",
                "        actually=int(row[\"actually\"])",
                "        y_predic=int(row[predic_Model+\"_predic\"])",
                "        IsTrue=str(row[predic_Model+\"_True\"])",
                "",
                "        fig,ax =  plt.subplots(rows,cols )",
                "        j=1",
                "        ax=plt.subplot(rows,cols,j)",
                "        ax.imshow(X_test[x] ,'gray' )",
                "        plt.xlabel(\"Test Number :\"+str(x)  )",
                "",
                "        j=2",
                "        ax=plt.subplot(rows,cols,j)",
                "        ax.imshow(class_mean[y_predic] ,'gray' )",
                "        plt.xlabel(\"Class \"+str(y_predic)+\" mean \" )",
                "",
                "        j=3",
                "        ax=plt.subplot(rows,cols,j)",
                "        ax.imshow(ThiefImage[IsTrue] ,'gray' )",
                "        plt.xlabel(\"Class \"+str(actually)+\" mean \" )",
                "",
                "",
                "",
                "        fig.tight_layout(pad=2.0)",
                "        plt.show()   ",
                "   "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ShowTrainingData2(5)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "distanceTable=np.array([(i,y_test[i],c,distance.euclidean(X_test[i].flatten() , class_mean[c].flatten() )) for c in ClASSES  for i in range(len(X_test))])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "distanceTable"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "distanceTable=distanceTable.T",
                "# distanceTable.shape=(4,6400)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                " ",
                "d = {'x': distanceTable[0], 'actually':distanceTable[1],'KNN_predic':distanceTable[2],'distance':distanceTable[3]}",
                "df= pd.DataFrame(data=d)",
                "df.head()",
                ""
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[df.x==0]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "y_predictions=pd.merge(df ,df.groupby([\"x\",\"actually\"]).distance.min(), how = 'inner',  on=[\"x\",\"actually\",\"distance\"])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "y_predictions[\"KNN_True\"]=y_predictions[\"KNN_predic\"]==y_predictions[\"actually\"]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "correct_predictions = np.nonzero(y_predictions[\"KNN_True\"].values==1)[0]",
                "incorrect_predictions = np.nonzero(y_predictions[\"KNN_True\"].values==0)[0]",
                "print(len(correct_predictions),\" classified correctly\")",
                "print(len(incorrect_predictions),\" classified incorrectly\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(\"KNN_predic\")",
                "print(\"=============\")",
                "",
                "ShowPredictions(\"KNN\",5)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "X_train = X_train.reshape(-1,64,64,1)",
                "X_test = X_test.reshape(-1,64,64,1)",
                " ",
                "",
                "print(\"X_train shape: \",X_train.shape,\"y_train shape: \",y_train.shape)",
                "print(\"x_test shape: \", X_test.shape,\"y_test shape: \",y_test.shape)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "model = Sequential()",
                "",
                "model.add(Conv2D(filters = 20, kernel_size = (5,5),padding = 'Same', ",
                "                 activation ='relu', input_shape = (64,64,1)))",
                "",
                "model.add(MaxPool2D(pool_size=(2,2)))",
                "model.add(Dropout(0.25))",
                "",
                "model.add(Conv2D(filters = 50, kernel_size = (6,6),padding = 'Same', ",
                "                 activation ='relu'))",
                "",
                "model.add(MaxPool2D(pool_size=(2,2)))",
                "model.add(Dropout(0.25))",
                "",
                "model.add(Conv2D(filters = 150, kernel_size = (5,5),padding = 'Same', ",
                "                 activation ='relu', input_shape = (64,64,1)))",
                "",
                "model.add(Flatten())",
                "model.add(Dense(256, activation = \"relu\"))",
                "model.add(Dropout(0.5))",
                "model.add(Dense(40, activation = \"softmax\"))",
                "",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', ",
                "                                            patience=3, ",
                "                                            verbose=1, ",
                "                                            factor=0.7, ",
                "                                            min_lr=0.00000000001)",
                "early_stopping_monitor = EarlyStopping(patience=2)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "model.summary()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data",
                "transfer_results"
            ],
            "content": [
                "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)",
                "model.compile(optimizer = optimizer , loss='sparse_categorical_crossentropy',",
                "            metrics=['sparse_categorical_accuracy'])",
                "epoch = 37",
                "batch_size = 20",
                "",
                "datagen = ImageDataGenerator(",
                "        featurewise_center=False,  # set input mean to 0 over the dataset",
                "        samplewise_center=False,  # set each sample mean to 0",
                "        featurewise_std_normalization=False,  # divide inputs by std of the dataset",
                "        samplewise_std_normalization=False,  # divide each input by its std",
                "        zca_whitening=False,  # apply ZCA whitening",
                "        rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)",
                "        zoom_range = 0.05, # Randomly zoom image ",
                "        width_shift_range=0,  # randomly shift images horizontally (fraction of total width)",
                "        height_shift_range=0,  # randomly shift images vertically (fraction of total height)",
                "        horizontal_flip=False,  # randomly flip images",
                "        vertical_flip=False)  # randomly flip images",
                "datagen.fit(X_train)",
                "",
                "history = model.fit_generator(",
                "                              datagen.flow(X_train,y_train, batch_size=batch_size),",
                "                              epochs = epoch, ",
                "                              validation_data = (X_test,y_test),",
                "                              verbose = 2, ",
                "                              steps_per_epoch=X_train.shape[0] // batch_size,",
                "                              callbacks=[learning_rate_reduction]",
                "                             )"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "print(history.history.keys())",
                "# summarize history for accuracy",
                "plt.plot(history.history['sparse_categorical_accuracy'])",
                "plt.plot(history.history['val_sparse_categorical_accuracy'])",
                "plt.title('model accuracy')",
                "plt.ylabel('accuracy')",
                "plt.xlabel('epoch')",
                "plt.legend(['train', 'test'], loc='upper left')",
                "plt.show()",
                "# summarize history for loss",
                "plt.plot(history.history['loss'])",
                "plt.plot(history.history['val_loss'])",
                "plt.title('model loss')",
                "plt.ylabel('loss')",
                "plt.xlabel('epoch')",
                "plt.legend(['train', 'test'], loc='upper left')",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "print(\"CNN_predic\")",
                "print(\"=============\")",
                "",
                "score = model.evaluate(X_test,y_test,batch_size=32)",
                "print(score)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "CNN_predic=model.predict_classes(X_test)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_test = X_test.reshape(-1,64,64)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "del y_predictions",
                "",
                "distanceTable=np.array([(i,y_test[i],CNN_predic[i] )  for i in range(len(X_test))])",
                "distanceTable=distanceTable.T",
                "d = {'x': distanceTable[0], 'actually':distanceTable[1],'CNN_predic':distanceTable[2] }",
                "y_predictions= pd.DataFrame(data=d)",
                "y_predictions.head()",
                "y_predictions[\"CNN_True\"]=y_predictions[\"CNN_predic\"]==y_predictions[\"actually\"]",
                "",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y_predictions.head(100)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "correct_predictions = np.nonzero(y_predictions[\"CNN_True\"].values==1)[0]",
                "incorrect_predictions = np.nonzero(y_predictions[\"CNN_True\"].values==0)[0]",
                "print(len(correct_predictions),\" classified correctly\")",
                "print(len(incorrect_predictions),\" classified incorrectly\")",
                ""
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "",
                "",
                "ShowPredictions(\"CNN\")",
                "",
                "",
                ""
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import cv2 ",
                "import matplotlib.pyplot as plt",
                "from matplotlib.patches import Rectangle "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "face_cascade = cv2.CascadeClassifier('../input/opencv-haarcascade/data/haarcascades/haarcascade_frontalface_default.xml')",
                "gray =cv2.imread( \"../input/opencv-samples-images/data/lena.jpg\",0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10, 10))",
                "faces = face_cascade.detectMultiScale(gray, 1.3, 5)",
                "for (x,y,w,h) in faces:",
                "    ax = plt.gca()",
                "    ax.add_patch( Rectangle((x,y), ",
                "                       w,   h,",
                "                        fc ='none',  ",
                "                        ec ='b', ",
                "                        lw = 4) ) ",
                "",
                "plt.imshow(gray,cmap = 'gray')",
                "plt.title('template'), plt.xticks([]), plt.yticks([])",
                "",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import cv2 as cv",
                "import numpy as np",
                "import matplotlib.pyplot as plt",
                "import matplotlib.image as mpimg",
                "from matplotlib.patches import Rectangle "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "content": [
                "plt.figure(figsize=(20, 20))",
                "plt.title(\"Original\")",
                "plt.imshow(mpimg.imread('../input/opencv-samples-images/WaldoBeach.jpg'))",
                "plt.show()",
                ""
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "content": [
                "",
                "",
                "img = cv.imread('../input/opencv-samples-images/WaldoBeach.jpg',0)",
                "",
                "template =img[500:650, 500:600]",
                "# template =img[500:650, 200:300]",
                "plt.imshow(template,cmap = 'gray')",
                "plt.title('template'), plt.xticks([]), plt.yticks([])",
                "",
                "plt.show()",
                ""
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "content": [
                "img2 = img.copy()",
                "w, h = template.shape[::-1]",
                "# All the 6 methods for comparison in a list",
                "methods = ['cv.TM_CCOEFF', 'cv.TM_CCOEFF_NORMED', 'cv.TM_CCORR',",
                "            'cv.TM_CCORR_NORMED', 'cv.TM_SQDIFF', 'cv.TM_SQDIFF_NORMED']",
                "plt.figure(figsize=(20, 20))",
                "plt.imshow(mpimg.imread('../input/opencv-samples-images/WaldoBeach.jpg'))",
                "plt.title('Detected Point')",
                "result=[]",
                "",
                "for meth in methods:",
                "    img = img2.copy()",
                "    method = eval(meth)",
                "    # Apply template Matching",
                "    res = cv.matchTemplate(img,template,method)",
                "    min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)",
                "    # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum",
                "    if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:",
                "        top_left = min_loc",
                "    else:",
                "        top_left = max_loc",
                "",
                "    result+=[(meth,top_left,w,   h)]",
                " ",
                "    ax = plt.gca()",
                "    ax.add_patch( Rectangle(top_left, ",
                "                       w,   h,",
                "                        fc ='none',  ",
                "                        ec ='b', ",
                "                        lw = 4) ) ",
                "",
                "    ",
                "plt.show()",
                "    "
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "index=160",
                "plt.figure(figsize=(20, 20))",
                "for r in result:",
                "    index+=1",
                "    plt.subplot(index),plt.imshow(res,cmap = 'gray')",
                "    template =img[ r[1][1]:r[1][1]+r[3], r[1][0]: r[1][0]+r[2]]",
                "    plt.imshow(template,cmap = 'gray')",
                "    plt.title(r[0]), plt.xticks([]), plt.yticks([])",
                "",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "!pip install pycaret==2.0"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import random",
                "import numpy as np",
                "import pandas as pd",
                "import matplotlib.pyplot as plt",
                "from pycaret.classification import *",
                "from sklearn.model_selection import train_test_split"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "sampleNumber=10000"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "",
                "d = {",
                "    'x1': np.random.randint(0,3,sampleNumber) ,#Make sacrifices",
                "    'x2': np.random.randint(0,3,sampleNumber) ,#Punctuality",
                "    'x3': np.random.randint(0,2,sampleNumber) ,#Not feeling bored while you are together",
                "    'x4': np.random.randint(0,3,sampleNumber) ,#You evaluate gifts",
                "    'x5': np.random.randint(0,2,sampleNumber) ,#Take care of my problems",
                "    'x6': np.random.randint(0,2,sampleNumber) ,#Rai respect ",
                "    'y' : np.zeros(sampleNumber, dtype=bool)",
                "    }",
                "data  = pd.DataFrame(data=d)",
                "",
                "",
                " ",
                "data[\"y\"]=(data['x1'] +   data['x2'] +    data['x3'] +   data['x4'] +   data['x5']+    data['x6'])>4",
                "",
                "",
                "X=data[['x1','x2','x3','x4','x5','x6']]",
                "y=data[[\"y\"]]",
                "",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_train, X_test, y_train, y_test = train_test_split(    ",
                "    X, y, test_size=.20, random_state=42)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train_Data=X_train",
                "train_Data[\"y\"]=y_train",
                "del X_train",
                "del y_train"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "",
                "clf= setup(data = train_Data, target = \"y\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "compare_models()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "content": [
                "lr = create_model('lr')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_model(lr,\"confusion_matrix\")"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "evaluate_model(lr)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "lr_pred = predict_model(lr, data = X_test) #new_data is pd dataframe"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "lr_pred"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "y_test.reset_index(drop=True, inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y_test"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "y_test=y_test.astype(\"int\").values.T"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "correct_predictions = np.nonzero(lr_pred[\"Label\"].values==y_test)[0]",
                "incorrect_predictions = np.nonzero(lr_pred[\"Label\"].values!=y_test)[0]",
                "print(len(correct_predictions),\" classified correctly\")",
                "print(len(incorrect_predictions),\" classified incorrectly\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import random",
                "import re",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "def getpatterns(symbol):",
                "    return \"$$$.{6}|.{3}$$$.{3}|.{6}$$$|$.{2}$.{2}$.{2}|.$.{2}$.{2}$.|.{2}$.{2}$.{2}$|$.{3}$.{3}$|.{2}$.$.$.{2}\".replace(\"$\",symbol)",
                "",
                "def checkPatterns(pattern,TicTecBoard):",
                "    return len(re.findall(pattern,\"\".join(TicTecBoard)))",
                "",
                "def printTicTecBoard(TicTecBoard):",
                "    print(\"\\n\")",
                "    strTicTecBoard=\"\"",
                "    strTicTecBoardLearn=\"\"",
                "    for i in range(0,9):",
                "        strTicTecBoardLearn+= 3*\" \" +  str(i+1) + \" \"*3+\"|\"",
                "        strTicTecBoard+= 3*\" \" + TicTecBoard[i] + \" \"*3+\"|\"",
                "        if (i+1)%3==0:",
                "            strTicTecBoardLearn=strTicTecBoardLearn[0:-1]+\" \"*25+strTicTecBoard[0:-1] +\"\\n\"+\"_\"*25+\" \"*25+\"_\"*20+\"\\n\"",
                "            strTicTecBoard=\"\"",
                "    ",
                "    print(strTicTecBoardLearn[0:-75],\"\\n\")",
                "",
                "def getValidPlace(TicTecBoard):",
                "    return [str(i+1) for i,x in enumerate(TicTecBoard) if x==\"-\"]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "",
                "patternO=getpatterns(\"O\")",
                "patternX=getpatterns(\"X\")",
                "TicTecBoard=[\"-\" for i in range(9)]",
                "player=\"O\"",
                "try:",
                "",
                "",
                "",
                "    while True:    ",
                "        ValidIndexList=getValidPlace(TicTecBoard)",
                "        if player==\"O\":",
                "            while True:  ",
                "                printTicTecBoard(TicTecBoard)",
                "                index=input(\" \\n Enter Cell Number from Valid Index List  \"+str(ValidIndexList )+\" : \\n\")",
                "",
                "                if index in ValidIndexList:",
                "                    index=int(index)-1",
                "                    break",
                "                else:",
                "                    print(\"Plz Enter Valied Place\") ",
                "",
                "        else:",
                "            machineWin=0",
                "            for place in ValidIndexList:",
                "                testTicTecBoard=list(TicTecBoard)",
                "                testIndex=int(place)-1",
                "                testTicTecBoard[testIndex]=player",
                "                if checkPatterns(patternX,testTicTecBoard)>0: ",
                "                    index=testIndex",
                "                    machineWin=1",
                "            if machineWin==0:",
                "                for place in ValidIndexList:",
                "                    testTicTecBoard=list(TicTecBoard)",
                "                    testIndex=int(place)-1",
                "                    testTicTecBoard[testIndex]=\"O\"",
                "                    if checkPatterns(patternO,testTicTecBoard)>0: ",
                "                        index=testIndex",
                "                        machineWin=-1",
                "            if machineWin==0:",
                "                index=4 if \"5\" in ValidIndexList else int(random.choice(ValidIndexList))-1",
                "",
                "",
                "",
                "        TicTecBoard[index]=player",
                "",
                "",
                "        if checkPatterns(getpatterns(player),TicTecBoard)>0: ",
                "            printTicTecBoard(TicTecBoard)",
                "            print('\\x1b[6;30;42m'  +player+\" is Win \"+ '\\x1b[0m')  ",
                "            break",
                "        elif checkPatterns(\"-\",TicTecBoard)==0: ",
                "            printTicTecBoard(TicTecBoard)",
                "            print(\"\\033[93m Game is Draw  \\033[0m\")",
                "            break",
                "",
                "        else:",
                "            player=\"X\" if player==\"O\" else \"O\"",
                "",
                "",
                "",
                "except:",
                "    print(\"Plz Enter Valied Index\")",
                "",
                "",
                "",
                ""
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "from sklearn import preprocessing",
                "import matplotlib.pyplot as plt",
                "import seaborn as sns",
                "",
                "# Input data files are available in the read-only \"../input/\" directory",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                "",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" ",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "#read data files",
                "X=pd.read_csv('/kaggle/input/titanic/train.csv')",
                "test=pd.read_csv('/kaggle/input/titanic/test.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X.head()",
                ""
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.head()",
                ""
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "test['Survived']=np.nan",
                "full=pd.concat([X,test])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "",
                "def data_inv(df):",
                "    print('Number of Persons: ',df.shape[0])",
                "    print('dataset variables: ',df.shape[1])",
                "    print('-'*20)",
                "    print('dateset columns: \\n')",
                "    print(df.columns)",
                "    print('-'*20)",
                "    print('data-type of each column: \\n')",
                "    print(df.dtypes)",
                "    print('-'*20)",
                "    print('missing rows in each column: \\n')",
                "    c=df.isnull().sum()",
                "    print(c[c>0])",
                "    print('-'*20)",
                "    print('Missing vaules %age vise:\\n')",
                "    print((100*(df.isnull().sum()/len(df.index))))",
                "    print('-'*20)",
                "    print('Pictorial Representation:')",
                "    plt.figure(figsize=(8,6))",
                "    sns.heatmap(df.isnull(), yticklabels=False,cbar=False, cmap='viridis')",
                "    plt.show()   ",
                "data_inv(full)#function call"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.heatmap(full.corr(), annot = True)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "#fillna",
                "from statistics import mode",
                "full['Embarked']=full['Embarked'].fillna(mode(full['Embarked'])) ",
                "full['Fare'].fillna(full['Fare'].dropna().median(),inplace=True)",
                "full['Age'] = full.groupby(\"Pclass\")['Age'].transform(lambda x: x.fillna(x.median()))",
                "full.isnull().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "full['Fam']=full['Parch']+full['SibSp']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "full=pd.get_dummies(data=full,columns=['Sex','Embarked'],drop_first=True)",
                "full.info()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "full=full.drop(['Cabin','Ticket','Name','Parch','SibSp'],axis=1)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#Data Standardization ",
                "preprocessing.StandardScaler().fit(full).transform(full.astype(float))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "test = full[full['Survived'].isna()].drop(['Survived'], axis = 1)",
                "train = full[full['Survived'].notna()]",
                "train.info()",
                ""
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "",
                "",
                "",
                "X=train[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']]",
                "",
                "y=train[['Survived']].astype(np.int8)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "Scores=[]",
                "hidden_layer_sizes=[]",
                "",
                "",
                "for cols in range(50,55):",
                "    for rows in range(3,5):",
                "        hidden_layer=(cols,rows)",
                "",
                "        from sklearn.neural_network import MLPClassifier",
                "        MLPClassifierModel = MLPClassifier(activation='logistic', # can be also identity , tanh,logistic , relu",
                "                                           solver='lbfgs',  # can be lbfgs also sgd , adam",
                "                                           alpha=0.1 ,hidden_layer_sizes=hidden_layer,random_state=33)",
                "        MLPClassifierModel.fit(X_train, y_train)",
                "",
                "        MLPClassifier_y_pred = MLPClassifierModel.predict(X_test)",
                "        Scores.append(MLPClassifierModel.score(X_test, y_test))",
                "        hidden_layer_sizes.append(str(hidden_layer))",
                "        ",
                "",
                "",
                ""
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "models = pd.DataFrame({",
                "    'hidden_layer': hidden_layer_sizes,",
                "    'Score': Scores})",
                "models.sort_values(by='Score', ascending=False )",
                "",
                "",
                ""
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                " ",
                "plt.plot(hidden_layer_sizes,Scores)",
                "plt.ylabel('Accuracy ')",
                "plt.xlabel('hidden_layer_sizes ')",
                "plt.tight_layout()",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.neural_network import MLPClassifier",
                "MLPClassifierModel = MLPClassifier(activation='logistic', # can be also identity , tanh,logistic , relu",
                "                                   solver='lbfgs',  # can be lbfgs also sgd , adam",
                "                                   learning_rate='adaptive', # can be constant also invscaling , adaptive",
                "                                   early_stopping= False,",
                "                                   alpha=0.1 ,hidden_layer_sizes=(52, 3),random_state=33)",
                "MLPClassifierModel.fit(X_train, y_train)",
                "",
                "MLPClassifier_y_pred = MLPClassifierModel.predict(X_test)",
                "MLPClassifierModel.fit(X, y)",
                "MLPClassifier_y_pred= MLPClassifierModel.predict(test[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']])"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "transfer_results",
                "validate_data"
            ],
            "content": [
                "Id=test['PassengerId']",
                "sub_df=pd.DataFrame({'PassengerId':Id,'Survived':MLPClassifier_y_pred})",
                "sub_df.to_csv('submission.csv',index=False)",
                "sub_df.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import os",
                "import json"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "",
                "",
                "data = {}",
                "data['people'] = []",
                "data['people'].append({",
                "    'name': 'Scott',",
                "    'website': 'stackabuse.com',",
                "    'from': 'Nebraska'",
                "})",
                "data['people'].append({",
                "    'name': 'Larry',",
                "    'website': 'google.com',",
                "    'from': 'Michigan'",
                "})",
                "data['people'].append({",
                "    'name': 'Tim',",
                "    'website': 'apple.com',",
                "    'from': 'Alabama'",
                "})",
                "",
                "with open('data.json', 'w') as outfile:",
                "    json.dump(data, outfile)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "dataset_name=\"people\"",
                "",
                "API={\"username\":\"tareksherif\",\"key\":\"f4cf963ba526c529b3a9b0ea5058e6f0\"}"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "",
                "os.environ['KAGGLE_USERNAME'] = API[\"username\"]",
                "os.environ['KAGGLE_KEY'] = API[\"key\"]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "data = {",
                "  \"title\": dataset_name,",
                "  \"id\": os.environ['KAGGLE_USERNAME']+\"/\"+dataset_name,",
                "  \"licenses\": [",
                "    {",
                "      \"name\": \"CC0-1.0\"",
                "    }",
                "  ]",
                "}",
                " ",
                "with open('dataset-metadata.json', 'w') as outfile:",
                "    json.dump(data, outfile)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "!kaggle datasets create -p ."
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "from scipy.stats import norm\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.mixture import GaussianMixture\n",
                "import scipy.stats as st\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "%matplotlib inline\n",
                "import missingno as msno"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "TRAIN_DATASET_PATH = '/kaggle/input/realestatepriceprediction/train.csv'\n",
                "\n",
                "df_train = pd.read_csv(TRAIN_DATASET_PATH)\n",
                "\n",
                "df_train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train.dtypes"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train.describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "msno.matrix(df_train.sample(250));"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "total = df_train.isnull().sum().sort_values(ascending=False)\n",
                "percentage = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
                "missing_data = pd.concat([total, percentage], axis=1, keys=['Total', 'Persent'])\n",
                "missing_data.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "corrmat = df_train.corr()\n",
                "f, ax = plt.subplots(figsize=(12, 9))\n",
                "sns.heatmap(corrmat, vmax=.8, annot=True, fmt=' .2f', square=True);"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "k = 10\n",
                "cols = corrmat.nlargest(k, 'Price')['Price'].index\n",
                "sns.set(font_scale=1.5)\n",
                "hm = sns.heatmap(df_train[cols].corr(), annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values);"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.set()\n",
                "cols = ['Price','DistrictId', 'Rooms', 'Square', 'Social_3', 'Floor', 'Helthcare_2', 'Shops_1', 'Healthcare_1']\n",
                "sns.pairplot(df_train[cols], size = 2.8)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "cols = ['Price','DistrictId', 'Rooms', 'Square', 'LifeSquare', 'Social_1', 'Shops_1']\n",
                "df_train_temp = pd.concat([df_train[cols], pd.Series(np.int8(df_train['Rooms'] == 0), name='flag')], axis=1)\n",
                "df_train_temp.loc[df_train_temp['Rooms'] == 0]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.pairplot(df_train_temp, size = 2.5, hue='flag')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize = (45, 10))\n",
                "sns.countplot(x = 'DistrictId', data = df_train)\n",
                "xt = plt.xticks(rotation=90)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "district = df_train['DistrictId'].value_counts()\n",
                "district_gr_50 = district[district > 50]\n",
                "district_ls_50 = district[district <= 50]\n",
                "districts = {'count_districts_gr_50': district_gr_50.count(), 'count_districts_ls_50': district_ls_50.count()}\n",
                "districts_2 = {'pop_districts_gr_50': district_gr_50.sum(), 'pop_districts_ls_50': district_ls_50.sum()}\n",
                "\n",
                "fig, ax = plt.subplots(1, 2)\n",
                "\n",
                "fig.set_size_inches(15, 8)\n",
                "fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
                "\n",
                "#plt.figure(figsize=(12,8))    \n",
                "ax[0].set_title('Districts count')\n",
                "sns.barplot(list(districts.keys()), list(districts.values()), ax=ax[0])\n",
                "\n",
                "ax[1].set_title('Districts pop')\n",
                "sns.barplot(list(districts_2.keys()), list(districts_2.values()), ax=ax[1])\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.distplot(df_train['Square']);"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.distplot(df_train.loc[df_train['Square'] < 200,'Square'])\n",
                "plt.plot([25 for x in range(330)], [x/10000 for x in range(330)], ls='--', c='r')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train.loc[df_train['Square'] < 25, 'Square'].count()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train.loc[df_train['Rooms'] > 5, ['DistrictId', 'Square', 'KitchenSquare', 'Rooms', 'Price']]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_train.loc[df_train['Rooms'] == 0, ['DistrictId', 'Square', 'KitchenSquare', 'Rooms', 'Price']]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(df_train['Square'], df_train['KitchenSquare'])\n",
                "plt.plot([x for x in range(120)], [y for y in range(120)], c = 'r')\n",
                "plt.plot([y for y in range(200)], [ 0 for x in range(200)], c = 'g')\n",
                "plt.xlabel('Square')\n",
                "plt.ylabel('KitchenSquare')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "cond = df_train['KitchenSquare'] < df_train['Square'] + 0.5*df_train['Square'].std()\n",
                "plt.scatter(df_train.loc[cond, 'Square'], df_train.loc[cond, 'KitchenSquare'])\n",
                "plt.plot([x for x in range(120)], [y for y in range(120)], c = 'r')\n",
                "plt.plot([y for y in range(300)], [ 0 for x in range(300)], c = 'g')\n",
                "plt.xlabel('Square')\n",
                "plt.ylabel('KitchenSquare')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "cond = (df_train['KitchenSquare'] >= 3) & (abs(df_train['Square'] - df_train['KitchenSquare']) > 10) &\\\n",
                "                    (df_train['KitchenSquare'] < 50) &  (df_train['Square'] < 200)\n",
                "temp = df_train.loc[cond, ['Square', 'KitchenSquare']]\n",
                "grid = sns.jointplot(temp['Square'], temp['KitchenSquare'], kind='reg')\n",
                "plt.plot(np.arange(0, 40), np.arange(0, 40), color = 'red', linestyle='--')\n",
                "grid.fig.set_figwidth(8)\n",
                "grid.fig.set_figheight(8)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "cond = ~df_train['LifeSquare'].isna()\n",
                "plt.plot([x for x in range(600)], [y for y in range(600)], c = 'r')\n",
                "sns.scatterplot(df_train.loc[cond, 'Square'], df_train.loc[cond, 'LifeSquare']);"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "cond = (~df_train['LifeSquare'].isna()) & (df_train['LifeSquare'] < df_train['LifeSquare'].quantile(q = 0.999)) & \\\n",
                "                                        (df_train['Square'] < df_train['Square'].quantile(q = 0.999))\n",
                "grid = sns.jointplot(df_train.loc[cond, 'Square'], df_train.loc[cond, 'LifeSquare'], kind='reg')\n",
                "plt.plot(np.arange(0, 200), np.arange(0, 200), color = 'red', linestyle='--')\n",
                "grid.fig.set_figwidth(8)\n",
                "grid.fig.set_figheight(8)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train['HouseYear'].unique()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_train.loc[(df_train['HouseYear'] == 20052011) | (df_train['HouseYear'] == 4968), 'HouseYear'].value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "colors = ['g' if i == 0.0 else 'b' for i in df_train.loc[df_train['HouseFloor'] < 60, 'HouseFloor']]\n",
                "colors.count('g')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "\n",
                "plt.scatter(df_train.loc[df_train['HouseFloor'] < 60, 'HouseFloor'], df_train.loc[df_train['HouseFloor'] < 60, 'Floor'], c=colors)\n",
                "\n",
                "plt.plot([x for x in range(40)], [y for y in range(40)], c = 'r')\n",
                "plt.plot([x for x in range(40)], [y + 2 for y in range(40)], c = 'black')\n",
                "plt.xlabel('HouseFloor')\n",
                "plt.ylabel('Floor')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.scatterplot(df_train.loc[~df_train['Healthcare_1'].isna(), 'Healthcare_1'], df_train.loc[~df_train['Healthcare_1'].isna(), 'DistrictId']);"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(35, 8))\n",
                "cond = ~df_train['Healthcare_1'].isna()\n",
                "\n",
                "s = sns.boxplot(df_train.loc[cond, 'DistrictId'], df_train.loc[cond, 'Healthcare_1'])\n",
                "s.set_xticklabels(s.get_xticklabels(), rotation=90)\n",
                "\n",
                "plt.xlabel('DistrictId')\n",
                "plt.ylabel('Healthcare_1')\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train['Price'].describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(\"Skewness: %f\" % df_train['Price'].skew())\n",
                "print(\"Kurtosis: %f\" % df_train['Price'].kurt())"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "y = df_train['Price']\n",
                "\n",
                "fig, ax = plt.subplots(1, 2)\n",
                "\n",
                "fig.set_size_inches(14, 6)\n",
                "fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
                "\n",
                "sns.distplot(y, kde=False, fit=st.norm, ax=ax[0])\n",
                "\n",
                "sns.distplot(pd.Series(np.log(y), name ='LogPrice'), kde=False, fit=st.norm, ax=ax[1])\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "y = df_train['Price']\n",
                "\n",
                "fig, ax = plt.subplots(1, 2)\n",
                "\n",
                "fig.set_size_inches(14, 6)\n",
                "fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
                "\n",
                "sns.distplot(y, kde=False, fit=st.johnsonsu, ax=ax[0])\n",
                "\n",
                "#      \n",
                "params = st.johnsonsu.fit(y)\n",
                "t = (y-params[2])/params[3]\n",
                "y_norm = params[0] + params[1]*np.log(t + np.sqrt(np.power(t, 2) + 1))\n",
                "\n",
                "sns.distplot(pd.Series(y_norm, name ='TransformedToNormalPrice'), fit=st.norm, ax=ax[1])\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "content": [
                "y_back_to_Johnson = params[2]*np.sinh((y_norm - params[0])/params[1]) + params[3]\n",
                "\n",
                "sns.distplot(y_back_to_Johnson, fit=st.johnsonsu)\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "%matplotlib inline\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "from IPython.display import Image \n",
                "pil_img = Image(filename='/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/C/s/color_18_0100.png')\n",
                "\n",
                "display(pil_img)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/C/a/color_0_0002.png"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "#TO_DO\n",
                "#1 prepare the X_train , X_test \n",
                "\n",
                "import os\n",
                "\n",
                "path, dirs, files = next(os.walk(\"/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/b\"))\n",
                "file_count = len(files)\n",
                "\n",
                "file_count"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "for filename in glob.glob(os.path.join(directory_a, '*.png')):\n",
                "    im1 =cv2.imread(filename,0)\n",
                "    print(im1.shape)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from skimage.feature import hog\n",
                "def giveMeFeatures(image):\n",
                "    res = hog(image, orientations=8, pixels_per_cell=(16,16),cells_per_block=(4, 4),block_norm= 'L2')\n",
                "#     =  hog(img, orientations=9, pixels_per_cell=(6, 6),cells_per_block=(2, 2),block_norm='L1', visualize=False,transform_sqrt=False,feature_vector=True)\n",
                "    return res\n",
                "\n",
                "\n",
                "    \n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data"
            ],
            "content": [
                "import glob\n",
                "import cv2\n",
                "directory_a = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/a'\n",
                "directory_b = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/b'\n",
                "\n",
                "X = []\n",
                "y = []\n",
                "\n",
                "for filename in glob.glob(os.path.join(directory_a, '*.png')):\n",
                "    im1 =cv2.imread(filename,0)\n",
                "    im1 = cv2.resize(im1,(64,64))\n",
                "    features = giveMeFeatures(im1)\n",
                "    X.append(features)\n",
                "    y.append(0)\n",
                "\n",
                "for filename in glob.glob(os.path.join(directory_b, '*.png')):\n",
                "    im1 =cv2.imread(filename,0)\n",
                "    im1 = cv2.resize(im1,(64,64))\n",
                "    features = giveMeFeatures(im1)\n",
                "    X.append(features)\n",
                "    y.append(1)\n",
                "    \n",
                "X = np.array(np.float32(X))\n",
                "y = np.array(np.float32(y))\n",
                "\n",
                "\n",
                "rand = np.random.RandomState(321)\n",
                "shuffle = rand.permutation(len(X))\n",
                "X = X[shuffle]\n",
                "y = y[shuffle]\n",
                "    \n",
                "    \n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "#With Hyper Parameters Tuning\n",
                "#2-3,SVM\n",
                "#importing modules\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn import svm\n",
                "#making the instance\n",
                "model=svm.SVC()\n",
                "#Hyper Parameters Set\n",
                "params = {'C': [2,3,4,5,6,7,8,9,10,11,12], \n",
                "          'kernel': ['rbf']}\n",
                "#Making models with hyper parameters sets\n",
                "model1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n",
                "#Learning\n",
                "model1.fit(X_train,y_train)\n",
                "#The best hyper parameters set\n",
                "print(\"Best Hyper Parameters:\\n\",model1.best_params_)\n",
                "#Prediction\n",
                "prediction=model1.predict(X_test)\n"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import classification_report,accuracy_score\n",
                "classification_report(y_test, prediction)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(\"Accuracy: \"+str(accuracy_score(y_test, prediction)))\n",
                "# model1.score(X_test,y_test)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "def testModel(path):\n",
                "    im1 =cv2.imread(path,0)\n",
                "    im1 = cv2.resize(im1,(64,64))\n",
                "    features=[]\n",
                "    features.append(giveMeFeatures(im1))\n",
                "    features = np.array(np.float32(features))    \n",
                "    res =model1.predict(features)\n",
                "    if(res[0]==0):\n",
                "        return 'fist'\n",
                "    else:\n",
                "        return 'palm'\n",
                "        \n",
                "    return \n",
                "    "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "testModel('/kaggle/input/testdata2/palm2.jpg')"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "testModel('/kaggle/input/testdata2/palm1.jpg')"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "import matplotlib.pyplot as plt \n",
                "import cv2 as cv\n",
                "\n",
                "from keras.layers import Conv2D, Input, LeakyReLU, Dense, Activation, Flatten, Dropout, MaxPool2D\n",
                "from keras import models\n",
                "from keras.optimizers import Adam,RMSprop \n",
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "from keras.callbacks import ReduceLROnPlateau\n",
                "\n",
                "import pickle\n",
                "\n",
                "%matplotlib inline"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "model = models.Sequential()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\n",
                "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
                "model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
                "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
                "model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
                "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(units = 512 , activation = 'relu'))\n",
                "model.add(Dropout(0.2))\n",
                "model.add(Dense(units = 2 , activation = 'softmax'))\n",
                "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
                "model.summary()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "initial_lr = 0.001\n",
                "loss = \"sparse_categorical_crossentropy\"\n",
                "model.compile(Adam(lr=initial_lr), loss=loss ,metrics=['accuracy'])\n",
                "model.summary()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "import glob\n",
                "import cv2\n",
                "directory_a = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/a'\n",
                "directory_b = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/b'\n",
                "directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'\n",
                "directory_s = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/s'\n",
                "\n",
                "ADirectories = []\n",
                "BDirectories = []\n",
                "\n",
                "ADirectories.append(directory_a)\n",
                "ADirectories.append(directory_s)\n",
                "BDirectories.append(directory_b)\n",
                "BDirectories.append(directory_5)\n",
                "\n",
                "\n",
                "X = []\n",
                "y = []\n",
                "types = ['*.png', '*.jpg']\n",
                "countA=0\n",
                "countB=0\n",
                "for typ in types:\n",
                "    for directory in ADirectories:\n",
                "        for filename in glob.glob(os.path.join(directory, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(28,28))\n",
                "            X.append(im1)\n",
                "            y.append([1,0])\n",
                "            countA+=1\n",
                "    for directory in BDirectories:\n",
                "        for filename in glob.glob(os.path.join(directory, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(28,28))\n",
                "            X.append(im1)\n",
                "            y.append([0,1])\n",
                "            countB+=1\n",
                "print('A: ', countA)\n",
                "print('B: ', countB)\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X = np.asarray(X)\n",
                "y = np.asarray(y)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X=X/255"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# y = y.reshape(4527,1)\n",
                "X = X.reshape(4527, 28, 28,1)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y[2].shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.preprocessing import LabelBinarizer\n",
                "label_binarizer = LabelBinarizer()\n",
                "y_train = label_binarizer.fit_transform(y_train)\n",
                "y_test = label_binarizer.fit_transform(y_test)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "process_data"
            ],
            "content": [
                "# With data augmentation to prevent overfitting\n",
                "\n",
                "datagen = ImageDataGenerator(\n",
                "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
                "        samplewise_center=False,  # set each sample mean to 0\n",
                "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
                "        samplewise_std_normalization=False,  # divide each input by its std\n",
                "        zca_whitening=False,  # apply ZCA whitening\n",
                "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
                "        zoom_range = 0.1, # Randomly zoom image \n",
                "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
                "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
                "        horizontal_flip=False,  # randomly flip images\n",
                "        vertical_flip=False)  # randomly flip images\n",
                "\n",
                "\n",
                "datagen.fit(X_train)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "epochs = 20\n",
                "batch_size = 256\n",
                "# history_1 = model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=[X_test,y_test])\n",
                "history_1 = model.fit(datagen.flow(X_train,y_train, batch_size = 128) ,epochs = 20 , validation_data = (X_test, y_test))"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Diffining Figure\n",
                "f = plt.figure(figsize=(20,7))\n",
                "\n",
                "#Adding Subplot 1 (For Accuracy)\n",
                "f.add_subplot(121)\n",
                "\n",
                "plt.plot(history_1.epoch,history_1.history['accuracy'],label = \"accuracy\") # Accuracy curve for training set\n",
                "plt.plot(history_1.epoch,history_1.history['val_accuracy'],label = \"val_accuracy\") # Accuracy curve for validation set\n",
                "\n",
                "plt.title(\"Accuracy Curve\",fontsize=18)\n",
                "plt.xlabel(\"Epochs\",fontsize=15)\n",
                "plt.ylabel(\"Accuracy\",fontsize=15)\n",
                "plt.grid(alpha=0.3)\n",
                "plt.legend()\n",
                "\n",
                "#Adding Subplot 1 (For Loss)\n",
                "f.add_subplot(122)\n",
                "\n",
                "plt.plot(history_1.epoch,history_1.history['loss'],label=\"loss\") # Loss curve for training set\n",
                "plt.plot(history_1.epoch,history_1.history['val_loss'],label=\"val_loss\") # Loss curve for validation set\n",
                "\n",
                "plt.title(\"Loss Curve\",fontsize=18)\n",
                "plt.xlabel(\"Epochs\",fontsize=15)\n",
                "plt.ylabel(\"Loss\",fontsize=15)\n",
                "plt.grid(alpha=0.3)\n",
                "plt.legend()\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "def testCNNModel(path,model):\n",
                "    im1 =cv2.imread(path,0)\n",
                "    im1 = cv2.resize(im1,(28,28))\n",
                "    t = []\n",
                "    t.append(im1.reshape(28,28))\n",
                "    t = np.asarray(t)\n",
                "    t = t.reshape(1,28,28,1)\n",
                "    res =model.predict(t)\n",
                "    return res\n",
                "        \n",
                "    return "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "testCNNModel('/kaggle/input/testdata2/fist1.jpg',model)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "label_binarizer.transform(np.asarray([0]))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data"
            ],
            "content": [
                "import glob\n",
                "import cv2\n",
                "directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'\n",
                "directory_2 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/2'\n",
                "directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "X = []\n",
                "y = []\n",
                "types = ['*.png', '*.jpg']\n",
                "for typ in types:\n",
                "        for filename in glob.glob(os.path.join(directory_2, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(0)\n",
                "        for filename in glob.glob(os.path.join(directory_5, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(1)\n",
                "        for filename in glob.glob(os.path.join(directory_unk, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(2)\n",
                "            \n",
                "\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X = np.asarray(X)\n",
                "y = np.asarray(y)\n",
                "\n",
                "\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "X = X.reshape(-1,64,64,1)\n",
                "X.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "IMG_SIZE=64\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.datasets import cifar10\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
                "from tensorflow.keras.callbacks import TensorBoard\n",
                "import time\n",
                "\n",
                "import pickle\n",
                "\n",
                "NAME = \"Numbers-CNN-Model-{}\".format(str(time.ctime())) # Model Name\n",
                "\n",
                "\n",
                "X = X/255.0\n",
                "\n",
                "model = Sequential()\n",
                "\n",
                "model.add(Conv2D(16, (2,2), input_shape=X.shape[1:], activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
                "model.add(Conv2D(32, (3,3), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))\n",
                "model.add(Conv2D(64, (5,5), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(128, activation='relu'))\n",
                "model.add(Dropout(0.2))\n",
                "model.add(Dense(3, activation='softmax')) # size must be equal to number of classes i.e. 11\n",
                "\n",
                "tensorboard = TensorBoard(log_dir=\"/kaggle/working/logs/{}\".format(NAME))\n",
                "\n",
                "model.compile(loss='sparse_categorical_crossentropy',\n",
                "              optimizer='adam',\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[tensorboard])"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "def testCNNModel(path,model):\n",
                "    im1 =cv2.imread(path,0)\n",
                "    im1 = cv2.resize(im1,(64,64))\n",
                "    t = []\n",
                "    t.append(im1.reshape(64,64))\n",
                "    t = np.asarray(t)\n",
                "    t = t.reshape(1,64,64,1)\n",
                "    res =model.predict(t)\n",
                "    return res\n",
                "        \n",
                "    return "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "testCNNModel('/kaggle/input/testdata2/two14.jpg',model)"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data"
            ],
            "content": [
                "import glob\n",
                "import cv2\n",
                "directory_1 = '/kaggle/input/3shapesdataset/resized/1'\n",
                "directory_2 = '/kaggle/input/3shapesdataset/resized/2'\n",
                "directory_3 = '/kaggle/input/3shapesdataset/resized/3'\n",
                "#directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "X = []\n",
                "y = []\n",
                "types = ['*.png', '*.jpg']\n",
                "for typ in types:\n",
                "        for filename in glob.glob(os.path.join(directory_1, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(0)\n",
                "        for filename in glob.glob(os.path.join(directory_2, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(1)\n",
                "        for filename in glob.glob(os.path.join(directory_3, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(2)\n",
                "#         for filename in glob.glob(os.path.join(directory_unk, typ)):\n",
                "#             im1 =cv2.imread(filename,0)\n",
                "#             im1 = cv2.resize(im1,(64,64))\n",
                "#             X.append(im1)\n",
                "#             y.append(3)\n",
                "            \n",
                "            \n",
                "\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X = np.asarray(X)\n",
                "y = np.asarray(y)\n",
                "\n",
                "\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "X = X.reshape(-1,64,64,1)\n",
                "X.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "IMG_SIZE=64\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.datasets import cifar10\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
                "from tensorflow.keras.callbacks import TensorBoard\n",
                "import time\n",
                "\n",
                "import pickle\n",
                "\n",
                "NAME = \"Numbers-CNN-Model-{}\".format(str(time.ctime())) # Model Name\n",
                "\n",
                "\n",
                "X = X/255.0\n",
                "\n",
                "model = Sequential()\n",
                "\n",
                "model.add(Conv2D(16, (2,2), input_shape=X.shape[1:], activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
                "model.add(Conv2D(32, (3,3), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))\n",
                "model.add(Conv2D(64, (5,5), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(128, activation='relu'))\n",
                "model.add(Dropout(0.2))\n",
                "model.add(Dense(3, activation='softmax')) # size must be equal to number of classes i.e. 11\n",
                "\n",
                "tensorboard = TensorBoard(log_dir=\"/kaggle/working/logs/{}\".format(NAME))\n",
                "\n",
                "model.compile(loss='sparse_categorical_crossentropy',\n",
                "              optimizer='adam',\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[tensorboard])"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "def testCNNModel(path,model):\n",
                "    im1 =cv2.imread(path,0)\n",
                "    im1 = cv2.resize(im1,(64,64))\n",
                "    t = []\n",
                "    t.append(im1.reshape(64,64))\n",
                "    t = np.asarray(t)\n",
                "    t = t.reshape(1,64,64,1)\n",
                "    res =model.predict(t)\n",
                "    return res\n",
                "        \n",
                "    return "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "testCNNModel('/kaggle/input/testdata2/fist5.jpg',model)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print('asd')\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "import glob\n",
                "import cv2\n",
                "directory_1 = '/kaggle/input/3shapesdatasetunk/All Data/1'\n",
                "directory_2 = '/kaggle/input/3shapesdatasetunk/All Data/2'\n",
                "directory_3 = '/kaggle/input/3shapesdatasetunk/All Data/3'\n",
                "# directory_4 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/4'\n",
                "# directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'\n",
                "directory_unk = '/kaggle/input/3shapesdatasetunk/All Data/unknown'\n",
                "#directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "X = []\n",
                "y = []\n",
                "types = ['*.png', '*.jpg']\n",
                "for typ in types:\n",
                "        for filename in glob.glob(os.path.join(directory_1, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(0)\n",
                "        print('finished')\n",
                "        for filename in glob.glob(os.path.join(directory_2, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(1)\n",
                "        print('finished')\n",
                "        for filename in glob.glob(os.path.join(directory_3, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(2)\n",
                "        print('finished')\n",
                "        for filename in glob.glob(os.path.join(directory_unk, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(3)\n",
                "        print('finished')\n",
                "            \n",
                "            \n",
                "\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X = np.asarray(X)\n",
                "y = np.asarray(y)\n",
                "\n",
                "\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "X = X.reshape(-1,64,64,1)\n",
                "X.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "IMG_SIZE=64\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.datasets import cifar10\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
                "from tensorflow.keras.callbacks import TensorBoard\n",
                "import time\n",
                "\n",
                "import pickle\n",
                "\n",
                "NAME = \"Numbers-CNN-Model-{}\".format(str(time.ctime())) # Model Name\n",
                "\n",
                "\n",
                "X = X/255.0\n",
                "\n",
                "model = Sequential()\n",
                "\n",
                "model.add(Conv2D(16, (2,2), input_shape=X.shape[1:], activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
                "model.add(Conv2D(32, (3,3), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))\n",
                "model.add(Conv2D(64, (5,5), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(128, activation='relu'))\n",
                "model.add(Dropout(0.2))\n",
                "model.add(Dense(4, activation='softmax')) # size must be equal to number of classes i.e. 11\n",
                "\n",
                "tensorboard = TensorBoard(log_dir=\"/kaggle/working/logs/{}\".format(NAME))\n",
                "\n",
                "model.compile(loss='sparse_categorical_crossentropy',\n",
                "              optimizer='adam',\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[tensorboard])"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "def testCNNModel(path,model):\n",
                "    im1 =cv2.imread(path,0)\n",
                "    im1 = cv2.resize(im1,(64,64))\n",
                "    t = []\n",
                "    t.append(im1.reshape(64,64))\n",
                "    t = np.asarray(t)\n",
                "    t = t.reshape(1,64,64,1)\n",
                "    res =model.predict(t)\n",
                "    return res\n",
                "        \n",
                "    return "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "testCNNModel('/kaggle/input/testdata2/two4.jpg',model)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import numpy as np",
                "import matplotlib.pyplot as plt",
                "import random",
                "%matplotlib inline",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "X = range(1000)",
                "#changing our list to numpy array to benifit from numpy's broadcasting",
                "X = np.asarray(X)",
                "w1 = 2",
                "b  = 500",
                "def line_function(X):",
                "    y = w1 * X + b",
                "    return y",
                "y = line_function(X)",
                "",
                "plt.plot(X,y)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "C = range(1000)",
                "C = np.asarray(C)",
                "w1 = 9/2",
                "b  = 32",
                "F = line_function(C)",
                "",
                "fig = plt.figure(figsize=(4,3))",
                "ax = fig.add_subplot(111)",
                "ax.set_title('change of F with respect to C')",
                "# ax.scatter(x=data[:,0],y=data[:,1],label='Data')",
                "plt.plot(C,F)",
                "ax.set_xlabel('Celsius (C)')",
                "ax.set_ylabel('Fahrenheit (F)')",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "X1 = range(1000)",
                "X2 = range(1000)",
                "X1 = np.asarray(X1)",
                "X2 = np.asarray(X2)",
                "w1 = 8",
                "w2 = 6",
                "Ws = [w1,w2]",
                "b  = 500",
                "def hyperplane(Xs):",
                "    y=b",
                "    for (w,x) in zip(Ws,Xs):",
                "        y+=w*x",
                "    return y",
                "ax = plt.axes(projection='3d')",
                "xv, yv = np.meshgrid(X1, X2)",
                "Xs = [xv, yv]",
                "y = hyperplane(Xs)",
                "ax.plot_surface(xv, yv,y);",
                "",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "import pandas as pd",
                "df = pd.read_csv('../input/random-linear-regression/train.csv')",
                "plt.scatter(df.x,df.y,s = 4) "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "import pandas as pd",
                "df = pd.read_csv('../input/random-linear-regression/train.csv')",
                "plt.scatter(df.x,df.y,s = 4) ",
                "X= range(100)",
                "Y= X",
                "plt.plot(X,Y,c='red')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(df.x,df.y,s = 4) ",
                "X= np.asarray((range(100)))",
                "Y= -X",
                "plt.plot(X,Y,c='red')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "#loading data from the data set ",
                "import pandas as pd",
                "df = pd.read_csv('../input/random-linear-regression/train.csv')",
                "df = df.dropna()",
                "plt.scatter(df.x,df.y,s = 4) "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def getXYfromDF(df):",
                "    X = []",
                "    for i in list(df.x):",
                "        if(type(i)!=list):",
                "            i = [i]",
                "        i.append(1)",
                "        X.append(i)",
                "    X = np.asarray(X)",
                "    y = np.asarray(df.y)",
                "    return X,y",
                "def randomWeights(m):",
                "    w= []",
                "    for i in range(m):",
                "        w.append(random.randint(1,9))",
                "    w = np.asarray(w)",
                "    return w",
                "",
                "",
                "X,y = getXYfromDF(df) ",
                "",
                "",
                "n = X.shape[0]",
                "m = X.shape[1]",
                "",
                "w = randomWeights(m)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "#finding the best wights analytically",
                "w = np.dot(np.dot(np.linalg.inv(np.dot((X.T),X)),(X.T)),y) ",
                "w"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotTheLineWithData(X,w):",
                "    plt.scatter(df.x,df.y,s = 4) ",
                "    #this X is to generate test samples",
                "    X=[]",
                "    for i in range(100):",
                "        X.append([i,1])",
                "    X = np.asarray(X)",
                "    predicted_y = np.dot(X,w) ",
                "    plt.plot(X[:,0],predicted_y,c='red')",
                "plotTheLineWithData(X,w)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X,y = getXYfromDF(df) ",
                "w = randomWeights(m)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "def MSE(y,y_predicted):",
                "    return ((y- y_predicted)**2).mean()",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "def gradient_descent(X,y,w,max_iteration=1000,lr=0.00001): ",
                "    w_history  = []",
                "    loss_hostory = []",
                "    for iteration in range(max_iteration):",
                "        predicted_y = np.dot(X,w)",
                "        loss =  MSE(y,predicted_y)",
                "        loss = round(loss,9)",
                "        w_history.append(w)",
                "        loss_hostory.append(loss)",
                "        derivative = -(2/y.shape[0])* X.dot(loss).sum()",
                "        w = w + lr * derivative",
                "    return w_history,loss_hostory",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "w_history,loss_hostory = gradient_descent(X,y,w,lr = 0.0000001)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "perfect_i = loss_hostory.index(min(loss_hostory)) ",
                "perfect_w = w_history[perfect_i]",
                "w= perfect_w"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotTheLineWithData(X,w)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')",
                "plt.scatter(df.X1,df.X2,c= df.Y)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')",
                "plt.scatter(df.X1,df.X2,c= df.Y)",
                "X = range(1000)",
                "y= X",
                "plt.plot(X,y,c='red')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "X = np.asarray(range(1000))",
                "y = -(2/3)*X ",
                "plt.plot(X,y)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "X = np.asarray(range(1000))",
                "y = -(2/3)*X ",
                "plt.plot(X,y)",
                "plt.plot(800,-400,'+',c='green')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "X = np.asarray(range(1000))",
                "y = -(2/3)*X ",
                "plt.plot(X,y)",
                "plt.plot(400,-500,'_',c='red')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def getXYfromDF(df):",
                "    X = []",
                "    for i in df[['X1','X2']].values.tolist():",
                "        if(type(i)!=list):",
                "            i = [i]",
                "        i.append(1)",
                "        X.append(i)",
                "    X = np.asarray(X)",
                "    y = np.asarray(df.Y)",
                "    return X,y",
                "def randomWeights(m):",
                "    w= []",
                "    for i in range(m):",
                "        w.append(random.randint(1,9)/100)",
                "    w = np.asarray(w)",
                "    return w",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "X,y = getXYfromDF(df)",
                "w= randomWeights(3)",
                "print(w)",
                "",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "plt.scatter(df.X1,df.X2,c=df.Y)",
                "X12 = np.column_stack((range(1000),np.ones(1000)))",
                "print(X12.shape)",
                "print(w[1:2].shape)",
                "y0 =  -np.divide(np.dot(X12,w[1:3]),w[0])",
                "res = [sub[0] for sub in X12] ",
                "X1 = res",
                "plt.plot(X1,y0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "train_model"
            ],
            "content": [
                "def equales(list1,list2):",
                "    if(len(list1)!=len(list2)):",
                "        return False",
                "    else: ",
                "        for i in range(len(list1)):",
                "            if(list1[i]!=list2[i]):",
                "                return False",
                "    return True",
                "def perceptron(X,y,w,learning_rate = 0.0001,max_iterations= 1000):",
                "    for iteration in range(max_iterations):",
                "        prev_w = w",
                "        for i in range(w.shape[0]):",
                "            if(np.dot(np.dot(X[i],w),y[i]) < 0 and y[i]<0):",
                "                w=w- learning_rate * X[i]",
                "                ",
                "            elif(np.dot(np.dot(X[i],w),y[i]) < 0 and y[i]>0):",
                "                w=w+ learning_rate * X[i]",
                "        if(equales(prev_w,w)):",
                "            print('prev_w == w in ',iteration)",
                "            break",
                "        ",
                "        ",
                "    return w"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "new_w = perceptron(X,y,w,learning_rate=0.000001,max_iterations= 100000)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "w= new_w"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "plt.scatter(df.X1,df.X2,c=df.Y)",
                "X12 = np.column_stack((range(1000),np.ones(1000)))",
                "print(X12.shape)",
                "print(w[1:2].shape)",
                "y0 =  -np.divide(np.dot(X12,w[1:3]),w[0])",
                "res = [sub[0] for sub in X12] ",
                "X1 = res",
                "plt.plot(X1,y0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def softmax(z):",
                "    e_z = np.exp(z)",
                "    return e_z / e_z.sum()",
                "z = (3,12,-5,0,10) ",
                "np.round(softmax(z),1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "#load the data ",
                "df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#this function changes the output from numerical values to one hot vectors",
                "#example:",
                "#[1,-1,1] becomes: ",
                "#[[0,1],[1,0],[0,1]",
                "#this function is not general it's just for the case if the data has -1 and 1 cases only,",
                "#the generalized version will be coded next.",
                "def getOneHot(y):",
                "    newY = []",
                "    for i in range(y.shape[0]):",
                "        if(y[i]==-1):",
                "            newY.append([1,0])",
                "        else:",
                "            newY.append([0,1])",
                "    return np.asarray(newY)",
                "#this function loads the data to X and y vectors",
                "def getXYfromDF(df):",
                "    X = []",
                "    for i in df[['X1','X2']].values.tolist():",
                "        if(type(i)!=list):",
                "            i = [i]",
                "        i.append(1)",
                "        X.append(i)",
                "    X = np.asarray(X)",
                "    y = np.asarray(df.Y)",
                "    return X,y",
                "#this function generates random weights to initailize the weights(+ biases ofcourse)",
                "def randomWeights(m,k):",
                "    w= []",
                "    for i in range(m):",
                "        temp = []",
                "        for j in range(k):",
                "            temp.append(random.randint(1,9))",
                "        w.append(temp)",
                "    w = np.asarray(w)",
                "    return w",
                "",
                "",
                "X,y = getXYfromDF(df) ",
                "y = getOneHot(y)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X.shape"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y.shape"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "n = X.shape[0] #number of data samples",
                "m = X.shape[1] #number of features for each sample ",
                "k = 2 #number of classes",
                "w = randomWeights(m,k)",
                "w=np.asarray(w,'float64')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "w.shape"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "#visualize the data that we are trying to fit, and plotting the current lines with the random weights",
                "plt.scatter(df.X1,df.X2,c=df.Y)",
                "X12 = np.column_stack((range(1000),np.ones(1000)))",
                "y0 =  -np.divide(np.dot(X12,w[1:3]),w[0])",
                "res = [sub[0] for sub in X12] ",
                "X1 = np.asarray(res)",
                "print(X1.shape) ",
                "print(y0.shape)",
                "#plot the first line that represents the first linear model",
                "plt.plot(X1,y0[:,0],c='blue')",
                "#plot the second line that represents the second linear model",
                "plt.plot(X1,y0[:,1],c='red')",
                "plt.show"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#the softmax that we use previously was right, but numerically it wasn't stable",
                "#this 'edited softmax' is more numerically stable ",
                "def softmax(x):",
                "    temp = np.exp(x - np.max(x))  # for numerical stability",
                "    return temp / temp.sum(axis=0)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "content": [
                "EPS = 1e-9",
                "#same as in softmax,the first line in this function just gives numerical stability for cross entropy ",
                "def cross_entropy(y, y_hat):",
                "    y_hat = np.clip(y_hat, EPS, 1-EPS) # for numerical stability",
                "    return -np.sum(y * np.log(y_hat)/n)",
                ""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "history = [] #loss history ",
                "numberOfRounds =1000 # max number of times the optimization algorithm will run",
                "learningRate = 0.1",
                "for _ in range(numberOfRounds):",
                "    z = np.dot(X,w)",
                "    y_hat = []",
                "    for i in range(n):",
                "        y_hat.append(softmax(z[i]))",
                "    y_hat = np.asarray(y_hat)",
                "    history.append(cross_entropy(y,y_hat))",
                "",
                "    for j in range(k):",
                "        deltaTemp=0 ",
                "        #deltaTemp is the loss derivative , and we aggregate it from all n samples (in the simple form of gradient descent,",
                "        #and it works fine in case of offline training and smalle number of samples) ",
                "        for i in range(n):",
                "            deltaTemp += np.dot(X.T,(y-y_hat))",
                "        deltaTemp  = - deltaTemp/n",
                "        deltaTemp = np.asarray(deltaTemp)",
                "        w-=learningRate*deltaTemp"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.plot(history)",
                "plt.title('the change of loss with iterations')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "plt.scatter(df.X1,df.X2,c=df.Y)",
                "X12 = np.column_stack((range(1000),np.ones(1000)))",
                "print(X12.shape)",
                "print(w[1:2].shape)",
                "y0 =  -np.divide(np.dot(X12,w[1:3]),w[0])",
                "res = [sub[0] for sub in X12] ",
                "X1 = res",
                "plt.plot(X1,y0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "from sklearn.datasets import load_digits",
                "digits = load_digits()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X = digits.data",
                "y = digits.target"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X.shape"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "import matplotlib.pyplot as plt",
                "fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')",
                "currNum = 0",
                "for i in range(2):",
                "    for j in range(3):",
                "        img = X[currNum,0:64]",
                "        currNum += 1",
                "        img = np.array(img, dtype='float')",
                "        pixels = img.reshape((8, 8))",
                "        ax[i, j].imshow(pixels, cmap='gray')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#simple functions to add 1 to each sample's vector for the bias",
                "def add_bias(X):",
                "    newX = [] ",
                "    for i in range(X.shape[0]):",
                "        newX.append(np.append(X[i],1))",
                "    return np.asarray(newX)",
                "X = add_bias(X)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X.shape"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#change the form of the target values from a single digit to a onehot so we can apply out algorithm",
                "targets=[0,1,2,3,4,5,6,7,8,9]",
                "def oneHot(y,targets):",
                "    newY = []",
                "    for i in range(y.shape[0]): ",
                "        temp = [] ",
                "        for j in targets:",
                "            if(y[i]==targets[j]):",
                "                temp.append(1)",
                "            else:",
                "                temp.append(0)",
                "        newY.append(temp)",
                "    return np.asarray(newY)",
                "y = oneHot(y,targets)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "n = X.shape[0] #number of data samples",
                "m = X.shape[1] #number of features for each sample ",
                "k = 10 #number of classes",
                "w = randomWeights(m,k)",
                "w=np.asarray(w,'float64')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "w.shape"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "history = []",
                "maxNumOfIterations = 20",
                "for iteration in range(maxNumOfIterations): ",
                "    print('iteration: ',iteration)",
                "    z = np.dot(X,w)",
                "    y_hat = []",
                "    for i in range(n):",
                "        y_hat.append(softmax(z[i]))",
                "    y_hat = np.asarray(y_hat)",
                "    history.append(cross_entropy(y,y_hat))",
                "    for j in range(k):",
                "        deltaTemp=0",
                "        for i in range(n):",
                "            deltaTemp += np.dot(X.T,(y-y_hat))",
                "        deltaTemp  = - deltaTemp/n",
                "        deltaTemp = np.asarray(deltaTemp)",
                "        w-=0.1*deltaTemp"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.plot(history)",
                "plt.title('the change of loss with iterations')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def giveMeValueFromOneHot(y_hat):",
                "    return np.where(y_hat == 1)[0][0]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "def predictDis(x):",
                "    img = np.array(x, dtype='float')",
                "    pixels = x[0:64].reshape((8, 8))",
                "    plt.imshow(pixels, cmap='gray')",
                "    z = np.dot(x,w)",
                "    print (\"the class of this Image is: \",giveMeValueFromOneHot(softmax(z)))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "x = X[0]",
                "predictDis(x)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "x = X[1]",
                "predictDis(x)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "x = X[3]",
                "predictDis(x)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "\"\"\" \n",
                "A Python Module to do automated Exploratory Data Analysis and some light weight data prep.\n",
                "https://github.com/TareqAlKhatib/Lazy-EDA\n",
                "\"\"\"\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
                "    \n",
                "def full_report(df, target_column=None):\n",
                "\t\"\"\"Tries to run every possible report on the provided dataframe\"\"\"\n",
                "\tdisplay(HTML(\"<h1>Lazy EDA Report</h1>\"))\n",
                "\t\n",
                "\tbreakdown_date(df)\n",
                "\tshow_dtypes(df)\n",
                "\tplot_nulls(df)\n",
                "\tplot_long_lat(df, target_column)\n",
                "\tif target_column is not None:\n",
                "\t\tplot_scatter_target(df, target_column)\n",
                "\t\tplot_hist_target(df, target_column)\n",
                "\t\tplot_correlations(df, target_column)            \n",
                "\n",
                "def plot_correlations(df, target_column):\n",
                "\tdisplay(HTML(\"<h2>Column Data Types</h2>\"))\n",
                "\tdisplay(HTML(\"<p>Below is a plot of the correlation coefficients of the dataframe's numeric columns and the target column</p>\"))\n",
                "\t\n",
                "\tnum_df = df.select_dtypes(include=numerics)\n",
                "\tdel(num_df[target_column])\n",
                "\tnum_df.corrwith(df[target_column]).sort_values(ascending=False).plot(\n",
                "\t\tkind='barh', figsize=(12,12), title=\"Correlation Coefficient with Target\")\n",
                "\tplt.show()\n",
                "\t\n",
                "def breakdown_date(df):\n",
                "\t\"\"\"\n",
                "\tCreates new columns in a dataframe representing the components of a date (year, month, day of year, & week day name)\n",
                "\t\"\"\"\n",
                "\tdate_cols = df.dtypes[df.dtypes == 'datetime64[ns]'].index\n",
                "\tdisplay(HTML(\"<h2>Breaking down date columns</h2>\"))\n",
                "\tif len(date_cols) > 0:\n",
                "\t\tdisplay(HTML(\"<p>The following columns will be broken down into year, month, day of year, and weekday columns</p> <ul>\"))\n",
                "\t\t\n",
                "\t\tfor date_column in date_cols:\n",
                "\t\t\tdisplay(HTML(\"<li>{}</li>\".format(date_column)))\n",
                "\t\t\tdf['{}_year'.format(date_column)] = df[date_column].dt.year\n",
                "\t\t\tdf['{}_month'.format(date_column)] = df[date_column].dt.month\n",
                "\t\t\tdf['{}_dayofyear'.format(date_column)] = df[date_column].dt.dayofyear\n",
                "\t\t\tdf['{}_weekday'.format(date_column)] = df[date_column].dt.weekday_name\n",
                "\t\t\n",
                "\t\tdisplay(HTML(\"</ul>\"))\n",
                "\telse:\n",
                "\t\tdisplay(HTML(\"<p>No Date columns found to breakdown.</p>\"))\n",
                "\t\t\n",
                "\treturn df\n",
                "\n",
                "def plot_nulls(df):\n",
                "\t\"\"\"\n",
                "\tDisplays a horizontal bar chart representing the percentage of nulls in each column\n",
                "\t\"\"\"\n",
                "\tdisplay(HTML(\"<h2>Plot Nulls</h2>\"))\n",
                "\t\n",
                "\tnull_percentage = df.isnull().sum()/df.shape[0]*100\n",
                "\tnull_percentage_filtered = null_percentage[null_percentage > 0].sort_values()\n",
                "\t\n",
                "\tif len(null_percentage_filtered) > 0:\n",
                "\t\tdisplay(HTML(\"<p>The plot below shows the percentage of NaNs in each column in the dataframe</p>\"))\n",
                "\t\tnull_percentage_filtered.plot(kind='barh', figsize=(12,12), title=\"Plot Null Percentages\")\n",
                "\t\tplt.show()\n",
                "\t\t\n",
                "\telse:\n",
                "\t\tdisplay(HTML(\"<p>The dataframe does not contain any missing data</p>\"))\n",
                "\treturn null_percentage_filtered\n",
                "\n",
                "def show_dtypes(df):\n",
                "\t\"\"\"Shows the data types of all columns\"\"\"\n",
                "\t\n",
                "\tdisplay(HTML(\"<h2>Column Data Types</h2>\"))\n",
                "\t\n",
                "\t# Saving the old display max\n",
                "\told_max = pd.options.display.max_rows\n",
                "\tpd.options.display.max_rows = len(df.columns)\n",
                "\t\n",
                "\t# Display DTypes\n",
                "\tdtype_df = pd.DataFrame({\"Column Name\": df.dtypes.index,\"DType\": df.dtypes.values})\n",
                "\tdisplay(dtype_df)\n",
                "\t\n",
                "\t# Restoring the old display max\n",
                "\tpd.options.display.max_rows = old_max\n",
                "\t\n",
                "def plot_scatter_target(df, target_column):\n",
                "\t\"\"\"Plots a sorted scatter plot of the values in a numerical target column\"\"\"\n",
                "\tdisplay(HTML(\"<h2>Plot Scatter Target</h2>\"))\n",
                "\tdisplay(HTML(\"<p>Below is a sorted scatter plot of the values in the target column</p>\"))\n",
                "\t\n",
                "\tplt.scatter(range(df[target_column].shape[0]), np.sort(df[target_column].values))\n",
                "\tplt.xlabel('index', fontsize=12)\n",
                "\tplt.ylabel(target_column, fontsize=12)\n",
                "\tplt.show()\n",
                "\n",
                "def plot_hist_target(df, target_column):\n",
                "\tdisplay(HTML(\"<h2>Plot Histogram Target</h2>\"))\n",
                "\tdisplay(HTML(\"<p>Below is a histogram of the values in the target column</p>\"))\n",
                "\t\n",
                "\t# Filter 1st and 99th percentiles\n",
                "\tulimit = np.percentile(df.logerror.values, 99)\n",
                "\tllimit = np.percentile(df.logerror.values, 1)\n",
                "\tdf['tempTarget'] = df[target_column]\n",
                "\tdf['tempTarget'].ix[df['tempTarget']>ulimit] = ulimit\n",
                "\tdf['tempTarget'].ix[df['tempTarget']<llimit] = llimit\n",
                "\t\n",
                "\t# Plot\n",
                "\tplt.figure(figsize=(12,8))\n",
                "\tsns.distplot(df['tempTarget'])\n",
                "\tplt.xlabel(target_column, fontsize=12)\n",
                "\tplt.show()\n",
                "\t\n",
                "\tdel[df['tempTarget']]\n",
                "\t\n",
                "def plot_long_lat(df, target_column):\n",
                "\tif 'latitude' in df.columns.str.lower() and 'longitude' in df.columns.str.lower():\n",
                "\t\tdisplay(HTML(\"<h2>Plot longitude/latitude</h2>\"))\n",
                "\t\tdisplay(HTML(\"<p>Below is a scatter plot of long/lat coordinate in the dataframe</p>\"))\n",
                "\t\t\n",
                "\t\tplt.figure(figsize=(12,12))\n",
                "\t\t\n",
                "\t\tif target_column is None:\n",
                "\t\t\tsns.jointplot(x=df.latitude.values, y=df.longitude.values, size=10)\n",
                "\t\telse:\n",
                "\t\t\tdf['tempTarget'] = (df['logerror'] - df['logerror'].min())/(df['logerror'].max() - df['logerror'].min())\n",
                "\t\t\tplt.scatter(x=df.latitude.values, y=df.longitude.values, c=df['tempTarget'].values)\n",
                "\t\t\tdel(df['tempTarget'])\n",
                "\t\tplt.ylabel('Longitude', fontsize=12)\n",
                "\t\tplt.xlabel('Latitude', fontsize=12)\n",
                "\t\tplt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "train_df = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
                "prop_df = pd.read_csv(\"../input/properties_2016.csv\", dtype={\n",
                "    'hashottuborspa': 'object', \n",
                "    'propertycountylandusecode': 'object',\n",
                "    'propertyzoningdesc': 'object',\n",
                "    'fireplaceflag': 'object',\n",
                "    'taxdelinquencyflag': 'object'\n",
                "})\n",
                "\n",
                "train_df = pd.merge(train_df, prop_df, on='parcelid', how='left')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "full_report(train_df, target_column='logerror')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "import plotly.offline as py\n",
                "py.init_notebook_mode(connected=True)\n",
                "import plotly.graph_objs as go\n",
                "import plotly.tools as tls\n",
                "import seaborn as sns\n",
                "import matplotlib.image as mpimg\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib\n",
                "%matplotlib inline\n",
                "\n",
                "# Import the 3 dimensionality reduction methods\n",
                "from sklearn.manifold import TSNE\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "train = pd.read_csv('../input/train.csv')\n",
                "train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(train.shape)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# save the labels to a Pandas series target\n",
                "target = train['label']\n",
                "# Drop the label feature\n",
                "train = train.drop(\"label\",axis=1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "# Standardize the data\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "X = train.values\n",
                "X_std = StandardScaler().fit_transform(X)\n",
                "\n",
                "# Calculating Eigenvectors and eigenvalues of Cov matirx\n",
                "mean_vec = np.mean(X_std, axis=0)\n",
                "cov_mat = np.cov(X_std.T)\n",
                "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
                "# Create a list of (eigenvalue, eigenvector) tuples\n",
                "eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
                "\n",
                "# Sort the eigenvalue, eigenvector pair from high to low\n",
                "eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
                "\n",
                "# Calculation of Explained Variance from the eigenvalues\n",
                "tot = sum(eig_vals)\n",
                "var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
                "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "trace1 = go.Scatter(\n",
                "    x=list(range(784)),\n",
                "    y= cum_var_exp,\n",
                "    mode='lines+markers',\n",
                "    name=\"'Cumulative Explained Variance'\",\n",
                "#     hoverinfo= cum_var_exp,\n",
                "    line=dict(\n",
                "        shape='spline',\n",
                "        color = 'goldenrod'\n",
                "    )\n",
                ")\n",
                "trace2 = go.Scatter(\n",
                "    x=list(range(784)),\n",
                "    y= var_exp,\n",
                "    mode='lines+markers',\n",
                "    name=\"'Individual Explained Variance'\",\n",
                "#     hoverinfo= var_exp,\n",
                "    line=dict(\n",
                "        shape='linear',\n",
                "        color = 'black'\n",
                "    )\n",
                ")\n",
                "fig = tls.make_subplots(insets=[{'cell': (1,1), 'l': 0.7, 'b': 0.5}],\n",
                "                          print_grid=True)\n",
                "\n",
                "fig.append_trace(trace1, 1, 1)\n",
                "fig.append_trace(trace2,1,1)\n",
                "fig.layout.title = 'Explained Variance plots - Full and Zoomed-in'\n",
                "fig.layout.xaxis = dict(range=[0, 80], title = 'Feature columns')\n",
                "fig.layout.yaxis = dict(range=[0, 60], title = 'Explained Variance')\n",
                "# fig['data'] = []\n",
                "# fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))\n",
                "# fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))\n",
                "\n",
                "# fig['data'] = go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance')]\n",
                "# fig['data'] += [go.Scatter(x=list(range(784)), y=var_exp, xaxis='x2', yaxis='y2',name = 'Individual Explained Variance')]\n",
                "\n",
                "# # fig['data'] = data\n",
                "# # fig['layout'] = layout\n",
                "# # fig['data'] += data2\n",
                "# # fig['layout'] += layout2\n",
                "# py.iplot(fig, filename='inset example')"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Invoke SKlearn's PCA method\n",
                "n_components = 30\n",
                "pca = PCA(n_components=n_components).fit(train.values)\n",
                "\n",
                "eigenvalues = pca.components_.reshape(n_components, 28, 28)\n",
                "\n",
                "# Extracting the PCA components ( eignevalues )\n",
                "#eigenvalues = pca.components_.reshape(n_components, 28, 28)\n",
                "eigenvalues = pca.components_"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "n_row = 4\n",
                "n_col = 7\n",
                "\n",
                "# Plot the first 8 eignenvalues\n",
                "plt.figure(figsize=(13,12))\n",
                "for i in list(range(n_row * n_col)):\n",
                "    offset =0\n",
                "    plt.subplot(n_row, n_col, i + 1)\n",
                "    plt.imshow(eigenvalues[i].reshape(28,28), cmap='jet')\n",
                "    title_text = 'Eigenvalue ' + str(i + 1)\n",
                "    plt.title(title_text, size=6.5)\n",
                "    plt.xticks(())\n",
                "    plt.yticks(())\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# plot some of the numbers\n",
                "plt.figure(figsize=(14,12))\n",
                "for digit_num in range(0,70):\n",
                "    plt.subplot(7,10,digit_num+1)\n",
                "    grid_data = train.iloc[digit_num].as_matrix().reshape(28,28)  # reshape from 1d to 2d pixel array\n",
                "    plt.imshow(grid_data, interpolation = \"none\", cmap = \"afmhot\")\n",
                "    plt.xticks([])\n",
                "    plt.yticks([])\n",
                "plt.tight_layout()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Delete our earlier created X object\n",
                "del X\n",
                "# Taking only the first N rows to speed things up\n",
                "X= train[:6000].values\n",
                "del train\n",
                "# Standardising the values\n",
                "X_std = StandardScaler().fit_transform(X)\n",
                "\n",
                "# Call the PCA method with 5 components. \n",
                "pca = PCA(n_components=5)\n",
                "pca.fit(X_std)\n",
                "X_5d = pca.transform(X_std)\n",
                "\n",
                "# For cluster coloring in our Plotly plots, remember to also restrict the target values \n",
                "Target = target[:6000]"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "trace0 = go.Scatter(\n",
                "    x = X_5d[:,0],\n",
                "    y = X_5d[:,1],\n",
                "#     name = Target,\n",
                "#     hoveron = Target,\n",
                "    mode = 'markers',\n",
                "    text = Target,\n",
                "    showlegend = False,\n",
                "    marker = dict(\n",
                "        size = 8,\n",
                "        color = Target,\n",
                "        colorscale ='Jet',\n",
                "        showscale = False,\n",
                "        line = dict(\n",
                "            width = 2,\n",
                "            color = 'rgb(255, 255, 255)'\n",
                "        ),\n",
                "        opacity = 0.8\n",
                "    )\n",
                ")\n",
                "data = [trace0]\n",
                "\n",
                "layout = go.Layout(\n",
                "    title= 'Principal Component Analysis (PCA)',\n",
                "    hovermode= 'closest',\n",
                "    xaxis= dict(\n",
                "         title= 'First Principal Component',\n",
                "        ticklen= 5,\n",
                "        zeroline= False,\n",
                "        gridwidth= 2,\n",
                "    ),\n",
                "    yaxis=dict(\n",
                "        title= 'Second Principal Component',\n",
                "        ticklen= 5,\n",
                "        gridwidth= 2,\n",
                "    ),\n",
                "    showlegend= True\n",
                ")\n",
                "\n",
                "\n",
                "fig = dict(data=data, layout=layout)\n",
                "py.iplot(fig, filename='styled-scatter')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.cluster import KMeans # KMeans clustering \n",
                "# Set a KMeans clustering with 9 components ( 9 chosen sneakily ;) as hopefully we get back our 9 class labels)\n",
                "kmeans = KMeans(n_clusters=9)\n",
                "# Compute cluster centers and predict cluster indices\n",
                "X_clustered = kmeans.fit_predict(X_5d)\n",
                "\n",
                "trace_Kmeans = go.Scatter(x=X_5d[:, 0], y= X_5d[:, 1], mode=\"markers\",\n",
                "                    showlegend=False,\n",
                "                    marker=dict(\n",
                "                            size=8,\n",
                "                            color = X_clustered,\n",
                "                            colorscale = 'Portland',\n",
                "                            showscale=False, \n",
                "                            line = dict(\n",
                "            width = 2,\n",
                "            color = 'rgb(255, 255, 255)'\n",
                "        )\n",
                "                   ))\n",
                "\n",
                "layout = go.Layout(\n",
                "    title= 'KMeans Clustering',\n",
                "    hovermode= 'closest',\n",
                "    xaxis= dict(\n",
                "         title= 'First Principal Component',\n",
                "        ticklen= 5,\n",
                "        zeroline= False,\n",
                "        gridwidth= 2,\n",
                "    ),\n",
                "    yaxis=dict(\n",
                "        title= 'Second Principal Component',\n",
                "        ticklen= 5,\n",
                "        gridwidth= 2,\n",
                "    ),\n",
                "    showlegend= True\n",
                ")\n",
                "\n",
                "data = [trace_Kmeans]\n",
                "fig1 = dict(data=data, layout= layout)\n",
                "# fig1.append_trace(contour_list)\n",
                "py.iplot(fig1, filename=\"svm\")"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from IPython.display import display, Math, Latex"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "lda = LDA(n_components=5)\n",
                "# Taking in as second argument the Target as labels\n",
                "X_LDA_2D = lda.fit_transform(X_std, Target.values )"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "8# Using the Plotly library again\n",
                "traceLDA = go.Scatter(\n",
                "    x = X_LDA_2D[:,0],\n",
                "    y = X_LDA_2D[:,1],\n",
                "#     name = Target,\n",
                "#     hoveron = Target,\n",
                "    mode = 'markers',\n",
                "    text = Target,\n",
                "    showlegend = True,\n",
                "    marker = dict(\n",
                "        size = 8,\n",
                "        color = Target,\n",
                "        colorscale ='Jet',\n",
                "        showscale = False,\n",
                "        line = dict(\n",
                "            width = 2,\n",
                "            color = 'rgb(255, 255, 255)'\n",
                "        ),\n",
                "        opacity = 0.8\n",
                "    )\n",
                ")\n",
                "data = [traceLDA]\n",
                "\n",
                "layout = go.Layout(\n",
                "    title= 'Linear Discriminant Analysis (LDA)',\n",
                "    hovermode= 'closest',\n",
                "    xaxis= dict(\n",
                "         title= 'First Linear Discriminant',\n",
                "        ticklen= 5,\n",
                "        zeroline= False,\n",
                "        gridwidth= 2,\n",
                "    ),\n",
                "    yaxis=dict(\n",
                "        title= 'Second Linear Discriminant',\n",
                "        ticklen= 5,\n",
                "        gridwidth= 2,\n",
                "    ),\n",
                "    showlegend= False\n",
                ")\n",
                "\n",
                "fig = dict(data=data, layout=layout)\n",
                "py.iplot(fig, filename='styled-scatter')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Invoking the t-SNE method\n",
                "tsne = TSNE(n_components=2)\n",
                "tsne_results = tsne.fit_transform(X_std) "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "traceTSNE = go.Scatter(\n",
                "    x = tsne_results[:,0],\n",
                "    y = tsne_results[:,1],\n",
                "    name = Target,\n",
                "     hoveron = Target,\n",
                "    mode = 'markers',\n",
                "    text = Target,\n",
                "    showlegend = True,\n",
                "    marker = dict(\n",
                "        size = 8,\n",
                "        color = Target,\n",
                "        colorscale ='Jet',\n",
                "        showscale = False,\n",
                "        line = dict(\n",
                "            width = 2,\n",
                "            color = 'rgb(255, 255, 255)'\n",
                "        ),\n",
                "        opacity = 0.8\n",
                "    )\n",
                ")\n",
                "data = [traceTSNE]\n",
                "\n",
                "layout = dict(title = 'TSNE (T-Distributed Stochastic Neighbour Embedding)',\n",
                "              hovermode= 'closest',\n",
                "              yaxis = dict(zeroline = False),\n",
                "              xaxis = dict(zeroline = False),\n",
                "              showlegend= False,\n",
                "\n",
                "             )\n",
                "\n",
                "fig = dict(data=data, layout=layout)\n",
                "py.iplot(fig, filename='styled-scatter')"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "import os\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "import pandas as pd\n",
                "\n",
                "df = pd.read_csv(\n",
                "    filepath_or_buffer='../input/Seed_Data.csv',\n",
                "    header=None,\n",
                "    sep=',')\n",
                "\n",
                "df.columns=['A', 'B', 'C', 'D', 'class']\n",
                "df.dropna(how=\"all\", inplace=True) # drops the empty line at file-end\n",
                "\n",
                "df.tail()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# split data table into data X and class labels y\n",
                "\n",
                "X = df.ix[:,0:4].values\n",
                "y = df.ix[:,4].values\n",
                "\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "from matplotlib import pyplot as plt\n",
                "import numpy as np\n",
                "import math\n",
                "\n",
                "label_dict = {1: 'type0',\n",
                "              2: 'type1',\n",
                "              3: 'type2'}\n",
                "\n",
                "feature_dict = {0: 'A',\n",
                "                1: 'B',\n",
                "                2: 'C',\n",
                "                3: 'D'}\n",
                "\n",
                "with plt.style.context('seaborn-whitegrid'):\n",
                "    plt.figure(figsize=(8, 6))\n",
                "    for cnt in range(4):\n",
                "        plt.subplot(2, 2, cnt+1)\n",
                "        for lab in ('type0', 'type1', 'type2'):\n",
                "            plt.hist(X[y==lab, cnt],\n",
                "                     label=lab,\n",
                "                     bins=10,\n",
                "                     alpha=0.3,)\n",
                "        plt.xlabel(feature_dict[cnt])\n",
                "    plt.legend(loc='upper right', fancybox=True, fontsize=8)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.preprocessing import StandardScaler\n",
                "X_std = StandardScaler().fit_transform(X)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_std"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "import numpy as np\n",
                "mean_vec = np.mean(X_std, axis=0)\n",
                "cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n",
                "print('Covariance matrix \\n%s' %cov_mat)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "cov_mat = np.cov(X_std.T)\n",
                "\n",
                "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
                "\n",
                "print('Eigenvectors \\n%s' %eig_vecs)\n",
                "print('\\nEigenvalues \\n%s' %eig_vals)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Make a list of (eigenvalue, eigenvector) tuples\n",
                "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
                "\n",
                "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
                "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
                "\n",
                "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
                "print('Eigenvalues in descending order:')\n",
                "for i in eig_pairs:\n",
                "    print(i[0])"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "tot = sum(eig_vals)\n",
                "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
                "cum_var_exp = np.cumsum(var_exp)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "with plt.style.context('seaborn-whitegrid'):\n",
                "    plt.figure(figsize=(6, 4))\n",
                "\n",
                "    plt.bar(range(4), var_exp, alpha=0.5, align='center',\n",
                "            label='individual explained variance')\n",
                "    plt.step(range(4), cum_var_exp, where='mid',\n",
                "             label='cumulative explained variance')\n",
                "    plt.ylabel('Explained variance ratio')\n",
                "    plt.xlabel('Principal components')\n",
                "    plt.legend(loc='best')\n",
                "    plt.tight_layout()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1),\n",
                "                      eig_pairs[1][1].reshape(4,1)))\n",
                "\n",
                "print('Matrix W:\\n', matrix_w)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Y = X_std.dot(matrix_w)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "with plt.style.context('seaborn-whitegrid'):\n",
                "    plt.figure(figsize=(6, 4))\n",
                "    for lab, col in zip(('type0', 'type1', 'type2'),\n",
                "                        ('blue', 'red', 'green')):\n",
                "        plt.scatter(Y[y==lab, 0],\n",
                "                    Y[y==lab, 1],\n",
                "                    label=lab,\n",
                "                    c=col)\n",
                "    plt.xlabel('Principal Component 1')\n",
                "    plt.ylabel('Principal Component 2')\n",
                "    plt.legend(loc='lower center')\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "import os\n",
                "import random\n",
                "import math\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "chromosome = []\n",
                "fitval = []\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def generateCromosome():\n",
                "    chromosome.clear()\n",
                "    for i in range(4):\n",
                "        singleChromosome = []\n",
                "        for j in range(6):\n",
                "            singleChromosome.append(random.randint(0,1))\n",
                "        chromosome.append(singleChromosome)\n",
                "\n",
                "    print (\"Generated chromosome = \",chromosome)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def evaluateSolution():\n",
                "    fitval.clear()\n",
                "    for i in range(4):\n",
                "        val = 0\n",
                "        for j in range(1,6):\n",
                "            val += math.pow(2,5-j)*chromosome[i][j]\n",
                "        if chromosome[i][0] == 1:\n",
                "            val = - val\n",
                "        fitval.append(val)\n",
                "    print (\"Fitval = \",fitval)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "def func(x):\n",
                "    return -(x*x)+5"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "def selection():\n",
                "    ftval2 = [[0 for i in range(2)] for j in range(4)]\n",
                "    print (ftval2)\n",
                "    c1=-1\n",
                "    c2=-1\n",
                "    for i in range(4):\n",
                "        ftval2[i][0]=func(fitval[i])\n",
                "        ftval2[i][1]=i\n",
                "        \n",
                "    ftval2=sorted(ftval2,key=lambda l:l[0], reverse=True)\n",
                "    \n",
                "    c1=ftval2[0][1]\n",
                "    c2=ftval2[1][1]\n",
                "    \n",
                "    bstval = ftval2[0][0]\n",
                "    print(\"Bestval = \",bstval)\n",
                "    return c1, c2, bstval\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "def crossover(s1,s2):\n",
                "    select = random.randint(0,5)\n",
                "    \n",
                "    for i in range(select, 6):\n",
                "        chromosome[s1][i],chromosome[s2][i] = chromosome[s2][i],chromosome[s1][i]\n",
                "    print(\"For crossover, Selected = \",select,\", Chromose no = \",c1,c2,\", After Crossover = \",chromosome)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "def mutation():\n",
                "    select = random.randint(1,50)\n",
                "    if select == 30:\n",
                "        select2 = random.randint(0,3)\n",
                "        select3 = random.randint(0,5)\n",
                "        chromosome[select2][select3] = 1 - chromosome[select2][select3]\n",
                "        print(\"Mutation occured at, Chromosome = \",select2,\", Position = \",select3)\n",
                "    "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "generateCromosome()\n",
                "for i in range(1000):\n",
                "    print(\"Iteration\",i+1)\n",
                "    evaluateSolution()\n",
                "    c1,c2,bstval=selection()\n",
                "    if bstval==5.0:\n",
                "        break\n",
                "    crossover(c1,c2)\n",
                "    mutation()\n",
                "\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train_data=pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n",
                "test_data=pd.read_csv('/kaggle/input/digit-recognizer/test.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_data.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# No data for train_data is missing\n",
                "Col_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]\n",
                "print(Col_with_missing)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# No data for test_data is missing\n",
                "Col_with_missing = [col for col in test_data.columns if test_data[col].isnull().any()]\n",
                "print(Col_with_missing)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test_data.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Y_train = train_data[\"label\"]\n",
                "\n",
                "# Drop 'label' column\n",
                "X_train = train_data.drop([\"label\"],axis = 1) "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Y_train.value_counts()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "import seaborn as sns\n",
                "%matplotlib inline\n",
                "sns.countplot(Y_train)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "img_row=28\n",
                "img_col=28\n",
                "def data_prep_X(X):\n",
                "    num_img=len(X)\n",
                "    x_as_array=X.values.reshape(num_img,img_row,img_col,1)\n",
                "    X_out=x_as_array/255\n",
                "    return X_out"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from keras.utils.np_utils import to_categorical\n",
                "num_classes=10\n",
                "def data_prep_Y(Y):\n",
                "    out_y = to_categorical(Y, num_classes)\n",
                "    return out_y"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_train = data_prep_X(X_train)\n",
                "test_data = data_prep_X(test_data)\n",
                "Y_train = data_prep_Y(Y_train)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "# Some examples images\n",
                "import matplotlib.pyplot as plt\n",
                "g = plt.imshow(X_train[0][:,:,0])"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "g = plt.imshow(X_train[1][:,:,0])"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "from tensorflow.python import keras\n",
                "from tensorflow.python.keras.models import Sequential\n",
                "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D\n",
                "\n",
                "model = Sequential()\n",
                "model.add(Conv2D(20, kernel_size=(3, 3),\n",
                "                 activation='relu',\n",
                "                 input_shape=(img_row, img_col, 1)))\n",
                "model.add(Conv2D(20, kernel_size=(3, 3), activation='relu'))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(128, activation='relu'))\n",
                "model.add(Dense(num_classes, activation='softmax'))\n",
                "\n",
                "model.compile(loss=keras.losses.categorical_crossentropy,\n",
                "              optimizer='adam',\n",
                "              metrics=['accuracy'])\n",
                "model.fit(X_train, Y_train,\n",
                "          batch_size=128,\n",
                "          epochs=30,\n",
                "          validation_split = 0.2)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "# predict results\n",
                "results = model.predict(test_data)\n",
                "\n",
                "# select the indix with the maximum probability\n",
                "results = np.argmax(results,axis = 1)\n",
                "\n",
                "results = pd.Series(results,name=\"Label\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
                "\n",
                "submission.to_csv(\"mySubmission.csv\",index=False)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "pip install face_recognition"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import os\n",
                "import cv2\n",
                "import face_recognition"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "from PIL import Image, ImageDraw\n",
                "from IPython.display import display\n",
                "\n",
                "# Sample Image\n",
                "virat_img = Image.open('/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/unknown_faces/Kohli-Williamson.jpg')\n",
                "display(virat_img)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "KNOWN_FACES_DIR = '/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/known_faces'\n",
                "UNKNOWN_FACES_DIR = '/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/unknown_faces'\n",
                "TOLERANCE = 0.6"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print('Loading known faces...')\n",
                "known_faces = []\n",
                "known_names = []"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "for name in os.listdir(KNOWN_FACES_DIR):\n",
                "\n",
                "    # Next we load every file of faces of known person\n",
                "    for filename in os.listdir(f'{KNOWN_FACES_DIR}/{name}'):\n",
                "\n",
                "        # Load an image\n",
                "        image = face_recognition.load_image_file(f'{KNOWN_FACES_DIR}/{name}/{filename}')\n",
                "\n",
                "        # Get 128-dimension face encoding\n",
                "        # Always returns a list of found faces, for this purpose we take first face only (assuming one face per image as you can't be twice on one image)\n",
                "        encoding = face_recognition.face_encodings(image)[0]\n",
                "\n",
                "        # Append encodings and name\n",
                "        known_faces.append(encoding)\n",
                "        known_names.append(name)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(known_names)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "for filename in os.listdir(UNKNOWN_FACES_DIR):\n",
                "\n",
                "    # Load image\n",
                "    print(f'Filename {filename}', end='')\n",
                "    unknown_image = face_recognition.load_image_file(f'{UNKNOWN_FACES_DIR}/{filename}')\n",
                "    # Load an image with an unknown face\n",
                "    #unknown_image = face_recognition.load_image_file(\"/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/known_faces/Virat_Kohli/gettyimages-463104486-2048x2048.jpg\")\n",
                "\n",
                "    # Find all the faces and face encodings in the unknown image\n",
                "    face_locations = face_recognition.face_locations(unknown_image)\n",
                "    face_encodings = face_recognition.face_encodings(unknown_image, face_locations)\n",
                "\n",
                "    # Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\n",
                "    # See http://pillow.readthedocs.io/ for more about PIL/Pillow\n",
                "    pil_image = Image.fromarray(unknown_image)\n",
                "    # Create a Pillow ImageDraw Draw instance to draw with\n",
                "    draw = ImageDraw.Draw(pil_image)\n",
                "\n",
                "    # Loop through each face found in the unknown image\n",
                "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
                "        # See if the face is a match for the known face(s)\n",
                "        matches = face_recognition.compare_faces(known_faces, face_encoding,TOLERANCE)\n",
                "\n",
                "        name = \"Unknown\"\n",
                "\n",
                "        # Or instead, use the known face with the smallest distance to the new face\n",
                "        face_distances = face_recognition.face_distance(known_faces, face_encoding)\n",
                "        best_match_index = np.argmin(face_distances)\n",
                "        if matches[best_match_index]:\n",
                "            name = known_names[best_match_index]\n",
                "\n",
                "        # Draw a box around the face using the Pillow module\n",
                "        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n",
                "\n",
                "        # Draw a label with a name below the face\n",
                "        text_width, text_height = draw.textsize(name)\n",
                "        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n",
                "        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n",
                "\n",
                "    # Remove the drawing library from memory as per the Pillow docs\n",
                "    del draw\n",
                "\n",
                "    # Display the resulting image\n",
                "    display(pil_image)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train_data = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\")\n",
                "test_data = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/test.csv\")\n",
                "test=test_data"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_data.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test_data.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Col_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]\n",
                "print(Col_with_missing)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Col_with_missing = [col for col in test_data.columns if test_data[col].isnull().any()]\n",
                "print(Col_with_missing)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "drop_column=[]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "for col in train_data.columns:\n",
                "    if train_data[col].isnull().any():\n",
                "        x=train_data[col].isnull().sum()\n",
                "        if x>50:\n",
                "            print(col+\"\\t\"+str(x))\n",
                "            drop_column.append(col)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "for col in test_data.columns:\n",
                "    if test_data[col].isnull().any():\n",
                "        x=test_data[col].isnull().sum()\n",
                "        if x>50:\n",
                "            print(col+\"\\t\"+str(x))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(drop_column)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train_data=train_data.drop(drop_column,axis=1)\n",
                "test_data=test_data.drop(drop_column,axis=1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_train=train_data.drop([\"SalePrice\"],axis=1)\n",
                "Y_train=train_data[\"SalePrice\"]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Y_train.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "cat_column=[]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for col in X_train.columns:\n",
                "    if X_train[col].dtype=='object':\n",
                "        cat_column.append(col)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(cat_column)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for col in cat_column:\n",
                "    print(col)\n",
                "    print(X_train[col].value_counts())\n",
                "    print(\"-\"*50)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "from xgboost import XGBRegressor\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "\n",
                "# function for comparing different approaches\n",
                "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
                "    my_model = XGBRegressor(n_estimators=1000, learning_rate=0.01)\n",
                "    my_model.fit(X_train, y_train, early_stopping_rounds=50, \n",
                "             eval_set=[(X_valid, y_valid)], verbose=False)\n",
                "    my_model.fit(X_train, y_train)\n",
                "    preds = my_model.predict(X_valid)\n",
                "    return mean_absolute_error(y_valid, preds)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split\n",
                "x_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train,\n",
                "                                                      train_size=0.8, test_size=0.2,\n",
                "                                                      random_state=0)\n",
                "x_test=test_data"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.impute import SimpleImputer\n",
                "my_imputer=SimpleImputer(strategy=\"most_frequent\")\n",
                "imputed_X_train= pd.DataFrame(my_imputer.fit_transform(x_train))\n",
                "imputed_X_test=pd.DataFrame(my_imputer.transform(x_test))\n",
                "imputed_X_valid=pd.DataFrame(my_imputer.transform(x_valid))\n",
                "imputed_X_train.index = x_train.index\n",
                "imputed_X_valid.index = x_valid.index\n",
                "imputed_X_test.index = x_test.index\n",
                "imputed_X_train.columns=x_train.columns\n",
                "imputed_X_valid.columns=x_valid.columns\n",
                "imputed_X_test.columns=x_test.columns"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Col_with_missing_2 = [col for col in imputed_X_valid.columns if imputed_X_valid[col].isnull().any()]\n",
                "print(Col_with_missing_2)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "Num_col=[]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "for col in x_train.columns:\n",
                "    if(x_train[col].dtype!=\"object\"):\n",
                "        print(col+\"\\t\"+str(x_train[col].dtype))\n",
                "        if col!=\"Id\":\n",
                "            Num_col.append(col)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(Num_col)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#feature=[\"LotArea\",\"OverallQual\",\"OverallCond\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"1stFlrSF\",\"GrLivArea\",'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars',\"GarageArea\"]\n",
                "\"\"\"\n",
                "feature=[]\n",
                "for col in Num_col:\n",
                "    feature.append(col)\n",
                "\"\"\"\n",
                "feature=['TotalBsmtSF', 'WoodDeckSF', 'BsmtUnfSF', 'YearRemodAdd', '3SsnPorch', 'KitchenAbvGr', '2ndFlrSF', 'ScreenPorch', 'PoolArea', 'TotRmsAbvGrd', 'MoSold', 'BedroomAbvGr', 'MiscVal', 'BsmtHalfBath', '1stFlrSF', 'GarageCars', 'OverallQual', 'YrSold', 'HalfBath', 'OpenPorchSF', 'BsmtFullBath', 'LowQualFinSF', 'LotArea', 'OverallCond', 'YearBuilt', 'EnclosedPorch', 'FullBath', 'Fireplaces', 'BsmtFinSF2', 'BsmtFinSF1', 'MSSubClass', 'GrLivArea', 'GarageArea']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(feature)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "for col in cat_column:\n",
                "    print(col)\n",
                "    print(X_train[col].value_counts())\n",
                "    print(\"-\"*50)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#cat_enc_col=[\"Street\",\"LotShape\",\"LotConfig\",\"BldgType\",\"HouseStyle\",\"MasVnrType\",\"ExterQual\",\"Foundation\",\"BsmtQual\",\"BsmtExposure\",\"BsmtFinType1\",\"KitchenQual\"]\n",
                "cat_enc_col=['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for col in cat_enc_col:\n",
                "    feature.append(col)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "imputed_X_train=imputed_X_train[feature]\n",
                "imputed_X_valid=imputed_X_valid[feature]\n",
                "imputed_X_test=imputed_X_test[feature]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "imputed_X_train.head(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Cat_cols=cat_enc_col"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.preprocessing import OneHotEncoder\n",
                "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
                "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(imputed_X_train[Cat_cols]))\n",
                "OH_cols_valid = pd.DataFrame(OH_encoder.transform(imputed_X_valid[Cat_cols]))\n",
                "OH_cols_test = pd.DataFrame(OH_encoder.transform(imputed_X_test[Cat_cols]))\n",
                "OH_cols_train.index = imputed_X_train.index\n",
                "OH_cols_valid.index = imputed_X_valid.index\n",
                "OH_cols_test.index = imputed_X_test.index\n",
                "num_X_train = imputed_X_train.drop(Cat_cols, axis =1)\n",
                "num_X_valid = imputed_X_valid.drop(Cat_cols, axis =1)\n",
                "num_X_test = imputed_X_test.drop(Cat_cols, axis =1)\n",
                "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
                "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
                "OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "OH_X_train = OH_X_train.apply(pd.to_numeric)\n",
                "OH_X_valid = OH_X_valid.apply(pd.to_numeric)\n",
                "OH_X_test = OH_X_test.apply(pd.to_numeric)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "OH_X_test.head(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(\"MAE:\",end=\" \")\n",
                "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.ensemble import GradientBoostingRegressor\n",
                "my_model = GradientBoostingRegressor(loss=\"ls\",learning_rate=0.01,n_estimators=1000,max_depth=4,alpha=0.08)\n",
                "my_model.fit(OH_X_train, y_train)\n",
                "preds = my_model.predict(OH_X_valid)\n",
                "print(\"MAE:\",mean_absolute_error(y_valid, preds))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from xgboost import XGBRegressor\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "my_model_1 = XGBRegressor(n_estimators=2000, learning_rate=0.008)\n",
                "my_model_1.fit(OH_X_train, y_train, early_stopping_rounds=50, \n",
                "             eval_set=[(OH_X_valid, y_valid)], verbose=False)\n",
                "preds_1 = my_model_1.predict(OH_X_valid)\n",
                "print(\"MAE:\",mean_absolute_error(y_valid, preds_1))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.ensemble import GradientBoostingRegressor\n",
                "my_model_2 = GradientBoostingRegressor(loss=\"ls\",learning_rate=0.01,n_estimators=2000,max_depth=4,alpha=0.08)\n",
                "my_model_2.fit(OH_X_train, y_train)\n",
                "preds_2 = my_model_2.predict(OH_X_valid)\n",
                "print(\"MAE:\",mean_absolute_error(y_valid, preds_2))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "preds_3=(preds_1+preds_2)/2\n",
                "print(\"MAE:\",mean_absolute_error(y_valid, preds_3))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "predictions1 = my_model_1.predict(OH_X_test)\n",
                "predictions2 = my_model_2.predict(OH_X_test)\n",
                "Preds_last=(predictions1+predictions2)/2"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Preds_last"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "output = pd.DataFrame({'Id': test.Id,'SalePrice': Preds_last})\n",
                "output.to_csv('submission1.csv', index=False)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
                "test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Col_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]\n",
                "print(Col_with_missing)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Col_with_missing = [col for col in test_data.columns if test_data[col].isnull().any()]\n",
                "print(Col_with_missing)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Get list of categorical variables\n",
                "s = (train_data.dtypes == 'object')\n",
                "object_cols = list(s[s].index)\n",
                "\n",
                "print(\"Categorical variables:\")\n",
                "print(object_cols)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "feature_name=['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']\n",
                "X=train_data[feature_name]\n",
                "y=train_data[\"Survived\"]\n",
                "X_test=test_data[feature_name]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.impute import SimpleImputer\n",
                "my_imputer=SimpleImputer(strategy=\"most_frequent\")\n",
                "imputed_X_train= pd.DataFrame(my_imputer.fit_transform(X_train))\n",
                "imputed_X_test=pd.DataFrame(my_imputer.transform(X_test))\n",
                "imputed_X_valid=pd.DataFrame(my_imputer.transform(X_valid))\n",
                "imputed_X_train.index = X_train.index\n",
                "imputed_X_valid.index = X_valid.index\n",
                "imputed_X_test.index = X_test.index\n",
                "imputed_X_train.columns=X_train.columns\n",
                "imputed_X_valid.columns=X_valid.columns\n",
                "imputed_X_test.columns=X_test.columns"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "Col_with_missing_2 = [col for col in imputed_X_test.columns if imputed_X_test[col].isnull().any()]\n",
                "print(Col_with_missing_2)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Feature Generation\n",
                "New_feature_train = imputed_X_train['Sex'] + \"_\" + imputed_X_train['Embarked']\n",
                "New_feature_valid = imputed_X_valid['Sex'] + \"_\" + imputed_X_valid['Embarked']\n",
                "New_feature_test = imputed_X_test['Sex'] + \"_\" + imputed_X_test['Embarked']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "imputed_X_train[\"Sex_Embarked\"]=New_feature_train\n",
                "imputed_X_valid[\"Sex_Embarked\"]=New_feature_valid\n",
                "imputed_X_test[\"Sex_Embarked\"]=New_feature_test"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "imputed_X_test.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Cat_cols=['Sex','Embarked','Sex_Embarked']"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.preprocessing import OneHotEncoder\n",
                "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
                "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(imputed_X_train[Cat_cols]))\n",
                "OH_cols_valid = pd.DataFrame(OH_encoder.transform(imputed_X_valid[Cat_cols]))\n",
                "OH_cols_test = pd.DataFrame(OH_encoder.transform(imputed_X_test[Cat_cols]))\n",
                "OH_cols_train.index = imputed_X_train.index\n",
                "OH_cols_valid.index = imputed_X_valid.index\n",
                "OH_cols_test.index = imputed_X_test.index\n",
                "num_X_train = imputed_X_train.drop(Cat_cols, axis =1)\n",
                "num_X_valid = imputed_X_valid.drop(Cat_cols, axis =1)\n",
                "num_X_test = imputed_X_test.drop(Cat_cols, axis =1)\n",
                "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
                "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
                "OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "OH_X_train = OH_X_train.apply(pd.to_numeric)\n",
                "OH_X_valid = OH_X_valid.apply(pd.to_numeric)\n",
                "OH_X_test = OH_X_test.apply(pd.to_numeric)\n",
                "OH_X_train=OH_X_train.rename(columns={0:\"Sex1\", 1:\"Sex2\"})\n",
                "OH_X_train=OH_X_train.rename(columns={2:\"C\", 3:\"Q\",4:\"S\"})\n",
                "OH_X_valid=OH_X_valid.rename(columns={0:\"Sex1\", 1:\"Sex2\"})\n",
                "OH_X_valid=OH_X_valid.rename(columns={2:\"C\", 3:\"Q\",4:\"S\"})\n",
                "OH_X_test=OH_X_test.rename(columns={0:\"Sex1\", 1:\"Sex2\"})\n",
                "OH_X_test=OH_X_test.rename(columns={2:\"C\", 3:\"Q\",4:\"S\"})"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "OH_X_test.head(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from xgboost import XGBClassifier\n",
                "from sklearn import metrics\n",
                "my_model = XGBClassifier(n_estimators=1000, learning_rate=0.001)\n",
                "my_model.fit(OH_X_train, y_train, early_stopping_rounds=50, \n",
                "             eval_set=[(OH_X_valid, y_valid)], verbose=False)\n",
                "my_model.fit(OH_X_train, y_train)\n",
                "y_pred5 = my_model.predict(OH_X_valid)\n",
                "print(\"Accuracy:\",metrics.accuracy_score(y_valid, y_pred5))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results",
                "validate_data"
            ],
            "content": [
                "predictions2 = my_model.predict(OH_X_test)\n",
                "\n",
                "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions2})\n",
                "output.to_csv('my_submission_02_06.csv', index=False)\n",
                "print(\"Your submission was successfully saved!\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import os\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. wpd.read_csv)\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "from skopt import gp_minimize\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "classifier_output = pd.read_csv('/kaggle/input/tractable_ds_excercise_data/classifier_output.csv')\n",
                "print(f'Classifier output data shape:  {classifier_output.shape}')\n",
                "classifier_output.head(10)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Rows with no urr_score aren't needed for this analysis.  We're missing a lot for some reason\n",
                "classifier_output.dropna(inplace=True)\n",
                "print(f'Classifier output data shape without nans:  {classifier_output.shape}')"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Get line data\n",
                "\n",
                "metadata_files = []\n",
                "for dirname, _, filenames in os.walk('/kaggle/input/tractable_ds_excercise_data/metadata'):\n",
                "    for filename in filenames:\n",
                "        metadata_files.append(os.path.join(dirname, filename))\n",
                "\n",
                "line_data = pd.concat([pd.read_csv(filepath) for filepath in metadata_files])\n",
                "print(f'Line data shape: {line_data.shape}')\n",
                "line_data.head(10)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# We seem to be missing about 5000 of the promised claims - perhaps ones where no repairs or replacements were made\n",
                "\n",
                "print(f'Unique claims: {len(line_data[\"claim_id\"].unique())}')"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Merge the claim-level data first, and then the line-level data\n",
                "claim_merged = classifier_output.merge(line_data[['claim_id', 'make', 'model', 'year','poi']].drop_duplicates(subset=['claim_id'], keep='first'),\n",
                "                                       how='left', on='claim_id')\n",
                "\n",
                "print(f'Classifier outputs not associated with a claim: {claim_merged[\"make\"].isna().sum()}')\n",
                "# Remove any classifier outputs that can't be associated with a claim\n",
                "claim_merged.dropna(subset=['make'], inplace=True)\n",
                "\n",
                "data = pd.merge(claim_merged, line_data[['claim_id', 'line_num', 'part', 'operation', 'part_price', 'labour_amt']],\n",
                "                how='left', on=['claim_id', 'part'])\n",
                "\n",
                "data['operation'].fillna('undamaged', inplace=True)\n",
                "print(f'Merge data shape: {data.shape}')\n",
                "data.head(10)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Visualise the effectiveness of the classifier on the test set\n",
                "\n",
                "data['rounded_urr_score'] = data['urr_score'].apply(lambda x: round(x, 2))\n",
                "\n",
                "bucket_counts = (data[(data['set']==2)][['rounded_urr_score', 'operation', 'urr_score']]\n",
                "                 .groupby(['rounded_urr_score', 'operation'])\n",
                "                 .count()\n",
                "                 .reset_index()\n",
                "                 .rename(columns={'urr_score': 'count'})\n",
                "                 .set_index('rounded_urr_score')\n",
                "                 .pivot(columns='operation', values='count')\n",
                "                 .fillna(0)\n",
                "                )\n",
                "\n",
                "bucket_counts = bucket_counts[['undamaged', 'repair', 'replace']]\n",
                "\n",
                "bucket_counts.head(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "bucket_counts.sum(axis=1).plot.bar()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "bucket_counts_divided = bucket_counts.divide(bucket_counts.sum(axis=1), axis=0)\n",
                "\n",
                "bucket_counts_divided.plot.area()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "operation_ranks = {'undamaged': 0,\n",
                "                   'repair': 1,\n",
                "                   'replace': 2}\n",
                "\n",
                "data['operation_rank'] = data['operation'].apply(lambda x: operation_ranks[x])\n",
                "\n",
                "def mae_single_point(urr_score, operation_rank, repair_threshold, replace_threshold):\n",
                "    classified_outcome_rank = int(urr_score > repair_threshold) + int(urr_score > replace_threshold)\n",
                "\n",
                "    return abs(classified_outcome_rank - operation_rank)\n",
                "\n",
                "assert(mae_single_point(0.9, 0, 0.4, 0.7) == 2)\n",
                "assert(mae_single_point(0.5, 1, 0.4, 0.7) == 0)\n",
                "assert(mae_single_point(0.5, 2, 0.4, 0.7) == 1)\n",
                "    "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def mae_dataset(data, repair_threshold, replace_threshold):\n",
                "    class_maes =[]\n",
                "    for i in range(2):\n",
                "        class_data = data[(data['operation_rank']==i)]\n",
                "        class_mae = sum(class_data\n",
                "                        .apply(lambda row: mae_single_point(row['urr_score'], row['operation_rank'], repair_threshold, replace_threshold), axis=1))/len(class_data)\n",
                "        class_maes.append(class_mae)\n",
                "    total_mae = sum(class_maes)/3\n",
                "    return total_mae"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "# Use only the test set to evaluate the best thresholds\n",
                "\n",
                "test_set = data[(data['set']==2)][['urr_score', 'operation_rank']]\n",
                "\n",
                "def mae(thresholds):\n",
                "    return mae_dataset(test_set, thresholds[0], thresholds[1])\n",
                "\n",
                "# Calculating mse is somewhat expensive at a couple of seconds a time, so use an optimizer and small number of iterations\n",
                "# Takes about 2.5 minutes\n",
                "opt = gp_minimize(mae, dimensions=[(0.0, 1.0, 'uniform'), (0.0, 1.0, 'uniform')], n_calls=50, verbose=True)\n",
                "\n",
                "print(f'Best thresholds: {opt.x}')\n",
                "print(f'Best average mse: {opt.fun}')"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import requests\n",
                "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
                "r = requests.get(url, allow_redirects=True)\n",
                "open('./time_series_covid19_confirmed_US.csv', 'wb').write(r.content)\n",
                "#\n",
                "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
                "r = requests.get(url, allow_redirects=True)\n",
                "open('./time_series_covid19_confirmed_global.csv', 'wb').write(r.content)\n",
                "#\n",
                "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
                "r = requests.get(url, allow_redirects=True)\n",
                "open('./time_series_covid19_deaths_US.csv', 'wb').write(r.content)\n",
                "#\n",
                "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
                "r = requests.get(url, allow_redirects=True)\n",
                "open('./time_series_covid19_deaths_global.csv', 'wb').write(r.content)\n",
                "#\n",
                "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'\n",
                "r = requests.get(url, allow_redirects=True)\n",
                "open('./time_series_covid19_recovered_global.csv', 'wb').write(r.content)\n",
                "#\n",
                "from datetime import datetime, timedelta\n",
                "url=datetime.strftime(datetime.today() - timedelta(1), 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/%m-%d-%Y.csv')\n",
                "r = requests.get(url, allow_redirects=True)\n",
                "open('./csse_covid_19_daily_reports.csv', 'wb').write(r.content)\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "import shutil\n",
                "\n",
                "original = r'../input/input1/covid19_by_country.csv'\n",
                "target = r'./covid19_by_country.csv'\n",
                "shutil.copyfile(original, target)\n",
                "original = r'../input/input1/GlobalLandTemperaturesByCountry.csv'\n",
                "target = r'./GlobalLandTemperaturesByCountry.csv'\n",
                "shutil.copyfile(original, target)\n",
                "original = r'../input/input1/GlobalLandTemperaturesByMajorCity.csv'\n",
                "target = r'./GlobalLandTemperaturesByMajorCity.csv'\n",
                "shutil.copyfile(original, target)\n",
                "original = r'../input/input1/API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv'\n",
                "target = r'./API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv'\n",
                "shutil.copyfile(original, target)\n",
                "original = r'../input/input1/shift.JPG'\n",
                "target = r'./shift.JPG'\n",
                "shutil.copyfile(original, target)\n",
                "\n",
                "print(os.listdir(\"./\"))\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# Loading datasets required for analysis\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "sns.set(style=\"white\", color_codes=True)\n",
                "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "import os\n",
                "print(os.listdir(\"./\"))\n",
                "#print(os.listdir(\"../input/input\"))\n",
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "import plotly.io as pio\n",
                "pio.renderers.default = \"browser\""
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "codiv_country=pd.read_csv('./covid19_by_country.csv')\n",
                "codiv_country.sample(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country.describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#nomalize some datas, also change the population in real size\n",
                "codiv_country['Population 2020']=codiv_country['Population 2020']*1000\n",
                "codiv_country['Tests_per_10kp']=codiv_country['Tests']*10000.0/codiv_country['Population 2020']\n",
                "codiv_country['Tests_per_10kp_log1p']=np.log1p(codiv_country['Tests_per_10kp'])\n",
                "codiv_country['GDP 2018 per_1p_log1p']=np.log1p((codiv_country['GDP 2018']/codiv_country['Population 2020']))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country=codiv_country.drop([\n",
                "    'Sex Ratio','Crime Index','Density','Tests_per_10kp','Test Pop','sex0','sex14','sex25','sex54','sex64','sex65plus','Total Infected','Total Deaths','Total Recovered','Tests','GDP 2018','Population 2020'\n",
                "], axis=1)\n",
                "codiv_country.sample(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "Health_expenditure=pd.read_csv(os.path.join('./', 'API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv'))\n",
                "Health_expenditure=Health_expenditure.set_index('Country Name')\n",
                "Health_expenditure.sample(3)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country['Health expenditure Ratio'] = 0.0\n",
                "for countryindex in Health_expenditure.index:\n",
                "    # If the country exists in the other table\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Health expenditure Ratio']=Health_expenditure[Health_expenditure.index==countryindex]['2017'][0]\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "codiv_country.hist(figsize=(12, 12))\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country.dtypes"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country.isna().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country[codiv_country['Health expenditure Ratio'].isnull()]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country['Health expenditure Ratio'] = codiv_country['Health expenditure Ratio'].fillna(codiv_country['Health expenditure Ratio'].mean())"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country['Tests_per_10kp_log1p'] = codiv_country['Tests_per_10kp_log1p'].fillna(0) "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country.Country[codiv_country.Country == \"United States\"] = \"US\""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "z_scores_Females= (codiv_country['Females 2018']- codiv_country['Females 2018'].mean()) /codiv_country['Females 2018'].std()\n",
                "z_scores_Females.plot()\n",
                "plt.hlines(-1,0,100,colors=\"red\")\n",
                "plt.hlines(-2,0,100,colors=\"yellow\")\n",
                "plt.hlines(-3,0,100,colors=\"green\")\n",
                "plt.hlines(1,0,100,colors=\"red\")\n",
                "plt.hlines(2,0,100,colors=\"yellow\")\n",
                "plt.hlines(3,0,100,colors=\"green\")\n",
                "plt.ylabel(\"z_scores_Females\")\n",
                "plt.xlabel(\"country index\")"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "codiv_country['Females 2018'].plot()\n",
                "plt.ylabel(\"Females %\")\n",
                "plt.xlabel(\"country index\")"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "idx_Females = (np.abs(z_scores_Females) > 2)\n",
                "codiv_country['Females 2018'][idx_Females]=codiv_country['Females 2018'].mean()\n",
                "print('z-scores_Females:', z_scores_Females.shape)\n",
                "codiv_country['Females 2018'].plot()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "z_scores_Female_Lung= (codiv_country['Female Lung']- codiv_country['Female Lung'].mean()) /codiv_country['Female Lung'].std()\n",
                "z_scores_Female_Lung.plot()\n",
                "plt.hlines(-1,0,100,colors=\"red\")\n",
                "plt.hlines(-2,0,100,colors=\"yellow\")\n",
                "plt.hlines(-3,0,100,colors=\"green\")\n",
                "plt.hlines(1,0,100,colors=\"red\")\n",
                "plt.hlines(2,0,100,colors=\"yellow\")\n",
                "plt.hlines(3,0,100,colors=\"green\")\n",
                "plt.ylabel(\"z_scores_Female_Lung\")\n",
                "plt.xlabel(\"country index\")"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "idx_Female_Lung = (np.abs(z_scores_Female_Lung) > 2)\n",
                "codiv_country['Female Lung'][idx_Female_Lung]=codiv_country['Female Lung'].mean()\n",
                "print('z-scores_Female_Lung:', z_scores_Female_Lung.shape)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "z_scores_Male_Lung= (codiv_country['Male Lung']- codiv_country['Male Lung'].mean()) /codiv_country['Male Lung'].std()\n",
                "z_scores_Male_Lung.plot()\n",
                "plt.hlines(-1,0,100,colors=\"red\")\n",
                "plt.hlines(-2,0,100,colors=\"yellow\")\n",
                "plt.hlines(-3,0,100,colors=\"green\")\n",
                "plt.hlines(1,0,100,colors=\"red\")\n",
                "plt.hlines(2,0,100,colors=\"yellow\")\n",
                "plt.hlines(3,0,100,colors=\"green\")\n",
                "plt.ylabel(\"z_scores_Male_Lung\")\n",
                "plt.xlabel(\"country index\")"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "idx_Male_Lung = (np.abs(z_scores_Male_Lung) > 2)\n",
                "codiv_country['Male Lung'][idx_Male_Lung]=codiv_country['Male Lung'].mean()\n",
                "print('z-scores_Male_Lung:', z_scores_Male_Lung.shape)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "z_scores_Lung= (codiv_country['lung']- codiv_country['lung'].mean()) /codiv_country['lung'].std()\n",
                "z_scores_Lung.plot()\n",
                "plt.hlines(-1,0,100,colors=\"red\",label=\"np.abs(z_scores_Females) > 1)\")\n",
                "plt.hlines(-2,0,100,colors=\"yellow\",label=\"np.abs(z_scores_Females) > 2)\")\n",
                "plt.hlines(-3,0,100,colors=\"green\",label=\"np.abs(z_scores_Females) > 3)\")\n",
                "plt.hlines(1,0,100,colors=\"red\",label=\"np.abs(z_scores_Females) > 1)\")\n",
                "plt.hlines(2,0,100,colors=\"yellow\",label=\"np.abs(z_scores_Females) > 2)\")\n",
                "plt.hlines(3,0,100,colors=\"green\",label=\"np.abs(z_scores_Females) > 3)\")\n",
                "plt.ylabel(\"z_scores_Lung\")\n",
                "plt.xlabel(\"country index\")"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "idx_Lung = (np.abs(z_scores_Lung) > 2)\n",
                "codiv_country['lung'][idx_Lung]=codiv_country['lung'].mean()\n",
                "print('z-scores_Lung:', z_scores_Lung.shape)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "codiv_csse=pd.read_csv(os.path.join('./', 'csse_covid_19_daily_reports.csv'))\n",
                "codiv_csse=codiv_csse.drop(['FIPS','Admin2','Last_Update','Combined_Key','Long_','Lat'], axis=1)\n",
                "codiv_csse.sample(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_csse = codiv_csse.rename(columns={'Confirmed': 'Infected'})"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "codiv_csse=codiv_csse.groupby('Country_Region').sum().reset_index()\n",
                "codiv_csse.sample(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_csse[\"Deaths Ratio\"]=codiv_csse[\"Deaths\"]*1000/codiv_csse[\"Infected\"]\n",
                "codiv_csse.sample(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "set(codiv_country['Country'].unique()) - set(codiv_csse['Country_Region'].unique())"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_csse.Country_Region[codiv_csse.Country_Region == \"Korea, South\"] = \"South Korea\"\n",
                "codiv_csse.Country_Region[codiv_csse.Country_Region == \"Czechia\"] = \"Czech Republic\""
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "codiv_csse=codiv_csse.set_index('Country_Region')\n",
                "codiv_csse.sample(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_csse.describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_csse.dtypes"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_csse.isna().sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country['Total Infected'] = 0.0\n",
                "codiv_country['Total Deaths'] = 0.0\n",
                "codiv_country['Total Recovered'] = 0.0\n",
                "codiv_country['Total Active'] = 0.0\n",
                "codiv_country['Deaths Ratio'] = 0.0\n",
                "for countryindex in codiv_csse.index:\n",
                "    # If the country exists in the other table\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Total Infected Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Infected'][0])\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Total Infected']=codiv_csse[codiv_csse.index==countryindex]['Infected'][0]\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Total Deaths Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Deaths'][0])\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Total Deaths']=codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Total Recovered Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Recovered'][0])\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Total Recovered']=codiv_csse[codiv_csse.index==countryindex]['Recovered'][0]\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Deaths Ratio']=codiv_csse[codiv_csse.index==countryindex]['Deaths Ratio'][0]\n",
                "        if codiv_csse[codiv_csse.index==countryindex]['Active'][0] ==0.0:\n",
                "            codiv_country.at[codiv_country['Country']==countryindex,'Total Active Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Infected'][0]-codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]-codiv_csse[codiv_csse.index==countryindex]['Recovered'][0])\n",
                "            codiv_country.at[codiv_country['Country']==countryindex,'Total Active']=codiv_csse[codiv_csse.index==countryindex]['Infected'][0]-codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]-codiv_csse[codiv_csse.index==countryindex]['Recovered'][0]\n",
                "        else:\n",
                "            codiv_country.at[codiv_country['Country']==countryindex,'Total Active Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Active'][0])\n",
                "            codiv_country.at[codiv_country['Country']==countryindex,'Total Active']=codiv_csse[codiv_csse.index==countryindex]['Active'][0]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country.isnull()[['Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active', 'Deaths Ratio']].sum()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "codiv_country_qurantine=codiv_country[codiv_country['Quarantine'].notnull()]\n",
                "codiv_country_Restrictions=codiv_country[codiv_country['Restrictions'].notnull()]\n",
                "codiv_country_without_Restrictions_qurantine=codiv_country[codiv_country['Restrictions'].isnull() & codiv_country['Quarantine'].isnull()]\n",
                "\n",
                "print(\"codiv_country_qurantine shape\" , codiv_country_qurantine.shape)\n",
                "print(\"codiv_country_Restrictionse shape\" , codiv_country_Restrictions.shape)\n",
                "print(\"codiv_country_without_Restrictions_quarantine\" , codiv_country_without_Restrictions_qurantine.shape)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "codiv_time_confirmed=pd.read_csv(os.path.join('./', 'time_series_covid19_confirmed_global.csv'))\n",
                "codiv_time_confirmed=codiv_time_confirmed.drop(['Lat','Long'], axis=1)\n",
                "\n",
                "codiv_time_deaths=pd.read_csv(os.path.join('./', 'time_series_covid19_deaths_global.csv'))\n",
                "codiv_time_deaths=codiv_time_deaths.drop(['Lat','Long'], axis=1)\n",
                "\n",
                "codiv_time_recovered=pd.read_csv(os.path.join('./', 'time_series_covid19_recovered_global.csv'))\n",
                "codiv_time_recovered=codiv_time_recovered.drop(['Lat','Long'], axis=1)\n",
                "\n",
                "codiv_time_confirmed.sample(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "codiv_country_temp=pd.read_csv('.//GlobalLandTemperaturesByCountry.csv')\n",
                "#codiv_country_temp.loc['2020-01-01':'2020-04-01']\n",
                "codiv_country_temp=codiv_country_temp.drop(['AverageTemperatureUncertainty'], axis=1)\n",
                "codiv_country_temp=codiv_country_temp[((codiv_country_temp['dt'] > '2013-01-01') & (codiv_country_temp['dt'] < '2013-04-01')) | \n",
                "                                      ((codiv_country_temp['dt'] > '2012-01-01') & (codiv_country_temp['dt'] < '2012-04-01')) | \n",
                "                                      ((codiv_country_temp['dt'] > '2011-01-01') & (codiv_country_temp['dt'] < '2011-04-01'))]\n",
                "codiv_country_temp = codiv_country_temp.groupby(['Country'])['AverageTemperature'].mean()\n",
                "print(codiv_country_temp.sample(10))\n",
                "print(\"codiv_country_temp\",codiv_country_temp.shape)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(codiv_country_temp.describe())"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "codiv_country_tempMajorCity=pd.read_csv('.//GlobalLandTemperaturesByMajorCity.csv')\n",
                "\n",
                "codiv_country_tempMajorCity=codiv_country_tempMajorCity.drop(['AverageTemperatureUncertainty','Latitude','Longitude'], axis=1)\n",
                "codiv_country_tempMajorCity=codiv_country_tempMajorCity[((codiv_country_tempMajorCity['dt'] > '2013-01-01') & (codiv_country_tempMajorCity['dt'] < '2013-04-01')) | \n",
                "                                      ((codiv_country_tempMajorCity['dt'] > '2012-01-01') & (codiv_country_tempMajorCity['dt'] < '2012-04-01')) | \n",
                "                                      ((codiv_country_tempMajorCity['dt'] > '2011-01-01') & (codiv_country_tempMajorCity['dt'] < '2011-04-01'))]\n",
                "codiv_country_tempMajorCity= codiv_country_tempMajorCity.groupby(['Country'])['AverageTemperature'].mean()\n",
                "print(codiv_country_tempMajorCity.sample(10))\n",
                "print(\"codiv_country_tempMajorCity\",codiv_country_tempMajorCity.shape)\n",
                "print(\"243-49=194 countries dont have temperature for big city\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Temperature_difference=(codiv_country_tempMajorCity-codiv_country_temp).dropna()\n",
                "Temperature_difference=Temperature_difference.sort_values( ascending=False)\n",
                "x=Temperature_difference.index\n",
                "y=Temperature_difference\n",
                "fig, ax = plt.subplots(figsize=(20,10))\n",
                "ax.scatter(x, y, alpha=0.5)\n",
                "plt.xticks(rotation=45)\n",
                "plt.ylabel(\"Delta Temp AvrMaincity-AvrCountry\")\n",
                "plt.grid(True)\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country['Temp_mean_jan_apr'] = 0.0\n",
                "for countryindex in codiv_country_tempMajorCity.index:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Temp_mean_jan_apr']=codiv_country_tempMajorCity.loc[countryindex]\n",
                "codiv_country['Temp_mean_jan_apr'].sample(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "for countryindex in codiv_country_temp.index:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty:\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Temp_mean_jan_apr'].values[0] == 0:\n",
                "            codiv_country.at[codiv_country['Country']==countryindex,'Temp_mean_jan_apr']=codiv_country_temp.loc[countryindex]\n",
                "codiv_country.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "transit=codiv_country.sort_values(by='Total Infected', ascending=False)\n",
                "transit = transit.reset_index(drop=True)\n",
                "transit.head(11).style.background_gradient(cmap='Blues')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "transit=codiv_country.sort_values(by='Total Active', ascending=False)\n",
                "transit = transit.reset_index(drop=True)\n",
                "transit.head(11).style.background_gradient(cmap='Greens')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "transit=codiv_country.sort_values(by='Total Deaths', ascending=False)\n",
                "transit = transit.reset_index(drop=True)\n",
                "transit.head(11).style.background_gradient(cmap='Reds')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "transit=codiv_country.sort_values(by='Deaths Ratio', ascending=False)\n",
                "transit = transit.reset_index(drop=True)\n",
                "transit.head(11).style.background_gradient(cmap='Oranges')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "codiv_country_short=codiv_country.sort_values(by='Total Infected', ascending=False).head(n=60).reset_index(drop=True)\n",
                "codiv_country_short"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country_short.describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\\n",
                "   .map(plt.scatter, \"Tests_per_10kp_log1p\", \"Total Infected Log10\") \\\n",
                "   .add_legend()\n",
                "\n",
                "plt.grid(True)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#histogram of \"total infected\" for countries that have Tests_per_10kp_log1p == 0\n",
                "codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] == 0][\"Total Infected Log10\"].hist(figsize=(6, 6), alpha=0.5, density=True)\n",
                "plt.xlabel('Total Infected Log1p')\n",
                "plt.ylabel('Nr Country density')\n",
                "plt.title('Histogram Tests_per_10kp_log1p==0')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#histogram of \"total infected\" for countries that have Tests_per_10kp_log1p != 0\n",
                "codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] != 0][\"Total Infected Log10\"].hist(figsize=(6,6), alpha=0.5, density=True)\n",
                "plt.xlabel('Total Infected Log1p')\n",
                "plt.ylabel('Nr Country density')\n",
                "plt.title('Histogram Tests_per_10kp_log1p not 0')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\\n",
                "   .map(plt.scatter, \"Tests_per_10kp_log1p\", \"Total Active Log10\") \\\n",
                "   .add_legend()\n",
                "plt.grid(True)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] == 0][\"Total Active Log10\"].hist(figsize=(6, 6), alpha=0.5, density=True)\n",
                "plt.xlabel('Total Active Log10')\n",
                "plt.ylabel('Nr Country density')\n",
                "plt.title('Histogram Tests_per_10kp_log1p==0')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] != 0][\"Total Active Log10\"].hist(figsize=(6,6), alpha=0.5, density=True)\n",
                "plt.xlabel('Total Active Log10')\n",
                "plt.ylabel('Nr Country density')\n",
                "plt.title('Histogram Tests_per_10kp_log1p not 0')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country_short.drop([   'Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active'], axis=1)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "import seaborn as sns\n",
                "import numpy as np\n",
                "from scipy.stats import pearsonr\n",
                "#I use a temp table to hide some columns not needed for the sns.\n",
                "codiv_country_short_tmp=codiv_country_short.drop([ 'Deaths Ratio','Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active','Total Recovered Log10','Total Infected Log10','Total Deaths Log10','Total Active Log10'], axis=1)\n",
                "for col in codiv_country_short_tmp.loc[:, codiv_country_short_tmp.dtypes == np.number].keys():\n",
                "    sns.jointplot(codiv_country_short['Total Infected Log10'], col, data=codiv_country_short, height=6, s=1, color=\"blue\")\n",
                "    corr, _ = pearsonr(codiv_country_short['Total Infected Log10'], codiv_country_short[col])\n",
                "    print(\"Pearsons correlation Total Infected Log10 <-> %s : %.3f\" %(col, corr))\n",
                "    sns.jointplot(codiv_country_short['Total Deaths Log10'], col, data=codiv_country_short, height=6, s=1, color=\"red\")\n",
                "    corr, _ = pearsonr(codiv_country_short['Total Deaths Log10'], codiv_country_short[col])\n",
                "    print(\"Pearsons correlation Total Deaths Log10 <-> %s : %.3f\" %(col, corr))\n",
                "    sns.jointplot(codiv_country_short['Total Active Log10'], col, data=codiv_country_short, height=6, s=1, color=\"green\")\n",
                "    corr, _ = pearsonr(codiv_country_short['Total Active Log10'], codiv_country_short[col])\n",
                "    print(\"Pearsons correlation Total Active Log10 <-> %s : %.3f\" %(col, corr))\n",
                "    sns.jointplot(codiv_country_short['Deaths Ratio'], col, data=codiv_country_short, height=6, s=1, color=\"orange\")\n",
                "    corr, _ = pearsonr(codiv_country_short['Deaths Ratio'], codiv_country_short[col])\n",
                "    print(\"Pearsons correlation Deaths Ratio <-> %s : %.3f\" %(col, corr))\n",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "coefs_poly3 = np.polyfit(codiv_country_short[\"Temp_mean_jan_apr\"], codiv_country_short[\"Total Infected Log10\"], deg=8) \n",
                "sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\\n",
                "   .map(plt.scatter, \"Temp_mean_jan_apr\", \"Total Infected Log10\") \\\n",
                "   .add_legend()\n",
                "for indexq in range(-80,300,1):\n",
                "    plt.plot(indexq/10,np.polyval(coefs_poly3, indexq/10), '.', color='black')\n",
                "plt.vlines(7,7,14,colors=\"red\",linestyles='dashed')\n",
                "plt.vlines(24,7,14,colors=\"red\",linestyles='dashed')\n",
                "plt.grid(True)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "m,b = np.polyfit(codiv_country_short[\"Health expenditure Ratio\"], codiv_country_short[\"Total Infected Log10\"], 1) \n",
                "\n",
                "sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\\n",
                "   .map(plt.scatter, \"Health expenditure Ratio\", \"Total Infected Log10\") \\\n",
                "   .add_legend()\n",
                "plt.plot(codiv_country_short[\"Health expenditure Ratio\"], m*codiv_country_short[\"Health expenditure Ratio\"]+b, '--k') \n",
                "plt.grid(True)"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Working on a copy\n",
                "codiv_country_analyze = codiv_country.copy()\n",
                "\n",
                "# Creating categorical variables\n",
                "codiv_country_analyze['Quarantine_cat'] = codiv_country_analyze['Quarantine'].notnull().astype(int)\n",
                "codiv_country_analyze['Restrictions_cat'] = codiv_country_analyze['Restrictions'].notnull().astype(int)\n",
                "codiv_country_analyze['Schools_cat'] = codiv_country_analyze['Schools'].notnull().astype(int)\n",
                "\n",
                "# \n",
                "codiv_country_analyze=codiv_country_analyze.drop([\n",
                "    'Quarantine','Schools','Restrictions', # now categorical\n",
                "    'Country', # not helpful\n",
                "    \n",
                "    # Only keep \"Total Infected Log10\"\n",
                "    'Total Deaths','Total Infected','Total Active','Total Recovered', \n",
                "    \"Total Deaths Log10\",\"Total Recovered Log10\",\"Total Active Log10\",\"Deaths Ratio\"\n",
                "], axis=1)\n",
                "\n",
                "codiv_country_analyze.sample(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "codiv_country_analyze[codiv_country_analyze.isnull().any(axis=1)]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country_analyze = codiv_country_analyze[codiv_country_analyze.notnull().all(axis=1)]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "columns = ['method','Cross-validation']\n",
                "index=range(8)\n",
                "df_resultat = pd.DataFrame(index=index, columns=columns)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "codiv_country_analyze['category_balanced'], bin_edges_balanced=pd.qcut(codiv_country_analyze['Total Infected Log10'], q=6,labels=False, retbins=True)\n",
                "print(bin_edges_balanced)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#explanation of these values\n",
                "pd.qcut(codiv_country_analyze['Total Infected Log10'], q=6)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#balanced bins deliver category for 0-5, euqidistant from 1-6, so i will add 1 to the category numbers\n",
                "codiv_country_analyze['category_balanced'] = codiv_country_analyze['category_balanced'] +1"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ylevel0_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][0]\n",
                "ylevel1_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][1]\n",
                "ylevel2_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][2]\n",
                "ylevel3_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][3]\n",
                "ylevel4_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][4]\n",
                "ylevel5_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][5]\n",
                "ylevel6_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][6]\n",
                "Level1_balanced='%.3f<Infected<%.3f'%(ylevel0_balanced,ylevel1_balanced,)\n",
                "Level2_balanced='%.3f<Infected<%.3f'%(ylevel1_balanced,ylevel2_balanced,)\n",
                "Level3_balanced='%.3f<Infected<%.3f'%(ylevel2_balanced,ylevel3_balanced,)\n",
                "Level4_balanced='%.3f<Infected<%.3f'%(ylevel3_balanced,ylevel4_balanced,)\n",
                "Level5_balanced='%.3f<Infected<%.3f'%(ylevel4_balanced,ylevel5_balanced,)\n",
                "Level6_balanced='%.3f<Infected<%.3f'%(ylevel5_balanced,ylevel6_balanced,)\n",
                "print(\"Level1_balanced= \",Level1_balanced)\n",
                "print(\"Level2_balanced= \",Level2_balanced)\n",
                "print(\"Level3_balanced= \",Level3_balanced)\n",
                "print(\"Level4_balanced= \",Level4_balanced)\n",
                "print(\"Level5_balanced= \",Level5_balanced)\n",
                "print(\"Level6_balanced= \",Level6_balanced)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "list(codiv_country_analyze['category_balanced'].value_counts())"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ymax=codiv_country_analyze['Total Infected Log10'].values.max()\n",
                "ymax=ymax+ymax*0.001\n",
                "print(\"ymax = \",ymax)\n",
                "ymin=(codiv_country_analyze['Total Infected Log10'].values.min())\n",
                "ymin=ymin-ymin*0.001\n",
                "print(\"ymin = \",ymin)\n",
                "yinterval=(ymax-ymin)/6\n",
                "# \n",
                "ylevel0_equidistant = ymin\n",
                "ylevel1_equidistant = ymin+yinterval*1\n",
                "ylevel2_equidistant = ymin+yinterval*2\n",
                "ylevel3_equidistant = ymin+yinterval*3\n",
                "ylevel4_equidistant = ymin+yinterval*4\n",
                "ylevel5_equidistant = ymin+yinterval*5\n",
                "ylevel6_equidistant = ymin+yinterval*6\n",
                "\n",
                "#\n",
                "Level1_equidistant='%.3f<Infected<%.3f'%(ylevel0_equidistant,ylevel1_equidistant,)\n",
                "Level2_equidistant='%.3f<Infected<%.3f'%(ylevel1_equidistant,ylevel2_equidistant,)\n",
                "Level3_equidistant='%.3f<Infected<%.3f'%(ylevel2_equidistant,ylevel3_equidistant,)\n",
                "Level4_equidistant='%.3f<Infected<%.3f'%(ylevel3_equidistant,ylevel4_equidistant,)\n",
                "Level5_equidistant='%.3f<Infected<%.3f'%(ylevel4_equidistant,ylevel5_equidistant,)\n",
                "Level6_equidistant='%.3f<Infected<%.3f'%(ylevel5_equidistant,ylevel6_equidistant,)\n",
                "codiv_country_analyze['category_equidistant']=np.digitize(codiv_country_analyze['Total Infected Log10'].values,\n",
                "                          bins=[ylevel0_equidistant,ylevel1_equidistant,ylevel2_equidistant,ylevel3_equidistant,ylevel4_equidistant,ylevel5_equidistant,ylevel6_equidistant])\n",
                "print(\"Level1_equidistant= \",Level1_equidistant)\n",
                "print(\"Level2_equidistant= \",Level2_equidistant)\n",
                "print(\"Level3_equidistant= \",Level3_equidistant)\n",
                "print(\"Level4_equidistant= \",Level4_equidistant)\n",
                "print(\"Level5_equidistant= \",Level5_equidistant)\n",
                "print(\"Level6_equidistant= \",Level6_equidistant)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "y_equidistant = codiv_country_analyze['category_equidistant']\n",
                "y_balanced = codiv_country_analyze['category_balanced']\n",
                "X = codiv_country_analyze.drop(['Total Infected Log10','category_balanced','category_equidistant'], axis=1) \n",
                "sns.countplot(y_balanced)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(y_equidistant)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X.sample(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split\n",
                "X_tr_equid, X_te_equid, y_tr_equid, y_te_equid = train_test_split(X, y_equidistant, test_size=0.15, random_state=1)\n",
                "print('Train set equidistant:', X_tr_equid.shape, y_tr_equid.shape)\n",
                "print('Testn set equidistant:', X_te_equid.shape, y_te_equid.shape)\n",
                "X_tr_balan, X_te_balan, y_tr_balan, y_te_balan = train_test_split(X, y_balanced, test_size=0.15, random_state=1)\n",
                "print('Train set balanced:', X_tr_balan.shape, y_tr_balan.shape)\n",
                "print('Testn set balanced:', X_te_balan.shape, y_te_balan.shape)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(y_te_equid)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(y_te_balan)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.model_selection import KFold\n",
                "cv_strategy = KFold(n_splits=5, shuffle=False, random_state=None)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.dummy import DummyClassifier\n",
                "from sklearn.model_selection import cross_validate\n",
                "from sklearn.model_selection import cross_val_score\n",
                "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X, y_equidistant)\n",
                "dummy_clf.predict(X)\n",
                "print(\"scores baseline mostfrequent classifier equidistant = \", dummy_clf.score(X, y_equidistant))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "df_resultat.at[df_resultat.index==0,\"method\"]=\"baseline_equidistant\"\n",
                "Baseline_training_scores = cross_validate(dummy_clf, X, y_equidistant, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==0,\"Cross-validation\"]=np.mean(Baseline_training_scores['test_score'])\n",
                "print(\"crossvalidation baseline mostfrequent classifier equidistant = \", np.mean(Baseline_training_scores['test_score']) )"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.metrics import confusion_matrix\n",
                "dummy_clf= DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X_tr_equid, y_tr_equid)\n",
                "y_pred_equid=dummy_clf.predict(X_te_equid)\n",
                "matrix = confusion_matrix(y_true=y_te_equid, y_pred=y_pred_equid)\n",
                "print(matrix)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(dummy_clf, X_te_equid, y_te_equid, cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "df_resultat.at[df_resultat.index==1,\"method\"]=\"baseline balanced bins\"\n",
                "Baseline_training_scores = cross_validate(dummy_clf, X, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==1,\"Cross-validation\"]=np.mean(Baseline_training_scores['test_score'])\n",
                "print(\"crossvalidation baseline mostfrequent classifier balanced = \", np.mean(Baseline_training_scores['test_score']) )"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X_tr_balan, y_tr_balan)\n",
                "y_pred_balan=dummy_clf.predict(X_te_balan)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "matrix = confusion_matrix(y_true=y_te_balan, y_pred=y_pred_balan)\n",
                "print(matrix)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(dummy_clf, X_te_balan, y_te_balan, cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.tree import DecisionTreeClassifier"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "depths=range(1,15)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for depth in depths:\n",
                "    DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)\n",
                "    DecisionTree.fit(X, y_equidistant)\n",
                "    cv_val_scores = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(DecisionTree.score(X, y_equidistant))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_equidistant))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "print(\"The maximum is by depth = \",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)\n",
                "OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"decision tree accurency\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Tree depth', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(depths)\n",
                "ax.legend()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "DecisionTree = DecisionTreeClassifier(\n",
                "    criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "# Fit decision tree\n",
                "DecisionTree.fit(X, y_equidistant)\n",
                "Tree_scores = cross_validate(DecisionTree, X, y_equidistant, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==2,\"method\"]=\"decision-tree equidistant\"\n",
                "df_resultat.at[df_resultat.index==2,\"Cross-validation\"]=np.mean(Tree_scores['test_score'])\n",
                "                                                              \n",
                "print(\"score DecistionTree equidistant =\",DecisionTree.score(X, y_equidistant))\n",
                "print(\"cross validation DecisionTree equidistant =\",np.mean(Tree_scores['test_score']))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import confusion_matrix\n",
                "DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "DecisionTree_confusion.fit(X_tr_equid, y_tr_equid)\n",
                "confusion_matrix(y_te_equid, DecisionTree_confusion.predict(X_te_equid))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(DecisionTree_confusion, X_te_equid, y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.tree import export_graphviz\n",
                "import os\n",
                "os.environ[\"PATH\"] += os.pathsep + 'D:/Program Files (x86)/Graphviz2.38/bin/'\n",
                "# Export decision tree\n",
                "dot_data = export_graphviz(\n",
                "    DecisionTree, out_file=None,\n",
                "    feature_names=X.columns, class_names=[Level1_equidistant,Level2_equidistant, Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant],\n",
                "    filled=True, rounded=True, proportion=True   \n",
                ")\n",
                "import graphviz\n",
                "\n",
                "# Display decision tree\n",
                "graphviz.Source(dot_data)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "depths=range(1,15)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for depth in depths:\n",
                "    DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)\n",
                "    DecisionTree.fit(X, y_balanced)\n",
                "    cv_val_scores = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(DecisionTree.score(X, y_balanced))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "print(\"The maximum is by depth_balanced = \",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)\n",
                "OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"decision tree accurency - _balanced bins\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Tree depth', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(depths)\n",
                "ax.legend()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "DecisionTree = DecisionTreeClassifier(\n",
                "    criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "# Fit decision tree\n",
                "DecisionTree.fit(X, y_balanced)\n",
                "# Get score\n",
                "Tree_scores = cross_validate(DecisionTree, X, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==3,\"method\"]=\"decision-tree _balanced bins\"\n",
                "df_resultat.at[df_resultat.index==3,\"Cross-validation\"]=np.mean(Tree_scores['test_score'])\n",
                "print(\"score DecisionTree _balanced bins =\",DecisionTree.score(X, y_balanced))\n",
                "print(\"cross validation DecisionTree balanced bins=\",np.mean(Tree_scores['test_score']))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import confusion_matrix\n",
                "DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "DecisionTree_confusion.fit(X_tr_balan, y_tr_balan)\n",
                "confusion_matrix(y_te_balan, DecisionTree_confusion.predict(X_te_balan))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(DecisionTree_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.tree import export_graphviz\n",
                "import os\n",
                "os.environ[\"PATH\"] += os.pathsep + 'D:/Program Files (x86)/Graphviz2.38/bin/'\n",
                "# Export decision tree\n",
                "dot_data = export_graphviz(\n",
                "    DecisionTree, out_file=None,\n",
                "    feature_names=X.columns, class_names=[Level1_balanced,Level2_balanced, Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced],\n",
                "    filled=True, rounded=True, proportion=True   \n",
                ")\n",
                "import graphviz\n",
                "\n",
                "# Display decision tree\n",
                "graphviz.Source(dot_data)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "Estimators=range(100,2000,100)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for Estimator in Estimators:\n",
                "    forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)\n",
                "    forest.fit(X, y_equidistant)\n",
                "    cv_val_scores = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(forest.fit(X, y_equidistant).score(X, y_equidistant))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "print(\"The maximum is by Estimator_balanced = \",(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)\n",
                "OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"Forest accurency - _balanced bins\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Forest Estimator', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(Estimators)\n",
                "ax.legend()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "# Build a forest and compute the feature importances\n",
                "forest = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\n",
                "forest.fit(X, y_equidistant)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "# Mean test score of a 250x decision tree \n",
                "Forest_scores = cross_validate(forest, X, y_equidistant, cv=cv_strategy)\n",
                "print('Forest by equidistant interval score {:.3f}'.format(forest.score(X, y_equidistant)))\n",
                "df_resultat.at[df_resultat.index==4,\"method\"]=\"Forest equidistant\"\n",
                "df_resultat.at[df_resultat.index==4,\"Cross-validation\"]=np.mean(Forest_scores['test_score'])\n",
                "print('Forest by equidistant interval cross validation {:.3f}'.format(np.mean(Forest_scores['test_score'])))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\n",
                "forest_confusion.fit(X_tr_equid, y_tr_equid)\n",
                "confusion_matrix(y_te_equid, forest_confusion.predict(X_te_equid))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_confusion_matrix(forest_confusion, X_te_equid,y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "importances = forest.feature_importances_\n",
                "std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
                "indices = np.argsort(importances)[::-1]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Print the feature ranking\n",
                "print(\"Feature ranking by equidistand interval:\")\n",
                "\n",
                "for f in range(X.shape[1]):\n",
                "    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n",
                "\n",
                "# Plot the feature importances of the forest\n",
                "plt.figure(figsize=(20,10))\n",
                "plt.title(\"Feature importances\")\n",
                "plt.bar(range(X.shape[1]), importances[indices],\n",
                "       color=\"r\", yerr=std[indices], align=\"center\")\n",
                "plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)\n",
                "plt.xlim([-1, X.shape[1]])\n",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "Estimators=range(100,2000,100)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for Estimator in Estimators:\n",
                "    forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)\n",
                "    forest.fit(X, y_balanced)\n",
                "    cv_val_scores = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(forest.fit(X, y_balanced).score(X, y_balanced))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "print(\"The maximum is by Estimator_balanced = \",(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)\n",
                "OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"Forest accurency - _balanced bins\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Forest Estimator', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(Estimators)\n",
                "ax.legend()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.model_selection import cross_validate\n",
                "Forest_scores = cross_validate(forest, X, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==5,\"method\"]=\"Forest balanced bins\"\n",
                "df_resultat.at[df_resultat.index==5,\"Cross-validation\"]=np.mean(Forest_scores['test_score'])\n",
                "print('Forest by balanced bins of element score {:.3f}'.format(forest.score(X, y_balanced)))\n",
                "print('Forest by balanced bins of element - mean test {:.3f}'.format(np.mean(Forest_scores['test_score'])))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\n",
                "forest_confusion.fit(X_tr_balan, y_tr_balan)\n",
                "confusion_matrix(y_te_balan, forest_confusion.predict(X_te_balan))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(forest_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "importances = forest.feature_importances_\n",
                "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
                "             axis=0)\n",
                "indices = np.argsort(importances)[::-1]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Print the feature ranking\n",
                "print(\"Feature ranking by balanced bins:\")\n",
                "\n",
                "for f in range(X.shape[1]):\n",
                "    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n",
                "\n",
                "# Plot the feature importances of the forest\n",
                "plt.figure(figsize=(20,10))\n",
                "plt.title(\"Feature importances\")\n",
                "plt.bar(range(X.shape[1]), importances[indices],\n",
                "       color=\"r\", yerr=std[indices], align=\"center\")\n",
                "plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)\n",
                "plt.xlim([-1, X.shape[1]])\n",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.decomposition import PCA\n",
                "pca = PCA(n_components=2)\n",
                "# Apply PCA\n",
                "pca.fit(X, y=None); # Unsupervised learning, no y variable\n",
                "feature_2= pca.transform(X)\n",
                "feature_2_df=pd.DataFrame(feature_2)\n",
                "len_cluster, col=feature_2_df.shape"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split\n",
                "X_fea_2_tr_equid, X_fea_2_te_equid, y_fea_2_tr_equid, y_fea_2_te_equid = train_test_split(feature_2, y_equidistant, test_size=0.15, random_state=1)\n",
                "print('Train set equidistant:', X_fea_2_tr_equid.shape, y_fea_2_tr_equid.shape)\n",
                "print('Testn set equidistant:', X_fea_2_te_equid.shape, y_fea_2_te_equid.shape)\n",
                "X_fea_2_tr_balan, X_fea_2_te_balan, y_fea_2_tr_balan, y_fea_2_te_balan = train_test_split(feature_2, y_balanced, test_size=0.15, random_state=1)\n",
                "print('Train set balanced:', X_fea_2_tr_balan.shape, y_fea_2_tr_balan.shape)\n",
                "print('Testn set balanced:', X_fea_2_te_balan.shape, y_fea_2_te_balan.shape)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Create k-NN classifier\n",
                "pipe = Pipeline([\n",
                "    #('scaler', StandardScaler()), # With standardization\n",
                "    ('scaler', None), # Better performance without standardization!\n",
                "    ('knn', KNeighborsClassifier(\n",
                "        n_jobs=-1 # As many parallel jobs as possible\n",
                "    ))\n",
                "])\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from matplotlib.colors import ListedColormap\n",
                "from sklearn import neighbors, datasets\n",
                "from sklearn import metrics\n",
                "from sklearn.model_selection import cross_val_score\n",
                "neighbors = list(range(1,35))\n",
                "CV_scores = []\n",
                "\n",
                "\n",
                "for n_neighborss in neighbors:\n",
                "    weights='distance'\n",
                "    feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "    feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "    h = .02  # step size in the mesh\n",
                "    # we create an instance of Neighbours Classifier and fit the data.\n",
                "    # Create a k-NN pipeline\n",
                "    knn_pipe = Pipeline([\n",
                "        ('scaler', StandardScaler()),\n",
                "        ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights)) ])\n",
                "    # Fit estimator\n",
                "    knn_pipe.fit(feature_2, y_equidistant)\n",
                "    scores = cross_val_score(knn_pipe,feature_2,y_equidistant,cv=cv_strategy )\n",
                "    CV_scores.append(scores.mean())\n",
                "Missclassification_Error = [1-x for x in CV_scores]\n",
                "print(\"the optimal k is \",format(Missclassification_Error.index(min(Missclassification_Error))))\n",
                "k_Optimal=Missclassification_Error.index(min(Missclassification_Error))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "from matplotlib.legend_handler import HandlerLine2D\n",
                "plt.plot(Missclassification_Error)\n",
                "plt.xlabel(\"number of neighbors K\")\n",
                "plt.ylabel(\"Missclassification Error\")\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "from matplotlib.colors import ListedColormap\n",
                "from sklearn import neighbors, datasets\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "# Create color maps\n",
                "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])\n",
                "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])\n",
                "\n",
                "feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "\n",
                "weights='distance'\n",
                "h = .02  # step size in the mesh\n",
                "# we create an instance of Neighbours Classifier and fit the data.\n",
                "# Create a k-NN pipeline\n",
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal))\n",
                "    #, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_pipe.fit(feature_2, y_equidistant)\n",
                "# Plot the decision boundary. For that, we will assign a color to each\n",
                "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
                "feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1\n",
                "feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1\n",
                "xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))\n",
                "Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "Zo = Z.reshape(xx.shape)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Put the result into a color plot\n",
                "plt.figure(figsize=(18, 10))\n",
                "plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)\n",
                "#Label1=pd.DataFrame(list(class_index.items()))\n",
                "Label1=pd.DataFrame([1,2,3,4,5])\n",
                "# Plot also the training points\n",
                "plt.scatter(feature1, feature2, c=y_equidistant, cmap=cmap_bold)\n",
                "#color bar with label\n",
                "cbar = plt.colorbar() \n",
                "cbar.ax.set_yticklabels([Level1_equidistant,Level2_equidistant,Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant])\n",
                "plt.xlim(xx.min(), xx.max())\n",
                "plt.ylim(yy.min(), yy.max())\n",
                "plt.xlabel(\"feature 1\")\n",
                "plt.ylabel(\"feature 2\")\n",
                "plt.title(\"6-Class classification,   equidistant bins -  total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.model_selection import cross_validate\n",
                "knn_scores = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==6,\"method\"]=\"knn equidistant bins\"\n",
                "df_resultat.at[df_resultat.index==6,\"Cross-validation\"]=np.mean(knn_scores['test_score'])\n",
                "print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant)))\n",
                "print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score'])))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_confusion=knn_pipe.fit(X_fea_2_tr_equid, y_fea_2_tr_equid)\n",
                "knn_predict = knn_pipe.predict(X_fea_2_te_equid)\n",
                "confusion_matrix(y_fea_2_te_equid, knn_predict)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_confusion_matrix(knn_confusion, X_fea_2_te_equid, y_fea_2_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "from sklearn.metrics import classification_report\n",
                "print(classification_report(y_true=y_fea_2_te_equid, y_pred=knn_predict))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "neighbors = list(range(1,35))\n",
                "CV_scores = []\n",
                "\n",
                "for n_neighborss in neighbors:\n",
                "    weights='distance'\n",
                "    feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "    feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "    h = .02  # step size in the mesh\n",
                "    # we create an instance of Neighbours Classifier and fit the data.\n",
                "    # Create a k-NN pipeline\n",
                "    knn_pipe = Pipeline([\n",
                "        ('scaler', StandardScaler()),\n",
                "        ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights))\n",
                "    ])\n",
                "    # Fit estimator\n",
                "    knn_pipe.fit(feature_2, y_balanced)\n",
                "    #\n",
                "    scores = cross_val_score(knn_pipe,feature_2,y_balanced,cv=cv_strategy )\n",
                "    CV_scores.append(scores.mean())\n",
                "    \n",
                "Missclassification_Error = [1-x for x in CV_scores]\n",
                "print(\"the optimal k is \",format(Missclassification_Error.index(min(Missclassification_Error))))\n",
                "k_Optimal=Missclassification_Error.index(min(Missclassification_Error))\n",
                "    \n",
                "from matplotlib.legend_handler import HandlerLine2D\n",
                "plt.plot( Missclassification_Error)\n",
                "plt.xlabel(\"number of neighbors K\")\n",
                "plt.ylabel(\"Missclassification Error\")\n",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "# Create color maps\n",
                "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])\n",
                "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])\n",
                "#\n",
                "feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "#\n",
                "weights='distance'\n",
                "h = .02  # step size in the mesh\n",
                "# Create a k-NN pipeline\n",
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_pipe.fit(feature_2, y_balanced)\n",
                "# Plot the decision boundary. For that, we will assign a color to each\n",
                "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
                "feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1\n",
                "feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1\n",
                "xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))\n",
                "Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "Zo = Z.reshape(xx.shape)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Put the result into a color plot\n",
                "plt.figure(figsize=(18, 10))\n",
                "plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)\n",
                "#Label1=pd.DataFrame(list(class_index.items()))\n",
                "Label1=pd.DataFrame([1,2,3,4,5])\n",
                "# Plot also the training points\n",
                "plt.scatter(feature1, feature2, c=y_balanced, cmap=cmap_bold)\n",
                "cbar = plt.colorbar() \n",
                "cbar.ax.set_yticklabels([Level1_balanced,Level2_balanced,Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced])\n",
                "plt.xlim(xx.min(), xx.max())\n",
                "plt.ylim(yy.min(), yy.max())\n",
                "plt.xlabel(\"feature 1\")\n",
                "plt.ylabel(\"feature 2\")\n",
                "plt.title(\"6-Class classification,   balanced bins -  total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.model_selection import cross_validate\n",
                "knn_scores = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==7,\"method\"]=\"knn balanced bins\"\n",
                "df_resultat.at[df_resultat.index==7,\"Cross-validation\"]=np.mean(knn_scores['test_score'])\n",
                "print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced)))\n",
                "print('knn by equidistant bins of element - mean test_score {:.3f}'.format(np.mean(knn_scores['test_score'])))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_confusion=knn_pipe.fit(X_fea_2_tr_balan, y_fea_2_tr_balan)\n",
                "knn_predict = knn_pipe.predict(X_fea_2_te_balan)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "confusion_matrix(y_fea_2_te_balan, knn_predict)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_confusion_matrix(knn_confusion, X_fea_2_te_balan, y_fea_2_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_resultat"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.barplot(x=df_resultat[\"Cross-validation\"],y=df_resultat['method'])\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Working on a copy\n",
                "codiv_country_analyze = codiv_country.copy()\n",
                "\n",
                "# Creating categorical variables\n",
                "codiv_country_analyze['Quarantine_cat'] = codiv_country_analyze['Quarantine'].notnull().astype(int)\n",
                "codiv_country_analyze['Restrictions_cat'] = codiv_country_analyze['Restrictions'].notnull().astype(int)\n",
                "codiv_country_analyze['Schools_cat'] = codiv_country_analyze['Schools'].notnull().astype(int)\n",
                "\n",
                "# \n",
                "codiv_country_analyze=codiv_country_analyze.drop([\n",
                "    'Quarantine','Schools','Restrictions', # now categorical\n",
                "    'Country', # not helpful\n",
                "    'Total Deaths','Total Infected','Total Active','Total Recovered', \n",
                "    \"Total Deaths Log10\",\"Total Recovered Log10\",\"Total Active Log10\",\"Total Infected Log10\"\n",
                "], axis=1)\n",
                "\n",
                "codiv_country_analyze.sample(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "columns = ['method','Cross-validation']\n",
                "index=range(8)\n",
                "df_resultat = pd.DataFrame(index=index, columns=columns)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country_analyze['Deaths Ratio'].describe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country_analyze['category_balanced'], bin_edges_balanced=pd.qcut(codiv_country_analyze['Deaths Ratio'], q=6,labels=False, retbins=True)\n",
                "#balanced bins deliver category for 0-5, euqidistant from 1-6, so i will add 1 to the category numbers\n",
                "codiv_country_analyze['category_balanced'] = codiv_country_analyze['category_balanced'] +1"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ylevel0_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][0]\n",
                "ylevel1_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][1]\n",
                "ylevel2_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][2]\n",
                "ylevel3_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][3]\n",
                "ylevel4_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][4]\n",
                "ylevel5_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][5]\n",
                "ylevel6_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][6]\n",
                "Level1_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel0_balanced,ylevel1_balanced,)\n",
                "Level2_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel1_balanced,ylevel2_balanced,)\n",
                "Level3_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel2_balanced,ylevel3_balanced,)\n",
                "Level4_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel3_balanced,ylevel4_balanced,)\n",
                "Level5_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel4_balanced,ylevel5_balanced,)\n",
                "Level6_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel5_balanced,ylevel6_balanced,)\n",
                "print(\"Level1_balanced= \",Level1_balanced)\n",
                "print(\"Level2_balanced= \",Level2_balanced)\n",
                "print(\"Level3_balanced= \",Level3_balanced)\n",
                "print(\"Level4_balanced= \",Level4_balanced)\n",
                "print(\"Level5_balanced= \",Level5_balanced)\n",
                "print(\"Level6_balanced= \",Level6_balanced)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ymax=codiv_country_analyze['Deaths Ratio'].values.max()\n",
                "print(ymax)\n",
                "ymax=ymax+ymax*0.001\n",
                "print(\"ymax = \",ymax)\n",
                "ymin=(codiv_country_analyze['Deaths Ratio'].values.min())\n",
                "ymin=ymin-ymin*0.001\n",
                "print(\"ymin = \",ymin)\n",
                "yinterval=(ymax-ymin)/6\n",
                "print(ymin,ymax)\n",
                "# \n",
                "ylevel0_equidistant = ymin\n",
                "ylevel1_equidistant = ymin+yinterval*1\n",
                "ylevel2_equidistant = ymin+yinterval*2\n",
                "ylevel3_equidistant = ymin+yinterval*3\n",
                "ylevel4_equidistant = ymin+yinterval*4\n",
                "ylevel5_equidistant = ymin+yinterval*5\n",
                "ylevel6_equidistant = ymin+yinterval*6\n",
                "\n",
                "#\n",
                "Level1_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel0_equidistant,ylevel1_equidistant,)\n",
                "Level2_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel1_equidistant,ylevel2_equidistant,)\n",
                "Level3_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel2_equidistant,ylevel3_equidistant,)\n",
                "Level4_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel3_equidistant,ylevel4_equidistant,)\n",
                "Level5_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel4_equidistant,ylevel5_equidistant,)\n",
                "Level6_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel5_equidistant,ylevel6_equidistant,)\n",
                "codiv_country_analyze['category_equidistant']=np.digitize(codiv_country_analyze['Deaths Ratio'].values,\n",
                "                          bins=[ylevel0_equidistant,ylevel1_equidistant,ylevel2_equidistant,ylevel3_equidistant,ylevel4_equidistant,ylevel5_equidistant,ylevel6_equidistant])\n",
                "print(\"Level1_equidistant= \",Level1_equidistant)\n",
                "print(\"Level2_equidistant= \",Level2_equidistant)\n",
                "print(\"Level3_equidistant= \",Level3_equidistant)\n",
                "print(\"Level4_equidistant= \",Level4_equidistant)\n",
                "print(\"Level5_equidistant= \",Level5_equidistant)\n",
                "print(\"Level6_equidistant= \",Level6_equidistant)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "y_equidistant = codiv_country_analyze['category_equidistant']\n",
                "y_balanced = codiv_country_analyze['category_balanced']\n",
                "X = codiv_country_analyze.drop(['Deaths Ratio','category_balanced','category_equidistant'], axis=1) \n",
                "sns.countplot(y_balanced)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(y_equidistant)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split\n",
                "X_tr_equid, X_te_equid, y_tr_equid, y_te_equid = train_test_split(X, y_equidistant, test_size=0.15, random_state=1)\n",
                "print('Train set equidistant:', X_tr_equid.shape, y_tr_equid.shape)\n",
                "print('Testn set equidistant:', X_te_equid.shape, y_te_equid.shape)\n",
                "X_tr_balan, X_te_balan, y_tr_balan, y_te_balan = train_test_split(X, y_balanced, test_size=0.15, random_state=1)\n",
                "print('Train set balanced:', X_tr_balan.shape, y_tr_balan.shape)\n",
                "print('Testn set balanced:', X_te_balan.shape, y_te_balan.shape)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(y_te_equid)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(y_te_balan)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X, y_equidistant)\n",
                "dummy_clf.predict(X)\n",
                "print(\"scores baseline mostfrequent classifier equidistant = \", dummy_clf.score(X, y_equidistant))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "df_resultat.at[df_resultat.index==0,\"method\"]=\"baseline_equidistant\"\n",
                "Baseline_training_scores = cross_validate(dummy_clf, X, y_equidistant, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==0,\"Cross-validation\"]=np.mean(Baseline_training_scores['test_score'])\n",
                "print(\"crossvalidation baseline mostfrequent classifier equidistant = \", np.mean(Baseline_training_scores['test_score']) )"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.metrics import confusion_matrix\n",
                "dummy_clf= DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X_tr_equid, y_tr_equid)\n",
                "y_pred_equid=dummy_clf.predict(X_te_equid)\n",
                "matrix = confusion_matrix(y_true=y_te_equid, y_pred=y_pred_equid)\n",
                "print(matrix)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(dummy_clf, X_te_equid, y_te_equid, cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X, y_balanced)\n",
                "dummy_clf.predict(X)\n",
                "print(\"training_scores baseline mostfrequent classifier balanced = \", dummy_clf.score(X, y_balanced))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "df_resultat.at[df_resultat.index==1,\"method\"]=\"baseline balanced bins\"\n",
                "Baseline_training_scores = cross_validate(dummy_clf, X, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==1,\"Cross-validation\"]=np.mean(Baseline_training_scores['test_score'])\n",
                "print(\"crossvalidation baseline mostfrequent classifier balanced = \", np.mean(Baseline_training_scores['test_score']) )"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X_tr_balan, y_tr_balan)\n",
                "y_pred_balan=dummy_clf.predict(X_te_balan)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "matrix = confusion_matrix(y_true=y_te_balan, y_pred=y_pred_balan)\n",
                "print(matrix)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(dummy_clf, X_te_balan, y_te_balan, cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "depths=range(1,15)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for depth in depths:\n",
                "    DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)\n",
                "    DecisionTree.fit(X, y_equidistant)\n",
                "    cv_val_scores = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(DecisionTree.score(X, y_equidistant))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f std %f score %f\"%(depth,cv_val_scores.mean(),cv_val_scores.std(),DecisionTree.score(X, y_equidistant))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "print(\"The maximum is by depth = \",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)\n",
                "OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"decision tree accurency\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Tree depth', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(depths)\n",
                "ax.legend()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "DecisionTree = DecisionTreeClassifier(\n",
                "    criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "# Fit decision tree\n",
                "DecisionTree.fit(X, y_equidistant)\n",
                "Tree_scores = cross_validate(DecisionTree, X, y_equidistant, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==2,\"method\"]=\"decision-tree equidistant\"\n",
                "df_resultat.at[df_resultat.index==2,\"Cross-validation\"]=np.mean(Tree_scores['test_score'])\n",
                "                                                              \n",
                "print(\"score DecistionTree equidistant =\",DecisionTree.score(X, y_equidistant))\n",
                "print(\"cross validation DecisionTree equidistant =\",np.mean(Tree_scores['test_score']))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import confusion_matrix\n",
                "DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "DecisionTree_confusion.fit(X_tr_equid, y_tr_equid)\n",
                "confusion_matrix(y_te_equid, DecisionTree_confusion.predict(X_te_equid))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(DecisionTree_confusion, X_te_equid, y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "transfer_results"
            ],
            "content": [
                "os.environ[\"PATH\"] += os.pathsep + 'D:/Program Files (x86)/Graphviz2.38/bin/'\n",
                "# Export decision tree\n",
                "dot_data = export_graphviz(\n",
                "    DecisionTree, out_file=None,\n",
                "    feature_names=X.columns, class_names=[Level1_equidistant,Level2_equidistant, Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant],\n",
                "    filled=True, rounded=True, proportion=True   \n",
                ")\n",
                "import graphviz\n",
                "\n",
                "# Display decision tree\n",
                "graphviz.Source(dot_data)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "depths=range(1,15)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for depth in depths:\n",
                "    DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)\n",
                "    DecisionTree.fit(X, y_balanced)\n",
                "    cv_val_scores = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(DecisionTree.score(X, y_balanced))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "print(\"The maximum is by depth_balanced = \",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)\n",
                "OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"decision tree accurency - _balanced bins\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Tree depth', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(depths)\n",
                "ax.legend()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "DecisionTree = DecisionTreeClassifier(\n",
                "    criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "# Fit decision tree\n",
                "DecisionTree.fit(X, y_balanced)\n",
                "# Get score\n",
                "Tree_scores = cross_validate(DecisionTree, X, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==3,\"method\"]=\"decision-tree _balanced bins\"\n",
                "df_resultat.at[df_resultat.index==3,\"Cross-validation\"]=np.mean(Tree_scores['test_score'])\n",
                "print(\"score DecisionTree _balanced bins =\",DecisionTree.score(X, y_balanced))\n",
                "print(\"cross validation DecisionTree balanced bins=\",np.mean(Tree_scores['test_score']))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import confusion_matrix\n",
                "DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "DecisionTree_confusion.fit(X_tr_balan, y_tr_balan)\n",
                "confusion_matrix(y_te_balan, DecisionTree_confusion.predict(X_te_balan))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(DecisionTree_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from sklearn.tree import export_graphviz\n",
                "import os\n",
                "os.environ[\"PATH\"] += os.pathsep + 'D:/Program Files (x86)/Graphviz2.38/bin/'\n",
                "# Export decision tree\n",
                "dot_data = export_graphviz(\n",
                "    DecisionTree, out_file=None,\n",
                "    feature_names=X.columns, class_names=[Level1_balanced,Level2_balanced, Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced],\n",
                "    filled=True, rounded=True, proportion=True   \n",
                ")\n",
                "import graphviz\n",
                "\n",
                "# Display decision tree\n",
                "graphviz.Source(dot_data)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "Estimators=range(100,2000,100)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for Estimator in Estimators:\n",
                "    forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)\n",
                "    forest.fit(X, y_equidistant)\n",
                "    cv_val_scores = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(forest.fit(X, y_equidistant).score(X, y_equidistant))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "print(\"The maximum is by Estimator_balanced = \",(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)\n",
                "OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"Forest accurency - _balanced bins\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Forest Estimator', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(Estimators)\n",
                "ax.legend()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "# Build a forest and compute the feature importances\n",
                "forest = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\n",
                "forest.fit(X, y_equidistant)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "# Mean test score of a 250x decision tree \n",
                "Forest_scores = cross_validate(forest, X, y_equidistant, cv=cv_strategy)\n",
                "print('Forest by equidistant interval score {:.3f}'.format(forest.score(X, y_equidistant)))\n",
                "df_resultat.at[df_resultat.index==4,\"method\"]=\"Forest equidistant\"\n",
                "df_resultat.at[df_resultat.index==4,\"Cross-validation\"]=np.mean(Forest_scores['test_score'])\n",
                "print('Forest by equidistant interval cross validation {:.3f}'.format(np.mean(Forest_scores['test_score'])))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\n",
                "forest_confusion.fit(X_tr_equid, y_tr_equid)\n",
                "confusion_matrix(y_te_equid, forest_confusion.predict(X_te_equid))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_confusion_matrix(forest_confusion, X_te_equid,y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "importances = forest.feature_importances_\n",
                "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
                "             axis=0)\n",
                "indices = np.argsort(importances)[::-1]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Print the feature ranking\n",
                "print(\"Feature ranking by equidistand interval:\")\n",
                "\n",
                "for f in range(X.shape[1]):\n",
                "    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n",
                "\n",
                "# Plot the feature importances of the forest\n",
                "plt.figure(figsize=(20,10))\n",
                "plt.title(\"Feature importances\")\n",
                "plt.bar(range(X.shape[1]), importances[indices],\n",
                "       color=\"r\", yerr=std[indices], align=\"center\")\n",
                "plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)\n",
                "plt.xlim([-1, X.shape[1]])\n",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "Estimators=range(100,2000,100)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for Estimator in Estimators:\n",
                "    forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)\n",
                "    forest.fit(X, y_balanced)\n",
                "    cv_val_scores = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(forest.fit(X, y_balanced).score(X, y_balanced))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "print(\"The maximum is by Estimator_balanced = \",(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)\n",
                "OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"Forest accurency - _balanced bins\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Forest Estimator', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(Estimators)\n",
                "ax.legend()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.model_selection import cross_validate\n",
                "Forest_scores = cross_validate(forest, X, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==5,\"method\"]=\"Forest balanced bins\"\n",
                "df_resultat.at[df_resultat.index==5,\"Cross-validation\"]=np.mean(Forest_scores['test_score'])\n",
                "print('Forest by balanced bins of element score {:.3f}'.format(forest.score(X, y_balanced)))\n",
                "print('Forest by balanced bins of element - mean test {:.3f}'.format(np.mean(Forest_scores['test_score'])))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\n",
                "forest_confusion.fit(X_tr_balan, y_tr_balan)\n",
                "confusion_matrix(y_te_balan, forest_confusion.predict(X_te_balan))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(forest_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "importances = forest.feature_importances_\n",
                "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
                "             axis=0)\n",
                "indices = np.argsort(importances)[::-1]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Print the feature ranking\n",
                "print(\"Feature ranking by balanced bins:\")\n",
                "\n",
                "for f in range(X.shape[1]):\n",
                "    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n",
                "\n",
                "# Plot the feature importances of the forest\n",
                "plt.figure(figsize=(20,10))\n",
                "plt.title(\"Feature importances\")\n",
                "plt.bar(range(X.shape[1]), importances[indices],\n",
                "       color=\"r\", yerr=std[indices], align=\"center\")\n",
                "plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)\n",
                "plt.xlim([-1, X.shape[1]])\n",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "pca = PCA(n_components=2)\n",
                "# Apply PCA\n",
                "pca.fit(X, y=None); # Unsupervised learning, no y variable\n",
                "feature_2= pca.transform(X)\n",
                "feature_2_df=pd.DataFrame(feature_2)\n",
                "len_cluster, col=feature_2_df.shape"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "X_fea_2_tr_equid, X_fea_2_te_equid, y_fea_2_tr_equid, y_fea_2_te_equid = train_test_split(feature_2, y_equidistant, test_size=0.15, random_state=1)\n",
                "print('Train set equidistant:', X_fea_2_tr_equid.shape, y_fea_2_tr_equid.shape)\n",
                "print('Testn set equidistant:', X_fea_2_te_equid.shape, y_fea_2_te_equid.shape)\n",
                "X_fea_2_tr_balan, X_fea_2_te_balan, y_fea_2_tr_balan, y_fea_2_te_balan = train_test_split(feature_2, y_balanced, test_size=0.15, random_state=1)\n",
                "print('Train set balanced:', X_fea_2_tr_balan.shape, y_fea_2_tr_balan.shape)\n",
                "print('Testn set balanced:', X_fea_2_te_balan.shape, y_fea_2_te_balan.shape)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "neighbors = list(range(1,25))\n",
                "CV_scores = []\n",
                "\n",
                "for n_neighborss in neighbors:\n",
                "    weights='distance'\n",
                "    feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "    feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "    h = .02  # step size in the mesh\n",
                "    # we create an instance of Neighbours Classifier and fit the data.\n",
                "    # Create a k-NN pipeline\n",
                "    knn_pipe = Pipeline([\n",
                "        ('scaler', StandardScaler()),\n",
                "        ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights))\n",
                "    ])\n",
                "    # Fit estimator\n",
                "    knn_pipe.fit(feature_2, y_equidistant)\n",
                "    #\n",
                "    Z = knn_pipe.predict(feature_2)\n",
                "    scores = cross_val_score(knn_pipe,feature_2,y_equidistant,cv=cv_strategy )\n",
                "    CV_scores.append(scores.mean())\n",
                "    \n",
                "Missclassification_Error = [1-x for x in CV_scores]\n",
                "print(\"the optimal k is \",format(Missclassification_Error.index(min(Missclassification_Error))))\n",
                "k_Optimal=Missclassification_Error.index(min(Missclassification_Error))\n",
                "    \n",
                "from matplotlib.legend_handler import HandlerLine2D\n",
                "plt.plot( Missclassification_Error)\n",
                "plt.xlabel(\"number of neighbors K\")\n",
                "plt.ylabel(\"Missclassification Error\")\n",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "# Create color maps\n",
                "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])\n",
                "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])\n",
                "\n",
                "feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "\n",
                "weights='distance'\n",
                "h = .02  # step size in the mesh\n",
                "# we create an instance of Neighbours Classifier and fit the data.\n",
                "# Create a k-NN pipeline\n",
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_pipe.fit(feature_2, y_equidistant)\n",
                "# Plot the decision boundary. For that, we will assign a color to each\n",
                "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
                "feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1\n",
                "feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1\n",
                "xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))\n",
                "Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "Zo = Z.reshape(xx.shape)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Put the result into a color plot\n",
                "plt.figure(figsize=(18, 10))\n",
                "plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)\n",
                "#Label1=pd.DataFrame(list(class_index.items()))\n",
                "Label1=pd.DataFrame([1,2,3,4,5])\n",
                "# Plot also the training points\n",
                "plt.scatter(feature1, feature2, c=y_equidistant, cmap=cmap_bold)\n",
                "cbar = plt.colorbar() \n",
                "cbar.ax.set_yticklabels([Level1_equidistant,Level2_equidistant,Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant])\n",
                "plt.xlim(xx.min(), xx.max())\n",
                "plt.ylim(yy.min(), yy.max())\n",
                "plt.xlabel(\"feature 1\")\n",
                "plt.ylabel(\"feature 2\")\n",
                "plt.title(\"6-Class classification,   equidistant bins -  total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "knn_scores = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==6,\"method\"]=\"knn equidistant bins\"\n",
                "df_resultat.at[df_resultat.index==6,\"Cross-validation\"]=np.mean(knn_scores['test_score'])\n",
                "print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant)))\n",
                "print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score'])))\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_confusion=knn_pipe.fit(X_fea_2_tr_equid, y_fea_2_tr_equid)\n",
                "knn_predict = knn_pipe.predict(X_fea_2_te_equid)\n",
                "confusion_matrix(y_fea_2_te_equid, knn_predict)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_confusion_matrix(knn_confusion, X_fea_2_te_equid, y_fea_2_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "neighbors = list(range(1,25))\n",
                "CV_scores = []\n",
                "\n",
                "for n_neighborss in neighbors:\n",
                "    weights='distance'\n",
                "    feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "    feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "    h = .02  # step size in the mesh\n",
                "    # we create an instance of Neighbours Classifier and fit the data.\n",
                "    # Create a k-NN pipeline\n",
                "    knn_pipe = Pipeline([\n",
                "        ('scaler', StandardScaler()),\n",
                "        ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights))\n",
                "    ])\n",
                "    # Fit estimator\n",
                "    knn_pipe.fit(feature_2, y_balanced)\n",
                "    #\n",
                "    Z = knn_pipe.predict(feature_2)\n",
                "    scores = cross_val_score(knn_pipe,feature_2,y_balanced,cv=cv_strategy )\n",
                "    CV_scores.append(scores.mean())\n",
                "    \n",
                "Missclassification_Error = [1-x for x in CV_scores]\n",
                "print(\"the optimal k is \",format(Missclassification_Error.index(min(Missclassification_Error))))\n",
                "k_Optimal=Missclassification_Error.index(min(Missclassification_Error))\n",
                "    \n",
                "from matplotlib.legend_handler import HandlerLine2D\n",
                "plt.plot( Missclassification_Error)\n",
                "plt.xlabel(\"number of neighbors K\")\n",
                "plt.ylabel(\"Missclassification Error\")\n",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "# Create color maps\n",
                "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])\n",
                "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])\n",
                "\n",
                "feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "\n",
                "weights='distance'\n",
                "h = .02  # step size in the mesh\n",
                "# we create an instance of Neighbours Classifier and fit the data.\n",
                "# Create a k-NN pipeline\n",
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_pipe.fit(feature_2, y_balanced)\n",
                "# Plot the decision boundary. For that, we will assign a color to each\n",
                "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
                "feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1\n",
                "feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1\n",
                "xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))\n",
                "Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "Zo = Z.reshape(xx.shape)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Put the result into a color plot\n",
                "plt.figure(figsize=(18, 10))\n",
                "plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)\n",
                "#Label1=pd.DataFrame(list(class_index.items()))\n",
                "Label1=pd.DataFrame([1,2,3,4,5])\n",
                "# Plot also the training points\n",
                "plt.scatter(feature1, feature2, c=y_balanced, cmap=cmap_bold)\n",
                "cbar = plt.colorbar() \n",
                "cbar.ax.set_yticklabels([Level1_balanced,Level2_balanced,Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced])\n",
                "plt.xlim(xx.min(), xx.max())\n",
                "plt.ylim(yy.min(), yy.max())\n",
                "plt.xlabel(\"feature 1\")\n",
                "plt.ylabel(\"feature 2\")\n",
                "plt.title(\"6-Class classification,   balanced bins -  total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))\n",
                "\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "\n",
                "knn_scores = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==7,\"method\"]=\"knn balanced bins\"\n",
                "df_resultat.at[df_resultat.index==7,\"Cross-validation\"]=np.mean(knn_scores['test_score'])\n",
                "print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced)))\n",
                "print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score'])))\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_confusion=knn_pipe.fit(X_fea_2_tr_balan, y_fea_2_tr_balan)\n",
                "knn_predict = knn_pipe.predict(X_fea_2_te_balan)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "confusion_matrix(y_fea_2_te_balan, knn_predict)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_confusion_matrix(knn_confusion, X_fea_2_te_balan, y_fea_2_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_resultat"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.barplot(x=df_resultat[\"Cross-validation\"],y=df_resultat['method'])\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_time_confirmed"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "codiv_time_confirmed_grouped=codiv_time_confirmed.groupby('Country/Region').sum().reset_index()\n",
                "dfsum=codiv_time_confirmed_grouped.agg(['sum'])\n",
                "dfsum['Country/Region']=\"total_confirmed\"\n",
                "codiv_time_confirmed_grouped_sum=codiv_time_confirmed_grouped.append(dfsum)\n",
                "#\n",
                "codiv_time_recovered_grouped=codiv_time_recovered.groupby('Country/Region').sum().reset_index()\n",
                "dfsum=codiv_time_recovered_grouped.agg(['sum'])\n",
                "dfsum['Country/Region']=\"total_recovered\"\n",
                "codiv_time_recovered_grouped_sum=codiv_time_recovered_grouped.append(dfsum)\n",
                "#\n",
                "codiv_time_deaths_grouped=codiv_time_deaths.groupby('Country/Region').sum().reset_index()\n",
                "dfsum=codiv_time_deaths_grouped.agg(['sum'])\n",
                "dfsum['Country/Region']=\"total_deaths\"\n",
                "codiv_time_deaths_grouped_sum=codiv_time_deaths_grouped.append(dfsum)\n",
                "codiv_time_deaths_grouped_sum"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "#Recovered\n",
                "Start=1\n",
                "for countryInd in codiv_country_qurantine['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_qurantine_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_qurantine_recovered=codiv_country_qurantine_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd])\n",
                "#\n",
                "Start=1\n",
                "for countryInd in codiv_country_Restrictions['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_Restrictions_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_Restrictions_recovered=codiv_country_Restrictions_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd])\n",
                "#\n",
                "Start=1\n",
                "for countryInd in codiv_country_without_Restrictions_qurantine['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_without_Restrictions_qurantine_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_without_Restrictions_qurantine_recovered=codiv_country_without_Restrictions_qurantine_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]) \n",
                "#deaths   \n",
                "Start=1\n",
                "for countryInd in codiv_country_qurantine['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_qurantine_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_qurantine_deaths=codiv_country_qurantine_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd])\n",
                "#\n",
                "Start=1\n",
                "for countryInd in codiv_country_Restrictions['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_Restrictions_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_Restrictions_deaths=codiv_country_Restrictions_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd])\n",
                "#\n",
                "Start=1\n",
                "for countryInd in codiv_country_without_Restrictions_qurantine['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_without_Restrictions_qurantine_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_without_Restrictions_qurantine_deaths=codiv_country_without_Restrictions_qurantine_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]) \n",
                "#confirmed\n",
                "\n",
                "Start=1\n",
                "for countryInd in codiv_country_qurantine['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_qurantine_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_qurantine_confirmed=codiv_country_qurantine_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd])\n",
                "\n",
                "Start=1\n",
                "for countryInd in codiv_country_Restrictions['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_Restrictions_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_Restrictions_confirmed=codiv_country_Restrictions_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd])\n",
                "\n",
                "#codiv_country_Restrictions_confirmed=codiv_country_Restrictions_confirmed.set_index('Country/Region')\n",
                "#\n",
                "Start=1\n",
                "for countryInd in codiv_country_without_Restrictions_qurantine['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_without_Restrictions_qurantine_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_without_Restrictions_qurantine_confirmed=codiv_country_without_Restrictions_qurantine_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]) \n",
                "\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country_without_Restrictions_qurantine_active= codiv_country_without_Restrictions_qurantine_confirmed.set_index('Country/Region')-codiv_country_without_Restrictions_qurantine_deaths.set_index('Country/Region')-codiv_country_without_Restrictions_qurantine_recovered.set_index('Country/Region')\n",
                "codiv_country_qurantine_active= codiv_country_qurantine_confirmed.set_index('Country/Region')-codiv_country_qurantine_deaths.set_index('Country/Region')-codiv_country_qurantine_recovered.set_index('Country/Region')\n",
                "codiv_country_Restrictions_active= codiv_country_Restrictions_confirmed.set_index('Country/Region')- codiv_country_Restrictions_deaths.set_index('Country/Region')-codiv_country_Restrictions_recovered.set_index('Country/Region')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "import datetime\n",
                "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
                "                               AutoMinorLocator)\n",
                "row, col= codiv_country_qurantine_confirmed.shape\n",
                "fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))\n",
                "ts = pd.Series(\n",
                "    np.random.randn(col-1),\n",
                "    index=pd.date_range('20/1/2020', periods=col-1))\n",
                "df_qua_conf = pd.DataFrame((np.log1p(codiv_country_qurantine_confirmed.set_index('Country/Region').T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_qurantine_confirmed.set_index('Country/Region').index))\n",
                "df_res_conf = pd.DataFrame((np.log1p(codiv_country_Restrictions_confirmed.set_index('Country/Region').T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_Restrictions_confirmed.set_index('Country/Region').index))\n",
                "df_whi_conf = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_confirmed.set_index('Country/Region').T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_without_Restrictions_qurantine_confirmed.set_index('Country/Region').index))\n",
                "\n",
                "ax0.plot(df_qua_conf.index,df_qua_conf)\n",
                "ax1.plot(df_res_conf.index, df_res_conf)\n",
                "ax2.plot(df_whi_conf.index, df_whi_conf)\n",
                "ax0.set_title(\"quarantine_confirmed\")\n",
                "ax1.set_title(\"Restrictions_confirmed\")\n",
                "ax2.set_title(\"without_Restrictions_quarantine_confirmed\")\n",
                "ax0.legend(codiv_country_qurantine_confirmed['Country/Region'].tolist(),loc='upper left')\n",
                "ax1.legend(codiv_country_Restrictions_confirmed['Country/Region'].tolist(),loc='upper left')\n",
                "ax2.legend(codiv_country_without_Restrictions_qurantine_confirmed['Country/Region'].tolist(),loc='upper left')\n",
                "ax0.set_ylabel(\"quarantine_confirmed\")\n",
                "ax1.set_ylabel(\"Restrictions_confirmed\")\n",
                "ax2.set_ylabel(\"without_Restrictions_quarantine_confirmed\")\n",
                "\n",
                "ax0.grid(True)\n",
                "ax0.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax1.grid(True)\n",
                "ax1.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax2.grid(True)\n",
                "ax2.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax0.tick_params(axis=\"x\", rotation=45)\n",
                "ax1.tick_params(axis=\"x\", rotation=45)\n",
                "ax2.tick_params(axis=\"x\", rotation=45)\n",
                "####\n",
                "for countryindex in codiv_country_qurantine_confirmed['Country/Region']:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=\"2000-01-01\" and countryindex!=\"New Zealand\":\n",
                "            QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]\n",
                "            QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), \"%m/%d/%Y\")\n",
                "            Centervalue=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]\n",
                "            ax0.annotate('Quarantine', (QuarantineTimeD, Centervalue),\n",
                "                xytext=(0.2, 0.95), textcoords='axes fraction',\n",
                "                arrowprops=dict(facecolor='red', shrink=0.001),\n",
                "                fontsize=16,\n",
                "                horizontalalignment='right', verticalalignment='top')\n",
                "####\n",
                "for countryindex in codiv_country_Restrictions_confirmed['Country/Region']:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=\"2000-01-01\":\n",
                "            RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]\n",
                "            RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), \"%m/%d/%Y\")\n",
                "            Centervalue=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]\n",
                "            ax1.annotate('Restrictions', (RestrictionsTimeD, Centervalue),\n",
                "                xytext=(0.2, 0.95), textcoords='axes fraction',\n",
                "                arrowprops=dict(facecolor='red', shrink=0.001),\n",
                "                fontsize=16,\n",
                "                horizontalalignment='right', verticalalignment='top')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "import datetime\n",
                "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
                "                               AutoMinorLocator)\n",
                "row, col= codiv_country_qurantine_deaths.shape\n",
                "fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))\n",
                "ts = pd.Series(\n",
                "    np.random.randn(col-1),\n",
                "    index=pd.date_range('20/1/2020', periods=col-1))\n",
                "df_qua_conf = pd.DataFrame((np.log1p(codiv_country_qurantine_deaths.set_index('Country/Region').T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_qurantine_deaths.set_index('Country/Region').index))\n",
                "df_res_conf = pd.DataFrame((np.log1p(codiv_country_Restrictions_deaths.set_index('Country/Region').T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_Restrictions_deaths.set_index('Country/Region').index))\n",
                "df_whi_conf = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_deaths.set_index('Country/Region').T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_without_Restrictions_qurantine_deaths.set_index('Country/Region').index))\n",
                "\n",
                "ax0.plot(df_qua_conf.index,df_qua_conf)\n",
                "ax1.plot(df_res_conf.index, df_res_conf)\n",
                "ax2.plot(df_whi_conf.index, df_whi_conf)\n",
                "ax0.set_title(\"quarantine_deaths\")\n",
                "ax1.set_title(\"Restrictions_deaths\")\n",
                "ax2.set_title(\"without_Restrictions_quarantine_deaths\")\n",
                "ax0.legend(codiv_country_qurantine_deaths['Country/Region'].tolist(),loc='upper left')\n",
                "ax1.legend(codiv_country_Restrictions_deaths['Country/Region'].tolist(),loc='upper left')\n",
                "ax2.legend(codiv_country_without_Restrictions_qurantine_deaths['Country/Region'].tolist(),loc='upper left')\n",
                "ax0.set_ylabel(\"quarantine_deaths\")\n",
                "ax1.set_ylabel(\"Restrictions_deaths\")\n",
                "ax2.set_ylabel(\"without_Restrictions_quarantine_deaths\")\n",
                "\n",
                "ax0.grid(True)\n",
                "ax0.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax1.grid(True)\n",
                "ax1.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax2.grid(True)\n",
                "ax2.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax0.tick_params(axis=\"x\", rotation=45)\n",
                "ax1.tick_params(axis=\"x\", rotation=45)\n",
                "ax2.tick_params(axis=\"x\", rotation=45)\n",
                "####\n",
                "for countryindex in codiv_country_qurantine_deaths['Country/Region']:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=\"2000-01-01\" and countryindex!=\"New Zealand\":\n",
                "            QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]\n",
                "            QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), \"%m/%d/%Y\")\n",
                "            Centervalue=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]\n",
                "            ax0.annotate('Quarantine', (QuarantineTimeD, Centervalue),\n",
                "                xytext=(0.2, 0.95), textcoords='axes fraction',\n",
                "                arrowprops=dict(facecolor='red', shrink=0.001),\n",
                "                fontsize=16,\n",
                "                horizontalalignment='right', verticalalignment='top')\n",
                "####\n",
                "for countryindex in codiv_country_Restrictions_deaths['Country/Region']:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=\"2000-01-01\":\n",
                "            RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]\n",
                "            RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), \"%m/%d/%Y\")\n",
                "            Centervalue=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]\n",
                "            ax1.annotate('Restrictions', (RestrictionsTimeD, Centervalue),\n",
                "                xytext=(0.2, 0.95), textcoords='axes fraction',\n",
                "                arrowprops=dict(facecolor='red', shrink=0.001),\n",
                "                fontsize=16,\n",
                "                horizontalalignment='right', verticalalignment='top')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "import datetime\n",
                "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
                "                               AutoMinorLocator)\n",
                "row, col= codiv_country_qurantine_active.shape\n",
                "fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))\n",
                "ts = pd.Series(\n",
                "    np.random.randn(col),\n",
                "    index=pd.date_range('20/1/2020', periods=col))\n",
                "df_qua_conf = pd.DataFrame((np.log1p(codiv_country_qurantine_active.T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_qurantine_active.index))\n",
                "df_res_conf = pd.DataFrame((np.log1p(codiv_country_Restrictions_active.T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_Restrictions_active.index))\n",
                "df_whi_conf = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_active.T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_without_Restrictions_qurantine_active.index))\n",
                "\n",
                "ax0.plot(df_qua_conf.index,df_qua_conf)\n",
                "ax1.plot(df_res_conf.index, df_res_conf)\n",
                "ax2.plot(df_whi_conf.index, df_whi_conf)\n",
                "ax0.set_title(\"quarantine_active\")\n",
                "ax1.set_title(\"Restrictions_active\")\n",
                "ax2.set_title(\"without_Restrictions_quarantine_active\")\n",
                "ax0.legend(codiv_country_qurantine_deaths['Country/Region'].tolist(),loc='upper left')\n",
                "ax1.legend(codiv_country_Restrictions_deaths['Country/Region'].tolist(),loc='upper left')\n",
                "ax2.legend(codiv_country_without_Restrictions_qurantine_deaths['Country/Region'].tolist(),loc='upper left')\n",
                "ax0.set_ylabel(\"quarantine_active\")\n",
                "ax1.set_ylabel(\"Restrictions_active\")\n",
                "ax2.set_ylabel(\"without_Restrictions_qurantine_active\")\n",
                "\n",
                "ax0.grid(True)\n",
                "ax0.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax1.grid(True)\n",
                "ax1.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax2.grid(True)\n",
                "ax2.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax0.tick_params(axis=\"x\", rotation=45)\n",
                "ax1.tick_params(axis=\"x\", rotation=45)\n",
                "ax2.tick_params(axis=\"x\", rotation=45)\n",
                "####\n",
                "for countryindex in codiv_country_qurantine_deaths['Country/Region']:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=\"2000-01-01\" and countryindex!=\"New Zealand\":\n",
                "            QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]\n",
                "            QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), \"%m/%d/%Y\")\n",
                "            Centervalue=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]\n",
                "            ax0.annotate('Quarantine', (QuarantineTimeD, Centervalue),\n",
                "                xytext=(0.2, 0.95), textcoords='axes fraction',\n",
                "                arrowprops=dict(facecolor='red', shrink=0.001),\n",
                "                fontsize=16,\n",
                "                horizontalalignment='right', verticalalignment='top')\n",
                "####\n",
                "for countryindex in codiv_country_Restrictions_deaths['Country/Region']:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=\"2000-01-01\":\n",
                "            RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]\n",
                "            RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), \"%m/%d/%Y\")\n",
                "            Centervalue=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]\n",
                "            ax1.annotate('Restrictions', (RestrictionsTimeD, Centervalue),\n",
                "                xytext=(0.2, 0.95), textcoords='axes fraction',\n",
                "                arrowprops=dict(facecolor='red', shrink=0.001),\n",
                "                fontsize=16,\n",
                "                horizontalalignment='right', verticalalignment='top')"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from IPython.display import Image\n",
                "Image(filename=os.path.join('.//', 'shift.JPG'))"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# evalue the stadt date of the stats\n",
                "StartDate=codiv_country_Restrictions_active.T.index[0]\n",
                "print(\" Epidemy StartDate = \",StartDate)\n",
                "column_names = [\"date\"]\n",
                "dfdate = pd.DataFrame(columns = column_names)\n",
                "for Dateindex in range(20):\n",
                "    df_date1 = pd.DataFrame ([[str(pd.Timestamp(StartDate,unit=\"D\")+ pd.Timedelta(days=Dateindex))]], columns = [\"date\"])\n",
                "    dfdate=dfdate.append(df_date1, ignore_index = True)\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "# Root mean squared error (RMSE)\n",
                "def rmse(y, y_pred):\n",
                "    return np.sqrt(np.mean(np.square(y - y_pred)))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "import scipy.stats as stats\n",
                "import datetime\n",
                "def Polifq(Country,Version=\"restrictions\",shift=0,trigger=40):\n",
                "    # Version= works for restrictions, quarantin and without_Restrictions_qurantine.\n",
                "    if Version==\"restrictions\":\n",
                "        df_codiv=codiv_country_Restrictions_active.T[Country]\n",
                "    else:\n",
                "        if Version==\"quarantine\":\n",
                "            df_codiv=codiv_country_qurantine_active.T[Country]\n",
                "        else:\n",
                "            df_codiv=codiv_country_without_Restrictions_qurantine_active.T[Country]\n",
                "    #######################################\n",
                "    ######################## parameter setup\n",
                "    StartCountryIndex=0\n",
                "    ShiftIndex=0\n",
                "    x_tr1_China_model=0\n",
                "    y_tr1_China_model=0\n",
                "    highfactor=1.0\n",
                "    LargeFactor=1.0\n",
                "    ##############################################################################\n",
                "    ####################################### fitting the Country ##################\n",
                "    ######################## filter outlier for the Country\n",
                "    filter0 = np.abs(df_codiv - df_codiv.mean()) > (3 * df_codiv.std())\n",
                "    outliers = df_codiv.loc[filter0]\n",
                "    df_codiv = df_codiv.drop(outliers.index, axis=0)\n",
                "    ###########\n",
                "    # max actual value of the country\n",
                "    maxCountry=max(df_codiv)\n",
                "    #######################################\n",
                "    ########### Build the dataframe for the country\n",
                "    df_country = pd.DataFrame(columns = [Country,\"Value\",\"time\"], dtype='int')\n",
                "    for Index in range(0,len(df_codiv)):\n",
                "        df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [Country,\"Value\",\"time\"])\n",
                "        df_country=df_country.append(df_codiv1, ignore_index = True)\n",
                "    #######################################\n",
                "    #################### polyfit 10gr for the country value\n",
                "    x_tr1_Country=df_country[\"Value\"]\n",
                "    y_tr1_Country=df_country[Country]\n",
                "    #\n",
                "    # * Polyfit with degree 10\n",
                "    coefs_Country_poly10 = np.polyfit(x_tr1_Country, y_tr1_Country, deg=10) # Fit to train data\n",
                "    ##############################################################################\n",
                "    ##############################################################################\n",
                "    coefs_predict_poly10=0\n",
                "    #######################################\n",
                "    ############### Now shifting the china model to fitt to the country\n",
                "    if shift==1:\n",
                "        Start=1\n",
                "        ####### for the country\n",
                "        maxChina=max(codiv_country_qurantine_active.T[\"China\"])\n",
                "        for Index in range(0,len(df_codiv)):\n",
                "            #look for the start position of the country epidemy, it should be higher then the trigger value, it will be setup only by start of procedure ( STart=1)\n",
                "            if df_codiv[Index]>trigger*(maxCountry*1.0/maxChina) and Start==1 and shift==1:\n",
                "                \n",
                "                if maxCountry>maxChina:\n",
                "                    trigger=trigger*(maxCountry*1.5/maxChina)\n",
                "                StartCountryIndex=Index\n",
                "                Start=0\n",
                "            # register the index of the coutry max position\n",
                "            if df_codiv[Index]==maxCountry:\n",
                "                maxCountryIndex=Index\n",
                "        # deliver the actual distance between start and maximum for the country\n",
                "        LargeCountry=maxCountryIndex-StartCountryIndex\n",
                "        #################################\n",
                "        #######################################\n",
                "        ####### for china\n",
                "        df_codiv_China_ref=codiv_country_qurantine_active.T[\"China\"]\n",
                "        #######################################\n",
                "        ###### outiler\n",
                "        filter0 = np.abs(df_codiv_China_ref - df_codiv_China_ref.mean()) > (3 * df_codiv_China_ref.std())\n",
                "        outliers = df_codiv_China_ref.loc[filter0]\n",
                "        df_codiv = df_codiv_China_ref.drop(outliers.index, axis=0)\n",
                "        #######################################\n",
                "        ###### check for the start point by 40 infected\n",
                "        maxChina=max(df_codiv_China_ref)\n",
                "        Start=1\n",
                "        for Index in range(0,len(df_codiv_China_ref)):\n",
                "            if df_codiv_China_ref[Index]>60 and Start==1 and shift==1:\n",
                "                StartChinaIndex=Index\n",
                "                Start=0\n",
                "            if df_codiv_China_ref[Index]==maxChina:\n",
                "                maxChinaIndex=Index\n",
                "        #######################################\n",
                "        ###### start value for the fitting\n",
                "        highfactor=maxCountry*1.0/maxChina\n",
                "        LargeChina=maxChinaIndex-StartChinaIndex\n",
                "        LargeFactor=LargeCountry*1.0/LargeChina\n",
                "        ShiftIndex=StartCountryIndex-StartChinaIndex\n",
                "        ##########OPTIMISATION \n",
                "        step=0.02\n",
                "        ############# the target\n",
                "        # Predictions with the current a,b values\n",
                "        ################# generate the function country is china\n",
                "        df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n",
                "        for Index in range(0,len(df_codiv)):\n",
                "            df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n",
                "            df_country=df_country.append(df_codiv1, ignore_index = True)\n",
                "        x_tr1_China_model=df_country[\"Value\"]\n",
                "        y_tr1_China_model=df_country[\"China\"]      \n",
                "        error0 = rmse(y_tr1_China_model,y_tr1_Country)\n",
                "        error1=0\n",
                "        for runNr in range(1,60,1):\n",
                "            if error1<error0:\n",
                "                LargeFactor=LargeFactor+step\n",
                "                error0=error1\n",
                "                ################# generate the function\n",
                "                df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n",
                "                for Index in range(0,len(df_codiv)):\n",
                "                    df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n",
                "                    df_country=df_country.append(df_codiv1, ignore_index = True)\n",
                "                x_tr1_China_model=df_country[\"Value\"]\n",
                "                y_tr1_China_model=df_country[\"China\"] \n",
                "                error1 = rmse(y_tr1_China_model,y_tr1_Country)\n",
                "            if error1<error0:\n",
                "                LargeFactor=LargeFactor+step\n",
                "                error0=error1\n",
                "                ################# generate the function\n",
                "                df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n",
                "                for Index in range(0,len(df_codiv)):\n",
                "                    df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n",
                "                    df_country=df_country.append(df_codiv1, ignore_index = True)\n",
                "                x_tr1_China_model=df_country[\"Value\"]\n",
                "                y_tr1_China_model=df_country[\"China\"] \n",
                "                error1 = rmse(y_tr1_China_model,y_tr1_Country)\n",
                "            ##############\n",
                "            if error1<error0:\n",
                "                highfactor=highfactor+step\n",
                "                error0=error1\n",
                "                ################# generate the function\n",
                "                df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n",
                "                for Index in range(0,len(df_codiv)):\n",
                "                    df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n",
                "                    df_country=df_country.append(df_codiv1, ignore_index = True)\n",
                "                x_tr1_China_model=df_country[\"Value\"]\n",
                "                y_tr1_China_model=df_country[\"China\"] \n",
                "                error1 = rmse(y_tr1_China_model,y_tr1_Country)\n",
                "            #################\n",
                "            if error1<error0:\n",
                "                highfactor=highfactor+step\n",
                "                error0=error1\n",
                "                ################# generate the function\n",
                "                df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n",
                "                for Index in range(0,len(df_codiv)):\n",
                "                    df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n",
                "                    df_country=df_country.append(df_codiv1, ignore_index = True)\n",
                "                x_tr1_China_model=df_country[\"Value\"]\n",
                "                y_tr1_China_model=df_country[\"China\"] \n",
                "                error1 = rmse(y_tr1_China_model,y_tr1_Country)\n",
                "        #after having this fitting, we can fit the modified china data  on a 10grade polynome\n",
                "        #\n",
                "        # * Polyfit with degree 10\n",
                "        coefs_predict_poly10 = np.polyfit(x_tr1_China_model, y_tr1_China_model, deg=10) # Fit to train data\n",
                "    return coefs_Country_poly10,x_tr1_Country,y_tr1_Country,coefs_predict_poly10,x_tr1_China_model,y_tr1_China_model,StartCountryIndex"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# how much data are available for a country\n",
                "number,lendata=codiv_country_Restrictions_active.shape\n",
                "x_values = np.linspace(0, lendata+40, num=lendata+30)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#compute the ymax china\n",
                "coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(\"China\",shift=0,Version=\"quarantine\",trigger=70)\n",
                "lendataChina=max(x_tr_China)-7\n",
                "y_China = np.polyval(coefs_China, x_values)\n",
                "ymaxchina=int(max(y_tr_China)*1.5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "StartDate=codiv_country_Restrictions_active.T.index[0]\n",
                "column_names = [\"date\"]\n",
                "dfdate = pd.DataFrame(columns = column_names)\n",
                "for Dateindex in range(0,lendata*20,10):\n",
                "    df_date1 = pd.DataFrame ([[str(pd.Timestamp(StartDate,unit=\"D\")+ pd.Timedelta(days=Dateindex))]], columns = [\"date\"])\n",
                "    dfdate=dfdate.append(df_date1, ignore_index = True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "for countryindex in codiv_country_qurantine_active.T:\n",
                "#codiv_country_quarantine_active.T:\n",
                "    if countryindex==\"China\":\n",
                "        coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=0,Version=\"quarantine\",trigger=70)\n",
                "        y_Country = np.polyval(coefs_Country, x_values)\n",
                "        ymax=0\n",
                "    else:\n",
                "        coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=\"quarantine\",trigger=170)\n",
                "        y_Country = np.polyval(coefs_Country, x_values)\n",
                "        y_Country_prev = np.polyval(coefs_Country_prev, x_values)\n",
                "        #adjust the ymax of the graphic\n",
                "        ymax=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40)\n",
                "    if ymaxchina>ymax:\n",
                "        ymax=ymaxchina\n",
                "    #\n",
                "    fig = plt.figure(figsize= (18, 10))\n",
                "    #china reference country\n",
                "    plt.scatter(x_tr_China, y_tr_China, s=10)\n",
                "    plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=\"blue\")\n",
                "    #plot the second country\n",
                "    if countryindex!=\"China\":\n",
                "        plt.scatter(x_tr_Country, y_tr_Country, s=10)\n",
                "        plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=\"magenta\")\n",
                "        plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=\"black\")\n",
                "        PredictionName='%s-prediction'%(countryindex,)\n",
                "        plt.plot(x_values, y_Country_prev, label=PredictionName,c=\"brown\",ls='dashed')\n",
                "    plt.ylim(-5,ymax)\n",
                "    plt.ylabel(\"Number\")\n",
                "    plt.grid(True)\n",
                "    tickvalues = range(0,len(dfdate),10)\n",
                "    plt.xticks(ticks = tickvalues ,labels = dfdate[\"date\"], rotation = 75)\n",
                "    if countryindex!=\"China\":\n",
                "        Title='%s-prediction - Quarantine'%(countryindex,)\n",
                "        plt.title(Title)\n",
                "    else:\n",
                "        plt.title(\"China\")\n",
                "    Texte=\"\"\n",
                "    if countryindex==\"Belgium\":\n",
                "        Texte=\"Belgium only success to slow the infection. The model wait for the peak to fit better\"\n",
                "    if countryindex==\"France\":\n",
                "        Texte=\"France was relaxing the quarantine? the model need the next days data to fit better. the datas are not clear\"      \n",
                "    if countryindex==\"Germany\" or countryindex==\"Austria\":\n",
                "        Texte=\"Germany was relaxing the quarantine. but the model fit mostly good\"      \n",
                "    if countryindex==\"India\" or countryindex==\"Argentina\" or countryindex==\"Peru\" or countryindex==\"Colombia\":\n",
                "        Texte=\" The model wait for the peak to fit better\"       \n",
                "    if countryindex==\"Spain\":\n",
                "        Texte=\" Spain had no clear data, the model need the next days data to fit better\"  \n",
                "    if countryindex==\"China\":\n",
                "        Texte=\" China is the reference country, but we see they have to face to some epidemic restart ( imported case).\"  \n",
                "    plt.text(1, ymax-10000, Texte, fontsize=15)\n",
                "    plt.legend()\n",
                "    plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "number,lendata=codiv_country_Restrictions_active.shape\n",
                "x_values = np.linspace(0, lendata+40, num=lendata+30)\n",
                "coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(\"China\",shift=0,Version=\"quarantine\",trigger=70)\n",
                "lendataChina=max(x_tr_China)-7\n",
                "y_China = np.polyval(coefs_China, x_values)\n",
                "ymaxchina=int(max(y_tr_China)*1.5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "content": [
                "for countryindex in codiv_country_Restrictions_active.T:\n",
                "#codiv_country_quarantine_active.T:\n",
                "    # China is only in quarantine group\n",
                "    if countryindex==\"Japan\":\n",
                "        trigger=4500\n",
                "    else:\n",
                "        trigger=250\n",
                "    coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=\"restrictions\",trigger=trigger)\n",
                "    y_Country = np.polyval(coefs_Country, x_values)\n",
                "    y_Country_prev = np.polyval(coefs_Country_prev, x_values)\n",
                "    ymax=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40)\n",
                "    #adjust the ymax limit\n",
                "    if ymaxchina>ymax:\n",
                "        ymax=ymaxchina\n",
                "    #\n",
                "    fig = plt.figure(figsize= (18, 10))\n",
                "    plt.scatter(x_tr_China, y_tr_China, s=10)\n",
                "    plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=\"blue\")\n",
                "    if countryindex!=\"China\":\n",
                "        plt.scatter(x_tr_Country, y_tr_Country, s=10)\n",
                "        plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=\"magenta\")\n",
                "        plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=\"black\")\n",
                "        PredictionName='%s-prediction'%(countryindex,)\n",
                "        plt.plot(x_values, y_Country_prev, label=PredictionName,c=\"brown\",ls='dashed')\n",
                "    plt.ylim(-5,ymax)\n",
                "    plt.ylabel(\"Number\")\n",
                "    plt.grid(True)\n",
                "    tickvalues = range(0,len(dfdate),10)\n",
                "    plt.xticks(ticks = tickvalues ,labels = dfdate[\"date\"], rotation = 75)\n",
                "    if countryindex!=\"China\":\n",
                "        Title='%s-prediction - Restrictions'%(countryindex,)\n",
                "        plt.title(Title)\n",
                "    else:\n",
                "        plt.title(\"China\")\n",
                "    ##\n",
                "    Texte=\"\"\n",
                "    if countryindex==\"Ireland\":\n",
                "        Texte=\"Ireland had no clear data, the model need the next days data to fit better. Are the information trustfull?\"\n",
                "    if countryindex==\"Japan\":\n",
                "        Texte=\"Japan tried long time to contain the epidemy, after it gone up. the start threshold should be higher\" \n",
                "    if countryindex==\"Canada\" or countryindex==\"Netherlands\" or countryindex==\"Peru\" or countryindex==\"Colombia\"or countryindex==\"Portugal\"or countryindex==\"Sweden\"or countryindex==\"United Kingdom\":\n",
                "        Texte=\" The model wait for the peak to fit better\"     \n",
                "    if countryindex==\"Norway\" or countryindex==\"Poland\":\n",
                "        Texte=\" The model wait for the peak to fit better,they take long time to reach it\" \n",
                "    plt.text(1, ymax-10000, Texte, fontsize=15)\n",
                "    ##\n",
                "    plt.legend()\n",
                "    plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "number,lendata=codiv_country_Restrictions_active.shape\n",
                "x_values = np.linspace(0, lendata+40, num=lendata+30)\n",
                "coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(\"China\",shift=0,Version=\"quarantine\",trigger=70)\n",
                "lendataChina=max(x_tr_China)-7\n",
                "y_China = np.polyval(coefs_China, x_values)\n",
                "ymaxchina=int(max(y_tr_China)*1.5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "content": [
                "for countryindex in codiv_country_without_Restrictions_qurantine_active.T:\n",
                "#codiv_country_quarantine_active.T:\n",
                "    # china is only in quarntine group\n",
                "    if countryindex==\"Singapore\" or countryindex==\"Qatar\" or countryindex==\"Kuwait\":\n",
                "        trigger=2000\n",
                "    else:\n",
                "        trigger=300\n",
                "    coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=\"NoQuaNoRES\",trigger=trigger)\n",
                "    y_Country = np.polyval(coefs_Country, x_values)\n",
                "    y_Country_prev = np.polyval(coefs_Country_prev, x_values)\n",
                "    # adjust the ymax graph limit\n",
                "    if countryindex==\"US\":\n",
                "        ymax=1500000\n",
                "    else:\n",
                "        if countryindex==\"Brazil\":\n",
                "            ymax=180000\n",
                "        else:\n",
                "            ymax=60000\n",
                "    fig = plt.figure(figsize= (18, 10))\n",
                "    plt.scatter(x_tr_China, y_tr_China, s=10)\n",
                "    plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=\"blue\")\n",
                "    if countryindex!=\"China\":\n",
                "        plt.scatter(x_tr_Country, y_tr_Country, s=10)\n",
                "        plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=\"magenta\")\n",
                "        plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=\"black\")\n",
                "        PredictionName='%s-prediction'%(countryindex,)\n",
                "        plt.plot(x_values, y_Country_prev, label=PredictionName,c=\"brown\",ls='dashed')\n",
                "    plt.ylim(-5,ymax)\n",
                "    plt.ylabel(\"Number\")\n",
                "    plt.grid(True)\n",
                "    tickvalues = range(0,len(dfdate),10)\n",
                "    plt.xticks(ticks = tickvalues ,labels = dfdate[\"date\"], rotation = 75)\n",
                "    if countryindex!=\"China\":\n",
                "        Title='%s-prediction - No-Restrictions No-quarantine'%(countryindex,)\n",
                "        plt.title(Title)\n",
                "    else:\n",
                "        plt.title(\"China\")\n",
                "    ##\n",
                "    Texte=\"\"\n",
                "    if countryindex==\"Armenia\" or countryindex==\"Bahrain\" or countryindex==\"Bangladesh\" or countryindex==\"Belarus\"or countryindex==\"Brazil\"or countryindex==\"Chile\"or countryindex==\"United Kingdom\"or countryindex==\"US\":\n",
                "        Texte=\" The model wait for the peak to fit better\"  \n",
                "    if countryindex==\"Ecuador\" or countryindex==\"Indonesia\" or countryindex==\"Kuwait\" or countryindex==\"Mexico\" or countryindex==\"Oman\" or countryindex==\"Pakistan\" or countryindex==\"Quatar\" or countryindex==\"South Africa\":\n",
                "        Texte=\" The model wait for the peak to fit better\"\n",
                "    if countryindex==\"Dominican Republic\" or countryindex==\"Ghana\":\n",
                "        Texte=\" The model wait for more data to fit better\"\n",
                "    plt.text(1, ymax-10000, Texte, fontsize=15)\n",
                "    plt.legend()\n",
                "    plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(os.listdir('../input'))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "import matplotlib.pyplot as plt",
                "import seaborn as sns  # visualization tool",
                "",
                "# Input data files are available in the \"../input/\" directory.",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory",
                "",
                "import os",
                "print(os.listdir(\"../input\"))",
                "",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "forest_data = pd.read_csv(\"../input/forest-area-of-land-area/forest_area.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "forest_data.head(20)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "forest_data.info()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "happiness_data = pd.read_csv(\"../input/world-happiness/2015.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "happiness_data.info()",
                "happiness_data.head(5)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "new_forest_data = pd.concat([forest_data.iloc[:,:1],forest_data.iloc[:,-1]],axis=1) # getting country name and 2015 forest rate."
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "#merging happiness data and forest data, and then rename 2015 column to ForestRatio and Drop CountryName column which is dublicate because of merging.",
                "result = happiness_data.merge(new_forest_data, left_on='Country',right_on='CountryName') ",
                "result.rename(columns={'2015': 'ForestRatio'}, inplace=True)",
                "result.drop([\"CountryName\"], axis= 1, inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "result.head(20)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "main_data = result[pd.notnull(result.ForestRatio) & result.ForestRatio > 0] #ForestRatio should be not null and greater than 0",
                "main_data.columns = [each.replace(\" \",\"\").replace(\"(\",\"_\").replace(\")\",\"\") for each in main_data.columns]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "main_data.corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(x = main_data.HappinessScore, y = main_data.ForestRatio)",
                "plt.xlabel(\"Happiness Score\")",
                "plt.ylabel(\"Forest Ratio\")",
                "plt.title(\"Happiness Score vs Forest Ratio\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "#ok lets start to change our perspective",
                "new_data = main_data.groupby([\"Region\"]).mean()",
                "new_data.corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "new_data"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(x = new_data.ForestRatio, y = new_data.Freedom)",
                "plt.xlabel(\"Forest Ratio\")",
                "plt.ylabel(\"Freedom\")",
                "plt.title(\"Forest Ratio vs Freedom\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "happiness_data.columns = [ each.replace(\" \",\"\").replace(\"(\",\"_\").replace(\")\",\"\") for each in happiness_data.columns ]",
                "happiness_data.columns"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "happiness_data.corr()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "#correlation map",
                "f,ax = plt.subplots(figsize=(15, 15))",
                "sns.heatmap(happiness_data.corr(), annot=True, linewidths=.5, fmt= '.2f',ax=ax)",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "happiness_data.Family.plot(color = 'r',label = 'Family',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.Economy_GDPperCapita.plot(color = 'orange',label = 'Economy_GDPperCapita',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.Health_LifeExpectancy.plot(color = 'yellow',label = 'Health (Life Expectancy)',linewidth=1, alpha = 1.0,grid = True)",
                "happiness_data.Freedom.plot(color = 'g',label = 'Freedom',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.Trust_GovernmentCorruption.plot(color = 'black',label = 'Trust (Government Corruption)',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.Generosity.plot(color = 'b',label = 'Generosity',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.DystopiaResidual.plot(color = 'gray',label = 'Dystopia Residual',linewidth=1, alpha = 0.9,grid = True)",
                "",
                "plt.legend() ",
                "plt.xlabel('Happiness Rank')",
                "plt.ylabel('Score')",
                "plt.title('Happiness Factors Line Plot Graph')",
                "fig = plt.gcf()",
                "fig.set_size_inches(18.5, 10.5, forward=True)",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, axes = plt.subplots(nrows = 2, ncols = 4, figsize=(30, 15))",
                "happiness_data.plot(kind = \"scatter\", x= \"Family\", y = \"HappinessScore\", ax = axes[0][0])",
                "happiness_data.plot(kind = \"scatter\", x= \"Economy_GDPperCapita\", y = \"HappinessScore\", ax = axes[0][1])",
                "happiness_data.plot(kind = \"scatter\", x= \"Health_LifeExpectancy\", y = \"HappinessScore\", ax = axes[0][2])",
                "happiness_data.plot(kind = \"scatter\", x= \"Freedom\", y = \"HappinessScore\", ax = axes[0][3])",
                "happiness_data.plot(kind = \"scatter\", x= \"Trust_GovernmentCorruption\", y = \"HappinessScore\", ax = axes[1][0])",
                "happiness_data.plot(kind = \"scatter\", x= \"Generosity\", y = \"HappinessScore\", ax = axes[1][1])",
                "happiness_data.plot(kind = \"scatter\", x= \"DystopiaResidual\", y = \"HappinessScore\", ax = axes[1][2])",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, axes = plt.subplots(nrows = 2, ncols = 4, figsize=(30, 15))",
                "happiness_data.Family.plot(kind = 'hist',bins = 50, ax = axes[0][0])",
                "happiness_data.Economy_GDPperCapita.plot(kind = \"hist\", ax = axes[0][1])",
                "happiness_data.Health_LifeExpectancy.plot(kind = \"hist\", ax = axes[0][2])",
                "happiness_data.Freedom.plot(kind = \"hist\", ax = axes[0][3])",
                "happiness_data.Trust_GovernmentCorruption.plot(kind = \"hist\", ax = axes[1][0])",
                "happiness_data.Generosity.plot(kind = \"hist\", ax = axes[1][1])",
                "happiness_data.DystopiaResidual.plot(kind = \"hist\",  ax = axes[1][2])",
                "plt.show()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "happiness_data.boxplot(column='HappinessScore',by = 'Region', figsize=(30, 15))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "happiness_data[\"HappinessDegree\"] = ['Happy' if each > 6 else 'Normal' if each > 5 else 'Unhappy' for each in happiness_data.HappinessScore ]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "pivot_data = happiness_data.pivot_table( index=['Region'], columns = \"HappinessDegree\", values = \"HappinessRank\",aggfunc='count')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "pivot_data"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "class SimpleLinearRegression:",
                "    coef = 0",
                "    intercept = 0",
                "    rsquared = 0",
                "    def fit(self, x_train, y_train):",
                "        sum_of_x = sum(x_train)",
                "        sum_of_y = sum(y_train)",
                "        sum_of_x2 = np.sum(np.square(x_train))",
                "        sum_of_y2 = np.sum(np.square(y_train))",
                "        dotproduct = np.dot(x_train,y_train)",
                "        length = len(x_train)",
                "        dif_x = sum_of_x2 - sum_of_x * sum_of_x/length",
                "        dif_y = sum_of_y2 - sum_of_y * sum_of_y/length",
                "        numerator = length * dotproduct - sum_of_x * sum_of_y",
                "        denom = (length * sum_of_x2 - sum_of_x * sum_of_x) * (length * sum_of_y2 - (sum_of_y * sum_of_y))",
                "        co = dotproduct - sum_of_x * sum_of_y / length",
                "        self.rsquared = np.square(numerator / np.sqrt(denom))",
                "        self.intercept = sum_of_y / length - ((co / dif_x) * sum_of_x/length)",
                "        self.coef = co / dif_x",
                "    def predict(self,x_test):",
                "        return x_test * self.coef + self.intercept",
                "        "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "x_train = np.array([ 1, 2, 3, 4])",
                "y_train = np.array([ 2, 3, 4, 4])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "slr = SimpleLinearRegression()",
                "slr.fit(x_train,y_train)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(\"Coefficient:\", slr.coef)",
                "print('Y-Intercept:',slr.intercept)",
                "print('R-Squared:',slr.rsquared)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.linear_model import LinearRegression",
                "lr = LinearRegression()",
                "lr.fit(x_train.reshape(-1,1), y_train.reshape(-1,1))",
                "print(lr.coef_)",
                "print(lr.intercept_)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn import metrics \n",
                "from sklearn.metrics import r2_score\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "sales_data = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "sales_data.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "itemcategories_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\n",
                "items_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\n",
                "shops_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\n",
                "test_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "itemcategories_data.info()\n",
                "itemcategories_data.head()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "items_data.info()\n",
                "items_data.head()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "shops_data.info()\n",
                "shops_data.head()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test_data.info()\n",
                "test_data.head()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "shops_data.isnull().sum()\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "sales_data.isnull().sum()\n"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,4))\n",
                "sns.scatterplot(x=sales_data.item_cnt_day, y=sales_data.item_price, data=sales_data)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "sales_data = sales_data[sales_data.item_price<45000]\n",
                "sales_data = sales_data[sales_data.item_cnt_day<600]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,4))\n",
                "sns.scatterplot(x=sales_data.item_cnt_day, y=sales_data.item_price, data=sales_data)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "sales_train_sub = sales_data\n",
                "sales_train_sub['month'] = pd.DatetimeIndex(sales_train_sub['date']).month\n",
                "sales_train_sub['year'] = pd.DatetimeIndex(sales_train_sub['date']).year\n",
                "sales_train_sub.head(10)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "sats_grup = sales_train_sub.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\"item_cnt_day\"].agg('sum').reset_index()\n",
                "\n",
                "x=sats_grup.iloc[:,:-1]\n",
                "y=sats_grup.iloc[:,-1:]\n",
                "x_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.25, random_state=0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "x"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "from sklearn.ensemble import ExtraTreesRegressor\n",
                "etr = ExtraTreesRegressor(n_estimators=25,random_state=16)\n",
                "etr.fit(x_train,y_train.values.ravel())\n",
                "y_pred = etr.predict(x_test)\n",
                "\n",
                "\n",
                "print(\"R2 Score:\",r2_score(y_test,y_pred))\n",
                "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
                "print('Root Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred, squared=False))"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# Set up feedack system\n",
                "from learntools.core import binder\n",
                "binder.bind(globals())\n",
                "from learntools.sql.ex1 import *\n",
                "print(\"Setup Complete\")"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data"
            ],
            "content": [
                "from google.cloud import bigquery\n",
                "\n",
                "# Create a \"Client\" object\n",
                "client = bigquery.Client()\n",
                "\n",
                "# Construct a reference to the \"chicago_crime\" dataset\n",
                "dataset_ref = client.dataset(\"chicago_crime\", project=\"bigquery-public-data\")\n",
                "\n",
                "# API request - fetch the dataset\n",
                "dataset = client.get_dataset(dataset_ref)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "# Write the code you need here to figure out the answer\n",
                "tables = list(client.list_tables(dataset))"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# Write the code to figure out the answer\n",
                "table=client.get_table(dataset_ref.table(\"crime\"))\n",
                "table.schema"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "# Write the code here to explore the data so you can find the answer\n",
                "client.list_rows(table, max_results=5).to_dataframe()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "import matplotlib.pyplot as plt\n",
                "df=pd.read_csv('../input/boston-housing-dataset/HousingData.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(df)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                " df.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                " df.dtypes"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df['RAD'] = df['RAD'].astype('float64')\n",
                "df['TAX'] = df['TAX'].astype('float64')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                " df.dtypes"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.isnull().any()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.dropna(inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.isnull().any()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['CHAS'].value_counts(dropna=False)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df1 = df[['CRIM','ZN','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV','CHAS']]\n",
                "print(df1)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df1 = df1.dropna()"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X = df1.iloc[:, :-1].values\n",
                "y = df1.iloc[:,13].values"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df1.dtypes"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train,X_test,y_train,y_test =train_test_split(X,y,test_size =0.2,random_state =0)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_train"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y_train"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "from sklearn.linear_model import LinearRegression\n",
                "regressor = LinearRegression()\n",
                "regressor.fit(X_train,y_train)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "y_pred =regressor.predict(X_test)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "import statsmodels.formula.api as sm\n",
                "X = np.append(arr=np.ones((394,1)).astype(int),values=X,axis=1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(X)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import statsmodels.api as sm"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "X_opt = X[:,:13] \n",
                "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
                "regressor_OLS.summary()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "X_opt =X[:,[1,3,5,7,8,9,10,11,12]]\n",
                "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
                "regressor_OLS.summary()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "X_opt =X[:,[1,3,5,7,8,9,10,11]]\n",
                "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
                "regressor_OLS.summary()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "X_opt =X[:,[3,5,8,9,10,11]]\n",
                "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
                "regressor_OLS.summary()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "X_opt =X[:,[0,1,4,10,14]]\n",
                "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
                "regressor_OLS.summary()"
            ],
            "output_type": "error"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "from sklearn.model_selection import KFold\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "train_data = pd.read_csv('../input/train.csv')\n",
                "test_data = pd.read_csv('../input/test.csv')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "features = train_data.columns[2:]\n",
                "\n",
                "numeric_features = []\n",
                "categorical_features = []\n",
                "\n",
                "for dtype, feature in zip(train_data.dtypes[2:], train_data.columns[2:]):\n",
                "    if dtype == object:\n",
                "        #print(column)\n",
                "        #print(train_data[column].describe())\n",
                "        categorical_features.append(feature)\n",
                "    else:\n",
                "        numeric_features.append(feature)\n",
                "categorical_features"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# This way we have randomness and are able to reproduce the behaviour within this cell.\n",
                "np.random.seed(13)\n",
                "\n",
                "def impact_coding(data, feature, target='y'):\n",
                "    '''\n",
                "    In this implementation we get the values and the dictionary as two different steps.\n",
                "    This is just because initially we were ignoring the dictionary as a result variable.\n",
                "    \n",
                "    In this implementation the KFolds use shuffling. If you want reproducibility the cv \n",
                "    could be moved to a parameter.\n",
                "    '''\n",
                "    n_folds = 20\n",
                "    n_inner_folds = 10\n",
                "    impact_coded = pd.Series()\n",
                "    \n",
                "    oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)\n",
                "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
                "    oof_mean_cv = pd.DataFrame()\n",
                "    split = 0\n",
                "    for infold, oof in kf.split(data[feature]):\n",
                "            impact_coded_cv = pd.Series()\n",
                "            kf_inner = KFold(n_splits=n_inner_folds, shuffle=True)\n",
                "            inner_split = 0\n",
                "            inner_oof_mean_cv = pd.DataFrame()\n",
                "            oof_default_inner_mean = data.iloc[infold][target].mean()\n",
                "            for infold_inner, oof_inner in kf_inner.split(data.iloc[infold]):\n",
                "                # The mean to apply to the inner oof split (a 1/n_folds % based on the rest)\n",
                "                oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean()\n",
                "                impact_coded_cv = impact_coded_cv.append(data.iloc[infold].apply(\n",
                "                            lambda x: oof_mean[x[feature]]\n",
                "                                      if x[feature] in oof_mean.index\n",
                "                                      else oof_default_inner_mean\n",
                "                            , axis=1))\n",
                "\n",
                "                # Also populate mapping (this has all group -> mean for all inner CV folds)\n",
                "                inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')\n",
                "                inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)\n",
                "                inner_split += 1\n",
                "\n",
                "            # Also populate mapping\n",
                "            oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')\n",
                "            oof_mean_cv.fillna(value=oof_default_mean, inplace=True)\n",
                "            split += 1\n",
                "            \n",
                "            impact_coded = impact_coded.append(data.iloc[oof].apply(\n",
                "                            lambda x: inner_oof_mean_cv.loc[x[feature]].mean()\n",
                "                                      if x[feature] in inner_oof_mean_cv.index\n",
                "                                      else oof_default_mean\n",
                "                            , axis=1))\n",
                "\n",
                "    return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean\n",
                "\n",
                "# Apply the encoding to training and test data, and preserve the mapping\n",
                "impact_coding_map = {}\n",
                "for f in categorical_features:\n",
                "    print(\"Impact coding for {}\".format(f))\n",
                "    train_data[\"impact_encoded_{}\".format(f)], impact_coding_mapping, default_coding = impact_coding(train_data, f)\n",
                "    impact_coding_map[f] = (impact_coding_mapping, default_coding)\n",
                "    mapping, default_mean = impact_coding_map[f]\n",
                "    test_data[\"impact_encoded_{}\".format(f)] = test_data.apply(lambda x: mapping[x[f]]\n",
                "                                                                         if x[f] in mapping\n",
                "                                                                         else default_mean\n",
                "                                                               , axis=1)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_data[['y', 'X0'] + list(train_data.columns[-8:])]"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "#STEP 1: Get right arrows in quiver\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "\n",
                "from sklearn.preprocessing import StandardScaler,normalize\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.mixture import GaussianMixture as GMM\n",
                "from sklearn.manifold import TSNE\n",
                "\n",
                "import warnings #To hide warnings\n",
                "\n",
                "#1.1: Set the stage\n",
                "from IPython.core.interactiveshell import InteractiveShell\n",
                "from IPython.core.display import display, HTML\n",
                "\n",
                "InteractiveShell.ast_node_interactivity = \"all\"\n",
                "warnings.filterwarnings(\"ignore\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "transfer_results",
                "validate_data"
            ],
            "content": [
                "cc = pd.read_csv('/kaggle/input/ccdata/CC GENERAL.csv')\n",
                "cc.info()\n",
                "cc[cc.columns[cc.isna().any()]].isna().sum().to_frame().T\n",
                "cc.sample(5)\n",
                "cc.describe()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "cc.quantile([0.75,0.8,.85,.9,.95,1])"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "display(HTML('<h4>There are '+str(np.sum(cc.BALANCE>cc.CREDIT_LIMIT))\n",
                "             +' customers in the list who have more balance than the credit limit assigned. '\n",
                "             +'It may be due to more payament than usage and/or continuous pre-payment.</h4>'))"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "cc.rename(columns = {col:col.lower() for col in cc.columns.values},inplace=True)\n",
                "sns.jointplot(cc.credit_limit,cc.minimum_payments,kind = 'kde', dropna=True)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "cc.fillna(cc.median(),inplace=True) #More outliers thus median in both cases\n",
                "cust = cc.cust_id\n",
                "cc.drop(columns = ['cust_id'],inplace=True)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ss = StandardScaler()\n",
                "X= normalize(ss.fit_transform(cc.copy()))\n",
                "X = pd.DataFrame(X,columns=cc.columns.values)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "fig, axs = plt.subplots(6,3, figsize=(20, 20))\n",
                "for i in range(17):\n",
                "        p = sns.distplot(cc[cc.columns[i]], ax=axs[i//3,i%3],kde_kws = {'bw':2})\n",
                "        p = sns.despine()\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "X.boxplot(figsize = (30,25),grid=True,fontsize=25,rot=90)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(16,12))\n",
                "p = sns.heatmap(cc.corr(),annot=True,cmap='jet').set_title(\"Correlation of credit card data\\'s features\",fontsize=20)\n",
                "plt.show()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "content": [
                "#Selecting correct number of components for GMM\n",
                "models = [GMM(n,random_state=0).fit(X) for n in range(1,12)]\n",
                "d = pd.DataFrame({'BIC Score':[m.bic(X) for m in models],\n",
                "                  'AIC Score': [m.aic(X) for m in models]},index=np.arange(1,12))\n",
                "d.plot(use_index=True,title='AIC and BIC Scores for GMM wrt n_Compnents',figsize = (10,5),fontsize=12)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "from sklearn.base import ClusterMixin\n",
                "from yellowbrick.cluster import KElbow\n",
                "\n",
                "class GMClusters(GMM, ClusterMixin):\n",
                "\n",
                "    def __init__(self, n_clusters=1, **kwargs):\n",
                "        kwargs[\"n_components\"] = n_clusters\n",
                "        kwargs['covariance_type'] = 'full'\n",
                "        super(GMClusters, self).__init__(**kwargs)\n",
                "\n",
                "    def fit(self, X):\n",
                "        super(GMClusters, self).fit(X)\n",
                "        self.labels_ = self.predict(X)\n",
                "        return self \n",
                "\n",
                "oz = KElbow(GMClusters(), k=(2,12), force_model=True)\n",
                "oz.fit(X)\n",
                "oz.show()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "model= models[6]\n",
                "model.n_init = 10\n",
                "model"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "visualize_data"
            ],
            "content": [
                "clusters = model.fit_predict(X)\n",
                "display(HTML('<b>The model has converged :</b>'+str(model.converged_)))\n",
                "display(HTML('<b>The model has taken iterations :</b>'+str(model.n_iter_)))"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(clusters).set_title('Cluster sizes',fontsize=20)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "cc1 = cc.copy()\n",
                "cc1['cluster']=clusters\n",
                "for c in cc1:\n",
                "    if c != 'cluster':\n",
                "        grid= sns.FacetGrid(cc1, col='cluster',sharex=False,sharey=False)\n",
                "        p = grid.map(sns.distplot, c,kde_kws = {'bw':2})\n",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data",
                "validate_data"
            ],
            "content": [
                "#cc1.groupby('cluster').agg({np.min,np.max,np.mean}).T\n",
                "for i in range(7):\n",
                "    display(HTML('<h2>Cluster'+str(i)+'</h2>'))\n",
                "    cc1[cc1.cluster == i].describe()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "tsne = TSNE(n_components = 2)\n",
                "tsne_out = tsne.fit_transform(X.copy())\n",
                "\n",
                "plt.scatter(tsne_out[:, 0], tsne_out[:, 1],\n",
                "            marker=10,\n",
                "            s=10,              # marker size\n",
                "            linewidths=5,      # linewidth of marker edges\n",
                "            c=clusters   # Colour as per gmm\n",
                "            )"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "density = model.score_samples(X)\n",
                "density_threshold = np.percentile(density,4)\n",
                "cc1['cluster']=clusters\n",
                "cc1['Anamoly'] = density<density_threshold\n",
                "cc1"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df = cc1.melt(['Anamoly'], var_name='cols',  value_name='vals')\n",
                "\n",
                "g = sns.FacetGrid(df, row='cols', hue=\"Anamoly\", palette=\"Set1\",sharey=False,sharex=False,aspect=3)\n",
                "g = (g.map(sns.distplot, \"vals\", hist=True, rug=True,kde_kws = {'bw':2}).add_legend())"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "unanomaly = X[density>=density_threshold]\n",
                "c = clusters[density>=density_threshold]\n",
                "tsne = TSNE(n_components = 2)\n",
                "tsne_out = tsne.fit_transform(unanomaly)\n",
                "plt.figure(figsize=(15,10))\n",
                "plt.scatter(tsne_out[:, 0], tsne_out[:, 1],marker='x',s=10, linewidths=5, c=c)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "!pip install ycimpute\n",
                "\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn import datasets, metrics, model_selection, svm\n",
                "import missingno as msno\n",
                "from ycimpute.imputer import iterforest,EM\n",
                "from fancyimpute import KNN\n",
                "from sklearn.preprocessing import OrdinalEncoder\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd \n",
                "import statsmodels.api as sm\n",
                "import statsmodels.formula.api as smf\n",
                "import seaborn as sns\n",
                "from sklearn.preprocessing import scale \n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
                "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
                "from sklearn.metrics import roc_auc_score,roc_curve\n",
                "import statsmodels.formula.api as smf\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "from xgboost import XGBClassifier\n",
                "from lightgbm import LGBMClassifier\n",
                "from catboost import CatBoostClassifier\n",
                "\n",
                "from warnings import filterwarnings\n",
                "filterwarnings('ignore')\n",
                "\n",
                "pd.set_option('display.max_columns', None)\n",
                "import gc"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "%%time\n",
                "# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
                "# WARNING! THIS CAN DAMAGE THE DATA \n",
                "def reduce_mem_usage2(df):\n",
                "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
                "        to reduce memory usage.        \n",
                "    \"\"\"\n",
                "    start_mem = df.memory_usage().sum() / 1024**2\n",
                "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
                "    \n",
                "    for col in df.columns:\n",
                "        col_type = df[col].dtype\n",
                "        \n",
                "        if col_type != object:\n",
                "            c_min = df[col].min()\n",
                "            c_max = df[col].max()\n",
                "            if str(col_type)[:3] == 'int':\n",
                "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
                "                    df[col] = df[col].astype(np.int8)\n",
                "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
                "                    df[col] = df[col].astype(np.int16)\n",
                "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
                "                    df[col] = df[col].astype(np.int32)\n",
                "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
                "                    df[col] = df[col].astype(np.int64)  \n",
                "            else:\n",
                "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
                "                    df[col] = df[col].astype(np.float16)\n",
                "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
                "                    df[col] = df[col].astype(np.float32)\n",
                "                else:\n",
                "                    df[col] = df[col].astype(np.float64)\n",
                "        else:\n",
                "            df[col] = df[col].astype('category')\n",
                "\n",
                "    end_mem = df.memory_usage().sum() / 1024**2\n",
                "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
                "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
                "    \n",
                "    return df"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "encoder=OrdinalEncoder()\n",
                "imputer=KNN()\n",
                "\n",
                "def encode(data):\n",
                "    '''function to encode non-null data and replace it in the original data'''\n",
                "    #retains only non-null values\n",
                "    nonulls = np.array(data.dropna())\n",
                "    #reshapes the data for encoding\n",
                "    impute_reshape = nonulls.reshape(-1,1)\n",
                "    #encode date\n",
                "    impute_ordinal = encoder.fit_transform(impute_reshape)\n",
                "    #Assign back encoded values to non-null values\n",
                "    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n",
                "    return data"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "Ktest_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv')\n",
                "Ktest_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")\n",
                "Ktrain_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\n",
                "Ktrain_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Ktrain= pd.merge(Ktrain_transaction, Ktrain_identity, on='TransactionID', how='left', left_index=True, right_index=True)\n",
                "Ktest= pd.merge(Ktest_transaction, Ktest_identity, on='TransactionID', how='left', left_index=True, right_index=True)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Ktrain_cat=Ktrain.select_dtypes(include='object')\n",
                "Ktest_cat =Ktest.select_dtypes(include='object')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Ktrain_cat1=Ktrain_cat.drop(['P_emaildomain','R_emaildomain','id_30','id_31','id_33','DeviceInfo'], axis=1)\n",
                "Ktest_cat1=Ktest_cat.drop(['P_emaildomain','R_emaildomain','id-30','id-31','id-33','DeviceInfo'], axis=1)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for i in Ktrain_cat1:\n",
                "    encode(Ktrain[i])\n",
                "for i in Ktest_cat1:\n",
                "    encode(Ktest[i])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Ktrain_cat2=pd.concat([Ktrain['P_emaildomain'],Ktrain['R_emaildomain'],Ktrain['id_30'],Ktrain['id_31'],Ktrain['id_33'],Ktrain['DeviceInfo']], axis=1)\n",
                "Ktest_cat2=pd.concat([Ktest['P_emaildomain'],Ktest['R_emaildomain'],Ktest['id-30'],Ktest['id-31'],Ktest['id-33'],Ktest['DeviceInfo']], axis=1)\n"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for i in Ktrain_cat2:\n",
                "    encode(Ktrain[i])\n",
                "for i in Ktest_cat2:\n",
                "    encode(Ktest[i])"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import gc\n",
                "del Ktest_identity\n",
                "del Ktest_transaction\n",
                "del Ktrain_identity\n",
                "del Ktrain_transaction\n",
                "del Ktrain_cat1\n",
                "del Ktest_cat1\n",
                "del Ktrain_cat2\n",
                "del Ktest_cat2\n",
                "gc.collect()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Ktest = reduce_mem_usage2(Ktest)\n",
                "Ktrain = reduce_mem_usage2(Ktrain)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Ktrain.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Ktest.shape"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Ktest.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "z= Ktest.loc[:,'id-01':'id-38'].columns.str.replace('-','_')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "z=list(z)\n",
                "z"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for x,y in zip(Ktest.loc[:,'id-01':'id-38'].columns, z):\n",
                "    Ktest[y]=Ktest[x]\n",
                "    del Ktest[x]\n",
                "gc.collect()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "y=Ktrain[\"isFraud\"]\n",
                "X=Ktrain.drop([\"isFraud\", \"TransactionID\"], axis=1).astype('float64')\n",
                "X= X.fillna(-999)\n",
                "\n",
                "Ktest_id = Ktest['TransactionID']\n",
                "X_Ktest = Ktest.drop(['TransactionID'], axis=1).astype('float64')\n",
                "X_Ktest = X_Ktest.fillna(-999)\n",
                "\n",
                "X_Ktest = X_Ktest[X.columns]"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data",
                "process_data"
            ],
            "content": [
                "'''\n",
                "X_fit=StandardScaler().fit_transform(X)\n",
                "X_pca=PCA().fit(X_fit)\n",
                "plt.plot(np.cumsum(X_pca.explained_variance_ratio_))\n",
                "plt.title('All columns included', color='gray')\n",
                "plt.xlabel(\"Number of Component\", color='green')\n",
                "plt.ylabel(\"Cumulative Variance Ratio\", color='green')\n",
                "plt.grid(color='gray', linestyle='-', linewidth=0.3)\n",
                "plt.show()\n",
                "\n",
                "X_Ktest_fit=StandardScaler().fit_transform(X_Ktest)\n",
                "X_Ktest_pca=PCA().fit(X_Ktest_fit)\n",
                "plt.plot(np.cumsum(X_Ktest_pca.explained_variance_ratio_))\n",
                "plt.title('All columns included', color='gray')\n",
                "plt.xlabel(\"Number of Component\", color='green')\n",
                "plt.ylabel(\"Cumulative Variance Ratio\", color='green')\n",
                "plt.grid(color='gray', linestyle='-', linewidth=0.3)\n",
                "plt.show()\n",
                "'''"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "# PCA Analysis is misleading here because we filled missing data. \n",
                "\n",
                "\"\"\"\n",
                "#Final Model for Ktrain\n",
                "X_pca = PCA(n_components=100).fit(X_fit)\n",
                "X_fit = X_pca.fit_transform(X_fit)\n",
                "X_pca.explained_variance_ratio_\n",
                "\n",
                "#Final Model for Ktest\n",
                "\n",
                "X_Ktest_pca = PCA(n_components=100).fit(X_Ktest_fit)\n",
                "X_Ktest_fit = X_Ktest_pca.fit_transform(X_Ktest_fit)\n",
                "print(X_pca.explained_variance_ratio_.sum())\n",
                "print(X_Ktest_pca.explained_variance_ratio_.sum())\n",
                "\"\"\""
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "'''\n",
                "xlist=[]\n",
                "for i in range(1,101):\n",
                "    xlist.append(\"a\"+str(i))\n",
                "\n",
                "X_new= pd.DataFrame(data=X_fit, columns= xlist)\n",
                "X_Ktest_new= pd.DataFrame(data=X_Ktest_fit, columns= xlist)\n",
                "X_Ktest_new.head()\n",
                "'''"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.25, random_state=42)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X = reduce_mem_usage2(X)\n",
                "X_Ktest = reduce_mem_usage2(X_Ktest)"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "gc.collect()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(Ktrain.shape)\n",
                "print(Ktest.shape)\n",
                "print(X.shape)\n",
                "print(X_Ktest.shape)\n"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "# It gets so much time to run with each models, so passed for now.\n",
                "\n",
                "'''models = [LogisticRegression,\n",
                "          KNeighborsClassifier,\n",
                "          GaussianNB,\n",
                "          SVC,\n",
                "          DecisionTreeClassifier,\n",
                "          RandomForestClassifier,\n",
                "          GradientBoostingClassifier,\n",
                "          LGBMClassifier,\n",
                "          XGBClassifier\n",
                "          #CatBoostClassifier\n",
                "         ]\n",
                "\n",
                "def compML (df, y, algorithm):\n",
                "    \n",
                "    #y=df[y]\n",
                "    #X=df.drop([\"PassengerId\",\"Survived\"], axis=1).astype('float64')\n",
                "    #X_train, X_test,y_train,y_test=train_test_split(X,y, test_size=0.25, random_state=42)\n",
                "    \n",
                "    model=algorithm().fit(X_train, y_train)\n",
                "    y_pred=model.predict(X_test)\n",
                "    accuracy= accuracy_score(y_test, y_pred)\n",
                "    #return accuracy\n",
                "    model_name= algorithm.__name__\n",
                "    print(model_name,\": \",accuracy)\n",
                "    \n",
                "    \n",
                "for i in models:\n",
                "    compML(X,\"isFraud\",i)\n",
                "    \n",
                "'''"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "model= LGBMClassifier().fit(X_train, y_train)\n",
                "y_pred=model.predict(X_test)\n",
                "accuracy_score(y_test,y_pred)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "model"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results"
            ],
            "content": [
                "predictions=model.predict(X_Ktest)\n",
                "output=pd.DataFrame({\"TransactionID\":Ktest_id, \"isFraud\":predictions})\n",
                "output.to_csv('submission_model.csv', index=False)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "import random as rd",
                "import matplotlib.pyplot as plt"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### create Student Class",
                "class Student:",
                "",
                "  def __init__(self, id, shyness, attitude, cap, usageRate, year):",
                "    self.id = id",
                "",
                "    ##characteristics (integers)",
                "    #low score means more shy (normal distribution, mean 0 and s.d 1)",
                "    self.shyness = shyness",
                "    '''",
                "    attitude towards drinking ",
                "    (normal distribution, mean is MeanAttitude and s.d 1)",
                "    determines usagerate, whether student may drink alone and addiction ",
                "    '''",
                "    self.attitude = attitude",
                "    '''",
                "    updated at the end of the school year",
                "    if below a threshold, student can't continue next year (inSchool = 0)",
                "    '''",
                "    self.cap = cap",
                "    #number of times drinking (in the past 30 days?)>should update every 30 or just keep it for the whole sem?",
                "    self.usageRate = usageRate",
                "",
                "    self.inSchool = True    ",
                "    self.year = year",
                "",
                "    ##social life (lists)",
                "    self.friends = []",
                "    #list of days they are hosting a party in the next week, and a list of attendees",
                "    self.host = []",
                "    self.guestList = []",
                "    #list of days student will attend someone else's party",
                "    self.whenAttend = []",
                "",
                "",
                "  def host_party(self, student_list):   ",
                "    #determine if hosting that week,",
                "    attending = self.whenAttend     ",
                "    p_host = (0.2 * self.shyness**2) - (len(attending))/5",
                "    num = rd.random()",
                "    if num <= p_host:",
                "",
                "      #choose which day to host",
                "      week = ['Sun','Mon','Tue','Wed','Thu','Fri','Sat']",
                "      for a in attending:",
                "        if a in week:",
                "          week.remove(a)",
                "      #",
                "      day = rd.choices(week, weights = [0.05,0.05,0.05,0.05,0.1,0.35,0.35])",
                "      self.host.append(day)   ",
                "",
                "      #send out invites",
                "      ",
                "      for others in student_list:",
                "        #is there a better way to select the respective students based on their id?",
                "        #two nested for loops - might get hard over large student populations",
                "        if others.id in self.friends:",
                "          others.willAttend(day) ",
                "          if day in others.whenAttend:",
                "            self.guestList.append(others.id)",
                "  ",
                "  #when student gets invited to a party",
                "  def willAttend(self, day):",
                "    if day not in self.whenAttend:",
                "      num = rd.random()",
                "      weekend = ['Sun','Sat']",
                "      if day in weekend:",
                "        if num <= 0.8:",
                "          self.whenAttend.append(day)",
                "      else:",
                "        if num <= 0.4:",
                "          self.whenAttend.append(day)",
                "       ",
                "  def partyDrink(self, peer_pressure):",
                "    # arbitrary coefficients ",
                "    # limits - self.attitude = [2, 3]",
                "    #        - attitudeUse = [1,1]",
                "    #        - peer_pressure = [0, 1]",
                "    #        - usageRate = []",
                "    #        - usageRateUse = []",
                "    # take note of the limits",
                "    p_drink = (self.attitude/ attitudeUse) + (2*peer_pressure) - 0.1*(self.usageRate/usageRateUse)",
                "    num = rd.random()",
                "    if num <= p_drink:",
                "      self.usageRate += 1",
                "      self.experience()",
                "      return 1",
                "    return 0",
                "     ",
                "      ",
                "",
                "  def drinkAlone(self):",
                "    if self.attitude > 3:",
                "      p_alone = self.attitude/15",
                "      num = rd.random()",
                "      if num <= p_alone:",
                "        self.usageRate += 1",
                "",
                "  #after a drink at the party  ",
                "  def experience(self):",
                "    num = rd.noramlvariate(meanExperience, stDevExperience)",
                "    if num < -6:",
                "      num = -6",
                "    elif num > 3:",
                "      num = 3",
                "",
                "    #if party gets busted",
                "    bust = rd.random()",
                "    if bust < probabilityofBust:",
                "      num -= 3",
                "",
                "    #update attitude towards drinking",
                "    # attitude update is arbitrary at 0.1",
                "    self.attitude += num*0.1",
                "#    self.attitude += num*0.5",
                "",
                "    ",
                "  #every 3 weeks",
                "  def gradeExperience(self):",
                "    if self.usageRate > usageRateGradeDrop:",
                "      poor_exp = -1*(self.usageRate/15)",
                "      self.attitude += (poor_exp * 0.2)",
                "",
                "  #at the end of the semester",
                "  def gradeUpdate(self):",
                "    std_dev = 0.2/self.year",
                "    change = rd.normalvariate(0.1,std_dev)",
                "",
                "    #add on to grade drop due to drinking",
                "    if self.usageRate > usageRateGradeDrop:",
                "      mean_drop = self.usageRate/(30*self.year)",
                "      s_dev = 0.5/self.year",
                "      grade_drop = rd.normalvariate(mean_drop,s_dev)",
                "      change += grade_drop",
                "    ",
                "    student.cap -= change    "
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### students interacting",
                "",
                "def make_friends(student1, student2):",
                "  if student2.id not in student1.friends:",
                "    shy1, shy2 = student1.shyness, student2.shyness",
                "    fr1,fr2 = len(student1.friends), len(student2.friends)",
                "    att1, att2 = student1.attitude, student2.attitude",
                "    p = ((wShy*(shy1 + shy2 + 4)) - (wFr*((fr1/(3+shy1)) + (fr2/(3+shy2)))) - (wAtt*(att1 - att2))) / (8*wShy)",
                "    num = rd.random()",
                "    if num <= p:",
                "      student1.friends.append(student2.id) ",
                "      student2.friends.append(student1.id)",
                "",
                "#for all students at the party",
                "def party_friends(attendees):",
                "  ",
                "  for guest in attendees:",
                "    for others in attendees: ",
                "      if guest != others:",
                "        make_friends(guest, others)",
                "  #will it be running the function twice for some of them? How to avoid?",
                "",
                "#when there's a party host",
                "def party_time(host, student_list, day):",
                "  invited = host.guestList",
                "  attendees = [host]",
                "  peer_pressure = 0",
                "  for others in student_list:",
                "    if others.id in invited:",
                "      attendees.append(others)",
                "",
                "  party_friends(attendees)",
                "",
                "  #3 rounds of drinking or not",
                "  for j in range(3):",
                "    for member in attendees:",
                "      x = member.partyDrink(peer_pressure)",
                "      peer_pressure += (x/len(attendees))",
                "",
                "  host.guestList.remove(day)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "## Input variables",
                "",
                "MeanAttitude = 3",
                "NumberOfAgents = 1000",
                "wShy = 0.8 ",
                "wFr = 0.8 ",
                "wAtt = 0.5 ",
                "usageRateUse = 6 ",
                "attitudeUse = 1 ",
                "meanExperience = 1.6 ",
                "stdDevExperience = 1.6 ",
                "probabilityOfBust = 0.001",
                "maxGoodExperience = 3 ",
                "usageRateGradeDrop = 13"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "##making lists for the variables",
                "",
                "attitude_list = []",
                "shyness_list = []",
                "cap_list = []",
                "usagerate_list = []",
                "year_list = []",
                "",
                "",
                "for student in range(NumberOfAgents):",
                "  shyness_ = rd.normalvariate(0,1)",
                "  attitude_ = rd.normalvariate(MeanAttitude,0.5)",
                "  cap_ = rd.normalvariate(3.5, 0.5)",
                "  if cap_ < 2:",
                "    cap_ = 2",
                "  elif cap_ > 5:",
                "    cap_ = 5",
                "",
                "  if attitude_ < 0:",
                "    usagerate_ = 0",
                "  elif attitude_ <= 5:",
                "    usagerate_ = round(attitude_)",
                "  else:",
                "    usagerate_ = round(2*attitude_)",
                "",
                "  year_ = rd.choice([1,2,3,4])",
                "  ",
                "  shyness_list.append(shyness_)",
                "  attitude_list.append(attitude_)",
                "  cap_list.append(cap_)",
                "  year_list.append(year_)",
                "  usagerate_list.append(usagerate_)",
                "",
                "  #id, shyness, attitude, cap, usageRate, year"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "## making a dataframe that represents student attributes",
                "",
                "import pandas as pd",
                "",
                "id_list = rd.sample(range(1, 100000), NumberOfAgents)",
                "",
                "df = pd.DataFrame({\"id\": id_list,",
                "                   \"shyness\": shyness_list,",
                "                   \"attitude\": attitude_list,",
                "                   \"cap\": cap_list,",
                "                   \"usage\": usagerate_list,",
                "                   \"year\": year_list,",
                "                   })",
                "df.head()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### making a list of student objects",
                "",
                "student_list = []",
                "",
                "for index, row in df.iterrows():",
                "  #print(index)",
                "  #print(row)",
                "  student = Student(row['id'], ",
                "                    row['shyness'],",
                "                    row['attitude'],",
                "                    row['cap'],",
                "                    row['usage'],",
                "                    row['year'])",
                "  student_list.append(student)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "start_cap = []",
                "total_start_cap = 0",
                "for student in student_list:",
                "  start_cap.append(student.cap)",
                "  total_start_cap += student.cap",
                "",
                "avg_start_cap = total_start_cap/len(student_list)",
                "print(avg_start_cap)",
                "",
                "plt.hist(start_cap, color = 'purple')",
                "plt.title('Starting CAP')",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "now_attitude = []",
                "for student in student_list:",
                "  now_attitude.append(student.attitude)",
                "",
                "plt.hist(now_attitude)",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "start_usage = []",
                "for student in student_list:",
                "  start_usage.append(student.usageRate)",
                "",
                "plt.hist(start_usage)",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### The simulation",
                "usage_list = {}",
                "attitude_list = {}",
                "",
                "for week in range(13):",
                "  for student in student_list:",
                "    student.host_party(student_list)",
                "  if week == 0:",
                "    #make some friends",
                "    for student in student_list:            ",
                "      if student.year == 1:             ",
                "        for i in range(20):",
                "          if student != student_list[i]:",
                "            make_friends(student, student_list[i])",
                "      else:             ",
                "        for j in range(30):",
                "          if student != student_list[j]:",
                "            make_friends(student, student_list[j])",
                "",
                "  elif week %3 == 0 and week != 12:",
                "    #grade experience",
                "    for student in student_list:",
                "      student.gradeExperience()",
                "",
                "  elif week == 12:",
                "    #grade update",
                "    for student in student_list:",
                "      student.gradeUpdate()",
                "      if student.cap < 2:",
                "        student.inSchool = False",
                "        student_list.remove(student)",
                "",
                "  days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']",
                "  for day in days:",
                "    for student in student_list:",
                "      if day in student.host:",
                "        #party starts",
                "        party_time(student, student_list, day)",
                "                  ",
                "    #if there's no one hosting",
                "    else:",
                "      for loners in student_list:",
                "        loners.drinkAlone()",
                "",
                "  total_use = 0",
                "  for student in student_list:",
                "    total_use += student.usageRate",
                "  avg_use = total_use/len(student_list)  ",
                "  usage_list[week] = avg_use        ",
                "",
                "  total_att = 0",
                "  for student in student_list:",
                "    total_att += student.attitude",
                "  avg_att = total_att/len(student_list)",
                "  attitude_list[week] = avg_att"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "attitude_list"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "weeks = []",
                "att = []",
                "",
                "for key, value in attitude_list.items():",
                "  weeks.append(key)",
                "  att.append(value)",
                "",
                "plt.plot(weeks, att)",
                "plt.title('Average attitude over the semester')",
                "plt.xlabel('Week')",
                "plt.ylabel('Average attitude')",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "end_cap = []",
                "total_cap = 0",
                "for student in student_list:",
                "  end_cap.append(student.cap)",
                "  total_cap += student.cap",
                "",
                "avg_cap = total_cap/len(student_list)",
                "print(avg_cap)",
                "print(max(start_cap))",
                "print(max(end_cap))",
                "",
                "plt.hist(start_cap, color = 'orange')",
                "plt.hist(end_cap, color = 'indigo')",
                "plt.legend(['Start', 'End'])",
                "plt.title('Average CAP Score')",
                "plt.show()"
            ],
            "output_type": "stream"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "end_usage = []",
                "for student in student_list:",
                "  end_usage.append(student.usageRate)",
                "",
                "plt.hist(start_usage, color = 'blue')",
                "plt.hist(end_usage, color = 'red')",
                "plt.title('Number of drinks taken over the semester')",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "end_test = pd.DataFrame({'usage': end_usage, 'cap':end_cap})",
                "",
                "g1 = end_test[end_test['usage'] < 8]",
                "g2 = end_test[end_test['usage'] > 8]",
                "plt.hist(g1['cap'], color = 'blue')",
                "plt.hist(g2['cap'], color = 'orange')"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "##making lists for the variables - second run",
                "",
                "attitude_list = []",
                "shyness_list = []",
                "cap_list = []",
                "usagerate_list = []",
                "year_list = []",
                "",
                "",
                "for student in range(NumberOfAgents):",
                "  shyness_ = rd.normalvariate(0,1)",
                "  attitude_ = rd.normalvariate(MeanAttitude,0.5)",
                "  cap_ = rd.normalvariate(3.5, 0.5)",
                "  if cap_ < 2:",
                "    cap_ = 2",
                "  elif cap_ > 5:",
                "    cap_ = 5",
                "",
                "  if attitude_ < 0:",
                "    usagerate_ = 0",
                "  elif attitude_ <= 5:",
                "    usagerate_ = round(attitude_)",
                "  else:",
                "    usagerate_ = round(2*attitude_)",
                "",
                "  year_ = rd.choice([1,2,3,4])",
                "  ",
                "  shyness_list.append(shyness_)",
                "  attitude_list.append(attitude_)",
                "  cap_list.append(cap_)",
                "  year_list.append(year_)",
                "  usagerate_list.append(usagerate_)",
                "",
                "  #id, shyness, attitude, cap, usageRate, year",
                "",
                "",
                "id_list = rd.sample(range(1, 100000), NumberOfAgents)",
                "",
                "df = pd.DataFrame({\"id\": id_list,",
                "                   \"shyness\": shyness_list,",
                "                   \"attitude\": attitude_list,",
                "                   \"cap\": cap_list,",
                "                   \"usage\": usagerate_list,",
                "                   \"year\": year_list,",
                "                   })",
                "",
                "### making a list of student objects",
                "",
                "student_list = []",
                "",
                "for index, row in df.iterrows():",
                "  #print(index)",
                "  #print(row)",
                "  student = Student(row['id'], ",
                "                    row['shyness'],",
                "                    row['attitude'],",
                "                    row['cap'],",
                "                    row['usage'],",
                "                    row['year'])",
                "  student_list.append(student)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### The simulation - 2nd run",
                "usage_list_2 = {}",
                "attitude_list_2 = {}",
                "",
                "for week in range(13):",
                "  for student in student_list:",
                "    student.host_party(student_list)",
                "  if week == 0:",
                "    #make some friends",
                "    for student in student_list:            ",
                "      if student.year == 1:             ",
                "        for i in range(20):",
                "          if student != student_list[i]:",
                "            make_friends(student, student_list[i])",
                "      else:             ",
                "        for j in range(30):",
                "          if student != student_list[j]:",
                "            make_friends(student, student_list[j])",
                "",
                "  elif week %3 == 0 and week != 12:",
                "    #grade experience",
                "    for student in student_list:",
                "      student.gradeExperience()",
                "",
                "  elif week == 12:",
                "    #grade update",
                "    for student in student_list:",
                "      student.gradeUpdate()",
                "      if student.cap < 2:",
                "        student.inSchool = False",
                "        student_list.remove(student)",
                "",
                "  days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']",
                "  for day in days:",
                "    for student in student_list:",
                "      if day in student.host:",
                "        #party starts",
                "        party_time(student, student_list, day)",
                "                  ",
                "    #if there's no one hosting",
                "    else:",
                "      for loners in student_list:",
                "        loners.drinkAlone()",
                "",
                "  total_use = 0",
                "  for student in student_list:",
                "    total_use += student.usageRate",
                "  avg_use = total_use/len(student_list)  ",
                "  usage_list_2[week] = avg_use        ",
                "",
                "  total_att = 0",
                "  for student in student_list:",
                "    total_att += student.attitude",
                "  avg_att = total_att/len(student_list)",
                "  attitude_list_2[week] = avg_att"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "##making lists for the variables - third run",
                "",
                "attitude_list = []",
                "shyness_list = []",
                "cap_list = []",
                "usagerate_list = []",
                "year_list = []",
                "",
                "",
                "for student in range(NumberOfAgents):",
                "  shyness_ = rd.normalvariate(0,1)",
                "  attitude_ = rd.normalvariate(MeanAttitude,0.5)",
                "  cap_ = rd.normalvariate(3.5, 0.5)",
                "  if cap_ < 2:",
                "    cap_ = 2",
                "  elif cap_ > 5:",
                "    cap_ = 5",
                "",
                "  if attitude_ < 0:",
                "    usagerate_ = 0",
                "  elif attitude_ <= 5:",
                "    usagerate_ = round(attitude_)",
                "  else:",
                "    usagerate_ = round(2*attitude_)",
                "",
                "  year_ = rd.choice([1,2,3,4])",
                "  ",
                "  shyness_list.append(shyness_)",
                "  attitude_list.append(attitude_)",
                "  cap_list.append(cap_)",
                "  year_list.append(year_)",
                "  usagerate_list.append(usagerate_)",
                "",
                "  #id, shyness, attitude, cap, usageRate, year",
                "",
                "",
                "id_list = rd.sample(range(1, 100000), NumberOfAgents)",
                "",
                "df = pd.DataFrame({\"id\": id_list,",
                "                   \"shyness\": shyness_list,",
                "                   \"attitude\": attitude_list,",
                "                   \"cap\": cap_list,",
                "                   \"usage\": usagerate_list,",
                "                   \"year\": year_list,",
                "                   })",
                "",
                "### making a list of student objects",
                "",
                "student_list = []",
                "",
                "for index, row in df.iterrows():",
                "  #print(index)",
                "  #print(row)",
                "  student = Student(row['id'], ",
                "                    row['shyness'],",
                "                    row['attitude'],",
                "                    row['cap'],",
                "                    row['usage'],",
                "                    row['year'])",
                "  student_list.append(student)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### The simulation - run 3",
                "usage_list_3 = {}",
                "attitude_list_3 = {}",
                "",
                "for week in range(13):",
                "  for student in student_list:",
                "    student.host_party(student_list)",
                "  if week == 0:",
                "    #make some friends",
                "    for student in student_list:            ",
                "      if student.year == 1:             ",
                "        for i in range(20):",
                "          if student != student_list[i]:",
                "            make_friends(student, student_list[i])",
                "      else:             ",
                "        for j in range(30):",
                "          if student != student_list[j]:",
                "            make_friends(student, student_list[j])",
                "",
                "  elif week %3 == 0 and week != 12:",
                "    #grade experience",
                "    for student in student_list:",
                "      student.gradeExperience()",
                "",
                "  elif week == 12:",
                "    #grade update",
                "    for student in student_list:",
                "      student.gradeUpdate()",
                "      if student.cap < 2:",
                "        student.inSchool = False",
                "        student_list.remove(student)",
                "",
                "  days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']",
                "  for day in days:",
                "    for student in student_list:",
                "      if day in student.host:",
                "        #party starts",
                "        party_time(student, student_list, day)",
                "                  ",
                "    #if there's no one hosting",
                "    else:",
                "      for loners in student_list:",
                "        loners.drinkAlone()",
                "",
                "  total_use = 0",
                "  for student in student_list:",
                "    total_use += student.usageRate",
                "  avg_use = total_use/len(student_list)  ",
                "  usage_list_3[week] = avg_use        ",
                "",
                "  total_att = 0",
                "  for student in student_list:",
                "    total_att += student.attitude",
                "  avg_att = total_att/len(student_list)",
                "  attitude_list_3[week] = avg_att"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "##making lists for the variables - fourth run",
                "",
                "attitude_list = []",
                "shyness_list = []",
                "cap_list = []",
                "usagerate_list = []",
                "year_list = []",
                "",
                "",
                "for student in range(NumberOfAgents):",
                "  shyness_ = rd.normalvariate(0,1)",
                "  attitude_ = rd.normalvariate(MeanAttitude,0.5)",
                "  cap_ = rd.normalvariate(3.5, 0.5)",
                "  if cap_ < 2:",
                "    cap_ = 2",
                "  elif cap_ > 5:",
                "    cap_ = 5",
                "",
                "  if attitude_ < 0:",
                "    usagerate_ = 0",
                "  elif attitude_ <= 5:",
                "    usagerate_ = round(attitude_)",
                "  else:",
                "    usagerate_ = round(2*attitude_)",
                "",
                "  year_ = rd.choice([1,2,3,4])",
                "  ",
                "  shyness_list.append(shyness_)",
                "  attitude_list.append(attitude_)",
                "  cap_list.append(cap_)",
                "  year_list.append(year_)",
                "  usagerate_list.append(usagerate_)",
                "",
                "  #id, shyness, attitude, cap, usageRate, year",
                "",
                "",
                "id_list = rd.sample(range(1, 100000), NumberOfAgents)",
                "",
                "df = pd.DataFrame({\"id\": id_list,",
                "                   \"shyness\": shyness_list,",
                "                   \"attitude\": attitude_list,",
                "                   \"cap\": cap_list,",
                "                   \"usage\": usagerate_list,",
                "                   \"year\": year_list,",
                "                   })",
                "",
                "### making a list of student objects",
                "",
                "student_list = []",
                "",
                "for index, row in df.iterrows():",
                "  #print(index)",
                "  #print(row)",
                "  student = Student(row['id'], ",
                "                    row['shyness'],",
                "                    row['attitude'],",
                "                    row['cap'],",
                "                    row['usage'],",
                "                    row['year'])",
                "  student_list.append(student)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "### The simulation - run 4",
                "usage_list_4 = {}",
                "attitude_list_4 = {}",
                "",
                "for week in range(13):",
                "  for student in student_list:",
                "    student.host_party(student_list)",
                "  if week == 0:",
                "    #make some friends",
                "    for student in student_list:            ",
                "      if student.year == 1:             ",
                "        for i in range(20):",
                "          if student != student_list[i]:",
                "            make_friends(student, student_list[i])",
                "      else:             ",
                "        for j in range(30):",
                "          if student != student_list[j]:",
                "            make_friends(student, student_list[j])",
                "",
                "  elif week %3 == 0 and week != 12:",
                "    #grade experience",
                "    for student in student_list:",
                "      student.gradeExperience()",
                "",
                "  elif week == 12:",
                "    #grade update",
                "    for student in student_list:",
                "      student.gradeUpdate()",
                "      if student.cap < 2:",
                "        student.inSchool = False",
                "        student_list.remove(student)",
                "",
                "  days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']",
                "  for day in days:",
                "    for student in student_list:",
                "      if day in student.host:",
                "        #party starts",
                "        party_time(student, student_list, day)",
                "                  ",
                "    #if there's no one hosting",
                "    else:",
                "      for loners in student_list:",
                "        loners.drinkAlone()",
                "",
                "  total_use = 0",
                "  for student in student_list:",
                "    total_use += student.usageRate",
                "  avg_use = total_use/len(student_list)  ",
                "  usage_list_4[week] = avg_use        ",
                "",
                "  total_att = 0",
                "  for student in student_list:",
                "    total_att += student.attitude",
                "  avg_att = total_att/len(student_list)",
                "  attitude_list_4[week] = avg_att"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "week2 = []",
                "usage2 = []",
                "",
                "for key, value in usage_list_2.items():",
                "  week2.append(key)",
                "  usage2.append(value)",
                "",
                "week3 = []",
                "usage3 = []",
                "",
                "for key, value in usage_list_3.items():",
                "  week3.append(key)",
                "  usage3.append(value)",
                "",
                "week4 = []",
                "usage4 = []",
                "",
                "for key, value in usage_list_4.items():",
                "  week4.append(key)",
                "  usage4.append(value)",
                "",
                "plt.plot(week4, usage4, label = 'sem 4')",
                "plt.plot(week3, usage3, label = 'sem 3')",
                "plt.plot(week2, usage2, label = 'sem 2')",
                "#plt.plot(weeks, usages, label = 'sem 1')",
                "plt.title('Average usage over the sem')",
                "plt.xlabel('Week')",
                "plt.ylabel('Average usage rate')",
                "plt.legend()",
                "#plt.show()"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "week2 = []",
                "att2 = []",
                "",
                "for key, value in attitude_list_2.items():",
                "  week2.append(key)",
                "  att2.append(value)",
                "",
                "week3 = []",
                "att3 = []",
                "",
                "for key, value in attitude_list_3.items():",
                "  week3.append(key)",
                "  att3.append(value)",
                "",
                "week4 = []",
                "att4 = []",
                "",
                "for key, value in attitude_list_4.items():",
                "  week4.append(key)",
                "  att4.append(value)",
                "",
                "plt.plot(week4, att4, label = 'sem 4')",
                "plt.plot(week3, att3, label = 'sem 3')",
                "plt.plot(week2, att2, label = 'sem 2')",
                "plt.plot(weeks, att, label = 'sem 1')",
                "plt.title('Average attitude over the sem')",
                "plt.xlabel('Week')",
                "plt.ylabel('Average attitude level')",
                "plt.legend()",
                "plt.show()"
            ],
            "output_type": "display_data"
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "friendship_len = []",
                "shyness_score = []",
                "",
                "for student in student_list:",
                "  friendship_len.append(len(student.friends))",
                "  shyness_score.append(student.shyness)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(shyness_score, friendship_len)"
            ],
            "output_type": "execute_result"
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "content": [
                "from sklearn.metrics import log_loss\n",
                "def calc_logloss(targets, outputs, eps=1e-6):\n",
                "    logloss_classes = [log_loss(np.floor(targets[:,i]), np.clip(outputs[:,i], eps, 1-eps)) for i in range(6)]\n",
                "    return np.average(logloss_classes, weights=[2,1,1,1,1,1])\n",
                "\n",
                "import pandas as pd\n",
                "import pickle\n",
                "import os\n",
                "import numpy as np\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "dup = pd.read_csv(\"../input/stage1-test-gt/dup_s1_test.csv\")\n",
                "test = pd.read_csv(\"../input/stage1-test-gt/s1_test_results.csv\")\n",
                "test = test.merge(dup, on = 'SOPInstanceUID', how = 'left')"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "def get_split_result(filename, test, eps, rm_dup=False):\n",
                "    f1 = pd.read_csv(filename)\n",
                "\n",
                "    f1['type'] = f1['ID'].apply(lambda x: x.split('_')[2])\n",
                "    f1['name'] = f1['ID'].apply(lambda x: x.split('_')[1])\n",
                "\n",
                "    name = f1[['name']]\n",
                "\n",
                "    f1_epidural = f1[['name','Label']][f1['type'] == 'epidural']\n",
                "    f1_epidural.columns = ['name','epidural']\n",
                "    f1_intraparenchymal = f1[['name','Label']][f1['type'] == 'intraparenchymal']\n",
                "    f1_intraparenchymal.columns = ['name','intraparenchymal']\n",
                "    f1_intraventricular = f1[['name','Label']][f1['type'] == 'intraventricular']\n",
                "    f1_intraventricular.columns = ['name','intraventricular']\n",
                "    f1_subarachnoid = f1[['name','Label']][f1['type'] == 'subarachnoid']\n",
                "    f1_subarachnoid.columns = ['name','subarachnoid']\n",
                "    f1_subdural = f1[['name','Label']][f1['type'] == 'subdural']\n",
                "    f1_subdural.columns = ['name','subdural']\n",
                "    f1_any = f1[['name','Label']][f1['type'] == 'any']\n",
                "    f1_any.columns = ['name','any']\n",
                "\n",
                "    name = name.merge(f1_any, on = 'name', how = 'left')\n",
                "    name = name.merge(f1_epidural, on = 'name', how = 'left')\n",
                "    name = name.merge(f1_intraparenchymal, on = 'name', how = 'left')\n",
                "    name = name.merge(f1_intraventricular, on = 'name', how = 'left')\n",
                "    name = name.merge(f1_subarachnoid, on = 'name', how = 'left')\n",
                "    name = name.merge(f1_subdural, on = 'name', how = 'left')\n",
                "    name = name.drop_duplicates()\n",
                "    name.rename(columns = {'name': 'SOPInstanceUID'}, inplace=True)\n",
                "    name['SOPInstanceUID'] = 'ID_' + name['SOPInstanceUID']\n",
                "    \n",
                "    name = name.merge(test, on = 'SOPInstanceUID', how = 'left')\n",
                "    \n",
                "    if rm_dup:\n",
                "        name_use = name[name['dup'].isnull() == True] #remove duplicate patientID\n",
                "    else:\n",
                "        name_use = name.copy()  #all test\n",
                "    gt = name_use[['any_y',\n",
                "           'epidural_y', 'subdural_y', 'subarachnoid_y', 'intraventricular_y',\n",
                "           'intraparenchymal_y']].values\n",
                "    pred = name_use[['any',\n",
                "               'epidural', 'subdural', 'subarachnoid', 'intraventricular',\n",
                "               'intraparenchymal']].values\n",
                "    return calc_logloss(gt, pred, eps=eps)"
            ],
            "output_type": "not_existent"
        },
        {
            "tags": [
                "process_data"
            ],
            "output_type": "execute_result",
            "content": [
                "#come from https://www.kaggle.com/krishnakatyal/keras-efficientnet-b3\n",
                "get_split_result(\"../input/kernel-0076/submission.csv\", test, 1e-6)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "output_type": "execute_result",
            "content": [
                "get_split_result(\"../input/kernel-0076/submission.csv\", test, 1e-6, rm_dup=True)"
            ]
        }
    ]
}